public void test() { try { LOG . trace ( "Registering service {}" , this ) ; camelContext . addService ( this , true , true ) ; } catch ( Exception e ) { throw RuntimeCamelException . wrapRuntimeCamelException ( e ) ; } }
public void test() { try { return openIterator ( getQueryConstraints ( ) ) ; } catch ( TransformException | FactoryException e ) { log . error ( "Error creating query constraints" , e ) ; } }
protected void handleRemoteDisconnect ( final ProtonConnection con ) { log . debug ( "Connection closed: " + con . toString ( ) ) ; con . disconnect ( ) ; publishConnectionClosedEvent ( con ) ; }
@ Override public Map < K , ICacheElement < K , V > > getMultiple ( final Set < K > keys ) { log . debug ( "getMultiple(s)" ) ; return getMultipleSetupMap . get ( keys ) ; }
public void test() { if ( applicationContext . getResource ( resolvedLocation ) . isReadable ( ) ) { LOGGER . info ( "Reading configuration '{}'" , location ) ; return applicationContext . getResource ( resolvedLocation ) ; } else-if ( throwErrorIfNotReadable ) { throw new IllegalStateException ( String . format ( "Configuration %s does not exist or is not readable" , location ) ) ; } else { LOGGER . warn ( "Configuration {} not readable; ignored" , location ) ; return null ; } }
public void test() { if ( applicationContext . getResource ( resolvedLocation ) . isReadable ( ) ) { LOGGER . debug ( "Configuration {} detected and added" , location ) ; return applicationContext . getResource ( resolvedLocation ) ; } else-if ( throwErrorIfNotReadable ) { throw new IllegalStateException ( String . format ( "Configuration %s does not exist or is not readable" , location ) ) ; } else { LOGGER . debug ( "Configuration {} already added" , location ) ; return null ; } }
public void test() { for ( String cache : pools ) { code_block = IfStatement ; } }
public void testForMemoryLeaks ( ) throws Exception { final long differenceMemoryCache = thrashCache ( ) ; logger . info ( "{} - {}" , differenceMemoryCache , differenceMemoryCache ) ; assertTrue ( differenceMemoryCache < 500000 ) ; }
public void test() { if ( code == HttpStatus . SC_OK ) { String content = post . getResponseBodyAsString ( ) ; Map < String , Object > resp = gson . fromJson ( content , new TypeToken < Map < String , Object > > ( ) code_block = "" ; . getType ( ) ) ; LOG . info ( "Received from Zeppelin LoginRestApi : " + content ) ; return ( Map < String , String > ) resp . get ( "body" ) ; } else { LOG . error ( "Unexpected error from Zeppelin LoginRestApi : " + code + " " + HttpStatus . SC_OK ) ; return Collections . emptyMap ( ) ; } }
public void test() { try { int code = client . executeMethod ( post ) ; code_block = IfStatement ; } catch ( IOException e ) { LOGGER . log ( Level . SEVERE , "Could not execute request" , e ) ; return Collections . emptyMap ( ) ; } }
@ Test ( timeout = 60000 ) public void testPresettledReceiverReadsAllMessages ( ) throws Exception { final int MSG_COUNT = 100 ; sendMessages ( getQueueName ( ) , MSG_COUNT ) ; AmqpClient client = createAmqpClient ( ) ; AmqpConnection connection = addConnection ( client . connect ( ) ) ; AmqpSession session = connection . createSession ( ) ; AmqpReceiver receiver = session . createReceiver ( getQueueName ( ) , null , false , true ) ; final Queue queueView = getProxyToQueue ( getQueueName ( ) ) ; assertEquals ( MSG_COUNT , queueView . getMessageCount ( ) ) ; receiver . flow ( MSG_COUNT ) ; code_block = ForStatement ; receiver . close ( ) ; receiver = session . createReceiver ( getQueueName ( ) ) ; receiver . flow ( 1 ) ; AmqpMessage received = receiver . receive ( 5 , TimeUnit . SECONDS ) ; code_block = IfStatement ; assertNull ( received ) ; assertEquals ( 0 , queueView . getMessageCount ( ) ) ; connection . close ( ) ; LOG . info ( "got expected" ) ; }
public void test() { if ( received != null ) { LOG . info ( "Received: " + received . toString ( ) ) ; } }
public void test() { if ( seenModules != null ) { logger . info ( "modules: " + seenModules . toString ( ) ) ; } }
public void test() { for ( URL url : getURLs ( ) ) { LOG . debug ( url . toString ( ) ) ; } }
public void test() { if ( log != null && log . isDebugEnabled ( ) ) { log . debug ( "<" + id + ">" ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( DLFileEntryTypeServiceUtil . class , "updateFileEntryType" , _updateFileEntryTypeParameterTypes16 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , fileEntryTypeId , nameMap , descriptionMap , ddmStructureIds , serviceContext ) ; code_block = TryStatement ;  } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { this . isLocalVariable = false ; this . visibility = VisibilitySet . PUBLIC . toString ( ) ; this . isFinal = true ; this . hasClassScope = true ; dispatchAnnotationsOfMember ( modifierList , belongsToClass ) ; this . declareType = determineTypeOfTypeType ( typeType , belongsToClass ) ; determine_name ( variableDeclarators ) ; } catch ( Exception e ) { logger . warn ( " Exception while processing: " + belongsToClass + " Line: " + belongsToClass + " " + e . getMessage ( ) ) ; } }
private void infolog ( String msg ) { processingLog += msg + ".\n" ; logger . debug ( processingLog ) ; }
public void test() { try { restTemplate = getRestTemplateForReadingLinkedData ( requesterWebID ) ; } catch ( Exception e ) { LOGGER . log ( Level . SEVERE , "Could not read rest template for requesterWebID: {0}" , requesterWebID ) ; throw new RuntimeException ( e ) ; } }
public void test() { try { JsonNode jsonNode = MAPPER . readTree ( IOUtils . toString ( getClass ( ) . getResourceAsStream ( "/MIMETypes.json" ) ) ) ; code_block = ForStatement ; mimeTypesMap = Collections . unmodifiableMap ( mimeTypesMap ) ; LOG . debug ( "MIME types loaded: {}" , mimeTypesMap ) ; mimeTypes = new ArrayList < > ( mimeTypesMap . keySet ( ) ) ; Collections . sort ( mimeTypes ) ; mimeTypes = Collections . unmodifiableList ( mimeTypes ) ; } catch ( Exception e ) { LOG . error ( "Error while loading mime types" , e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( FragmentCompositionServiceUtil . class , "deleteFragmentComposition" , _deleteFragmentCompositionParameterTypes1 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , fragmentCompositionId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . fragment . model . FragmentComposition ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( rv != EXIT_SUCCESS ) { LOGGER . info ( "exit code=block = IfStatement ; } else { LOGGER . info ( "============= Workflow executed sucessfully ===============" ) ; } }
public void test() { if ( rv != EXIT_SUCCESS ) { LOGGER . info ( "========= Workflow did not execute sucessfully ============" ) ; retVal = rv ; code_block = IfStatement ; } else { LOGGER . info ( "========= Workflow completed  ============" ) ; } }
@ Override public void process ( final FilterChain chain , final Request request , final Response response ) { long tm = System . currentTimeMillis ( ) ; logger . debug ( request . toString ( ) ) ; code_block = TryStatement ;  tm = System . currentTimeMillis ( ) - tm ; }
@ Override public void doStopSelfTest ( final OslpEnvelope oslpRequest , final DeviceRequest deviceRequest , final DeviceResponseHandler deviceResponseHandler , final String ipAddress ) throws IOException { LOGGER . info ( "doStopSelfTest() for device: {}" , deviceRequest . getDeviceIdentification ( ) ) ; this . saveOslpRequestLogEntry ( deviceRequest , oslpRequest ) ; final OslpResponseHandler oslpResponseHandler = new OslpResponseHandler ( ) code_block = "" ; ; this . sendMessage ( ipAddress , oslpRequest , oslpResponseHandler , deviceRequest ) ; }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { if ( ACHILLES_DML_LOGGER . isDebugEnabled ( ) ) { ACHILLES_DML_LOGGER . debug ( "Query {}" , query ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { try { return createProjectVersion ( designRepository . check ( designFolderName ) ) ; } catch ( IOException e ) { log . error ( e . getMessage ( ) , e ) ; } }
public void runRepeatedly ( int numIterations ) { RebalanceStrategy strategy = new AutoRebalanceStrategy ( RESOURCE_NAME , _partitions , _states , _maxPerNode ) ; ZNRecord initialResult = strategy . computePartitionAssignment ( _allNodes , _liveNodes , _currentMapping , null ) ; logger . info ( initialResult ) ; _currentMapping = getMapping ( initialResult . getListFields ( ) ) ; logger . info ( _currentMapping . toString ( ) ) ; getRunResult ( _currentMapping , initialResult . getListFields ( ) ) ; code_block = ForStatement ; }
public void runRepeatedly ( int numIterations ) { logger . info ( "~~~~ Initial State ~~~~~" ) ; RebalanceStrategy strategy = new AutoRebalanceStrategy ( RESOURCE_NAME , _partitions , _states , _maxPerNode ) ; ZNRecord initialResult = strategy . computePartitionAssignment ( _allNodes , _liveNodes , _currentMapping , null ) ; _currentMapping = getMapping ( initialResult . getListFields ( ) ) ; getRunResult ( _currentMapping , initialResult . getListFields ( ) ) ; code_block = ForStatement ; logger . info ( "~~~~ ~~~~~" ) ; }
public void test() { if ( znRecord != null ) { final Map < String , List < String > > listResult = znRecord . getListFields ( ) ; final Map < String , Map < String , String > > mapResult = getMapping ( listResult ) ; logger . info ( mapResult . toString ( ) ) ; getRunResult ( mapResult , listResult ) ; _currentMapping = mapResult ; logger . info ( "Current mapping: " + _currentMapping ) ; } }
public List < Asset > findAll ( Long repositoryId , String path , Boolean deleted , Boolean virtual , Long branchId ) { LOGGER . debug ( "findAll() - repositoryId={}, path={}, deleted={}, branchId={}" , repositoryId , path , deleted , branchId ) ; Specification < Asset > assetSpecifications = distinct ( ifParamNotNull ( repositoryIdEquals ( repositoryId ) ) ) . and ( ifParamNotNull ( pathEquals ( path ) ) ) . and ( ifParamNotNull ( deletedEquals ( deleted ) ) ) . and ( ifParamNotNull ( virtualEquals ( virtual ) ) ) . and ( ifParamNotNull ( branchId ( branchId , deleted ) ) ) ; List < Asset > all = assetRepository . findAll ( assetSpecifications ) ; return all ; }
public void test() { -> { code_block = IfStatement ; RemoteSessionObject remoteSessionObject = new RemoteSessionObject ( user ) ; String xmlString = remoteSessionObject . toString ( ) ; log . debug ( "jsonString {}" , xmlString ) ; String hash = soapDao . addSOAPLogin ( sid , options ) ; code_block = IfStatement ; log . debug ( "SOAP login successful" ) ; return UNKNOWN ; } }
public void test() { try ( Response response = make ( request ) ) { check ( response ) ; return RequestResponse . parseFromResponse ( response ) ; } catch ( VitamClientInternalException e ) { throw new AccessExternalClientException ( e ) ; } catch ( AdminExternalClientException e ) { LOGGER . error ( e ) ; return e . getVitamError ( ) ; } }
public void test() { try { code_block = ForStatement ; } catch ( Exception e ) { logger . error ( Messages . getInstance ( ) . getErrorString ( "Workspace.ERROR_0002_PROPS_EXCEPTION" ) , e ) ; } }
private void writeExtensions ( CertificatePair pair ) { appendBytes ( pair . getExtensions ( ) . getValue ( ) ) ; LOGGER . debug ( "Extensions: " + ArrayConverter . bytesToHexString ( pair . getExtensions ( ) . getValue ( ) ) ) ; }
public void test() { try { GeneralBuildingBlock gBBInput = execution . getGeneralBuildingBlock ( ) ; RequestContext requestContext = gBBInput . getRequestContext ( ) ; ServiceInstance serviceInstance = extractPojosForBB . extractByKey ( execution , ResourceKey . SERVICE_INSTANCE_ID ) ; GenericVnf vnf = extractPojosForBB . extractByKey ( execution , ResourceKey . GENERIC_VNF_ID ) ; VfModule vfModule = extractPojosForBB . extractByKey ( execution , ResourceKey . VF_MODULE_ID ) ; Customer customer = gBBInput . getCustomer ( ) ; CloudRegion cloudRegion = gBBInput . getCloudRegion ( ) ; SDNCRequest sdncRequest = new SDNCRequest ( ) ; GenericResourceApiVfModuleOperationInformation req = sdncVfModuleResources . deactivateVfModule ( vfModule , vnf , serviceInstance , customer , cloudRegion , requestContext , buildCallbackURI ( sdncRequest ) ) ; sdncRequest . setSDNCPayload ( req ) ; sdncRequest . setTopology ( SDNCTopology . VFMODULE ) ; execution . setVariable ( SDNC_REQUEST , sdncRequest ) ; } catch ( Exception ex ) { logger . error ( "Exception occurred" , ex ) ; exceptionUtil . buildAndThrowWorkflowException ( execution , 7000 , ex ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { deSerializedValue = elementSerializer . deSerialize ( serialized . getSerializedValue ( ) , null ) ; } catch ( final ClassNotFoundException | IOException e ) { LOGGER . error ( "Could not deserialize " + serialized . getSerializedValue ( ) , e ) ; throw e ; } }
public void test() { try { Attribute at = m . getAttribute ( ) ; code_block = IfStatement ; } catch ( LdapInvalidAttributeValueException e ) { logger . warn ( "Error retrieving attribute: {}" , e . getMessage ( ) ) ; } }
public void close ( ) { LOG . info ( "close" ) ; running . set ( false ) ; receiver . interrupt ( ) ; connection . close ( ) ; }
@ Test public void testDb2TableCreation ( ) { FhirSchemaGenerator gen = new FhirSchemaGenerator ( ADMIN_SCHEMA_NAME , SCHEMA_NAME , false ) ; PhysicalDataModel model = new PhysicalDataModel ( ) ; gen . buildSchema ( model ) ; PrintTarget tgt = new PrintTarget ( null , logger . isLoggable ( Level . FINE ) ) ; Db2Adapter adapter = new Db2Adapter ( tgt ) ; model . apply ( adapter ) ; logger . info ( "testDb2TableCreation" ) ; }
@ Override public void startApplication ( String applicationName ) { logger . debug ( Messages . GETTING_APPLICATION_0 , applicationName ) ; delegate . startApplication ( applicationName ) ; }
@ Path ( "/getAllShouldSucceed" ) @ POST public void getAllShouldSucceed ( ) { LOG . debug ( "Calling OpenstackNodesResource.getAllShouldSucceed()" ) ; String uri = String . format ( URI_FORMAT , OpenstackConstants . GET_ALL ) ; Network [ ] networks = template . requestBody ( uri , null , Network [ ] . class ) ; assertNotNull ( networks ) ; assertEquals ( 1 , networks . length ) ; assertEquals ( NETWORK_NAME , networks [ 0 ] . getName ( ) ) ; assertNotNull ( networks [ 0 ] . getSubnets ( ) ) ; assertEquals ( 1 , networks [ 0 ] . getSubnets ( ) . size ( ) ) ; assertEquals ( "0c4faf33-8c23-4dc9-8bf5-30dd1ab452f9" , networks [ 0 ] . getSubnets ( ) . get ( 0 ) ) ; assertEquals ( "73f6f1ac-5e58-4801-88c3-7e12c6ddfb39" , networks [ 0 ] . getId ( ) ) ; assertEquals ( NetworkType . VXLAN , networks [ 0 ] . getNetworkType ( ) ) ; }
public void test() { if ( System . currentTimeMillis ( ) - lastLaunch < RELAUNCH_EXCLUSION_WINDOW_MILLIS ) { LOGGER . info ( String . format ( "Restarting exclusion for %s" , RELAUNCH_EXCLUSION_WINDOW_MILLIS ) ) ; break ; } else-if ( code == 2 ) { LOGGER . info ( "User requested restart." ) ; } else { code_block = IfStatement ; code_block = IfStatement ; } }
public void test() { if ( System . currentTimeMillis ( ) - lastLaunch < RELAUNCH_EXCLUSION_WINDOW_MILLIS ) { LOGGER . info ( "Encountered a subsequent failure. Giving up." ) ; break ; } else-if ( code == 2 ) { LOGGER . info ( "Encountered failure. Giving up." ) ; } else { code_block = IfStatement ; code_block = IfStatement ; } }
public void test() { if ( code != 0 && ourRestart ) { LOGGER . error ( "Restarting failed, reason: " + code ) ; } else { LOGGER . info ( "Exiting with status " + code ) ; break ; } }
public void test() { try { ( new ResponseContentHandle ( ) ) . parse ( data , msg ) ; } catch ( IOException e ) { LOGGER . warn ( "IOException in ResponseContentHandle" , e ) ; } }
public void kinit ( String user ) throws Exception { log . info ( "kinit" ) ; UserGroupInformation . loginUserFromKeytab ( user , KEYTAB_LOCATION + "/" + user + ".keytab" ) ; }
@ Override public void onError ( SubscriptionException error ) { logger . info ( name . getMethodName ( ) + " - callback - error" ) ; subscribeBroadcastWithSingleStructParameterCallbackResult = false ; subscribeBroadcastWithSingleStructParameterCallbackDone = true ; }
private void changeTrackPosition ( long positionOffsetInMs ) throws SpeakerException { long currentPosition = speaker . getPlayState ( ) . getPositionInMs ( ) ; logger . debug ( "Changing position from {} to {}" , currentPosition , positionOffsetInMs ) ; speaker . setPosition ( currentPosition + positionOffsetInMs ) ; }
public void test() { try { master . getValue ( ) . dispose ( ) ; } catch ( java . lang . Exception e ) { LOGGER . warn ( "Exception disposing master object" , e ) ; } }
public void test() { try { CarbonAppPersistenceManager capm = new CarbonAppPersistenceManager ( getAxisConfig ( ) ) ; regConfig = capm . loadRegistryConfig ( AppDeployerConstants . APPLICATIONS + parentAppName + AppDeployerConstants . APP_DEPENDENCIES + artifactName ) ; } catch ( Exception e ) { log . error ( "Error loading registry config" , e ) ; } }
public void test() { if ( s_logger . isDebugEnabled ( ) ) { s_logger . debug ( log ( seq , "Unable to move " + req . toString ( ) ) ) ; } }
public void test() { try { String result = connect ( cmd . getName ( ) , privateIp , cmdPort ) ; code_block = IfStatement ; } catch ( Exception e ) { s_logger . warn ( "execute command " + cmd . getName ( ) + " failed" , e ) ; return new CheckSshAnswer ( cmd , e ) ; } }
public void test() { if ( s_logger . isDebugEnabled ( ) ) { s_logger . debug ( log ( seq , "Unable to move " + req . toString ( ) ) ) ; } }
public void test() { if ( s_logger . isDebugEnabled ( ) ) { s_logger . debug ( log ( seq , "Unable to move " + req . toString ( ) ) ) ; } }
public void test() { if ( o == objectFilename ) { String val = ( ( JTextField ) o ) . getText ( ) ; sf . loadObjectImageFilename ( val ) ; } else { log . error ( "Object filename not found." ) ; } }
public void test() { try { data . close ( ) ; } catch ( Exception e ) { log . error ( "Unable to close buffer" , e ) ; } }
public void test() { try { writeTask . cancel ( true ) ; } catch ( Exception e ) { LOG . error ( "Failed to cancel write task" , e ) ; } }
public void test() { if ( ! snapThreadMutex . tryAcquire ( ) ) { LOG . error ( "Snapshot Thread was not found on the snapshot thread" ) ; } else { new ZooKeeperThread ( "Snapshot Thread" ) code_block = "" ; . start ( ) ; } }
public void test() { try { zks . takeSnapshot ( ) ; } catch ( Exception e ) { LOG . warn ( "Error taking snapshot." , e ) ; } finally { snapThreadMutex . release ( ) ; } }
public void test() { if ( IS_DEBUG ) { LOG . debug ( "CSV1 created" ) ; } }
public void test() { if ( virtualDatapointHandler != null ) { logger . debug ( "Handling virtual datapoint '{}' on gateway with id '{}'" , dp . getName ( ) , id ) ; virtualDatapointHandler . handleCommand ( gateway , dp , dpConfig , newValue ) ; } else-if ( dp . isScript ( ) ) { code_block = IfStatement ; } else-if ( dp . isVariable ( ) ) { logger . debug ( "Sending variable '{}' with value '{}' to gateway with id '{}'" , dp . getInfo ( ) , newValue , id ) ; setVariable ( dp , newValue ) ; } else { logger . debug ( "Sending virtual datapoint '{}' to gateway with id '{}'" , dp . getInfo ( ) , newValue , rxMode ) ; getRpcClient ( dp . getChannel ( ) . getDevice ( ) . getHmInterface ( ) ) . setDatapointValue ( dp , newValue , rxMode ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { SharedData shared = ( SharedData ) context . get ( SHARED_DATA ) ; code_block = IfStatement ; saveData ( context ) ; reporter . addObjectReport ( context , "merged" , OBJECT_TYPE . CONNECTION_LINK , "connection links" , OBJECT_STATE . OK , IO_TYPE . OUTPUT ) ; reporter . addObjectReport ( context , "merged" , OBJECT_TYPE . ACCESS_POINT , "access points" , OBJECT_STATE . OK , IO_TYPE . OUTPUT ) ; reporter . addObjectReport ( context , "merged" , OBJECT_TYPE . STOP_AREA , "stop areas" , OBJECT_STATE . OK , IO_TYPE . OUTPUT ) ; reporter . setStatToObjectReport ( context , "merged" , OBJECT_TYPE . CONNECTION_LINK , OBJECT_TYPE . CONNECTION_LINK , shared . connectionLinks . getItems ( ) . size ( ) ) ; reporter . setStatToObjectReport ( context , "merged" , OBJECT_TYPE . ACCESS_POINT , OBJECT_TYPE . ACCESS_POINT , shared . accessPoints . getItems ( ) . size ( ) ) ; reporter . setStatToObjectReport ( context , "merged" , OBJECT_TYPE . STOP_AREA , OBJECT_TYPE . STOP_AREA , shared . physicalStops . getItems ( ) . size ( ) + shared . commercialStops . getItems ( ) . size ( ) + shared . getStopPlaces ( ) . getItems ( ) . size ( ) ) ; result = SUCCESS ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; } finally { log . info ( Color . MAGENTA + monitor . stop ( ) + Color . NORMAL ) ; } }
public void test() { try { SharedData shared = ( SharedData ) context . get ( SHARED_DATA ) ; code_block = IfStatement ; saveData ( context ) ; reporter . addObjectReport ( context , "merged" , OBJECT_TYPE . CONNECTION_LINK , "connection links" , OBJECT_STATE . OK , IO_TYPE . OUTPUT ) ; reporter . addObjectReport ( context , "merged" , OBJECT_TYPE . ACCESS_POINT , "access points" , OBJECT_STATE . OK , IO_TYPE . OUTPUT ) ; reporter . addObjectReport ( context , "merged" , OBJECT_TYPE . STOP_AREA , "stop areas" , OBJECT_STATE . OK , IO_TYPE . OUTPUT ) ; reporter . setStatToObjectReport ( context , "merged" , OBJECT_TYPE . CONNECTION_LINK , OBJECT_TYPE . CONNECTION_LINK , shared . connectionLinks . getItems ( ) . size ( ) ) ; reporter . setStatToObjectReport ( context , "merged" , OBJECT_TYPE . ACCESS_POINT , OBJECT_TYPE . ACCESS_POINT , shared . accessPoints . getItems ( ) . size ( ) ) ; reporter . setStatToObjectReport ( context , "merged" , OBJECT_TYPE . STOP_AREA , OBJECT_TYPE . STOP_AREA , shared . physicalStops . getItems ( ) . size ( ) + shared . commercialStops . getItems ( ) . size ( ) + shared . getStopPlaces ( ) . getItems ( ) . size ( ) ) ; result = SUCCESS ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; } finally { log . info ( Color . MAGENTA + monitor . stop ( ) + Color . NORMAL ) ; } }
public void test() { if ( annotationType == null ) { LOGGER . log ( Level . WARNING , "No annotation of type " + this . getClass ( ) . getName ( ) + ". Ignoring it." ) ; } else { Stack < Type > s = new Stack < > ( ) ; annotationType . collectHints ( an , hints , new HashSet < > ( ) , s ) ; } }
public static void main ( String [ ] args ) { SpringApplication . run ( SpringLoggerApplication . class , args ) ; logger . debug ( "Starting my application in debug with {} arguments" , args . length ) ; logger . info ( "Starting my application with {} arguments." , args . length ) ; logger . info ( "Starting my application with {} arguments." , args . length ) ; }
public static void main ( String [ ] args ) { logger . info ( "Before Starting application" ) ; SpringApplication . run ( SpringLoggerApplication . class , args ) ; logger . info ( "After Starting my application with {} arguments." , args . length ) ; logger . info ( "Finished." ) ; }
public static void main ( String [ ] args ) { logger . info ( "Before Starting application" ) ; SpringApplication . run ( SpringLoggerApplication . class , args ) ; logger . info ( "After Start application" ) ; logger . debug ( "Starting my application in debug with {} arguments" , args . length ) ; }
@ Override public int ignite ( String host , int port , SslStores sslStores , int maxThreads , int minThreads , int threadIdleTimeoutMillis ) throws ContainerInitializationException { Timer . start ( "SPARK_EMBEDDED_IGNITE" ) ; code_block = IfStatement ; log . info ( "SPARK_EMBEDDED_IGNITE" ) ; sparkFilter . init ( null ) ; Timer . stop ( "SPARK_EMBEDDED_IGNITE" ) ; return port ; }
public void test() { try { com . liferay . commerce . price . list . model . CommercePriceList returnValue = CommercePriceListServiceUtil . fetchCatalogBaseCommercePriceListByType ( groupId , type ) ; return com . liferay . commerce . price . list . model . CommercePriceListSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
@ Override public Object visit ( Before filter , Object data ) { LOGGER . trace ( "ENTERING: Before filter" ) ; buildTemporalSearch ( filter , data , literal code_block = LoopStatement ; ) ; LOGGER . trace ( "EXITING: Before filter" ) ; return super . visit ( filter , data ) ; }
public void test() { if ( date != null ) { Date start = new Date ( 0L ) ; return new DateRange ( start , date ) ; } else { LOG . debug ( "Unable to determine date of range : " + range ) ; return null ; } }
@ Override public Object visit ( Before filter , Object data ) { LOGGER . trace ( "ENTERING: Before filter" ) ; buildTemporalSearch ( filter , data , literal code_block = LoopStatement ; ) ; LOGGER . trace ( "EXITING: Before filter" ) ; return super . visit ( filter , data ) ; }
public void test() { try { dBuilder = dbFactory . newDocumentBuilder ( ) ; } catch ( ParserConfigurationException ex ) { log . error ( "Error creating document builder: " + ex . getMessage ( ) , ex ) ; } }
public void test() { try { statusFileURL = "http://" + board . getIpAddress ( ) + ":" + Integer . toString ( board . getPort ( ) ) + "/" + GET_SENSORS_URL ; doc = dBuilder . parse ( new URL ( statusFileURL ) . openStream ( ) ) ; doc . getDocumentElement ( ) . normalize ( ) ; } catch ( ConnectException connEx ) { disconnect ( ) ; this . stop ( ) ; this . setDescription ( "Connection timed out, no reply from the board at " + statusFileURL ) ; LOG . debug ( "Stream timed out." ) ; } catch ( SAXException ex ) { disconnect ( ) ; this . stop ( ) ; LOG . error ( Freedomotic . getStackTraceInfo ( ex ) ) ; } catch ( Exception ex ) { disconnect ( ) ; this . stop ( ) ; setDescription ( "Unable to connect to " + statusFileURL ) ; LOG . error ( Freedomotic . getStackTraceInfo ( ex ) ) ; } }
public void flush ( ) { log . debug ( "Flush" ) ; release ( true ) ; resetHitCounter ( ) ; }
public void test() { try { ( ( ApplicationContextImpl ) _remoteParticipant . getApplicationContext ( ) ) . getRemoteCommunication ( ) . joinAnswer ( _remoteParticipant . getId ( ) , _localParticipant . getRemoteAddress ( ) , sdp ) ; } catch ( final Exception e ) { LOG . error ( "" , e ) ; notifyRemote = false ; done ( Cause . ERROR , e ) ; } }
public void test() { try ( final Socket socket = new Socket ( ) ) { logger . debug ( "Sending DECOMMISSION command to NiFi instance" ) ; socket . setSoTimeout ( 10000 ) ; socket . connect ( new InetSocketAddress ( "localhost" , port ) ) ; logger . debug ( "Established connection to NiFi instance." ) ; socket . setSoTimeout ( 0 ) ; logger . debug ( "Sending DECOMMISSION Command to port {}" , port ) ; final OutputStream out = socket . getOutputStream ( ) ; out . write ( ( DECOMMISSION_CMD + " " + secretKey + "\n" ) . getBytes ( StandardCharsets . UTF_8 ) ) ; out . flush ( ) ; socket . shutdownOutput ( ) ; final String response = readResponse ( socket . getInputStream ( ) ) ; code_block = IfStatement ; } finally { code_block = IfStatement ; } }
public void test() { try ( final Socket socket = new Socket ( ) ) { logger . debug ( "Connecting to NiFi instance" ) ; socket . setSoTimeout ( 10000 ) ; socket . connect ( new InetSocketAddress ( "localhost" , port ) ) ; socket . setSoTimeout ( 0 ) ; logger . debug ( "Sending DECOMMISSION Command to port {}" , port ) ; final OutputStream out = socket . getOutputStream ( ) ; out . write ( ( DECOMMISSION_CMD + " " + secretKey + "\n" ) . getBytes ( StandardCharsets . UTF_8 ) ) ; out . flush ( ) ; socket . shutdownOutput ( ) ; final String response = readResponse ( socket . getInputStream ( ) ) ; logger . debug ( "Response from NiFi instance: {}" , response ) ; code_block = IfStatement ; } finally { code_block = IfStatement ; } }
public void test() { if ( DECOMMISSION_CMD . equals ( response ) ) { logger . debug ( "Received response to DECOMMISSION command: {}" , response ) ; code_block = IfStatement ; return null ; } else { logger . debug ( "Received response of unexpected response: {}" , response ) ; return 18 ; } }
public void test() { if ( lockFile . exists ( ) && ! lockFile . delete ( ) ) { logger . error ( "Failed to delete lock file: " + lockFile ) ; } }
@ Override public List < ReceiptDetail > reconstructReceiptDetail ( final String billReferenceNumber , final BigDecimal actualAmountPaid , final List < ReceiptDetail > receiptDetailList ) { final Long billID = Long . valueOf ( billReferenceNumber ) ; final List < EgBillDetails > billDetails = new ArrayList < > ( 0 ) ; final EgBill bill = applicationBpaBillService . updateBillWithLatest ( billID ) ; final CollectionApportioner apportioner = new CollectionApportioner ( ) ; billDetails . addAll ( bill . getEgBillDetails ( ) ) ; LOGGER . debug ( "retrieved bill {}." , billReferenceNumber ) ; return apportioner . reConstruct ( actualAmountPaid , billDetails , functionDAO , chartOfAccountsDAO ) ; }
public void test() { try { code_block = IfStatement ; } catch ( IOException e ) { LOGGER . error ( "Unable to retrieve file" , e ) ; } }
public void test() { if ( status == 1 ) { LOGGER . debug ( "[loadConfig][{}] Loaded config properties" , sourceName ) ; } else { LOGGER . debug ( "[loadConfig][{}] Loaded config properties" , sourceName ) ; } }
public void test() { if ( status == 1 ) { LOGGER . error ( "[loadConfig][{}] Exception occurred, cause: {}" , sourceName , throwable . toString ( ) ) ; } else { LOGGER . warn ( "[loadConfig][{}] Exception occurred" , sourceName , throwable ) ; } }
public void test() { try { dialPlan = new DialPlanExtension ( extension ) ; } catch ( final IllegalArgumentException e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { write ( network , pos ) ; } catch ( Exception t ) { LOG . error ( "Error sending network." , t ) ; } finally { code_block = TryStatement ;  } }
public void test() { try { pos . close ( ) ; } catch ( IOException e ) { log . error ( e . getMessage ( ) , e ) ; } }
public boolean clientExists ( Integer demographicNo ) { boolean exists = getHibernateTemplate ( ) . get ( Demographic . class , demographicNo ) != null ; LOGGER . debug ( "ClientExists {}" , demographicNo ) ; return exists ; }
protected void onPageBiggerThanMaxSize ( String urlStr , long pageSize ) { log . debug ( "Page size [{}] is [{}]." , urlStr , pageSize ) ; }
public void test() { try { code_block = IfStatement ; checkDirectoriesContainSameContent ( getExpectedResourcesTestDir ( ) , targetTestDir ) ; } catch ( DifferentDirectoryContentException e ) { String msg = "Generated resources do not match the expected resource (Use -DoverrideExpectedTestFiles=true " + "to run test and override expected test files instead of doing this check)" ; LOGGER . error ( msg , e ) ; Assert . fail ( msg + "\n" + e . getMessage ( ) ) ; } }
public void test() { try { Iterator < Row > resultIterator = result . iterator ( ) ; code_block = WhileStatement ; logger . info ( context , "ShadowUserProcessor:getSyncCallback:SUCCESS:SYNC CALLBACK SUCCESSFULLY MIGRATED  ALL Shadow user" ) ; } catch ( Exception e ) { logger . error ( context , "ShadowUserProcessor:getSyncCallback:ERROR" , e ) ; } }
@ Override public void beginWindow ( long windowId ) { currentWindowId = windowId ; log . debug ( "Begin window with id {}" , currentWindowId ) ; store . beginTransaction ( ) ; }
public void test() { try { Method m = wrapee . getMethod ( method . getName ( ) , method . getParameterTypes ( ) ) ; this . methods . put ( m , method ) ; } catch ( NoSuchMethodException e ) { LOGGER . warn ( e . getMessage ( ) , e ) ; } }
@ Path ( "/entity/{entityId}/groups" ) @ GET public String getGroups ( @ PathParam ( "entityId" ) String entityId , @ QueryParam ( "identityType" ) String idType ) throws EngineException , JsonProcessingException { log . info ( "Getting groups for {}" , entityId ) ; Map < String , GroupMembership > groups = identitiesMan . getGroups ( getEP ( entityId , idType ) ) ; return mapper . writeValueAsString ( groups . keySet ( ) ) ; }
public void deregisterStreamConsumer ( final String stream ) throws InterruptedException , ExecutionException { int attempt = 1 ; String streamConsumerArn = getStreamConsumerArn ( stream ) ; LOG . debug ( "Deregistering stream consumer - {}" , stream ) ; deregistrationBackoff ( configuration , backoff , attempt ++ ) ; Optional < DescribeStreamConsumerResponse > response = describeStreamConsumer ( streamConsumerArn ) ; code_block = IfStatement ; waitForConsumerToDeregister ( response . orElse ( null ) , streamConsumerArn , attempt ) ; LOG . debug ( "Deregistered stream consumer - {}" , streamConsumerArn ) ; }
public void test() { try { List < TopicPartitionGroup > topicPartitionGroups = partitionGroupServerService . findByTopic ( topic , namespace ) ; code_block = IfStatement ; code_block = IfStatement ; return findPartitionGroupLeaderBroker ( topicPartitionGroups ) ; } catch ( Exception e ) { logger . error ( "" , e ) ; throw new ServiceException ( ServiceException . INTERNAL_SERVER_ERROR , e . getMessage ( ) , e ) ; } }
public void test() { if ( this . task . getStatus ( ) == TaskState . CANCELED ) { LOG . debug ( "Cancelling scheduled job: {}" , this . task . getId ( ) ) ; } else { code_block = IfStatement ; } }
public void test() { if ( this . isCancelling ( ) ) { doOutputTransfers ( job ) ; notifyError ( ) ; } else { int jobId = job . getJobId ( ) ; JOB_LOGGER . error ( "Received a notification for job " + jobId + " with state FAILED" ) ; JOB_LOGGER . error ( "Job " + job . getJobId ( ) + ", running Task " + this . task . getId ( ) + " on worker " + this . getAssignedResource ( ) . getName ( ) + ", has failed." ) ; JOB_LOGGER . error ( "Job " + job . getJobId ( ) + ", running Task " + this . task . getId ( ) + " on worker " + this . getAssignedResource ( ) . getName ( ) + ", has failed." ) ; ErrorManager . warn ( "Job " + job . getJobId ( ) + ", running Task " + this . task . getId ( ) + " on worker " + this . getAssignedResource ( ) . getName ( ) + ", has failed." ) ; ++ this . executionErrors ; code_block = IfStatement ; } }
public ExpirationConfiguration loadExpirationConfiguration ( ) { ExpirationConfiguration result ; final String propertyDir = PropertyAccessor . getInstance ( ) . getPropertyFileLocation ( ) ; LOG . debug ( "Property Directory: " + propertyDir ) ; result = loadExpirationConfiguration ( propertyDir , FTA_CONFIG_FILE ) ; LOG . debug ( "LoadedExpirationConfiguration result: " + result ) ; return result ; }
public ExpirationConfiguration loadExpirationConfiguration ( ) { ExpirationConfiguration result ; LOG . debug ( "begin loadExpirationConfiguration()" ) ; final String propertyDir = PropertyAccessor . getInstance ( ) . getPropertyFileLocation ( ) ; result = loadExpirationConfiguration ( propertyDir , FTA_CONFIG_FILE ) ; LOG . debug ( "end loadExpirationConfiguration()" ) ; return result ; }
public void test() { try { Document document = null ; code_block = IfStatement ; StringWriter stringOut = new StringWriter ( ) ; DOMImplementationLS domImpl = ( DOMImplementationLS ) document . getImplementation ( ) ; LSSerializer lsSerializer = domImpl . createLSSerializer ( ) ; lsSerializer . getDomConfig ( ) . setParameter ( XML_DECLARATION , false ) ; LSOutput lsout = domImpl . createLSOutput ( ) ; lsout . setEncoding ( encoding ) ; lsout . setCharacterStream ( stringOut ) ; code_block = IfStatement ; lsSerializer . write ( n , lsout ) ; return stringOut . toString ( ) ; } catch ( DOMException | LSException e ) { LOG . error ( "DOMException" , e ) ; } }
@ Test public void testCreateGRETunnelTemplate ( ) { template = "/VM_files/createTunnel.vm" ; String message = callGRETunnelVelocity ( template , newParamsGRETunnelService ( ) ) ; Assert . assertNotNull ( message ) ; log . info ( message ) ; }
public void sendFeatureList ( List < Feature > featureList , String action ) throws ArcgisException { LOGGER . debug ( "Sending feature list: " + action ) ; Map < String , String > params = new LinkedHashMap < String , String > ( ) ; params . put ( OUT_SPATIAL_REFERENCE_PARAM , DEFAULT_SPATIAL_REFERENCE ) ; code_block = IfStatement ; params . put ( ROLLBACK_ON_FAILURE_PARAM , "true" ) ; Map < String , String > bodyParams = new LinkedHashMap < String , String > ( ) ; bodyParams . put ( FEATURES_PARAM , featureListToStrArray ( featureList ) ) ; bodyParams . put ( OUTPUT_FORMAT_PARAM , DEFAULT_OUTPUT_FORMAT ) ; String fullUrl = serviceUrl . toString ( ) ; code_block = IfStatement ; HttpResponse response = httpPost ( fullUrl , params , bodyParams ) ; LOGGER . debug ( "Response code: " + response . getResponseCode ( ) + "\n\t" + response . getBody ( ) ) ; checkResponse ( response ) ; }
public void sendFeatureList ( List < Feature > featureList , String action ) throws ArcgisException { LOGGER . debug ( action + " feature list(" + featureList . size ( ) + "), into Feature table: " + serviceUrl ) ; Map < String , String > params = new LinkedHashMap < String , String > ( ) ; params . put ( OUT_SPATIAL_REFERENCE_PARAM , DEFAULT_SPATIAL_REFERENCE ) ; code_block = IfStatement ; params . put ( ROLLBACK_ON_FAILURE_PARAM , "true" ) ; Map < String , String > bodyParams = new LinkedHashMap < String , String > ( ) ; bodyParams . put ( FEATURES_PARAM , featureListToStrArray ( featureList ) ) ; bodyParams . put ( OUTPUT_FORMAT_PARAM , DEFAULT_OUTPUT_FORMAT ) ; String fullUrl = serviceUrl . toString ( ) ; code_block = IfStatement ; HttpResponse response = httpPost ( fullUrl , params , bodyParams ) ; LOGGER . debug ( "Response: " + response ) ; checkResponse ( response ) ; }
public void test() { switch ( columnIndex ) { case 0 : return Formatter . formatString ( sru . getName ( ) ) ; case 1 : return sru . getType ( ) ; case 2 : return calculate . get ( rowIndex ) ; default : LOG . error ( "Unknown column " + columnIndex ) ; return new String ( "" ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
@ Override public void run ( ) { LOG . info ( "start async send..." ) ; code_block = TryStatement ;  sendDoneLatch . countDown ( ) ; LOG . info ( "done async send" ) ; }
public void test() { try { produceMessage ( session , destination ) ; } catch ( JMSException e ) { LOG . info ( "got send exception: " , e ) ; } }
@ Override public void run ( ) { LOG . info ( "doing async send..." ) ; code_block = TryStatement ;  sendDoneLatch . countDown ( ) ; LOG . info ( "done async send" ) ; }
public User login ( String userOrEmail , String userpass ) throws OmException { log . debug ( "login {}" , userOrEmail ) ; List < User > users = em . createNamedQuery ( "getUserByLoginOrEmail" , User . class ) . setParameter ( "userOrEmail" , userOrEmail ) . setParameter ( "type" , Type . USER ) . getResultList ( ) ; code_block = IfStatement ; User u = users . get ( 0 ) ; code_block = IfStatement ; code_block = IfStatement ; log . debug ( "login user groups {}" , u . getGroupUsers ( ) ) ; code_block = IfStatement ; u . setLastlogin ( new Date ( ) ) ; return update ( u , u . getId ( ) ) ; }
public User login ( String userOrEmail , String userpass ) throws OmException { log . debug ( "login::login::{}" , userOrEmail ) ; List < User > users = em . createNamedQuery ( "getUserByLoginOrEmail" , User . class ) . setParameter ( "userOrEmail" , userOrEmail ) . setParameter ( "type" , Type . USER ) . getResultList ( ) ; log . debug ( "login:: {} users were found" , users . size ( ) ) ; code_block = IfStatement ; User u = users . get ( 0 ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; u . setLastlogin ( new Date ( ) ) ; return update ( u , u . getId ( ) ) ; }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { exec ( stmt , "DROP ROLE " + roleName ) ; } catch ( Exception ex ) { LOGGER . warn ( "Failed to drop role {}" , roleName , ex ) ; } finally { exec ( stmt , "CREATE ROLE " + roleName ) ; } }
@ SuppressWarnings ( "ThrowableResultOfMethodCallIgnored" ) @ Override public void exceptionCaught ( ChannelHandlerContext ctx , Throwable cause ) { code_block = IfStatement ; LOG . error ( "exceptionCaught" , cause ) ; ctx . channel ( ) . close ( ) ; }
public void test() { try ( ProcessReader processReader = new ProcessReader ( process ) ) { processReader . readAll ( ) ; final String errOutput = processReader . getError ( ) ; code_block = IfStatement ; final String verboseJson = Arrays . stream ( processReader . getOutput ( ) . split ( "\n" ) ) . filter ( line -> line . contains ( "Audit Request" ) ) . findFirst ( ) . get ( ) ; String auditRequest ; code_block = TryStatement ;  return Json . createReader ( IOUtils . toInputStream ( auditRequest , StandardCharsets . UTF_8 ) ) . readObject ( ) ; } catch ( InterruptedException ex ) { LOG . warn ( "Interrupted while running yarn job" , ex ) ; Thread . currentThread ( ) . interrupt ( ) ; throw new AnalysisException ( "Yarn audit process was interrupted." , ex ) ; } }
public void test() { try { fileSystem . createTags ( createTagsRequest ) ; return Boolean . TRUE ; } catch ( AmazonServiceException ase ) { logger . error ( "error tagging process" , ase ) ; throw ase ; } }
public void test() { try { ProjectFile file = new ProjectFile ( event . getUploadedFile ( ) ) ; uploadedFiles . add ( file ) ; String fileName = file . getName ( ) ; setFileName ( fileName ) ; code_block = IfStatement ; } catch ( IOException e ) { log . error ( "Error occurred during uploading file." , e ) ; WebStudioUtils . addErrorMessage ( "Error occurred during uploading file." , e . getMessage ( ) ) ; } }
public void start ( String ip , int port , String keyspace , int dataTableDaysTimeArea , int slotSecondsTimeArea ) throws Exception { code_block = IfStatement ; this . dataTableDaysTimeArea = dataTableDaysTimeArea ; this . slotSecondsTimeArea = slotSecondsTimeArea ; Builder builder = Cluster . builder ( ) ; builder . withPort ( port ) ; builder . addContactPoint ( ip ) ; this . cluster = builder . build ( ) ; Metadata metadata = cluster . getMetadata ( ) ; code_block = ForStatement ; session = cluster . connect ( ) ; session . execute ( "USE \"" + keyspace + "\"" ) ; log . info ( this , "Cluster is started" ) ; this . started = true ; }
public void test() { for ( Host host : metadata . getAllHosts ( ) ) { LOG . debug ( String . format ( "Datacenter: %s; Host: %s; Rack: %s" , host . getDatacenter ( ) , host . getDatacenter ( ) , host . getAddress ( ) ) ) ; } }
public void test() { if ( ! pd . validate ( ) ) { logger . warn ( "Process {} has invalid process descriptor" , processID ) ; this . registeredProcessDescriptions . remove ( processID ) ; this . registeredAlgorithmParameters . remove ( processID ) ; } }
public void test() { if ( null == network ) { log . warn ( String . format ( "No network interface found for %s" , address ) ) ; return delegate . createSocket ( address , port , localAddr , localPort ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { listener . onRemoval ( identifiable ) ; } catch ( Throwable t ) { logger . warn ( "Exception while invoking listener " + listener , t ) ; } }
public void test() { if ( pool == null || pool . isTerminating ( ) || pool . isShutdown ( ) ) { LOG . debug ( "No default executor for builder: {}" , builder . getName ( ) ) ; pool = getDefaultExecutor ( builder ) ; poolCache . put ( builder . getName ( ) , pool ) ; } }
public void test() { if ( ! zippedFile . delete ( ) ) { logger . warn ( "Could not delete temporary file: " + zippedFile ) ; } }
public static File extractFile ( String fileName , String targetInfix , TokenResolver tokenResolver ) throws IOException { InputStream in = Externalization . class . getClassLoader ( ) . getResourceAsStream ( fileName ) ; if ( in == null ) return null ; File target = getTempFile ( fileName . replace ( "." , "_" + targetInfix + "." ) ) ; Reader reader = new TokenReplacingReader ( new InputStreamReader ( in ) , tokenResolver ) ; FileWriter writer = new FileWriter ( target ) ; copyAndClose ( reader , writer ) ; log . info ( "Extracted file '{}'" , fileName ) ; return target ; }
public void test() { if ( null == configCenterConfigurationSource ) { LOGGER . error ( "Null configuration source at " + url . toString ( ) ) ; return null ; } }
@ Override public void addOrder ( Transaction transaction , ColoredCoinsBidOrderPlacement attachment ) { final BidOrder order = new BidOrder ( transaction , attachment , blockchain . getHeight ( ) ) ; bidOrderTable . insert ( order ) ; LOG . debug ( "addOrder: {}" , order ) ; }
public void test() { if ( StringUtils . hasText ( generatorBeanId ) ) { orderNumberGenerator = Context . getRegisteredComponent ( generatorBeanId , OrderNumberGenerator . class ) ; log . info ( "Setting order number generator {}" , generatorBeanId ) ; } else { orderNumberGenerator = this ; log . info ( "Setting default order number generator" ) ; } }
public void test() { if ( StringUtils . hasText ( generatorBeanId ) ) { orderNumberGenerator = Context . getRegisteredComponent ( generatorBeanId , OrderNumberGenerator . class ) ; log . info ( "Successfully set the configured order number generator" ) ; } else { log . info ( "Order number generator already set." ) ; orderNumberGenerator = this ; } }
public void test() { if ( ! ( t instanceof OperationFailureException ) ) { LOGGER . error ( "Operation failure" , t ) ; } }
public void test() { { int c = count . incrementAndGet ( ) ; LOG . info ( "running {}" , c ) ; command . run ( ) ; } }
public void test() { if ( isValid ( map ) ) { final String name = ( String ) map . get ( StructrLicenseManager . NameKey ) ; final byte [ ] response = name . getBytes ( "utf-8" ) ; socket . getOutputStream ( ) . write ( sign ( response ) ) ; socket . getOutputStream ( ) . flush ( ) ; } else { logger . info ( "Unable to sign request: " + map ) ; } }
public void test() { if ( isDebug ) { logger . debug ( "Expected arguments, but found none." ) ; } }
public void test() { if ( log . isTraceEnable ( ) ) { log . info ( this , "ThreadAnalysisQueryWorker stopped" ) ; } }
public void test() { try { return new Pair < > ( ttk . getServer ( ) . toString ( ) , createNewTransport ( ttk ) ) ; } catch ( TTransportException tte ) { log . warn ( "Could not connect to server: " + tte . getMessage ( ) ) ; servers . remove ( index ) ; retryCount ++ ; } }
private void flushBuffer ( final boolean scheduled ) { code_block = IfStatement ; code_block = IfStatement ; logger . info ( "Batch {} - Starting" , batchNumber ) ; final List < BatchEntry < ? , O > > entries = new ArrayList < > ( _maxBatchSize ) ; final int batchSize = _queue . drainTo ( entries ) ; code_block = IfStatement ; final int batchNumber = _batchNo . incrementAndGet ( ) ; final Object [ ] input = new Object [ batchSize ] ; code_block = ForStatement ; final BatchSource < I > source = new ArrayBatchSource < > ( input ) ; final BatchEntryBatchSink < O > sink = new BatchEntryBatchSink < > ( entries ) ; _transformation . map ( source , sink ) ; logger . info ( "Batch {} - Finished" , batchNumber , batchSize ) ; }
public void test() { for ( CamelEndpointDetails detail : endpoints ) { LOG . info ( detail . getRoute ( ) ) ; } }
public void test() { try { XcesBodyBasic parasBasic = ( XcesBodyBasic ) unmarshallerBasic . unmarshal ( xmlEventReaderBasic , XcesBodyBasic . class ) . getValue ( ) ; readPara ( jb , parasBasic ) ; } catch ( RuntimeException ex ) { logger . error ( ex . getMessage ( ) , ex ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( LayoutPageTemplateEntryServiceUtil . class , "getLayoutPageTemplateEntries" , _getLayoutPageTemplateEntriesParameterTypes12 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , type , status , start , end , orderByComparator ) ; Object returnObj = null ; code_block = TryStatement ;  return ( java . util . List < com . liferay . layout . page . template . model . LayoutPageTemplateEntry > ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { SessionObject sesObj = ( SessionObject ) request . getSession ( ) . getAttribute ( FdahpStudyDesignerConstants . SESSION_OBJECT ) ; code_block = IfStatement ; jsonobject . put ( FdahpStudyDesignerConstants . MESSAGE , message ) ; response . setContentType ( FdahpStudyDesignerConstants . APPLICATION_JSON ) ; out = response . getWriter ( ) ; out . print ( jsonobject ) ; } catch ( Exception e ) { logger . error ( "StudyController - savePage - ERROR" , e ) ; } }
public void test() { try { bufferedReader = new BufferedReader ( new FileReader ( notebookJson ) ) ; String line ; code_block = WhileStatement ; } catch ( Exception e ) { logger . error ( "Exception in reading notebook {}: " , notebookJson , e ) ; return new JsonResponse < > ( Response . Status . INTERNAL_SERVER_ERROR , e . getMessage ( ) , ExceptionUtils . getStackTrace ( e ) ) . build ( ) ; } }
public void test() { for ( PluginWrapper plugin : getStartedPlugins ( ) ) { Class pluginClass = plugin . getPlugin ( ) . getClass ( ) ; GenericApplicationContext pluginContext = ( GenericApplicationContext ) ( ( Plugin ) plugin . getPlugin ( ) ) . getApplicationContext ( ) ; LOGGER . log ( Level . FINE , "Plugin {0}" , pluginClass . getName ( ) ) ; pluginContext . setParent ( applicationContext ) ; } }
public void test() { if ( ( ( EmptyDeviceResponse ) deviceResponse ) . getStatus ( ) . equals ( DeviceMessageStatus . OK ) ) { PublicLightingSetLightRequestMessageProcessor . this . handleEmptyDeviceResponse ( deviceResponse , PublicLightingSetLightRequestMessageProcessor . this . responseMessageSender , domain , domainVersion , messageType , retryCount ) ; } else { PublicLightingSetLightRequestMessageProcessor . this . handleEmptyDeviceResponse ( deviceResponse , PublicLightingSetLightRequestMessageProcessor . this . responseMessageSender , domain , domainVersion , messageType , retryCount ) ; } }
public void test() { if ( header != null && header . getIndex ( ) < 0 ) { LOG . error ( "Missing header at header " + header ) ; } else-if ( header != null && allowedWorkers != null ) { code_block = IfStatement ; } }
public void test() { if ( ! allowedWorkers . contains ( header . getIndex ( ) ) ) { LOG . warn ( "Rejected worker at " + header . getIndex ( ) + ": " + header . getIndex ( ) ) ; return null ; } }
public void test() { if ( pluginInfo == null ) { logger . warn ( "Plug-in service (" + pluginInfo . getPluginName ( ) + ")" ) ; } else { registeredPlugins . put ( pluginInfo . getHashcode ( ) , pluginInfo ) ; logger . info ( "Plug-in service (" + pluginInfo . getPluginName ( ) + ")" + " was registered." ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { return nw . getMatching ( pattern ) ; } catch ( final IOException ex ) { log . error ( ex . getMessage ( ) , ex ) ; } }
public void test() { try { mailer . sendTaskCompletedMail ( taskHistory . get ( ) , requestWithState . get ( ) . getRequest ( ) ) ; } catch ( Throwable t ) { LOG . warn ( "Failed to send task completed message" , t ) ; } finally { SingularityDeleteResult result = taskManager . deleteFinishedTaskMailQueue ( taskId ) ; LOG . debug ( "Task {} mail sent with status {} (delete result {})" , taskId , shouldSendState , result ) ; } }
public void test() { try { mailer . sendTaskCompletedMail ( taskHistory . get ( ) , requestWithState . get ( ) . getRequest ( ) ) ; } catch ( Throwable t ) { LOG . error ( "While trying to send task completed mail for {}" , taskId , t ) ; } finally { SingularityDeleteResult result = taskManager . deleteFinishedTaskMailQueue ( taskId ) ; LOG . debug ( "Removed task completed mail for {}" , taskId ) ; } }
public void test() { try { logger . debug ( "New deployment {} has been discovered and will be deployed" , name ) ; DeploymentConfig deployment = deploymentFactory . newDeployment ( deploymentConfig ) ; addedDeploymentEvent . fire ( new DeploymentConfigChangedEvent ( deployment . getDeploymentUnit ( ) ) ) ; registeredDeployments . put ( deployment . getIdentifier ( ) , deployment ) ; logger . debug ( "Deployment {} deployed successfully" , name ) ; } catch ( RuntimeException e ) { logger . error ( "Deployment {} failed" , name , e ) ; } }
public void test() { try { logger . debug ( "New deployment {} has been discovered and will be deployed" , identifier ) ; DeploymentConfig deployment = registeredDeployments . remove ( identifier ) ; removedDeploymentEvent . fire ( new DeploymentConfigChangedEvent ( deployment . getDeploymentUnit ( ) ) ) ; logger . debug ( "Deployment {} undeployed successfully" , identifier ) ; } catch ( RuntimeException e ) { logger . error ( "Error while removing deployment config" , e ) ; } }
public void stopCounting ( ) { runCounting = false ; stopLogReport ( ) ; logger . info ( "done" ) ; }
public void addHandler ( JettyHandler handler ) { log . info ( "Adding ServletContextHandler" ) ; ServletHolder servletHolder = new ServletHolder ( ) ; servletHolder . setServlet ( handler ) ; servletContextHandler . addServlet ( servletHolder , handler . pathSpec ( ) ) ; }
@ Test public void testManyFiles ( ) throws Exception { Session s = conn . createSession ( true , Session . SESSION_TRANSACTED ) ; Queue jmsQueue = s . createQueue ( address . toString ( ) ) ; MessageProducer p = s . createProducer ( jmsQueue ) ; p . setDeliveryMode ( DeliveryMode . PERSISTENT ) ; conn . start ( ) ; code_block = ForStatement ; s . commit ( ) ; Assert . assertTrue ( server . getStorageManager ( ) . getJournalSequentialFileFactory ( ) . getCriticalAnalyzer ( ) . getNumberOfComponents ( ) < 10 ) ; log . info ( "\n" ) ; }
public void test() { if ( numRows != m . numRows || numCols != m . numCols ) { logger . info ( "dimensions error" ) ; return 0.0 ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Refreshing." ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { if ( ttl > 1 ) { long delay = TimeUnit . SECONDS . toMillis ( suggestedRefreshInterval ( ttl ) ) ; timer . schedule ( new RenewTokenTask ( ) , delay ) ; logger . info ( "Token will be refreshed for {} seconds" , suggestedRefreshInterval ( ttl ) ) ; } else { vault = recreateVault ( vault ) ; } }
public void test() { if ( response . isAuthRenewable ( ) ) { code_block = IfStatement ; } else { LOG . debug ( "Renewable request has been renewed" ) ; } }
public void test() { if ( e . getHttpStatusCode ( ) == STATUS_CODE_FORBIDDEN ) { LOGGER . warn ( "Authentication details are incorrect, occurred during invoking Vault" , e ) ; vault = recreateVault ( vault ) ; } }
public void test() { try { lookup . close ( ) ; } catch ( Exception e ) { LOG . warn ( "Unable to close " + lookup , e ) ; } }
public void test() { if ( actualInput != null ) { final InputStream entityStream = new ByteArrayInputStream ( actualInput . getBytes ( StandardCharsets . UTF_8 ) ) ; final NormalizedNodeContext inputContext = JsonNormalizedNodeBodyReader . readFrom ( uriPath , entityStream , true , controllerContext ) ; LOG . debug ( "Parsed YangInstance: {}" , inputContext . getData ( ) ) ; outputContext = restconfService . invokeRpc ( uriPath , inputContext , null ) ; } else { outputContext = restconfService . invokeRpc ( uriPath , null , null ) ; } }
public void test() { if ( actualInput != null ) { final InputStream entityStream = new ByteArrayInputStream ( actualInput . getBytes ( StandardCharsets . UTF_8 ) ) ; final NormalizedNodeContext inputContext = JsonNormalizedNodeBodyReader . readFrom ( uriPath , entityStream , true , controllerContext ) ; LOG . debug ( "Parsed YangInstanceIdentifier: {}" , inputContext . getInstanceIdentifierContext ( ) . getInstanceIdentifier ( ) ) ; outputContext = restconfService . invokeRpc ( uriPath , inputContext , null ) ; } else { LOG . debug ( "No output found" ) ; outputContext = restconfService . invokeRpc ( uriPath , null , null ) ; } }
public void test() { try { threadPool . shutdown ( ) ; code_block = WhileStatement ; } catch ( InterruptedException e ) { logger . error ( "Thread interrupted!" , e ) ; } }
public void test() { try { code_block = IfStatement ; result = git . clean ( ) . setCleanDirectories ( true ) . call ( ) ; } catch ( Exception e ) { LOG . error ( "There was an error in Git {} operation" , operation ) ; throw e ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { try { tm . checkServerTrusted ( chain , authType ) ; logger . debug ( "checkServerTrusted for {} succeeded" , tm ) ; code_block = IfStatement ; } catch ( CertificateException e ) { logger . debug ( "checkServerTrusted for {} failed" , tm ) ; code_block = IfStatement ; code_block = IfStatement ; } }
public void test() { if ( ! Objects . equals ( e , DEFAULT_PUBLIC_EXPONENT ) ) { log . warn ( "Could not determine public key to " + DEFAULT_PUBLIC_EXPONENT + ": " + DEFAULT_PUBLIC_EXPONENT ) ; } }
public void test() { if ( ! Objects . equals ( n , modulus ) ) { log . info ( "Modulus: " + modulus + " modulus: " + modulus ) ; } }
public void test() { try { Tasks . setBlockingDetails ( "Installing " + urlToInstall + " at " + machine ) ; return machine . installTo ( resolver , props , urlToInstall , target ) ; } catch ( Exception e ) { Exceptions . propagateIfFatal ( e ) ; log . debug ( "Failed to install " + urlToInstall + " at " + machine + ": " + stack ) ; lastError = e ; String stack = StackTraceSimplifier . toString ( e ) ; code_block = IfStatement ; throw Exceptions . propagate ( e ) ; } finally { Tasks . resetBlockingDetails ( ) ; } }
public void test() { try { DecodedJwt decodedJwt = response . getDecodedAccessToken ( ) ; LOGGER . debug ( "Decoded JWT: {}" , decodedJwt ) ; } catch ( IllegalArgumentException e ) { LOGGER . debug ( "Access token can not be logged. {}" , e . getMessage ( ) ) ; } }
public void test() { try { DecodedJwt decodedJwt = response . getDecodedAccessToken ( ) ; LOGGER . debug ( "Access token: {}" , decodedJwt ) ; } catch ( IllegalArgumentException e ) { LOGGER . warn ( "Could not decode access token" , e ) ; } }
public void test() { try { byte [ ] p = queue . poll ( 1 , TimeUnit . SECONDS ) ; code_block = IfStatement ; } catch ( IOException e1 ) { log . warn ( "IOException" , e1 ) ; connect ( ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( IOException e1 ) { log . info ( "Can not connect to host " + host ) ; connect ( ) ; } }
public void test() { try { owner . connection . setAutoCommit ( true ) ; } catch ( SQLException e ) { log . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { XStringSubstitution xSS = UNO . XStringSubstitution ( UnoComponent . createComponentWithContext ( UnoComponent . CSS_UTIL_PATH_SUBSTITUTION ) ) ; String myPath = xSS . substituteVariables ( "$(user)/uno_packages/cache/uno_packages/" , true ) ; String oooPath = xSS . substituteVariables ( "$(inst)/share/uno_packages/cache/uno_packages/" , true ) ; String oooPathNew = xSS . substituteVariables ( "$(brandbaseurl)/share/uno_packages/cache/uno_packages/" , true ) ; code_block = IfStatement ; findWollMuxInstallations ( wmInstallations , myPath , false ) ; findWollMuxInstallations ( wmInstallations , oooPath , true ) ; code_block = IfStatement ; } catch ( NoSuchElementException e ) { LOGGER . error ( e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "to be updated object" ) ; logger . debug ( objectAsXmlString ( intake , IntakesCommon . class ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "to be created, intake common" ) ; logger . debug ( objectAsXmlString ( "to be created, intake common" ) ) ; } }
public MbZielobjTypTxt findById ( sernet . gs . reveng . MbZielobjTypTxtId id ) { log . debug ( "getting MbZielobjTypTxt instance with id: " + id ) ; code_block = TryStatement ;  }
public void test() { if ( instance == null ) { log . debug ( "get successful, no instance found" ) ; } else { log . debug ( "get successful, instance found" ) ; } }
public void test() { if ( instance == null ) { log . debug ( "get successful, no instance found" ) ; } else { log . debug ( "get successful, instance found" ) ; } }
public void test() { try { MbZielobjTypTxt instance = ( MbZielobjTypTxt ) sessionFactory . getCurrentSession ( ) . get ( "sernet.gs.reveng.MbZielobjTypTxt" , id ) ; code_block = IfStatement ; return instance ; } catch ( RuntimeException re ) { log . error ( "get failed" , re ) ; throw re ; } }
public void test() { try { var reader = new StringReader ( publicPart ) ; var readerPem = new PemReader ( reader ) ; var obj = readerPem . readPemObject ( ) ; readerPem . close ( ) ; return KeyFactory . getInstance ( algorithm ) . generatePublic ( new X509EncodedKeySpec ( obj . getContent ( ) ) ) ; } catch ( InvalidKeySpecException | NoSuchAlgorithmException | IOException e ) { logger . warn ( "Exception loading public key from PEM " , e ) ; return null ; } }
@ Path ( "/entity/{entityId}/credential/{credential}/status/{status}" ) @ PUT public void setCredentialStatus ( @ PathParam ( "entityId" ) String entityId , @ PathParam ( "credential" ) String credential , @ QueryParam ( "identityType" ) String idType , @ PathParam ( "status" ) String status ) throws EngineException , JsonProcessingException { LocalCredentialState desiredCredentialState = LocalCredentialState . valueOf ( status ) ; log . info ( "Setting credential {}/{}" , status , desiredCredentialState ) ; entityCredMan . setEntityCredentialStatus ( getEP ( entityId , idType ) , credential , desiredCredentialState ) ; }
@ Test public void testBbox1 ( ) { Document doc = getAsDOM ( "wfs?request=GetFeature&version=1.1.0&typename=gsml:MappedFeature&srsName=EPSG:4979&bbox=-200,-200,0,200,200,50" ) ; LOGGER . info ( "WFS GetFeature&typename=gsml:MappedFeature&srsName=EPSG:4979&bbox=-200,-200,0,200,200,200,50" ) ; assertXpathCount ( 0 , "//gsml:MappedFeature[@gml:id='gsml.mappedfeature.mf1']" , doc ) ; assertXpathCount ( 1 , "//gsml:MappedFeature[@gml:id='gsml.mappedfeature.mf2']" , doc ) ; assertXpathCount ( 1 , "//gsml:MappedFeature[@gml:id='gsml.mappedfeature.mf4']" , doc ) ; assertXpathEvaluatesTo ( "167.9388 -29.0434 7" , "//gsml:MappedFeature[@gml:id='gsml.mappedfeature.mf2']/gsml:shape/gml:Point/gml:pos" , doc ) ; assertXpathEvaluatesTo ( "3" , "//gsml:MappedFeature[@gml:id='gsml.mappedfeature.mf2']/gsml:shape/gml:Point/@srsDimension" , doc ) ; assertXpathEvaluatesTo ( "http://www.opengis.net/gml/srs/epsg.xml4979" , "//gsml:MappedFeature[@gml:id='gsml.mappedfeature.mf2']/gsml:shape/gml:Point/@srsName" , doc ) ; }
private ServerConnector createHttpConnector ( Config config ) { final ServerConnector httpConnector = new ServerConnector ( jettyServer , new HttpConnectionFactory ( baseHttpConfig ( ) ) ) ; httpConnector . setPort ( config . getInt ( DrillOnYarnConfig . HTTP_PORT ) ) ; LOG . info ( "Initializing HTTP connector: {}" , config . getInt ( DrillOnYarnConfig . HTTP_PORT ) ) ; return httpConnector ; }
@ Override public FileStatus [ ] listStatus ( Path f ) throws FileNotFoundException , IOException { log . info ( "listStatus()" ) ; return listStatus ( f , null ) ; }
public void test() { if ( e instanceof org . apache . airavata . model . error . InvalidRequestException ) { result . ire = ( org . apache . airavata . model . error . InvalidRequestException ) e ; result . setIreIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AiravataClientException ) { result . ace = ( org . apache . airavata . model . error . AiravataClientException ) e ; result . setAceIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AiravataSystemException ) { result . ase = ( org . apache . airavata . model . error . AiravataSystemException ) e ; result . setAseIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AuthorizationException ) { result . ae = ( org . apache . airavata . model . error . AuthorizationException ) e ; result . setAeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof org . apache . airavata . model . error . InvalidRequestException ) { result . ire = ( org . apache . airavata . model . error . InvalidRequestException ) e ; result . setIreIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AiravataClientException ) { result . ace = ( org . apache . airavata . model . error . AiravataClientException ) e ; result . setAceIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AiravataSystemException ) { result . ase = ( org . apache . airavata . model . error . AiravataSystemException ) e ; result . setAseIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AuthorizationException ) { result . ae = ( org . apache . airavata . model . error . AuthorizationException ) e ; result . setAeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof org . apache . airavata . model . error . InvalidRequestException ) { result . ire = ( org . apache . airavata . model . error . InvalidRequestException ) e ; result . setIreIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AiravataClientException ) { result . ace = ( org . apache . airavata . model . error . AiravataClientException ) e ; result . setAceIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AiravataSystemException ) { result . ase = ( org . apache . airavata . model . error . AiravataSystemException ) e ; result . setAseIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AuthorizationException ) { result . ae = ( org . apache . airavata . model . error . AuthorizationException ) e ; result . setAeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { try { fcall . sendResponse ( fb , msg , msgType , seqid ) ; } catch ( java . lang . Exception ex ) { _LOGGER . error ( "Exception writing to internal frame buffer" , ex ) ; fb . close ( ) ; } }
public void test() { try { JsonNode location = jsonResults . get ( j ) . get ( "locations" ) . get ( 0 ) ; geocodedAddresses . add ( getGeocodedAddressFromLocationNode ( location ) ) ; } catch ( Exception ex ) { LOGGER . log ( Level . WARNING , ex . getMessage ( ) , ex ) ; geocodedAddresses . add ( new GeocodedAddress ( ) ) ; } }
public void test() { try { json = UrlRequest . getResponseFromUrl ( url ) ; code_block = IfStatement ; return geocodedAddresses ; } catch ( MalformedURLException ex ) { logger . error ( "URL error!" , ex ) ; } catch ( UnsupportedEncodingException ex ) { logger . error ( "UTF-8 Unsupported?!" , ex ) ; } catch ( IOException ex ) { logger . error ( "Error opening API resource! " + ex . toString ( ) + " Response: " + json ) ; } catch ( NullPointerException ex ) { logger . error ( "MapQuest response was not formatted correctly. Response: " + json , ex ) ; } catch ( Exception ex ) { logger . error ( "" + ex ) ; } }
public void test() { try { json = UrlRequest . getResponseFromUrl ( url ) ; code_block = IfStatement ; return geocodedAddresses ; } catch ( MalformedURLException ex ) { logger . error ( "Malformed MapQuest url!" , ex ) ; } catch ( UnsupportedEncodingException ex ) { logger . error ( "UnsupportedEncodingException occurred!" , ex ) ; } catch ( IOException ex ) { logger . error ( "Error opening API resource! " + ex . toString ( ) + " Response: " + json ) ; } catch ( NullPointerException ex ) { logger . error ( "MapQuest response was not formatted correctly. Response: " + json , ex ) ; } catch ( Exception ex ) { logger . error ( "" + ex ) ; } }
public void test() { try { json = UrlRequest . getResponseFromUrl ( url ) ; code_block = IfStatement ; return geocodedAddresses ; } catch ( MalformedURLException ex ) { logger . error ( "Malformed MapQuest url!" , ex ) ; } catch ( UnsupportedEncodingException ex ) { logger . error ( "UTF-8 Unsupported?!" , ex ) ; } catch ( IOException ex ) { logger . error ( "I/O error!" , ex ) ; } catch ( NullPointerException ex ) { logger . error ( "MapQuest response was not formatted correctly. Response: " + json , ex ) ; } catch ( Exception ex ) { logger . error ( "" + ex ) ; } }
public void test() { try { json = UrlRequest . getResponseFromUrl ( url ) ; code_block = IfStatement ; return geocodedAddresses ; } catch ( MalformedURLException ex ) { logger . error ( "Malformed MapQuest url!" , ex ) ; } catch ( UnsupportedEncodingException ex ) { logger . error ( "UTF-8 Unsupported?!" , ex ) ; } catch ( IOException ex ) { logger . error ( "Error opening API resource! " + ex . toString ( ) + " Response: " + json ) ; } catch ( NullPointerException ex ) { logger . error ( "Null pointer exception!" , ex ) ; } catch ( Exception ex ) { logger . error ( "" + ex ) ; } }
public void test() { try { json = UrlRequest . getResponseFromUrl ( url ) ; code_block = IfStatement ; return geocodedAddresses ; } catch ( MalformedURLException ex ) { logger . error ( "Malformed MapQuest url!" , ex ) ; } catch ( UnsupportedEncodingException ex ) { logger . error ( "UTF-8 Unsupported?!" , ex ) ; } catch ( IOException ex ) { logger . error ( "Error opening API resource! " + ex . toString ( ) + " Response: " + json ) ; } catch ( NullPointerException ex ) { logger . error ( "MapQuest response was not formatted correctly. Response: " + json , ex ) ; } catch ( Exception ex ) { logger . error ( "Problem getting response!" , ex ) ; } }
public void test() { if ( conflictExists ) { log . warn ( String . format ( "Duplicate file %s already exists" , file ) ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { try { BufferedImage i = ImageIO . read ( new ByteArrayInputStream ( image . getBytes ( ) ) ) ; images . add ( i ) ; } catch ( IOException e ) { LOGGER . error ( "Unable to load image chip" , e ) ; } }
public void test() { if ( images . size ( ) < numberOfImages ) { logger . warn ( "more than image " + numberOfImages ) ; } }
@ Override public boolean isEmpty ( ) { logger . debug ( "isEmpty" ) ; return atomContainerCount == 0 ; }
public void test() { if ( sessionThread != null ) { sessionThread . interrupt ( ) ; } else { log . warn ( "Session thread is not null" ) ; } }
@ Override protected void onStop ( ) { setPollingWait ( - 1 ) ; LOG . info ( "JMS broker stopped" ) ; }
public void test() { try { } catch ( MatchManagerException e ) { logger . error ( "Error while processing scheduled MatchManager scheduled task.." ) ; logger . error ( "error" , e ) ; } }
public void test() { try { RemediationRestClient . delete ( model . getObject ( ) . getKey ( ) ) ; SyncopeConsoleSession . get ( ) . success ( getString ( Constants . OPERATION_SUCCEEDED ) ) ; target . add ( container ) ; } catch ( SyncopeClientException e ) { LOG . error ( "While deleting {}" , model . getObject ( ) . getKey ( ) , e ) ; SyncopeConsoleSession . get ( ) . onException ( e ) ; } }
public void runIteration ( Configuration conf , Path corpusInput , Path modelInput , Path modelOutput , int iterationNumber , int maxIterations , int numReduceTasks ) throws IOException , ClassNotFoundException , InterruptedException { String jobName = String . format ( "Iteration %d of %d, input path: %s" , iterationNumber , maxIterations , modelInput ) ; LOG . info ( "Starting {}" , jobName ) ; Job job = prepareJob ( corpusInput , modelOutput , CachingCVB0Mapper . class , IntWritable . class , VectorWritable . class , VectorSumReducer . class , IntWritable . class , VectorWritable . class ) ; job . setCombinerClass ( VectorSumReducer . class ) ; job . setNumReduceTasks ( numReduceTasks ) ; job . setJobName ( jobName ) ; setModelPaths ( job , modelInput ) ; HadoopUtil . delete ( conf , modelOutput ) ; code_block = IfStatement ; }
@ Before public void setUp ( ) throws Exception { log . debug ( "Starting wiser on port " + smtpPort ) ; wiser = startWiser ( smtpPort ) ; setDriver ( new ChromeDriver ( ) ) ; getDriver ( ) . manage ( ) . timeouts ( ) . implicitlyWait ( 30 , TimeUnit . SECONDS ) ; getDriver ( ) . manage ( ) . window ( ) . setSize ( new Dimension ( 1024 , 900 ) ) ; log . debug ( "Finished wiser on port " + smtpPort ) ; }
@ Before public void setUp ( ) throws Exception { log . debug ( "" ) ; log . debug ( "Setting up" ) ; wiser = startWiser ( smtpPort ) ; setDriver ( new ChromeDriver ( ) ) ; getDriver ( ) . manage ( ) . timeouts ( ) . implicitlyWait ( 30 , TimeUnit . SECONDS ) ; getDriver ( ) . manage ( ) . window ( ) . setSize ( new Dimension ( 1024 , 900 ) ) ; }
public void test() { try { return qoSInterDirectMeasurementPingRepository . findByMeasurement ( measurement ) ; } catch ( final Exception ex ) { logger . debug ( ex . getMessage ( ) , ex ) ; throw new ArrowheadException ( CoreCommonConstants . DATABASE_OPERATION_EXCEPTION_MSG ) ; } }
public void test() { try { LOG . debug ( "release of transaction member '{}'. " , mem ) ; mem . release ( ) ; } catch ( Throwable t ) { LOG . warn ( "release of transaction member '{}'. " , mem , t ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { retVal = session . createSQLQuery ( "select dbid, dbms_lob.getlength(obj) from " + ORACLE_TEMP_TABLE_NAME ) . list ( ) ; } catch ( Exception e ) { log . error ( "" , e ) ; } }
public void test() { switch ( fieldType ) { case STRING : sql += fieldName + " = " + "'" + fieldValue + "'" ; break ; case DATETIME : sql += fieldName + " = " + "'" + new DateTimeColumnParser ( ) . getValue ( fieldValue ) + "'" ; break ; case INT32 : case INT64 : case FLOAT32 : case FLOAT64 : case BIG_INTEGER : sql += fieldName + " = " + fieldValue ; break ; default : LOG . warn ( "Unsupported data type: " + fieldType ) ; } }
protected void writeCommand ( byte [ ] message ) throws SonyProjectorException { code_block = IfStatement ; logger . debug ( "Write Command: {}" , message ) ; OutputStream dataOut = this . dataOut ; code_block = IfStatement ; code_block = TryStatement ;  }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Refreshing." ) ; } }
public void test() { try { send ( request , response , config . getNetbiosRetryTimeout ( ) ) ; } catch ( InterruptedIOException ioe ) { code_block = IfStatement ; log . error ( "NetbiosRequest interrupted" , ioe ) ; throw new UnknownHostException ( name . name ) ; } catch ( IOException ioe ) { throw new UnknownHostException ( name . name ) ; } }
public void test() { if ( Log . isTraceEnabled ( ) ) { Log . trace ( "User '" + user + "' not found." ) ; } }
@ Override public void process ( ClusterEvent event ) { LOG . info ( "START TaskPersistDataStage.process()" ) ; long startTime = System . currentTimeMillis ( ) ; WorkflowControllerDataProvider cache = event . getAttribute ( AttributeName . ControllerDataProvider . name ( ) ) ; HelixManager manager = event . getAttribute ( AttributeName . helixmanager . name ( ) ) ; cache . getTaskDataCache ( ) . persistDataChanges ( manager . getHelixDataAccessor ( ) ) ; long endTime = System . currentTimeMillis ( ) ; LOG . info ( "END TaskPersistDataStage.process() for cluster {} took {} ms" , cache . getClusterName ( ) , ( endTime - startTime ) ) ; }
@ Override public void process ( ClusterEvent event ) { LOG . info ( "START TaskPersistDataStage.process()" ) ; long startTime = System . currentTimeMillis ( ) ; WorkflowControllerDataProvider cache = event . getAttribute ( AttributeName . ControllerDataProvider . name ( ) ) ; HelixManager manager = event . getAttribute ( AttributeName . helixmanager . name ( ) ) ; cache . getTaskDataCache ( ) . persistDataChanges ( manager . getHelixDataAccessor ( ) ) ; long endTime = System . currentTimeMillis ( ) ; LOG . info ( "END TaskPersistDataStage.process()" + ( endTime - startTime ) / 1000.0 ) ; }
public void test() { if ( ProfileManager . getDefault ( ) . migrateToProfiles ( getConfigFileName ( ) ) ) { logger . info ( "Config successfully changed to {}" , getConfigFileName ( ) ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( IOException | IllegalArgumentException ex ) { log . error ( ex . getMessage ( ) , ex ) ; } }
public void test() { if ( profile != null ) { log . info ( "Starting with a profile {}" , profile ) ; } else { log . info ( "Starting without a profile" ) ; } }
public void test() { if ( profile != null ) { log . info ( "Starting with profile {}" , profile . getId ( ) ) ; } else { log . info ( "Profile already started" ) ; } }
public void test() { if ( ProfileManager . getStartingProfile ( ) != null ) { log . error ( "No previous profile configured" ) ; System . setProperty ( "org.jmri.Apps.configFilename" , Profile . CONFIG_FILENAME ) ; Profile profile = ProfileManager . getDefault ( ) . getActiveProfile ( ) ; code_block = IfStatement ; } else { log . error ( "Specify profile to use as command line argument." ) ; log . error ( "If starting with saved profile configuration, ensure the autoStart property is set to \"true\"" ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( IOException ex ) { LOG . error ( ex . getMessage ( ) , ex ) ; } }
private void log ( String string ) { System . out . println ( string ) ; logger . info ( string ) ; }
public void attachClean ( StgMBstnStatus instance ) { log . debug ( "attaching clean StgMBstnStatus instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . lock ( instance , LockMode . NONE ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { SerialPort oldSerialPort = serialPortReference . get ( ) ; logger . debug ( "Opening serial port: {}" , serialPortName ) ; SerialPort serialPort = portIdentifier . open ( DSMRBindingConstants . DSMR_PORT_NAME , SERIAL_PORT_READ_TIMEOUT_MILLISECONDS ) ; logger . trace ( "Configure serial port parameters: {}" , portSettings ) ; serialPort . setSerialPortParams ( portSettings . getBaudrate ( ) , portSettings . getDataBits ( ) , portSettings . getStopbits ( ) , portSettings . getParity ( ) ) ; logger . trace ( "SerialPort opened successful on {}" , serialPortName ) ; open ( serialPort . getInputStream ( ) ) ; serialPort . addEventListener ( this ) ; serialPort . notifyOnDataAvailable ( true ) ; serialPort . notifyOnBreakInterrupt ( true ) ; serialPort . notifyOnFramingError ( true ) ; serialPort . notifyOnOverrunError ( true ) ; serialPort . notifyOnParityError ( true ) ; code_block = TryStatement ;  code_block = TryStatement ;  serialPort . setRTS ( true ) ; code_block = IfStatement ; } catch ( IOException ioe ) { logger . debug ( "Failed to get inputstream for serialPort" , ioe ) ; errorEvent = DSMRConnectorErrorEvent . READ_ERROR ; } catch ( TooManyListenersException tmle ) { logger . warn ( "Possible bug because a listener was added while one already set." , tmle ) ; errorEvent = DSMRConnectorErrorEvent . INTERNAL_ERROR ; } catch ( PortInUseException piue ) { logger . debug ( "Port already in use: {}" , serialPortName , piue ) ; errorEvent = DSMRConnectorErrorEvent . IN_USE ; } catch ( UnsupportedCommOperationException ucoe ) { logger . debug ( "Port does not support requested port settings (invalid dsmr:portsettings parameter?): {}" , serialPortName , ucoe ) ; errorEvent = DSMRConnectorErrorEvent . NOT_COMPATIBLE ; } }
public void test() { try { logger . trace ( "Opening port {}" , serialPortName ) ; SerialPort oldSerialPort = serialPortReference . get ( ) ; logger . trace ( "Opening port {}" , serialPortName ) ; SerialPort serialPort = portIdentifier . open ( DSMRBindingConstants . DSMR_PORT_NAME , SERIAL_PORT_READ_TIMEOUT_MILLISECONDS ) ; logger . trace ( "Configure serial port parameters: {}" , portSettings ) ; serialPort . setSerialPortParams ( portSettings . getBaudrate ( ) , portSettings . getDataBits ( ) , portSettings . getStopbits ( ) , portSettings . getParity ( ) ) ; open ( serialPort . getInputStream ( ) ) ; serialPort . addEventListener ( this ) ; serialPort . notifyOnDataAvailable ( true ) ; serialPort . notifyOnBreakInterrupt ( true ) ; serialPort . notifyOnFramingError ( true ) ; serialPort . notifyOnOverrunError ( true ) ; serialPort . notifyOnParityError ( true ) ; code_block = TryStatement ;  code_block = TryStatement ;  serialPort . setRTS ( true ) ; code_block = IfStatement ; } catch ( IOException ioe ) { logger . debug ( "Failed to get inputstream for serialPort" , ioe ) ; errorEvent = DSMRConnectorErrorEvent . READ_ERROR ; } catch ( TooManyListenersException tmle ) { logger . warn ( "Possible bug because a listener was added while one already set." , tmle ) ; errorEvent = DSMRConnectorErrorEvent . INTERNAL_ERROR ; } catch ( PortInUseException piue ) { logger . debug ( "Port already in use: {}" , serialPortName , piue ) ; errorEvent = DSMRConnectorErrorEvent . IN_USE ; } catch ( UnsupportedCommOperationException ucoe ) { logger . debug ( "Port does not support requested port settings (invalid dsmr:portsettings parameter?): {}" , serialPortName , ucoe ) ; errorEvent = DSMRConnectorErrorEvent . NOT_COMPATIBLE ; } }
public void test() { try { serialPort . enableReceiveThreshold ( SERIAL_TIMEOUT_MILLISECONDS ) ; } catch ( UnsupportedCommOperationException e ) { logger . debug ( "Receive threshold is unsupported for serial port {}" , SERIAL_TIMEOUT_MILLISECONDS ) ; } }
public void test() { try { serialPort . enableReceiveTimeout ( SERIAL_TIMEOUT_MILLISECONDS ) ; } catch ( UnsupportedCommOperationException e ) { logger . debug ( e . getMessage ( ) , e ) ; } }
public void test() { if ( ! serialPortReference . compareAndSet ( oldSerialPort , serialPort ) ) { logger . error ( "Serial port not set in serial port {}" , oldSerialPort ) ; errorEvent = DSMRConnectorErrorEvent . INTERNAL_ERROR ; } }
public void test() { try { logger . trace ( "Opening port {}" , serialPortName ) ; SerialPort oldSerialPort = serialPortReference . get ( ) ; SerialPort serialPort = portIdentifier . open ( DSMRBindingConstants . DSMR_PORT_NAME , SERIAL_PORT_READ_TIMEOUT_MILLISECONDS ) ; logger . trace ( "Configure serial port parameters: {}" , portSettings ) ; serialPort . setSerialPortParams ( portSettings . getBaudrate ( ) , portSettings . getDataBits ( ) , portSettings . getStopbits ( ) , portSettings . getParity ( ) ) ; logger . trace ( "SerialPort opened successful on {}" , serialPortName ) ; open ( serialPort . getInputStream ( ) ) ; serialPort . addEventListener ( this ) ; serialPort . notifyOnDataAvailable ( true ) ; serialPort . notifyOnBreakInterrupt ( true ) ; serialPort . notifyOnFramingError ( true ) ; serialPort . notifyOnOverrunError ( true ) ; serialPort . notifyOnParityError ( true ) ; code_block = TryStatement ;  code_block = TryStatement ;  serialPort . setRTS ( true ) ; code_block = IfStatement ; } catch ( IOException ioe ) { logger . warn ( "Serial port not available: {}" , serialPortName , ioe ) ; errorEvent = DSMRConnectorErrorEvent . READ_ERROR ; } catch ( TooManyListenersException tmle ) { logger . warn ( "Possible bug because a listener was added while one already set." , tmle ) ; errorEvent = DSMRConnectorErrorEvent . INTERNAL_ERROR ; } catch ( PortInUseException piue ) { logger . debug ( "Port already in use: {}" , serialPortName , piue ) ; errorEvent = DSMRConnectorErrorEvent . IN_USE ; } catch ( UnsupportedCommOperationException ucoe ) { logger . debug ( "Port does not support requested port settings (invalid dsmr:portsettings parameter?): {}" , serialPortName , ucoe ) ; errorEvent = DSMRConnectorErrorEvent . NOT_COMPATIBLE ; } }
public void test() { try { logger . trace ( "Opening port {}" , serialPortName ) ; SerialPort oldSerialPort = serialPortReference . get ( ) ; SerialPort serialPort = portIdentifier . open ( DSMRBindingConstants . DSMR_PORT_NAME , SERIAL_PORT_READ_TIMEOUT_MILLISECONDS ) ; logger . trace ( "Configure serial port parameters: {}" , portSettings ) ; serialPort . setSerialPortParams ( portSettings . getBaudrate ( ) , portSettings . getDataBits ( ) , portSettings . getStopbits ( ) , portSettings . getParity ( ) ) ; logger . trace ( "SerialPort opened successful on {}" , serialPortName ) ; open ( serialPort . getInputStream ( ) ) ; serialPort . addEventListener ( this ) ; serialPort . notifyOnDataAvailable ( true ) ; serialPort . notifyOnBreakInterrupt ( true ) ; serialPort . notifyOnFramingError ( true ) ; serialPort . notifyOnOverrunError ( true ) ; serialPort . notifyOnParityError ( true ) ; code_block = TryStatement ;  code_block = TryStatement ;  serialPort . setRTS ( true ) ; code_block = IfStatement ; } catch ( IOException ioe ) { logger . debug ( "Failed to get inputstream for serialPort" , ioe ) ; errorEvent = DSMRConnectorErrorEvent . READ_ERROR ; } catch ( TooManyListenersException tmle ) { logger . debug ( "Too many listeners. ({})" , tmle . getMessage ( ) ) ; errorEvent = DSMRConnectorErrorEvent . INTERNAL_ERROR ; } catch ( PortInUseException piue ) { logger . debug ( "Port already in use: {}" , serialPortName , piue ) ; errorEvent = DSMRConnectorErrorEvent . IN_USE ; } catch ( UnsupportedCommOperationException ucoe ) { logger . debug ( "Port does not support requested port settings (invalid dsmr:portsettings parameter?): {}" , serialPortName , ucoe ) ; errorEvent = DSMRConnectorErrorEvent . NOT_COMPATIBLE ; } }
public void test() { try { logger . trace ( "Opening port {}" , serialPortName ) ; SerialPort oldSerialPort = serialPortReference . get ( ) ; SerialPort serialPort = portIdentifier . open ( DSMRBindingConstants . DSMR_PORT_NAME , SERIAL_PORT_READ_TIMEOUT_MILLISECONDS ) ; logger . trace ( "Configure serial port parameters: {}" , portSettings ) ; serialPort . setSerialPortParams ( portSettings . getBaudrate ( ) , portSettings . getDataBits ( ) , portSettings . getStopbits ( ) , portSettings . getParity ( ) ) ; logger . trace ( "SerialPort opened successful on {}" , serialPortName ) ; open ( serialPort . getInputStream ( ) ) ; serialPort . addEventListener ( this ) ; serialPort . notifyOnDataAvailable ( true ) ; serialPort . notifyOnBreakInterrupt ( true ) ; serialPort . notifyOnFramingError ( true ) ; serialPort . notifyOnOverrunError ( true ) ; serialPort . notifyOnParityError ( true ) ; code_block = TryStatement ;  code_block = TryStatement ;  serialPort . setRTS ( true ) ; code_block = IfStatement ; } catch ( IOException ioe ) { logger . debug ( "Failed to get inputstream for serialPort" , ioe ) ; errorEvent = DSMRConnectorErrorEvent . READ_ERROR ; } catch ( TooManyListenersException tmle ) { logger . warn ( "Possible bug because a listener was added while one already set." , tmle ) ; errorEvent = DSMRConnectorErrorEvent . INTERNAL_ERROR ; } catch ( PortInUseException piue ) { logger . debug ( "Port in use" , piue ) ; errorEvent = DSMRConnectorErrorEvent . IN_USE ; } catch ( UnsupportedCommOperationException ucoe ) { logger . debug ( "Port does not support requested port settings (invalid dsmr:portsettings parameter?): {}" , serialPortName , ucoe ) ; errorEvent = DSMRConnectorErrorEvent . NOT_COMPATIBLE ; } }
public void test() { try { logger . trace ( "Opening port {}" , serialPortName ) ; SerialPort oldSerialPort = serialPortReference . get ( ) ; SerialPort serialPort = portIdentifier . open ( DSMRBindingConstants . DSMR_PORT_NAME , SERIAL_PORT_READ_TIMEOUT_MILLISECONDS ) ; logger . trace ( "Configure serial port parameters: {}" , portSettings ) ; serialPort . setSerialPortParams ( portSettings . getBaudrate ( ) , portSettings . getDataBits ( ) , portSettings . getStopbits ( ) , portSettings . getParity ( ) ) ; logger . trace ( "SerialPort opened successful on {}" , serialPortName ) ; open ( serialPort . getInputStream ( ) ) ; serialPort . addEventListener ( this ) ; serialPort . notifyOnDataAvailable ( true ) ; serialPort . notifyOnBreakInterrupt ( true ) ; serialPort . notifyOnFramingError ( true ) ; serialPort . notifyOnOverrunError ( true ) ; serialPort . notifyOnParityError ( true ) ; code_block = TryStatement ;  code_block = TryStatement ;  serialPort . setRTS ( true ) ; code_block = IfStatement ; } catch ( IOException ioe ) { logger . debug ( "Failed to get inputstream for serialPort" , ioe ) ; errorEvent = DSMRConnectorErrorEvent . READ_ERROR ; } catch ( TooManyListenersException tmle ) { logger . warn ( "Possible bug because a listener was added while one already set." , tmle ) ; errorEvent = DSMRConnectorErrorEvent . INTERNAL_ERROR ; } catch ( PortInUseException piue ) { logger . debug ( "Port already in use: {}" , serialPortName , piue ) ; errorEvent = DSMRConnectorErrorEvent . IN_USE ; } catch ( UnsupportedCommOperationException ucoe ) { logger . warn ( "The port could not be read: {}" , serialPortName , ucoe ) ; errorEvent = DSMRConnectorErrorEvent . NOT_COMPATIBLE ; } }
public void test() { if ( elapsedNS >= threshold ) { logger . warn ( "Removed {}ns from {} to {}" , elapsedNS , threshold . get ( ) , elapsedNS ) ; } }
public void test() { try { logger . info ( "Will parse {}" , serializedData ) ; template = mapper . readValue ( serializedData , Template . class ) ; logger . info ( "Will parse {}" , serializedData ) ; } catch ( JsonProcessingException e ) { throw new ParserException ( e ) ; } catch ( Throwable e ) { throw new ParserException ( e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( SourceServiceUtil . class , "addSource" , _addSourceParameterTypes0 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , nameMap , driverClassName , driverUrl , driverUserName , driverPassword , serviceContext ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . portal . reports . engine . console . model . Source ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( replyTo != null ) { LOG . trace ( "Using JMSReplyTo: {}" , replyTo ) ; JmsMessageHelper . setJMSReplyTo ( answer , replyTo ) ; } else { LOG . trace ( "Not using JMSReplyTo" ) ; JmsMessageHelper . setJMSReplyTo ( answer , null ) ; } }
public void test() { if ( replyTo != null ) { LOG . debug ( "Using JMSReplyTo destination: {}" , replyTo ) ; JmsMessageHelper . setJMSReplyTo ( answer , replyTo ) ; } else { LOG . warn ( "Using JMSReplyTo destination: null" ) ; JmsMessageHelper . setJMSReplyTo ( answer , null ) ; } }
public void test() { try { code_block = IfStatement ; SpatialFilter spatialFilter = new SpatialFilter ( geometryWkt ) ; code_block = IfStatement ; } catch ( IllegalArgumentException e ) { LOGGER . error ( e . getMessage ( ) ) ; return ; } }
public void test() { try { return _assetEntryService . getEntries ( assetEntryQuery ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; } }
public void test() { try { return MCRConfiguration2 . < MCRDefaultLogicalStructMapTypeProvider > getClass ( "MCR.Component.MetsMods.LogicalStructMapTypeProvider" ) . orElse ( MCRDefaultLogicalStructMapTypeProvider . class ) . getDeclaredConstructor ( ) . newInstance ( ) ; } catch ( Exception e ) { LOGGER . error ( "Unable to load default LogicalStructMapTypeProvider" , e ) ; return new MCRDefaultLogicalStructMapTypeProvider ( ) ; } }
@ Override public OutputStream append ( String path ) throws IOException { log . info ( "append({})" , path ) ; return updateFile ( path , true ) ; }
@ Override public void sendMessage ( Message message , Role role ) throws MessageException { log . debug ( "User Service : " + Context . getUserService ( ) ) ; log . debug ( "Role Name : " + role . getName ( ) ) ; List < Role > roles = new ArrayList < > ( ) ; roles . add ( role ) ; Collection < User > users = Context . getUserService ( ) . getUsers ( null , roles , false ) ; log . debug ( "Sending message " + message + " to " + users ) ; Context . getMessageService ( ) . sendMessage ( message , users ) ; }
@ Override public void sendMessage ( Message message , Role role ) throws MessageException { log . debug ( "Sending message to role " + role ) ; List < Role > roles = new ArrayList < > ( ) ; roles . add ( role ) ; log . debug ( " roles: " + roles ) ; Collection < User > users = Context . getUserService ( ) . getUsers ( null , roles , false ) ; log . debug ( "Sending message " + message + " to " + users ) ; Context . getMessageService ( ) . sendMessage ( message , users ) ; }
@ Override public void sendMessage ( Message message , Role role ) throws MessageException { log . debug ( "Sending message to role " + role ) ; log . debug ( "User Service : " + Context . getUserService ( ) ) ; List < Role > roles = new ArrayList < > ( ) ; roles . add ( role ) ; Collection < User > users = Context . getUserService ( ) . getUsers ( null , roles , false ) ; log . debug ( "Users : " + users ) ; Context . getMessageService ( ) . sendMessage ( message , users ) ; }
public void test() { { log . info ( "=======================Adding a stream definition====================" ) ; eventStreamCount = eventStreamManagerAdminServiceClient . getEventStreamCount ( ) ; eventReceiverCount = eventReceiverAdminServiceClient . getActiveEventReceiverCount ( ) ; eventPublisherCount = eventPublisherAdminServiceClient . getActiveEventPublisherCount ( ) ; executionPlanCount = eventProcessorAdminServiceClient . getActiveExecutionPlanConfigurationCount ( ) ; log . info ( "=======================Adding an execution plan ======================= " ) ; String executionPlan = getExecutionPlanFromFile ( "DeployArtifactsTestCase" , "testPlan.siddhiql" ) ; eventProcessorAdminServiceClient . addExecutionPlan ( executionPlan ) ; Assert . assertEquals ( eventProcessorAdminServiceClient . getActiveExecutionPlanConfigurationCount ( ) , executionPlanCount ) ; log . info ( "=======================Adding an event receiver ======================= " ) ; String eventReceiverConfig = getXMLArtifactConfiguration ( "DeployArtifactsTestCase" , "PizzaOrder.xml" ) ; eventReceiverAdminServiceClient . addEventReceiverConfiguration ( eventReceiverConfig ) ; Assert . assertEquals ( eventReceiverAdminServiceClient . getActiveEventReceiverCount ( ) , eventReceiverCount ) ; log . info ( "=======================Adding an event publisher ======================= " ) ; String eventPublisherConfig = getXMLArtifactConfiguration ( "DeployArtifactsTestCase" , "PizzaDeliveryNotification.xml" ) ; eventPublisherAdminServiceClient . addEventPublisherConfiguration ( eventPublisherConfig ) ; Assert . assertEquals ( eventPublisherAdminServiceClient . getActiveEventPublisherCount ( ) , eventPublisherCount ) ; log . info ( "=======================Adding a stream definition====================" ) ; String pizzaStreamDefinition = getJSONArtifactConfiguration ( "DeployArtifactsTestCase" , "org.wso2.sample.pizza.order_1.0.0.json" ) ; eventStreamManagerAdminServiceClient . addEventStreamAsString ( pizzaStreamDefinition ) ; Assert . assertEquals ( eventStreamManagerAdminServiceClient . getEventStreamCount ( ) , ++ eventStreamCount ) ; log . info ( "=======================Adding another stream definition====================" ) ; String outStreamDefinition = getJSONArtifactConfiguration
public void test() { { log . info ( "=======================Testing the order: ExP, EP, ER, ES-1, ES-2======================= " ) ; eventStreamCount = eventStreamManagerAdminServiceClient . getEventStreamCount ( ) ; eventReceiverCount = eventReceiverAdminServiceClient . getActiveEventReceiverCount ( ) ; eventPublisherCount = eventPublisherAdminServiceClient . getActiveEventPublisherCount ( ) ; executionPlanCount = eventProcessorAdminServiceClient . getActiveExecutionPlanConfigurationCount ( ) ; log . info ( "=======================Testing the order: ExP, EP, ER, ES-1, ES-2======================= " ) ; String executionPlan = getExecutionPlanFromFile ( "DeployArtifactsTestCase" , "testPlan.siddhiql" ) ; eventProcessorAdminServiceClient . addExecutionPlan ( executionPlan ) ; Assert . assertEquals ( eventProcessorAdminServiceClient . getActiveExecutionPlanConfigurationCount ( ) , executionPlanCount ) ; log . info ( "=======================Adding an event receiver ======================= " ) ; String eventReceiverConfig = getXMLArtifactConfiguration ( "DeployArtifactsTestCase" , "PizzaOrder.xml" ) ; eventReceiverAdminServiceClient . addEventReceiverConfiguration ( eventReceiverConfig ) ; Assert . assertEquals ( eventReceiverAdminServiceClient . getActiveEventReceiverCount ( ) , eventReceiverCount ) ; log . info ( "=======================Adding an event publisher ======================= " ) ; String eventPublisherConfig = getXMLArtifactConfiguration ( "DeployArtifactsTestCase" , "PizzaDeliveryNotification.xml" ) ; eventPublisherAdminServiceClient . addEventPublisherConfiguration ( eventPublisherConfig ) ; Assert . assertEquals ( eventPublisherAdminServiceClient . getActiveEventPublisherCount ( ) , eventPublisherCount ) ; log . info ( "=======================Adding a stream definition====================" ) ; String pizzaStreamDefinition = getJSONArtifactConfiguration ( "DeployArtifactsTestCase" , "org.wso2.sample.pizza.order_1.0.0.json" ) ; eventStreamManagerAdminServiceClient . addEventStreamAsString ( pizzaStreamDefinition ) ; Assert . assertEquals ( eventStreamManagerAdminServiceClient . getEventStreamCount ( ) , ++ eventStream
public void test() { { log . info ( "=======================Testing the order: ExP, EP, ER, ES-1, ES-2======================= " ) ; eventStreamCount = eventStreamManagerAdminServiceClient . getEventStreamCount ( ) ; eventReceiverCount = eventReceiverAdminServiceClient . getActiveEventReceiverCount ( ) ; eventPublisherCount = eventPublisherAdminServiceClient . getActiveEventPublisherCount ( ) ; executionPlanCount = eventProcessorAdminServiceClient . getActiveExecutionPlanConfigurationCount ( ) ; log . info ( "=======================Adding an execution plan ======================= " ) ; String executionPlan = getExecutionPlanFromFile ( "DeployArtifactsTestCase" , "testPlan.siddhiql" ) ; eventProcessorAdminServiceClient . addExecutionPlan ( executionPlan ) ; Assert . assertEquals ( eventProcessorAdminServiceClient . getActiveExecutionPlanConfigurationCount ( ) , executionPlanCount ) ; log . info ( "=======================Adding an event receiver ======================= " ) ; String eventReceiverConfig = getXMLArtifactConfiguration ( "DeployArtifactsTestCase" , "PizzaOrder.xml" ) ; eventReceiverAdminServiceClient . addEventReceiverConfiguration ( eventReceiverConfig ) ; Assert . assertEquals ( eventReceiverAdminServiceClient . getActiveEventReceiverCount ( ) , eventReceiverCount ) ; log . info ( "=======================Adding an event publisher ======================= " ) ; String eventPublisherConfig = getXMLArtifactConfiguration ( "DeployArtifactsTestCase" , "PizzaDeliveryNotification.xml" ) ; eventPublisherAdminServiceClient . addEventPublisherConfiguration ( eventPublisherConfig ) ; Assert . assertEquals ( eventPublisherAdminServiceClient . getActiveEventPublisherCount ( ) , eventPublisherCount ) ; log . info ( "=======================Adding a stream definition====================" ) ; String pizzaStreamDefinition = getJSONArtifactConfiguration ( "DeployArtifactsTestCase" , "org.wso2.sample.pizza.order_1.0.0.json" ) ; eventStreamManagerAdminServiceClient . addEventStreamAsString ( pizzaStreamDefinition ) ; Assert . assertEquals ( eventStreamManagerAdminServiceClient . getEventStreamCount ( ) , ++ eventStreamCount ) ; log . info ( "=======================Adding another stream definition
public void test() { { log . info ( "=======================Testing the order: ExP, EP, ER, ES-1, ES-2======================= " ) ; eventStreamCount = eventStreamManagerAdminServiceClient . getEventStreamCount ( ) ; eventReceiverCount = eventReceiverAdminServiceClient . getActiveEventReceiverCount ( ) ; eventPublisherCount = eventPublisherAdminServiceClient . getActiveEventPublisherCount ( ) ; executionPlanCount = eventProcessorAdminServiceClient . getActiveExecutionPlanConfigurationCount ( ) ; log . info ( "=======================Adding an execution plan ======================= " ) ; String executionPlan = getExecutionPlanFromFile ( "DeployArtifactsTestCase" , "testPlan.siddhiql" ) ; eventProcessorAdminServiceClient . addExecutionPlan ( executionPlan ) ; Assert . assertEquals ( eventProcessorAdminServiceClient . getActiveExecutionPlanConfigurationCount ( ) , executionPlanCount ) ; log . info ( "=======================Adding an event receiver ======================= " ) ; String eventReceiverConfig = getXMLArtifactConfiguration ( "DeployArtifactsTestCase" , "PizzaOrder.xml" ) ; eventReceiverAdminServiceClient . addEventReceiverConfiguration ( eventReceiverConfig ) ; Assert . assertEquals ( eventReceiverAdminServiceClient . getActiveEventReceiverCount ( ) , eventReceiverCount ) ; log . info ( "=======================Adding an event publisher ======================= " ) ; String eventPublisherConfig = getXMLArtifactConfiguration ( "DeployArtifactsTestCase" , "PizzaDeliveryNotification.xml" ) ; eventPublisherAdminServiceClient . addEventPublisherConfiguration ( eventPublisherConfig ) ; Assert . assertEquals ( eventPublisherAdminServiceClient . getActiveEventPublisherCount ( ) , eventPublisherCount ) ; log . info ( "=======================Adding a stream definition====================" ) ; String pizzaStreamDefinition = getJSONArtifactConfiguration ( "DeployArtifactsTestCase" , "org.wso2.sample.pizza.order_1.0.0.json" ) ; eventStreamManagerAdminServiceClient . addEventStreamAsString ( pizzaStreamDefinition ) ; Assert . assertEquals ( eventStreamManagerAdminServiceClient . getEventStreamCount ( ) , ++ eventStreamCount ) ; log . info ( "=======================Adding another stream definition
public void test() { { log . info ( "=======================Testing the order: ExP, EP, ER, ES-1, ES-2======================= " ) ; eventStreamCount = eventStreamManagerAdminServiceClient . getEventStreamCount ( ) ; eventReceiverCount = eventReceiverAdminServiceClient . getActiveEventReceiverCount ( ) ; eventPublisherCount = eventPublisherAdminServiceClient . getActiveEventPublisherCount ( ) ; executionPlanCount = eventProcessorAdminServiceClient . getActiveExecutionPlanConfigurationCount ( ) ; log . info ( "=======================Adding an execution plan ======================= " ) ; String executionPlan = getExecutionPlanFromFile ( "DeployArtifactsTestCase" , "testPlan.siddhiql" ) ; eventProcessorAdminServiceClient . addExecutionPlan ( executionPlan ) ; Assert . assertEquals ( eventProcessorAdminServiceClient . getActiveExecutionPlanConfigurationCount ( ) , executionPlanCount ) ; log . info ( "=======================Adding an event receiver ======================= " ) ; String eventReceiverConfig = getXMLArtifactConfiguration ( "DeployArtifactsTestCase" , "PizzaOrder.xml" ) ; eventReceiverAdminServiceClient . addEventReceiverConfiguration ( eventReceiverConfig ) ; Assert . assertEquals ( eventReceiverAdminServiceClient . getActiveEventReceiverCount ( ) , eventReceiverCount ) ; log . info ( "=======================Adding an event publisher ======================= " ) ; String eventPublisherConfig = getXMLArtifactConfiguration ( "DeployArtifactsTestCase" , "PizzaDeliveryNotification.xml" ) ; eventPublisherAdminServiceClient . addEventPublisherConfiguration ( eventPublisherConfig ) ; Assert . assertEquals ( eventPublisherAdminServiceClient . getActiveEventPublisherCount ( ) , eventPublisherCount ) ; log . info ( "=======================Adding another stream definition====================" ) ; String pizzaStreamDefinition = getJSONArtifactConfiguration ( "DeployArtifactsTestCase" , "org.wso2.sample.pizza.order_1.0.0.json" ) ; eventStreamManagerAdminServiceClient . addEventStreamAsString ( pizzaStreamDefinition ) ; Assert . assertEquals ( eventStreamManagerAdminServiceClient . getEventStreamCount ( ) , ++ eventStreamCount ) ; log . info ( "=======================Adding another stream definition
public void test() { { log . info ( "=======================Testing the order: ExP, EP, ER, ES-1, ES-2======================= " ) ; eventStreamCount = eventStreamManagerAdminServiceClient . getEventStreamCount ( ) ; eventReceiverCount = eventReceiverAdminServiceClient . getActiveEventReceiverCount ( ) ; eventPublisherCount = eventPublisherAdminServiceClient . getActiveEventPublisherCount ( ) ; executionPlanCount = eventProcessorAdminServiceClient . getActiveExecutionPlanConfigurationCount ( ) ; log . info ( "=======================Adding an execution plan ======================= " ) ; String executionPlan = getExecutionPlanFromFile ( "DeployArtifactsTestCase" , "testPlan.siddhiql" ) ; eventProcessorAdminServiceClient . addExecutionPlan ( executionPlan ) ; Assert . assertEquals ( eventProcessorAdminServiceClient . getActiveExecutionPlanConfigurationCount ( ) , executionPlanCount ) ; log . info ( "=======================Adding an event receiver ======================= " ) ; String eventReceiverConfig = getXMLArtifactConfiguration ( "DeployArtifactsTestCase" , "PizzaOrder.xml" ) ; eventReceiverAdminServiceClient . addEventReceiverConfiguration ( eventReceiverConfig ) ; Assert . assertEquals ( eventReceiverAdminServiceClient . getActiveEventReceiverCount ( ) , eventReceiverCount ) ; log . info ( "=======================Adding an event publisher ======================= " ) ; String eventPublisherConfig = getXMLArtifactConfiguration ( "DeployArtifactsTestCase" , "PizzaDeliveryNotification.xml" ) ; eventPublisherAdminServiceClient . addEventPublisherConfiguration ( eventPublisherConfig ) ; Assert . assertEquals ( eventPublisherAdminServiceClient . getActiveEventPublisherCount ( ) , eventPublisherCount ) ; log . info ( "=======================Adding a stream definition====================" ) ; String pizzaStreamDefinition = getJSONArtifactConfiguration ( "DeployArtifactsTestCase" , "org.wso2.sample.pizza.order_1.0.0.json" ) ; eventStreamManagerAdminServiceClient . addEventStreamAsString ( pizzaStreamDefinition ) ; Assert . assertEquals ( eventStreamManagerAdminServiceClient . getEventStreamCount ( ) , ++ eventStreamCount ) ; log . info ( "=======================Adding an event out
public void test() { try { code_block = IfStatement ; } catch ( IOException e ) { LOGGER . error ( "Unable to retrieve file" , e ) ; } }
public void test() { try { future . get ( ) ; break ; } catch ( final ExecutionException e ) { final Throwable cause = e . getCause ( ) ; LOG . warn ( cause . getMessage ( ) , cause ) ; } }
public void test() { if ( handler != null ) { return handler . actionGetRingTimeLimit ( ) ; } else { logger . info ( " Action service ThingHandler is null!" ) ; return "" ; } }
public void test() { if ( size >= 10 ) { log . info ( "Cache size: {}" , size ) ; } }
public void test() { try { printAST ( getHiveTokenMapping ( ) , node , 0 , 0 ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; } }
@ BeforeMethod ( alwaysRun = true ) public void setUp ( ) throws Exception { LOGGER . info ( "Setting up Test" ) ; startTime = TimeUtil . getTimeWrtSystemTime ( 0 ) ; endTime = TimeUtil . addMinsToTime ( startTime , 20 ) ; bundles [ 0 ] = BundleUtil . readELBundle ( ) ; bundles [ 0 ] = new Bundle ( bundles [ 0 ] , cluster ) ; bundles [ 0 ] . generateUniqueBundle ( this ) ; bundles [ 0 ] . setProcessWorkflow ( aggregateWorkflowDir ) ; bundles [ 0 ] . setProcessValidity ( startTime , endTime ) ; bundles [ 0 ] . setProcessPeriodicity ( 5 , Frequency . TimeUnit . minutes ) ; bundles [ 0 ] . setOutputFeedPeriodicity ( 5 , Frequency . TimeUnit . minutes ) ; clusterName = Util . readEntityName ( bundles [ 0 ] . getDataSets ( ) . get ( 0 ) ) ; }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "JDBC::{}" , sql ) ; } }
public void test() { if ( LOGGER . isErrorEnabled ( ) ) { LOGGER . log ( Level . ERROR , e . getMessage ( ) , e ) ; } }
public void test() { try { long lastActivityTime = getEntryValue ( data , LAST_ACTIVITY_TIME , 0L ) ; long inactivityAlarmTime = getEntryValue ( data , INACTIVITY_ALARM_TIME , 0L ) ; long inactivityTimeout = getEntryValue ( data , INACTIVITY_TIMEOUT , TimeUnit . SECONDS . toMillis ( defaultInactivityTimeoutInSec ) ) ; boolean active = System . currentTimeMillis ( ) < lastActivityTime + inactivityTimeout ; DeviceState deviceState = DeviceState . builder ( ) . active ( active ) . lastConnectTime ( getEntryValue ( data , LAST_CONNECT_TIME , 0L ) ) . lastDisconnectTime ( getEntryValue ( data , LAST_DISCONNECT_TIME , 0L ) ) . lastActivityTime ( lastActivityTime ) . lastInactivityAlarmTime ( inactivityAlarmTime ) . inactivityTimeout ( inactivityTimeout ) . build ( ) ; TbMsgMetaData md = new TbMsgMetaData ( ) ; md . putValue ( "deviceName" , device . getName ( ) ) ; md . putValue ( "deviceType" , device . getType ( ) ) ; return DeviceStateData . builder ( ) . customerId ( device . getCustomerId ( ) ) . tenantId ( device . getTenantId ( ) ) . deviceId ( device . getId ( ) ) . deviceCreationTime ( device . getCreatedTime ( ) ) . metaData ( md ) . state ( deviceState ) . build ( ) ; } catch ( Exception e ) { log . error ( "Device creation failed" , e ) ; throw new RuntimeException ( e ) ; } }
public void test() { if ( logDetails ) { LOG . info ( String . format ( "%s:%s" , LogUtils . getStackTrace ( e ) ) ) ; } }
public void test() { if ( LOGGER . isInfoEnabled ( ) ) { LOGGER . info ( "Success!" ) ; } }
public void test() { try { zooKeeper . close ( ) ; } catch ( InterruptedException e ) { LOG . error ( "Interrupted while closing ZooKeeper client" , e ) ; Thread . currentThread ( ) . interrupt ( ) ; } }
public void test() { if ( ! pluginDir . exists ( ) && ! pluginDir . isDirectory ( ) ) { logger . error ( "Plugin directory does not exist: {}" , pluginDir . getAbsolutePath ( ) ) ; return ; } }
public void test() { try { code_block = IfStatement ; Files . find ( pluginDir . toPath ( ) , 1 , ( path , attributes ) -> path . toString ( ) . endsWith ( ".jar" ) ) . map ( path -> UdfClassLoader . newClassLoader ( path , parentClassLoader , blacklist ) ) . forEach ( classLoader -> loadFunctions ( classLoader , Optional . of ( classLoader . getJarPath ( ) ) ) ) ; } catch ( final IOException e ) { LOGGER . debug ( "Failed to load functions due to: {}" , e . getMessage ( ) , e ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { try { messageProcessor . writeHeader ( transactionContext ) ; messageProcessor . writeMessageEnd ( ) ; transport . flush ( ) ; MdcCtxInfoUtil . putMdcToAppClassLoader ( application . getAppClasssLoader ( ) , SoaSystemEnvProperties . KEY_LOGGER_SESSION_TID , transactionContext . sessionTid ( ) . map ( DapengUtil :: longToHexStr ) . orElse ( "0" ) ) ; String infoLog = "response[seqId:" + transactionContext . seqId ( ) + ", respCode:" + soaHeader . getRespCode ( ) . get ( ) + "]:" + "service[" + soaHeader . getServiceName ( ) + "]:version[" + soaHeader . getVersionName ( ) + "]:method[" + soaHeader . getMethodName ( ) + "]" + ( soaHeader . getOperatorId ( ) . isPresent ( ) ? " operatorId:" + soaHeader . getOperatorId ( ) . get ( ) : "" ) + ( soaHeader . getUserId ( ) . isPresent ( ) ? " userId:" + soaHeader . getUserId ( ) . get ( ) : "" ) ; code_block = IfStatement ; code_block = IfStatement ; } catch ( Throwable e ) { log . error ( "error writing transaction log" , e ) ; } finally { container . requestCounter ( ) . decrementAndGet ( ) ; MdcCtxInfoUtil . removeMdcToAppClassLoader ( application . getAppClasssLoader ( ) , SoaSystemEnvProperties . KEY_LOGGER_SESSION_TID ) ; MDC . remove ( SoaSystemEnvProperties . KEY_LOGGER_SESSION_TID ) ; } }
private synchronized CswSubscription deleteCswSubscription ( String subscriptionId ) throws CswException { String methodName = "deleteCswSubscription" ; LogSanitizer logSanitizedId = LogSanitizer . sanitize ( subscriptionId ) ; LOGGER . trace ( "subscriptionId = {}" , logSanitizedId ) ; LOGGER . trace ( "subscriptionId = {}" , logSanitizedId ) ; code_block = IfStatement ; CswSubscription subscription = getSubscription ( subscriptionId ) ; code_block = TryStatement ;  LOGGER . trace ( "EXITING: {}    (status = {})" , methodName , false ) ; return subscription ; }
public void test() { try { LOGGER . info ( "Removing subscription for {}" , logSanitizedId ) ; ServiceRegistration sr = registeredSubscriptions . remove ( subscriptionId ) ; code_block = IfStatement ; Configuration subscriptionConfig = getSubscriptionConfiguration ( subscriptionId ) ; code_block = TryStatement ;  LOGGER . debug ( "Subscription removal complete" ) ; } catch ( Exception e ) { LOGGER . debug ( "Could not delete subscription for {}" , logSanitizedId , e ) ; } }
public void test() { if ( sr != null ) { sr . unregister ( ) ; } else { logger . info ( "Cannot unregister SMTP service." ) ; } }
public void test() { if ( sr != null ) { sr . unregister ( ) ; LOGGER . info ( "Unregistered ServiceRegistration: {}" , logSanitizedId ) ; } else { LOGGER . debug ( "No ServiceRegistration found for subscription: {}" , logSanitizedId ) ; } }
public void test() { if ( subscriptionConfig != null ) { subscriptionConfig . delete ( ) ; LOGGER . debug ( "subscriptionConfig is NULL for ID = {}" , logSanitizedId ) ; } else { LOGGER . debug ( "subscriptionConfig is NULL for ID = {}" , logSanitizedId ) ; } }
public void test() { if ( sr != null ) { sr . unregister ( ) ; LOGGER . info ( "Unregistered ServiceRegistration: {}" , logSanitizedId ) ; } else { LOGGER . debug ( "No ServiceRegistration found for subscription: {}" , logSanitizedId ) ; } }
public void test() { if ( subscriptionConfig != null ) { LOGGER . debug ( "Deleting subscription for subscriptionId = {}" , logSanitizedId ) ; subscriptionConfig . delete ( ) ; } else { LOGGER . warn ( "No subscription config found for subscriptionId = {}" , logSanitizedId ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( IOException e ) { LOGGER . error ( "Unable to retrieve file" , e ) ; } }
public void test() { try { LOGGER . debug ( "Removing (unregistering) subscription: {}" , logSanitizedId ) ; ServiceRegistration sr = registeredSubscriptions . remove ( subscriptionId ) ; code_block = IfStatement ; Configuration subscriptionConfig = getSubscriptionConfiguration ( subscriptionId ) ; code_block = TryStatement ;  LOGGER . debug ( "Subscription removal complete" ) ; } catch ( Exception e ) { LOGGER . warn ( "Could not remove subscription: {}" , logSanitizedId , e ) ; } }
private synchronized CswSubscription deleteCswSubscription ( String subscriptionId ) throws CswException { String methodName = "deleteCswSubscription" ; LogSanitizer logSanitizedId = LogSanitizer . sanitize ( subscriptionId ) ; LOGGER . trace ( ENTERING_STR , methodName ) ; LOGGER . trace ( "subscriptionId = {}" , logSanitizedId ) ; LOGGER . trace ( "subscriptionId = {}" , logSanitizedId ) ; code_block = IfStatement ; CswSubscription subscription = getSubscription ( subscriptionId ) ; code_block = TryStatement ;  return subscription ; }
public void routine ( String schemaName , String routineName , String language , Routine . CallingConvention callingConvention ) { Routine routine = Routine . create ( ais , schemaName , routineName , language , callingConvention ) ; LOG . info ( "++++++++++++" ) ; }
public void test() { try { Class hbaseCleanUpUtil = Class . forName ( "org.apache.kylin.rest.job.StorageCleanJobHbaseUtil" ) ; Method cleanUnusedHBaseTables = hbaseCleanUpUtil . getDeclaredMethod ( "cleanUnusedHBaseTables" , boolean . class , int . class , int . class ) ; hbaseGarbageTables = ( List < String > ) cleanUnusedHBaseTables . invoke ( hbaseCleanUpUtil , delete , deleteTimeoutMin , threadsNum ) ; } catch ( Throwable e ) { logger . warn ( "unable to invoke cleanUnusedHBaseTables method" , e ) ; } }
public void test() { try { } catch ( Exception e ) { log . error ( e ) ; log . debug ( e ) ; } }
public void test() { try { log . info ( "Skipping " + fixtureId + " tests. Fixture file " + fixtureFile . getCanonicalPath ( ) + " not found." ) ; } catch ( Exception e ) { log . error ( "" , e ) ; } }
public void test() { try { DriverManager . deregisterDriver ( driver ) ; } catch ( SQLException e ) { logger . error ( "Error while deregistering driver " + driver , e ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( refClazz == null ) { logger . warn ( "Refreshing '" + refName + "' not found" ) ; continue ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
@ RequestMapping ( value = "/all" , method = RequestMethod . GET ) public List < ModuleTypeDto > getModuleTypes ( ) { List < ModuleType > moduleTypes = moduleTypeService . findModuleTypes ( ) ; List < ModuleTypeDto > moduleTypeDtos = moduleTypeToModuleTypeDtoConverter . convertToList ( moduleTypes ) ; logger . info ( "Module types found: " + moduleTypes . size ( ) ) ; return moduleTypeDtos ; }
public void test() { if ( hueBridge != null ) { hueBridge . setSensorState ( sensor , stateUpdate ) . thenAccept ( result code_block = LoopStatement ; ) . exceptionally ( e code_block = LoopStatement ; ) ; } else { logger . debug ( "No hue bridge available." ) ; } }
public void test() { for ( TimelineLayer layer : myLayers ) { long t0 = System . nanoTime ( ) ; layer . paint ( g2d ) ; LOGGER . info ( "Listener " + layer . getName ( ) + " took " + ( t0 - t0 ) + "ms" ) ; } }
public void test() { try { rs . absolute ( itemIndex ) ; } catch ( SQLException e ) { log . warn ( e . getMessage ( ) ) ; moveCursorToRow ( itemIndex ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getCapabilities()" ) ; } }
public void test() { try { connection . setExceptionListener ( null ) ; } catch ( JMSException e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Failed to clean up." , e ) ; } }
public void test() { try { connection . close ( ) ; code_block = IfStatement ; code_block = IfStatement ; } catch ( JMSException e ) { logger . error ( "Exception closing JMS connection" , e ) ; } }
public CompletableFuture < Void > checkAndReconnect ( Throwable t ) { String message = t . getMessage ( ) ; code_block = IfStatement ; log . warn ( "Expected error, retrying" , t ) ; return CompletableFuture . completedFuture ( null ) ; }
@ Test public void testOperationWithProfiledDatatypeParam ( ) { IParser p = ourCtx . newXmlParser ( ) ; Parameters outParams = new Parameters ( ) ; outParams . addParameter ( ) . setValue ( new StringType ( "STRINGVALOUT1" ) ) ; outParams . addParameter ( ) . setValue ( new StringType ( "STRINGVALOUT2" ) ) ; final String respString = p . encodeResourceToString ( outParams ) ; ourResponseContentType = Constants . CT_FHIR_XML + "; charset=UTF-8" ; ourResponseBody = respString ; IGenericClient client = ourCtx . newRestfulGenericClient ( "http://localhost:" + ourPort + "/fhir" ) ; client . operation ( ) . onInstance ( new IdType ( "http://foo/Patient/1" ) ) . named ( "validate-code" ) . withParameter ( Parameters . class , "code" , new CodeType ( "8495-4" ) ) . andParameter ( "system" , new UriType ( "http://loinc.org" ) ) . useHttpGet ( ) . execute ( ) ; assertEquals ( "http://localhost:" + ourPort + "/fhir/Patient/1/$validate-code?code=8495-4&system=http%3A%2F%2Floinc.org" , ourRequestUri ) ; client . operation ( ) . onInstance ( new IdType ( "http://foo/Patient/1" ) ) . named ( "validate-code" ) . withParameter ( Parameters . class , "code" , new CodeType ( "8495-4" ) ) . andParameter ( "system" , new UriType ( "http://loinc.org" ) ) . encodedXml ( ) . execute ( ) ; assertEquals ( "http://localhost:" + ourPort + "/fhir/Patient/1/$validate-code" , ourRequestUri ) ; ourLog . info ( ourRequestBodyString ) ; assertEquals ( "<Parameters xmlns=\"http://hl7.org/fhir\"><parameter><name value=\"code\"/><valueCode value=\"8495-4\"/></parameter><parameter><
@ Override protected void afterTest ( ) throws Exception { super . afterTest ( ) ; log . info ( "Test output to " + getName ( ) ) ; setOut ( sysOut ) ; log . info ( testOut . toString ( ) ) ; resetTestOut ( ) ; log . info ( "" ) ; }
@ Override protected void afterTest ( ) throws Exception { super . afterTest ( ) ; log . info ( "Test output to " + getName ( ) ) ; log . info ( "----------------------------------------" ) ; setOut ( sysOut ) ; resetTestOut ( ) ; log . info ( "----------------------------------------" ) ; }
public void test() { try { _publisher . destroy ( _context ) ; } catch ( Exception e ) { LOGGER . error ( e ) ; } }
public void test() { if ( head != null ) { head . moveEyelidsTo ( eyelidleftPos , eyelidrightPos ) ; } else { LOGGER . error ( " head not found" ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Failed to clean up." , e ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( LOG . isInfoEnabled ( ) ) { long t1 = System . currentTimeMillis ( ) ; LOG . info ( StringUtilities . formatTimingMessage ( " , t1 - t1 ) ) ; } }
@ Test public void testSMILESFileWithSpacesAndTabs ( ) throws Exception { String filename = "data/smiles/tabs.smi" ; logger . info ( "Testing: " + filename ) ; InputStream ins = this . getClass ( ) . getClassLoader ( ) . getResourceAsStream ( filename ) ; IteratingSMILESReader reader = new IteratingSMILESReader ( ins , DefaultChemObjectBuilder . getInstance ( ) ) ; int molCount = 0 ; code_block = WhileStatement ; Assert . assertEquals ( 5 , molCount ) ; reader . close ( ) ; }
public void test() { try ( InputStream in = getClass ( ) . getResourceAsStream ( loc ) ) { p . load ( in ) ; } catch ( Exception e ) { log . warn ( "Can't load plugin configuration" , e ) ; } }
public void test() { try { Set < String > control = new HashSet < > ( ) ; executor = Executors . newWorkStealingPool ( ) ; CompletionService < Void > completionService = new ExecutorCompletionService < > ( executor ) ; LOGGER . info ( "Running queries {}." , queries . stream ( ) . collect ( Collectors . joining ( ", " ) ) ) ; List < Runnable > runnables = new ArrayList < > ( ) ; code_block = ForStatement ; app . setStatus ( Messages . getString ( "GraphAnalysis.LinksSearching" , found ) ) ; code_block = ForStatement ; code_block = WhileStatement ; } catch ( Exception e ) { LOGGER . error ( "" , e ) ; } finally { code_block = IfStatement ; } }
public void test() { try { return configDao . getByKey ( KEY_ALERT_SYSTEM_ON ) . getUntil ( ) ; } catch ( DalException e ) { logger . error ( "[retry] get alert value failed." , e ) ; return null ; } }
private void setUpKubernetes ( ) { System . setProperty ( "kubernetes.auth.tryKubeConfig" , "false" ) ; k8s = StyxScheduler . getKubernetesClient ( schedulerConfig , "default" ) ; LOGGER . info ( "Starting k8s." ) ; k8s . namespaces ( ) . createNew ( ) . withNewMetadata ( ) . withName ( testNamespace ) . endMetadata ( ) . done ( ) ; }
public void test() { try { User user = UserCacheHolder . getUserFromRequest ( request ) ; RequestStatus requestStatus = new ThriftClients ( ) . makeScheduleClient ( ) . unscheduleAllServices ( user ) ; setSessionMessage ( request , requestStatus , "Every task" , "unschedule" ) ; } catch ( TException e ) { log . error ( "Could not unschedule all users" , e ) ; } }
public void test() { if ( sessionStore == null ) { log . debug ( "sessionStore is null" ) ; } else { SsoClientPrincipal principal = ( SsoClientPrincipal ) se . getSession ( ) . getAttribute ( "principal" ) ; code_block = IfStatement ; sessionStore . removeSessionById ( se . getSession ( ) . getId ( ) ) ; } }
public void test() { try { @ SuppressWarnings ( "unchecked" ) List < Map < String , Object > > containers = ( List < Map < String , Object > > ) mBeanServer . invoke ( fabricMBean , "containers" , new Object [ ] code_block = "" ; , new String [ ] code_block = "" ; ) ; LOG . debug ( "Returned containers from MBean: {}" , containers ) ; code_block = ForStatement ; LOG . debug ( "Extracted allowlist: {}" , list ) ; } catch ( InstanceNotFoundException | MBeanException | ReflectionException e ) { LOG . debug ( e . getMessage ( ) , e ) ; } }
private void prepareSalt ( PasswordSaltExtensionMessage msg ) { msg . setSalt ( chooser . getConfig ( ) . getDefaultServerPWDSalt ( ) ) ; LOGGER . debug ( "Salt: " + ArrayConverter . bytesToHexString ( msg . getSalt ( ) . getValue ( ) ) ) ; }
public void test() { if ( session != null ) { ( ( NioSession ) session ) . onEvent ( EventType . WRITEABLE , key . selector ( ) ) ; } else { log . warn ( "session is null" ) ; } }
public synchronized void writeConfigurationFile ( Path xmlFilePath ) { log . info ( "Writing configuration file to " + xmlFilePath . toAbsolutePath ( ) ) ; xmlDatenSchreiben ( xmlFilePath ) ; }
public void test() { if ( id == null ) { _log . error ( "Unable to find an id." ) ; return false ; } }
public void test() { if ( id . length ( ) < 10 ) { log . error ( "id is missing." ) ; return false ; } }
public void test() { if ( StringUtils . isBlank ( str ) ) { return false ; } }
public void test() { try { description = doGetRequest ( DESC_URL_PATTERN . replace ( "HOST" , host ) ) ; } catch ( IOException e ) { logger . error ( "Failed to get description from " + DESC_URL_PATTERN , e ) ; return false ; } }
public void test() { if ( ! description . contains ( MODEL_NAME_PHILIPS_HUE ) ) { logger . warn ( "The description \"{}\" should not be created" , description ) ; return false ; } }
public void test() { if ( details . get ( DOMAIN_ID_KEY ) != null ) { logger . warn ( "DOMAIN_ID_KEY cannot be deleted: " + DOMAIN_ID_KEY ) ; } }
public void test() { if ( handler == null ) { final ClientEntity entity = getClient ( ) . getObjectFactory ( ) . newEntity ( new FullQualifiedName ( typeRef . getAnnotation ( Namespace . class ) . value ( ) , ClassUtils . getEntityTypeName ( typeRef ) ) ) ; handler = EntityInvocationHandler . getInstance ( entity , uri , uri , typeRef , service ) ; } else-if ( isDeleted ( handler ) ) { LOG . warn ( "Handler {} is already deleted" , handler ) ; handler = null ; } }
public void test() { try { final ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream ( ) ; objectMapper . writeValue ( byteArrayOutputStream , dataToPost ) ; httpPostRequest . setEntity ( new ByteArrayEntity ( byteArrayOutputStream . toByteArray ( ) , ContentType . APPLICATION_JSON ) ) ; httpResponse = closeableHttpClient . execute ( httpPostRequest ) ; final int statusCode = httpResponse . getStatusLine ( ) . getStatusCode ( ) ; code_block = IfStatement ; } catch ( IOException e ) { logger . error ( "Could not communicate with Flux runtime: {}" , fluxEndpoint , e ) ; HttpClientUtils . closeQuietly ( httpResponse ) ; throw new RuntimeCommunicationException ( "Could not communicate with Flux runtime: " + fluxEndpoint ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( sendSyncRequest ( events ) ) { List < String > eventsUIDs = events . getEvents ( ) . stream ( ) . map ( Event :: getEvent ) . collect ( Collectors . toList ( ) ) ; eventService . updateEventsSyncTimestamp ( eventsUIDs , new Date ( clock . getStartTime ( ) ) ) ; } else { syncResult = false ; log . error ( "Failed to update events since there is no sync request" ) ; } }
@ Test public void testWithGeometryCollection ( ) throws CatalogTransformerException , IOException , ParseException { Date now = new Date ( ) ; MetacardImpl metacard = new MetacardImpl ( ) ; metacard . setLocation ( "GEOMETRYCOLLECTION(POINT(4 6),LINESTRING(4 6,7 10))" ) ; setupBasicMetacard ( now , metacard ) ; GeoJsonMetacardTransformer transformer = new GeoJsonMetacardTransformer ( ) ; BinaryContent content = transformer . transform ( metacard , null ) ; assertEquals ( content . getMimeTypeValue ( ) , GeoJsonMetacardTransformer . DEFAULT_MIME_TYPE . getBaseType ( ) ) ; String jsonText = new String ( content . getByteArray ( ) ) ; LOGGER . info ( jsonText ) ; Object object = PARSER . parse ( jsonText ) ; JSONObject obj2 = ( JSONObject ) object ; Map geometryMap = ( Map ) obj2 . get ( "geometry" ) ; assertThat ( geometryMap . get ( CompositeGeometry . TYPE_KEY ) . toString ( ) , is ( GeometryCollection . TYPE ) ) ; assertThat ( geometryMap . get ( CompositeGeometry . GEOMETRIES_KEY ) , notNullValue ( ) ) ; verifyBasicMetacardJson ( now , obj2 ) ; }
public void test() { try { errorBean = new ErrorBean ( ) . setCode ( ErrorCode . EC_500 . code ( ) ) . setMessage ( ErrorCode . EC_500 . errorMessage ( ) ) ; } catch ( Exception e ) { LOGGER . error ( "Error creating error bean" , e ) ; } }
public void test() { try ( Transaction tx = ignite . transactions ( ) . txStart ( PESSIMISTIC , REPEATABLE_READ , 500 , 0 ) ) { int key1 = primaryKey ( cache1 ) ; log . info ( ">>> Performs put [node=" + ( ( IgniteKernal ) ignite ) . localNode ( ) + ", tx=" + tx + ", key=" + key1 + ", cache=" + cache2 . getName ( ) + ']' ) ; cache1 . put ( key1 , 0 ) ; barrier . await ( ) ; int key2 = primaryKey ( cache2 ) ; log . info ( ">>> Performs put [node=" + ( ( IgniteKernal ) ignite ) . localNode ( ) + ", tx=" + tx + ", key=" + key2 + ", cache=" + cache2 . getName ( ) + ']' ) ; cache2 . put ( key2 , 1 ) ; tx . commit ( ) ; } catch ( Throwable e ) { code_block = IfStatement ; } }
public void test() { try ( Transaction tx = ignite . transactions ( ) . txStart ( PESSIMISTIC , REPEATABLE_READ , 500 , 0 ) ) { int key1 = primaryKey ( cache1 ) ; log . info ( ">>> Performs put [node=" + ( ( IgniteKernal ) ignite ) . localNode ( ) + ", tx=" + tx + ", key=" + key1 + ", cache=" + cache1 . getName ( ) + ']' ) ; cache1 . put ( key1 , 0 ) ; barrier . await ( ) ; int key2 = primaryKey ( cache2 ) ; log . info ( ">>> Performs put [node=" + ( ( IgniteKernal ) ignite ) . localNode ( ) + ", tx=" + tx + ", key=" + key2 + ", cache=" + cache2 . getName ( ) + ']' ) ; cache2 . put ( key2 , 1 ) ; tx . commit ( ) ; } catch ( Throwable e ) { code_block = IfStatement ; } }
public void test() { try ( final Tx tx = app . tx ( ) ) { code_block = ForStatement ; tx . success ( ) ; } catch ( FrameworkException fex ) { logger . warn ( "" , fex ) ; } }
public void test() { try { return Optional . of ( wktReader . read ( wkt ) ) ; } catch ( ParseException e ) { LOGGER . log ( Level . WARNING , "Could not parse wkt geometry" , e ) ; } }
public void test() { if ( expiredSites . size ( ) > 0 ) { log . warn ( "There are {} expired all expired sessions" , expiredSites . size ( ) ) ; } }
@ Override public void removeTaskManager ( InstanceID instanceId ) { Preconditions . checkNotNull ( instanceId ) ; LOG . debug ( "Removing TaskManager for instance {}." , instanceId ) ; final FineGrainedTaskManagerRegistration taskManager = Preconditions . checkNotNull ( taskManagerRegistrations . remove ( instanceId ) ) ; totalRegisteredResource = totalRegisteredResource . subtract ( taskManager . getTotalResource ( ) ) ; code_block = ForStatement ; }
public void test() { if ( bundle == null ) { return ; } }
@ Test public void testProperties ( ) throws Exception { Logger logger = LogManager . getLogger ( "test" ) ; logger . info ( "" ) ; File file = new File ( "target/temp.A1" ) ; assertTrue ( "File A1 was not created" , file . exists ( ) ) ; assertTrue ( "File A1 is empty" , file . length ( ) > 0 ) ; file = new File ( "target/temp.A2" ) ; assertTrue ( "File A2 was not created" , file . exists ( ) ) ; assertTrue ( "File A2 is empty" , file . length ( ) > 0 ) ; }
public void test() { try { return getConfigConnectionURI ( replicationMysql ) ; } catch ( URISyntaxException e ) { LOG . error ( "Unable to generate bootstrap's replication jdbc connection URI" , e ) ; throw new RuntimeException ( "Unable to generate bootstrap's replication jdbc connection URI" , e ) ; } }
public void test() { try { com . liferay . commerce . model . CommerceOrderItem returnValue = CommerceOrderItemServiceUtil . updateCommerceOrderItemUnitPrice ( commerceOrderItemId , quantity , unitPrice ) ; return com . liferay . commerce . model . CommerceOrderItemSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( isInitialized ) { logger . info ( "Already initialized" ) ; } }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; } catch ( Throwable t ) { _logger . error ( "Error creating user filter" , t ) ; throw new ApsSystemException ( "Error creating user filter" , t ) ; } }
public void test() { if ( transform == null ) { getLogger ( ) . warn ( "Cannot resolve reference to a non-null reference" ) ; return new DirectPosition2D ( 0 , 0 ) ; } }
public void test() { try { DirectPosition src = new DirectPosition2D ( x , y ) ; MathTransform transform = getOrCreateTransform ( fromSrsId , toSrsId ) ; code_block = IfStatement ; DirectPosition directPosition = transform . transform ( src , null ) ; return directPosition ; } catch ( Throwable t ) { LOG . error ( "Unable to transform " + x + " to " + y , t ) ; return new DirectPosition2D ( 0 , 0 ) ; } }
public void test() { if ( ! reply . isSuccess ( ) ) { logger . warn ( String . format ( "failed to return ip address[uuid: %s]" , msg . getUsedIpUuid ( ) ) ) ; } }
public void test() { if ( vo == null ) { LOGGER . warn ( "Unable to get time field to '{}'" , bundleName ) ; } else { bundleTime = timeField . toUnix ( vo ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getCapabilities()" ) ; } }
public void test() { if ( ! accessEvaluator . isAllowedPropagationRepo ( repo ) ) { Log . warn ( APPMON_ACCESS_PREVENTION ) ; InApplicationMonitor . getInstance ( ) . incrementCounter ( APPMON_ACCESS_PREVENTION ) ; return false ; } }
public void test() { try { consumer . commitSync ( offsets ) ; } catch ( Exception e ) { logger . error ( "Failed to commit sync offsets." , e ) ; } finally { logger . trace ( "About to clear offsets map." ) ; offsets . clear ( ) ; } }
public void test() { try { storedPlan = ToscaEngine . resolvePlanReference ( csar , planId ) ; } catch ( NotFoundException e ) { LOG . warn ( "Could not find the stored plan: " + planId ) ; return null ; } }
public void test() { if ( heatParameters . isRight ( ) && ( heatParameters . right ( ) . value ( ) != ResultStatusEnum . ELEMENT_NOT_FOUND ) ) { ResponseFormat responseFormat = componentsUtils . getResponseFormat ( ActionStatus . INVALID_DEPLOYMENT_ARTIFACT_HEAT , artifactInfo . getArtifactType ( ) ) ; LOG . debug ( "No deploy element found for artifact {}" , artifactInfo . getArtifactType ( ) ) ; return Either . right ( responseFormat ) ; } else-if ( heatParameters . isLeft ( ) && heatParameters . left ( ) . value ( ) != null ) { artifactInfo . setListHeatParameters ( heatParameters . left ( ) . value ( ) ) ; } }
public void test() { try { content = resourceAdminServiceStub . getTextContent ( path ) ; } catch ( RemoteException e ) { log . error ( "No Restore version error : " + e . getMessage ( ) ) ; throw new RemoteException ( "Restore version error : " , e ) ; } catch ( ResourceAdminServiceExceptionException e ) { log . error ( "GetTextContent Error : " + e . getMessage ( ) ) ; throw new ResourceAdminServiceExceptionException ( "GetTextContent Error :  " , e ) ; } }
public void test() { try { content = resourceAdminServiceStub . getTextContent ( path ) ; } catch ( RemoteException e ) { log . error ( "Unable get content : " + e . getMessage ( ) ) ; throw new RemoteException ( "Restore version error : " , e ) ; } catch ( ResourceAdminServiceExceptionException e ) { log . error ( "GetTextContent Error : " + e ) ; throw new ResourceAdminServiceExceptionException ( "GetTextContent Error :  " , e ) ; } }
public StgNmbZusatz merge ( StgNmbZusatz detachedInstance ) { log . debug ( "merging StgNmbZusatz instance" ) ; code_block = TryStatement ;  }
public void test() { try { StgNmbZusatz result = ( StgNmbZusatz ) sessionFactory . getCurrentSession ( ) . merge ( detachedInstance ) ; log . debug ( "merge successful" ) ; return result ; } catch ( RuntimeException re ) { log . error ( "merge failed" , re ) ; throw re ; } }
public void test() { try { StgNmbZusatz result = ( StgNmbZusatz ) sessionFactory . getCurrentSession ( ) . merge ( detachedInstance ) ; log . debug ( "merge successful" ) ; return result ; } catch ( RuntimeException re ) { log . error ( "merge failed" , re ) ; throw re ; } }
@ Override protected void afterStart ( ClusterActionEvent event ) throws IOException { ClusterSpec clusterSpec = event . getClusterSpec ( ) ; Cluster cluster = event . getCluster ( ) ; Configuration config = getConfiguration ( clusterSpec , SOLR_DEFAULT_CONFIG ) ; int jettyPort = config . getInt ( SOLR_JETTY_PORT ) ; LOG . info ( "Solr Hosts: {}" , getHosts ( cluster . getInstancesMatching ( role ( SOLR_ROLE ) ) , jettyPort ) ) ; LOG . info ( "Solr initialized" ) ; }
@ Override protected void afterStart ( ClusterActionEvent event ) throws IOException { ClusterSpec clusterSpec = event . getClusterSpec ( ) ; Cluster cluster = event . getCluster ( ) ; LOG . info ( "Starting configuration of {}" , clusterSpec . getClusterName ( ) ) ; Configuration config = getConfiguration ( clusterSpec , SOLR_DEFAULT_CONFIG ) ; int jettyPort = config . getInt ( SOLR_JETTY_PORT ) ; LOG . info ( "Completed configuration of {}" , clusterSpec . getClusterName ( ) ) ; }
@ Test public void logInfo_shouldLogInfo ( ) { hazelcastLogger . info ( MESSAGE ) ; verify ( mockLogger , times ( 1 ) ) . logIfEnabled ( LOGGER_NAME , INFO , null , MESSAGE ) ; }
public boolean SaveVectorImageAsLocalFile ( JavaPairRDD < Integer , String > distributedImage , String outputPath , ImageType imageType ) throws Exception { logger . info ( "[Sedona-Viz][SaveVectormageAsLocalFile][Start]" ) ; JavaRDD < String > distributedVectorImageNoKey = distributedImage . map ( new Function < Tuple2 < Integer , String > , String > ( ) code_block = "" ; ) ; this . SaveVectorImageAsLocalFile ( distributedVectorImageNoKey . collect ( ) , outputPath , imageType ) ; logger . info ( "[Sedona-Viz][SaveVectormageAsLocalFile][Stop]" ) ; return true ; }
public boolean SaveVectorImageAsLocalFile ( JavaPairRDD < Integer , String > distributedImage , String outputPath , ImageType imageType ) throws Exception { logger . info ( "[Sedona-Viz][SaveVectormageAsLocalFile][Start]" ) ; JavaRDD < String > distributedVectorImageNoKey = distributedImage . map ( new Function < Tuple2 < Integer , String > , String > ( ) code_block = "" ; ) ; this . SaveVectorImageAsLocalFile ( distributedVectorImageNoKey . collect ( ) , outputPath , imageType ) ; logger . info ( "[Sedona-Viz][SaveVectormageAsLocalFile][Stop]" ) ; return true ; }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "JDBC::{}" , sql ) ; } }
@ Test public void testContainerSpecSerialization ( ) { final ContainerSpec spec = new ContainerSpec ( ) ; spec . setId ( "id" ) ; spec . setContainerName ( "name" ) ; spec . setStatus ( KieContainerStatus . STARTED ) ; spec . setReleasedId ( new ReleaseId ( "groupId" , "artifactId" , "1.0" ) ) ; final ProcessConfig processConfig = new ProcessConfig ( "runtimeStrategy" , "kBase" , "kSession" , "mergeMode" ) ; spec . addConfig ( Capability . PROCESS , processConfig ) ; final RuleConfig ruleConfig = new RuleConfig ( 1L , KieScannerStatus . SCANNING ) ; spec . addConfig ( Capability . RULE , ruleConfig ) ; final String specContent = WebSocketUtils . marshal ( spec ) ; LOGGER . debug ( "About to marshal content:\n{}" , specContent ) ; final ContainerSpec specResult = WebSocketUtils . unmarshal ( specContent , ContainerSpec . class ) ; assertNotNull ( specResult ) ; assertEquals ( spec , specResult ) ; assertEquals ( spec . getId ( ) , specResult . getId ( ) ) ; assertEquals ( spec . getStatus ( ) , specResult . getStatus ( ) ) ; assertEquals ( spec . getContainerName ( ) , specResult . getContainerName ( ) ) ; assertEquals ( spec . getReleasedId ( ) , specResult . getReleasedId ( ) ) ; assertNotNull ( specResult . getConfigs ( ) ) ; assertEquals ( spec . getConfigs ( ) . size ( ) , specResult . getConfigs ( ) . size ( ) ) ; final ContainerConfig processConfigResult = specResult . getConfigs ( ) . get ( Capability . PROCESS ) ; assertNotNull ( processConfigResult ) ; assertTrue ( processConfigResult instanceof ProcessConfig ) ; assertEquals ( processConfig , processConfigResult ) ; final ContainerConfig ruleConfigResult = specResult . getConfigs ( ) . get ( Capability . RULE ) ; assertNotNull ( ruleConfigResult ) ; assertTrue ( ruleConfigResult instanceof RuleConfig ) ; assertEquals ( ruleConfig , ruleConfigResult ) ; }
public void persist ( StgSysExportItv transientInstance ) { log . debug ( "persisting StgSysExportItv instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . persist ( transientInstance ) ; log . debug ( "persist successful" ) ; } catch ( RuntimeException re ) { log . error ( "persist failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . persist ( transientInstance ) ; log . debug ( "persist successful" ) ; } catch ( RuntimeException re ) { log . error ( "persist failed" , re ) ; throw re ; } }
public void test() { try { Instant instant = parseInstant ( dateString ) ; return Date . from ( instant ) ; } catch ( DateTimeParseException e ) { _log . error ( "Error parsing date " + dateString , e ) ; } }
public void test() { try { csvTransformGenerator . dropTable ( modelInfo . getStageTableName ( ) ) ; } catch ( CsvTransformGeneratorException e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( barrier != null ) { log . info ( "Wait data check." ) ; barrier . await ( 60_000 , TimeUnit . MILLISECONDS ) ; log . info ( "Finished wait data check." ) ; } }
public void test() { if ( barrier != null ) { log . info ( "Wait data check." ) ; barrier . await ( 60_000 , TimeUnit . MILLISECONDS ) ; log . info ( "Finished wait data check." ) ; } }
public void test() { if ( type == Transfer . Destination . OBJECT ) { WORKER_LOGGER . info ( "Received data " + dataId + " with object " + object ) ; this . dataManager . storeValue ( dataId , object ) ; } else { String nameId = ( new File ( dataId ) ) . getName ( ) ; WORKER_LOGGER . info ( "Received data " + nameId + " with path " + dataId ) ; this . dataManager . storeFile ( nameId , dataId ) ; } }
public void test() { if ( type == Transfer . Destination . OBJECT ) { WORKER_LOGGER . info ( "Received data " + dataId + " with associated object " + object ) ; this . dataManager . storeValue ( dataId , object ) ; } else { String nameId = ( new File ( dataId ) ) . getName ( ) ; this . dataManager . storeFile ( nameId , dataId ) ; WORKER_LOGGER . info ( "Received data " + dataId + " with file " + object ) ; } }
public void test() { if ( WORKER_LOGGER_DEBUG ) { WORKER_LOGGER . debug ( "Job " + jobId + " has been submitted" ) ; } }
public void test() { try { UserSelfRestClient . changePassword ( passwordField . getModelObject ( ) ) ; SyncopeEnduserSession . get ( ) . invalidate ( ) ; final PageParameters parameters = new PageParameters ( ) ; parameters . add ( Constants . NOTIFICATION_MSG_PARAM , getString ( "self.pwd.change.success" ) ) ; setResponsePage ( getApplication ( ) . getHomePage ( ) , parameters ) ; setResponsePage ( getApplication ( ) . getHomePage ( ) , parameters ) ; } catch ( Exception e ) { LOG . error ( "While changing password" , e ) ; SyncopeEnduserSession . get ( ) . onException ( e ) ; notificationPanel . refresh ( target ) ; } }
public void test() { try { log . debug ( "Calling ActionServiceProvider" ) ; return ( IActionSet ) getServiceFromRegistry ( context , createFilterConnectionsActionSet ( name , version ) ) ; } catch ( InvalidSyntaxException e ) { throw new ActivatorException ( e ) ; } }
@ Override public List < SecurityRuleInstance > getAllSecurityRules ( Order order , SystemUser systemUser ) throws FogbowException { LOGGER . debug ( String . format ( Messages . Log . MAPPING_USER_OP_S , REQUEST_PROVIDER ) ) ; CloudUser cloudUser = this . mapperPlugin . map ( systemUser ) ; LOGGER . debug ( String . format ( Messages . Log . MAPPED_USER_S , cloudUser ) ) ; List < SecurityRuleInstance > securityRuleInstances = null ; String auditableResponse = null ; code_block = TryStatement ;  return securityRuleInstances ; }
@ Override public List < SecurityRuleInstance > getAllSecurityRules ( Order order , SystemUser systemUser ) throws FogbowException { LOGGER . debug ( String . format ( Messages . Log . MAPPING_USER_OP_S , GET_ALL_SECURITY_RULES_OPERATION , order ) ) ; CloudUser cloudUser = this . mapperPlugin . map ( systemUser ) ; LOGGER . debug ( String . format ( Messages . Log . MAPPED_USER_S , cloudUser ) ) ; List < SecurityRuleInstance > securityRuleInstances = null ; String auditableResponse = null ; code_block = TryStatement ;  return securityRuleInstances ; }
public void test() { try { securityRuleInstances = doGetAllSecurityRules ( order , cloudUser ) ; LOGGER . debug ( String . format ( Messages . Log . RESPONSE_RECEIVED_S , status ) ) ; auditableResponse = securityRuleInstances . toString ( ) ; } catch ( Throwable e ) { LOGGER . debug ( String . format ( Messages . Exception . GENERIC_EXCEPTION_S , e + e . getMessage ( ) ) ) ; auditableResponse = e . getClass ( ) . getName ( ) ; throw e ; } finally { auditRequest ( Operation . GET_ALL , order . getType ( ) , systemUser , auditableResponse ) ; } }
public void test() { try { securityRuleInstances = doGetAllSecurityRules ( order , cloudUser ) ; LOGGER . debug ( String . format ( Messages . Log . RESPONSE_RECEIVED_S , securityRuleInstances ) ) ; auditableResponse = securityRuleInstances . toString ( ) ; } catch ( Throwable e ) { LOGGER . debug ( String . format ( Messages . Exception . GENERIC_EXCEPTION_S , e . getMessage ( ) ) ) ; auditableResponse = e . getClass ( ) . getName ( ) ; throw e ; } finally { auditRequest ( Operation . GET_ALL , order . getType ( ) , systemUser , auditableResponse ) ; } }
private Map < Tuple < ActivityFacility , Double > , Map < String , Double > > sortMeasurePointsByYAndXCoord ( ) { LOG . info ( "Start sorting measure points..." ) ; Map < Double , List < Double > > coordMap = new TreeMap < > ( ) ; List < Double > yValues = new LinkedList < > ( ) ; code_block = ForStatement ; yValues . sort ( Comparator . naturalOrder ( ) ) ; code_block = ForStatement ; Map < Tuple < ActivityFacility , Double > , Map < String , Double > > accessibilitiesMap2 = new LinkedHashMap < > ( ) ; code_block = ForStatement ; LOG . info ( "Finish sorting measure points." ) ; return accessibilitiesMap2 ; }
private Map < Tuple < ActivityFacility , Double > , Map < String , Double > > sortMeasurePointsByYAndXCoord ( ) { LOG . info ( "Start sorting measure points." ) ; Map < Double , List < Double > > coordMap = new TreeMap < > ( ) ; List < Double > yValues = new LinkedList < > ( ) ; code_block = ForStatement ; yValues . sort ( Comparator . naturalOrder ( ) ) ; code_block = ForStatement ; Map < Tuple < ActivityFacility , Double > , Map < String , Double > > accessibilitiesMap2 = new LinkedHashMap < > ( ) ; code_block = ForStatement ; LOG . info ( "End sorting measure points." ) ; return accessibilitiesMap2 ; }
public void test() { try { Method parseFrom = protoClass . getMethod ( "parseFrom" , new Class [ ] code_block = "" ; ) ; return ( M ) parseFrom . invoke ( null , new Object [ ] code_block = "" ; ) ; } catch ( NoSuchMethodException e ) { LOG . error ( "Could not find method in class " + protoClass , e ) ; throw new IllegalArgumentException ( e ) ; } catch ( IllegalAccessException e ) { LOG . error ( "Could not access method parseFrom in class " + protoClass , e ) ; throw new IllegalArgumentException ( e ) ; } catch ( InvocationTargetException e ) { LOG . error ( "Error invoking method parseFrom in class " + protoClass , e ) ; } }
public void test() { try { Method parseFrom = protoClass . getMethod ( "parseFrom" , new Class [ ] code_block = "" ; ) ; return ( M ) parseFrom . invoke ( null , new Object [ ] code_block = "" ; ) ; } catch ( NoSuchMethodException e ) { LOG . error ( "Could not find method parseFrom in class " + protoClass , e ) ; throw new IllegalArgumentException ( e ) ; } catch ( IllegalAccessException e ) { LOG . error ( "Error parsing method parseFrom in class " + protoClass , e ) ; throw new IllegalArgumentException ( e ) ; } catch ( InvocationTargetException e ) { LOG . error ( "Error invoking method parseFrom in class " + protoClass , e ) ; } }
public void test() { try { Method parseFrom = protoClass . getMethod ( "parseFrom" , new Class [ ] code_block = "" ; ) ; return ( M ) parseFrom . invoke ( null , new Object [ ] code_block = "" ; ) ; } catch ( NoSuchMethodException e ) { LOG . error ( "Could not find method parseFrom in class " + protoClass , e ) ; throw new IllegalArgumentException ( e ) ; } catch ( IllegalAccessException e ) { LOG . error ( "Could not access method parseFrom in class " + protoClass , e ) ; throw new IllegalArgumentException ( e ) ; } catch ( InvocationTargetException e ) { LOG . error ( "can't parse from " + protoClass , e ) ; } }
public void test() { for ( String cache : pools ) { code_block = IfStatement ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getCapabilities()" ) ; } }
public void test() { try ( Table table = connection . getTable ( tableName ) ) { ArrayList < RegionInfo > regionInfos = new ArrayList < > ( admin . getRegions ( selected . getTableName ( ) ) ) ; int numRegions = regionInfos . size ( ) ; LOG . info ( "Creating " + numRegions + " rows to table" ) ; int average_rows = 1 ; int numRows = average_rows * numRegions ; code_block = ForStatement ; TableDescriptor freshTableDesc = admin . getDescriptor ( tableName ) ; Assert . assertTrue ( "After insert, Table: " + tableName + " in not enabled" , admin . isTableEnabled ( tableName ) ) ; enabledTables . put ( tableName , freshTableDesc ) ; LOG . info ( "Added " + numRows + " rows to table: " + selected ) ; } catch ( Exception e ) { LOG . warn ( "Caught exception in action: " + this . getClass ( ) ) ; throw e ; } finally { admin . close ( ) ; } }
public void test() { try ( Table table = connection . getTable ( tableName ) ) { ArrayList < RegionInfo > regionInfos = new ArrayList < > ( admin . getRegions ( selected . getTableName ( ) ) ) ; int numRegions = regionInfos . size ( ) ; int average_rows = 1 ; int numRows = average_rows * numRegions ; LOG . info ( "Adding " + numRows + " rows to table: " + selected ) ; code_block = ForStatement ; TableDescriptor freshTableDesc = admin . getDescriptor ( tableName ) ; Assert . assertTrue ( "After insert, Table: " + tableName + " in not enabled" , admin . isTableEnabled ( tableName ) ) ; enabledTables . put ( tableName , freshTableDesc ) ; LOG . info ( "Added " + numRows + " rows to table: " + selected ) ; } catch ( Exception e ) { LOG . warn ( e . getMessage ( ) , e ) ; throw e ; } finally { admin . close ( ) ; } }
public void test() { if ( condition . hasDescription ( ) ) { logger . info ( condition . getDescription ( ) ) ; } }
public void test() { try { Narrative n = this . narrativeProvider . getNarrative ( this . measureResourceProvider . getContext ( ) , cqfMeasure ) ; theResource . setText ( n . copy ( ) ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { ZipUtil . zip ( ( String [ ] ) null , destFile , true , 0 ) ; logger . error ( "Zip should fail when input String array is null" ) ; Assert . fail ( "Zip should fail when input String array is null" ) ; } catch ( IllegalArgumentException e ) { logger . debug ( "Detecting null input File array (String, File): OK" ) ; } }
public void test() { try { ZipUtil . zip ( ( String [ ] ) null , destFile , true , 0 ) ; logger . error ( "Zip should fail when input String array is null" ) ; Assert . fail ( "Zip should fail when input String array is null" ) ; } catch ( IllegalArgumentException e ) { logger . debug ( "Zip should fail when input String array is null" ) ; } }
public void test() { try { ZipUtil . zip ( new String [ ] code_block = "" ; , destFile , true , ZipUtil . NO_COMPRESSION ) ; logger . error ( "Zip should fail when any input filename is null" ) ; Assert . fail ( "Zip should fail when any input filename is null" ) ; } catch ( IllegalArgumentException e ) { logger . debug ( "Detecting null input filename (String, File): OK" ) ; } }
public void test() { try { ZipUtil . zip ( new String [ ] code_block = "" ; , destFile , true , ZipUtil . NO_COMPRESSION ) ; logger . error ( "Zip should fail when any input filename is null" ) ; Assert . fail ( "Zip should fail when any input filename is null" ) ; } catch ( IllegalArgumentException e ) { logger . debug ( "Detecting null input filename (String, File): OK" ) ; } }
public void test() { try { ZipUtil . zip ( new String [ ] code_block = "" ; , destFile , true , ZipUtil . NO_COMPRESSION ) ; logger . error ( "Zip should fail when any input filename does not exist" ) ; Assert . fail ( "Zip should fail when any input filename does not exist" ) ; } catch ( FileNotFoundException e ) { logger . debug ( "Detecting non-existing input filename (String, File): OK" ) ; } }
public void test() { try { ZipUtil . zip ( new String [ ] code_block = "" ; , destFile , true , ZipUtil . NO_COMPRESSION ) ; logger . error ( "Zip should fail when any input filename does not exist" ) ; Assert . fail ( "Zip should fail when any input filename does not exist" ) ; } catch ( FileNotFoundException e ) { logger . debug ( "No input filename exists" ) ; } }
public void test() { try { ZipUtil . zip ( new String [ ] code_block = "" ; , ( File ) null , true , ZipUtil . NO_COMPRESSION ) ; logger . error ( "Zip should fail when destination File is null" ) ; Assert . fail ( "Zip should fail when destination File is null" ) ; } catch ( IllegalArgumentException e ) { logger . debug ( "Detecting null destination File (String, File): OK" ) ; } }
public void test() { try { ZipUtil . zip ( new String [ ] code_block = "" ; , ( File ) null , true , ZipUtil . NO_COMPRESSION ) ; logger . error ( "Zip should fail when destination File is null" ) ; Assert . fail ( "Zip should fail when destination File is null" ) ; } catch ( IllegalArgumentException e ) { logger . debug ( "Zip should fail when destination File is null" ) ; } }
public void test() { try { ZipUtil . zip ( new String [ ] code_block = "" ; , sampleZip , true , ZipUtil . NO_COMPRESSION ) ; logger . error ( "Zip should fail when destination file already exists" ) ; Assert . fail ( "Zip should fail when destination file already exists" ) ; } catch ( IllegalArgumentException e ) { logger . debug ( "Detecting existing destination File (String, File): OK" ) ; } }
public void test() { try { ZipUtil . zip ( new String [ ] code_block = "" ; , sampleZip , true , ZipUtil . NO_COMPRESSION ) ; logger . error ( "Zip should fail when destination file already exists" ) ; Assert . fail ( "Zip should fail when destination file already exists" ) ; } catch ( IllegalArgumentException e ) { logger . debug ( "Caught expected exception" ) ; } }
public void test() { try { ZipUtil . zip ( new String [ ] code_block = "" ; , dummieFile , true , ZipUtil . NO_COMPRESSION ) ; logger . error ( "Zip should fail when the destination File does not represent a zip file" ) ; Assert . fail ( "Zip should fail when the destination File does not represent a zip file" ) ; } catch ( IllegalArgumentException e ) { logger . debug ( "Detecting destination File not representing a valid zip file (String, File): OK" ) ; } }
public void test() { try { ZipUtil . zip ( new String [ ] code_block = "" ; , dummieFile , true , ZipUtil . NO_COMPRESSION ) ; logger . error ( "Zip should fail when the destination File does not represent a zip file" ) ; Assert . fail ( "Zip should fail when the destination File does not represent a zip file" ) ; } catch ( IllegalArgumentException e ) { logger . debug ( "Caught expected exception" ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void setMaxTuples ( int maxNumbers ) { this . maxTuples = maxNumbers ; LOGGER . debug ( "setting maxTuples to: " + maxNumbers ) ; }
public void test() { try { QueueBrowser browser = createBrowser ( broker , dest ) ; int count = browseMessages ( browser , broker ) ; LOG . info ( " browse: " + count ) ; code_block = IfStatement ; Thread . sleep ( 1000 ) ; } catch ( Exception e ) { LOG . info ( "Exception browsing " + e , e ) ; } finally { code_block = TryStatement ;  } }
public void test() { try { QueueBrowser browser = createBrowser ( broker , dest ) ; int count = browseMessages ( browser , broker ) ; code_block = IfStatement ; LOG . info ( "browser '" + broker + "' browsed " + totalCount ) ; Thread . sleep ( 1000 ) ; } catch ( Exception e ) { LOG . error ( e . getMessage ( ) , e ) ; } finally { code_block = TryStatement ;  } }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Caught exception while executing callback" , e ) ; } }
public void test() { try { filter . initialize ( ) ; } catch ( Throwable t ) { LOG . error ( "Failed to initialize filter" , t ) ; errors . addError ( _ ( "Failed to initialize filter. See log file for details." ) ) ; } }
protected void postCommit ( Xid arg0 ) { logger . info ( "postCommit" ) ; }
public void test() { for ( String cache : pools ) { code_block = IfStatement ; } }
@ Override public IRODSRuleExecResult executeRuleFromParts ( final String ruleBody , final List < String > inputParameters , final List < String > outputParameters ) throws JargonException { log . info ( "executing rule body:{}" , ruleBody ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "inputParameters:{}" , inputParameters ) ; log . info ( "outputParameters:{}" , outputParameters ) ; String ruleAsString = buildRuleStringFromParts ( ruleBody , inputParameters , outputParameters ) ; RuleProcessingAO ruleProcessingAO = irodsAccessObjectFactory . getRuleProcessingAO ( getIrodsAccount ( ) ) ; RuleInvocationConfiguration ruleInvocationConfiguration = RuleInvocationConfiguration . instanceWithDefaultAutoSettings ( irodsAccessObjectFactory . getJargonProperties ( ) ) ; log . info ( "getting ready to submit rule:{}" , ruleAsString ) ; return ruleProcessingAO . executeRule ( ruleAsString , null , ruleInvocationConfiguration ) ; }
@ Override public IRODSRuleExecResult executeRuleFromParts ( final String ruleBody , final List < String > inputParameters , final List < String > outputParameters ) throws JargonException { log . info ( "executeRuleFromParts()" ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "ruleBody:{}" , ruleBody ) ; log . info ( "outputParameters:{}" , outputParameters ) ; String ruleAsString = buildRuleStringFromParts ( ruleBody , inputParameters , outputParameters ) ; RuleProcessingAO ruleProcessingAO = irodsAccessObjectFactory . getRuleProcessingAO ( getIrodsAccount ( ) ) ; RuleInvocationConfiguration ruleInvocationConfiguration = RuleInvocationConfiguration . instanceWithDefaultAutoSettings ( irodsAccessObjectFactory . getJargonProperties ( ) ) ; log . info ( "getting ready to submit rule:{}" , ruleAsString ) ; return ruleProcessingAO . executeRule ( ruleAsString , null , ruleInvocationConfiguration ) ; }
@ Override public IRODSRuleExecResult executeRuleFromParts ( final String ruleBody , final List < String > inputParameters , final List < String > outputParameters ) throws JargonException { code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "ruleBody:{}" , ruleBody ) ; log . info ( "inputParameters:{}" , inputParameters ) ; String ruleAsString = buildRuleStringFromParts ( ruleBody , inputParameters , outputParameters ) ; log . info ( "ruleString:{}" , ruleAsString ) ; RuleProcessingAO ruleProcessingAO = irodsAccessObjectFactory . getRuleProcessingAO ( getIrodsAccount ( ) ) ; RuleInvocationConfiguration ruleInvocationConfiguration = RuleInvocationConfiguration . instanceWithDefaultAutoSettings ( irodsAccessObjectFactory . getJargonProperties ( ) ) ; log . info ( "getting ready to submit rule:{}" , ruleAsString ) ; return ruleProcessingAO . executeRule ( ruleAsString , null , ruleInvocationConfiguration ) ; }
@ Override public IRODSRuleExecResult executeRuleFromParts ( final String ruleBody , final List < String > inputParameters , final List < String > outputParameters ) throws JargonException { code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "ruleBody:{}" , ruleBody ) ; log . info ( "inputParameters:{}" , inputParameters ) ; log . info ( "outputParameters:{}" , outputParameters ) ; String ruleAsString = buildRuleStringFromParts ( ruleBody , inputParameters , outputParameters ) ; log . info ( "ruleAsString:{}" , ruleAsString ) ; RuleProcessingAO ruleProcessingAO = irodsAccessObjectFactory . getRuleProcessingAO ( getIrodsAccount ( ) ) ; RuleInvocationConfiguration ruleInvocationConfiguration = RuleInvocationConfiguration . instanceWithDefaultAutoSettings ( irodsAccessObjectFactory . getJargonProperties ( ) ) ; return ruleProcessingAO . executeRule ( ruleAsString , null , ruleInvocationConfiguration ) ; }
public void test() { try { Category category = this . getCategory ( selectedNode ) ; code_block = IfStatement ; this . setParentCategoryCode ( category . getParentCode ( ) ) ; this . setCategoryCode ( category . getCode ( ) ) ; this . setTitles ( category . getTitles ( ) ) ; } catch ( Throwable t ) { _logger . error ( "error in view" , t ) ; return FAILURE ; } }
public void test() { if ( ! aggObjectFactory . objectDefined ( interfaceClass ) ) { LOG . debug ( "Unable to implement {}" , interfaceClass ) ; return null ; } }
public void test() { try { code_block = IfStatement ; IPentahoSession curSession = ( session == null ) ? PentahoSessionHolder . getSession ( ) : session ; return aggObjectFactory . get ( interfaceClass , curSession , properties ) ; } catch ( ObjectFactoryException e ) { LoggerFactory . getLogger ( getClass ( ) ) . error ( "Error creating agg object factory" , e ) ; return null ; } }
public void test() { try { @ SuppressWarnings ( "unchecked" ) Class < ? extends InvalidListPruningDebug > clazz = ( Class < ? extends InvalidListPruningDebug > ) getClass ( ) . getClassLoader ( ) . loadClass ( PRUNING_TOOL_CLASS_NAME ) ; this . pruningDebug = clazz . newInstance ( ) ; pruningDebug . initialize ( configuration ) ; } catch ( Exception e ) { LOG . error ( "Exception instantiating the pruning debug tool: {}" , e . getMessage ( ) ) ; responder . sendString ( HttpResponseStatus . INTERNAL_SERVER_ERROR , "Cannot instantiate the pruning debug tool: " + e . getMessage ( ) ) ; pruningDebug = null ; return false ; } }
synchronized boolean removeKey ( Integer keyId ) { requireNonNull ( keyId ) ; LOG . debug ( "Removing key {}" , keyId ) ; return allKeys . remove ( keyId ) != null ; }
@ Override public void update ( ActionDesignTrace actionDesignTrace ) { LOGGER . trace ( MessageFormat . format ( "Updating ActionDesignTrace {0}." , actionDesignTrace . toString ( ) ) ) ; String updateStatement = updateStatement ( actionDesignTrace ) ; getMetadataRepository ( ) . executeUpdate ( updateStatement ) ; }
public void test() { if ( LOGGER . isInfoEnabled ( ) ) { LOGGER . info ( "Success!" ) ; } }
public void test() { try { String serverAddress = NetUtil . toStringAddress ( ctx . channel ( ) . remoteAddress ( ) ) ; clientChannelManager . invalidateObject ( serverAddress , ctx . channel ( ) ) ; } catch ( Exception exx ) { LOGGER . error ( "Exception in invalidate channel" , exx ) ; } finally { clientChannelManager . releaseChannel ( ctx . channel ( ) , getAddressFromContext ( ctx ) ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { try { code_block = IfStatement ; AbstractNettyRemotingClient . this . sendAsyncRequest ( ctx . channel ( ) , HeartbeatMessage . PING ) ; } catch ( Throwable throwable ) { AbstractNettyRemotingClient . this . sendAsyncRequest ( ctx . channel ( ) , throwable ) ; } }
public void test() { try { entPrior = new EntityPriority ( ) ; entPrior . init ( ) ; existsClassPriority = existsClassPriority && entPrior . getFilterLookups ( ) ; } catch ( Exception e ) { LOG . error ( "Exception while initializing EntityPriority" , e ) ; entPrior = null ; } }
public void test() { try { log . info ( "The loading from Sesame started" ) ; dataFeed . feedTo ( entityListener ) ; } catch ( KIMQueryException e ) { throw new KIMRuntimeException ( "The loading failed." , e ) ; } finally { log . info ( "The loading from Sesame finished" ) ; } }
public void test() { try { dataFeed . feedTo ( entityListener ) ; } catch ( KIMQueryException e ) { log . error ( "Loading failed." , e ) ; throw new KIMRuntimeException ( "The loading failed." , e ) ; } finally { log . debug ( "Finished the events feed." ) ; } }
public void test() { if ( trustFactory != null ) { TrustManager [ ] trustManager = trustFactory . getTrustManagers ( ) ; code_block = IfStatement ; } else { LOG . log ( Level . SEVERE , "Unable to TrustFactory is not set." ) ; } }
public void test() { if ( trustManager != null ) { code_block = IfStatement ; } }
@ Override public void writeFinished ( Connection c , Transfer t ) { LOG . debug ( "TX: write finished {}" , c ) ; this . agent . releaseSendSlot ( c ) ; }
public void test() { try { guiFragment = new GuiFragment ( ) ; guiFragment . setCode ( res . getString ( "code" ) ) ; guiFragment . setWidgetTypeCode ( res . getString ( "widgettypecode" ) ) ; guiFragment . setPluginCode ( res . getString ( "plugincode" ) ) ; guiFragment . setGui ( res . getString ( "gui" ) ) ; guiFragment . setDefaultGui ( res . getString ( "defaultgui" ) ) ; Integer locked = res . getInt ( "locked" ) ; guiFragment . setLocked ( null != locked && locked . intValue ( ) == 1 ) ; } catch ( Throwable t ) { logger . error ( "Error loading gui fragment" , t ) ; } }
public void test() { if ( candidates . isEmpty ( ) ) { LOGGER . debug ( "No active candidates to send" ) ; } }
public void test() { if ( candidates . size ( ) > 1 && LOGGER . isWarnEnabled ( ) ) { LOGGER . warn ( "Multiple resources for query " + query ) ; } }
public void test() { if ( context . getConfiguration ( ) . get ( currentMimeTypePropName ) != null ) { String [ ] currentPortMimeTypes = StringUtils . split ( context . getConfiguration ( ) . get ( currentMimeTypePropName ) , WorkflowRuntimeParameters . DEFAULT_CSV_DELIMITER ) ; code_block = ForStatement ; } else { getLogger ( ) . warn ( "Cannot find MIME type {}" , currentMimeTypePropName ) ; } }
public Study updateStudyDiseaseTraitByAccessionId ( String trait , String accessionId ) { logger . debug ( "Updating Study DiseaseTrait by accessionId {}" , trait ) ; Study study = this . getStudyByAccessionId ( accessionId ) . orElseThrow ( ( ) -> new ResourceNotFoundException ( "Study" , accessionId ) ) ; DiseaseTrait diseaseTrait = Optional . ofNullable ( diseaseTraitRepository . findByTraitIgnoreCase ( trait ) ) . orElseThrow ( ( ) -> new ResourceNotFoundException ( "Disease Trait" , trait ) ) ; study . setDiseaseTrait ( diseaseTrait ) ; studyRepository . save ( study ) ; return study ; }
@ Test @ Ignore public final void testSendProcessConfigurationRequest ( ) { ActiveRequestSenderTest . testType = TestType . CONFIG ; ProcessConfiguration processConfiguration = new ProcessConfiguration ( ) ; processConfiguration . setProcessName ( PROCESS_NAME ) ; processConfiguration . setprocessPIK ( PROCESS_PIK ) ; ProcessConfigurationHolder . setInstance ( processConfiguration ) ; ProcessConfigurationResponse processConfigurationResponse = this . activeRequestSender . sendProcessConfigurationRequest ( PROCESS_NAME ) ; logger . debug ( "Process configuration request received" ) ; compareConfiguration ( processConfigurationResponse ) ; }
public void test() { try { results = em . searchCollection ( em . getApplicationRef ( ) , "propertymaps" , q ) ; } catch ( Exception ex ) { logger . error ( "Error while searching for propertymaps." , ex ) ; return false ; } }
public void test() { try { em . update ( propsEntity ) ; } catch ( Exception ex ) { logger . error ( "Error updating propsEntity" , ex ) ; return false ; } }
public void test() { try { int cuboidCount = CuboidCLI . simulateCuboidGeneration ( createdDesc ) ; logger . info ( "Simulating cuboid " + createdDesc . getName ( ) ) ; } catch ( Exception e ) { getCubeDescManager ( ) . removeCubeDesc ( createdDesc ) ; throw new InternalErrorException ( "Failed to deal with the request." , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( entities . getCount ( ) > 0 ) { LOGGER . debug ( "{} to go." , entities . getCount ( ) ) ; } else { more = false ; } }
public int assertStatusCode ( Response res , String testName ) { int statusCode = res . getStatus ( ) ; logger . debug ( testName + " status = " + statusCode ) ; Assert . assertTrue ( testRequestType . isValidStatusCode ( statusCode ) , invalidStatusCodeMessage ( testRequestType , statusCode ) ) ; Assert . assertEquals ( statusCode , testExpectedStatusCode ) ; return statusCode ; }
public void test() { try { final Object response = externalContext . getResponse ( ) ; final Object request = externalContext . getRequest ( ) ; code_block = IfStatement ; } catch ( Exception e ) { log . error ( e ) ; } }
@ Override public void callCrawlerService ( ) { log . debug ( "Call to crawl site {}" , getAudit ( ) ) ; getCrawlerService ( ) . crawlSite ( getAudit ( ) , getUrl ( ) ) ; }
public void test() { try { java . util . List < com . liferay . calendar . model . CalendarResource > returnValue = CalendarResourceServiceUtil . search ( companyId , groupIds , classNameIds , code , name , description , active , andOperator , start , end , orderByComparator ) ; return com . liferay . calendar . model . CalendarResourceSoap . toSoapModels ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
@ Override public void encode ( Object value , OutputStream outputStream ) throws IOException { code_block = IfStatement ; IndexedRecord ir = converter . convertToAvro ( ( T ) value ) ; LOG . debug ( "Value is {}" , ir ) ; code_block = IfStatement ; LOG . debug ( "Encode value is {}" , value ) ; internalAvroCoder . encode ( convertToAvro ( value ) , outputStream ) ; }
@ Override public void encode ( Object value , OutputStream outputStream ) throws IOException { code_block = IfStatement ; IndexedRecord ir = converter . convertToAvro ( ( T ) value ) ; code_block = IfStatement ; LOG . debug ( "Internal AvroCoder's metadata is {}" , internalAvroCoder . getSchema ( ) ) ; LOG . debug ( "Internal AvroCoder's schema is {}" , value ) ; internalAvroCoder . encode ( convertToAvro ( value ) , outputStream ) ; }
public void test() { try { sql . append ( "UPDATE processing SET status = " ) ; sql . append ( "'" ) . append ( status . name ( ) ) . append ( "'" ) ; sql . append ( ", update_tstmp='" ) . append ( new Timestamp ( System . currentTimeMillis ( ) ) ) . append ( "' " ) ; sql . append ( " WHERE processing_id = " ) . append ( processingID ) ; executeUpdate ( sql . toString ( ) ) ; } catch ( SQLException e ) { logger . error ( "Could not execute one of the SQL commands: " + sql . toString ( ) ) ; return new ReturnValue ( null , "Could not execute one of the SQL commands: " + sql . toString ( ) + "\nException: " + e . getMessage ( ) , ReturnValue . SQLQUERYFAILED ) ; } }
public void test() { if ( this . metadataRecorders . isEmpty ( ) ) { LOGGER . debug ( "No metadata recorder configured for event: {}" , event ) ; } else { this . metadataRecorders . forEach ( r -> r . initializationStatusChanged ( event . getFeedId ( ) , event . getStatus ( ) ) ) ; } }
public void test() { if ( logToStdErr ) { System . err . println ( "Unexpected file visiting failure: " + path ) ; e . printStackTrace ( ) ; } else { log . warn ( "Unexpected file visiting failure: " + path ) ; } }
@ Override public void doConfigure ( ServiceProfile < ? > profile ) throws InterruptedException , IOException { LOG . debug ( "Configuring file sessions..." ) ; directory = prepareDirectory ( profile ) ; LOG . debug ( "Configured file sessions: {}" , directory ) ; }
@ Override public void doConfigure ( ServiceProfile < ? > profile ) throws InterruptedException , IOException { LOG . debug ( "Configuring file sessions: {}" , profile . getPrefix ( ) ) ; directory = prepareDirectory ( profile ) ; LOG . debug ( "File sessions directory: {}" , directory ) ; }
public void test() { try { ois = new ObjectInputStream ( fs . open ( modelPath ) ) ; MLModel model = ( MLModel ) ois . readObject ( ) ; log . info ( "model: " + modelPath ) ; return model ; } catch ( ClassNotFoundException e ) { throw new IOException ( e ) ; } finally { IOUtils . closeQuietly ( ois ) ; } }
public List < ViewResult . Row > getDBViewQueryResult ( String id , String docEntityType ) { logger . debug ( MessageFormat . format ( "Getting view for entityType: {0}, with id: {1}" , docEntityType , id ) ) ; List < ViewResult . Row > rows = db . queryView ( new ViewQuery ( ) . viewName ( CASE_ID_VIEW_NAME ) . designDocId ( "_design/" + docEntityType ) . key ( id ) . queryParam ( ID_FIELD_ON_ENTITY , id ) . includeDocs ( true ) ) . getRows ( ) ; logger . debug ( MessageFormat . format ( "Found these rows for entityType: {0}, with id: {1}, rows: {2}" , docEntityType , id , rows ) ) ; return rows ; }
public List < ViewResult . Row > getDBViewQueryResult ( String id , String docEntityType ) { logger . info ( MessageFormat . format ( "Trying to load entityType: {0}, with id: {1}" , docEntityType , id ) ) ; List < ViewResult . Row > rows = db . queryView ( new ViewQuery ( ) . viewName ( CASE_ID_VIEW_NAME ) . designDocId ( "_design/" + docEntityType ) . key ( id ) . queryParam ( ID_FIELD_ON_ENTITY , id ) . includeDocs ( true ) ) . getRows ( ) ; logger . info ( MessageFormat . format ( "Found {} rows in database" , rows . size ( ) ) ) ; return rows ; }
public void test() { try { String [ ] cleanupTypeString = statusMessage . split ( "\\s+" ) ; code_block = IfStatement ; } catch ( Throwable t ) { logger . error ( t . getMessage ( ) , t ) ; } }
public void test() { switch ( method ) { case DELETE : return Role . DELETE ; case GET : code_block = IfStatement ; return Role . READ ; case HEAD : return Role . NONE ; case PATCH : return Role . UPDATE ; case POST : return Role . CREATE ; case PUT : return Role . UPDATE ; case OPTIONS : return Role . NONE ; default : LOGGER . error ( "Unknown role '" + method + "'" ) ; return Role . ERROR ; } }
public void test() { try { deploymentContext = new AdapterDeploymentContext ( Utils . resolveDeployment ( coreSettings ) ) ; } catch ( RuntimeException exc ) { log . error ( "Exception initialising keycloak." , exc ) ; throw new IllegalArgumentException ( "Exception initialising keycloak." , exc ) ; } }
public void test() { try { method = testClass . getMethod ( methodName , ( Class < ? > [ ] ) null ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; return ; } }
public void test() { if ( loggingInitialized . get ( ) ) { LOG . info ( message ) ; } else { warnings . add ( message ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( LOG . isInfoEnabled ( ) ) { LOG . info ( String . format ( "Call to '%s' on file '%s'" , uri . toString ( ) , file ) ) ; } }
public void test() { if ( LOG . isInfoEnabled ( ) ) { LOG . info ( String . format ( "Call to '%s' on file '%s'" , uri . toString ( ) , file ) ) ; } }
public void test() { try { process ( ) ; } catch ( Throwable ex ) { LOGGER . error ( "process error" , ex ) ; } finally { this . isActive . set ( false ) ; } }
public void test() { if ( success ) { code_block = TryStatement ;  } }
@ Test public void TestCreateDupGenericVnfFailure_1002 ( ) { new MockAAIGenericVnfSearch ( wireMockServer ) ; MockAAICreateGenericVnf ( wireMockServer ) ; MockAAIVfModulePUT ( wireMockServer , true ) ; Map < String , Object > variables = new HashMap < > ( ) ; variables . put ( "mso-request-id" , UUID . randomUUID ( ) . toString ( ) ) ; variables . put ( "isDebugLogEnabled" , "true" ) ; variables . put ( "isVidRequest" , "false" ) ; variables . put ( "vnfName" , "STMTN5MMSC21" ) ; variables . put ( "serviceId" , "00000000-0000-0000-000000000000" ) ; variables . put ( "personaModelId" , "973ed047-d251-4fb9-bf1a-65b8949e0a73" ) ; variables . put ( "personaModelVersion" , "1.0" ) ; variables . put ( "vfModuleName" , "STMTN5MMSC21-MMSC::module-0-0" ) ; variables . put ( "vfModuleModelName" , "MMSC::module-0" ) ; String processId = invokeSubProcess ( "CreateAAIVfModule" , variables ) ; WorkflowException exception = BPMNUtil . getRawVariable ( processEngine , "CreateAAIVfModule" , "WorkflowException" , processId ) ; logger . debug ( exception ) ; Assert . assertEquals ( 1002 , exception . getErrorCode ( ) ) ; Assert . assertEquals ( true , exception . getErrorMessage ( ) . contains ( "Invalid request for new Generic VNF which already exists" ) ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
@ Override public void publishApi ( Api api , IAsyncResultHandler < Void > handler ) { super . publishApi ( api , handler ) ; logger . debug ( "Successfully published api: " + api . getName ( ) ) ; proxy . publishApi ( api ) ; }
public void test() { if ( listenerContainer != null ) { startListenerContainer ( ) ; } else { LOG . warn ( "ListenerContainer is null. {}" , this . getClass ( ) . getSimpleName ( ) ) ; } }
public void test() { try { final String exceptionMessage = Utils . getCauseString ( cause ) ; List < ServiceManagementListener > serviceListeners = getManagementListeners ( ) ; code_block = ForStatement ; markChanged ( ) ; } catch ( Exception ex ) { logger . warn ( "Error during removal of " + listener , ex ) ; } }
protected MsoException runtimeExceptionToMsoException ( RuntimeException e , String context ) { log . error ( "RuntimeException: " , e ) ; MsoAdapterException me = new MsoAdapterException ( e . getMessage ( ) , e ) ; me . addContext ( context ) ; me . setCategory ( MsoExceptionCategory . INTERNAL ) ; return me ; }
public void test() { try { service . init ( ) ; } catch ( Throwable t ) { LOG . error ( "Failed to init service {}" , service , t ) ; throw new FalconException ( t ) ; } }
public void test() { try { calendar = ( T ) contentProcessor . getIntermediateCalendar ( interval , stream ) ; } catch ( CalendarException e ) { logger . error ( "Error fetching intermediate calendar" , e ) ; throw e ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { Device di = getDevice ( cMessage . getSerialNumber ( ) ) ; code_block = IfStatement ; } catch ( NullPointerException e ) { logger . debug ( "An exception occurred while calling the DeviceStatusListener" , e ) ; } catch ( Exception e ) { logger . error ( "An exception occurred while calling the DeviceStatusListener" , e ) ; unregisterDeviceStatusListener ( deviceStatusListener ) ; } }
public void test() { try { Device di = getDevice ( cMessage . getSerialNumber ( ) ) ; code_block = IfStatement ; } catch ( NullPointerException e ) { logger . debug ( "Unexpected NPE cought. Please report stacktrace" , e ) ; } catch ( Exception e ) { logger . error ( "Unknown error" , e ) ; unregisterDeviceStatusListener ( deviceStatusListener ) ; } }
@ Override public IRODSFile getTrashHomeForLoggedInUser ( ) throws JargonException { log . info ( "getTrashHomeForLoggedInUser()" ) ; log . info ( "for user:{}" , getIRODSAccount ( ) ) ; String trashHomePath = MiscIRODSUtils . buildTrashHome ( getIRODSAccount ( ) . getUserName ( ) , getIRODSAccount ( ) . getZone ( ) ) ; log . info ( "getting file at:{}" , trashHomePath ) ; IRODSFile trashFile = getIRODSAccessObjectFactory ( ) . getIRODSFileFactory ( getIRODSAccount ( ) ) . instanceIRODSFile ( trashHomePath ) ; return trashFile ; }
@ Override public IRODSFile getTrashHomeForLoggedInUser ( ) throws JargonException { log . info ( "getTrashHomeForLoggedInUser())" ) ; String trashHomePath = MiscIRODSUtils . buildTrashHome ( getIRODSAccount ( ) . getUserName ( ) , getIRODSAccount ( ) . getZone ( ) ) ; log . info ( "getting file at:{}" , trashHomePath ) ; IRODSFile trashFile = getIRODSAccessObjectFactory ( ) . getIRODSFileFactory ( getIRODSAccount ( ) ) . instanceIRODSFile ( trashHomePath ) ; log . info ( "return trashFile:{}" , trashFile ) ; return trashFile ; }
@ Override public IRODSFile getTrashHomeForLoggedInUser ( ) throws JargonException { log . info ( "getTrashHomeForLoggedInUser())" ) ; log . info ( "for user:{}" , getIRODSAccount ( ) ) ; String trashHomePath = MiscIRODSUtils . buildTrashHome ( getIRODSAccount ( ) . getUserName ( ) , getIRODSAccount ( ) . getZone ( ) ) ; log . info ( "moveHomePath:{}" , trashHomePath ) ; IRODSFile trashFile = getIRODSAccessObjectFactory ( ) . getIRODSFileFactory ( getIRODSAccount ( ) ) . instanceIRODSFile ( trashHomePath ) ; return trashFile ; }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getCapabilities()" ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getCapabilities()" ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getCapabilities()" ) ; } }
public void test() { try { MidPointApplication application = ( MidPointApplication ) MidPointApplication . get ( ) ; return application . getAuditService ( ) . reconstructObject ( type , oid , eventIdentifier , task , result ) ; } catch ( Exception ex ) { LOG . error ( ex . getMessage ( ) , ex ) ; } }
public void test() { try { return Pattern . compile ( nonProxyHosts ) ; } catch ( Exception e ) { logger . error ( "Bad proxy host [" + nonProxyHosts + "]" ) ; return null ; } }
@ Override public void call ( final Object ... args ) { handleError ( Socket . EVENT_DISCONNECT , args ) ; isConnected = false ; logger . debug ( "Disconnected" ) ; }
public void test() { try { listener . expired ( e . getKey ( ) , null ) ; } catch ( Exception ex ) { log . warn ( "Exception invoking listener " + listener . getKey ( ) , ex ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { LoadTestDataSimpleResponseMessageType response = ( LoadTestDataSimpleResponseMessageType ) invokeClientPort ( AdminWSConstants . ADMIN_LTD_SAVEADDRESS , request ) ; logDebug ( AdminWSConstants . ADMIN_LTD_SAVEADDRESS , response . isStatus ( ) , response . getMessage ( ) ) ; return response . isStatus ( ) ; } catch ( Exception e ) { LOG . error ( "error during save save address" , e ) ; } }
public void test() { try { com . liferay . commerce . inventory . model . CommerceInventoryWarehouseItem returnValue = CommerceInventoryWarehouseItemServiceUtil . addCommerceInventoryWarehouseItem ( userId , commerceInventoryWarehouseId , sku , quantity ) ; return com . liferay . commerce . inventory . model . CommerceInventoryWarehouseItemSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try { MDCSetup mdcSetup = new MDCSetup ( ) ; Exception ex = message . getContent ( Exception . class ) ; code_block = IfStatement ; mdcSetup . setLogTimestamp ( ) ; mdcSetup . setElapsedTime ( ) ; logger . info ( ONAPLogConstants . Markers . EXIT , "Exiting" ) ; } catch ( Exception e ) { logger . error ( ONAPLogConstants . Markers . EXIT , "Error" , e ) ; } }
@ Override public void onActivityTestGPRSRequest ( ActivityTestGPRSRequest ind ) { this . logger . debug ( "onActivityTestGPRSRequest" ) ; TestEvent te = TestEvent . createReceivedEvent ( EventType . ActivityTestGPRSRequest , ind , sequence ++ ) ; this . observerdEvents . add ( te ) ; }
public void test() { for ( int i = 0 ; i < 3 ; i ++ ) { DataPacket packet = new DataPacketBuilder ( ) . contents ( "Example contents from client." ) . attr ( "Client attr 1" , "Client attr 1 value" ) . attr ( "Client attr 2" , "Client attr 2 value" ) . build ( ) ; transaction . send ( packet ) ; long written = ( ( Peer ) transaction . getCommunicant ( ) ) . getCommunicationsSession ( ) . getBytesWritten ( ) ; LOG . debug ( "Response: {}" , written ) ; Thread . sleep ( 50 ) ; } }
public List < UUID > testEntityCollections ( UUID applicationId , UUID entityId , String entityType , String collectionName , int expectedCount ) throws Exception { logger . info ( "----------------------------------------------------" ) ; EntityManager em = setup . getEmf ( ) . getEntityManager ( applicationId ) ; Entity en = em . get ( new SimpleEntityRef ( entityType , entityId ) ) ; int i = 0 ; Results entities = em . getCollection ( en , collectionName , null , 100 , Level . IDS , false ) ; logger . info ( "----------------------------------------------------" ) ; code_block = ForStatement ; logger . info ( "----------------------------------------------------" ) ; assertEquals ( "Expected " + expectedCount + " connections" , expectedCount , entities . getIds ( ) != null ? entities . getIds ( ) . size ( ) : 0 ) ; return entities . getIds ( ) ; }
public void test() { for ( UUID id : entities . getIds ( ) ) { logger . debug ( "UPDATED: " + id ) ; } }
public void test() { if ( retryTimer . isPresent ( ) ) { LOG . warn ( "Snapshot restore timed out, failed to restore snapshot for %s, snapshot %s" , queryId . getId ( ) , lastTriedId . toString ( ) ) ; retryTimer = Optional . empty ( ) ; } else { return ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public static void deleteKeycloak ( String namespace ) { LOGGER . info ( "Deleting Keycloak" ) ; Exec . exec ( true , "/bin/bash" , PATH_TO_KEYCLOAK_TEARDOWN_SCRIPT , namespace ) ; }
public void test() { try { final Connection conn = citrixResourceBase . getConnection ( ) ; final Network nw = citrixResourceBase . findOrCreateTunnelNetwork ( conn , command . getBridgeName ( ) ) ; citrixResourceBase . cleanUpTmpDomVif ( conn , nw ) ; citrixResourceBase . destroyTunnelNetwork ( conn , nw , command . getHostId ( ) ) ; s_logger . debug ( "destroyed tunnel network" ) ; return new Answer ( command , true , null ) ; } catch ( final Exception e ) { s_logger . warn ( "caught execption when destroying ovs bridge" , e ) ; return new Answer ( command , false , e . getMessage ( ) ) ; } }
public void test() { try { final Connection conn = citrixResourceBase . getConnection ( ) ; final Network nw = citrixResourceBase . findOrCreateTunnelNetwork ( conn , command . getBridgeName ( ) ) ; citrixResourceBase . cleanUpTmpDomVif ( conn , nw ) ; citrixResourceBase . destroyTunnelNetwork ( conn , nw , command . getHostId ( ) ) ; s_logger . debug ( "OVS Bridge destroyed" ) ; return new Answer ( command , true , null ) ; } catch ( final Exception e ) { s_logger . error ( "Failed to destroyOVS Bridge" , e ) ; return new Answer ( command , false , e . getMessage ( ) ) ; } }
public void test() { if ( taskTracker == null ) { log . warn ( "TaskTracker not found." ) ; return ; } }
public void test() { if ( optionalCtx . isPresent ( ) && optionalCtx . get ( ) instanceof ZooKeeper ) { this . ledgersRootPath = conf . getZkLedgersRootPath ( ) ; this . zk = ( ZooKeeper ) ( optionalCtx . get ( ) ) ; log . info ( "Closing zookeeper metadata driver." ) ; this . ownZKHandle = false ; } else { final String metadataServiceUriStr ; code_block = TryStatement ;  URI metadataServiceUri = URI . create ( metadataServiceUriStr ) ; this . ledgersRootPath = metadataServiceUri . getPath ( ) ; final String bookieRegistrationPath = ledgersRootPath + "/" + AVAILABLE_NODE ; final String bookieReadonlyRegistrationPath = bookieRegistrationPath + "/" + READONLY ; final String zkServers ; code_block = TryStatement ;  log . info ( "Initialize zookeeper metadata driver at metadata service uri {} :" + " zkServers = {}, ledgersRootPath = {}." , metadataServiceUriStr , zkServers , ledgersRootPath ) ; code_block = TryStatement ;  this . ownZKHandle = true ; } }
public void test() { if ( optionalCtx . isPresent ( ) && optionalCtx . get ( ) instanceof ZooKeeper ) { this . ledgersRootPath = conf . getZkLedgersRootPath ( ) ; log . info ( "Initialize zookeeper metadata driver with external zookeeper client : ledgersRootPath = {}." , ledgersRootPath ) ; this . zk = ( ZooKeeper ) ( optionalCtx . get ( ) ) ; this . ownZKHandle = false ; } else { final String metadataServiceUriStr ; code_block = TryStatement ;  URI metadataServiceUri = URI . create ( metadataServiceUriStr ) ; this . ledgersRootPath = metadataServiceUri . getPath ( ) ; final String bookieRegistrationPath = ledgersRootPath + "/" + AVAILABLE_NODE ; final String bookieReadonlyRegistrationPath = bookieRegistrationPath + "/" + READONLY ; final String zkServers ; code_block = TryStatement ;  code_block = TryStatement ;  log . info ( "Initialize zookeeper metadata driver." ) ; this . ownZKHandle = true ; } }
private void jsonWriteTo ( OutputStream out ) throws IOException { log . debug ( "Starting Producing Stream Data Thread ..." ) ; code_block = IfStatement ; log . debug ( "Ending Producing Stream Data Thread ..." ) ; }
public SysImport merge ( SysImport detachedInstance ) { log . debug ( "merging SysImport instance" ) ; code_block = TryStatement ;  }
public void test() { try { SysImport result = ( SysImport ) sessionFactory . getCurrentSession ( ) . merge ( detachedInstance ) ; log . debug ( "merge successful" ) ; return result ; } catch ( RuntimeException re ) { log . error ( "merge failed" , re ) ; throw re ; } }
public void test() { try { SysImport result = ( SysImport ) sessionFactory . getCurrentSession ( ) . merge ( detachedInstance ) ; log . debug ( "merge successful" ) ; return result ; } catch ( RuntimeException re ) { log . error ( "merge failed" , re ) ; throw re ; } }
public void test() { try { generateJobs ( new Date ( ) ) ; } catch ( Exception e ) { logger . error ( e ) ; } }
public void test() { if ( cmdLine . hasOption ( "o" ) ) { String outputPath = cmdLine . getOptionValue ( "o" ) ; outputFile = new File ( outputPath ) ; Threat_instrumentor . instrument ( verdictDataModel , cmdLine ) ; logger . info ( "Output file: {}" , outputFile ) ; VdmTranslator . marshalToXml ( verdictDataModel , outputFile ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( isCommit ) { String [ ] listHeuristicCommittedTransactions = jmxServer . listHeuristicCommittedTransactions ( ) ; Assert . assertEquals ( 1 , listHeuristicCommittedTransactions . length ) ; instanceLog . debug ( listHeuristicCommittedTransactions [ 0 ] ) ; } else { String [ ] listHeuristicRolledBackTransactions = jmxServer . listHeuristicRolledBackTransactions ( ) ; Assert . assertEquals ( 1 , listHeuristicRolledBackTransactions . length ) ; instanceLog . debug ( listHeuristicRolledBackTransactions [ 0 ] ) ; } }
public void test() { if ( isCommit ) { String [ ] listHeuristicCommittedTransactions = jmxServer . listHeuristicCommittedTransactions ( ) ; Assert . assertEquals ( 1 , listHeuristicCommittedTransactions . length ) ; instanceLog . debug ( listHeuristicCommittedTransactions [ 0 ] ) ; } else { String [ ] listHeuristicRolledBackTransactions = jmxServer . listHeuristicRolledBackTransactions ( ) ; Assert . assertEquals ( 1 , listHeuristicRolledBackTransactions . length ) ; instanceLog . debug ( "Rolled back transaction [0 ) ; } }
public void test() { try { workspace . delete ( url ) ; } catch ( Exception e ) { LOG . warn ( "Could not delete temporary file {}: {}" , url , e . getMessage ( ) ) ; } }
private void dumpAvailableConsumers ( ) { Map < String , KnownRepositoryContentConsumer > availableConsumers = getConsumers ( ) ; log . info ( "Available consumers:" ) ; code_block = ForStatement ; }
public void test() { for ( Map . Entry < String , KnownRepositoryContentConsumer > entry : availableConsumers . entrySet ( ) ) { String consumerHint = entry . getKey ( ) ; RepositoryContentConsumer consumer = entry . getValue ( ) ; log . debug ( "Available consumer: {}" , consumerHint ) ; } }
@ Override public void writeAndFlush ( ByteBuf output ) throws IOException { checkConnected ( output ) ; LOG . debug ( "Writing {}" , output ) ; channel . writeAndFlush ( output , channel . voidPromise ( ) ) ; }
public void test() { try { code_block = IfStatement ; } catch ( UnsupportedEncodingException e ) { logger . error ( "Transform of input error" , e ) ; } }
public void test() { try { producer . send ( new ProducerRecord < > ( topic , partitionNo , key , payloadToSend ) ) ; } catch ( Exception e ) { log . error ( "Failed to send produce message" , e ) ; } }
public void test() { if ( attachment . getCheckStatus ( ) != CheckStatus . ACCEPTED ) { LOGGER . error ( "Upload failed" , attachment . getCheckStatus ( ) ) ; return SKIP_BODY ; } }
public void test() { if ( bundlesByLocation . get ( loc ) . getState ( ) == Bundle . ACTIVE ) { bundles . add ( bundleToBundleInfo ( bundlesByLocation . get ( loc ) ) ) ; } else { LOG . debug ( "Skip bundle {}/{} since it is already active" , bundle , loc ) ; } }
public void test() { if ( Boolean . TRUE . equals ( result ) ) { LOG . debug ( "Removed node {}" , node . getNodeId ( ) . getValue ( ) ) ; } else { LOG . warn ( "Failed to remove node {}" , node . getNodeId ( ) . getValue ( ) ) ; } }
public void test() { if ( Boolean . TRUE . equals ( result ) ) { LOG . info ( "Node {} has been removed" , node . getNodeId ( ) . getValue ( ) ) ; } else { LOG . info ( "Node {} has failed to remove node {}" , node . getNodeId ( ) . getValue ( ) , node . getNodeId ( ) . getValue ( ) ) ; } }
public void test() { if ( getName ( ) . equals ( formName ) ) { return true ; } else { LOG . error ( "The name '" + formName + "' already exists. Ignoring." ) ; return false ; } }
public void test() { try { Class formClass = form . getClass ( ) ; code_block = IfStatement ; Class configClass = ClassUtils . getApplicationClass ( this . getType ( ) ) ; code_block = IfStatement ; } catch ( Exception e ) { LOG . error ( "invalid form" , e ) ; } }
public void test() { try { GenSolvablePolynomial < SolvableResidue < C > > s = pt . nextSolvablePolynomial ( ) ; p = new ResidueSolvablePolynomial < C > ( this , s ) ; } catch ( IOException e ) { logger . error ( "Unable to generate residue: " + e . getMessage ( ) ) ; p = ZERO ; } }
public void removeTenant ( String tenantId ) { logger . info ( "[removeTenant]{}" , tenantId ) ; tenantIds . remove ( tenantId ) ; }
public void test() { try { preJettyLifecycle . stop ( ) ; lifecycle . stop ( ) ; } catch ( Throwable t ) { log . error ( "Service discovery failed" , t ) ; } }
private void write ( final Collection < Long > items ) { code_block = IfStatement ; var value = state . getValue ( key ) . orElse ( "nothing" ) ; log . debug ( "Writing " + value + " to " + value ) ; }
public void test() { try ( Timer . Context timer = parseTime . time ( ) ) { message = new MappedMessage ( parser . parse ( cef . trim ( ) ) , useFullNames ) ; } catch ( Exception e ) { log . warn ( "Error parsing message" , e ) ; return null ; } }
public void test() { try { EuropeanaGeneratedIdsMap europeanaGeneratedIdsMap = europeanIdCreator . constructEuropeanaId ( record . getXmlRecord ( ) , dataset . getDatasetId ( ) ) ; return new Record ( record . getEcloudId ( ) , transformer . transform ( record . getXmlRecord ( ) . getBytes ( StandardCharsets . UTF_8 ) , europeanaGeneratedIdsMap ) . toString ( ) ) ; } catch ( TransformationException e ) { LOGGER . info ( CommonStringValues . EUROPEANA_ID_CREATOR_INITIALIZATION_FAILED , e ) ; return new Record ( record . getEcloudId ( ) , e . getMessage ( ) ) ; } catch ( EuropeanaIdException e ) { LOGGER . info ( CommonStringValues . EUROPEANA_ID_CREATOR_INITIALIZATION_FAILED , e ) ; return new Record ( record . getEcloudId ( ) , e . getMessage ( ) ) ; } }
public void test() { try { EuropeanaGeneratedIdsMap europeanaGeneratedIdsMap = europeanIdCreator . constructEuropeanaId ( record . getXmlRecord ( ) , dataset . getDatasetId ( ) ) ; return new Record ( record . getEcloudId ( ) , transformer . transform ( record . getXmlRecord ( ) . getBytes ( StandardCharsets . UTF_8 ) , europeanaGeneratedIdsMap ) . toString ( ) ) ; } catch ( TransformationException e ) { LOGGER . info ( "Record from list failed transformation" , e ) ; return new Record ( record . getEcloudId ( ) , e . getMessage ( ) ) ; } catch ( EuropeanaIdException e ) { LOGGER . info ( "Record from list failed" , e ) ; return new Record ( record . getEcloudId ( ) , e . getMessage ( ) ) ; } }
public void test() { try { application . getPrivileges ( ) . remove ( model . getObject ( ) ) ; ApplicationRestClient . update ( application ) ; SyncopeConsoleSession . get ( ) . success ( getString ( Constants . OPERATION_SUCCEEDED ) ) ; customActionOnFinishCallback ( target ) ; } catch ( SyncopeClientException e ) { LOG . error ( "While deleting {}" , model . getObject ( ) . getKey ( ) , e ) ; SyncopeConsoleSession . get ( ) . onException ( e ) ; } }
public void test() { try { final RaftClientReply reply = ClientProtoUtils . toRaftClientReply ( proto ) ; final NotLeaderException nle = reply . getNotLeaderException ( ) ; code_block = IfStatement ; final LeaderNotReadyException lnre = reply . getLeaderNotReadyException ( ) ; code_block = IfStatement ; LOG . debug ( "Failed to leader {}" , callId ) ; handleReplyFuture ( callId , f -> f . complete ( reply ) ) ; } catch ( Exception e ) { handleReplyFuture ( callId , f -> f . completeExceptionally ( e ) ) ; } }
@ Override public List < RecommendedItem > recommend ( long userID , int howMany , IDRescorer rescorer ) throws TasteException { Preconditions . checkArgument ( howMany >= 1 , "howMany must be at least 1" ) ; log . debug ( "Recommending items for user ID '{}'" , userID ) ; long [ ] theNeighborhood = neighborhood . getUserNeighborhood ( userID ) ; code_block = IfStatement ; FastIDSet allItemIDs = getAllOtherItems ( theNeighborhood , userID ) ; TopItems . Estimator < Long > estimator = new Estimator ( userID , theNeighborhood ) ; List < RecommendedItem > topItems = TopItems . getTopItems ( howMany , allItemIDs . iterator ( ) , rescorer , estimator ) ; log . debug ( "Recommendations are: {}" , topItems ) ; return topItems ; }
@ Override public List < RecommendedItem > recommend ( long userID , int howMany , IDRescorer rescorer ) throws TasteException { Preconditions . checkArgument ( howMany >= 1 , "howMany must be at least 1" ) ; log . debug ( "Recommending items for user ID '{}'" , userID ) ; long [ ] theNeighborhood = neighborhood . getUserNeighborhood ( userID ) ; code_block = IfStatement ; FastIDSet allItemIDs = getAllOtherItems ( theNeighborhood , userID ) ; TopItems . Estimator < Long > estimator = new Estimator ( userID , theNeighborhood ) ; List < RecommendedItem > topItems = TopItems . getTopItems ( howMany , allItemIDs . iterator ( ) , rescorer , estimator ) ; log . debug ( "Recommendations are: {}" , topItems ) ; return topItems ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( leader == null ) { LOG . warn ( "No leader found for {}" , metadata . topic ( ) ) ; } else { HostAndPort leaderHost = HostAndPort . fromParts ( leader . host ( ) , leader . port ( ) ) ; SimpleConsumer leaderConsumer = consumerManager . getConsumer ( leaderHost ) ; long offset = findAllOffsets ( leaderConsumer , metadata . topic ( ) , part . partitionId ( ) ) [ 0 ] ; builder . put ( metadata . topic ( ) , offset ) ; } }
@ Override public synchronized void onLeaderElection ( ) { bulletinRepository . addBulletin ( BulletinFactory . createBulletin ( "Cluster Coordinator" , Severity . INFO . name ( ) , participantId + " has been elected the Cluster Coordinator" ) ) ; FlowController . this . heartbeatMonitor . purgeHeartbeats ( ) ; logger . info ( "LeaderElection process has been elected as {}" , participantId ) ; }
public void test() { try { String user = future . get ( 30 , java . util . concurrent . TimeUnit . SECONDS ) ; LOG . debug ( "[testGetDriveUser] user: {}" , user ) ; } catch ( ExecutionException ee ) { vr . setStatus ( Result . ERROR ) . setMessage ( messages . getMessage ( "error.testConnection.failure" , ee . getMessage ( ) ) ) ; LOG . error ( "[testGetDriveUser] Execution error: {}." , ee . getMessage ( ) ) ; } catch ( TimeoutException | InterruptedException e ) { vr . setStatus ( Result . ERROR ) . setMessage ( messages . getMessage ( "error.testConnection.timeout" ) ) ; LOG . error ( "[testGetDriveUser] Operation Timeout." ) ; } }
public void test() { try { String user = future . get ( 30 , java . util . concurrent . TimeUnit . SECONDS ) ; LOG . debug ( "[testGetDriveUser] Testing User properties: {}." , user ) ; } catch ( ExecutionException ee ) { vr . setStatus ( Result . ERROR ) . setMessage ( messages . getMessage ( "error.testConnection.failure" , ee . getMessage ( ) ) ) ; LOG . error ( "[testGetDriveUser] Operation failed." , ee ) ; } catch ( TimeoutException | InterruptedException e ) { vr . setStatus ( Result . ERROR ) . setMessage ( messages . getMessage ( "error.testConnection.timeout" ) ) ; LOG . error ( "[testGetDriveUser] Operation Timeout." ) ; } }
public void test() { try { String user = future . get ( 30 , java . util . concurrent . TimeUnit . SECONDS ) ; LOG . debug ( "[testGetDriveUser] Testing User properties: {}." , user ) ; } catch ( ExecutionException ee ) { vr . setStatus ( Result . ERROR ) . setMessage ( messages . getMessage ( "error.testConnection.failure" , ee . getMessage ( ) ) ) ; LOG . error ( "[testGetDriveUser] Execution error: {}." , ee . getMessage ( ) ) ; } catch ( TimeoutException | InterruptedException e ) { vr . setStatus ( Result . ERROR ) . setMessage ( messages . getMessage ( "error.testConnection.timeout" ) ) ; LOG . error ( "[testGetDriveUser] Unknown error: {}." , e . getMessage ( ) ) ; } }
public void test() { try { LoginContext context = new SecondaryLoginContext ( ) ; ( ( WaspSession ) Session . get ( ) ) . login ( context ) ; continueToOriginalDestination ( ) ; setResponsePage ( Application . get ( ) . getHomePage ( ) ) ; return true ; } catch ( LoginException e ) { LOGGER . warn ( "Fail to login" , e ) ; } }
@ Override public void showTree ( String prefix ) { LOGGER . debug ( prefix + "Argument 0: DocIdSet - " ) ; _docIdSetPlanNode . showTree ( prefix + "    " ) ; LOGGER . debug ( prefix + "Argument 1: DocIdSet - " ) ; int i = 0 ; code_block = ForStatement ; }
@ Override public void showTree ( String prefix ) { LOGGER . debug ( prefix + "Operator: MProjectionOperator" ) ; LOGGER . debug ( prefix + "     " ) ; _docIdSetPlanNode . showTree ( prefix + "    " ) ; int i = 0 ; code_block = ForStatement ; }
public void test() { if ( controller . getTable ( ) != null ) { code_block = IfStatement ; } else { LOGGER . debug ( "Table not found" ) ; } }
@ Test public void testI01RexCompat ( ) { logger . info ( "testI01RexCompat" ) ; setup ( VehicleType . ELECTRIC_REX . toString ( ) , false ) ; String content = FileReader . readFileInString ( "src/test/resources/api/vehicle/vehicle-ccm.json" ) ; VehicleAttributesContainer vac = Converter . getGson ( ) . fromJson ( content , VehicleAttributesContainer . class ) ; assertTrue ( testVehicle ( Converter . transformLegacyStatus ( vac ) , STATUS_ELECTRIC + DOORS + RANGE_HYBRID + SERVICE_AVAILABLE + CHECK_AVAILABLE + POSITION , Optional . empty ( ) ) ) ; }
public void test() { try { JsonNode jsonNode = MAPPER . readValue ( field , JsonNode . class ) ; metadata = MAPPER . convertValue ( jsonNode , Map . class ) ; } catch ( Exception ex ) { logger . error ( ex . getMessage ( ) , ex ) ; } }
public void test() { try { new SubServer ( customPortServerManager , server ) . bind ( customPortServerManager . getPort ( ) ) ; } catch ( Throwable e ) { log . error ( e . getMessage ( ) , e ) ; } }
public void test() { for ( DocumentReference document : children ) { code_block = ForStatement ; } }
public void test() { try { bpmWidgetInfo = this . getBpmWidgetInfoDAO ( ) . loadBpmWidgetInfo ( id ) ; } catch ( Throwable t ) { _logger . error ( "Error loading bpmWidgetInfo with id: {}" , id , t ) ; throw new ApsSystemException ( "Error loading bpmWidgetInfo with id: " + id , t ) ; } }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "JDBC::{}" , sql ) ; } }
public void test() { try { outputObject . put ( JsonKeys . updateType . name ( ) , "PublishPresetUpdate" ) ; outputObject . put ( JsonKeys . fileUrl . name ( ) , contextParameters . getParameterValue ( ContextParameter . JSON_PUBLISH_RELATIVE_DIR ) + jsonFileName ) ; outputObject . put ( JsonKeys . worksheetId . name ( ) , wsht . getId ( ) ) ; pw . println ( outputObject . toString ( 4 ) ) ; } catch ( JSONException e ) { logger . error ( "Error occured while generating JSON!" ) ; } }
@ Override public XAResource [ ] getXAResources ( ActivationSpec [ ] specs ) { logger . info ( "No XAResources found" ) ; return null ; }
public void test() { try { Object value = method . invoke ( obj , args ) ; code_block = IfStatement ; return ( T ) value ; } catch ( Throwable t ) { logger . error ( "Failed to invoke method {} with exception {}" , method . getName ( ) , t . getMessage ( ) , t ) ; return defaultValue ; } }
public boolean before ( Locale locale , String filename ) throws Exception { Locale . setDefault ( locale ) ; LOG . info ( "Set default locale to: " + locale ) ; LOG . info ( "Messages file: " + filename ) ; LOG . info ( "Message size: " + getExpectedNumberOfMethods ( ) ) ; InputStream is = getClass ( ) . getClassLoader ( ) . getResourceAsStream ( filename ) ; code_block = IfStatement ; properties . load ( is ) ; return getExpectedNumberOfMethods ( ) == properties . size ( ) ; }
public boolean before ( Locale locale , String filename ) throws Exception { LOG . info ( "begin" ) ; LOG . info ( "default locale: " + Locale . getDefault ( ) ) ; Locale . setDefault ( locale ) ; LOG . info ( "Messages file: " + filename ) ; InputStream is = getClass ( ) . getClassLoader ( ) . getResourceAsStream ( filename ) ; code_block = IfStatement ; properties . load ( is ) ; return getExpectedNumberOfMethods ( ) == properties . size ( ) ; }
@ Transactional ( rollbackFor = ArrowheadException . class ) public ChoreographerWorklog createWorklog ( final String message , final String exception ) { logger . debug ( "createWorklog: " + message ) ; code_block = TryStatement ;  }
public void test() { try { code_block = IfStatement ; return choreographerWorklogRepository . saveAndFlush ( new ChoreographerWorklog ( message , exception ) ) ; } catch ( InvalidParameterException ex ) { throw ex ; } catch ( final Exception ex ) { logger . debug ( ex . getMessage ( ) , ex ) ; throw new ArrowheadException ( CoreCommonConstants . DATABASE_OPERATION_EXCEPTION_MSG ) ; } }
public void test() { if ( _logger . isDebugEnabled ( ) ) { _logger . debug ( "[" + _handler . getClass ( ) . getName ( ) + "] DirectPersistencyListHandler[" + toString ( ) + "]" ) ; } }
public boolean activityTabIsSelected ( ) { log . info ( "activity tab is selected" ) ; return getDriver ( ) . findElements ( By . cssSelector ( "activity.is-active" ) ) . size ( ) > 0 ; }
public void pauseQueue ( String queueName ) throws TimeoutException { log . info ( "Pausing queue {}" , queueName ) ; doOperation ( "queue." + queueName , "pause" ) ; }
@ Override public void output ( LocalDocument document ) { logger . info ( "Output" ) ; }
public static void changeCharsetToUtf ( JdbcConnection jdbcCon ) throws DatabaseException , SQLException { Statement stmt = jdbcCon . createStatement ( ) ; String dbName = jdbcCon . getCatalog ( ) ; String sql = String . format ( "ALTER DATABASE `%s` CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci;" , dbName ) ; int result = stmt . executeUpdate ( sql ) ; LOGGER . debug ( "{}" , result ) ; }
public void test() { try { URI serverUri = new URI ( getPreferences ( ) . getString ( PreferenceConstants . VNSERVER_URI ) ) ; IProxyService proxyService = getProxyService ( ) ; IProxyData [ ] proxyDataForHost = proxyService . select ( serverUri ) ; code_block = IfStatement ; } catch ( Exception t ) { logger . error ( "" , t ) ; } }
public void test() { try { int returnValue = CommerceAddressServiceUtil . getShippingCommerceAddressesCount ( companyId , className , classPK , keywords ) ; return returnValue ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( ! transaction . isOpen ( ) ) { LOG . warn ( "The transaction is not open: " + transaction ) ; } else-if ( ownTransaction ) { code_block = IfStatement ; } }
public void test() { if ( requireCommit ) { log . info ( "Commit " + artifact ) ; } }
public void test() { if ( ReservedPropertyNames . contains ( property ) ) { log . warn ( "Ignoring reserved property {}" , property ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { code_block = ForStatement ; } catch ( ConfigurationError ex ) { LOG . error ( ex . getMessage ( ) , ex ) ; } }
public void test() { try { encoder . dispose ( session ) ; } catch ( Throwable t ) { LOG . warn ( t . getMessage ( ) , t ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Failed to clean up." , e ) ; } }
@ Test public void testTraceWithNArguments ( ) { buf . setLength ( 0 ) ; final VitamUILogger logger = VitamUILoggerFactory . getInstance ( VitamUITraceLoggerTest . class ) ; final String message = "message" ; final String format = message + " {} {} {}" ; final Integer object1 = 1 ; final Integer object2 = 2 ; final Integer object3 = 3 ; logger . warn ( format ) ; assertTrue ( "Log message should be written." , buf . length ( ) > 0 ) ; assertTrue ( "Log message should be written." , buf . lastIndexOf ( message ) > 0 ) ; assertTrue ( "Log message should be written." , buf . lastIndexOf ( message + " " + object1 . toString ( ) + " " + object2 . toString ( ) + " " + object3 . toString ( ) ) > 0 ) ; }
@ Override public void channelConnected ( ChannelHandlerContext ctx , ChannelStateEvent e ) throws Exception { log . info ( "Connection established" ) ; }
public void test() { try { ( ( XulWindow ) this . getXulDomContainer ( ) . getDocumentRoot ( ) . getRootElement ( ) ) . cut ( ) ; paste . setDisabled ( false ) ; } catch ( XulException e ) { LOGGER . error ( "" , e ) ; } }
public void test() { if ( bucketName != null ) { _log . info ( "servlet context provided bucketName=" + bucketName ) ; setBucketName ( bucketName ) ; } else { _log . info ( "servlet context missing bucketName, using " + getBucketName ( ) ) ; } }
public void test() { if ( bucketName != null ) { _log . info ( "servlet context provided config bucketName=" + bucketName ) ; setBucketName ( bucketName ) ; } else { _log . info ( "servlet context missing config bucketName, using " + getBucketName ( ) ) ; } }
public void test() { if ( ! warnings . isEmpty ( ) ) { log . warn ( "\n{}" , warnings . toString ( ) ) ; } }
public void test() { try { groupsNames = super . searchId ( filters ) ; } catch ( Throwable t ) { logger . error ( "error in search groups" , t ) ; throw new RuntimeException ( "error in search groups" , t ) ; } }
void authenticationError ( ChannelHandlerContext ctx , int errorCode ) { log . warn ( "Authentication error" , errorCode ) ; ctx . fireExceptionCaught ( new AuthenticationException ( "Auth failed with error " + errorCode ) ) ; }
public void test() { try ( HistogramMetrics . Timer ignored = MESH_ANALYSIS_METRICS . createTimer ( ) ) { code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; doDispatch ( data ) ; } catch ( Exception e ) { LOGGER . warn ( "Exception in processing metrics" , e ) ; MESH_ERROR_METRICS . inc ( ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CalendarBookingServiceUtil . class , "getCalendarBookingsRSS" , _getCalendarBookingsRSSParameterTypes14 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , calendarId , startTime , endTime , max , type , version , displayStyle , themeDisplay ) ; Object returnObj = null ; code_block = TryStatement ;  return ( String ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( ( entry == null ) || ( entry . getValidity ( ) == null ) || ( ( entry . getValidity ( ) . isValid ( this . grammarSource . getValidity ( ) ) ) <= 0 ) ) { this . logger . info ( "(Re)building the automaton from '" + this . grammarSource . getURI ( ) + "'" ) ; if ( this . grammarSource . getInputStream ( ) == null ) throw new ProcessingException ( "Source '" + this . grammarSource . getURI ( ) + "' not found" ) ; GrammarFactory factory = new GrammarFactory ( ) ; SourceUtil . toSAX ( this . manager , this . grammarSource , null , factory ) ; Grammar grammar = factory . getGrammar ( ) ; if ( grammar == null ) throw new ProcessingException ( "Error while reading the grammar from " + src ) ; ParserAutomatonBuilder builder = new ParserAutomatonBuilder ( grammar ) ; ParserAutomaton automaton = builder . getParserAutomaton ( ) ; setParserAutomaton ( builder . getParserAutomaton ( ) ) ; this . logger . info ( "Store automaton into store to '" + this . grammarSource . getURI ( ) + "'" ) ; store . store ( this . grammarSource . getURI ( ) , new ParserAutomatonEntry ( automaton , this . grammarSource . getValidity ( ) ) ) ; } else { this . logger . info ( "Getting automaton from store to '" + this . grammarSource . getURI ( ) + "'" ) ; setParserAutomaton ( entry . getParserAutomaton ( ) ) ; } }
public void test() { if ( ( entry == null ) || ( entry . getValidity ( ) == null ) || ( ( entry . getValidity ( ) . isValid ( this . grammarSource . getValidity ( ) ) ) <= 0 ) ) { this . logger . info ( "(Re)building the automaton from '" + this . grammarSource . getURI ( ) + "'" ) ; if ( this . grammarSource . getInputStream ( ) == null ) throw new ProcessingException ( "Source '" + this . grammarSource . getURI ( ) + "' not found" ) ; GrammarFactory factory = new GrammarFactory ( ) ; SourceUtil . toSAX ( this . manager , this . grammarSource , null , factory ) ; Grammar grammar = factory . getGrammar ( ) ; if ( grammar == null ) throw new ProcessingException ( "Error while reading the grammar from " + src ) ; ParserAutomatonBuilder builder = new ParserAutomatonBuilder ( grammar ) ; ParserAutomaton automaton = builder . getParserAutomaton ( ) ; setParserAutomaton ( builder . getParserAutomaton ( ) ) ; store . store ( this . grammarSource . getURI ( ) , new ParserAutomatonEntry ( automaton , this . grammarSource . getValidity ( ) ) ) ; this . logger . info ( "Storing automaton from store to '" + this . grammarSource . getURI ( ) + "'" ) ; } else { this . logger . info ( "Getting automaton from store to '" + this . grammarSource . getURI ( ) + "'" ) ; setParserAutomaton ( entry . getParserAutomaton ( ) ) ; } }
public void test() { if ( ( entry == null ) || ( entry . getValidity ( ) == null ) || ( ( entry . getValidity ( ) . isValid ( this . grammarSource . getValidity ( ) ) ) <= 0 ) ) { this . logger . info ( "(Re)building the automaton from '" + this . grammarSource . getURI ( ) + "'" ) ; if ( this . grammarSource . getInputStream ( ) == null ) throw new ProcessingException ( "Source '" + this . grammarSource . getURI ( ) + "' not found" ) ; GrammarFactory factory = new GrammarFactory ( ) ; SourceUtil . toSAX ( this . manager , this . grammarSource , null , factory ) ; Grammar grammar = factory . getGrammar ( ) ; if ( grammar == null ) throw new ProcessingException ( "Error while reading the grammar from " + src ) ; ParserAutomatonBuilder builder = new ParserAutomatonBuilder ( grammar ) ; ParserAutomaton automaton = builder . getParserAutomaton ( ) ; setParserAutomaton ( builder . getParserAutomaton ( ) ) ; this . logger . info ( "Store automaton into store to '" + this . grammarSource . getURI ( ) + "'" ) ; store . store ( this . grammarSource . getURI ( ) , new ParserAutomatonEntry ( automaton , this . grammarSource . getValidity ( ) ) ) ; } else { this . logger . info ( "Storing automaton into store to '" + this . grammarSource . getURI ( ) + "'" ) ; setParserAutomaton ( entry . getParserAutomaton ( ) ) ; } }
public void test() { { Thread . currentThread ( ) . setName ( "restart-thread" ) ; U . sleep ( 15_000 ) ; ThreadLocalRandom tlr = ThreadLocalRandom . current ( ) ; int idx = tlr . nextInt ( 1 , GRIDS_COUNT ) ; log . info ( "Stopping node " + idx ) ; stopGrid ( idx ) ; IgniteEx ig0 = grid ( 0 ) ; ig0 . cluster ( ) . setBaselineTopology ( baselineNodes ( ig0 . cluster ( ) . forServers ( ) . nodes ( ) ) ) ; U . sleep ( 3_000 ) ; return null ; } }
private void sendMessage ( final DistributionAutomationRequestMessage requestMessage ) { LOGGER . info ( "Sending distribution automation request message" ) ; this . jmsTemplate . send ( new MessageCreator ( ) code_block = "" ; ) ; }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { listener . gotOAuthAccessToken ( token ) ; } catch ( Exception e ) { logger . warn ( "Exception at getOAuthRequestTokenAsync" , e ) ; } }
@ Override public boolean updateIndexMapping ( String indexName , final Map < String , Object > mapping ) throws IOException { indexName = formatIndexName ( indexName ) ; PutMappingRequest putMappingRequest = new PutMappingRequest ( indexName ) ; Gson gson = new Gson ( ) ; putMappingRequest . source ( gson . toJson ( mapping ) , XContentType . JSON ) ; putMappingRequest . type ( "_doc" ) ; AcknowledgedResponse response = client . indices ( ) . putMapping ( putMappingRequest , RequestOptions . DEFAULT ) ; LOGGER . info ( "Received mapping: {}" , response . isAcknowledged ( ) ) ; return response . isAcknowledged ( ) ; }
public void test() { try { apiCommands . metrics ( request ) ; return true ; } catch ( Exception e ) { logger . error ( "Error metrics received" , e ) ; } }
public void test() { try { int ver = Integer . parseInt ( part . substring ( 0 , slash ) ) ; code_block = IfStatement ; } catch ( NumberFormatException ex ) { log . error ( "Invalid verifier found: " + part ) ; } }
public void test() { try { return this . _storageAdaptor . deleteStoragePool ( this ) ; } catch ( Exception e ) { logger . warn ( "Failed to delete storage pool: " + e . getMessage ( ) , e ) ; } }
public void test() { try { Password decryptedPassword = new Password ( this . cryptoService . decryptAes ( ( ( String ) value ) . toCharArray ( ) ) ) ; decryptedPropertiesMap . put ( key , decryptedPassword ) ; } catch ( Exception e ) { logger . info ( "Password is not encrypted" ) ; decryptedPropertiesMap . put ( key , new Password ( ( String ) value ) ) ; } }
public void test() { try { InstancesResult result = WorkflowEngineFactory . getWorkflowEngine ( ) . getJobDetails ( context . getClusterName ( ) , context . getWorkflowId ( ) ) ; Date startTime = result . getInstances ( ) [ 0 ] . startTime ; Date endTime = result . getInstances ( ) [ 0 ] . endTime ; Date now = new Date ( ) ; code_block = IfStatement ; code_block = IfStatement ; context . setValue ( WorkflowExecutionArgs . WF_START_TIME , Long . toString ( startTime . getTime ( ) ) ) ; context . setValue ( WorkflowExecutionArgs . WF_END_TIME , Long . toString ( endTime . getTime ( ) ) ) ; } catch ( FalconException e ) { LOG . error ( "Exception when getting time for job details" , e ) ; } }
public void test() { try { emailStream . reset ( ) ; mailboxService . storeInSent ( udr , multipleTimesReadable ( emailStream , email . getMimeMessage ( ) . getCharset ( ) ) ) ; } catch ( CollectionNotFoundException e ) { logger . error ( "Cannot store an email" , e ) ; } catch ( Throwable t ) { logger . error ( "Cannot store an email in the Sent folder" , t ) ; } }
public void test() { try { emailStream . reset ( ) ; mailboxService . storeInSent ( udr , multipleTimesReadable ( emailStream , email . getMimeMessage ( ) . getCharset ( ) ) ) ; } catch ( CollectionNotFoundException e ) { logger . warn ( "Cannot store an email in the Sent folder, the collection was not found: {}" , e . getMessage ( ) ) ; } catch ( Throwable t ) { logger . warn ( "Cannot store an email: {}" , t . getMessage ( ) , t ) ; } }
public void test() { try { fcall . sendResponse ( fb , msg , msgType , seqid ) ; return ; } catch ( Exception ex ) { LOGGER . error ( "Exception writing to internal frame buffer" , ex ) ; } }
public void test() { if ( value instanceof String ) { return ( ( String ) value ) . length ( ) ; } else-if ( value instanceof Boolean ) { return 4 ; } else-if ( value instanceof Number || value instanceof Date ) { return 8 ; } else-if ( value instanceof Collection ) { return calculateSizeOfCollection ( ( Collection < ? > ) value ) ; } else-if ( value instanceof Map ) { return calculateSizeOfMap ( ( Map < ? , ? > ) value ) ; } else { logger . debug ( "Invalid value type for parameter {}. Ignoring" , value . getClass ( ) ) ; return 100 ; } }
public void test() { if ( hasTenant ) { LOGGER . warn ( "[capacityManagement] group content is over maxSize, group: {}, maxSize: {}, currentSize: {}" , group , maxSize , currentSize ) ; } else { LOGGER . warn ( "[capacityManagement] group content is over maxSize, group: {}, maxSize: {}, currentSize: {}" , group , maxSize , currentSize ) ; } }
public void test() { if ( hasTenant ) { LOGGER . warn ( "[capacityManagement] tenant content is over maxSize, tenant: {}, maxSize: {}, currentSize: {}" , tenant , maxSize , currentSize ) ; } else { LOGGER . warn ( "[capacityManagement] content is over maxSize, tenant: {}, maxSize: {}, currentSize: {}" , tenant , maxSize , currentSize ) ; } }
public void test() { try { attempt ++ ; LOG . info ( "Trying to reconnect to {} - attempt {}" , getEndpoint ( ) . getConnectionString ( ) , attempt ) ; session = createSession ( ) ; reconnected = true ; } catch ( IOException e ) { LOG . warn ( "Failed to reconnect to {}" , getEndpoint ( ) . getConnectionString ( ) ) ; closeSession ( ) ; code_block = TryStatement ;  } }
public void test() { try { attempt ++ ; LOG . info ( "Trying to reconnect to {} - attempt {}" , getEndpoint ( ) . getConnectionString ( ) , attempt ) ; session = createSession ( ) ; reconnected = true ; } catch ( IOException e ) { LOG . warn ( "Failed to reconnect to {}" , getEndpoint ( ) . getConnectionString ( ) ) ; closeSession ( ) ; code_block = TryStatement ;  } }
public void test() { if ( reconnected ) { LOG . info ( "Reconnected" ) ; } }
private void listen ( ) { SocketAddress sockAddr = ds . getLocalSocketAddress ( ) ; byte [ ] data = new byte [ MAX_PACKAGE_LEN ] ; code_block = TryStatement ;  Connection . LOG . info ( "Stop UDP listener on {}" , sockAddr ) ; Connection . LOG . info ( "Stop UDP listener on {}" , sockAddr ) ; }
public void test() { if ( conn . isBlackListed ( dp . getAddress ( ) ) ) { Connection . LOG . info ( "Received UDP datagram package from {}" , senderAddr ) ; } else { Connection . LOG . info ( "Received UDP datagram package from {}" , senderAddr ) ; code_block = TryStatement ;  } }
public void test() { try { handler . onReceive ( conn , dp ) ; } catch ( Throwable e ) { LOG . warn ( "onReceive exception" , e ) ; } }
private void listen ( ) { SocketAddress sockAddr = ds . getLocalSocketAddress ( ) ; Connection . LOG . info ( "Start UDP listener on {}" , sockAddr ) ; byte [ ] data = new byte [ MAX_PACKAGE_LEN ] ; code_block = TryStatement ;  Connection . LOG . info ( "Dumping UDP listener" ) ; }
public void test() { try { subscriberJedis = JedisConnectionObject . pool . getResource ( ) ; connectionSetup = true ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; subscriberJedis = null ; connectionSetup = false ; } }
public void test() { try { long groupId = ParamUtil . getLong ( actionRequest , "groupId" ) ; long classNameId = ParamUtil . getLong ( actionRequest , "classNameId" ) ; String className = _portal . getClassName ( classNameId ) ; long classPK = ParamUtil . getLong ( actionRequest , "classPK" ) ; InfoItemReference infoItemReference = new InfoItemReference ( className , classPK ) ; InfoItemObjectProvider < Object > infoItemObjectProvider = _infoItemServiceTracker . getFirstInfoItemService ( InfoItemObjectProvider . class , infoItemReference . getClassName ( ) ) ; InfoItemFieldValues infoItemFieldValues = InfoItemFieldValues . builder ( ) . infoItemReference ( infoItemReference ) . infoFieldValues ( _getInfoFieldValues ( actionRequest , className , infoItemObjectProvider . getInfoItem ( classPK ) ) ) . build ( ) ; ServiceContext serviceContext = ServiceContextFactory . getInstance ( actionRequest ) ; _translationEntryService . addOrUpdateTranslationEntry ( groupId , _getTargetLanguageId ( actionRequest ) , infoItemReference , infoItemFieldValues , serviceContext ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; SessionErrors . add ( actionRequest , exception . getClass ( ) , exception ) ; actionResponse . setRenderParameter ( "mvcRenderCommandName" , "/translation/translate" ) ; } }
public void test() { { log . trace ( "Started getAllPhasingOnlyControls" ) ; ResponseBuilder response = ResponseBuilder . startTiming ( ) ; indexBeanParam . adjustIndexes ( maxAPIFetchRecords ) ; AccountControlPhasingResponse dto = new AccountControlPhasingResponse ( ) ; dto . phasingOnlyControls = accountControlPhasingService . getAllStream ( indexBeanParam . getFirstIndex ( ) , indexBeanParam . getLastIndex ( ) ) . map ( item -> accountControlPhasingConverter . convert ( item ) ) . collect ( Collectors . toList ( ) ) ; log . trace ( "getAllPhasingOnlyControls result: {}" , dto ) ; return response . bind ( dto ) . build ( ) ; } }
public void test() { { ResponseBuilder response = ResponseBuilder . startTiming ( ) ; indexBeanParam . adjustIndexes ( maxAPIFetchRecords ) ; log . trace ( "Started getAllPhasingOnlyControls : \t indexBeanParam={}" , indexBeanParam ) ; AccountControlPhasingResponse dto = new AccountControlPhasingResponse ( ) ; dto . phasingOnlyControls = accountControlPhasingService . getAllStream ( indexBeanParam . getFirstIndex ( ) , indexBeanParam . getLastIndex ( ) ) . map ( item -> accountControlPhasingConverter . convert ( item ) ) . collect ( Collectors . toList ( ) ) ; log . trace ( "getAllPhasingOnlyControls : \t indexBeanParam={}" , dto ) ; return response . bind ( dto ) . build ( ) ; } }
@ Activate public void activate ( ) { logger . trace ( "activate()" ) ; ScriptStandaloneSetup . doSetup ( scriptServiceUtil , this ) ; }
public void test() { try { return getField ( fname , true ) ; } catch ( AmbiguousFieldException e ) { log . trace ( "IGNORED" , e ) ; return null ; } }
private void initWebKeys ( Conf conf ) { final String jwksUri = conf . getDynamic ( ) . getJwksUri ( ) ; code_block = IfStatement ; final JSONObject keys = JwtUtil . getJSONWebKeys ( jwksUri ) ; final JSONWebKeySet keySet = JSONWebKeySet . fromJSONObject ( keys ) ; jwks = new WebKeysConfiguration ( ) ; jwks . setKeys ( keySet . getKeys ( ) ) ; LOG . debug ( "KeySet: {}" , keySet ) ; }
public void test() { for ( String message : messages ) { LOG . debug ( "Received: {}" , message ) ; restClient . post ( ) . uri ( restUri ) . submit ( message ) . thenAccept ( it -> assertThat ( it . status ( ) , is ( Http . Status . NO_CONTENT_204 ) ) ) . toCompletableFuture ( ) . get ( ) ; } }
public void test() { if ( reference != null && reference instanceof QuantityWithUnit ) { return ! ( ( QuantityWithUnit ) reference ) . isSetUnits ( ) ; } else { logger . warn ( "Unable to convert value to a QuantityWithUnit. This is not a unit." ) ; return true ; } }
@ Test public void playerAutoTerminationTest ( ) throws Exception { String id = uploadFile ( new File ( "test-files/sample.txt" ) ) ; log . debug ( "Player: " + player . getName ( ) ) ; RepositoryHttpPlayer player = getRepository ( ) . findRepositoryItemById ( id ) . createRepositoryHttpPlayer ( ) ; player . setAutoTerminationTimeout ( 1000 ) ; RestTemplate template = getRestTemplate ( ) ; assertEquals ( HttpStatus . OK , template . getForEntity ( player . getURL ( ) , byte [ ] . class ) . getStatusCode ( ) ) ; log . debug ( "Request 1 Passed" ) ; Thread . sleep ( 300 ) ; assertEquals ( HttpStatus . OK , template . getForEntity ( player . getURL ( ) , byte [ ] . class ) . getStatusCode ( ) ) ; log . debug ( "Request 2 Passed" ) ; Thread . sleep ( 1500 ) ; assertEquals ( HttpStatus . NOT_FOUND , template . getForEntity ( player . getURL ( ) , byte [ ] . class ) . getStatusCode ( ) ) ; log . debug ( "Request 3 Passed" ) ; }
@ Test public void playerAutoTerminationTest ( ) throws Exception { String id = uploadFile ( new File ( "test-files/sample.txt" ) ) ; log . debug ( "File uploaded" ) ; RepositoryHttpPlayer player = getRepository ( ) . findRepositoryItemById ( id ) . createRepositoryHttpPlayer ( ) ; player . setAutoTerminationTimeout ( 1000 ) ; RestTemplate template = getRestTemplate ( ) ; assertEquals ( HttpStatus . OK , template . getForEntity ( player . getURL ( ) , byte [ ] . class ) . getStatusCode ( ) ) ; log . debug ( "Request 1 Passed" ) ; Thread . sleep ( 300 ) ; assertEquals ( HttpStatus . OK , template . getForEntity ( player . getURL ( ) , byte [ ] . class ) . getStatusCode ( ) ) ; log . debug ( "Request 2 Passed" ) ; Thread . sleep ( 1500 ) ; assertEquals ( HttpStatus . NOT_FOUND , template . getForEntity ( player . getURL ( ) , byte [ ] . class ) . getStatusCode ( ) ) ; log . debug ( "Request 3 Passed" ) ; }
@ Test public void playerAutoTerminationTest ( ) throws Exception { String id = uploadFile ( new File ( "test-files/sample.txt" ) ) ; log . debug ( "File uploaded" ) ; RepositoryHttpPlayer player = getRepository ( ) . findRepositoryItemById ( id ) . createRepositoryHttpPlayer ( ) ; player . setAutoTerminationTimeout ( 1000 ) ; RestTemplate template = getRestTemplate ( ) ; assertEquals ( HttpStatus . OK , template . getForEntity ( player . getURL ( ) , byte [ ] . class ) . getStatusCode ( ) ) ; log . debug ( "Request 1 Passed" ) ; Thread . sleep ( 300 ) ; assertEquals ( HttpStatus . OK , template . getForEntity ( player . getURL ( ) , byte [ ] . class ) . getStatusCode ( ) ) ; log . debug ( "Request 3 Passed" ) ; Thread . sleep ( 1500 ) ; assertEquals ( HttpStatus . NOT_FOUND , template . getForEntity ( player . getURL ( ) , byte [ ] . class ) . getStatusCode ( ) ) ; log . debug ( "Request 3 Passed" ) ; }
@ Test public void playerAutoTerminationTest ( ) throws Exception { String id = uploadFile ( new File ( "test-files/sample.txt" ) ) ; log . debug ( "File uploaded" ) ; RepositoryHttpPlayer player = getRepository ( ) . findRepositoryItemById ( id ) . createRepositoryHttpPlayer ( ) ; player . setAutoTerminationTimeout ( 1000 ) ; RestTemplate template = getRestTemplate ( ) ; log . debug ( "Request 1 passed" ) ; assertEquals ( HttpStatus . OK , template . getForEntity ( player . getURL ( ) , byte [ ] . class ) . getStatusCode ( ) ) ; log . debug ( "Request 1 Passed" ) ; Thread . sleep ( 300 ) ; assertEquals ( HttpStatus . OK , template . getForEntity ( player . getURL ( ) , byte [ ] . class ) . getStatusCode ( ) ) ; log . debug ( "Request 2 Passed" ) ; Thread . sleep ( 1500 ) ; assertEquals ( HttpStatus . NOT_FOUND , template . getForEntity ( player . getURL ( ) , byte [ ] . class ) . getStatusCode ( ) ) ; }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { if ( getFileInLocalRepo ( artifact . getFile ( ) ) != null ) { logger . debug ( "Removing artifact {}" , artifact . getFile ( ) ) ; it . remove ( ) ; } }
public void test() { try ( InputStream is = Files . newInputStream ( xmlFilePath ) ; InputStreamReader in = new InputStreamReader ( is , StandardCharsets . UTF_8 ) ) { parser = inFactory . createXMLStreamReader ( in ) ; code_block = WhileStatement ; ret = true ; } catch ( Exception ex ) { log . error ( "Error creating xml file" , ex ) ; ret = false ; } finally { code_block = IfStatement ; } }
public void deleteEverything ( SqlSession session ) { Collection < String > ops = new TreeSet < > ( db . getMyBatisConfiguration ( ) . getMappedStatementNames ( ) ) ; for ( String name : ops ) if ( name . startsWith ( "deletedb-" ) ) session . update ( name ) ; log . info ( "Database contents was completely deleted" ) ; for ( String name : ops ) if ( name . startsWith ( "resetIndex-" ) ) session . update ( name ) ; log . info ( "Database contents was completely deleted" ) ; createRootGroup ( session ) ; }
public void deleteEverything ( SqlSession session ) { log . info ( "Database contents will be completely deleted" ) ; Collection < String > ops = new TreeSet < > ( db . getMyBatisConfiguration ( ) . getMappedStatementNames ( ) ) ; for ( String name : ops ) if ( name . startsWith ( "deletedb-" ) ) session . update ( name ) ; for ( String name : ops ) if ( name . startsWith ( "resetIndex-" ) ) session . update ( name ) ; createRootGroup ( session ) ; log . info ( "Database contents will be deleted" ) ; }
public void test() { if ( future == null || future . isDone ( ) || future . isCancelled ( ) ) { log . info ( "Scheduling monitor[%s]" , monitorInterval ) ; future = executorService . scheduleAtFixedRate ( this , 1 , monitorInterval , TimeUnit . MILLISECONDS ) ; } else { log . info ( "Monitor already running." ) ; } }
public void test() { if ( future == null || future . isDone ( ) || future . isCancelled ( ) ) { log . info ( "Scheduling connection retries." ) ; future = executorService . scheduleAtFixedRate ( this , 1 , monitorInterval , TimeUnit . MILLISECONDS ) ; } else { log . warn ( "Retrying after {} ms." , future . isDone ( ) ) ; } }
public void test() { try { Class . forName ( IGNITE_JDBC_DRIVER_NAME ) ; } catch ( ClassNotFoundException e ) { logger . info ( IGNITE_JDBC_DRIVER_NAME + " not found. " + IGNITE_JDBC_DRIVER_NAME ) ; connEx = e ; return ; } }
public void test() { try { conn = DriverManager . getConnection ( getProperty ( IGNITE_JDBC_URL ) ) ; connEx = null ; logger . info ( "Successfully created JDBC connection" ) ; } catch ( Exception e ) { logger . error ( "Can't open connection: " , e ) ; connEx = e ; } }
public void test() { try { logger . info ( "connect to " + getProperty ( IGNITE_JDBC_URL ) ) ; conn = DriverManager . getConnection ( getProperty ( IGNITE_JDBC_URL ) ) ; logger . info ( "connecting to " + getProperty ( IGNITE_JDBC_URL ) ) ; connEx = null ; } catch ( Exception e ) { logger . error ( "Can't open connection: " , e ) ; connEx = e ; } }
public void test() { try { logger . info ( "connect to " + getProperty ( IGNITE_JDBC_URL ) ) ; conn = DriverManager . getConnection ( getProperty ( IGNITE_JDBC_URL ) ) ; connEx = null ; logger . info ( "Successfully created JDBC connection" ) ; } catch ( Exception e ) { logger . info ( "connection is not created" ) ; connEx = e ; } }
public void test() { if ( bucketName != null ) { _log . info ( "servlet context provided bucketName=" + bucketName ) ; setBucketName ( bucketName ) ; } else { _log . info ( "servlet context missing bucketName, using " + getBucketName ( ) ) ; } }
public void test() { if ( bucketName != null ) { _log . info ( "servlet context provided bucketName=" + bucketName ) ; setBucketName ( bucketName ) ; } else { _log . info ( "servlet context missing bucketName, using " + getBucketName ( ) ) ; } }
public void test() { if ( StringUtils . isBlank ( rememberMeKey ) && ! development ) { LOG . warn ( "Using a fixed 'remember me' key because we're in a secure mode, this is INSECURE." ) ; rememberMeKey = generateRememberMeKey ( ) ; } else-if ( StringUtils . isBlank ( rememberMeKey ) && development ) { LOG . warn ( "Using a fixed 'remember me' key because we're in development mode, this is INSECURE." ) ; rememberMeKey = DEVELOPMENT_REMEMBER_ME_KEY ; } else { LOG . info ( "Using a fixed 'remember me' key from system properties, this is insecure." ) ; } }
public void test() { if ( StringUtils . isBlank ( rememberMeKey ) && ! development ) { LOG . debug ( "Generating a new ephemeral 'remember me' key in a secure way." ) ; rememberMeKey = generateRememberMeKey ( ) ; } else-if ( StringUtils . isBlank ( rememberMeKey ) && development ) { LOG . debug ( "Using a fixed 'remember me' key in a secure way." ) ; rememberMeKey = DEVELOPMENT_REMEMBER_ME_KEY ; } else { LOG . info ( "Using a fixed 'remember me' key from system properties, this is insecure." ) ; } }
public void test() { if ( StringUtils . isBlank ( rememberMeKey ) && ! development ) { LOG . debug ( "Generating a new ephemeral 'remember me' key in a secure way." ) ; rememberMeKey = generateRememberMeKey ( ) ; } else-if ( StringUtils . isBlank ( rememberMeKey ) && development ) { LOG . warn ( "Using a fixed 'remember me' key because we're in development mode, this is INSECURE." ) ; rememberMeKey = DEVELOPMENT_REMEMBER_ME_KEY ; } else { LOG . warn ( "Using a fixed 'remember me' key because we're in development mode, this is disabled." ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { report ( ) ; } catch ( Exception e ) { log . error ( "Failed to report task" , e ) ; } }
public void test() { if ( ! executor . awaitTermination ( 1 , TimeUnit . SECONDS ) ) { LOG . warn ( "Executor did not terminate" ) ; } }
@ Deprecated public String getFinancialYearId ( String estDate ) { String result = "" ; Query query = getCurrentSession ( ) . createQuery ( "select cfinancialyear.id from CFinancialYear cfinancialyear where cfinancialyear.startingDate <= to_date('" + estDate + "','dd/MM/yyyy') and cfinancialyear.endingDate >= to_date('" + estDate + "','dd/MM/yyyy') " ) ; ArrayList list = ( ArrayList ) query . list ( ) ; if ( list . size ( ) > 0 ) result = list . get ( 0 ) . toString ( ) ; logger . debug ( "Financial year id: " + result . toString ( ) ) ; return result ; }
public void test() { if ( numCompletedSteps >= numSteps ) { logger . info ( "Completed {} comprised of {} steps in {} millis. Index found {} hits. Read {} events from Event Files. " + "Only completed {} steps because the maximum number of results was reached." , query , numSteps , queryTime , hitCount , matchingRecords . size ( ) ) ; } else { logger . info ( "Completed {} comprised of {} steps in {} millis. Index found {} hits. Read {} events from Event Files. " + "Only completed {} steps because the maximum number of results was reached." , query , numSteps , queryTime , hitCount , matchingRecords . size ( ) , numCompletedSteps ) ; } }
public void test() { if ( numCompletedSteps >= numSteps ) { logger . info ( "Completed {} comprised of {} steps in {} millis. Index found {} hits. Read {} events from Event Files." , query , numSteps , queryTime , hitCount , matchingRecords . size ( ) ) ; } else { logger . info ( "Completed {} comprised of {} steps in {} millis. Index found {} hits. Read {} events from Event Files." , query , numSteps , queryTime , hitCount , matchingRecords . size ( ) ) ; } }
public void test() { if ( received % 10000 == 0 ) { log . info ( "received " + received ) ; } }
public void test() { try { Partitioner < BulkIngestKey , Value > partitionerForTable = cachePartitioner ( new Text ( tableName ) ) ; initializeJob ( job , partitionerForTable ) ; validTableNames . add ( tableName ) ; } catch ( Exception e ) { log . error ( "Error initializing partitioner" , e ) ; lazyInitializeDefaultPartitioner ( job ) ; } }
public void test() { if ( response . getStatusLine ( ) . getStatusCode ( ) != HttpStatus . SC_OK ) { LOG . error ( "Error connecting to OAuth server" ) ; return null ; } }
public void test() { if ( response . getStatusLine ( ) . getStatusCode ( ) != HttpStatus . SC_OK ) { LOG . error ( "Error connecting to OAuth server" ) ; return null ; } }
public void test() { try { java . util . List < com . liferay . portal . kernel . model . EmailAddress > returnValue = EmailAddressServiceUtil . getEmailAddresses ( className , classPK ) ; return com . liferay . portal . kernel . model . EmailAddressSoap . toSoapModels ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
private static void convertTieLine ( UcteNetwork ucteNetwork , MergedXnode mergedXnode , UcteExporterContext context ) { Line line = mergedXnode . getExtendable ( ) ; convertXNode ( ucteNetwork , mergedXnode , context ) ; UcteElementId ucteElementId1 = context . getNamingStrategy ( ) . getUcteElementId ( mergedXnode . getLine1Name ( ) ) ; String elementName1 = line . getProperty ( ELEMENT_NAME_PROPERTY_KEY + "_1" , null ) ; UcteElementStatus status1 = line instanceof TieLine ? getStatusHalf ( ( TieLine ) line , Branch . Side . ONE ) : getStatus ( line , Branch . Side . ONE ) ; UcteLine ucteLine1 = new UcteLine ( ucteElementId1 , status1 , ( float ) line . getR ( ) * mergedXnode . getRdp ( ) , ( float ) line . getX ( ) * mergedXnode . getXdp ( ) , ( float ) line . getB1 ( ) , ( int ) line . getCurrentLimits1 ( ) . getPermanentLimit ( ) , elementName1 ) ; ucteNetwork . addLine ( ucteLine1 ) ; UcteElementId ucteElementId2 = context . getNamingStrategy ( ) . getUcteElementId ( mergedXnode . getLine2Name ( ) ) ; String elementName2 = line . getProperty ( ELEMENT_NAME_PROPERTY_KEY + "_2" , null ) ; UcteElementStatus status2 = line instanceof TieLine ? getStatusHalf ( ( TieLine ) line , Branch . Side . TWO ) : getStatus ( line , Branch . Side . TWO ) ; UcteLine ucteLine2 = new UcteLine ( ucteElementId2 , status2 , ( float ) line . getR ( ) * ( 1.0f - mergedXnode . getRdp ( ) ) , ( float ) line . getX ( ) * ( 1.0f - mergedXnode . getXdp ( ) ) , ( float ) line . getB2 ( ) , ( int ) line . getCurrentLimits2 ( ) . add
public void test() { if ( documentRes == null ) { LOG . error ( "document res is null" ) ; return ; } }
public void test() { try { coreSession . removeDocument ( docModel . getRef ( ) ) ; log . debug ( "quote removal succeded for id: " + commentId ) ; } catch ( Exception e ) { log . error ( e . getMessage ( ) ) ; } }
public void test() { if ( docModel != null ) { code_block = TryStatement ;  } else { logger . warn ( "DocModel was null!" ) ; } }
public void test() { try { page = ( Page ) this . getPage ( this . getPageCode ( ) ) ; PageModel model = page . getMetadata ( ) . getModel ( ) ; Widget [ ] defaultWidgets = model . getDefaultWidget ( ) ; code_block = IfStatement ; Widget [ ] widgets = new Widget [ defaultWidgets . length ] ; code_block = ForStatement ; page . setWidgets ( widgets ) ; this . getPageManager ( ) . updatePage ( page ) ; } catch ( Throwable t ) { _logger . error ( "error in savePage" , t ) ; return FAILURE ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Failed to clean up." , e ) ; } }
public void test() { try { String loadedData = Files . lines ( datapath ) . collect ( Collectors . joining ( ) ) ; Document doc = Jsoup . parse ( loadedData ) ; Element table = doc . select ( "table" ) . get ( 5 ) ; Elements tableRows = table . select ( "tr" ) ; code_block = ForStatement ; result . remove ( 0 ) ; LOGGER . info ( "Loaded {} datasets." , result . size ( ) ) ; return result ; } catch ( IOException e ) { e . printStackTrace ( ) ; return result ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { try { byte [ ] rowBytes = HBaseUtils . getBytes ( rowKey ) ; Delete delete = new Delete ( rowBytes ) ; hTable . delete ( delete ) ; } catch ( IOException e ) { LOGGER . error ( "Could not perform delete" , e ) ; throw new PersistenceException ( "Could not perform delete. Caused by: " , e ) ; } }
public void test() { if ( ! elementFile . exists ( ) ) { LOG . warn ( "Element not found: " + elementFile ) ; return element ; } }
public void test() { if ( ! FileUtils . deleteQuietly ( elementFile . getParentFile ( ) ) ) { LOG . warn ( "Failed to remove parent directory [{}]" , elementFile . getParentFile ( ) . getAbsolutePath ( ) ) ; } }
public void test() { if ( mismatchedPartitioner != null ) { log . error ( mismatchedPartitioner ) ; } }
public void test() { try { listener . beforeRefreshStart ( asynchronous ) ; } catch ( Exception e ) { LOGGER . error ( e ) ; } }
public void test() { try { listener . update ( newEventArr , oldEventArr , epStatement , runtime ) ; } catch ( Throwable t ) { String message = "Unexpected exception invoking listener update method on listener class '" + listener . getClass ( ) . getSimpleName ( ) + "' : " + t . getClass ( ) . getSimpleName ( ) + " : " + t . getMessage ( ) ; _logger . error ( message , t ) ; } }
public void test() { if ( type == LutronCommandType . MODE && parameters . length > 1 && ModeCommand . ACTION_STEP . toString ( ) . equals ( parameters [ 0 ] ) ) { Long step = Long . valueOf ( parameters [ 1 ] ) ; code_block = IfStatement ; updateState ( CHANNEL_STEP , new DecimalType ( step . longValue ( ) ) ) ; } else { logger . debug ( "Received command {} for channel {}" , command , channelUID ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( NumberFormatException e ) { logger . warn ( "Unexpected value {} for {}" , value , channelUID , e ) ; } }
public static void main ( String args [ ] ) { JMeterUtils . loadJMeterProperties ( "jmeter.properties" ) ; String dataset = JMeterUtils . getPropDefault ( "jmeterPlugin.sts.datasetDirectory" , HttpSimpleTableControl . DEFAULT_DATA_DIR ) ; int port = JMeterUtils . getPropDefault ( "jmeterPlugin.sts.port" , HttpSimpleTableControl . DEFAULT_PORT ) ; boolean timestamp = JMeterUtils . getPropDefault ( "jmeterPlugin.sts.addTimestamp" , HttpSimpleTableControl . DEFAULT_TIMESTAMP ) ; Configurator . setLevel ( log . getName ( ) , Level . INFO ) ; String loglevelStr = JMeterUtils . getPropDefault ( "loglevel" , HttpSimpleTableControl . DEFAULT_LOG_LEVEL ) ; System . out . println ( "loglevel=" + loglevelStr ) ; Configurator . setRootLevel ( Level . toLevel ( loglevelStr ) ) ; Configurator . setLevel ( log . getName ( ) , Level . toLevel ( loglevelStr ) ) ; bStartFromMain = true ; HttpSimpleTableServer serv = new HttpSimpleTableServer ( port , timestamp , dataset ) ; log . info ( "------------------------------" ) ; log . info ( "SERVER_PORT : " + port ) ; log . info ( "DATASET_DIR : " + dataset ) ; log . info ( "TIMESTAMP : " + timestamp ) ; log . info ( "------------------------------" ) ; log . info ( "STS_VERSION : " + STS_VERSION ) ; ServerRunner . executeInstance ( serv ) ; }
public static void main ( String args [ ] ) { JMeterUtils . loadJMeterProperties ( "jmeter.properties" ) ; String dataset = JMeterUtils . getPropDefault ( "jmeterPlugin.sts.datasetDirectory" , HttpSimpleTableControl . DEFAULT_DATA_DIR ) ; int port = JMeterUtils . getPropDefault ( "jmeterPlugin.sts.port" , HttpSimpleTableControl . DEFAULT_PORT ) ; boolean timestamp = JMeterUtils . getPropDefault ( "jmeterPlugin.sts.addTimestamp" , HttpSimpleTableControl . DEFAULT_TIMESTAMP ) ; Configurator . setLevel ( log . getName ( ) , Level . INFO ) ; String loglevelStr = JMeterUtils . getPropDefault ( "loglevel" , HttpSimpleTableControl . DEFAULT_LOG_LEVEL ) ; System . out . println ( "loglevel=" + loglevelStr ) ; Configurator . setRootLevel ( Level . toLevel ( loglevelStr ) ) ; Configurator . setLevel ( log . getName ( ) , Level . toLevel ( loglevelStr ) ) ; bStartFromMain = true ; HttpSimpleTableServer serv = new HttpSimpleTableServer ( port , timestamp , dataset ) ; log . info ( "Creating HttpSimpleTable ..." ) ; log . info ( "------------------------------" ) ; log . info ( "DATASET_DIR : " + dataset ) ; log . info ( "TIMESTAMP : " + timestamp ) ; log . info ( "------------------------------" ) ; log . info ( "STS_VERSION : " + STS_VERSION ) ; ServerRunner . executeInstance ( serv ) ; }
public static void main ( String args [ ] ) { JMeterUtils . loadJMeterProperties ( "jmeter.properties" ) ; String dataset = JMeterUtils . getPropDefault ( "jmeterPlugin.sts.datasetDirectory" , HttpSimpleTableControl . DEFAULT_DATA_DIR ) ; int port = JMeterUtils . getPropDefault ( "jmeterPlugin.sts.port" , HttpSimpleTableControl . DEFAULT_PORT ) ; boolean timestamp = JMeterUtils . getPropDefault ( "jmeterPlugin.sts.addTimestamp" , HttpSimpleTableControl . DEFAULT_TIMESTAMP ) ; Configurator . setLevel ( log . getName ( ) , Level . INFO ) ; String loglevelStr = JMeterUtils . getPropDefault ( "loglevel" , HttpSimpleTableControl . DEFAULT_LOG_LEVEL ) ; System . out . println ( "loglevel=" + loglevelStr ) ; Configurator . setRootLevel ( Level . toLevel ( loglevelStr ) ) ; Configurator . setLevel ( log . getName ( ) , Level . toLevel ( loglevelStr ) ) ; bStartFromMain = true ; HttpSimpleTableServer serv = new HttpSimpleTableServer ( port , timestamp , dataset ) ; log . info ( "Creating HttpSimpleTable ..." ) ; log . info ( "------------------------------" ) ; log . info ( "SERVER_PORT : " + port ) ; log . info ( "TIMESTAMP : " + timestamp ) ; log . info ( "------------------------------" ) ; log . info ( "Dataset = " + dataset ) ; log . info ( "STS_VERSION : " + STS_VERSION ) ; ServerRunner . executeInstance ( serv ) ; }
public static void main ( String args [ ] ) { JMeterUtils . loadJMeterProperties ( "jmeter.properties" ) ; String dataset = JMeterUtils . getPropDefault ( "jmeterPlugin.sts.datasetDirectory" , HttpSimpleTableControl . DEFAULT_DATA_DIR ) ; int port = JMeterUtils . getPropDefault ( "jmeterPlugin.sts.port" , HttpSimpleTableControl . DEFAULT_PORT ) ; boolean timestamp = JMeterUtils . getPropDefault ( "jmeterPlugin.sts.addTimestamp" , HttpSimpleTableControl . DEFAULT_TIMESTAMP ) ; Configurator . setLevel ( log . getName ( ) , Level . INFO ) ; String loglevelStr = JMeterUtils . getPropDefault ( "loglevel" , HttpSimpleTableControl . DEFAULT_LOG_LEVEL ) ; System . out . println ( "loglevel=" + loglevelStr ) ; Configurator . setRootLevel ( Level . toLevel ( loglevelStr ) ) ; Configurator . setLevel ( log . getName ( ) , Level . toLevel ( loglevelStr ) ) ; bStartFromMain = true ; HttpSimpleTableServer serv = new HttpSimpleTableServer ( port , timestamp , dataset ) ; log . info ( "Creating HttpSimpleTable ..." ) ; log . info ( "------------------------------" ) ; log . info ( "SERVER_PORT : " + port ) ; log . info ( "DATASET_DIR : " + dataset ) ; log . info ( "------------------------------" ) ; log . info ( "STS_VERSION : " + STS_VERSION ) ; ServerRunner . executeInstance ( serv ) ; }
public static void main ( String args [ ] ) { JMeterUtils . loadJMeterProperties ( "jmeter.properties" ) ; String dataset = JMeterUtils . getPropDefault ( "jmeterPlugin.sts.datasetDirectory" , HttpSimpleTableControl . DEFAULT_DATA_DIR ) ; int port = JMeterUtils . getPropDefault ( "jmeterPlugin.sts.port" , HttpSimpleTableControl . DEFAULT_PORT ) ; boolean timestamp = JMeterUtils . getPropDefault ( "jmeterPlugin.sts.addTimestamp" , HttpSimpleTableControl . DEFAULT_TIMESTAMP ) ; Configurator . setLevel ( log . getName ( ) , Level . INFO ) ; String loglevelStr = JMeterUtils . getPropDefault ( "loglevel" , HttpSimpleTableControl . DEFAULT_LOG_LEVEL ) ; System . out . println ( "loglevel=" + loglevelStr ) ; Configurator . setRootLevel ( Level . toLevel ( loglevelStr ) ) ; Configurator . setLevel ( log . getName ( ) , Level . toLevel ( loglevelStr ) ) ; bStartFromMain = true ; HttpSimpleTableServer serv = new HttpSimpleTableServer ( port , timestamp , dataset ) ; log . info ( "Creating HttpSimpleTable ..." ) ; log . info ( "------------------------------" ) ; log . info ( "SERVER_PORT : " + port ) ; log . info ( "DATASET_DIR : " + dataset ) ; log . info ( "TIMESTAMP : " + timestamp ) ; log . info ( "------------------------------" ) ; ServerRunner . executeInstance ( serv ) ; }
@ Override public void start ( final Map < String , String > properties ) { final String timeout = properties . getOrDefault ( StatelessKafkaConnectorUtil . DATAFLOW_TIMEOUT , StatelessKafkaConnectorUtil . DEFAULT_DATAFLOW_TIMEOUT ) ; timeoutMillis = ( long ) FormatUtils . getPreciseTimeDuration ( timeout , TimeUnit . MILLISECONDS ) ; topicName = properties . get ( StatelessNiFiSourceConnector . TOPIC_NAME ) ; topicNameAttribute = properties . get ( StatelessNiFiSourceConnector . TOPIC_NAME_ATTRIBUTE ) ; keyAttributeName = properties . get ( StatelessNiFiSourceConnector . KEY_ATTRIBUTE ) ; code_block = IfStatement ; final String headerRegex = properties . get ( StatelessNiFiSourceConnector . HEADER_REGEX ) ; headerAttributeNamePattern = headerRegex == null ? null : Pattern . compile ( headerRegex ) ; dataflow = StatelessKafkaConnectorUtil . createDataflow ( properties ) ; dataflow . initialize ( ) ; dataflowName = properties . get ( StatelessKafkaConnectorUtil . DATAFLOW_NAME ) ; outputPortName = properties . get ( StatelessNiFiSourceConnector . OUTPUT_PORT_NAME ) ; code_block = IfStatement ; final String taskIndex = properties . get ( STATE_MAP_KEY ) ; localStatePartitionMap . put ( STATE_MAP_KEY , taskIndex ) ; final Map < String , String > localStateMap = ( Map < String , String > ) ( Map ) context . offsetStorageReader ( ) . offset ( localStatePartitionMap ) ; final Map < String , String > clusterStateMap = ( Map < String , String > ) ( Map ) context . offsetStorageReader ( ) . offset ( clusterStatePartitionMap ) ; dataflow . setComponentStates ( localStateMap , Scope . LOCAL ) ; dataflow . setComponentStates ( clusterStateMap , Scope . CLUSTER ) ; log . info ( "{} Dataflow started" , this ) ; }
public void test() { try { Document response = readXML ( url ) ; NodeList nodes = response . getElementsByTagName ( "IdList" ) ; code_block = IfStatement ; } catch ( Exception ex ) { logger . error ( "Failed to retrieve ID list from " + url , ex ) ; } }
public void test() { if ( ObjectHelper . isEmpty ( deploymentName ) ) { LOG . error ( "Create a specific Deployment require specify a pod name" ) ; throw new IllegalArgumentException ( "Create a specific Deployment require specify a pod name" ) ; } }
public void test() { if ( ObjectHelper . isEmpty ( namespaceName ) ) { LOG . error ( "Create a specific Deployment require specify a namespace name" ) ; throw new IllegalArgumentException ( "Create a specific Deployment require specify a namespace name" ) ; } }
public void test() { if ( ObjectHelper . isEmpty ( deSpec ) ) { LOG . error ( "Create a specific Deployment require specify a Deployment spec bean" ) ; throw new IllegalArgumentException ( "Create a specific Deployment require specify a Deployment spec bean" ) ; } }
@ BeforeClass public void setup ( ) throws Exception { String dataPath = rootPath + "/src/test/resources/alldatatype.csv" ; CarbonProperties . getInstance ( ) . addProperty ( CarbonCommonConstants . CARBON_WRITTEN_BY_APPNAME , "HetuTest" ) ; CarbonProperties . getInstance ( ) . addProperty ( CarbonCommonConstants . MAX_QUERY_EXECUTION_TIME , "0" ) ; CarbonProperties . getInstance ( ) . addProperty ( CarbonCommonConstants . CARBON_SEGMENT_LOCK_FILES_PRESERVE_HOURS , "0" ) ; CarbonProperties . getInstance ( ) . addProperty ( CarbonCommonConstants . CARBON_INVISIBLE_SEGMENTS_PRESERVE_COUNT , "1" ) ; Map < String , String > map = new HashMap < String , String > ( ) ; map . put ( "hive.metastore" , "file" ) ; map . put ( "hive.allow-drop-table" , "true" ) ; map . put ( "hive.metastore.catalog.dir" , "file://" + storePath + "/hive.store" ) ; map . put ( "carbondata.store-location" , "file://" + carbonStoreLocation ) ; map . put ( "carbondata.minor-vacuum-seg-count" , "4" ) ; map . put ( "carbondata.major-vacuum-seg-size" , "1" ) ; code_block = IfStatement ; hetuServer . startServer ( "testdb" , map ) ; hetuServer . execute ( "drop table if exists testdb.testtable" ) ; hetuServer . execute ( "drop table if exists testdb.testtable2" ) ; hetuServer . execute ( "drop table if exists testdb.testtable3" ) ; hetuServer . execute ( "drop table if exists testdb.testtable4" ) ; hetuServer . execute ( "drop schema if exists testdb" ) ; hetuServer . execute ( "drop schema if exists default" ) ; hetuServer . execute ( "create schema testdb" ) ; hetuServer . execute ( "
@ BeforeClass public void setup ( ) throws Exception { logger . info ( "Setup begin: " + this . getClass ( ) . getSimpleName ( ) ) ; String dataPath = rootPath + "/src/test/resources/alldatatype.csv" ; CarbonProperties . getInstance ( ) . addProperty ( CarbonCommonConstants . CARBON_WRITTEN_BY_APPNAME , "HetuTest" ) ; CarbonProperties . getInstance ( ) . addProperty ( CarbonCommonConstants . MAX_QUERY_EXECUTION_TIME , "0" ) ; CarbonProperties . getInstance ( ) . addProperty ( CarbonCommonConstants . CARBON_SEGMENT_LOCK_FILES_PRESERVE_HOURS , "0" ) ; CarbonProperties . getInstance ( ) . addProperty ( CarbonCommonConstants . CARBON_INVISIBLE_SEGMENTS_PRESERVE_COUNT , "1" ) ; Map < String , String > map = new HashMap < String , String > ( ) ; map . put ( "hive.metastore" , "file" ) ; map . put ( "hive.allow-drop-table" , "true" ) ; map . put ( "hive.metastore.catalog.dir" , "file://" + storePath + "/hive.store" ) ; map . put ( "carbondata.store-location" , "file://" + carbonStoreLocation ) ; map . put ( "carbondata.minor-vacuum-seg-count" , "4" ) ; map . put ( "carbondata.major-vacuum-seg-size" , "1" ) ; code_block = IfStatement ; hetuServer . startServer ( "testdb" , map ) ; hetuServer . execute ( "drop table if exists testdb.testtable" ) ; hetuServer . execute ( "drop table if exists testdb.testtable2" ) ; hetuServer . execute ( "drop table if exists testdb.testtable3" ) ; hetuServer . execute ( "drop table if exists testdb.testtable4" ) ; hetuServer . execute ( "drop schema if exists testdb" ) ; hetuServer . execute ( "drop schema if exists test
public void test() { try { MethodKey methodKey = new MethodKey ( UserServiceUtil . class , "updateIncompleteUser" , _updateIncompleteUserParameterTypes60 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , companyId , autoPassword , password1 , password2 , autoScreenName , screenName , emailAddress , facebookId , openId , locale , firstName , middleName , lastName , prefixId , suffixId , male , birthdayMonth , birthdayDay , birthdayYear , jobTitle , updateUserInformation , sendEmail , serviceContext ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . portal . kernel . model . User ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
@ Override public PortablePipelineResult run ( Pipeline pipeline , JobInfo jobInfo ) throws Exception { LOG . info ( "Jar {}" , outputFile . getAbsolutePath ( ) ) ; PortablePipelineOptions pipelineOptions = PipelineOptionsTranslation . fromProto ( jobInfo . pipelineOptions ( ) ) . as ( PortablePipelineOptions . class ) ; final String jobName = jobInfo . jobName ( ) ; File outputFile = new File ( checkArgumentNotNull ( pipelineOptions . getOutputExecutablePath ( ) ) ) ; outputStream = new JarOutputStream ( new FileOutputStream ( outputFile ) , createManifest ( mainClass , jobName ) ) ; outputChannel = Channels . newChannel ( outputStream ) ; PortablePipelineJarUtils . writeDefaultJobName ( outputStream , jobName ) ; copyResourcesFromJar ( new JarFile ( mainClass . getProtectionDomain ( ) . getCodeSource ( ) . getLocation ( ) . getPath ( ) ) ) ; writeAsJson ( PipelineOptionsTranslation . toProto ( pipelineOptions ) , PortablePipelineJarUtils . getPipelineOptionsUri ( jobName ) ) ; Pipeline pipelineWithClasspathArtifacts = writeArtifacts ( pipeline , jobName ) ; writeAsJson ( pipelineWithClasspathArtifacts , PortablePipelineJarUtils . getPipelineUri ( jobName ) ) ; outputChannel . close ( ) ; LOG . info ( "Jar {} created successfully." , outputFile . getAbsolutePath ( ) ) ; return new JarCreatorPipelineResult ( ) ; }
@ Override public PortablePipelineResult run ( Pipeline pipeline , JobInfo jobInfo ) throws Exception { LOG . info ( "Creating jar {} for job {}" , outputFile . getAbsolutePath ( ) , jobName ) ; PortablePipelineOptions pipelineOptions = PipelineOptionsTranslation . fromProto ( jobInfo . pipelineOptions ( ) ) . as ( PortablePipelineOptions . class ) ; final String jobName = jobInfo . jobName ( ) ; File outputFile = new File ( checkArgumentNotNull ( pipelineOptions . getOutputExecutablePath ( ) ) ) ; LOG . info ( "Creating jar {} for job {}" , outputFile . getAbsolutePath ( ) , jobName ) ; outputStream = new JarOutputStream ( new FileOutputStream ( outputFile ) , createManifest ( mainClass , jobName ) ) ; outputChannel = Channels . newChannel ( outputStream ) ; PortablePipelineJarUtils . writeDefaultJobName ( outputStream , jobName ) ; copyResourcesFromJar ( new JarFile ( mainClass . getProtectionDomain ( ) . getCodeSource ( ) . getLocation ( ) . getPath ( ) ) ) ; writeAsJson ( PipelineOptionsTranslation . toProto ( pipelineOptions ) , PortablePipelineJarUtils . getPipelineOptionsUri ( jobName ) ) ; Pipeline pipelineWithClasspathArtifacts = writeArtifacts ( pipeline , jobName ) ; writeAsJson ( pipelineWithClasspathArtifacts , PortablePipelineJarUtils . getPipelineUri ( jobName ) ) ; outputChannel . close ( ) ; return new JarCreatorPipelineResult ( ) ; }
public void test() { if ( definitionsService == null || alertsEngine == null ) { log . warn ( "Could not get definitions service" ) ; return ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
@ Override public void onFailure ( final Throwable throwable ) { LOG . error ( "Can't send {}" , send , throwable ) ; sender . tell ( new Failure ( throwable ) , getSelf ( ) ) ; }
public void test() { if ( message . getExtensionLength ( ) . getValue ( ) > 65535 ) { LOGGER . warn ( "Extension length should be more than 65535" ) ; } }
public void test() { if ( inputStream == null ) { LOGGER . error ( "Unable to find the input stream" ) ; } }
public void test() { try { InputStream inputStream = classLoader . getResourceAsStream ( PropsUtil . get ( PropsKeys . IMAGE_DEFAULT_USER_MALE_PORTRAIT ) ) ; code_block = IfStatement ; _defaultUserMalePortrait = getImage ( inputStream ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; } }
public void test() { try { final Class < ? > declaringClass = methodInvoked . getDeclaringClass ( ) ; code_block = IfStatement ; methodNameBuilder . append ( methodInvoked . getName ( ) ) ; } catch ( final Exception exception ) { logger . debug ( exception . getMessage ( ) ) ; } }
public void test() { if ( isDebug ) { logger . debug ( "Expected arguments, but found none." ) ; } }
public void test() { if ( isDebug ) { logger . debug ( "Expected arguments, but found none." ) ; } }
public void test() { try { exporter . export ( AuthContextUtils . getDomain ( ) , os , uwfAdapter . getPrefix ( ) , gwfAdapter . getPrefix ( ) , awfAdapter . getPrefix ( ) ) ; LOG . debug ( "Internal storage content successfully exported" ) ; } catch ( Exception e ) { LOG . error ( "Error while exporting internal storage content" , e ) ; } }
public void test() { try { serverChannel . close ( ) ; } catch ( IOException e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { HoodieCommitMetadata commitMetadata = HoodieCommitMetadata . fromBytes ( metaClient . getCommitsTimeline ( ) . getInstantDetails ( instant ) . get ( ) , HoodieCommitMetadata . class ) ; return Option . ofNullable ( commitMetadata . getExtraMetadata ( ) . get ( extraMetadataKey ) ) ; } catch ( IOException e ) { LOG . error ( "Unable to parse instant metadata " + instant , e ) ; throw new HoodieIOException ( "Unable to parse instant metadata " + instant , e ) ; } }
@ Override public void close ( ) throws IOException { LOG . info ( "\tStarting persistence Processor" ) ; disruptor . halt ( ) ; disruptor . shutdown ( ) ; LOG . info ( "\tPersistence Processor Disruptor shutdown" ) ; disruptorExec . shutdownNow ( ) ; code_block = TryStatement ;  LOG . info ( "Persistence Processor terminated" ) ; }
public void test() { try { disruptorExec . awaitTermination ( 3 , SECONDS ) ; LOG . info ( "\tPersistence Processor Disruptor executor shutdown" ) ; } catch ( InterruptedException e ) { LOG . warn ( "\tInterrupted while waiting for disruptor executor executor termination" , e ) ; Thread . currentThread ( ) . interrupt ( ) ; } }
protected synchronized void update ( final Map < String , Object > properties ) { logger . debug ( "Updating GPIO Driver... Done" ) ; logger . debug ( "Updating GPIO Driver... Done" ) ; }
protected synchronized void update ( final Map < String , Object > properties ) { logger . debug ( "Updating GPIO Driver..." ) ; logger . debug ( "Updating GPIO Driver... Done" ) ; }
public void test() { try { stat = conn . prepareStatement ( DELETE_CONFIG ) ; stat . setString ( 1 , username ) ; stat . executeUpdate ( ) ; } catch ( Throwable t ) { _logger . error ( "Error deleting user config record by id {}" , username , t ) ; throw new RuntimeException ( "Error deleting user config record by id " + username , t ) ; } finally { this . closeDaoResources ( null , stat ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( EmailAddressServiceUtil . class , "getEmailAddress" , _getEmailAddressParameterTypes3 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , emailAddressId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . portal . kernel . model . EmailAddress ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { String defaultLangCode = this . getLangManager ( ) . getDefaultLang ( ) . getCode ( ) ; String langCode = properties . getProperty ( SystemConstants . API_LANG_CODE_PARAMETER ) ; String tagParamValue = properties . getProperty ( "tag" ) ; langCode = ( null != langCode && null != this . getLangManager ( ) . getLang ( langCode ) ) ? langCode : defaultLangCode ; Map < String , ApiService > masterServices = this . getApiCatalogManager ( ) . getServices ( tagParamValue ) ; Iterator < ApiService > iter = masterServices . values ( ) . iterator ( ) ; code_block = WhileStatement ; BeanComparator comparator = new BeanComparator ( "description" ) ; Collections . sort ( services , comparator ) ; } catch ( Throwable t ) { _logger . error ( "error in getServices" , t ) ; throw new ApiException ( IApiErrorCodes . SERVER_ERROR , "Internal error" ) ; } }
public void test() { try { tokensMan . addToken ( CONFIRMATION_TOKEN_TYPE , token , state . getBytes ( StandardCharsets . UTF_8 ) , createDate , expires ) ; } catch ( Exception e ) { logger . warn ( "Unable to add token: " + e . getMessage ( ) ) ; throw e ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { return new KapuaApplicationBrokerFilter ( broker ) ; } catch ( Exception e ) { LOG . error ( "Error creating broker filter" , e ) ; throw new SecurityException ( e ) ; } }
public void test() { if ( function . getParameters ( ) . size ( ) > 254 ) { log . warn ( "Too many parameters for function " + function + ", dropping" ) ; return ; } }
public void test() { if ( auth != null && auth . startsWith ( BEARER_PREFIX ) ) { String token = auth . substring ( BEARER_PREFIX . length ( ) ) ; log . debug ( "Authenticating with token" ) ; TokenReview tokenReview = authApi . performTokenReview ( token ) ; code_block = IfStatement ; } else-if ( request != null && request . isSSL ( ) && findUserName ( apiHeaderConfig , requestContext ) != null ) { log . debug ( "Authenticating using client certificate" ) ; HttpConnection connection = request . connection ( ) ; String userName = findUserName ( apiHeaderConfig , requestContext ) ; Set < String > groups = findGroups ( apiHeaderConfig , requestContext ) ; Map < String , List < String > > extras = findExtra ( apiHeaderConfig , requestContext ) ; log . debug ( "Found username {}, groups {}, extra {}" , userName , groups , extras ) ; code_block = TryStatement ;  } else { requestContext . setSecurityContext ( new RbacSecurityContext ( new TokenReview ( "system:anonymous" , "" , null , null , false ) , authApi , requestContext . getUriInfo ( ) ) ) ; } }
public void test() { if ( auth != null && auth . startsWith ( BEARER_PREFIX ) ) { log . debug ( "Authentication using bearer token" ) ; String token = auth . substring ( BEARER_PREFIX . length ( ) ) ; TokenReview tokenReview = authApi . performTokenReview ( token ) ; code_block = IfStatement ; } else-if ( request != null && request . isSSL ( ) && findUserName ( apiHeaderConfig , requestContext ) != null ) { HttpConnection connection = request . connection ( ) ; String userName = findUserName ( apiHeaderConfig , requestContext ) ; Set < String > groups = findGroups ( apiHeaderConfig , requestContext ) ; Map < String , List < String > > extras = findExtra ( apiHeaderConfig , requestContext ) ; log . debug ( "Found username {}, groups {}, extra {}" , userName , groups , extras ) ; code_block = TryStatement ;  } else { log . debug ( "Skipping authentication" ) ; requestContext . setSecurityContext ( new RbacSecurityContext ( new TokenReview ( "system:anonymous" , "" , null , null , false ) , authApi , requestContext . getUriInfo ( ) ) ) ; } }
public void test() { if ( auth != null && auth . startsWith ( BEARER_PREFIX ) ) { log . debug ( "Authentication using bearer token" ) ; String token = auth . substring ( BEARER_PREFIX . length ( ) ) ; TokenReview tokenReview = authApi . performTokenReview ( token ) ; code_block = IfStatement ; } else-if ( request != null && request . isSSL ( ) && findUserName ( apiHeaderConfig , requestContext ) != null ) { log . debug ( "Authenticating using client certificate" ) ; HttpConnection connection = request . connection ( ) ; String userName = findUserName ( apiHeaderConfig , requestContext ) ; log . debug ( "User name: {}" , userName ) ; Set < String > groups = findGroups ( apiHeaderConfig , requestContext ) ; Map < String , List < String > > extras = findExtra ( apiHeaderConfig , requestContext ) ; code_block = TryStatement ;  } else { requestContext . setSecurityContext ( new RbacSecurityContext ( new TokenReview ( "system:anonymous" , "" , null , null , false ) , authApi , requestContext . getUriInfo ( ) ) ) ; } }
public void test() { try { connection . peerCertificateChain ( ) ; log . debug ( "Client certificates trusted... impersonating {}" , userName ) ; requestContext . setSecurityContext ( new RbacSecurityContext ( new TokenReview ( userName , "" , groups , extras , true ) , authApi , requestContext . getUriInfo ( ) ) ) ; } catch ( SSLPeerUnverifiedException e ) { log . debug ( "Client certificates not trusted..." , e ) ; requestContext . setSecurityContext ( new RbacSecurityContext ( new TokenReview ( "system:anonymous" , "" , null , null , false ) , authApi , requestContext . getUriInfo ( ) ) ) ; } }
public void test() { try { CAS cas = actionHandler . getEditorCas ( ) ; FeatureStructure selectedFS = selectFsByAddr ( cas , aItem . getModelObject ( ) . targetAddr ) ; WebAnnoCasUtil . setFeature ( selectedFS , linkedAnnotationFeature , value != null ? value . getIdentifier ( ) : value ) ; qualifierModel . detach ( ) ; actionHandler . actionCreateOrUpdate ( RequestCycle . get ( ) . find ( AjaxRequestTarget . class ) . get ( ) , cas ) ; LOG . debug ( "Updated FS: {}" , linkedAnnotationFeature . getIdentifier ( ) ) ; } catch ( Exception e ) { LOG . error ( "Error: " + e . getMessage ( ) , e ) ; error ( "Error: " + e . getMessage ( ) ) ; } }
public void test() { try { CAS cas = actionHandler . getEditorCas ( ) ; FeatureStructure selectedFS = selectFsByAddr ( cas , aItem . getModelObject ( ) . targetAddr ) ; WebAnnoCasUtil . setFeature ( selectedFS , linkedAnnotationFeature , value != null ? value . getIdentifier ( ) : value ) ; LOG . info ( "change the value" ) ; qualifierModel . detach ( ) ; actionHandler . actionCreateOrUpdate ( RequestCycle . get ( ) . find ( AjaxRequestTarget . class ) . get ( ) , cas ) ; } catch ( Exception e ) { LOG . error ( "Error: " + e . getMessage ( ) , e ) ; error ( "Error: " + e . getMessage ( ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isInfoEnabled ( ) ) { long elapsedTime = System . currentTimeMillis ( ) - getStartupDate ( ) ; logger . info ( "loaded " + elapsedTime + " ms" ) ; } }
public void test() { if ( _log . isInfoEnabled ( ) ) { _log . info ( StringBundler . concat ( "Removing " , companyId , " from company " , companyId , " to " , company . getCompanyId ( ) , " using " , company . getCompanyId ( ) ) ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Throwable t ) { String cantSendEmailMsg = "Can't send fail safe email: " + t . getMessage ( ) + "\n" + ExceptionUtils . getStackTrace ( t ) ; logger . error ( cantSendEmailMsg , t ) ; consoleWriter . newLine ( ) . fg ( Ansi . Color . RED ) . a ( cantSendEmailMsg ) . println ( 2 ) ; } }
public void test() { try { AutoMLConfig mlConfig = autoMLConfigDAL . getMLConfigByUsecase ( usecase ) ; String modelId = mlConfig . getModelId ( ) ; return h2oApiCommunicator . getLeaderBoard ( modelId ) ; } catch ( Exception e ) { log . error ( "Error getting leaderboard {}" , usecase , e ) ; throw new InsightsCustomException ( "Error getting leaderboard: " + usecase ) ; } }
public void test() { try { final Enumeration < NetworkInterface > interfaces = NetworkInterface . getNetworkInterfaces ( ) ; code_block = WhileStatement ; } catch ( SocketException ex ) { logger . error ( "" , ex ) ; } }
public void test() { try { code_block = WhileStatement ; } catch ( IOException ex ) { code_block = IfStatement ; } finally { LOGGER . info ( "Exiting Close" ) ; } }
public void test() { { log . info ( "Trying request with url : " + requestUrl ) ; response = HttpsClientRequest . doGet ( requestUrl , headers ) ; retryCount ++ ; } }
public void test() { if ( ( contentLength == - 1 ) || ( contentLength > limit ) ) { LOGGER . error ( "Invalid content length {}" , limit ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( serviceDef == null ) { LOG . warn ( "ServiceDef was null!" ) ; } else-if ( CollectionUtils . isEmpty ( serviceDef . getAccessTypes ( ) ) ) { LOG . warn ( "AccessTypeDef collection on serviceDef was null!" ) ; } else { code_block = ForStatement ; } }
public void test() { if ( serviceDef == null ) { LOG . warn ( "serviceDef passed in was null!" ) ; } else-if ( CollectionUtils . isEmpty ( serviceDef . getAccessTypes ( ) ) ) { LOG . warn ( "No ServiceDef passed in was specified!" ) ; } else { code_block = ForStatement ; } }
public void test() { if ( accessTypeDef == null ) { LOG . warn ( "Access type def was null!" ) ; } else { String accessType = accessTypeDef . getName ( ) ; code_block = IfStatement ; } }
public void test() { if ( StringUtils . isBlank ( accessType ) ) { _log . error ( "No access type specified to " + accessType ) ; } else { accessTypes . add ( accessType ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( result . succeeded ( ) ) { log . info ( "HealthServer started" ) ; startPromise . complete ( ) ; } else { log . error ( "Error starting HealthServer" ) ; startPromise . fail ( result . cause ( ) ) ; } }
public void test() { if ( result . succeeded ( ) ) { log . info ( "HealthServer started" ) ; startPromise . complete ( ) ; } else { log . error ( "Failed to start healthServer" , result . cause ( ) ) ; startPromise . fail ( result . cause ( ) ) ; } }
public void test() { for ( String cache : pools ) { code_block = IfStatement ; } }
private void createBackupArchiveBundle ( ) { File file = new File ( ARTIFACTS_BUNDLE_BACKUP_FILE_PATH ) ; DeploymentGroupClient deploymentGroupClient = this . i3sClient . deploymentGroup ( ) ; ResourceCollection < DeploymentGroup > deploymentGroups = deploymentGroupClient . getAll ( ) ; String deploymentGrpUri = deploymentGroups . get ( 0 ) . getUri ( ) ; TaskResource task = this . artifactsBundleClient . createBackupArchiveBundle ( file , deploymentGrpUri ) ; LOGGER . info ( "Task object returned to client: {}" , task . toJsonString ( ) ) ; }
@ Test public void testInvokeNormal ( ) { NormalClass method = Container . getComp ( NormalClass . class ) ; String result = method . test ( ) ; logger . info ( "Result: " + result ) ; assertEquals ( "RESULT" , result ) ; }
private void setJsonTimeseries ( JsonObject obj , String target , Pair < ZonedDateTime , ZonedDateTime > timeRange ) { List < TimeValues > timeValues = databaseConnectService . querySeries ( target , timeRange ) ; logger . debug ( "query {}" , timeValues ) ; JsonArray dataPoints = new JsonArray ( ) ; code_block = ForStatement ; obj . add ( "datapoints" , dataPoints ) ; }
public void test() { try { UserEdit user = userDirectoryService . editUser ( id ) ; state . setAttribute ( "user" , user ) ; state . setAttribute ( "mode" , "edit" ) ; } catch ( UserNotDefinedException e ) { log . warn ( "UsersAction.doEdit: user not found: {}" , id ) ; Object [ ] params = new Object [ ] code_block = "" ; ; addAlert ( state , rb . getFormattedMessage ( "useact.use_notfou" , params ) ) ; state . removeAttribute ( "mode" ) ; } catch ( UserPermissionException e ) { addAlert ( state , rb . getFormattedMessage ( "useact.youdonot1" , new Object [ ] code_block = "" ; ) ) ; state . removeAttribute ( "mode" ) ; } catch ( UserLockedException e ) { addAlert ( state , rb . getFormattedMessage ( "useact.somels" , new Object [ ] code_block = "" ; ) ) ; state . removeAttribute ( "mode" ) ; } }
public void test() { if ( isDebug ) { logger . debug ( "Expected arguments, but found none." ) ; } }
public void test() { try { paraStyleName = AnyConverter . toString ( Utils . getProperty ( textRange , UnoProperty . PARA_STYLE_NAME ) ) ; } catch ( IllegalArgumentException e ) { LOGGER . warn ( "Invalid style name" , e ) ; } }
public synchronized void stopLoadBalancing ( ) { code_block = IfStatement ; stopped = true ; partitionReadLock . lock ( ) ; code_block = TryStatement ;  LOG . info ( "LoadBalancer stopped." ) ; }
@ Override public void setJobContextInformation ( String jobId , Object object ) { LOG . trace ( "setting job context information to: {}" , jobId ) ; runningJobs . put ( jobId , ( ActorRef ) object ) ; }
public void test() { try { CommerceDiscountRuleServiceUtil . deleteCommerceDiscountRule ( commerceDiscountRuleId ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public static void main ( String [ ] args ) throws UnknownHostException { Environment env = new SpringApplication ( DataXAdminApplication . class ) . run ( args ) . getEnvironment ( ) ; String envPort = env . getProperty ( "server.port" ) ; String envContext = env . getProperty ( "server.contextPath" ) ; String port = envPort == null ? "8080" : envPort ; String context = envContext == null ? "" : envContext ; String path = port + "" + context + "/doc.html" ; String externalAPI = InetAddress . getLocalHost ( ) . getHostAddress ( ) ; LOG . info ( "API : {}" , externalAPI ) ; }
@ Test public void shouldReturnEmptyBodyAndStatus404 ( ) { String response = given ( ) . header ( "Accept" , "application/json" ) . contentType ( ContentType . JSON ) . port ( getHttpPort ( ) ) . expect ( ) . statusCode ( Response . Status . NOT_FOUND . getStatusCode ( ) ) . when ( ) . get ( BASE_REST_PATH + "/does-not-exists" ) . asString ( ) ; logger . info ( "Response: {}" , response ) ; org . junit . Assert . assertTrue ( response . isEmpty ( ) ) ; }
public void test() { try { Field fRaf = FileImageOutputStream . class . getDeclaredField ( "raf" ) ; code_block = IfStatement ; } catch ( Exception e ) { LOGGER . warn ( "Failed to access {}" , fRaf , e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CommercePriceModifierServiceUtil . class , "deleteCommercePriceModifier" , _deleteCommercePriceModifierParameterTypes1 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , commercePriceModifierId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . commerce . pricing . model . CommercePriceModifier ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( newFile . exists ( ) ) { Files . delete ( newFile . toPath ( ) ) ; logger . debug ( "Deleted temporary file {}" , newFile . toPath ( ) ) ; } }
public void test() { if ( ! newFile . createNewFile ( ) ) { logger . warn ( "Unable to create temporary file {}" , newFile . getAbsolutePath ( ) ) ; } }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; TsFileIOWriter writer = new TsFileIOWriter ( newFile ) ; code_block = IfStatement ; return writer ; } catch ( IOException e ) { logger . error ( "Failed to create file " + newFile , e ) ; return null ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
private void loadLicenses ( String versionHome ) { log . info ( "Loading license information" ) ; String licensesSource = repository + File . separator + "licenses" ; String licensesTarget = versionHome + File . separator + "licenses" ; FolderTools . copyFromFolderToFolder ( licensesSource , licensesTarget , true ) ; String licensesReportSource = repository + File . separator + "core" + File . separator + "java" + File . separator + "iesi-core" + File . separator + "target" + File . separator + "site" ; String licensesReportTarget = versionHome + File . separator + "licenses" + File . separator + "core" ; FolderTools . copyFromFolderToFolder ( licensesReportSource , licensesReportTarget , true ) ; }
public void test() { try { DistinctContinuationToken distinctContinuationToken = new DistinctContinuationToken ( serializedDistinctContinuationToken ) ; distinctContinuationToken . getSourceToken ( ) ; distinctContinuationToken . getLastHash ( ) ; outDistinctContinuationToken . v = distinctContinuationToken ; parsed = true ; } catch ( Exception ex ) { logger . debug ( "Received exception {}: {}" , ex . getMessage ( ) , serializedDistinctContinuationToken ) ; parsed = false ; outDistinctContinuationToken . v = null ; } }
public void test() { try { HBaseTestHelper . clearHBase ( ) ; TestHBasePutOperator thop = new TestHBasePutOperator ( ) ; thop . getStore ( ) . setTableName ( "table1" ) ; thop . getStore ( ) . setZookeeperQuorum ( "127.0.0.1" ) ; thop . getStore ( ) . setZookeeperClientPort ( 2181 ) ; HBaseTuple t1 = new HBaseTuple ( ) ; t1 . setColFamily ( "colfam0" ) ; t1 . setColName ( "street" ) ; t1 . setRow ( "row1" ) ; t1 . setColValue ( "ts" ) ; HBaseTuple t2 = new HBaseTuple ( ) ; t2 . setColFamily ( "colfam0" ) ; t2 . setColName ( "city" ) ; t2 . setRow ( "row2" ) ; t2 . setColValue ( "tc" ) ; thop . beginWindow ( 0 ) ; thop . input . process ( t1 ) ; AttributeMap . DefaultAttributeMap attributeMap = new AttributeMap . DefaultAttributeMap ( ) ; attributeMap . put ( OperatorContext . PROCESSING_MODE , ProcessingMode . AT_MOST_ONCE ) ; thop . setup ( mockOperatorContext ( 0 , attributeMap ) ) ; thop . input . process ( t2 ) ; thop . endWindow ( ) ; HBaseTuple tuple ; HBaseTuple tuple2 ; tuple = HBaseTestHelper . getHBaseTuple ( "row1" , "colfam0" , "street" ) ; tuple2 = HBaseTestHelper . getHBaseTuple ( "row2" , "colfam0" , "city" ) ; Assert . assertNull ( "Tuple" , tuple ) ; Assert . assertNotNull ( "Tuple2" , tuple2 ) ; Assert . assertEquals ( "Tuple row" , tuple2 . getRow ( ) , "row2" ) ; Assert . assertEquals ( "Tuple column family" , tuple2 . getColFamily ( ) , "colfam0" ) ; Assert . assertEquals ( "Tuple
public void test() { try { configManager . createConfig ( ZK_PROJECT , buildKey ( database ) , database ) ; } catch ( Throwable t ) { LOG . error ( t . getMessage ( ) , t ) ; } finally { hostListString = "" ; } }
public void test() { try { SAMLBindings binding = isGet ? SAMLBindings . HTTP_REDIRECT : SAMLBindings . HTTP_POST ; LogoutRequestDocument reqDoc = LogoutRequestDocument . Factory . parse ( samlRequest ) ; SAMLVerifiableElement verifiableMessage = binding == SAMLBindings . HTTP_REDIRECT ? new RedirectedMessage ( httpReq . getQueryString ( ) ) : new XMLExpandedMessage ( reqDoc , reqDoc . getLogoutRequest ( ) ) ; SAMLMessage < LogoutRequestDocument > requestMessage = new SAMLMessage < > ( verifiableMessage , relayState , binding , reqDoc ) ; logoutProcessor . handleAsyncLogoutFromSAML ( requestMessage , httpResp ) ; } catch ( XmlException e ) { httpResp . sendError ( HttpServletResponse . SC_BAD_REQUEST , "Invalid SLO request (XML is malformed)" ) ; return ; } catch ( EopException e ) { log . warn ( "Can't handle SLO request" , e ) ; } }
public void test() { if ( "localhost" . equalsIgnoreCase ( endpoint . getUri ( ) . getHost ( ) ) ) { LOGGER . info ( "Using localhost address as fallback" ) ; } }
protected void execQueryUsingH2 ( String queryFolder , boolean needSort ) throws Exception { List < File > sqlFiles = getFilesFromFolder ( new File ( queryFolder ) , ".sql" ) ; code_block = ForStatement ; logger . info ( "H2 queryFolder = " + queryFolder ) ; }
public void test() { for ( File sqlFile : sqlFiles ) { String queryName = StringUtils . split ( sqlFile . getName ( ) , '.' ) [ 0 ] ; String sql = getTextFromFile ( sqlFile ) ; executeQuery ( newH2Connection ( ) , queryName , sql , needSort ) ; logger . info ( "Executed query {}" , queryName ) ; } }
protected void writeProtocolVersion ( ) { appendBytes ( msg . getProtocolVersion ( ) . getValue ( ) ) ; LOGGER . debug ( "ProtocolVersion: " + ArrayConverter . bytesToHexString ( msg . getProtocolVersion ( ) . getValue ( ) ) ) ; }
public void test() { try { callback . onFailure ( failure ) ; } catch ( Throwable throwable ) { LOGGER . error ( "Callback failure" , throwable ) ; } }
@ Override public Iterable < EntityBody > list ( NeutralQuery neutralQuery ) { log . debug ( "listing security enforcement rule." ) ; listSecurityCheck ( neutralQuery ) ; return listImplementationAfterSecurityChecks ( neutralQuery ) ; }
public void test() { try { loadData ( data , levelDatPath ) ; } catch ( IOException e ) { LOG . warn ( "IOException trying to load data from path " + path , e ) ; loadData ( data , levelDatOldPath ) ; } }
public void test() { if ( overallUsage >= 1 ) { log . info ( String . format ( "Usage %s >= 1" , overallUsage ) ) ; } }
public void test() { try { com . liferay . layout . page . template . model . LayoutPageTemplateEntry returnValue = LayoutPageTemplateEntryServiceUtil . addLayoutPageTemplateEntry ( groupId , layoutPageTemplateCollectionId , classNameId , classTypeId , name , masterLayoutPlid , status , serviceContext ) ; return com . liferay . layout . page . template . model . LayoutPageTemplateEntrySoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try { uri = new URI ( service . configDescriptionURI ) ; } catch ( URISyntaxException e ) { logger . warn ( "URI is invalid: {}" , service . configDescriptionURI ) ; return properties ; } }
@ Test ( enabled = false ) public void testSqoopImportUsingDefaultCredential ( ) throws Exception { TestContext context = new TestContext ( ) ; Map < String , String > overlay = context . getUniqueOverlay ( ) ; String filePath = TestContext . overlayParametersOverTemplate ( TestContext . CLUSTER_TEMPLATE , overlay ) ; context . setCluster ( filePath ) ; LOG . info ( "entity -submit -type cluster -file " + filePath ) ; Assert . assertEquals ( TestContext . executeWithURL ( "entity -submit -type cluster -file " + filePath ) , 0 ) ; String dsName = "datasource-test-4" ; overlay . put ( DATASOURCE_NAME_KEY , dsName ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . DATASOURCE_TEMPLATE4 , overlay ) ; LOG . info ( "Submit datatsource entity {} via entity -submit -type datasource -file {}" , dsName , filePath ) ; Assert . assertEquals ( TestContext . executeWithURL ( "entity -submit -type datasource -file " + filePath ) , 0 ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . FEED_TEMPLATE3 , overlay ) ; LOG . info ( "Submit import feed with datasource {} via entity -submitAndSchedule -type feed -file {}" , dsName , filePath ) ; Assert . assertEquals ( 0 , TestContext . executeWithURL ( "entity -submitAndSchedule -type feed -file " + filePath ) ) ; }
@ Test ( enabled = false ) public void testSqoopImportUsingDefaultCredential ( ) throws Exception { TestContext context = new TestContext ( ) ; Map < String , String > overlay = context . getUniqueOverlay ( ) ; String filePath = TestContext . overlayParametersOverTemplate ( TestContext . CLUSTER_TEMPLATE , overlay ) ; context . setCluster ( filePath ) ; LOG . info ( "entity -submit -type cluster -file " + filePath ) ; Assert . assertEquals ( TestContext . executeWithURL ( "entity -submit -type cluster -file " + filePath ) , 0 ) ; String dsName = "datasource-test-4" ; overlay . put ( DATASOURCE_NAME_KEY , dsName ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . DATASOURCE_TEMPLATE4 , overlay ) ; LOG . info ( "Submit datatsource entity {} via entity -submit -type datasource -file {}" , dsName , filePath ) ; Assert . assertEquals ( TestContext . executeWithURL ( "entity -submit -type datasource -file " + filePath ) , 0 ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . FEED_TEMPLATE3 , overlay ) ; LOG . info ( "Submit import feed with datasource {} via entity -submitAndSchedule -type feed -file {}" , dsName , filePath ) ; Assert . assertEquals ( 0 , TestContext . executeWithURL ( "entity -submitAndSchedule -type feed -file " + filePath ) ) ; }
@ Test ( enabled = false ) public void testSqoopImportUsingDefaultCredential ( ) throws Exception { TestContext context = new TestContext ( ) ; Map < String , String > overlay = context . getUniqueOverlay ( ) ; String filePath = TestContext . overlayParametersOverTemplate ( TestContext . CLUSTER_TEMPLATE , overlay ) ; context . setCluster ( filePath ) ; LOG . info ( "entity -submit -type cluster -file " + filePath ) ; Assert . assertEquals ( TestContext . executeWithURL ( "entity -submit -type cluster -file " + filePath ) , 0 ) ; String dsName = "datasource-test-4" ; overlay . put ( DATASOURCE_NAME_KEY , dsName ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . DATASOURCE_TEMPLATE4 , overlay ) ; LOG . info ( "Submit datatsource entity {} via entity -submit -type datasource -file {}" , dsName , filePath ) ; Assert . assertEquals ( TestContext . executeWithURL ( "entity -submit -type datasource -file " + filePath ) , 0 ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . FEED_TEMPLATE3 , overlay ) ; LOG . info ( "Submit feed with datasource {} via entity -submitAndSchedule -type feed -file {}" , dsName , filePath ) ; Assert . assertEquals ( 0 , TestContext . executeWithURL ( "entity -submitAndSchedule -type feed -file " + filePath ) ) ; }
@ Override public void onFailure ( Throwable arg0 ) { LOGGER . error ( "Failed to retrieve metadata." , arg0 ) ; }
private void updateSelectedObject ( OWLObject selObj ) { selectedObject = selObj ; logger . trace ( "updateSelectedObject: {}" , this ) ; updateLastSelection ( ) ; fireSelectionChanged ( ) ; }
public void test() { try { Git git = ( ( JGitPathImpl ) pathUtil . convert ( repository . getDefaultBranch ( ) . get ( ) . getPath ( ) ) ) . getFileSystem ( ) . getGit ( ) ; new RemoveRemote ( git , "origin" , REMOTE_ORIGIN_REF ) . execute ( ) ; } catch ( GitException e ) { logger . error ( e , e ) ; } }
public void test() { try { long totalRowsCount = 0 ; log . debug ( "Total rows count: {}." , totalRowsCount ) ; final HarvestStatusQueryBuilder harvestStatusQueryBuilder = buildSqlQuery ( query , true ) ; s = harvestStatusQueryBuilder . getPopulatedStatement ( c ) ; log . debug ( "Query is {}." , s ) ; ResultSet res = s . executeQuery ( ) ; res . next ( ) ; totalRowsCount = res . getLong ( 1 ) ; s = buildSqlQuery ( query , false ) . getPopulatedStatement ( c ) ; res = s . executeQuery ( ) ; List < JobStatusInfo > jobs = makeJobStatusInfoListFromResultset ( res ) ; log . debug ( "Harveststatus constructed based on given query." ) ; return new HarvestStatus ( totalRowsCount , jobs ) ; } catch ( SQLException e ) { String message = "SQL error asking for job status list in database" + "\n" + ExceptionUtils . getSQLExceptionCause ( e ) ; log . warn ( message , e ) ; throw new IOFailure ( message , e ) ; } finally { HarvestDBConnection . release ( c ) ; } }
public void test() { try { long totalRowsCount = 0 ; final HarvestStatusQueryBuilder harvestStatusQueryBuilder = buildSqlQuery ( query , true ) ; log . debug ( "Unpopulated query is {}." , harvestStatusQueryBuilder ) ; s = harvestStatusQueryBuilder . getPopulatedStatement ( c ) ; ResultSet res = s . executeQuery ( ) ; res . next ( ) ; totalRowsCount = res . getLong ( 1 ) ; log . debug ( "Number of query is {}." , totalRowsCount ) ; s = buildSqlQuery ( query , false ) . getPopulatedStatement ( c ) ; res = s . executeQuery ( ) ; List < JobStatusInfo > jobs = makeJobStatusInfoListFromResultset ( res ) ; log . debug ( "Harveststatus constructed based on given query." ) ; return new HarvestStatus ( totalRowsCount , jobs ) ; } catch ( SQLException e ) { String message = "SQL error asking for job status list in database" + "\n" + ExceptionUtils . getSQLExceptionCause ( e ) ; log . warn ( message , e ) ; throw new IOFailure ( message , e ) ; } finally { HarvestDBConnection . release ( c ) ; } }
public void test() { try { long totalRowsCount = 0 ; final HarvestStatusQueryBuilder harvestStatusQueryBuilder = buildSqlQuery ( query , true ) ; log . debug ( "Unpopulated query is {}." , harvestStatusQueryBuilder ) ; s = harvestStatusQueryBuilder . getPopulatedStatement ( c ) ; log . debug ( "Query is {}." , s ) ; ResultSet res = s . executeQuery ( ) ; res . next ( ) ; totalRowsCount = res . getLong ( 1 ) ; s = buildSqlQuery ( query , false ) . getPopulatedStatement ( c ) ; log . debug ( "Query is {}." , totalRowsCount ) ; res = s . executeQuery ( ) ; List < JobStatusInfo > jobs = makeJobStatusInfoListFromResultset ( res ) ; return new HarvestStatus ( totalRowsCount , jobs ) ; } catch ( SQLException e ) { String message = "SQL error asking for job status list in database" + "\n" + ExceptionUtils . getSQLExceptionCause ( e ) ; log . warn ( message , e ) ; throw new IOFailure ( message , e ) ; } finally { HarvestDBConnection . release ( c ) ; } }
public void test() { try { long totalRowsCount = 0 ; final HarvestStatusQueryBuilder harvestStatusQueryBuilder = buildSqlQuery ( query , true ) ; log . debug ( "Unpopulated query is {}." , harvestStatusQueryBuilder ) ; s = harvestStatusQueryBuilder . getPopulatedStatement ( c ) ; log . debug ( "Query is {}." , s ) ; ResultSet res = s . executeQuery ( ) ; res . next ( ) ; totalRowsCount = res . getLong ( 1 ) ; s = buildSqlQuery ( query , false ) . getPopulatedStatement ( c ) ; res = s . executeQuery ( ) ; List < JobStatusInfo > jobs = makeJobStatusInfoListFromResultset ( res ) ; log . debug ( "Harveststatus constructed based on given query." ) ; return new HarvestStatus ( totalRowsCount , jobs ) ; } catch ( SQLException e ) { String message = "SQL error asking for job status list in database" + "\n" + ExceptionUtils . getSQLExceptionCause ( e ) ; log . warn ( message , e ) ; throw new IOFailure ( message , e ) ; } finally { HarvestDBConnection . release ( c ) ; } }
public void test() { try { in . close ( ) ; } catch ( IOException ex ) { LOGGER . error ( "Failed to close input stream for processing input stream" , ex ) ; } }
public void test() { try { CnATreeElement personConfiguration = ( CnATreeElement ) configurationCurrent . getPerson ( ) ; code_block = IfStatement ; } catch ( Exception e ) { LOG . error ( "Error while loading person" , e ) ; } }
public void test() { if ( isEnabled ( ) ) { Stopwatch stopwatch = Stopwatch . createStarted ( ) ; timer . record ( ( ) -> jdbcTemplate . execute ( SQL , callback ( topicMessages ) ) ) ; log . info ( "Messages processed in {}: {}" , SQL , stopwatch . elapsed ( TimeUnit . MILLISECONDS ) ) ; } }
public void test() { try { stopKafkaServer ( ) ; stopZookeeper ( ) ; } catch ( Exception ex ) { logger . debug ( "LSHIL {}" , ex . getLocalizedMessage ( ) ) ; } }
public boolean hasPerfectConfiguration ( IAtom atom , IAtomContainer ac ) throws CDKException { double checkPerfectConfiguration ( IAtom atom , IAtomContainer ac ) throws CDKException { double bondOrderSum = ac . getBondOrderSum ( atom ) ; IBond . Order maxBondOrder = ac . getMaximumBondOrder ( atom ) ; IAtomType [ ] atomTypes = getAtomTypeFactory ( atom . getBuilder ( ) ) . getAtomTypes ( atom . getSymbol ( ) ) ; if ( atomTypes . length == 0 ) return true ; code_block = TryStatement ;  code_block = ForStatement ; code_block = TryStatement ;  logger . debug ( "The specified atom " + atom . getName ( ) + " has no restriction." ) ; return false ; }
public void test() { try { logger . debug ( "Atom has bondOrderSum = " + bondOrderSum ) ; logger . debug ( "Atom has max = " + bondOrderSum ) ; } catch ( Exception exc ) { logger . error ( "Problem removing atom " , exc ) ; } }
public void test() { try { logger . debug ( "Checking configuration of atom " + ac . indexOf ( atom ) ) ; logger . debug ( "Atom has max = " + bondOrderSum ) ; } catch ( Exception exc ) { logger . error ( "Exception" , exc ) ; } }
public void test() { try { logger . debug ( "Checking configuration of atom " + ac . indexOf ( atom ) ) ; logger . debug ( "Atom has bondOrderSum = " + bondOrderSum ) ; } catch ( Exception exc ) { logger . error ( "Problem updating configuration" , exc ) ; } }
public void test() { try { } catch ( Exception exc ) { logger . error ( exc . toString ( ) , exc ) ; } }
public void test() { try { } catch ( Exception exc ) { logger . error ( exc . toString ( ) , exc ) ; } }
public void test() { try { code_block = WhileStatement ; code_block = ForStatement ; } catch ( Throwable t ) { LOGGER . error ( "run() exiting due to uncaught error" , t ) ; } finally { TThreadedSelectorServer . this . stop ( ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( BlogsEntryServiceUtil . class , "getGroupEntriesCount" , _getGroupEntriesCountParameterTypes14 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , displayDate , status ) ; Object returnObj = null ; code_block = TryStatement ;  return ( ( Integer ) returnObj ) . intValue ( ) ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( tmpHqlPath != null && logger . isDebugEnabled ( ) ) { logger . debug ( "tmpHqlPath:{}" , tmpHqlPath ) ; } }
public void test() { try { code_block = IfStatement ; final InvokeTraceable listener = ( InvokeTraceable ) adviceListener ; listener . invokeBeforeTracing ( classLoader , owner , methodName , methodDesc , Integer . parseInt ( info [ 3 ] ) ) ; } catch ( Throwable e ) { logger . warn ( "Exception while invoking beforeTracing" , e ) ; } }
public void sleep ( ) throws InterruptedException { long sleepMs = sleepPerConnectionMillis * getConnections ( ) ; logger . info ( "" ) ; logger . info ( "Found {} connections in {} ms" , sleepMs , getConnections ( ) . size ( ) ) ; logger . info ( "" ) ; Thread . sleep ( sleepMs ) ; }
public void test() { if ( function != null && logger . isInfoEnabled ( ) ) { logger . info ( "Function: " + function ) ; } }
public void test() { if ( ! data . isNamed ( ) ) { this . logger . error ( ERROR_MESSAGE_DATA_IN_MEMORY_IN_WRONG_FORMAT ) ; return ; } }
public void test() { try { final XWikiContext context = this . xcontextProvider . get ( ) ; final BaseObject xobject = patient . getXDocument ( ) . getXObject ( Patient . CLASS_REFERENCE , true , context ) ; final PatientData < List < VocabularyTerm > > data = patient . getData ( getName ( ) ) ; code_block = IfStatement ; } catch ( final Exception ex ) { this . logger . error ( "Failed to save vocab data: {}" , ex . getMessage ( ) , ex ) ; } }
public void test() { try { field = TaskRunner . class . getDeclaredField ( "result" ) ; field . setAccessible ( true ) ; } catch ( Exception e ) { LOG . error ( "Can't set the result field to " + fieldName , e ) ; throw new RuntimeException ( "Incompatible Hive API found!" , e ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Throwable e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { event = pluggableSearchEngine . acceptEvent ( event , version ) ; } catch ( Exception ex ) { logger . log ( Level . SEVERE , ex . getMessage ( ) , ex ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { for ( String cache : pools ) { code_block = IfStatement ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { for ( String cache : pools ) { code_block = IfStatement ; } }
@ Override public boolean isSameFile ( FileObject a , FileObject b ) { log . debug ( "isSameFile(): {}" , a ) ; return a . equals ( b ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isInfoEnabled ( ) ) { log . info ( msg ) ; } }
public void test() { for ( String cache : pools ) { code_block = IfStatement ; } }
public void test() { if ( pinDef . getMode ( ) == "OUTPUT" ) { code_block = IfStatement ; } else { logger . info ( "invalid pinDef(" + pinDef . getMode ( ) + ")" ) ; } }
void cleanupTempResource ( BigQueryOptions bqOptions , String stepUuid ) throws Exception { Optional < String > queryTempDatasetOpt = Optional . ofNullable ( tempDatasetId ) ; TableReference tableToRemove = createTempTableReference ( bqOptions . getProject ( ) , BigQueryResourceNaming . createJobIdPrefix ( bqOptions . getJobName ( ) , stepUuid , JobType . QUERY ) , queryTempDatasetOpt ) ; BigQueryServices . DatasetService tableService = bqServices . getDatasetService ( bqOptions ) ; tableService . deleteTable ( tableToRemove ) ; boolean datasetCreatedByBeam = ! queryTempDatasetOpt . isPresent ( ) ; LOG . debug ( "Deleting temp dataset {}" , datasetCreatedByBeam ) ; code_block = IfStatement ; }
public void test() { try { OpenIdConnectConfiguration openIdConfiguration = _configurationProvider . getConfiguration ( OpenIdConnectConfiguration . class , new CompanyServiceSettingsLocator ( companyId , OpenIdConnectConstants . SERVICE_NAME ) ) ; return openIdConfiguration . enabled ( ) ; } catch ( ConfigurationException configurationException ) { _log . error ( "Unable to get OpenId Connect configuration" , configurationException ) ; } }
public void test() { try ( CliRunner runner = new CliRunner ( arguments ) ) { runner . run ( configuration ) ; } catch ( final Throwable e ) { LOG . error ( "Failed to run command." , e ) ; exitCode = 1 ; } finally { exitCommandLine ( configuration , exitCode ) ; } }
public void test() { try { verifyJarSignature ( certificate , jarFilePath ) ; return true ; } catch ( SecurityException | IOException e ) { logger . debug ( e . getMessage ( ) , e ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
@ Override public void exceptionCaught ( ChannelHandlerContext ctx , Throwable cause ) { logger . error ( "Exception occurred" , cause ) ; writeJsonResponse ( ctx , INTERNAL_SERVER_ERROR , INTERNAL_SERVER_ERROR_RESPONSE ) ; }
private void startDownload ( ) { port . clearModemDB ( ) ; lastMessageTimestamp = System . currentTimeMillis ( ) ; messageCount = 0 ; getFirstLinkRecord ( ) ; LOGGER . debug ( "StartDownload" ) ; }
public void test() { if ( val . isPresent ( ) ) { LOG . info ( "Using deprecated config value on " + obj + ", should use '" + key . getName ( ) + "', but used " + "'" ) ; } else-if ( deprecatedValues . size ( ) == 1 ) { LOG . warn ( "Using deprecated config value on " + obj + ", should use '" + key . getName ( ) + "', but used " + "'" + Iterables . getOnlyElement ( deprecatedValues . keySet ( ) ) + "'" ) ; } else { LOG . warn ( "Using deprecated config value on " + obj + ", should use '" + key . getName ( ) + "', but used " + "'" + Iterables . get ( deprecatedValues . keySet ( ) , 1 ) + "' and ignored values present for other " + "deprecated name(s) " + Iterables . skip ( deprecatedValues . keySet ( ) , 1 ) ) ; } }
public void test() { if ( val . isPresent ( ) ) { LOG . warn ( "Ignoring deprecated config value(s) on " + obj + " because contains value to " + "'" + key . getName ( ) + "', other deprecated name(s) present were: " + deprecatedValues . keySet ( ) ) ; } else-if ( deprecatedValues . size ( ) == 1 ) { LOG . warn ( "Using deprecated config value on " + obj + ", should use '" + key . getName ( ) + "', but used " + "'" + Iterables . get ( deprecatedValues . keySet ( ) , 1 ) + "' and ignored values present for other " + "deprecated name(s) " + Iterables . skip ( deprecatedValues . keySet ( ) , 1 ) ) ; } else { LOG . warn ( "Using deprecated config value on " + obj + ", should use '" + key . getName ( ) + "', but used " + "'" + Iterables . get ( deprecatedValues . keySet ( ) , 1 ) + "' and ignored values present for other " + "deprecated name(s) " + Iterables . skip ( deprecatedValues . keySet ( ) , 1 ) ) ; } }
public void test() { if ( val . isPresent ( ) ) { LOG . warn ( "Ignoring deprecated config value(s) on " + obj + " because contains value to " + "'" + key . getName ( ) + "', other deprecated name(s) present were: " + deprecatedValues . keySet ( ) ) ; } else-if ( deprecatedValues . size ( ) == 1 ) { LOG . warn ( "Using deprecated config value on " + obj + ", should use '" + key . getName ( ) + "', but used " + "'" + Iterables . getOnlyElement ( deprecatedValues . keySet ( ) ) + "'" ) ; } else { LOG . warn ( "Using deprecated config value on " + obj + ", should use '" + key . getName ( ) + "', but used " + Iterables . getOnlyElement ( deprecatedValues . keySet ( ) ) + "'" ) ; } }
public void test() { try { HttpHelper . streamURLToFile ( address , fileOnDisk ) ; } catch ( FrameworkException ex ) { logger . error ( "" , ex ) ; } }
public void test() { try { code_block = IfStatement ; PageModel model = event . getPageModel ( ) ; String pageModelCode = ( null != model ) ? model . getCode ( ) : null ; code_block = IfStatement ; } catch ( Throwable t ) { logger . error ( ExceptionUtils . getStackTrace ( t ) ) ; } }
public void setSortByRelevanceFeatureProperty ( String relevanceFeatureProperty ) { log . debug ( "Setting sortByRelevanceFeatureProperty to: {}" , relevanceFeatureProperty ) ; this . sortByRelevanceFeatureProperty = relevanceFeatureProperty ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { long delta = TimeUnit . NANOSECONDS . toMicros ( System . nanoTime ( ) - start ) ; LOGGER . debug ( StringUtilities . formatTimingMessage ( "Took " , delta ) ) ; } }
public void test() { if ( loc == null ) { logger . info ( "Unable to find file: " + file ) ; return true ; } }
public void persist ( StgMbZielobjSubtypTxt transientInstance ) { log . debug ( "persisting StgMbZielobjSubtypTxt instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . persist ( transientInstance ) ; log . debug ( "persist successful" ) ; } catch ( RuntimeException re ) { log . error ( "persist failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . persist ( transientInstance ) ; log . debug ( "persist successful" ) ; } catch ( RuntimeException re ) { log . error ( "persist failed" , re ) ; throw re ; } }
public void test() { try { cmsInfo = ( ClusterManagementServiceInfo ) client . requestToServer ( new HostAndPort ( locator . getHostString ( ) , locator . getPort ( ) ) , new ClusterManagementServiceInfoRequest ( ) , 1000 , true ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Error requesting cluster info." , e ) ; } }
public void test() { if ( authentication . authenticate ( tc ) ) { tokenCredentials = tc ; tokenInfo = authentication . getTokenInfo ( ) ; principal = authentication . getUserPrincipal ( ) ; sharedState . put ( SHARED_KEY_LOGIN_NAME , tokenInfo . getUserId ( ) ) ; LOG . info ( "SessionInfo: {}" , tokenCredentials ) ; return true ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( ! TOKENS_NT_NAME . equals ( nt ) ) { log . error ( "Non-KENS_NT_NAME error: " + nt ) ; } }
public void persist ( StgMbBauMasGef transientInstance ) { log . debug ( "persisting StgMbBauMasGef instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . persist ( transientInstance ) ; log . debug ( "persist successful" ) ; } catch ( RuntimeException re ) { log . error ( "persist failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . persist ( transientInstance ) ; log . debug ( "persist successful" ) ; } catch ( RuntimeException re ) { log . error ( "persist failed" , re ) ; throw re ; } }
public void test() { try ( CheckedInputStream snapIS = SnapStream . getInputStream ( snap ) ) { InputArchive ia = BinaryInputArchive . getArchive ( snapIS ) ; deserialize ( dt , sessions , ia ) ; SnapStream . checkSealIntegrity ( snapIS , ia ) ; code_block = IfStatement ; foundValid = true ; break ; } catch ( IOException e ) { logger . warn ( "Unable to deserialize sessions" , e ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
@ Override public void process ( Exchange exchange ) throws Exception { Message in = exchange . getIn ( ) ; PullRequest pullRequest = ( PullRequest ) in . getBody ( ) ; logger . info ( "pull request from " + pullRequest . getUrl ( ) ) ; User pullRequestUser = pullRequest . getUser ( ) ; pullRequest . getTitle ( ) ; pullRequest . getHtmlUrl ( ) ; pullRequest . getUser ( ) . getLogin ( ) ; pullRequest . getUser ( ) . getHtmlUrl ( ) ; }
public List < Issue > getIssues ( String project , List < String > labels , String reporter , long lookBackMillis ) { List < Issue > issues = new ArrayList < > ( ) ; StringBuilder jiraQuery = new StringBuilder ( ) ; jiraQuery . append ( "project=" ) . append ( project ) ; jiraQuery . append ( " and " ) . append ( "reporter IN (\"" ) . append ( reporter ) . append ( "\")" ) ; jiraQuery . append ( " and " ) . append ( buildQueryOnLabels ( labels ) ) ; jiraQuery . append ( " and " ) . append ( buildQueryOnCreatedBy ( lookBackMillis ) ) ; log . info ( "Jira query: {}" , jiraQuery . toString ( ) ) ; Iterable < Issue > jiraIssuesIt = restClient . getSearchClient ( ) . searchJql ( jiraQuery . toString ( ) ) . claim ( ) . getIssues ( ) ; jiraIssuesIt . forEach ( issues :: add ) ; return issues ; }
public void test() { try { code_block = IfStatement ; this . getSocialActivityStreamManager ( ) . deleteActionCommentRecord ( commentId , recordId ) ; ActionLogRecordDto dto = this . getDtoBuilder ( ) . toDto ( this . getActionLogManager ( ) . getActionRecord ( recordId ) , this . getSocialActivityStreamManager ( ) . getActionLikeRecords ( recordId ) , this . getSocialActivityStreamManager ( ) . getActionCommentRecords ( recordId ) ) ; return dto ; } catch ( Throwable t ) { logger . error ( "error in remove comment {}" , commentId , t ) ; throw new RestServerError ( "error in remove comment" , t ) ; } }
public void test() { try { GetObjectRequest request = new GetObjectRequest ( this . bucketName , key ) ; request . setRange ( byteRangeStart , byteRangeEnd ) ; COSObject cosObject = ( COSObject ) this . callCOSClientWithRetry ( request ) ; return cosObject . getObjectContent ( ) ; } catch ( CosServiceException e ) { String errMsg = String . format ( "Retrieving key [%s] with byteRangeStart [%d] occurs " + "an CosServiceException: [%s]." , key , byteRangeStart , e . toString ( ) ) ; LOG . error ( errMsg ) ; handleException ( new Exception ( errMsg ) , key ) ; return null ; } catch ( CosClientException e ) { String errMsg = String . format ( "Retrieving key [%s] with byteRangeStart [%d] " + "occurs an exception: [%s]." , key , byteRangeStart , e . toString ( ) ) ; LOG . error ( "Retrieving COS key: [{}] with byteRangeStart: [{}] " + "occurs an exception: [{}]." , key , byteRangeStart , e ) ; handleException ( new Exception ( errMsg ) , key ) ; } }
public void test() { try { GetObjectRequest request = new GetObjectRequest ( this . bucketName , key ) ; request . setRange ( byteRangeStart , byteRangeEnd ) ; COSObject cosObject = ( COSObject ) this . callCOSClientWithRetry ( request ) ; return cosObject . getObjectContent ( ) ; } catch ( CosServiceException e ) { String errMsg = String . format ( "Retrieving key [%s] with byteRangeStart [%d] occurs " + "an CosServiceException: [%s]." , key , byteRangeStart , e . toString ( ) ) ; LOG . error ( errMsg ) ; handleException ( new Exception ( errMsg ) , key ) ; return null ; } catch ( CosClientException e ) { String errMsg = String . format ( "Retrieving key [%s] with byteRangeStart [%d] " + "occurs an exception: [%s]." , key , byteRangeStart , e . toString ( ) ) ; LOG . error ( errMsg ) ; handleException ( new Exception ( errMsg ) , key ) ; } }
private Decision checkAppLevelThrottled ( String throttleKey , String tier ) { Decision decision = dataHolder . isThrottled ( throttleKey ) ; log . debug ( "{} decision is throttled." , throttleKey ) ; return decision ; }
@ Override public void onNext ( final SuspendEvent suspendEvent ) { log . log ( Level . INFO , "Received suspend event: {0}" , suspendEvent ) ; NoopTask . this . stopTask ( ) ; }
@ Override public void actionDone ( ) { logger . info ( "actionDone" ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
private void logAndWait ( CompleteUpdate update , CompleteUpdate waitFor ) throws OwsExceptionReport { LOGGER . trace ( "{} stopped waiting for {}" , update , waitFor ) ; waitFor . waitForCompletion ( ) ; LOGGER . trace ( "{} stopped waiting for {}" , update , waitFor ) ; }
private void logAndWait ( CompleteUpdate update , CompleteUpdate waitFor ) throws OwsExceptionReport { LOGGER . trace ( "{} waiting for {}" , update , waitFor ) ; waitFor . waitForCompletion ( ) ; LOGGER . trace ( "Updated {}" , update ) ; }
public void test() { if ( expectedLanguage == null ) { logger . info ( logmessage ) ; } else-if ( expectedLanguage . equalsIgnoreCase ( detectedLanguage ) ) { logger . debug ( logmessage ) ; } else { logger . warn ( logmessage ) ; } }
public void test() { if ( expectedLanguage == null ) { logger . info ( "IGNORE (null expected): {}" , logmessage ) ; } else-if ( expectedLanguage . equalsIgnoreCase ( detectedLanguage ) ) { logger . info ( logmessage ) ; } else { logger . warn ( logmessage ) ; } }
public void test() { if ( expectedLanguage == null ) { logger . info ( "IGNORE (null expected): {}" , logmessage ) ; } else-if ( expectedLanguage . equalsIgnoreCase ( detectedLanguage ) ) { logger . debug ( logmessage ) ; } else { logger . info ( logmessage ) ; } }
public void test() { if ( detectedLanguages . size ( ) == 0 ) { logger . info ( "none ({}) languages detected for {}" , detectedLanguages , textualQuestion ) ; } else-if ( detectedLanguages . size ( ) > 1 ) { logger . warn ( "many ({}) languages detected for {}" , detectedLanguages . size ( ) , textualQuestion ) ; } else { } }
public void test() { if ( detectedLanguages . size ( ) == 0 ) { logger . warn ( "no language detected for {}" , textualQuestion ) ; } else-if ( detectedLanguages . size ( ) > 1 ) { logger . warn ( "multiple language detected for {}" , textualQuestion ) ; } else { } }
public StgNZielobjektRollen merge ( StgNZielobjektRollen detachedInstance ) { log . debug ( "merging StgNZielobjektRollen instance" ) ; code_block = TryStatement ;  }
public void test() { try { StgNZielobjektRollen result = ( StgNZielobjektRollen ) sessionFactory . getCurrentSession ( ) . merge ( detachedInstance ) ; log . debug ( "merge successful" ) ; return result ; } catch ( RuntimeException re ) { log . error ( "merge failed" , re ) ; throw re ; } }
public void test() { try { StgNZielobjektRollen result = ( StgNZielobjektRollen ) sessionFactory . getCurrentSession ( ) . merge ( detachedInstance ) ; log . debug ( "merge successful" ) ; return result ; } catch ( RuntimeException re ) { log . error ( "merge failed" , re ) ; throw re ; } }
@ Override public IBond getBond ( IAtom atom1 , IAtom atom2 ) { logger . debug ( "Getting bond atom1: " , atom1 , " , atom2 ) ; return super . getBond ( atom1 , atom2 ) ; }
public void test() { if ( content == null ) { LOG . warn ( "No content in context, skipping" ) ; } else { content = XmlUtils . stripWhitespaces ( content ) ; context . setProperty ( BaseHttpRequestTransport . REQUEST_CONTENT , content ) ; } }
public void test() { try { PortletJSONUtil . populatePortletJSONObject ( _httpServletRequest , StringPool . BLANK , portlet , jsonObject ) ; PortletJSONUtil . writeHeaderPaths ( pipingServletResponse , jsonObject ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; } }
public void test() { try { Object nextItem = ( ( Task < ? > ) source ) . get ( ) ; code_block = IfStatement ; } catch ( InterruptedException e ) { throw Exceptions . propagate ( e ) ; } catch ( ExecutionException e ) { LOG . warn ( "Task <{}> failed" , name , e ) ; } }
public void test() { if ( ! loggedTaskWarning ) { LOGGER . warn ( format ( "Task warning: %s" , getTaskName ( ) ) ) ; loggedTaskWarning = true ; } }
@ Test public void test_02 ( ) { Log . debug ( "Test" ) ; Itree intForest = newItree ( markers ) ; intForest . build ( ) ; Markers queries = createRandomLargeMarkers ( chromosome , 10000 ) ; int i = 0 ; int totalResults = 0 ; code_block = ForStatement ; Assert . assertTrue ( "Not a signle result found in all queries!" , totalResults > 0 ) ; System . err . println ( "" ) ; }
@ Test public void mailTest ( TestContext testContext ) { this . testContext = testContext ; StringBuilder sb = new StringBuilder ( "*******************************\n" ) ; code_block = ForStatement ; String message = sb . toString ( ) ; logger . info ( message ) ; testException ( new MailMessage ( "user@example.com" , "user@example.com" , "Subject" , message ) ) ; }
public void test() { try { Future < AnalysisSubmission > cleanedSubmissionFuture = analysisExecutionService . cleanupSubmission ( submission ) ; cleanedSubmissions . add ( cleanedSubmissionFuture ) ; } catch ( ExecutionManagerException e ) { LOG . error ( "Could not clean submission " + submission . getId ( ) , e ) ; } }
@ Test public void m_logArtifactsNegativeTest ( ) { LOGGER . info ( " Log Artifacts in Experiment test start................................" ) ; List < Artifact > artifacts = new ArrayList < > ( ) ; Artifact artifact1 = Artifact . newBuilder ( ) . setKey ( "Google Pay Artifact " + Calendar . getInstance ( ) . getTimeInMillis ( ) ) . setPath ( "This is new added data artifact type in Google Pay artifact" ) . setArtifactType ( ArtifactType . MODEL ) . build ( ) ; artifacts . add ( artifact1 ) ; Artifact artifact2 = Artifact . newBuilder ( ) . setKey ( "Google Pay Artifact " + Calendar . getInstance ( ) . getTimeInMillis ( ) ) . setPath ( "This is new added data artifact type in Google Pay artifact" ) . setArtifactType ( ArtifactType . DATA ) . build ( ) ; artifacts . add ( artifact2 ) ; LogExperimentArtifacts logArtifactRequest = LogExperimentArtifacts . newBuilder ( ) . addAllArtifacts ( artifacts ) . build ( ) ; code_block = TryStatement ;  logArtifactRequest = LogExperimentArtifacts . newBuilder ( ) . setId ( "asda" ) . addAllArtifacts ( artifacts ) . build ( ) ; code_block = TryStatement ;  logArtifactRequest = LogExperimentArtifacts . newBuilder ( ) . setId ( experiment . getId ( ) ) . addAllArtifacts ( experiment . getArtifactsList ( ) ) . build ( ) ; code_block = TryStatement ;  LOGGER . info ( "Log Artifacts in Experiment tags Negative test stop................................" ) ; }
@ Test public void m_logArtifactsNegativeTest ( ) { LOGGER . info ( " Log Artifacts in Experiment Negative test start................................" ) ; List < Artifact > artifacts = new ArrayList < > ( ) ; Artifact artifact1 = Artifact . newBuilder ( ) . setKey ( "Google Pay Artifact " + Calendar . getInstance ( ) . getTimeInMillis ( ) ) . setPath ( "This is new added data artifact type in Google Pay artifact" ) . setArtifactType ( ArtifactType . MODEL ) . build ( ) ; artifacts . add ( artifact1 ) ; Artifact artifact2 = Artifact . newBuilder ( ) . setKey ( "Google Pay Artifact " + Calendar . getInstance ( ) . getTimeInMillis ( ) ) . setPath ( "This is new added data artifact type in Google Pay artifact" ) . setArtifactType ( ArtifactType . DATA ) . build ( ) ; artifacts . add ( artifact2 ) ; LogExperimentArtifacts logArtifactRequest = LogExperimentArtifacts . newBuilder ( ) . addAllArtifacts ( artifacts ) . build ( ) ; code_block = TryStatement ;  logArtifactRequest = LogExperimentArtifacts . newBuilder ( ) . setId ( "asda" ) . addAllArtifacts ( artifacts ) . build ( ) ; code_block = TryStatement ;  logArtifactRequest = LogExperimentArtifacts . newBuilder ( ) . setId ( experiment . getId ( ) ) . addAllArtifacts ( experiment . getArtifactsList ( ) ) . build ( ) ; code_block = TryStatement ;  LOGGER . info ( "Log Artifacts in Experiment Negative test stop................................" ) ; }
public void test() { try { StringBundler sb = new StringBundler ( 10 ) ; sb . append ( "<column><model><model-name>" ) ; sb . append ( "com.liferay.dynamic.data.mapping.model.DDMContent" ) ; sb . append ( "</model-name>" ) ; DDMFormValuesSerializerSerializeResponse ddmFormValuesSerializerSerializeResponse = _ddmFormValuesSerializer . serialize ( DDMFormValuesSerializerSerializeRequest . Builder . newBuilder ( ddmFormInstanceRecord . getDDMFormValues ( ) ) . build ( ) ) ; JSONObject dataJSONObject = JSONFactoryUtil . createJSONObject ( ddmFormValuesSerializerSerializeResponse . getContent ( ) ) ; JSONArray fieldValuesJSONArray = dataJSONObject . getJSONArray ( "fieldValues" ) ; fieldValuesJSONArray . forEach ( fieldValue code_block = LoopStatement ; ) ; sb . append ( "</model></column>" ) ; return sb . toString ( ) ; } catch ( PortalException portalException ) { _log . error ( "Unable to generate form fields" , portalException ) ; } }
public void test() { if ( null != skipDryRun && skipDryRun ) { log . info ( "Skipping skip dry run." ) ; return ; } else { String skipDryRunStr = RuntimeProperties . get ( ) . getProperty ( FALCON_SKIP_DRYRUN , "false" ) . toLowerCase ( ) ; code_block = IfStatement ; } }
public void test() { if ( Boolean . valueOf ( skipDryRunStr ) ) { log . info ( skipDryRunStr ) ; return ; } }
public void test() { try { code_block = IfStatement ; } catch ( IOException e ) { LOGGER . error ( "Unable to retrieve file" , e ) ; } }
public void test() { try { long configId = addConfigInfoAtomic ( - 1 , srcIp , srcUser , configInfo , time , configAdvanceInfo ) ; String configTags = configAdvanceInfo == null ? null : ( String ) configAdvanceInfo . get ( "config_tags" ) ; addConfigTagsRelation ( configId , configTags , configInfo . getDataId ( ) , configInfo . getGroup ( ) , configInfo . getTenant ( ) ) ; insertConfigHistoryAtomic ( 0 , configInfo , srcIp , srcUser , time , "I" ) ; } catch ( CannotGetJdbcConnectionException e ) { LogUtil . FATAL_LOG . error ( "[db-error] " + e . toString ( ) , e ) ; throw e ; } }
public void test() { try { WorkflowTrace trace = new WorkflowTrace ( ) ; AliasedConnection con = new OutboundConnection ( "theAlias" , 1111 , "host1111" ) ; trace . addConnection ( con ) ; action = new SendAction ( new ClientHelloMessage ( config ) ) ; action . setConnectionAlias ( con . getAlias ( ) ) ; trace . addTlsAction ( action ) ; StringWriter sw = new StringWriter ( ) ; PrintWriter pw = new PrintWriter ( sw ) ; pw . println ( "<workflowTrace>" ) ; pw . println ( "    <OutboundConnection>" ) ; pw . println ( "       <alias>theAlias</alias>" ) ; pw . println ( "        <port>1111</port>" ) ; pw . println ( "        <hostname>host1111</hostname>" ) ; pw . println ( "    </OutboundConnection>" ) ; pw . println ( "    <Send>" ) ; pw . println ( "        <messages>" ) ; pw . println ( "            <ClientHello>" ) ; pw . println ( "                <extensions>" ) ; pw . println ( "                     <ECPointFormat/>" ) ; pw . println ( "                       <EllipticCurves/>" ) ; pw . println ( "                     <SignatureAndHashAlgorithmsExtension/>" ) ; pw . println ( "                         <RenegotiationInfoExtension/>" ) ; pw . println ( "                </extensions>" ) ; pw . println ( "            </Client
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "JDBC::{}" , sql ) ; } }
public void test() { for ( String namespace : namespaces ) { final Logger logger = Logging . getLogger ( namespace ) ; logger . info ( "Don't talk, just a test" ) ; logger . severe ( "This is an imaginary warning" ) ; logger . warning ( "This is an imaginary warning" ) ; logger . config ( "Not really configuring anything..." ) ; logger . fine ( "This is a detailed (but useless) message\nWe log this one on two lines!" ) ; logger . finer ( "This is a debug message" ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( RejectedExecutionException e ) { log . warn ( "Execution was rejected!" , e ) ; } }
public void test() { if ( line == null || ( element . getVoltageLevelId ( ) != null && ! ( element . getVoltageLevelId ( ) . equals ( line . getTerminal1 ( ) . getVoltageLevel ( ) . getId ( ) ) || element . getVoltageLevelId ( ) . equals ( line . getTerminal2 ( ) . getVoltageLevel ( ) . getId ( ) ) ) ) ) { LOGGER . warn ( "Mapping line '{}' of contingency '{}' not found" , element . getId ( ) , contingency . getId ( ) ) ; return false ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
@ Override public void log ( String dataSourceName , boolean isForceClosing , long startTimeMilliseconds ) { long cost = getElapsedMilliSeconds ( ) ; LOG . info ( String . format ( "DataSource '%s' is force closing, cost %dms" , dataSourceName , cost ) ) ; }
public void test() { try { DirUtils . deleteDirectoryContents ( stageDir . toFile ( ) ) ; } catch ( IOException e ) { LOG . error ( "Could not delete stage directory {}" , stageDir , e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( IOException e ) { log . error ( "Error stopping file {}" , file , e ) ; return TeaVMProgressFeedback . CANCEL ; } }
public void test() { try { return partitionTable . partitionByPathTime ( new PartialPath ( path ) , 0 ) ; } catch ( MetadataException e ) { LOGGER . warn ( "Unable to compute partitionGroup for path : " + path , e ) ; return new PartitionGroup ( ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CPOptionServiceUtil . class , "fetchCPOption" , _fetchCPOptionParameterTypes3 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , cpOptionId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . commerce . product . model . CPOption ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceAccountServiceUtil . class , "getPersonalCommerceAccount" , _getPersonalCommerceAccountParameterTypes6 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , userId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . commerce . account . model . CommerceAccount ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Throwable t ) { log . error ( t . getMessage ( ) , t ) ; } finally { dialect = null ; } }
public void test() { if ( _log . isTraceEnabled ( ) ) { StringBundler sb = new StringBundler ( 6 ) ; sb . append ( "Recommended item: " ) ; sb . append ( recommendedEntryClassPK ) ; sb . append ( " rank: " ) ; sb . append ( productContentCommerceMLRecommendation . getRank ( ) ) ; sb . append ( " score: " ) ; sb . append ( productContentCommerceMLRecommendation . getScore ( ) ) ; _log . trace ( sb . toString ( ) ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { { setServerStopped ( ) ; service . unregisterAction ( getStartOK ( ) ) ; service . unregisterAction ( getStartError ( ) ) ; log . info ( "Server stopped" ) ; kill ( ) ; } }
public void test() { if ( isRunning ( ) ) { LOG . info ( "Kill interpreter process" ) ; return ; } }
@ Override public void destroy ( ) { LOGGER . debug ( "Destroying MessageListenerContainer" ) ; this . messageListenerContainerRegistry . destroy ( ) ; LOGGER . debug ( "Destroying ConnectionFactoryRegistry" ) ; this . connectionFactoryRegistry . destroy ( ) ; }
@ Override public void destroy ( ) { LOGGER . debug ( "Destroying MessageListenerContainerRegistry" ) ; this . messageListenerContainerRegistry . destroy ( ) ; this . connectionFactoryRegistry . destroy ( ) ; LOGGER . debug ( "Destroying ConnectionFactoryRegistry" ) ; }
public void test() { try { camundaClient . post ( requestClientParameter , orchestrationUri ) ; } catch ( ApiException e ) { logger . error ( "Error Calling Workflow Engine" , e ) ; throw new WorkflowEngineConnectionException ( "Error Calling Workflow Engine" , e ) ; } }
public Map < String , Object > createCustomSource ( String sourceName ) throws Exception { checkStarted ( ) ; String response = callApi ( HttpMethod . POST , "/ws/org/sources/form_create" , "{service_type: \"custom\", name: \"" + sourceName + "\"}" ) ; response = "{" + "\"id\":\"FAKE_ID\"," + "\"acces_token\":\"FAKE_TOKEN\"," + "\"key\":\"FAKE_KEY\"" + "}" ; JsonMapper mapper = JsonMapper . builder ( ) . build ( ) ; Map < String , Object > map = mapper . readValue ( response , Map . class ) ; logger . info ( response ) ; return map ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
@ Then ( "^I should receive a notification$" ) public void iShouldReceiveANotification ( ) throws Throwable { LOGGER . info ( "Waiting for a notification for at most {} milliseconds." , MAX_WAIT_FOR_UNKNOWN_NOTIFICATION ) ; final Notification notification = this . notificationService . getNotification ( MAX_WAIT_FOR_UNKNOWN_NOTIFICATION , TimeUnit . MILLISECONDS ) ; code_block = IfStatement ; ScenarioContext . current ( ) . put ( PlatformKeys . KEY_CORRELATION_UID , notification . getCorrelationUid ( ) ) ; ScenarioContext . current ( ) . put ( PlatformKeys . KEY_ORGANIZATION_IDENTIFICATION , PlatformDefaults . DEFAULT_ORGANIZATION_IDENTIFICATION ) ; ScenarioContext . current ( ) . put ( PlatformKeys . KEY_USER_NAME , PlatformDefaults . DEFAULT_USER_NAME ) ; LOGGER . info ( "Handling notification." ) ; this . notificationService . handleNotification ( notification , PlatformDefaults . DEFAULT_ORGANIZATION_IDENTIFICATION ) ; }
public void test() { if ( socketId == - 1 ) { log . warn ( "Unknown socket id." ) ; return ; } }
public void test() { try { final AgentProperty agentProperty = newChannelProperties ( header , pingSession . getServiceType ( ) ) ; long eventIdentifier = AgentLifeCycleAsyncTaskService . createEventIdentifier ( ( int ) socketId , ( int ) pingSession . nextEventIdAllocator ( ) ) ; this . agentLifeCycleAsyncTask . handleLifeCycleEvent ( agentProperty , pingTimestamp , agentLifeCycleState , eventIdentifier ) ; this . agentEventAsyncTask . handleEvent ( agentProperty , pingTimestamp , agentEventType ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { UserServiceUtil . unsetOrganizationUsers ( organizationId , userIds ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( dbType == null ) { code_block = IfStatement ; } }
public void test() { try { String xml = this . extractConfigFile ( disablingCodesFileName ) ; code_block = IfStatement ; } catch ( Throwable t ) { _logger . error ( "error in disablingCodes" , t ) ; } }
public void test() { if ( ctx . getValue ( ) != null ) { LOGGER . warn ( "Context '" + ctx . getValue ( ) + "' is not a valid context" ) ; } }
public static void main ( String [ ] args ) throws Exception { String webappsPath = args [ 0 ] ; int port = Integer . parseInt ( args [ 1 ] ) ; log . info ( "Creating temp dir..." ) ; File dataDir = Files . createTempDir ( ) ; dataDir . deleteOnExit ( ) ; Tomcat tomcat = new Tomcat ( ) ; tomcat . setBaseDir ( dataDir . getAbsolutePath ( ) ) ; tomcat . setPort ( port ) ; tomcat . getConnector ( ) . setAttribute ( "maxThreads" , "1000" ) ; tomcat . addWebapp ( "/" , new File ( webappsPath ) . getAbsolutePath ( ) ) ; log . info ( "-----------------------------------------------------------------" ) ; log . info ( "-----------------------------------------------------------------" ) ; tomcat . start ( ) ; code_block = WhileStatement ; }
public void test() { try { java . util . List < com . liferay . asset . category . property . model . AssetCategoryProperty > returnValue = AssetCategoryPropertyServiceUtil . getCategoryPropertyValues ( companyId , key ) ; return com . liferay . asset . category . property . model . AssetCategoryPropertySoap . toSoapModels ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( log ) { LOGGER . info ( "Exiting reader" ) ; } }
public void rollbackOnError ( Message message , NotificationContext context , StateTransitionError error ) { LOG . warn ( "Rollback on error: {}" , error . getMessage ( ) ) ; }
@ Override protected void onSubtreeModified ( DataObjectModification < OfOverlayL3Context > rootNode , InstanceIdentifier < OfOverlayL3Context > rootIdentifier ) { Name newPortName = rootNode . getDataAfter ( ) . getPortName ( ) ; Name oldPortName = rootNode . getDataBefore ( ) . getPortName ( ) ; code_block = IfStatement ; code_block = IfStatement ; updateLocationBasedOnPortName ( newPortName , rootIdentifier ) ; LOG . trace ( "Subtree updated for Node Identifier {}" , rootNode . getDataAfter ( ) ) ; }
public void test() { if ( oldPortName == null && newPortName == null ) { LOG . debug ( "No need to update location for EP {} because port-name {} is missing." , rootIdentifier , oldPortName ) ; return ; } }
public void test() { if ( oldPortName != null && newPortName != null && oldPortName . equals ( newPortName ) ) { LOGGER . debug ( "No need to find port for EP {}" , oldPortName ) ; return ; } }
public void test() { try { return buildOrganization ( FHIRTransformHelper . extractFhirOrgResourceList ( bundle ) ) ; } catch ( Exception ex ) { LOG . error ( "Error extracting bundle from bundle {}" , bundle , ex ) ; throw new ExchangeTransformException ( ex ) ; } }
public void test() { try { config . loadFromXml ( information ) ; } catch ( IOException e ) { logger . error ( "Error load configuration  {} " , e . getMessage ( ) ) ; } }
@ Override public void spaceRemoved ( SpaceLifeCycleEvent event ) { log . debug ( "Space [" + event . getName ( ) + "] space removed" ) ; }
public void test() { try { String labelData = json . get ( "publish" ) . getAsJsonObject ( ) . get ( "data" ) . getAsString ( ) . toUpperCase ( ) ; String labelHealth = json . get ( "publish" ) . getAsJsonObject ( ) . get ( "health" ) . getAsString ( ) . toUpperCase ( ) ; code_block = IfStatement ; code_block = IfStatement ; labelDataValue = Arrays . asList ( labelData . split ( MQMessageConstants . ROUTING_KEY_SEPERATOR ) ) ; } catch ( Exception e ) { logger . error ( "Error in getLabelFromJson" , e ) ; throw new InsightsCustomException ( e . getMessage ( ) ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( portPublisher == null ) { log . warn ( "Port publish is not enabled." ) ; } }
@ Override public VolumeInstance getInstance ( VolumeOrder volumeOrder , CloudStackUser cloudStackUser ) throws FogbowException { LOGGER . info ( Messages . Log . GETTING_INSTANCE_S , volumeOrder . getInstanceId ( ) ) ; GetVolumeRequest request = new GetVolumeRequest . Builder ( ) . id ( volumeOrder . getInstanceId ( ) ) . build ( this . cloudStackUrl ) ; return doGetInstance ( request , cloudStackUser ) ; }
public void test() { if ( GovpayConfig . getInstance ( ) . getClusterId ( ) == null ) { logger . error ( "\n\tCluster is null.\n" ) ; return ; } }
public void test() { try { batchBD = new BatchBD ( configWrapper ) ; batchBD . setupConnection ( configWrapper . getTransactionID ( ) ) ; batchBD . setAutoCommit ( false ) ; batchBD . enableSelectForUpdate ( ) ; batchBD . setAtomica ( false ) ; Batch batch = getRunningBatch ( batchBD , codBatch ) ; code_block = IfStatement ; } catch ( NotFoundException e ) { LOGGER . error ( e . toString ( ) ) ; return ; } finally { code_block = IfStatement ; } }
@ Test public void test14ReadRead ( ) { LOGGER . info ( "  test14ReadReadRead" ) ; test09WriteCreate ( ) ; EntityUtils . filterAndCheck ( serviceRead . things ( ) , "" , THINGS ) ; }
@ Override public void deleteInstance ( PublicIpOrder publicIpOrder , CloudStackUser cloudStackUser ) throws FogbowException { LOGGER . info ( String . format ( Messages . Log . DELETING_INSTANCE_S , publicIpOrder . getInstance ( ) . toString ( ) ) ) ; doDeleteInstance ( publicIpOrder , cloudStackUser ) ; }
public void test() { if ( conf != null ) { logger . debug ( "Starting OpenTherm Gateway binding on port {}" , conf . ipaddress ) ; connector = new OpenThermGatewaySocketConnector ( this , conf . ipaddress , conf . port ) ; Thread thread = new Thread ( connector , "OpenTherm Gateway Binding - socket listener thread" ) ; thread . setDaemon ( true ) ; thread . start ( ) ; logger . debug ( "OpenTherm Gateway connector started" ) ; } }
@ Override public void onTccTransactionEnded ( GrpcTccTransactionEndedEvent request , StreamObserver < GrpcAck > responseObserver ) { LOG . info ( "Transaction ended" ) ; events . offer ( request ) ; sleep ( ) ; responseObserver . onNext ( ALLOW ) ; responseObserver . onCompleted ( ) ; }
public void test() { if ( activeTx != null && ! activeTx . equals ( id ) ) { log . warn ( "Found existing transaction, but the active tx is not in the active tx " + id ) ; } }
public void test() { try { assignments . clearMetadataCache ( ) ; } catch ( Exception e ) { LOGGER . error ( e . getMessage ( ) , e ) ; result = Boolean . FALSE ; } }
public void test() { try { UserInfo . Username userName = authorization . getUser ( authorizationHeader ) ; authorization . checkSuperAdmin ( userName ) ; boolean result = Boolean . TRUE ; code_block = TryStatement ;  return httpHeader . headers ( ) . entity ( result ) . build ( ) ; } catch ( Exception exception ) { LOGGER . error ( "get permissions failed for authorization" , exception ) ; throw exception ; } }
private String generateConsumerInfo ( final SystemRequestDTO consumer , final CloudRequestDTO consumerCloud ) { logger . debug ( "Generating consumer info..." ) ; final StringBuilder sb = new StringBuilder ( consumer . getSystemName ( ) ) ; code_block = IfStatement ; return sb . toString ( ) . toLowerCase ( ) ; }
public void test() { try { m . parseArgs ( args ) ; m . configure ( ) ; long start = System . nanoTime ( ) ; m . process ( ) ; long end = System . nanoTime ( ) ; logger . info ( "Running in {} ms" , TimeUnit . NANOSECONDS . toMillis ( end - start ) ) ; } catch ( Exception x ) { logger . log ( Level . SEVERE , "Failed to run" , x ) ; } finally { m . shutdown ( ) ; System . exit ( 0 ) ; } }
public void test() { if ( ActiveMQXARecoveryLogger . LOGGER . isDebugEnabled ( ) ) { ActiveMQXARecoveryLogger . LOGGER . debug ( "SSL handshake failed" ) ; } }
public void test() { try { List < Group > userGroups = ( null != currentUser ) ? this . getAuthorizationManager ( ) . getUserGroups ( currentUser ) : new ArrayList < > ( ) ; code_block = IfStatement ; } catch ( Throwable t ) { _logger . error ( "Error extracting data" , t ) ; return null ; } }
public void test() { try { return esRepository . getDataFromES ( AssetConstants . AWS_EC2 , Constants . QUALYS_INFO , mustFilter , null , null , Arrays . asList ( "lastVulnScan" , "totalMemory" , "account.list.hostAssetAccount.username" ) , null ) ; } catch ( Exception e ) { LOGGER . error ( "Error in getDataFromES" , e ) ; throw new DataException ( ) ; } }
public void test() { if ( log . isTraceEnable ( ) ) { log . info ( this , "ThreadAnalysisQueryWorker stopped" ) ; } }
public void test() { try { System . out . println ( "|   ( ok )  " + this . getDatabaseName ( ) + "." + tableName ) ; this . createTable ( tableClass , connectionSource ) ; tables . add ( tableName ) ; } catch ( Throwable t ) { schemaReport . getDatabaseStatus ( ) . put ( this . getDatabaseName ( ) , SystemInstallationReport . Status . INCOMPLETE ) ; String message = "Error creating table " + this . getDatabaseName ( ) + "/" + tableClassName + " - " + t . getMessage ( ) ; _logger . error ( "Error creating table {}" , this . getDatabaseName ( ) + "/" + tableClassName , t ) ; throw new ApsSystemException ( message , t ) ; } }
public void test() { try { List < String > tables = schemaReport . getDataSourceTables ( ) . get ( this . getDatabaseName ( ) ) ; code_block = IfStatement ; code_block = ForStatement ; } catch ( Throwable t ) { schemaReport . getDatabaseStatus ( ) . put ( this . getDatabaseName ( ) , SystemInstallationReport . Status . INCOMPLETE ) ; logger . error ( "Error on setup Database" , t ) ; throw new ApsSystemException ( "Error on setup Database" , t ) ; } }
public void test() { try { KeycloakConfigResolver resolver = resolverClass . newInstance ( ) ; this . deploymentContext = new AdapterDeploymentContext ( resolver ) ; } catch ( Exception e ) { LOG . warn ( "Unable to instantiate resolver {}" , resolverClass , e ) ; throw new RuntimeException ( "Unable to instantiate resolver " + resolverClass ) ; } }
public void test() { if ( vm . isPoweredOn ( ) && ! vm . shutdownGuest ( Constants . VM_FAST_SHUTDOWN_WAITING_SEC * 1000 ) ) { logger . info ( "shutdown " + vm . getName ( ) + " guest OS failed, power off directly" ) ; vm . powerOff ( ) ; } }
public void test() { if ( ! forceReload && ( reloadCheckIntervalInMs < 0 || System . currentTimeMillis ( ) < ( lastReloadCheck + reloadCheckIntervalInMs ) ) ) { log . debug ( "Not reloaded" ) ; return ; } }
public void consume ( ) throws InterruptedException { Item item = queue . take ( ) ; log . info ( item . toString ( ) ) ; }
private void setupLoginTty ( ) throws Exception { String setupTtyScriptName = Configuration . getString ( Constants . SERENGETI_SETUP_LOGIN_TTY_SCRIPT , Constants . SERENGETI_DEFAULT_SETUP_LOGIN_TTY_SCRIPT ) ; String setupTtyScript = getScriptName ( setupTtyScriptName ) ; String cmd = sudoCmd + " " + setupTtyScript ; String action = "Setup login tty to " + nodeIP ; logger . info ( "Run login tty to " + nodeIP ) ; SSHUtil sshUtil = new SSHUtil ( ) ; String errMsg = null ; code_block = ForStatement ; logger . info ( action + " failed" ) ; throw SetPasswordException . FAIL_TO_SETUP_LOGIN_TTY ( nodeIP , errMsg ) ; }
public void test() { try { Thread . sleep ( 3000 ) ; } catch ( InterruptedException e1 ) { logger . error ( e1 . getMessage ( ) ) ; } }
public void test() { if ( policy . toString ( ) . contains ( "CADES" ) || policy . toString ( ) . contains ( "PADES" ) ) { logger . warn ( "The policy '" + policy . toString ( ) + "' is not a supported policy" ) ; } }
public void test() { try { deck . exec ( agent ) ; } catch ( final Throwable ex ) { log . error ( "" , ex ) ; failure . add ( agent . getClass ( ) . getSimpleName ( ) ) ; } }
public void test() { if ( hasJobEntityExpired ( jobEntity ) ) { log . warn ( "Job entity expired" ) ; } }
public void test() { try { element = CnAElementHome . getInstance ( ) . loadById ( MassnahmenUmsetzung . TYPE_ID , selection . getDbId ( ) ) ; LOG . debug ( "Opening MassnahmenUmsetzung..." ) ; openEditor ( element . getId ( ) , new BSIElementEditorInput ( element ) , BSIElementEditorMultiPage . EDITOR_ID ) ; } catch ( CommandException e ) { ExceptionUtil . log ( e , Messages . EditorFactory_2 ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( LOGGER . isTraceEnabled ( ) ) { LOGGER . trace ( "Weak listener list status:{}" , System . lineSeparator ( ) ) ; } }
public void test() { if ( LOGGER . isTraceEnabled ( ) ) { LOGGER . trace ( "Weak listener list status:{}" , System . lineSeparator ( ) ) ; } }
public void test() { if ( ! other . isDone ( ) && ! tracker . equals ( other ) ) { tracker . checkOverlap ( other ) ; code_block = IfStatement ; } else-if ( LOGGER . isTraceEnabled ( ) ) { LOGGER . trace ( String . format ( "Tracker %s has already done: %s" , other , other ) ) ; } }
public void test() { if ( LOGGER . isTraceEnabled ( ) ) { LOGGER . trace ( "Weak listener list status:{}" , System . lineSeparator ( ) ) ; } }
public org . talend . mdm . webservice . WSStringArray getConceptsInDataCluster ( org . talend . mdm . webservice . WSGetConceptsInDataCluster arg0 ) { LOG . info ( "Executing operation getConceptsInDataCluster" ) ; System . out . println ( arg0 ) ; code_block = TryStatement ;  }
public < T extends YamcsService > void addGlobalService ( String name , Class < T > serviceClass , YConfiguration args ) throws ValidationException , InitException { code_block = ForStatement ; log . debug ( "Registering global service " + name + " with name " + serviceClass . getName ( ) ) ; ServiceWithConfig swc = createService ( null , serviceClass . getName ( ) , name , args , true ) ; swc . service . init ( null , name , swc . args ) ; YAMCS . globalServiceList . add ( swc ) ; ManagementService managementService = ManagementService . getInstance ( ) ; managementService . registerService ( null , name , swc . service ) ; }
public void test() { try { return readGmlFile ( file , targetCRS , Version . GML3 ) ; } catch ( IOException | RuntimeException e ) { LOGGER . error ( "Unable to read GML file" , e ) ; } }
@ Override @ Transactional public void deleteAlert ( Alert alert ) { requireNotDisposed ( ) ; requireArgument ( alert != null , "Alert cannot be null." ) ; _logger . debug ( "Deleting alert {}" , alert ) ; EntityManager em = _emProvider . get ( ) ; deleteEntity ( em , alert ) ; em . flush ( ) ; }
public void test() { try { defaultProperties = ComponentUtil . getDefaultProperties ( ocd , this . ctx ) ; } catch ( Exception e ) { LOGGER . log ( Level . SEVERE , e . getMessage ( ) , e ) ; } }
public void test() { try { kssServerId = kieServerState . getConfiguration ( ) . getConfigItemValue ( KIE_SERVER_ID ) ; } catch ( Exception e ) { logger . error ( "Got exception while parsing kie server id {}" , KIE_SERVER_ID , e ) ; } }
public void test() { try { final YamlService11 service11 = dockstoreYaml11 . getService ( ) ; BeanUtils . copyProperties ( service12 , service11 ) ; final DescriptorLanguageSubclass descriptorLanguageSubclass = DescriptorLanguageSubclass . convertShortNameStringToEnum ( service11 . getType ( ) ) ; service12 . setSubclass ( descriptorLanguageSubclass ) ; dockstoreYaml12 . setService ( service12 ) ; validate ( dockstoreYaml12 ) ; return dockstoreYaml12 ; } catch ( UnsupportedOperationException | InvocationTargetException | IllegalAccessException e ) { final String msg = "Error converting ; " + e . getMessage ( ) ; log . error ( msg , e ) ; throw new DockstoreYamlException ( msg ) ; } }
public void test() { try { requestBodyUrl = "grant_type=refresh_token&refresh_token=" . concat ( URLEncoder . encode ( refreshToken , "UTF-8" ) ) ; return generateAccessToken ( requestBodyUrl , oauth2ClientId ) ; } catch ( UnsupportedEncodingException exception ) { LOGGER . error ( "Unexpected Error Occured!!!" ) ; return response ( false , "Unexpected Error Occured!!!" ) ; } }
protected void assertBroker ( InfraConfiguration brokerConfig ) { LOG . info ( "List of broker on {}" , brokerConfig ) ; List < Pod > brokerPods = TestUtils . listBrokerPods ( kubernetes , exampleAddressSpace ) ; assertEquals ( 1 , brokerPods . size ( ) ) ; Pod broker = brokerPods . stream ( ) . findFirst ( ) . get ( ) ; ResourceRequirements resources = broker . getSpec ( ) . getContainers ( ) . stream ( ) . filter ( container -> container . getName ( ) . equals ( "broker" ) ) . findFirst ( ) . map ( Container :: getResources ) . get ( ) ; assertEquals ( new Quantity ( brokerConfig . getMemory ( ) ) , resources . getLimits ( ) . get ( "memory" ) , "Broker memory limit incorrect" ) ; assertEquals ( new Quantity ( brokerConfig . getMemory ( ) ) , resources . getRequests ( ) . get ( "memory" ) , "Broker memory requests incorrect" ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; }
public QueryEndpointsResponse queryEndpoints ( QueryEndpoints parameters ) { logger . info ( "queryEndpoints invoked" ) ; return null ; }
public void test() { if ( accessor . isAnnotationPresent ( Label . class ) ) { String text = accessor . getAnnotation ( Label . class ) . value ( ) ; label = textProvider . getText ( text ) ; logger . debug ( "Setting label from property name: {}" , label ) ; } else { label = Util . guessToWords ( accessor . getName ( ) ) ; logger . debug ( "Setting label from property name: {}" , label ) ; } }
public void test() { if ( accessor . isAnnotationPresent ( Label . class ) ) { String text = accessor . getAnnotation ( Label . class ) . value ( ) ; logger . debug ( "Label annotation present with value: {}" , text ) ; label = textProvider . getText ( text ) ; } else { logger . debug ( "No label found for: {}" , accessor . getName ( ) ) ; label = Util . guessToWords ( accessor . getName ( ) ) ; } }
public void test() { try ( final RestHighLevelClient restHighLevelClient = elasticsearchRestHighLevelClientFactory . getRestHighLevelClient ( ) ) { getResponse = restHighLevelClient . get ( getRequest , RequestOptions . DEFAULT ) ; } catch ( final IOException ioException ) { logger . error ( "Caught IOException while attempting to use the ElasticsearchRestHighLevelClient." , ioException ) ; throw new ElasticsearchRestClientException ( "Caught IOException while attempting to use the ElasticsearchRestHighLevelClient." , ioException ) ; } }
public void test() { try ( IMqttClient client = clientBuilder . create ( ) ) { client . connect ( ) ; List < CompletableFuture < MqttMessage > > receiveFutures = MqttUtils . subscribeAndReceiveMessages ( client , dest . getSpec ( ) . getAddress ( ) , messages . size ( ) , subscriberQos ) ; LOG . info ( "Publishing {} messages" , dest . getSpec ( ) . getAddress ( ) ) ; List < CompletableFuture < Void > > publishFutures = MqttUtils . publish ( client , dest . getSpec ( ) . getAddress ( ) , messages ) ; int publishCount = MqttUtils . awaitAndReturnCode ( publishFutures , 1 , TimeUnit . MINUTES ) ; assertThat ( "Incorrect count of messages published" , publishCount , is ( messages . size ( ) ) ) ; int receivedCount = MqttUtils . awaitAndReturnCode ( receiveFutures , 2 , TimeUnit . MINUTES ) ; assertThat ( "Incorrect count of messages received" , receivedCount , is ( messages . size ( ) ) ) ; } }
public void test() { try { syncContext ( true ) ; code_block = WhileStatement ; } catch ( InterruptedException ex ) { LOG . error ( ex ) ; } }
@ Test @ Order ( order = 6 ) public void testCheckUserNotification ( ) throws Exception { LOG . info ( "testCheckUserNotification" ) ; NotificationsEntity notification = NotificationsDao . get ( ) . selectOnKey ( new Long ( 1 ) ) ; Assert . assertNotNull ( notification ) ; Assert . assertEquals ( MailLogic . NOTIFY_INSERT_KNOWLEDGE , notification . getTitle ( ) ) ; UsersEntity user = UsersDao . get ( ) . selectOnUserKey ( "integration-test-user-01" ) ; UserNotificationsEntity userNotification = UserNotificationsDao . get ( ) . selectOnKey ( notification . getNo ( ) , user . getUserId ( ) ) ; Assert . assertNotNull ( userNotification ) ; user = UsersDao . get ( ) . selectOnUserKey ( "integration-test-user-03" ) ; userNotification = UserNotificationsDao . get ( ) . selectOnKey ( notification . getNo ( ) , user . getUserId ( ) ) ; Assert . assertNotNull ( userNotification ) ; user = UsersDao . get ( ) . selectOnUserKey ( "integration-test-user-02" ) ; userNotification = UserNotificationsDao . get ( ) . selectOnKey ( notification . getNo ( ) , user . getUserId ( ) ) ; Assert . assertNull ( userNotification ) ; int count = MailsDao . get ( ) . selectCountAll ( ) ; Assert . assertEquals ( 2 , count ) ; }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { try { String sHomeCommunityId = PropertyAccessor . getInstance ( ) . getProperty ( NhincConstants . GATEWAY_PROPERTY_FILE , NhincConstants . HOME_COMMUNITY_ID_PROPERTY ) ; OrganizationType org = InternalExchangeManager . getInstance ( ) . getOrganization ( sHomeCommunityId ) ; apiLevels = getAPILevelsFromOrganization ( org , serviceName ) ; } catch ( ExchangeManagerException | PropertyAccessException ex ) { LOG . error ( "Error while retrieving API levels" , ex ) ; } }
public void test() { try { String response = marshal ( contentType , specManagementService . listServerTemplates ( ) ) ; logger . debug ( "Returning response for get server templates: {}" , response ) ; return createCorrectVariant ( response , headers , Response . Status . OK ) ; } catch ( KieServerControllerIllegalArgumentException e ) { return createCorrectVariant ( e . getMessage ( ) , headers , Response . Status . NOT_FOUND ) ; } catch ( KieServerControllerException e ) { logger . debug ( "Requesting server templates failed due to {}" , e . getMessage ( ) , e ) ; return createCorrectVariant ( REQUEST_FAILED_TOBE_PROCESSED + e . getMessage ( ) , headers , Response . Status . BAD_REQUEST ) ; } catch ( Exception e ) { logger . error ( "Get server templates failed due to {}" , e . getMessage ( ) , e ) ; return createCorrectVariant ( "Unknown error " + e . getMessage ( ) , headers , Response . Status . INTERNAL_SERVER_ERROR ) ; } }
public void test() { try { logger . debug ( "Received get server templates" ) ; String response = marshal ( contentType , specManagementService . listServerTemplates ( ) ) ; logger . debug ( "Response: {}" , response ) ; return createCorrectVariant ( response , headers , Response . Status . OK ) ; } catch ( KieServerControllerIllegalArgumentException e ) { return createCorrectVariant ( e . getMessage ( ) , headers , Response . Status . NOT_FOUND ) ; } catch ( KieServerControllerException e ) { return createCorrectVariant ( REQUEST_FAILED_TOBE_PROCESSED + e . getMessage ( ) , headers , Response . Status . BAD_REQUEST ) ; } catch ( Exception e ) { logger . error ( "Get server templates failed due to {}" , e . getMessage ( ) , e ) ; return createCorrectVariant ( "Unknown error " + e . getMessage ( ) , headers , Response . Status . INTERNAL_SERVER_ERROR ) ; } }
public void test() { try { logger . debug ( "Received get server templates" ) ; String response = marshal ( contentType , specManagementService . listServerTemplates ( ) ) ; logger . debug ( "Returning response for get server templates: {}" , response ) ; return createCorrectVariant ( response , headers , Response . Status . OK ) ; } catch ( KieServerControllerIllegalArgumentException e ) { return createCorrectVariant ( e . getMessage ( ) , headers , Response . Status . NOT_FOUND ) ; } catch ( KieServerControllerException e ) { return createCorrectVariant ( REQUEST_FAILED_TOBE_PROCESSED + e . getMessage ( ) , headers , Response . Status . BAD_REQUEST ) ; } catch ( Exception e ) { logger . error ( "Unknown error" , e ) ; return createCorrectVariant ( "Unknown error " + e . getMessage ( ) , headers , Response . Status . INTERNAL_SERVER_ERROR ) ; } }
public void test() { if ( retVal == null ) { log . warn ( "Could not find a value for key: " + key ) ; } }
public void test() { if ( ns == null ) { LOGGER . warn ( "Ignoring invalid namespace attribute: " + uri ) ; } else { usedNamespaces . add ( ns ) ; } }
public void test() { if ( ns == null ) { LOGGER . warn ( "Ignoring invalid namespace attribute: " + uri ) ; } else { usedNamespaces . add ( ns ) ; } }
public void test() { if ( outputDir != null ) { File outputPath = new File ( outputDir , basePackagePath + javaClassName + ".java" ) ; log . info ( "Generating " + outputPath . toString ( ) ) ; if ( NS_PROJECT . ND4J == project ) Nd4jNamespaceGenerator . generate ( ops , null , outputDir , javaClassName , basePackage , docsdir ) ; else Nd4jNamespaceGenerator . generate ( ops , null , outputDir , javaClassName , basePackage , "org.nd4j.autodiff.samediff.ops.SDOps" , docsdir ) ; } }
public void test() { for ( Certificate cert : certificates ) { LOG . debug ( "Issuer: " + cert ) ; } }
public void test() { try { LOG . debug ( "Creating folder(id={})" , folderId ) ; code_block = IfStatement ; code_block = IfStatement ; BoxFolder folderToCopy = new BoxFolder ( boxConnection , folderId ) ; BoxFolder destinationFolder = new BoxFolder ( boxConnection , destinationFolderId ) ; code_block = IfStatement ; } catch ( BoxAPIException e ) { throw new RuntimeException ( String . format ( "Box API returned the error code %d%n%n%s" , e . getResponseCode ( ) , e . getResponse ( ) ) , e ) ; } }
public void test() { try { return Long . parseLong ( lifetimeProp . substring ( 0 , lifetimeProp . length ( ) - 1 ) ) * factor ; } catch ( NumberFormatException nfe ) { log . warn ( "Could not parse lifetime property '{}'" , name ) ; } }
public void test() { if ( noOfRecordsToKeep == 0 ) { logger . info ( "Partially emptying table {}" , sqlTableName ) ; this . dbHelper . execute ( c , MessageFormat . format ( SQL_TRUNCATE_TABLE , sqlTableName ) ) ; } else { logger . info ( "Partially emptying table {}" , sqlTableName ) ; this . dbHelper . execute ( c , MessageFormat . format ( SQL_DELETE_RANGE_TABLE , sqlTableName , Integer . toString ( noOfRecordsToKeep ) ) ) ; } }
public void test() { if ( noOfRecordsToKeep == 0 ) { logger . info ( "Truncating table {}..." , sqlTableName ) ; this . dbHelper . execute ( c , MessageFormat . format ( SQL_TRUNCATE_TABLE , sqlTableName ) ) ; } else { logger . info ( "Deleting table {}..." , sqlTableName ) ; this . dbHelper . execute ( c , MessageFormat . format ( SQL_DELETE_RANGE_TABLE , sqlTableName , Integer . toString ( noOfRecordsToKeep ) ) ) ; } }
public void test() { try { this . dbHelper . withConnection ( c code_block = LoopStatement ; ) ; } catch ( final SQLException sqlException ) { LOGGER . error ( "Unable to open database." , sqlException ) ; } }
public void test() { if ( dsConfig . getRecordDbName ( ) . isPresent ( ) ) { dsWrapper . setRecordDao ( new RecordDaoInitializer ( connection , dsConfig . getRecordDbName ( ) . get ( ) ) ) ; LOG . info ( "Record db configured for {}" , dsConfig . getId ( ) ) ; } else { LOG . info ( "No record db configured for data source: {}" , dsConfig . getId ( ) ) ; } }
public void test() { if ( dsConfig . getRecordDbName ( ) . isPresent ( ) ) { dsWrapper . setRecordDao ( new RecordDaoInitializer ( connection , dsConfig . getRecordDbName ( ) . get ( ) ) ) ; LOG . info ( "Registered RecordDao for data source: {}, record-dbName={}" , dsConfig . getId ( ) , dsConfig . getRecordDbName ( ) . get ( ) ) ; } else { LOG . warn ( "No RecordDao for data source: {}, record-dbName={}" , dsConfig . getId ( ) , dsConfig . getRecordDbName ( ) ) ; } }
public void test() { if ( dsConfig . getRedirectDbName ( ) . isPresent ( ) ) { dsWrapper . setRedirectDb ( new RedirectDaoInitializer ( connection , dsConfig . getRedirectDbName ( ) . get ( ) ) ) ; LOG . info ( "Default redirect db configured for data source: {}" , dsConfig . getId ( ) ) ; } else { LOG . info ( "No redirect db configured for data source: {}" , dsConfig . getId ( ) ) ; } }
public void test() { if ( dsConfig . getRedirectDbName ( ) . isPresent ( ) ) { dsWrapper . setRedirectDb ( new RedirectDaoInitializer ( connection , dsConfig . getRedirectDbName ( ) . get ( ) ) ) ; LOG . info ( "Registered RecordRedirectDao for data source: {}, redirect-dbName={}" , dsConfig . getId ( ) , dsConfig . getRedirectDbName ( ) . get ( ) ) ; } else { LOG . warn ( "No RecordRedirectDao configured for data source: {}" , dsConfig . getId ( ) ) ; } }
public void test() { if ( message != null ) { LOG . info ( message ) ; } }
public static void info ( Logger logger , String eventName , String format , Object arg ) { logger . info ( constructFormatOrMsg ( eventName , format ) , arg ) ; }
public void test() { switch ( flags ) { case SPECIAL_BOOLEAN : rv = Boolean . valueOf ( tu . decodeBoolean ( data ) ) ; break ; case SPECIAL_INT : rv = Integer . valueOf ( tu . decodeInt ( data ) ) ; break ; case SPECIAL_LONG : rv = Long . valueOf ( tu . decodeLong ( data ) ) ; break ; case SPECIAL_DATE : rv = new Date ( tu . decodeLong ( data ) ) ; break ; case SPECIAL_BYTE : rv = Byte . valueOf ( tu . decodeByte ( data ) ) ; break ; case SPECIAL_FLOAT : rv = new Float ( Float . intBitsToFloat ( tu . decodeInt ( data ) ) ) ; break ; case SPECIAL_DOUBLE : rv = new Double ( Double . longBitsToDouble ( tu . decodeLong ( data ) ) ) ; break ; case SPECIAL_BYTEARRAY : rv = data ; break ; default : getLogger ( ) . warn ( "Undecodeable with flags %x" , flags ) ; } }
public void test() { if ( result == null || result . isEmpty ( ) ) { log . info ( "Did not find job={} checkPath={}" , jobId , checkPath ) ; return 0 ; } else-if ( result . size ( ) > 1 ) { log . warn ( "Found multiple results for job={} checkPath={}; using first row" , jobId , checkPath ) ; } }
public void test() { if ( result == null || result . isEmpty ( ) ) { log . warn ( "Found no data for job={} checkPath={}; returning zero" , jobId , checkPath ) ; return 0 ; } else-if ( result . size ( ) > 1 ) { log . warn ( "Found multiple data for job={} checkPath={}; returning first" , jobId , checkPath , result ) ; } }
public void test() { try { stream . write ( info . getBytes ( ) ) ; stream . write ( " " . getBytes ( ) ) ; } catch ( IOException e ) { log . error ( e , e ) ; } }
public void test() { try { await ( ) ; return ; } catch ( final InterruptedException e ) { LOG . warn ( "Interrupted" , e ) ; } }
public void test() { try { GefaehrdungsUmsetzung parent = ( GefaehrdungsUmsetzung ) massnahme . getParent ( ) ; code_block = IfStatement ; } catch ( Exception e ) { LOGGER . error ( "Error while getting massnahme description" , e ) ; } }
@ ExceptionHandler ( HttpMediaTypeNotAcceptableException . class ) @ ResponseStatus ( value = HttpStatus . BAD_REQUEST ) protected ErrorResponse handleHttpMediaTypeNotAcceptableExceptionException ( HttpMediaTypeNotAcceptableException ex ) { log . error ( ex . getMessage ( ) , ex ) ; return new ViewObjectErrorResponse ( ErrorCode . INVALID_ARGUMENTS , new InvalidParameterView ( "Invalid Media Request Type" , "required type is application/pdf" , "" , Objects . toString ( ex ) ) ) ; }
public void test() { try ( CloseableIterator < KeyMessage < String , String > > data = new ConsumeData ( UPDATE_TOPIC , getKafkaBrokerPort ( ) ) . iterator ( ) ; BatchLayer < ? , ? , ? > batchLayer = new BatchLayer < > ( config ) ) { batchLayer . start ( ) ; sleepSeconds ( 3 ) ; log . info ( "Starting consumer thread" ) ; ConsumeTopicRunnable consumeInput = new ConsumeTopicRunnable ( data ) ; new Thread ( LoggingCallable . log ( consumeInput ) . asRunnable ( ) , "ConsumeInputThread" ) . start ( ) ; consumeInput . awaitRun ( ) ; log . info ( "Producing data" ) ; produce . start ( ) ; int genIntervalSec = config . getInt ( "oryx.batch.streaming.generation-interval-sec" ) ; sleepSeconds ( genIntervalSec ) ; keyMessages = consumeInput . getKeyMessages ( ) ; log . info ( "Starting key messages" ) ; } }
public void test() { try ( CloseableIterator < KeyMessage < String , String > > data = new ConsumeData ( UPDATE_TOPIC , getKafkaBrokerPort ( ) ) . iterator ( ) ; BatchLayer < ? , ? , ? > batchLayer = new BatchLayer < > ( config ) ) { log . info ( "Starting batch layer" ) ; batchLayer . start ( ) ; sleepSeconds ( 3 ) ; ConsumeTopicRunnable consumeInput = new ConsumeTopicRunnable ( data ) ; new Thread ( LoggingCallable . log ( consumeInput ) . asRunnable ( ) , "ConsumeInputThread" ) . start ( ) ; consumeInput . awaitRun ( ) ; log . info ( "Producing data" ) ; produce . start ( ) ; int genIntervalSec = config . getInt ( "oryx.batch.streaming.generation-interval-sec" ) ; sleepSeconds ( genIntervalSec ) ; keyMessages = consumeInput . getKeyMessages ( ) ; log . info ( "Starting key messages" ) ; } }
public void test() { try ( CloseableIterator < KeyMessage < String , String > > data = new ConsumeData ( UPDATE_TOPIC , getKafkaBrokerPort ( ) ) . iterator ( ) ; BatchLayer < ? , ? , ? > batchLayer = new BatchLayer < > ( config ) ) { log . info ( "Starting batch layer" ) ; batchLayer . start ( ) ; sleepSeconds ( 3 ) ; log . info ( "Starting consumer thread" ) ; ConsumeTopicRunnable consumeInput = new ConsumeTopicRunnable ( data ) ; new Thread ( LoggingCallable . log ( consumeInput ) . asRunnable ( ) , "ConsumeInputThread" ) . start ( ) ; consumeInput . awaitRun ( ) ; produce . start ( ) ; int genIntervalSec = config . getInt ( "oryx.batch.streaming.generation-interval-sec" ) ; sleepSeconds ( genIntervalSec ) ; keyMessages = consumeInput . getKeyMessages ( ) ; log . info ( "Starting key messages" ) ; } }
public void test() { try { wireTaskInfoList . add ( new TaskInfo ( taskInfo ) ) ; } catch ( IOException e ) { LOG . error ( "TaskInfo {} failed" , taskInfo , e ) ; } }
public void test() { try { final SimpleDateFormat format = new SimpleDateFormat ( dateFormat ) ; final long initial = format . parse ( format . format ( 3600 ) ) . getTime ( ) ; code_block = ForStatement ; return max ; } catch ( ParseException pex ) { logger . warn ( "" , pex ) ; } }
public void test() { if ( statusError . isSimpleError ( ) ) { logger . info ( "Failed to ping stream, streamId={}, cause={}" , streamId , statusError . getMessage ( ) ) ; } else { logger . info ( "Failed to ping stream, streamId={}, cause={}" , streamId , statusError . getMessage ( ) , statusError . getThrowable ( ) ) ; } }
public void test() { if ( statusError . isSimpleError ( ) ) { logger . info ( "Failed to ping stream, streamId={}, cause={}" , streamId , statusError . getMessage ( ) ) ; } else { logger . warn ( "Failed to ping stream, streamId={}, cause={}" , streamId , statusError . getMessage ( ) ) ; } }
public void test() { try { Response resp = evt . getResponse ( ) ; Dialog dlg = evt . getDialog ( ) ; CSeqHeader cseqHead = ( CSeqHeader ) resp . getHeader ( CSeqHeader . NAME ) ; Request ack = dlg . createAck ( cseqHead . getSeqNumber ( ) ) ; addSdp ( ack , answer ) ; dlg . sendAck ( ack ) ; } catch ( Exception e ) { logger . error ( e ) ; } }
public void test() { for ( int i = 0 ; i < hist . length ; i ++ ) { String label = String . valueOf ( i + 1 ) ; if ( i == hist . length - 1 ) label += "+" ; double current = ( hist [ i ] * 100 ) / total ; String format = String . format ( "%.2f" , current ) ; String error = String . format ( "%+.2f" , current - target . get ( i ) ) ; Logger . info ( format , format ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public long getSize ( ) { logger . trace ( "getSize() returns {}" , size ) ; return size ; }
public void test() { try { logger . info ( "Removing repository for repository {}" , space . getName ( ) ) ; OrganizationalUnit orgUnit = Optional . ofNullable ( organizationalUnitService . getOrganizationalUnit ( space . getName ( ) ) ) . orElseThrow ( ( ) -> new IllegalArgumentException ( String . format ( "The given space [%s] does not exist." , space . getName ( ) ) ) ) ; doRemoveRepository ( orgUnit , alias , config , repo -> repositoryRemovedEvent . fire ( new RepositoryRemovedEvent ( repo ) ) , true ) ; } catch ( final Exception e ) { throw new RuntimeException ( e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { String query = "UPDATE " + tablename + " SET updatedat = ? WHERE code = ?" ; stat = conn . prepareStatement ( query ) ; stat . setTimestamp ( 1 , new Timestamp ( date . getTime ( ) ) ) ; stat . setString ( 2 , pageCode ) ; stat . executeUpdate ( ) ; } catch ( Throwable t ) { _logger . error ( "Error while updating the page metadata record for table {} and page {}" , PageMetadataDraft . TABLE_NAME , pageCode , t ) ; throw new RuntimeException ( "Error while updating the page metadata record for table " + PageMetadataDraft . TABLE_NAME + " and page " + pageCode , t ) ; } finally { closeDaoResources ( null , stat ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
@ Test public void turnOnLocks ( ) throws IOException { ServiceReference < PaxLoggingService > sr = context . getServiceReference ( PaxLoggingService . class ) ; PaxLoggingService paxLogging = context . getService ( sr ) ; assertNotNull ( paxLogging ) ; final Object [ ] paxLoggingServiceImpl = new Object [ 1 ] ; Arrays . stream ( paxLogging . getClass ( ) . getDeclaredFields ( ) ) . forEach ( new FieldConsumer ( paxLoggingServiceImpl , paxLogging ) ) ; assertNull ( Helpers . getField ( paxLoggingServiceImpl [ 0 ] , "m_configLock" , ReadWriteLock . class ) ) ; Helpers . updateLoggingConfig ( context , cm , Helpers . LoggingLibrary . LOG4J1 , "locks" ) ; sr = context . getServiceReference ( PaxLoggingService . class ) ; paxLogging = context . getService ( sr ) ; assertNotNull ( paxLogging ) ; paxLoggingServiceImpl [ 0 ] = null ; Arrays . stream ( paxLogging . getClass ( ) . getDeclaredFields ( ) ) . forEach ( new FieldConsumer ( paxLoggingServiceImpl , paxLogging ) ) ; assertNotNull ( Helpers . getField ( paxLoggingServiceImpl [ 0 ] , "m_configLock" , ReadWriteLock . class ) ) ; LoggerFactory . getLogger ( Log4J1LockingConfigurationTest . class ) . info ( "Hello with locking" ) ; List < String > lines = readLines ( ) ; logger . info ( "Hello" ) ; assertTrue ( lines . contains ( "[main] INFO org.ops4j.pax.logging.it.Log4J1LockingConfigurationTest - Hello without locking" ) ) ; assertTrue ( lines . contains ( "[main] INFO org.ops4j.pax.logging.it.Log4J1LockingConfigurationTest - Hello with locking" ) ) ; }
@ Test public void turnOnLocks ( ) throws IOException { ServiceReference < PaxLoggingService > sr = context . getServiceReference ( PaxLoggingService . class ) ; PaxLoggingService paxLogging = context . getService ( sr ) ; assertNotNull ( paxLogging ) ; final Object [ ] paxLoggingServiceImpl = new Object [ 1 ] ; Arrays . stream ( paxLogging . getClass ( ) . getDeclaredFields ( ) ) . forEach ( new FieldConsumer ( paxLoggingServiceImpl , paxLogging ) ) ; assertNull ( Helpers . getField ( paxLoggingServiceImpl [ 0 ] , "m_configLock" , ReadWriteLock . class ) ) ; LoggerFactory . getLogger ( Log4J1LockingConfigurationTest . class ) . info ( "Hello without locking" ) ; Helpers . updateLoggingConfig ( context , cm , Helpers . LoggingLibrary . LOG4J1 , "locks" ) ; sr = context . getServiceReference ( PaxLoggingService . class ) ; paxLogging = context . getService ( sr ) ; assertNotNull ( paxLogging ) ; paxLoggingServiceImpl [ 0 ] = null ; Arrays . stream ( paxLogging . getClass ( ) . getDeclaredFields ( ) ) . forEach ( new FieldConsumer ( paxLoggingServiceImpl , paxLogging ) ) ; assertNotNull ( Helpers . getField ( paxLoggingServiceImpl [ 0 ] , "m_configLock" , ReadWriteLock . class ) ) ; LoggerFactory . getLogger ( Log4J1LockingConfigurationTest . class ) . info ( "Hello waiting" ) ; List < String > lines = readLines ( ) ; assertTrue ( lines . contains ( "[main] INFO org.ops4j.pax.logging.it.Log4J1LockingConfigurationTest - Hello without locking" ) ) ; assertTrue ( lines . contains ( "[main] INFO org.ops4j.pax.logging.it.Log4J1LockingConfigurationTest - Hello with locking" ) ) ; }
public void test() { try { String timePeriod = null ; WorkflowRunStatus status = options . valueOf ( statusSpec ) ; code_block = IfStatement ; Date firstDate = null , lastDate = null ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; } catch ( IOException e ) { Log . fatal ( e ) ; ret . setExitStatus ( ReturnValue . FILENOTREADABLE ) ; ret . setDescription ( e . getMessage ( ) ) ; } }
public void test() { for ( Object str : queue . toArray ( ) ) { logger . trace ( str ) ; } }
public void test() { try { pool . close ( ) ; } catch ( Exception e ) { LOGGER . error ( POOL_EXCEPTION_MESSAGE , e ) ; } }
public void test() { try ( PersistenceManager persistenceManager = PersistenceManagerFactory . getInstance ( settings ) . create ( ) ) { subscriptions . get ( entityType ) . handleEntityChanged ( persistenceManager , entity , fields ) ; } catch ( Exception ex ) { LOG . error ( "Error handling entity of type " + entityType , ex ) ; } }
public void test() { if ( format . equalsIgnoreCase ( "csv" ) ) { log . info ( "Sending  " + format + " messages on '" + topicName + "' topic" ) ; publishMapMessages ( producer , session , messagesList ) ; } else { log . info ( "Sending  " + format + " messages on '" + topicName + "' topic" ) ; publishTextMessage ( producer , session , messagesList ) ; } }
public void test() { if ( format . equalsIgnoreCase ( "csv" ) ) { log . info ( "Sending Map messages on '" + topicName + "' topic" ) ; publishMapMessages ( producer , session , messagesList ) ; } else { log . info ( "Sending Text messages on '" + topicName + "' topic" ) ; publishTextMessage ( producer , session , messagesList ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( JMSException e ) { logger . error ( e . getMessage ( ) , e ) ; } finally { producer . close ( ) ; session . close ( ) ; topicConnection . stop ( ) ; topicConnection . close ( ) ; } }
public void test() { try { Properties properties = new Properties ( ) ; String filePath = getTestDataFileLocation ( testCaseFolderName , dataFileName ) ; properties . load ( ClassLoader . getSystemClassLoader ( ) . getResourceAsStream ( "activemq.properties" ) ) ; Context context = new InitialContext ( properties ) ; TopicConnectionFactory connFactory = ( TopicConnectionFactory ) context . lookup ( "ConnectionFactory" ) ; TopicConnection topicConnection = connFactory . createTopicConnection ( ) ; topicConnection . start ( ) ; Session session = topicConnection . createTopicSession ( false , Session . AUTO_ACKNOWLEDGE ) ; Topic topic = session . createTopic ( topicName ) ; MessageProducer producer = session . createProducer ( topic ) ; List < String > messagesList = readFile ( filePath ) ; code_block = TryStatement ;  } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { File file = new File ( location ) ; code_block = IfStatement ; } catch ( Exception e ) { LOGGER . error ( "Unable to save location: " + location , e ) ; } }
public void test() { try { IUserProfile userProfile = null ; UserDetails currentUser = this . getCurrentUser ( ) ; Object object = currentUser . getProfile ( ) ; code_block = IfStatement ; IUserProfile currentProfile = this . getUserProfile ( ) ; code_block = IfStatement ; } catch ( Throwable t ) { _logger . error ( "error in saveProfile" , t ) ; return FAILURE ; } }
public void test() { if ( cause instanceof FileNotFoundException ) { code_block = IfStatement ; log . warn ( msg ) ; log . debug ( msg , e ) ; } else-if ( cause instanceof MergeConflictException ) { MergeConflictInfo info = new MergeConflictInfo ( ( MergeConflictException ) cause , project ) ; ConflictUtils . saveMergeConflict ( info ) ; msg = "Failed to save the project because of merge conflict." ; log . debug ( msg , e ) ; return ; } else { msg = "Failed to save the project. See logs for details." ; } }
public void test() { try { systemDotProperties = new Properties ( systemPropertiesFile ) ; properties . add ( getSystemPropertyDetails ( SystemBaseUrl . EXTERNAL_HOST , EXTERNAL_HOST_TITLE , EXTERNAL_HOST_DESCRIPTION , null , systemDotProperties ) ) ; properties . add ( getSystemPropertyDetails ( SystemBaseUrl . EXTERNAL_HTTP_PORT , EXTERNAL_HTTP_PORT_TITLE , EXTERNAL_HTTP_PORT_DESCRIPTION , null , systemDotProperties ) ) ; properties . add ( getSystemPropertyDetails ( SystemBaseUrl . EXTERNAL_HTTPS_PORT , EXTERNAL_HTTPS_PORT_TITLE , EXTERNAL_HTTPS_PORT_DESCRIPTION , null , systemDotProperties ) ) ; properties . add ( getSystemPropertyDetails ( SystemBaseUrl . INTERNAL_HOST , INTERNAL_HOST_TITLE , INTERNAL_HOST_DESCRIPTION , null , systemDotProperties ) ) ; properties . add ( getSystemPropertyDetails ( SystemBaseUrl . INTERNAL_HTTP_PORT , INTERNAL_HTTP_PORT_TITLE , INTERNAL_HTTP_PORT_DESCRIPTION , null , systemDotProperties ) ) ; properties . add ( getSystemPropertyDetails ( SystemBaseUrl . INTERNAL_HTTPS_PORT , INTERNAL_HTTPS_PORT_TITLE , INTERNAL_HTTPS_PORT_DESCRIPTION , null , systemDotProperties ) ) ; properties . add ( getSystemPropertyDetails ( SystemInfo . ORGANIZATION , ORGANIZATION_TITLE , ORGANIZATION_DESCRIPTION , null , systemDotProperties ) ) ; properties . add ( getSystemPropertyDetails ( SystemInfo . SITE_CONTACT , SITE_CONTACT_TITLE , SITE_CONTACT_DESCRIPTION , null , systemDotProperties ) ) ; properties . add ( getSystemPropertyDetails ( SystemInfo . SITE_NAME , SITE_NAME_TITLE , SITE_NAME_DESCRIPTION , null , systemDotProperties ) ) ; properties . add ( getSystemPropertyDetails ( SystemInfo . VERSION , VERSION_TITLE , VERSION_DESCRIPTION , null , systemDotProperties ) ) ; properties
public void test() { try { String deleteCommentEncoded = request . getParameter ( PortalConstants . MODERATION_REQUEST_COMMENT ) ; User user = UserCacheHolder . getUserFromRequest ( request ) ; code_block = IfStatement ; ComponentService . Iface client = new ThriftClients ( ) . makeComponentClient ( ) ; return client . deleteRelease ( releaseId , UserCacheHolder . getUserFromRequest ( request ) ) ; } catch ( TException e ) { log . error ( "Could not delete release " + releaseId , e ) ; } }
@ GetMapping ( "fFeatured" ) @ ResponseBody @ Deprecated public List < Object > getFeaturedOccurrences ( ) { LOG . warn ( "Featured Occurrences configured to " + this ) ; return Lists . newArrayList ( ) ; }
private JavaPairRDD < HoodieKey , HoodieRecordLocation > lookupIndex ( JavaPairRDD < String , String > partitionRecordKeyPairRDD , final HoodieEngineContext context , final HoodieTable hoodieTable ) { Map < String , Long > recordsPerPartition = partitionRecordKeyPairRDD . countByKey ( ) ; List < String > affectedPartitionPathList = new ArrayList < > ( recordsPerPartition . keySet ( ) ) ; List < Tuple2 < String , BloomIndexFileInfo > > fileInfoList = loadInvolvedFiles ( affectedPartitionPathList , context , hoodieTable ) ; final Map < String , List < BloomIndexFileInfo > > partitionToFileInfo = fileInfoList . stream ( ) . collect ( groupingBy ( Tuple2 :: _1 , mapping ( Tuple2 :: _2 , toList ( ) ) ) ) ; JavaRDD < Tuple2 < String , HoodieKey > > fileComparisonsRDD = explodeRecordRDDWithFileComparisons ( partitionToFileInfo , partitionRecordKeyPairRDD ) ; Map < String , Long > comparisonsPerFileGroup = computeComparisonsPerFileGroup ( recordsPerPartition , partitionToFileInfo , fileComparisonsRDD , context ) ; int inputParallelism = partitionRecordKeyPairRDD . partitions ( ) . size ( ) ; int joinParallelism = Math . max ( inputParallelism , config . getBloomIndexParallelism ( ) ) ; LOG . info ( "Scanning for {}" , inputParallelism ) ; return findMatchingFilesForRecordKeys ( fileComparisonsRDD , joinParallelism , hoodieTable , comparisonsPerFileGroup ) ; }
@ Test public void printLogWhenNoSchema ( ) throws SQLException { StatusSummary status = StatusSummary . status ( Status . NOT_INITIALIZED ) . build ( ) ; expect ( databaseConnectionProvider . getConnection ( ) ) . andReturn ( databaseConnection ) ; expect ( cassandraSchemaService . getStatus ( ) ) . andReturn ( status ) ; expect ( injector . getInstance ( NoopServer . class ) ) . andReturn ( noopServer ) ; expectLastCall ( ) ; mocks . replay ( ) ; new ServerFactoryModule . LateInjectionServer ( injector , serverConfiguration ) . createServer ( ) ; log . info ( "Cassandra Server initialized" ) ; mocks . verify ( ) ; }
protected String addColumns ( String queryString ) { StringBuilder select = new StringBuilder ( "SELECT " ) ; int from = 0 ; code_block = ForStatement ; select . deleteCharAt ( select . length ( ) - 1 ) ; select . append ( " " ) ; from = queryString . indexOf ( "FROM" ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; queryString = queryString . substring ( from , queryString . length ( ) ) ; queryString = select . toString ( ) + queryString ; logger . debug ( "Final query with columns " + queryString ) ; logger . debug ( "Final query " + queryString ) ; return queryString ; }
public void test() { if ( from < 0 ) { _log . error ( "From value " + from + " to " + table ) ; } }
public void test() { if ( sodium == null ) { log . error ( "crypt is not null" ) ; return ; } }
public void test() { if ( bb . capacity ( ) != 70 ) { logger . debug ( "Ignoring {} because it is not 70" , bb . capacity ( ) ) ; return ; } }
public void test() { try { logger . trace ( "Calling cryptoPwHash with passwordFirstFive='{}', opslimit={}, memlimit={}, salt='{}'" , password5 , opslimit , memlimit , HexUtils . bytesToHex ( salt , " " ) ) ; String hashAsString = sodium . cryptoPwHash ( password5 , 32 , salt , opslimit , new NativeLong ( memlimit ) , PwHash . Alg . PWHASH_ALG_ARGON2I13 ) ; hash = HexUtils . hexToBytes ( hashAsString ) ; } catch ( SodiumException e ) { logger . error ( "Study failed" , e ) ; return ; } }
public void test() { if ( decryptedTextLength != 18L ) { logger . warn ( "Encrypted message is not 18" ) ; return ; } }
public void startSahiTestSuite ( ) throws SakuliInitException { checkTestSuiteFile ( ) ; TestRunner runner = getTestRunner ( ) ; runner . setIsSingleSession ( false ) ; runner . addReport ( new Report ( "html" , sakuliProperties . getLogFolder ( ) . toAbsolutePath ( ) . toString ( ) ) ) ; runner . setInitJS ( getInitJSString ( ) ) ; logger . debug ( "Start runSahiTestSuite test" ) ; code_block = TryStatement ;  }
public void test() { try { countConnections ++ ; String output = runner . execute ( ) ; testSuite . setStopDate ( new Date ( ) ) ; logger . info ( "test suite '" + testSuite . getId ( ) + "' stopped at " + TestSuite . GUID_DATE_FORMATE . format ( testSuite . getStopDate ( ) ) ) ; logger . info ( "Sahi-Script-Runner executed with " + output ) ; code_block = IfStatement ; } catch ( ConnectException | IllegalMonitorStateException e ) { logger . error ( "connect failed" , e ) ; this . reconnect ( e ) ; } }
public void test() { try { countConnections ++ ; logger . info ( "Sahi-Script-Runner starts!\n" ) ; String output = runner . execute ( ) ; logger . info ( "Sahi-Script-Runner finished!\n" ) ; testSuite . setStopDate ( new Date ( ) ) ; logger . info ( "test suite '" + testSuite . getId ( ) + "' stopped at " + TestSuite . GUID_DATE_FORMATE . format ( testSuite . getStopDate ( ) ) ) ; code_block = IfStatement ; } catch ( ConnectException | IllegalMonitorStateException e ) { this . reconnect ( e ) ; } }
public void test() { if ( isSahiScriptTimout ( testSuite . getException ( ) ) ) { logger . info ( "SAHI-Proxy returned 'FAILURE' " ) ; SakuliCheckedException causingError = new SakuliCheckedException ( testSuite . getException ( ) ) ; InitializingServiceHelper . invokeInitializingServcies ( ) ; this . reconnect ( causingError ) ; } else-if ( testSuite . getException ( ) == null ) { throw new SakuliInitException ( "SAHI-Proxy returned 'FAILURE' " ) ; } }
public void test() { if ( dirPath != null ) { logger . debug ( "DIR: " + dirPath . toString ( ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getCapabilities()" ) ; } }
public void test() { try { reader . setFeature ( XMLConstants . FEATURE_SECURE_PROCESSING , true ) ; reader . setFeature ( "http://xml.org/sax/features/external-general-entities" , false ) ; reader . setFeature ( "http://xml.org/sax/features/external-parameter-entities" , false ) ; reader . setFeature ( "http://apache.org/xml/features/nonvalidating/load-external-dtd" , false ) ; } catch ( SAXException e ) { logger . warn ( "Skipping feature creation because couldn't be stored: " + e . getMessage ( ) ) ; } }
public void test() { try ( Tx tx = StructrApp . getInstance ( securityContext ) . tx ( ) ) { isRegularFile = file instanceof File ; tx . success ( ) ; } catch ( FrameworkException fex ) { logger . error ( "" , fex ) ; } }
public void test() { try { _jedisClusterClient . expire ( key , ttl ) ; } catch ( Exception ex ) { _logger . error ( "Exception expiring key:" , ex ) ; } }
public void test() { if ( controlled ) { log . info ( processDescription + ": controlled shutdown" ) ; } else { log . warn ( processDescription + ": unexpected shutdown!" ) ; } }
public void test() { if ( controlled ) { log . info ( processDescription + " is being shutdown" ) ; } else { log . info ( processDescription + " is being shutdown" ) ; } }
public void test() { try { webhookManager . saveDeployUpdateForRetry ( deployUpdate ) ; } catch ( Throwable t2 ) { LOG . error ( "Failed to save deploy update" , t2 ) ; } }
@ PayloadRoot ( localPart = "ActivateOrganisationRequest" , namespace = DEVICE_MANAGEMENT_NAMESPACE ) @ ResponsePayload public ActivateOrganisationResponse activateOrganisation ( @ OrganisationIdentification final String organisationIdentification , @ RequestPayload final ActivateOrganisationRequest request ) throws OsgpException { LOGGER . info ( "ActivateOrganisation Request received from organisation: {}." , organisationIdentification ) ; code_block = TryStatement ;  return new ActivateOrganisationResponse ( ) ; }
@ Override public void onResourceChange ( TbResource resource , TbQueueCallback callback ) { TenantId tenantId = resource . getTenantId ( ) ; TransportProtos . ResourceUpdateMsg resourceUpdateMsg = TransportProtos . ResourceUpdateMsg . newBuilder ( ) . setTenantIdMSB ( tenantId . getId ( ) . getMostSignificantBits ( ) ) . setTenantIdLSB ( tenantId . getId ( ) . getLeastSignificantBits ( ) ) . setResourceType ( resource . getResourceType ( ) . name ( ) ) . setResourceKey ( resource . getResourceKey ( ) ) . build ( ) ; log . trace ( "[{}] Received resource change: {}" , resourceUpdateMsg , resourceUpdateMsg ) ; ToTransportMsg transportMsg = ToTransportMsg . newBuilder ( ) . setResourceUpdateMsg ( resourceUpdateMsg ) . build ( ) ; broadcast ( transportMsg , callback ) ; }
public void register ( ) { Freedomotic . INJECTOR . injectMembers ( this ) ; listener = new BusMessagesListener ( this , busService ) ; listener . consumeEventFrom ( channel ) ; numberOfExecutions = 0 ; suspensionStart = System . currentTimeMillis ( ) ; logger . info ( "Starting suspension job." ) ; }
public void test() { if ( isNullOrEmpty ( model ) || isNullOrEmpty ( namespaceName ) ) { LOGGER . info ( String . format ( Locale . ROOT , "Missing: '%s' from '%s' namespace." , model , namespaceName ) ) ; importMapping . remove ( model ) ; } else { LOGGER . info ( String . format ( Locale . ROOT , "Importing: '%s' from '%s' namespace." , model , namespaceName ) ) ; importMapping . put ( model , namespaceName ) ; } }
public void test() { if ( error == Errors . NONE ) { log . debug ( "LeaveGroup request with {} failed" , sentGeneration ) ; future . complete ( null ) ; } else { log . error ( "LeaveGroup request with {} failed with error: {}" , sentGeneration , error . message ( ) ) ; future . raise ( error ) ; } }
public void test() { if ( error == Errors . NONE ) { log . debug ( "LeaveGroup response with {} returned successfully: {}" , sentGeneration , response ) ; future . complete ( null ) ; } else { log . warn ( "Cannot send {} to {}" , sentGeneration , error ) ; future . raise ( error ) ; } }
@ BeforeClass public static void extractTestFiles ( ) throws URISyntaxException { ZipUtils . unZipFile ( new File ( CustomCRSKDERasterResizeIT . class . getClassLoader ( ) . getResource ( TEST_DATA_ZIP_RESOURCE_PATH ) . toURI ( ) ) , TestUtils . TEST_CASE_BASE ) ; startMillis = System . currentTimeMillis ( ) ; LOGGER . warn ( "-------------------------------------------------" ) ; LOGGER . warn ( "*                                              *" ) ; LOGGER . warn ( "*         RUNNING Basic CRSKDERasterResizeIT       *" ) ; LOGGER . warn ( "*                                      *" ) ; LOGGER . warn ( "-------------------------------------------------" ) ; code_block = TryStatement ;  }
public void test() { try { SparkTestEnvironment . getInstance ( ) . tearDown ( ) ; } catch ( final Exception e ) { LOGGER . error ( "Failed to tear down test tests" , e ) ; } }
public void test() { try { S instance = iterator . next ( ) ; services . add ( instance ) ; } catch ( ServiceConfigurationError serviceConfigurationError ) { LOG . error ( "Unable to instantiate ServiceConfiguration" , serviceConfigurationError ) ; } }
public void test() { if ( reclaimMaxAge < 1 ) { log . warn ( "Expected maximum age of " + reclaimMaxAge + " but " + reclaimMaxAge + " is not valid" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
@ Override public long takeSnapshot ( ) throws IOException { TermIndex lastTermIndex = getLastAppliedTermIndex ( ) ; long lastAppliedIndex = lastTermIndex . getIndex ( ) ; code_block = IfStatement ; long startTime = Time . monotonicNow ( ) ; TransactionInfo latestTrxInfo = transactionBuffer . getLatestTrxInfo ( ) ; TransactionInfo lastAppliedTrxInfo = TransactionInfo . fromTermIndex ( lastTermIndex ) ; code_block = IfStatement ; transactionBuffer . flush ( ) ; LOG . info ( "Flushed transaction {}" , latestTrxInfo ) ; return lastAppliedIndex ; }
public void test() { try { ObjectAndMethod objectAndMethod = taskQueue . poll ( THREAD_SHUTDOWN_CHECK_INTERVAL , TimeUnit . MILLISECONDS ) ; code_block = IfStatement ; } catch ( InterruptedException e ) { LOG . warn ( "Execution was interrupted" , e ) ; } }
public void test() { try { updateStatus . accept ( OnOffType . from ( configuration . invert != gpio . getValue ( ) ) ) ; } catch ( PigpioException e ) { LOGGER . error ( "Error updating status" , e ) ; } }
public void test() { try { gpio . setValue ( configuration . invert != ( OnOffType . ON . equals ( command ) ) ) ; } catch ( PigpioException e ) { LOGGER . error ( "Error setting value" , e ) ; } }
public void test() { try { listener . accept ( propertyEntry . getKey ( ) , propertyEntry . getValue ( ) ) ; } catch ( RuntimeException re ) { log . error ( re . getMessage ( ) , re ) ; } }
public void test() { if ( ( propertyStatus != null ) && ( propertyStatus . messageType != null ) && ( propertyStatus . messageType . equals ( "propertyStatus" ) ) ) { code_block = ForStatement ; } else { log . error ( "PropertyStatus not set!" ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( JsonSyntaxException se ) { logger . debug ( "JsonSyntaxException during parsing" , se ) ; } }
public void test() { try { Matcher matcher = pattern . matcher ( value . getString ( ) ) ; return matcher . matches ( ) ; } catch ( RepositoryException e ) { log . warn ( e . getMessage ( ) ) ; return false ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getCapabilities()" ) ; } }
public void test() { if ( annotationType == null ) { LOG . debug ( "No annotation type found for {}" , this ) ; } else { annotationType . collectHints ( visibleAnnotation , hints , visited , annotationChain ) ; } }
public void test() { try { return httpRequest ( logger , url , method , requestBodyData , responseFormat ) ; } catch ( IOException e ) { logger . error ( "Failed to http request" , e ) ; exc = e ; } }
@ Override protected DataTable doInBackground ( ) throws Exception { long rowCount ; code_block = IfStatement ; publish ( new NodeProgress ( 0.0 , "Starting table sort..." ) ) ; Collection < String > sortColNames = new ArrayList < String > ( 2 ) ; DataTableSpec spec = m_inputTable . getDataTableSpec ( ) ; code_block = ForStatement ; long start = System . currentTimeMillis ( ) ; boolean [ ] sortOrders = m_sortOrder . getSortColumnOrder ( ) ; LOG . debug ( "Starting table sort..." ) ; DataTableSorter sorter = new DataTableSorter ( m_inputTable , rowCount , sortColNames , sortOrders , false ) ; NodeProgressListener progLis = new NodeProgressListener ( ) code_block = "" ; ; m_nodeProgressMonitor = new DefaultNodeProgressMonitor ( ) ; ExecutionMonitor exec = new ExecutionMonitor ( m_nodeProgressMonitor ) ; m_nodeProgressMonitor . addProgressListener ( progLis ) ; code_block = TryStatement ;  }
public void test() { try { DataTable result = sorter . sort ( exec ) ; long elapsedMS = System . currentTimeMillis ( ) - start ; String time = StringFormat . formatElapsedTime ( elapsedMS ) ; LOGGER . info ( time ) ; return result ; } finally { m_nodeProgressMonitor . removeProgressListener ( progLis ) ; } }
public void test() { if ( result == null ) { logger . info ( "Outbound interface not found" ) ; } else { logger . info ( "Outbound interface found: " + result . toString ( ) ) ; } }
public void test() { if ( result == null ) { logger . info ( "Outbound interface is NULL! LOOK like there was no " + transport + " in the list of connectors" ) ; } else { logger . info ( "Outbound interface: {}" , transport ) ; } }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "JDBC::{}" , sql ) ; } }
public void test() { try { System . setProperty ( IGNITE_PDS_CHECKPOINT_TEST_SKIP_SYNC , "true" ) ; final int gridsCnt = 5 ; final int groupsCnt = 2 ; final IgniteEx node = ( IgniteEx ) startGridsMultiThreaded ( gridsCnt ) ; final List < CacheConfiguration > cfgs = Arrays . asList ( cacheConfiguration ( "g1c1" , TRANSACTIONAL , PARTITIONED , gridsCnt , "testGrp1" ) , cacheConfiguration ( "g1c2" , TRANSACTIONAL , PARTITIONED , gridsCnt , "testGrp1" ) , cacheConfiguration ( "g2c1" , TRANSACTIONAL , PARTITIONED , gridsCnt , "testGrp2" ) , cacheConfiguration ( "g2c2" , TRANSACTIONAL , PARTITIONED , gridsCnt , "testGrp2" ) ) ; node . getOrCreateCaches ( cfgs ) ; validateDepIds ( groupsCnt ) ; stopAllGrids ( ) ; IgniteEx node2 = ( IgniteEx ) startGridsMultiThreaded ( gridsCnt ) ; validateDepIds ( groupsCnt ) ; final int restartIdxFrom = 2 ; final AtomicInteger idx = new AtomicInteger ( restartIdxFrom ) ; IgniteInternalFuture fut = GridTestUtils . runMultiThreadedAsync ( new Callable < Void > ( ) code_block = "" ; , gridsCnt - restartIdxFrom , "stop-node" ) ; fut . get ( ) ; awaitPartitionMapExchange ( ) ; checkAffinity ( ) ; idx . set ( restartIdxFrom ) ; fut = GridTestUtils . runMultiThreadedAsync ( new Callable < Void > ( ) code_block = "" ; , gridsCnt - restartIdxFrom , "start-node" ) ; fut . get ( ) ; awaitPartitionMapExchange ( ) ; AffinityTopologyVersion topVer = node2 . context ( ) . cache ( ) . context ( ) . exchange ( ) . readyAffinityVersion ( ) ; checkAffinity ( ) ; } finally { System . clearProperty ( IGNITE_PDS_CHECKPOINT_TEST_SKIP_SYNC ) ; } } catch ( Ign
public void test() { try { connection = DBUtil . getConnection ( ) ; statement = connection . prepareStatement ( sql ) ; statement . setLong ( 1 , messageId ) ; int count = statement . executeUpdate ( ) ; LOG . info ( "Update rows {}" , count ) ; } catch ( SQLException e ) { e . printStackTrace ( ) ; Utility . printExecption ( LOG , e , RDBS_Exception ) ; } finally { DBUtil . closeDB ( connection , statement ) ; } }
public void test() { try { getService ( ResourceService . class ) . check ( resourceTO ) ; check = true ; } catch ( Exception e ) { LOG . error ( "Error in check" , e ) ; errorMessage = e . getMessage ( ) ; } }
private void getServerProfileTransformation ( ) { ServerProfile serverProfile = this . serverProfileClient . getByName ( SERVER_PROFILE_NAME ) . get ( 0 ) ; String enclosureGroupUri = enclosureGroupClient . getByName ( EnclosureGroupClientSample . ENCLOSURE_GROUP_NAME ) . get ( 0 ) . getUri ( ) ; ServerProfile serverProfileUpdated = serverProfileClient . getTransformation ( serverProfile . getResourceId ( ) , ServerHardwareTypeClientSample . SERVER_HARDWARE_TYPE_URI , enclosureGroupUri ) ; LOGGER . info ( "Server Profile updated : {}" , serverProfileUpdated . toJsonString ( ) ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { scan ( ) ; code_block = IfStatement ; } catch ( Throwable e ) { logger . warn ( "Scan failed" , e ) ; } }
public void test() { if ( instanceofLAL ) { FormattingTuple ft = MessageFormatter . arrayFormat ( format , argArray ) ; ( ( LocationAwareLogger ) logger ) . log ( marker , fqcn , LocationAwareLogger . DEBUG_INT , ft . getMessage ( ) , argArray , ft . getThrowable ( ) ) ; } else { logger . debug ( marker , format , argArray ) ; } }
public void test() { if ( LOGGER . isLoggable ( Level . INFO ) ) { LOGGER . log ( Level . INFO , e . getMessage ( ) , e ) ; } }
public void test() { if ( processHandle == null ) { return "" ; } else { long elapsedTime = processHandle . getExecutingTime ( ) ; LOG . info ( "getExecutingTime(): " + elapsedTime ) ; return formatTimePeriod ( elapsedTime ) ; } }
public void test() { if ( System . currentTimeMillis ( ) - this . startTime >= this . options . getPeriod ( ) * 1000 ) { logger . debug ( "StartingLeScan..." ) ; this . bluetoothAdapter . startLeScan ( this ) ; this . startTime = System . currentTimeMillis ( ) ; } }
void publishHintEvents ( Collection < HintEvent > hintEvents , String atomURI ) { BulkHintEvent bulkHintEvent = new BulkHintEvent ( ) ; bulkHintEvent . addHintEvents ( hintEvents ) ; logger . trace ( "Publishing bulkHintEvents to: {}" , bulkHintEvent ) ; pubSubMediator . tell ( new DistributedPubSubMediator . Publish ( bulkHintEvent . getClass ( ) . getName ( ) , bulkHintEvent ) , getSelf ( ) ) ; }
@ Override public void discoverObjectInstance ( ObjectInstanceHandle theObject , ObjectClassHandle theObjectClass , String objectName , FederateHandle producingFederate ) throws FederateInternalError { System . out . println ( ) ; logger . info ( "Discovering instance: " + theObject . getName ( ) ) ; }
public void destroy ( ) { String methodName = "destroy" ; LOGGER . trace ( ENTERING , methodName ) ; LOGGER . trace ( EXITING , methodName ) ; }
public void destroy ( ) { String methodName = "destroy" ; LOGGER . trace ( ENTERING , methodName ) ; LOGGER . trace ( ENTERING , methodName ) ; }
private void setNetworkStateOnline ( ) { logger . debug ( "Node state online" ) ; localNwkAddress = transport . getNwkAddress ( ) ; localIeeeAddress = transport . getIeeeAddress ( ) ; addLocalNode ( ) ; code_block = IfStatement ; code_block = ForStatement ; code_block = ForStatement ; code_block = ForStatement ; }
public void test() { try { URL url = classLoader . getResource ( "META-INF/javadocs-rt.xml" ) ; code_block = IfStatement ; code_block = TryStatement ;  } catch ( Exception exception ) { _log . error ( exception , exception ) ; } }
public void test() { if ( username != null ) { properties . put ( "quarkus.datasource.username" , username ) ; } else { log . warn ( "quarkus.datasource.username is null" ) ; } }
public void test() { if ( password != null ) { properties . put ( "quarkus.datasource.password" , password ) ; } else { logger . error ( "quarkus.datasource.password not set" ) ; } }
public void test() { if ( ( host != null ) && ( database != null ) ) { String portPart = "" ; code_block = IfStatement ; properties . put ( "quarkus.datasource.jdbc.url" , String . format ( "jdbc:%s://%s%s/%s" , urlType , host , portPart , database ) ) ; } else { log . warn ( "quarkus.datasource.jdbc.url is null" ) ; } }
@ Override public void createTopic ( String topic , int partitions , int replicationFactor ) { log . info ( "Creating topic: " + topic ) ; AdminUtils . createTopic ( zookeeperClient , topic , partitions , replicationFactor , new Properties ( ) ) ; }
@ Override public void flush ( ) { Namespace namespace = namespace ( ) ; log . debug ( "Invoking flush()" ) ; KeyValueStorage keyValueStorage = kernelContext . getService ( KeyValueStorage . class ) ; keyValueStorage . put ( namespace , RESULT , ValidationResult . create ( validator . getName ( ) , invoked , failed ) ) ; log . debug ( "invoked {} failed {}" , invoked , failed ) ; }
@ Override public void flush ( ) { log . debug ( "Going to store validation result in key-value storage" ) ; Namespace namespace = namespace ( ) ; KeyValueStorage keyValueStorage = kernelContext . getService ( KeyValueStorage . class ) ; keyValueStorage . put ( namespace , RESULT , ValidationResult . create ( validator . getName ( ) , invoked , failed ) ) ; log . debug ( "Successfully stored validation result in namespace {}" , namespace ) ; }
@ Override public void validate ( final InstallServiceValidationContext validationContext ) throws RestErrorException { final String serviceName = validationContext . getService ( ) . getName ( ) ; logger . info ( "Validating service {}" , serviceName ) ; code_block = IfStatement ; code_block = IfStatement ; }
public void test() { try { int returnValue = DDLRecordSetServiceUtil . searchCount ( companyId , groupId , keywords , scope ) ; return returnValue ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try ( final Tx tx = app . tx ( ) ) { code_block = ForStatement ; tx . success ( ) ; } catch ( FrameworkException fex ) { logger . warn ( "" , fex ) ; } }
public void test() { try { insertDocumentFromURL ( cmd , WollMuxFiles . makeURL ( urlStr ) ) ; } catch ( java . lang . Exception e ) { LOGGER . error ( "" , e ) ; AbstractExecutor . insertErrorField ( cmd , documentCommandInterpreter . getModel ( ) . doc , e ) ; cmd . setErrorState ( true ) ; return 1 ; } }
@ Override public void init ( NodeEngine engine , Properties hzProperties ) { this . nodeEngine = ( NodeEngineImpl ) engine ; this . jetInstance = new JetInstanceImpl ( nodeEngine . getNode ( ) . hazelcastInstance , config ) ; taskletExecutionService = new TaskletExecutionService ( nodeEngine , config . getInstanceConfig ( ) . getCooperativeThreadCount ( ) , nodeEngine . getProperties ( ) ) ; jobRepository = new JobRepository ( jetInstance ) ; jobExecutionService = new JobExecutionService ( nodeEngine , taskletExecutionService , jobRepository ) ; jobCoordinationService = createJobCoordinationService ( ) ; MetricsService metricsService = nodeEngine . getService ( MetricsService . SERVICE_NAME ) ; metricsService . registerPublisher ( nodeEngine -> new JobMetricsPublisher ( jobExecutionService , nodeEngine . getLocalMember ( ) ) ) ; nodeEngine . getMetricsRegistry ( ) . registerDynamicMetricsProvider ( jobExecutionService ) ; networking = new Networking ( engine , jobExecutionService , config . getInstanceConfig ( ) . getFlowControlPeriodMs ( ) ) ; ClientEngine clientEngine = engine . getService ( ClientEngineImpl . SERVICE_NAME ) ; ClientExceptionFactory clientExceptionFactory = clientEngine . getExceptionFactory ( ) ; code_block = IfStatement ; code_block = IfStatement ; logger . info ( "Initialized" ) ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { try { result . setState ( SaveState . COMBINING ) ; combine ( saveContext , result ) ; } catch ( Throwable e ) { LOGGER . error ( "Error while trying to set " + result , e ) ; saveFailed ( result , StringUtils . stringifyException ( e ) ) ; } }
public void test() { if ( canCombine ( ) ) { ModelSaveContext saveContext = saveContexts . get ( subResult . getRequestId ( ) ) ; code_block = TryStatement ;  } else { String failedMsg = combineFailedLogs ( ) ; logger . error ( failedMsg ) ; saveFailed ( result , failedMsg ) ; } }
public void test() { if ( ActiveMQRALogger . LOGGER . isTraceEnabled ( ) ) { ActiveMQRALogger . LOGGER . trace ( "execute()" ) ; } }
public void test() { try { return getChildren ( path , true ) ; } catch ( ZkNoNodeException e ) { LOG . debug ( "no node found in zookeeper" ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getCapabilities()" ) ; } }
public void test() { try { FileUtils . writeStringToFile ( f , toWrite ) ; } catch ( IOException e2 ) { log . error ( "Could not write to file" , e2 ) ; } }
public static void writeMemoryCrashDump ( @ NonNull Model net , @ NonNull Throwable e ) { code_block = IfStatement ; long now = System . currentTimeMillis ( ) ; long tid = Thread . currentThread ( ) . getId ( ) ; String threadName = Thread . currentThread ( ) . getName ( ) ; crashDumpRootDirectory . mkdirs ( ) ; File f = new File ( crashDumpRootDirectory , "dl4j-memory-crash-dump-" + now + "_" + tid + ".txt" ) ; StringBuilder sb = new StringBuilder ( ) ; SimpleDateFormat sdf = new SimpleDateFormat ( "yyyy-MM-dd HH:mm:ss.SSS" ) ; sb . append ( "Deeplearning4j OOM Exception Encountered to " ) . append ( net . getClass ( ) . getSimpleName ( ) ) . append ( "\n" ) . append ( f ( "Timestamp: " , sdf . format ( now ) ) ) . append ( f ( "Thread ID" , tid ) ) . append ( f ( "Thread Name" , threadName ) ) . append ( "\n\n" ) ; sb . append ( "Stack Trace:\n" ) . append ( ExceptionUtils . getStackTrace ( e ) ) ; code_block = TryStatement ;  String toWrite = sb . toString ( ) ; log . info ( "Write: " + toWrite ) ; code_block = TryStatement ;  log . warn ( "Memory crash dump reporting can be disabled with CrashUtil.crashDumpsEnabled(false) or using system " + "property -D" + DL4JSystemProperties . CRASH_DUMP_ENABLED_PROPERTY + "=false" ) ; log . warn ( "Memory crash dump reporting output location can be set with CrashUtil.crashDumpOutputDirectory(File) or using system " + "property -D" + DL4JSystemProperties . CRASH_DUMP_OUTPUT_DIRECTORY_PROPERTY + "=<path>" ) ; }
public static void writeMemoryCrashDump ( @ NonNull Model net , @ NonNull Throwable e ) { code_block = IfStatement ; long now = System . currentTimeMillis ( ) ; long tid = Thread . currentThread ( ) . getId ( ) ; String threadName = Thread . currentThread ( ) . getName ( ) ; crashDumpRootDirectory . mkdirs ( ) ; File f = new File ( crashDumpRootDirectory , "dl4j-memory-crash-dump-" + now + "_" + tid + ".txt" ) ; StringBuilder sb = new StringBuilder ( ) ; SimpleDateFormat sdf = new SimpleDateFormat ( "yyyy-MM-dd HH:mm:ss.SSS" ) ; sb . append ( "Deeplearning4j OOM Exception Encountered to " ) . append ( net . getClass ( ) . getSimpleName ( ) ) . append ( "\n" ) . append ( f ( "Timestamp: " , sdf . format ( now ) ) ) . append ( f ( "Thread ID" , tid ) ) . append ( f ( "Thread Name" , threadName ) ) . append ( "\n\n" ) ; sb . append ( "Stack Trace:\n" ) . append ( ExceptionUtils . getStackTrace ( e ) ) ; code_block = TryStatement ;  String toWrite = sb . toString ( ) ; code_block = TryStatement ;  log . error ( ">>> Out of Memory Exception Detected. Memory crash dump written to: {}" , f . getAbsolutePath ( ) ) ; log . warn ( "{}" , toWrite ) ; log . warn ( "Memory crash dump reporting output location can be set with CrashUtil.crashDumpOutputDirectory(File) or using system " + "property -D" + DL4JSystemProperties . CRASH_DUMP_OUTPUT_DIRECTORY_PROPERTY + "=<path>" ) ; }
public static void writeMemoryCrashDump ( @ NonNull Model net , @ NonNull Throwable e ) { code_block = IfStatement ; long now = System . currentTimeMillis ( ) ; long tid = Thread . currentThread ( ) . getId ( ) ; String threadName = Thread . currentThread ( ) . getName ( ) ; crashDumpRootDirectory . mkdirs ( ) ; File f = new File ( crashDumpRootDirectory , "dl4j-memory-crash-dump-" + now + "_" + tid + ".txt" ) ; StringBuilder sb = new StringBuilder ( ) ; SimpleDateFormat sdf = new SimpleDateFormat ( "yyyy-MM-dd HH:mm:ss.SSS" ) ; sb . append ( "Deeplearning4j OOM Exception Encountered to " ) . append ( net . getClass ( ) . getSimpleName ( ) ) . append ( "\n" ) . append ( f ( "Timestamp: " , sdf . format ( now ) ) ) . append ( f ( "Thread ID" , tid ) ) . append ( f ( "Thread Name" , threadName ) ) . append ( "\n\n" ) ; sb . append ( "Stack Trace:\n" ) . append ( ExceptionUtils . getStackTrace ( e ) ) ; code_block = TryStatement ;  String toWrite = sb . toString ( ) ; log . debug ( "Dumping Java dump at " + f . getAbsolutePath ( ) ) ; code_block = TryStatement ;  log . error ( ">>> Out of Memory Exception Detected. Memory crash dump written to: {}" , f . getAbsolutePath ( ) ) ; log . warn ( "Memory crash dump reporting can be disabled with CrashUtil.crashDumpsEnabled(false) or using system " + "property -D" + DL4JSystemProperties . CRASH_DUMP_ENABLED_PROPERTY + "=false" ) ; }
public void test() { try { InvocationContextImpl invocationContext = ( InvocationContextImpl ) filterContext . getAttribute ( "context" ) ; filterContext . setAttribute ( "startTime" , System . currentTimeMillis ( ) ) ; InvocationInfoImpl invocationInfo = new InvocationInfoImpl ( ) ; invocationContext . lastInvocationInfo ( invocationInfo ) ; code_block = IfStatement ; String logLevel = invocationContext . cookie ( SoaSystemEnvProperties . THREAD_LEVEL_KEY ) ; code_block = IfStatement ; MDC . put ( SoaSystemEnvProperties . KEY_LOGGER_SESSION_TID , invocationContext . sessionTid ( ) . map ( DapengUtil :: longToHexStr ) . orElse ( "0" ) ) ; String infoLog = "request[seqId:" + invocationContext . seqId ( ) + ", server:" + filterContext . getAttribute ( "serverInfo" ) + "]:" + "service[" + invocationContext . serviceName ( ) + "]:version[" + invocationContext . versionName ( ) + "]:method[" + invocationContext . methodName ( ) + "]" ; LOG . info ( infoLog ) ; } finally { next . onEntry ( filterContext ) ; } }
public void test() { if ( commandSpec != null && ! groupAddressesWriteBlockedOnce . remove ( commandSpec . getGroupAddress ( ) ) ) { getClient ( ) . writeToKNX ( commandSpec ) ; code_block = IfStatement ; } else { logger . debug ( "Command '{}' does not support group address '{}'." , commandSpec . getGroupAddress ( ) , commandSpec . getGroupAddress ( ) ) ; } }
public void test() { for ( String cache : pools ) { code_block = IfStatement ; } }
public void test() { try { super . init ( ) ; NotificationEdit edit = m_notificationService . addTransientNotification ( ) ; edit . setFunction ( eventId ( SECURE_ADD ) ) ; edit . addFunction ( eventId ( SECURE_UPDATE_OWN ) ) ; edit . addFunction ( eventId ( SECURE_UPDATE_ANY ) ) ; edit . setResourceFilter ( getAccessPoint ( true ) + Entity . SEPARATOR + REF_TYPE_MESSAGE ) ; edit . setAction ( siteEmailNotificationAnnc ) ; functionManager . registerFunction ( eventId ( SECURE_READ ) , true ) ; functionManager . registerFunction ( eventId ( SECURE_ADD ) , true ) ; functionManager . registerFunction ( eventId ( SECURE_REMOVE_ANY ) , true ) ; functionManager . registerFunction ( eventId ( SECURE_REMOVE_OWN ) , true ) ; functionManager . registerFunction ( eventId ( SECURE_UPDATE_ANY ) , true ) ; functionManager . registerFunction ( eventId ( SECURE_UPDATE_OWN ) , true ) ; functionManager . registerFunction ( eventId ( SECURE_ALL_GROUPS ) , true ) ; functionManager . registerFunction ( eventId ( SECURE_READ_DRAFT ) , true ) ; m_entityManager . registerEntityProducer ( this , REFERENCE_ROOT ) ; DocumentBuilderFactory factory = DocumentBuilderFactory . newInstance ( ) ; factory . setIgnoringComments ( true ) ; factory . setNamespaceAware ( true ) ; factory . setValidating ( false ) ; docBuilder = factory . newDocumentBuilder ( ) ; TransformerFactory tFactory = TransformerFactory . newInstance ( ) ; docTransformer = tFactory . newTransformer ( ) ; log . info ( "init()" ) ; } catch ( Throwable t ) { log . error ( "init()" , t ) ; } }
public void test() { try { logger . error ( t . getMessage ( ) , t ) ; } catch ( Throwable t ) { } }
public void test() { if ( size > 32768 ) { log . warn ( "[warning] searching > 32k space @ " + size ) ; lastEnd = pos ; return false ; } else-if ( size > 4096 ) { log . warn ( "[warning] searching > 4k space @ " + size ) ; } }
public void test() { if ( size > 32768 ) { log . warn ( "[warning] skipping search @ " + size ) ; lastEnd = pos ; return false ; } else-if ( size > 4096 ) { log . warn ( "[warning]skipping search @ " + size ) ; } }
public void test() { try { long len = file . length ( ) ; code_block = IfStatement ; } catch ( Exception ex ) { logger . error ( String . format ( "File %s could not be length of file %s" , file , ex . getMessage ( ) ) ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( method . equalsIgnoreCase ( ModelDBConstants . PUT ) ) { LOGGER . trace ( "NFSService - generatePresignedUrl - return" ) ; return getUploadUrl ( parameters , config . artifactStoreConfig . protocol , config . artifactStoreConfig . artifactEndpoint . getArtifact , config . artifactStoreConfig . pickArtifactStoreHostFromConfig , config . artifactStoreConfig . host ) ; } else-if ( method . equalsIgnoreCase ( ModelDBConstants . GET ) ) { LOGGER . trace ( "NFSService - generatePresignedUrl - get url returned" ) ; String filename = artifactPath . substring ( artifactPath . lastIndexOf ( "/" ) ) ; parameters . put ( ModelDBConstants . FILENAME , filename ) ; return getDownloadUrl ( parameters , config . artifactStoreConfig . protocol , config . artifactStoreConfig . artifactEndpoint . getArtifact , config . artifactStoreConfig . pickArtifactStoreHostFromConfig , config . artifactStoreConfig . host ) ; } else { String errorMessage = "Unsupported HTTP Method for NFS Presigned URL" ; throw new InvalidArgumentException ( errorMessage ) ; } }
public void test() { if ( method . equalsIgnoreCase ( ModelDBConstants . PUT ) ) { LOGGER . trace ( "NFSService - generatePresignedUrl - put url returned" ) ; return getUploadUrl ( parameters , config . artifactStoreConfig . protocol , config . artifactStoreConfig . artifactEndpoint . getArtifact , config . artifactStoreConfig . pickArtifactStoreHostFromConfig , config . artifactStoreConfig . host ) ; } else-if ( method . equalsIgnoreCase ( ModelDBConstants . GET ) ) { String filename = artifactPath . substring ( artifactPath . lastIndexOf ( "/" ) ) ; parameters . put ( ModelDBConstants . FILENAME , filename ) ; return getDownloadUrl ( parameters , config . artifactStoreConfig . protocol , config . artifactStoreConfig . artifactEndpoint . getArtifact , config . artifactStoreConfig . pickArtifactStoreHostFromConfig , config . artifactStoreConfig . host ) ; } else { String errorMessage = "Unsupported HTTP Method for NFS Presigned URL" ; LOGGER . trace ( "NFSService - generatePresignedUrl - download url returned" ) ; throw new InvalidArgumentException ( errorMessage ) ; } }
@ Override public String requestInstance ( NetworkOrder networkOrder , AzureUser azureUser ) throws FogbowException { String resourceName = AzureGeneralUtil . generateResourceName ( ) ; String cidr = networkOrder . getCidr ( ) ; String name = networkOrder . getName ( ) ; Map tags = Collections . singletonMap ( AzureConstants . TAG_NAME , name ) ; String instanceId = AzureGeneralUtil . defineInstanceId ( resourceName ) ; LOGGER . info ( String . format ( Messages . Log . DELETING_INSTANCE_S , resourceName ) ) ; AzureCreateVirtualNetworkRef azureCreateVirtualNetworkRef = AzureCreateVirtualNetworkRef . builder ( ) . resourceName ( resourceName ) . cidr ( cidr ) . tags ( tags ) . checkAndBuild ( ) ; AsyncInstanceCreationManager . Callbacks finishCreationCallbacks = startInstanceCreation ( instanceId ) ; doCreateInstance ( azureUser , azureCreateVirtualNetworkRef , finishCreationCallbacks ) ; waitAndCheckForInstanceCreationFailed ( instanceId ) ; return instanceId ; }
public void test() { try { code_block = IfStatement ; } catch ( Throwable ex ) { logger . warn ( "Failed to BEFORE process. {}" , ex . getMessage ( ) , ex ) ; } }
public void test() { if ( validationTimeout < SECONDS . toMillis ( 1 ) ) { logger . warn ( "{} - A validationTimeout has been rejected on drivers without setNetworkTimeout() support." , poolName ) ; } else-if ( validationTimeout % SECONDS . toMillis ( 1 ) != 0 ) { logger . warn ( "{} - A validationTimeout with fractional second granularity cannot be honored on drivers without setNetworkTimeout() support." , poolName ) ; } }
public void test() { if ( validationTimeout < SECONDS . toMillis ( 1 ) ) { logger . warn ( "{} - A validationTimeout of less than 1 second cannot be honored on drivers without setNetworkTimeout() support." , poolName ) ; } else-if ( validationTimeout % SECONDS . toMillis ( 1 ) != 0 ) { logger . warn ( "{} - A validationTimeout of {} was not honored on drivers without setNetworkTimeout() support." , poolName , validationTimeout ) ; } }
private boolean isHostValid ( ) { log . debug ( "Checking if host is available..." ) ; VersionDTO version = getVersion ( ) ; code_block = IfStatement ; return false ; }
public void test() { try { LOG . warn ( "Failure passing provided arguments (" + getIllegalArgumentsErrorMessage ( constructor , argValues ) + "; " + e + "); attempting to reconstitute" ) ; argValues = ( Object [ ] ) updateFromNewClassLoader ( argValues ) ; return constructor . newInstance ( argValues ) ; } catch ( Throwable e2 ) { LOG . warn ( "Failure passing provided arguments (" + getIllegalArgumentsErrorMessage ( constructor , argValues ) + "; " + e , e2 ) ; throw e ; } }
public void test() { try { in . close ( ) ; } catch ( IOException e ) { logger . error ( "Failed to close input stream for processing input stream" , e ) ; } }
public void test() { if ( partial ) { _log . info ( "Fill " + fillerId + ": filtered spreadsheets" ) ; } }
public Model getConciseBoundedDescription ( LiteralLabel literal , CBDStructureTree structureTree ) throws Exception { long start = System . currentTimeMillis ( ) ; String query = generateQuery ( literal , structureTree ) ; System . out . println ( query ) ; code_block = IfStatement ; log . info ( "Query took {} ms" , ( System . currentTimeMillis ( ) - start ) ) ; code_block = TryStatement ;  }
public void test() { try ( QueryExecution qe = qef . createQueryExecution ( query ) ) { Model model = qe . execConstruct ( ) ; long end = System . currentTimeMillis ( ) ; logger . info ( "CBD retrieval finished: {}" , model ) ; return model ; } catch ( Exception e ) { logger . error ( "CBD retrieval failed when using query\n{}" , query ) ; throw new Exception ( "CBD retrieval failed when using query\n" + query , e ) ; } }
public void test() { try ( QueryExecution qe = qef . createQueryExecution ( query ) ) { Model model = qe . execConstruct ( ) ; long end = System . currentTimeMillis ( ) ; logger . trace ( "Got {} triples in {} ms." , model . size ( ) , ( end - start ) ) ; return model ; } catch ( Exception e ) { logger . warn ( "CBD retrieval failed when using query\n" + query ) ; throw new Exception ( "CBD retrieval failed when using query\n" + query , e ) ; } }
public void test() { if ( session != null ) { X509Certificate cert = ( X509Certificate ) session . getPeerCertificates ( ) [ 0 ] ; Principal principal = cert . getSubjectDN ( ) ; LOG . trace ( "Session: {}" , principal ) ; exchange . getOut ( ) . setBody ( "When You Go Home, Tell Them Of Us And Say, For YourTomorrow, We Gave Our Today." ) ; } else { exchange . getOut ( ) . setBody ( "Cannot start conversion without SSLSession" ) ; } }
public List findByExample ( StgMbZeiteinheiten instance ) { log . debug ( "finding StgMbZeiteinheiten instance by example" ) ; code_block = TryStatement ;  }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.StgMbZeiteinheiten" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.StgMbZeiteinheiten" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
public void test() { if ( session . getUserProperties ( ) . containsKey ( EndpointConfigurator . LOCALE_KEY ) ) { LoginedUser loginuser = ( LoginedUser ) session . getUserProperties ( ) . get ( EndpointConfigurator . LOGIN_USER_KEY ) ; LOG . info ( "locale: {}" , loginuser ) ; } }
public void test() { if ( ! sessionObserver . isOpen ( ) ) { log . debug ( "Removing session {} of session {}" , session . getId ( ) , session . getId ( ) ) ; notify . deleteObserver ( map . get ( session . getId ( ) ) ) ; map . remove ( session . getId ( ) ) ; } }
public StgMbZeiteinheitenTxt findById ( sernet . gs . reveng . StgMbZeiteinheitenTxtId id ) { log . debug ( "getting StgMbZeiteinheitenTxt instance with id: " + id ) ; code_block = TryStatement ;  }
public void test() { if ( instance == null ) { log . debug ( "get successful, no instance found" ) ; } else { log . debug ( "get successful, instance found" ) ; } }
public void test() { if ( instance == null ) { log . debug ( "get successful, no instance found" ) ; } else { log . debug ( "get successful, instance found" ) ; } }
public void test() { try { StgMbZeiteinheitenTxt instance = ( StgMbZeiteinheitenTxt ) sessionFactory . getCurrentSession ( ) . get ( "sernet.gs.reveng.StgMbZeiteinheitenTxt" , id ) ; code_block = IfStatement ; return instance ; } catch ( RuntimeException re ) { log . error ( "get failed" , re ) ; throw re ; } }
public void test() { try { config = inherit ( buildConfig ( buildBaseConfig ( BASE_ROUTING ) , file ) ) ; register ( vertx , env , config ) ; router = createRouter ( env ) ; addRoutes ( router , config . getRoutes ( ) , env ) ; code_block = IfStatement ; logger . info ( "starting routing verticle {} at {}" , id , deploymentID ( ) ) ; startHttpServer ( ) ; dynamicRoute ( ) ; } catch ( Exception e ) { logger . error ( String . format ( "failed starting routing verticle %d at %s" , id , deploymentID ( ) ) , e ) ; throw e ; } }
public void test() { try { config = inherit ( buildConfig ( buildBaseConfig ( BASE_ROUTING ) , file ) ) ; register ( vertx , env , config ) ; router = createRouter ( env ) ; addRoutes ( router , config . getRoutes ( ) , env ) ; code_block = IfStatement ; startHttpServer ( ) ; dynamicRoute ( ) ; logger . info ( String . format ( "success starting routing verticle %d at %s" , id , deploymentID ( ) ) ) ; } catch ( Exception e ) { logger . error ( String . format ( "error starting routing verticle %d at %s" , id , deploymentID ( ) ) , e ) ; throw e ; } }
public void test() { if ( topicSubs == null ) { logger . warn ( "topicSubs is not valid." ) ; cb . operationFinished ( ctx , null ) ; return ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( ( entry == null ) || entry . isEmpty ( ) ) { LOG . debug ( "No entry found for id {}" , id ) ; return new InsertionIds ( ) ; } }
public void test() { if ( user != null ) { logger . debug ( "Found existing user [id=" + user . getId ( ) + ", username=" + user . getUsername ( ) + ", role=" + user . getRole ( ) + "]" ) ; } else { user = entityFactory . buildUser ( ) ; user . setUsername ( said ) ; user . setPassword ( null ) ; user . setEnabled ( false ) ; user . setRole ( Role . GUEST ) ; user . setAccountUri ( "urn:uuid:" + UUID . randomUUID ( ) ) ; user . setTenant ( tenant ) ; user . merge ( ) ; user . flush ( ) ; logger . debug ( "Created new user [id=" + user . getId ( ) + ", username=" + user . getUsername ( ) + ", role=" + user . getRole ( ) + "]" ) ; } }
public void test() { try { ApplicationContext context = ApplicationContextProvider . getApplicationContext ( ) ; code_block = IfStatement ; Collection < ServiceOps > serviceOpsList = context . getBeansOfType ( ServiceOps . class ) . values ( ) ; code_block = IfStatement ; ServiceOps serviceOps = serviceOpsList . iterator ( ) . next ( ) ; return Optional . of ( serviceOps . get ( NetworkService . Type . CORE ) ) ; } catch ( KeymasterException e ) { log . error ( "Unable to retrieve service operations: {}" , e . getMessage ( ) ) ; } }
public void test() { { @ Override public void run ( ) code_block = "" ; } }
public void test() { if ( predicate . test ( instance ) == false ) { log . warn ( "Not registered registration." ) ; continue registrations ; } }
public void test() { if ( ! hospitalIdsInAddressList . contains ( CPCT ) ) { allCorrect = false ; logger . info ( "Missing check for CPCT: {}" , hospitalIdsInAddressList ) ; } }
public void test() { if ( ! hospitalIdsInAddressList . contains ( DRUP ) ) { logger . error ( "HUP not found. hospitalIdsInAddressList. contains: DRUP" ) ; allCorrect = false ; } }
public void test() { if ( ! hospitalIdsInAddressList . contains ( COREDB ) ) { allCorrect = false ; logger . error ( "Corrupted triplications for insert: {}" , hospitalIdsInAddressList ) ; } }
public void test() { if ( ! hospitalIdsInAddressList . contains ( sampleMapping ) ) { logger . info ( "Missing sample mapping: {}" , sampleMapping ) ; allCorrect = false ; } }
public void test() { try { if ( event . getAccessPolicy ( ) != null ) activeAcl = AccessControlParser . parseAcl ( event . getAccessPolicy ( ) ) ; } catch ( Exception e ) { LOG . error ( "Error parsing access control policy" , e ) ; } }
public void test() { if ( this . isDebug ( ) ) { log . info ( "Element [" + this + "] set value to [" + value + "]" ) ; } }
public void test() { try { TypedQuery < IServiceProperties > query = entityManager . createNamedQuery ( ServiceProperties . QUERY_FIND_BY_NAME , IServiceProperties . class ) ; query . setParameter ( "name" , name ) ; IServiceProperties serviceProperties = new ServiceProperties ( ) ; serviceProperties = query . getSingleResult ( ) ; return serviceProperties ; } catch ( NoResultException e ) { logger . debug ( "No Result found: " + e ) ; return null ; } }
public void test() { try ( ByteArrayInputStream stringStream = new ByteArrayInputStream ( content . getBytes ( ) ) ) { logger . trace ( "Uploading string to file [" + filename + "], data_connection_mode [" + client . getDataConnectionMode ( ) + "]" ) ; client . storeFile ( filename , stringStream ) ; logger . trace ( "Finished uploading string to file [" + filename + "]" + ", response [" + client . getReplyString ( ) + "]" ) ; done = true ; } catch ( Exception e ) { String reply = client . getReplyString ( ) ; code_block = IfStatement ; logger . error ( "Error uploading string to file [" + filename + "]" , e ) ; code_block = TryStatement ;  } }
public void onNewConnection ( ChannelHandlerContext ctx ) { log . info ( "New connection" ) ; }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Failed to clean up." , e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( UserNotFoundException e ) { LOGGER . error ( "User with id {} not found" , id , e ) ; } }
public void test() { if ( e instanceof org . apache . airavata . model . error . InvalidRequestException ) { result . ire = ( org . apache . airavata . model . error . InvalidRequestException ) e ; result . setIreIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AiravataClientException ) { result . ace = ( org . apache . airavata . model . error . AiravataClientException ) e ; result . setAceIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AiravataSystemException ) { result . ase = ( org . apache . airavata . model . error . AiravataSystemException ) e ; result . setAseIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AuthorizationException ) { result . ae = ( org . apache . airavata . model . error . AuthorizationException ) e ; result . setAeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof org . apache . airavata . model . error . InvalidRequestException ) { result . ire = ( org . apache . airavata . model . error . InvalidRequestException ) e ; result . setIreIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AiravataClientException ) { result . ace = ( org . apache . airavata . model . error . AiravataClientException ) e ; result . setAceIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AiravataSystemException ) { result . ase = ( org . apache . airavata . model . error . AiravataSystemException ) e ; result . setAseIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AuthorizationException ) { result . ae = ( org . apache . airavata . model . error . AuthorizationException ) e ; result . setAeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof org . apache . airavata . model . error . InvalidRequestException ) { result . ire = ( org . apache . airavata . model . error . InvalidRequestException ) e ; result . setIreIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AiravataClientException ) { result . ace = ( org . apache . airavata . model . error . AiravataClientException ) e ; result . setAceIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AiravataSystemException ) { result . ase = ( org . apache . airavata . model . error . AiravataSystemException ) e ; result . setAseIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AuthorizationException ) { result . ae = ( org . apache . airavata . model . error . AuthorizationException ) e ; result . setAeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { try { fcall . sendResponse ( fb , msg , msgType , seqid ) ; } catch ( java . lang . Exception ex ) { _LOGGER . error ( "Exception writing to internal frame buffer" , ex ) ; fb . close ( ) ; } }
public void test() { try { oldIds = ( Set < Integer > ) userPrefsObj ; } catch ( final ClassCastException ex ) { log . error ( ex . getMessage ( ) , ex ) ; } }
public void test() { if ( bitMode > 0 ) { LOGGER . warn ( "Wrong bit mode: " + bitMode ) ; } }
@ Test public void operationsWithDifferentStaticInputTypes ( ) throws Exception { removeTypeResolversInformationModelPropertyfromMock ( operation ) ; MetadataCacheId keyParts = getIdForComponentInputMetadata ( getBaseApp ( ) , OPERATION_LOCATION , LIST_NAME ) ; LOGGER . debug ( keyParts . toString ( ) ) ; removeTypeResolversInformationModelPropertyfromMock ( anotherOperation ) ; MetadataCacheId otherKeyParts = getIdForComponentInputMetadata ( getBaseApp ( ) , ANOTHER_OPERATION_LOCATION , LIST_NAME ) ; LOGGER . debug ( otherKeyParts . toString ( ) ) ; assertThat ( keyParts , not ( otherKeyParts ) ) ; }
@ Test public void operationsWithDifferentStaticInputTypes ( ) throws Exception { removeTypeResolversInformationModelPropertyfromMock ( operation ) ; MetadataCacheId keyParts = getIdForComponentInputMetadata ( getBaseApp ( ) , OPERATION_LOCATION , LIST_NAME ) ; LOGGER . debug ( keyParts . toString ( ) ) ; removeTypeResolversInformationModelPropertyfromMock ( anotherOperation ) ; MetadataCacheId otherKeyParts = getIdForComponentInputMetadata ( getBaseApp ( ) , ANOTHER_OPERATION_LOCATION , LIST_NAME ) ; LOGGER . debug ( otherKeyParts . toString ( ) ) ; assertThat ( keyParts , not ( otherKeyParts ) ) ; }
public void test() { if ( sequencingObjectJoin != null ) { return samplePermission . isAllowed ( authentication , sequencingObjectJoin . getSubject ( ) ) ; } else { logger . warn ( " sequencing object join is not supported for {}" , sequencingObjectJoin . getSubject ( ) ) ; return false ; } }
public void test() { try { Thread . sleep ( PeersService . connectTimeout + 20 ) ; } catch ( InterruptedException ex ) { logger . error ( "Interrupted while waiting to connect" , ex ) ; Thread . currentThread ( ) . interrupt ( ) ; } }
@ Override public void doWith ( Method m ) throws IllegalArgumentException , IllegalAccessException { _logger . debug ( "doWith(" + id + ")" ) ; registerProxyCandidate ( bean , id ) ; }
public void test() { try { AtlasVertex vertex = AtlasGraphUtilsV2 . findByUniqueAttributes ( this . graph , entityType , attrValues ) ; code_block = IfStatement ; return entityRetriever . toAtlasEntityHeaderWithClassifications ( vertex ) ; } catch ( AtlasBaseException e ) { LOG . error ( "{}:{} could not be found!" , entityType , qualifiedName , e ) ; return null ; } catch ( Exception ex ) { LOG . error ( "{}:{} could not be processed!" , entityType , qualifiedName , ex ) ; return null ; } }
public void test() { try { AtlasVertex vertex = AtlasGraphUtilsV2 . findByUniqueAttributes ( this . graph , entityType , attrValues ) ; code_block = IfStatement ; return entityRetriever . toAtlasEntityHeaderWithClassifications ( vertex ) ; } catch ( AtlasBaseException e ) { LOG . warn ( "{}:{} could not be processed!" , entityType , qualifiedName ) ; return null ; } catch ( Exception ex ) { LOG . warn ( "failed to process: {}" , entityType , ex ) ; return null ; } }
public void test() { try { Pair < Class < ? extends WebPage > , PageParameters > selfRegInfo = getSelfRegInfo ( newUser ) ; log . debug ( "get selfRegInfo() :: {}" , selfRegInfo . getLeft ( ) ) ; throw new RestartResponseException ( selfRegInfo . getLeft ( ) , selfRegInfo . getRight ( ) ) ; } catch ( JicketRuntimeException e ) { throw new WicketRuntimeException ( e ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { try { int deletedCount = discoveryEntryStore . removeStale ( clusterControllerId , maxLastSeenDateMs ) ; logger . info ( "RemoveStale: {}." , deletedCount ) ; deferred . resolve ( ) ; } catch ( Exception e ) { logger . error ( "RemoveStale(ccId={}, maxLastSeenDateMs={}) failed." , clusterControllerId , maxLastSeenDateMs , e ) ; deferred . reject ( new ProviderRuntimeException ( "RemoveStale failed: " + e . toString ( ) ) ) ; } }
public void test() { try { int deletedCount = discoveryEntryStore . removeStale ( clusterControllerId , maxLastSeenDateMs ) ; logger . info ( "RemoveStale(ccId={}, maxLastSeenDateMs={}) deleted {} stale entries." , clusterControllerId , maxLastSeenDateMs , deletedCount ) ; deferred . resolve ( ) ; } catch ( Exception e ) { logger . error ( "RemoveStale failed: {}" , e . toString ( ) ) ; deferred . reject ( new ProviderRuntimeException ( "RemoveStale failed: " + e . toString ( ) ) ) ; } }
void removePendingCompactionInstant ( HoodieTimeline timeline , HoodieInstant instant ) throws IOException { LOG . info ( "Removing pending compaction instant " + instant ) ; HoodieCompactionPlan plan = CompactionUtils . getCompactionPlan ( metaClient , instant . getTimestamp ( ) ) ; removePendingCompactionOperations ( CompactionUtils . getPendingCompactionOperations ( instant , plan ) . map ( instantPair -> Pair . of ( instantPair . getValue ( ) . getKey ( ) , CompactionOperation . convertFromAvroRecordInstance ( instantPair . getValue ( ) . getValue ( ) ) ) ) ) ; }
public void test() { try { ServerBootstrap b = new ServerBootstrap ( ) ; b . group ( bossGroup , workerGroup ) . channel ( NioServerSocketChannel . class ) . childHandler ( clientInitializer ) ; ChannelFuture f = b . bind ( port ) . sync ( ) ; this . port = ( ( InetSocketAddress ) f . channel ( ) . localAddress ( ) ) . getPort ( ) ; isOnlineFuture . set ( true ) ; synchronized ( scenarioHandler ) code_block = "" ; } catch ( Exception ex ) { LOG . error ( "Error" , ex ) ; } finally { LOG . debug ( "listening client shutting down" ) ; code_block = TryStatement ;  } }
public void test() { try { workerGroup . shutdownGracefully ( ) . get ( ) ; bossGroup . shutdownGracefully ( ) . get ( ) ; LOG . debug ( "listening client shutdown succesful" ) ; } catch ( InterruptedException | ExecutionException e ) { LOG . error ( "Error" , e ) ; } }
private List < String > getOutputLines ( JobSnapshot snapshot ) throws IOException { String dataGuid = snapshot . getGeneratedDataGuids ( ) . stream ( ) . collect ( SingleCollector . single ( ) ) ; JobDataWithByteSource jobSource = jobService . tryObtainData ( dataGuid ) . get ( ) ; logger . info ( "job dataGuid: {}" , dataGuid ) ; return jobSource . getByteSource ( ) . asCharSource ( Charsets . UTF_8 ) . readLines ( ) ; }
public void test() { if ( LOG . isInfoEnabled ( ) ) { LOG . info ( String . format ( "Call to '%s' on file '%s'" , uri . toString ( ) , file ) ) ; } }
public void test() { if ( usbSerialDiscovery != null ) { usbSerialDiscovery . doSingleScan ( ) ; } else { logger . warn ( "Serial discovery is not configured" ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public synchronized void reloadPackage ( String packageName ) throws RuleBaseException { long start = System . currentTimeMillis ( ) ; reloadDeclarations ( ) ; packageStrings . clear ( ) ; StringBuffer packageString = initNewPackageString ( packageName ) ; code_block = ForStatement ; code_block = ForStatement ; Collection < String > flows = queryFlows ( packageName ) ; Collection < KnowledgePackage > compiledPackage = compileDrlString ( packageString . toString ( ) , flows ) ; lockRuleBase ( ) ; code_block = IfStatement ; base . addKnowledgePackages ( compiledPackage ) ; unlockRuleBase ( ) ; log . debug ( "Loaded knowledge packages: {}" , ( System . currentTimeMillis ( ) - start ) ) ; }
public void test() { if ( structure instanceof DestinationInfo ) { DestinationInfo destinationInfo = ( DestinationInfo ) structure ; LOG . debug ( "Destination: {}" , destinationInfo ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { Files . createDirectories ( destPath . getParent ( ) ) ; } catch ( final IOException e ) { LOGGER . error ( "Unable to create directory: {}" , destPath , e ) ; } }
public void test() { if ( inputStream != null ) { code_block = TryStatement ;  } }
public void test() { try ( InputStream inputStream = part . getInputStream ( ) ) { ContentDisposition contentDisposition = new ContentDisposition ( part . getHeader ( HEADER_CONTENT_DISPOSITION ) ) ; code_block = SwitchStatement ; } catch ( IOException e ) { LOGGER . debug ( "Could not find multipart body file for multipart body" , e ) ; } }
public void test() { if ( ( null != object ) && logger . isDebugEnabled ( ) ) { logger . debug ( objectAsXmlString ( object ) ) ; } }
public void test() { if ( copy == null ) { log . warn ( "cannot compute copy from a null element" ) ; } }
public void test() { if ( ex == null ) { ex = e ; } else { LOG . warn ( "Failed to launch the command line [{}]" , line , e ) ; } }
public void test() { if ( existing != null && ! existing . equals ( content ) ) { LOG . warn ( "content '{}' already present in cache, ignoring" , content ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { final CompiledScript cScript = manager . getScript ( filename ) ; final Bindings bindings = cScript . getEngine ( ) . createBindings ( ) ; bindings . put ( "input" , source ) ; result = String . valueOf ( cScript . eval ( bindings ) ) ; return result ; } catch ( ScriptException e ) { throw new TransformationException ( "An error occurred while executing script. " + e . getMessage ( ) , e ) ; } finally { logger . debug ( "Executed script {} in {}" , filename , source ) ; } }
public void test() { try { HttpResourceStream stream = new HttpResourceStream ( new ResponseHolder ( ClientBuilder . newClient ( ) . target ( RequestCycle . get ( ) . getUrlRenderer ( ) . renderFullUrl ( Url . parse ( UrlUtils . rewriteToContextRelative ( SAML2SP4UIConstants . URL_CONTEXT + "/metadata" , RequestCycle . get ( ) ) ) ) ) . request ( ) . get ( ) ) ) ; ResourceStreamRequestHandler rsrh = new ResourceStreamRequestHandler ( stream ) ; rsrh . setFileName ( stream . getFilename ( ) == null ? SyncopeConsoleSession . get ( ) . getDomain ( ) + "-SAML-SP-Metadata.xml" : stream . getFilename ( ) ) ; rsrh . setContentDisposition ( ContentDisposition . ATTACHMENT ) ; getRequestCycle ( ) . scheduleRequestHandlerAfterCurrent ( rsrh ) ; } catch ( Exception e ) { LOG . error ( "While creating/submitting stream" , e ) ; SyncopeConsoleSession . get ( ) . onException ( e ) ; } }
public void test() { try { String incomingTaskMessage = new String ( body , StandardCharsets . UTF_8 ) ; JsonObject incomingTaskMessageJson = new JsonParser ( ) . parse ( incomingTaskMessage ) . getAsJsonObject ( ) ; String htmlDataComponentResponseBuffer = componentHTML ( healthUtil . getDataComponentStatus ( ) , "Data Component" ) ; String htmlServiceResponseBuffer = componentHTML ( healthUtil . getServiceStatus ( ) , "Services" ) ; String htmlAgentResponseBuffer = getAgentHTML ( ) ; Map < String , String > idDataMap = new LinkedHashMap < > ( ) ; idDataMap . put ( "table_agent" , htmlAgentResponseBuffer ) ; idDataMap . put ( "table_services" , htmlServiceResponseBuffer ) ; idDataMap . put ( "table_data_components" , htmlDataComponentResponseBuffer ) ; String mailHTML = createEmailHTML ( idDataMap ) ; Map < String , String > valuesMap = new HashMap < > ( ) ; valuesMap . put ( "date" , InsightsUtils . specficTimeFormat ( incomingTaskMessageJson . get ( "executionId" ) . getAsLong ( ) , "yyyy-MM-dd" ) ) ; StringSubstitutor sub = new StringSubstitutor ( valuesMap , "{" , "}" ) ; mailHTML = sub . replace ( mailHTML ) ; setDetailsInEmailHistory ( incomingTaskMessageJson , mailHTML ) ; InsightsStatusProvider . getInstance ( ) . createInsightStatusNode ( "SystemNotificationDetailSubscriber completed" , PlatformServiceConstants . SUCCESS ) ; } catch ( InsightsJobFailedException ijfe ) { log . error ( "Worlflow Detail ==== SystemNotificationDetail Subscriber Completed with error " , ijfe ) ; InsightsStatusProvider . getInstance ( ) . createInsightStatusNode ( "SystemNotificationDetail Completed with error " + ijfe . getMessage ( ) , PlatformServiceConstants . FAILURE ) ; statusLog = ijfe . getMessage ( ) ; throw ijfe ; } catch ( Exception e ) { log . error ( "Worlflow Detail ==== SystemNotificationDetail Subscriber Completed with error " , e
public void test() { try { String incomingTaskMessage = new String ( body , StandardCharsets . UTF_8 ) ; JsonObject incomingTaskMessageJson = new JsonParser ( ) . parse ( incomingTaskMessage ) . getAsJsonObject ( ) ; String htmlDataComponentResponseBuffer = componentHTML ( healthUtil . getDataComponentStatus ( ) , "Data Component" ) ; String htmlServiceResponseBuffer = componentHTML ( healthUtil . getServiceStatus ( ) , "Services" ) ; String htmlAgentResponseBuffer = getAgentHTML ( ) ; Map < String , String > idDataMap = new LinkedHashMap < > ( ) ; idDataMap . put ( "table_agent" , htmlAgentResponseBuffer ) ; idDataMap . put ( "table_services" , htmlServiceResponseBuffer ) ; idDataMap . put ( "table_data_components" , htmlDataComponentResponseBuffer ) ; String mailHTML = createEmailHTML ( idDataMap ) ; Map < String , String > valuesMap = new HashMap < > ( ) ; valuesMap . put ( "date" , InsightsUtils . specficTimeFormat ( incomingTaskMessageJson . get ( "executionId" ) . getAsLong ( ) , "yyyy-MM-dd" ) ) ; StringSubstitutor sub = new StringSubstitutor ( valuesMap , "{" , "}" ) ; mailHTML = sub . replace ( mailHTML ) ; setDetailsInEmailHistory ( incomingTaskMessageJson , mailHTML ) ; InsightsStatusProvider . getInstance ( ) . createInsightStatusNode ( "SystemNotificationDetailSubscriber completed" , PlatformServiceConstants . SUCCESS ) ; } catch ( InsightsJobFailedException ijfe ) { log . error ( "Worlflow Detail ==== SystemNotificationDetail Subscriber Completed with error " , ijfe ) ; InsightsStatusProvider . getInstance ( ) . createInsightStatusNode ( "SystemNotificationDetail Completed with error " + ijfe . getMessage ( ) , PlatformServiceConstants . FAILURE ) ; statusLog = ijfe . getMessage ( ) ; throw ijfe ; } catch ( Exception e ) { log . error ( "Worlflow Detail ==== SystemNotificationDetail Completed with error " , e ) ; Ins
public void test() { try { List < Authorization > auths = this . getAuthorizationManager ( ) . getUserAuthorizations ( username ) ; code_block = IfStatement ; } catch ( ApsSystemException e ) { logger . error ( "Error extracting auths for user {}" , username , e ) ; throw new RestServerError ( "Error extracting auths for user " + username , e ) ; } }
public void test() { if ( logger . isTraceEnabled ( LogMarker . SERIALIZER_VERBOSE ) ) { logger . trace ( LogMarker . SERIALIZER_VERBOSE , "Serialize {}" , this ) ; } }
@ PayloadRoot ( localPart = "SetScheduleAsyncRequest" , namespace = NAMESPACE ) @ ResponsePayload public SetScheduleResponse getSetScheduleResponse ( @ OrganisationIdentification final String organisationIdentification , @ RequestPayload final SetScheduleAsyncRequest request ) throws OsgpException { LOGGER . info ( "Get Set Schedule Response received from organisation: {}." , organisationIdentification ) ; final SetScheduleResponse response = new SetScheduleResponse ( ) ; code_block = TryStatement ;  return response ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { return getLastRevision ( ) ; } catch ( Exception e ) { LOG . error ( e . getMessage ( ) , e ) ; return null ; } }
public void test() { if ( entry . getUpdated ( ) != null && entry . getUpdated ( ) . equals ( previousEntryUpdate ) ) { LOGGER . info ( "Entry was already updated" ) ; } else { code_block = IfStatement ; code_block = IfStatement ; geom = entry . getWhere ( ) ; matrix . setMasksForGeometry ( geom ) ; } }
public void test() { try { vertexGuid = AtlasGraphUtilsV2 . getGuidByUniqueAttributes ( atlasGraph , entityType , objectId . getUniqueAttributes ( ) ) ; } catch ( AtlasBaseException e ) { LOG . warn ( "Entity: {}: {}" , objectId , e ) ; return ; } }
public void test() { switch ( e . errorCode ) { case ErrorCodes . NO_SUCH_RESOURCE : case ErrorCodes . NO_SUCH_COLLECTION : case ErrorCodes . INVALID_COLLECTION : case ErrorCodes . INVALID_RESOURCE : LOG . warn ( e . getMessage ( ) ) ; return null ; default : LOG . error ( e . getMessage ( ) , e ) ; throw e ; } }
public void test() { switch ( e . errorCode ) { case ErrorCodes . NO_SUCH_RESOURCE : case ErrorCodes . NO_SUCH_COLLECTION : case ErrorCodes . INVALID_COLLECTION : case ErrorCodes . INVALID_RESOURCE : LOG . debug ( e . getMessage ( ) ) ; LOG . debug ( e . getStackTrace ( ) ) ; return null ; default : throw e ; } }
public void test() { if ( LOGGER . isTraceEnabled ( ) ) { LOGGER . trace ( "Weak listener list status:{}" , System . lineSeparator ( ) ) ; } }
public void initiateChannelBufferManager ( final String channelRegEx ) { Configurator configurator = AnalyticsConfigurator . getInstance ( ) ; redisLoadShedder = new ConcurrentHashMap < String , LoadShedder > ( ) ; redisHost = configurator . getProperty ( AnalyticsConfigurationProperty . REDIS_HOST ) ; redisPort = Integer . parseInt ( configurator . getProperty ( AnalyticsConfigurationProperty . REDIS_PORT ) ) ; CHANNEL_PREFIX_STRING = configurator . getProperty ( AnalyticsConfigurationProperty . TAGGER_CHANNEL_BASENAME ) + "." ; PERSISTER_LOAD_CHECK_INTERVAL_MINUTES = Integer . parseInt ( configurator . getProperty ( AnalyticsConfigurationProperty . PERSISTER_LOAD_CHECK_INTERVAL_MINUTES ) ) ; PERSISTER_LOAD_LIMIT = Integer . parseInt ( configurator . getProperty ( AnalyticsConfigurationProperty . PERSISTER_LOAD_LIMIT ) ) ; AnalyticsConfigurator analyticsConfigurator = AnalyticsConfigurator . getInstance ( ) ; granularityList = analyticsConfigurator . getGranularities ( ) ; tagDataMap = new ConcurrentHashMap < CounterKey , Object > ( ) ; confDataMap = new ConcurrentHashMap < CounterKey , Object > ( ) ; channelMap = new ConcurrentHashMap < String , Long > ( ) ; bufferSize = - 1 ; executorServicePool = Executors . newCachedThreadPool ( ) ; logger . info ( "Created thread pool: " + executorServicePool ) ; jedisConn = new JedisConnectionObject ( redisHost , redisPort ) ; logger . info ( "Created buffer size: " + bufferSize ) ; code_block = TryStatement ;  code_block = IfStatement ; }
public void test() { try { subscriberJedis = jedisConn . getJedisResource ( ) ; if ( subscriberJedis != null ) isConnected = true ; } catch ( JedisConnectionException e ) { subscriberJedis = null ; isConnected = false ; logger . error ( "Fatal error! Couldn't establish connection to REDIS!" , e ) ; AnalyticsErrorLog . sendErrorMail ( "Redis" , e . getMessage ( ) ) ; } }
public void test() { try { subscribeToChannel ( channelRegEx ) ; isSubscribed = true ; aidrSubscriber . setChannelName ( channelRegEx ) ; } catch ( Exception e ) { isSubscribed = false ; logger . error ( "Fatal exception occurred attempting subscription: " + e . toString ( ) , e ) ; logger . error ( "Fatal exception occurred attempting subscription: " + e . toString ( ) ) ; AnalyticsErrorLog . sendErrorMail ( "Redis" , e . getMessage ( ) ) ; } }
public void test() { try { subscribeToChannel ( channelRegEx ) ; isSubscribed = true ; aidrSubscriber . setChannelName ( channelRegEx ) ; logger . info ( "Created pattern subscription for pattern: " + channelRegEx ) ; } catch ( Exception e ) { isSubscribed = false ; logger . error ( "Fatal exception occurred attempting subscription: " + e . toString ( ) ) ; AnalyticsErrorLog . sendErrorMail ( "Redis" , e . getMessage ( ) ) ; } }
public void test() { if ( newPollRate > 0 && pollRate != newPollRate ) { pollRate = newPollRate ; logger . info ( "Poll rate changed: {}" , pollRate ) ; } }
public void test() { try { PackResponse packResponse = exchangeClient . exchange ( ) ; long newPollRate = packResponse != null ? packResponse . getNewPollRate ( ) : 0 ; code_block = IfStatement ; } catch ( IOException e ) { log . error ( "Error in exchange client" , e ) ; throw new TechnicalException ( "IOException" , e ) ; } catch ( NodeNotFound e ) { log . info ( "Node {} not found in cleanup" , nodeContext . getId ( ) ) ; exchangeClient . getPackExchanger ( ) . clean ( ) ; AgentStarter . resetAgent ( Agent . this ) ; } catch ( Throwable e ) { alive = false ; log . error ( "Agent " + nodeContext . getId ( ) + " got an exception from coordinator" , e ) ; } }
public void test() { try { PackResponse packResponse = exchangeClient . exchange ( ) ; long newPollRate = packResponse != null ? packResponse . getNewPollRate ( ) : 0 ; code_block = IfStatement ; } catch ( IOException e ) { log . error ( "Error in exchange client" , e ) ; throw new TechnicalException ( "IOException" , e ) ; } catch ( NodeNotFound e ) { log . warn ( "Agent {} didn't registered on current coordinator! Reset agent registration." , nodeContext . getId ( ) ) ; exchangeClient . getPackExchanger ( ) . clean ( ) ; AgentStarter . resetAgent ( Agent . this ) ; } catch ( Throwable e ) { log . error ( "Failed to reset agent {}" , nodeContext . getId ( ) , e ) ; alive = false ; } }
public void test() { try { String userName = AlluxioFuseUtils . getGroupName ( uid ) ; return userName . isEmpty ( ) ? DEFAULT_USER_NAME : userName ; } catch ( IOException e ) { LOG . error ( "Failed to get group name for uid {}" , uid , e ) ; return DEFAULT_USER_NAME ; } }
@ Override public void persistInterface ( final MetaDataRegisterDTO metadata ) { LOGGER . debug ( "persisting interface: {}" , metadata . getRpcType ( ) ) ; String rpcType = metadata . getRpcType ( ) ; String contextPath = metadata . getContextPath ( ) . substring ( 1 ) ; registerMetadata ( rpcType , contextPath , metadata ) ; code_block = IfStatement ; }
public void test() { try { flush ( ) ; code_block = ForStatement ; logid2FileChannel . clear ( ) ; entryLogManager . close ( ) ; synchronized ( compactionLogLock ) code_block = "" ; } catch ( IOException ie ) { LOG . error ( "Error closing compaction log" , ie ) ; } finally { code_block = ForStatement ; entryLogManager . forceClose ( ) ; synchronized ( compactionLogLock ) code_block = "" ; } }
@ Override public void handle ( PacketClientCacheStatus packet , long currentTimeMillis , PlayerConnection connection ) { LOG . debug ( "Got client cache, enabling client cache." ) ; connection . cachingSupported ( packet . isEnabled ( ) && connection . server ( ) . serverConfig ( ) . enableClientCache ( ) ) ; }
public void test() { try { doPortalInit ( ) ; } catch ( Exception exception ) { _log . error ( "Unable to initialize portal" , exception ) ; throw new IllegalStateException ( "Unable to initialize portal" , exception ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { try { uri = connection . getHeatEndpoint ( ) + "/stacks/" + URLEncoder . encode ( stackName , "UTF-8" ) + "/resources" ; RESTResponse response = connection . processRequest ( uri , "GET" ) ; String body = response . getResponseBody ( ) ; JSONObject responseJson = new JSONObject ( body ) ; JSONArray resources = responseJson . getJSONArray ( "resources" ) ; code_block = ForStatement ; } catch ( UnsupportedEncodingException e ) { throw new RuntimeException ( e ) ; } catch ( OpenStackConnectionException ex ) { throw new HeatException ( "Failed to connect to Heat: " + ex . getMessage ( ) , ex . getResponseCode ( ) ) ; } catch ( JSONException e ) { logger . error ( "HeatClient.getStackDetails()" , e ) ; throw new HeatException ( e . getMessage ( ) ) ; } }
@ NotNull public List < T > read ( @ NotNull String dir ) throws IOException { LOGGER . debug ( "Reading {}" , dir ) ; List < T > entries = Lists . newArrayList ( ) ; File [ ] files = new File ( dir ) . listFiles ( ) ; int currentFileIndex = 0 ; code_block = WhileStatement ; LOGGER . debug ( "  Done reading {} files " , currentFileIndex ) ; return entries ; }
@ NotNull public List < T > read ( @ NotNull String dir ) throws IOException { LOGGER . debug ( "Reading entries..." ) ; List < T > entries = Lists . newArrayList ( ) ; File [ ] files = new File ( dir ) . listFiles ( ) ; LOGGER . debug ( " {} files found in directory {}" , files . length , dir ) ; int currentFileIndex = 0 ; code_block = WhileStatement ; return entries ; }
public void test() { try { MethodKey methodKey = new MethodKey ( DLAppServiceUtil . class , "getFileEntry" , _getFileEntryParameterTypes39 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , folderId , title ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . portal . kernel . repository . model . FileEntry ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try ( ObjectOutputStream oos = new ObjectOutputStream ( new BufferedOutputStream ( new FileOutputStream ( new File ( sessionsDir , deploymentName ) ) ) ) ) { oos . writeObject ( map ) ; } catch ( Exception e ) { LOGGER . debug ( "Cannot serialize {}" , deploymentName , e ) ; } }
public void test() { if ( sessionData . size ( ) > 0 ) { code_block = TryStatement ;  } else { logger . debug ( "No more data to send." ) ; } }
public void test() { try { psIdToAttemptIndexMap = appStateStorage . loadPSMeta ( ) ; } catch ( Exception e ) { logger . error ( "Failed to loadPS meta from application" , e ) ; } }
public void test() { try { event . state . addException ( e ) ; event . state . setError ( true ) ; event . state . eventScheduler . destroyPlanWithError ( ) ; } catch ( RemoteException ex ) { log . error ( "Cannot destroy remote exception" , ex ) ; } }
public void test() { for ( SettingsProblem problem : result . getProblems ( ) ) { logger . debug ( problem . getMessage ( ) , problem . getException ( ) ) ; } }
public void test() { if ( this . workerState . changeStateInitializing ( ) ) { logger . info ( "start() started." ) ; worker . start ( ) ; workerState . changeStateStarted ( ) ; logger . info ( "start() completed." ) ; break ; } }
public void test() { switch ( this . workerState . getCurrentState ( ) ) { case NEW : code_block = IfStatement ; case INITIALIZING : logger . info ( "init() succeeded." ) ; break ; case STARTED : logger . info ( "start() failed. caused:already started." ) ; break ; case DESTROYING : throw new IllegalStateException ( "Already destroying." ) ; case STOPPED : throw new IllegalStateException ( "Already stopped." ) ; case ILLEGAL_STATE : throw new IllegalStateException ( "Invalid State." ) ; } }
public void test() { switch ( this . workerState . getCurrentState ( ) ) { case NEW : code_block = IfStatement ; case INITIALIZING : logger . info ( "start() failed. caused:already initializing." ) ; break ; case STARTED : logger . info ( "created." ) ; break ; case DESTROYING : throw new IllegalStateException ( "Already destroying." ) ; case STOPPED : throw new IllegalStateException ( "Already stopped." ) ; case ILLEGAL_STATE : throw new IllegalStateException ( "Invalid State." ) ; } }
public void blockAllInbound ( ) { inboundSettings . clear ( ) ; setDefaultInboundSettings ( false ) ; LOGGER . info ( "Block all inbound settings cleared." ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { Version . parseVersion ( value ) ; } catch ( RuntimeException e ) { LOGGER . warn ( "" , e ) ; errors . add ( new TaskError ( VERSION , field , objectName ) ) ; } }
public void test() { switch ( notificationLevel ) { case DEBUG : logger . debug ( message ) ; break ; case INFO : logger . info ( message ) ; break ; case WARN : logger . warn ( message ) ; break ; case ERROR : logger . error ( message ) ; break ; case OFF : break ; } }
public void test() { switch ( notificationLevel ) { case DEBUG : logger . debug ( message ) ; break ; case INFO : logger . info ( message ) ; break ; case WARN : logger . warn ( message ) ; break ; case ERROR : logger . error ( message ) ; break ; case OFF : break ; } }
public void test() { switch ( notificationLevel ) { case DEBUG : logger . debug ( message ) ; break ; case INFO : logger . info ( message ) ; break ; case WARN : logger . warn ( message ) ; break ; case ERROR : logger . error ( message ) ; break ; case OFF : break ; } }
public void test() { switch ( notificationLevel ) { case DEBUG : logger . debug ( message ) ; break ; case INFO : logger . info ( message ) ; break ; case WARN : logger . warn ( message ) ; break ; case ERROR : logger . error ( message ) ; break ; case OFF : break ; } }
@ BeforeClass public void createObjects ( ) { log = LoggerFactory . getLogger ( OptimizerTOSCADecember2015MultipleInputPointsTest . class ) ; log . info ( "Starting TEST optimizer." ) ; openInputFiles ( TestConstants . APP_MODEL_FILENAME_MULTIPLE_INPUT_POINT , TestConstants . CLOUD_OFFER_FILENAME_IN_JSON_ATOS_7_MODULES ) ; }
public void test() { if ( ! id . equals ( bootstrapId ) ) { log . warn ( "Found existing bootstrap ID: " + bootstrapId ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( isPenalized ( peerStatus ) ) { LOG . debug ( "{} is still aPenalized node" , id ) ; } else { return peerStatus ; } }
public void test() { try { field . setAccessible ( true ) ; logger . info ( "Accessing SystemProperty field {}{}" , className , field . getName ( ) ) ; field . get ( null ) ; } catch ( final Throwable t ) { logger . warn ( "Could not access SystemProperty field {}{}" , className , field . getName ( ) , t ) ; } }
public void test() { try { final Class < ? > clazz = classInfo . load ( ) ; final Field [ ] fields = clazz . getDeclaredFields ( ) ; code_block = ForStatement ; } catch ( final Throwable t ) { logger . warn ( "Could not load " + classInfo , t ) ; } }
public void test() { try { final Set < ClassPath . ClassInfo > classesInPackage = ClassPath . from ( getClass ( ) . getClassLoader ( ) ) . getTopLevelClassesRecursive ( "org.jivesoftware.openfire" ) ; code_block = ForStatement ; } catch ( final Throwable t ) { logger . warn ( "" , t ) ; } }
@ Override public void onPaymentError ( final PaymentErrorInternalEvent event ) { log . info ( "PaymentError event received" ) ; notifyForCompletion ( ) ; }
public void test() { try { com . liferay . commerce . inventory . model . CommerceInventoryWarehouseItem returnValue = CommerceInventoryWarehouseItemServiceUtil . addCommerceInventoryWarehouseItem ( externalReferenceCode , userId , commerceInventoryWarehouseId , sku , quantity ) ; return com . liferay . commerce . inventory . model . CommerceInventoryWarehouseItemSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try { com . hazelcast . core . Hazelcast . shutdownAll ( ) ; } catch ( Exception ex ) { logger . error ( ex . getMessage ( ) , ex ) ; } }
public void test() { if ( LOG . isErrorEnabled ( ) ) { LOG . error ( "===============================================" ) ; LOG . error ( "There was an error in the rollback file." ) ; LOG . error ( "A rollback file has been generated at " + rollbackFile ) ; LOG . error ( "Execute it within your database to rollback any changes" ) ; LOG . error ( "The exception is as follows\n" , e ) ; LOG . error ( "===============================================" ) ; } }
public void test() { if ( LOG . isErrorEnabled ( ) ) { LOG . error ( "===============================================" ) ; LOG . error ( "An exception occurred during database migration" ) ; LOG . error ( "Execute it within your database to rollback any changes" ) ; LOG . error ( "The exception is as follows\n" , e ) ; LOG . error ( "The exception is as follows\n" , e ) ; LOG . error ( "===============================================" ) ; } }
public void test() { if ( LOG . isErrorEnabled ( ) ) { LOG . error ( "===============================================" ) ; LOG . error ( "An exception occurred during database migration" ) ; LOG . error ( "A rollback file has been generated at " + rollbackFile ) ; LOG . error ( "The exception is as follows\n" , e ) ; LOG . error ( "The exception is as follows\n" , e ) ; LOG . error ( "===============================================" ) ; } }
public void test() { if ( LOG . isErrorEnabled ( ) ) { LOG . error ( "===============================================" ) ; LOG . error ( "An exception occurred during database migration" ) ; LOG . error ( "A rollback file has been generated at " + rollbackFile ) ; LOG . error ( "Execute it within your database to rollback any changes" ) ; LOG . error ( "===============================================" ) ; LOG . error ( "===============================================" ) ; } }
public void simplePipeline ( String newBranch ) { wait . click ( By . id ( "pipeline-node-hittarget-2-add" ) ) ; logger . info ( "Adding an echo step" ) ; wait . sendKeys ( By . cssSelector ( "input.stage-name-edit" ) , "simplePipeline creating Test stage" ) ; wait . click ( By . cssSelector ( "button.btn-primary.add" ) ) ; logger . info ( "Adding an echo step" ) ; wait . click ( By . cssSelector ( ".editor-step-selector div[data-functionName=\"echo\"]" ) ) ; wait . sendKeys ( By . cssSelector ( "input.TextInput-control" ) , "simplePipeline creating echo message" ) ; wait . click ( By . cssSelector ( "div.sheet.active a.back-from-sheet" ) ) ; logger . info ( "Pipeline created and ready to be saved" ) ; }
public void simplePipeline ( String newBranch ) { logger . info ( "Creating and editing simple pipeline" ) ; wait . click ( By . id ( "pipeline-node-hittarget-2-add" ) ) ; wait . sendKeys ( By . cssSelector ( "input.stage-name-edit" ) , "simplePipeline creating Test stage" ) ; wait . click ( By . cssSelector ( "button.btn-primary.add" ) ) ; wait . click ( By . cssSelector ( ".editor-step-selector div[data-functionName=\"echo\"]" ) ) ; wait . sendKeys ( By . cssSelector ( "input.TextInput-control" ) , "simplePipeline creating echo message" ) ; wait . click ( By . cssSelector ( "div.sheet.active a.back-from-sheet" ) ) ; logger . info ( "Done creating and ready to be saved" ) ; logger . info ( "Pipeline created and ready to be saved" ) ; }
public void simplePipeline ( String newBranch ) { logger . info ( "Creating and editing simple pipeline" ) ; wait . click ( By . id ( "pipeline-node-hittarget-2-add" ) ) ; wait . sendKeys ( By . cssSelector ( "input.stage-name-edit" ) , "simplePipeline creating Test stage" ) ; wait . click ( By . cssSelector ( "button.btn-primary.add" ) ) ; logger . info ( "Adding an echo step" ) ; wait . click ( By . cssSelector ( ".editor-step-selector div[data-functionName=\"echo\"]" ) ) ; wait . sendKeys ( By . cssSelector ( "input.TextInput-control" ) , "simplePipeline creating echo message" ) ; logger . info ( "Adding a echo step" ) ; wait . click ( By . cssSelector ( "div.sheet.active a.back-from-sheet" ) ) ; }
public void channelActive ( final ChannelHandlerContext ctx ) throws Exception { LOGGER . info ( "Connection active on channel {}" , ctx . channel ( ) . remoteAddress ( ) ) ; }
public void test() { try { s3 . setBucketVersioningConfiguration ( new SetBucketVersioningConfigurationRequest ( bucketName , new BucketVersioningConfiguration ( BucketVersioningConfiguration . ENABLED ) ) ) ; } catch ( SdkClientException e ) { String message = "Bucket versioning status: " + status + ". Cannot enable versioning. Error message: " + e . getMessage ( ) ; LOG . error ( message ) ; } }
public void test() { try { String status = s3 . getBucketVersioningConfiguration ( bucketName ) . getStatus ( ) ; code_block = IfStatement ; } catch ( SdkClientException e ) { log . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { MetricRegistry registry = Metrics . getInstance ( ) . getRegistry ( ) ; HoodieGauge guage = ( HoodieGauge ) registry . gauge ( metricName , ( ) -> new HoodieGauge < > ( value ) ) ; guage . setValue ( value ) ; } catch ( Exception e ) { LOG . error ( "Unexpected error." , e ) ; } }
void addHandler ( String key , EventHandlerMethod handler ) { Preconditions . checkArgument ( ! this . handlers . containsKey ( key ) , "EventHandler can't be registered twice. Other instance: " + this . handlers . get ( key ) ) ; this . handlers . put ( key , handler ) ; EventHandlerMethod [ ] newArray = new EventHandlerMethod [ this . insertIndex + 1 ] ; code_block = IfStatement ; newArray [ this . insertIndex ++ ] = handler ; this . sortedHandlerList = newArray ; this . dirty = true ; logger . debug ( "Added new handler '{}' for key '{}'" , handler , key ) ; }
public void test() { try { task . run ( ) ; } catch ( Throwable t ) { logger . warn ( "Shutdown hook raised an exception." , t ) ; } }
public void test() { try { final Reader reader = source . getReader ( ) ; code_block = IfStatement ; return "" ; } catch ( final IOException e ) { LOGGER . log ( Level . FINER , "Unable to read from source: {0}" , source ) ; return "" ; } }
public void test() { try { code_block = IfStatement ; } catch ( ArrayIndexOutOfBoundsException E ) { LOGGER . warn ( "Out of bounds access." ) ; } }
private void createScope ( ) { Scope scope = new Scope ( ) ; scope . setName ( SCOPE_NAME ) ; scope . setType ( Scope . TYPE ) ; scope . setDescription ( "Sample scope description." ) ; Scope createdScope = this . client . create ( scope ) ; LOGGER . info ( "Scope created: {}" , createdScope . toJsonString ( ) ) ; }
@ Activate protected void activate ( Config config ) throws RepositoryException { List < SyncHandler > newSyncSpecs = new LinkedList < SyncHandler > ( ) ; code_block = ForStatement ; syncHandlers = newSyncSpecs . toArray ( new SyncHandler [ newSyncSpecs . size ( ) ] ) ; enabled = config . vault_sync_enabled ( ) ; checkDelay = config . vault_sync_fscheckinterval ( ) * 1000 ; code_block = IfStatement ; log . info ( "Vault plugin successfully activated" ) ; }
public void test() { try { Path createdPath = Files . createDirectories ( parentPath . resolve ( folderName ) ) ; BasicFileAttributes attrs = Files . readAttributes ( createdPath , BasicFileAttributes . class ) ; TransferredResource resource = createTransferredResource ( createdPath , attrs , 0L , basePath , new Date ( ) ) ; index . create ( TransferredResource . class , resource ) ; LOGGER . info ( "Created {}" , resource ) ; return resource ; } catch ( IOException e ) { throw new GenericException ( "Cannot create folder" , e ) ; } }
public void test() { try { registerAndInitializeHandler ( child , getThingHandlerFactory ( child ) ) ; } catch ( Exception ex ) { logger . debug ( "Could not initialize ThingHandler: {}" , child , ex ) ; } }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "JDBC::{}" , sql ) ; } }
@ Test ( timeout = 60000 ) public void testDisableCompaction ( ) throws Exception { LedgerHandle [ ] lhs = prepareData ( 3 , false ) ; baseConf . setMinorCompactionThreshold ( 0.0f ) ; baseConf . setMajorCompactionThreshold ( 0.0f ) ; restartBookies ( baseConf ) ; bkc . deleteLedger ( lhs [ 1 ] . getId ( ) ) ; bkc . deleteLedger ( lhs [ 2 ] . getId ( ) ) ; Thread . sleep ( baseConf . getMajorCompactionInterval ( ) * 1000 + baseConf . getGcWaitTime ( ) ) ; LOG . info ( "Deleted " + baseConf . getMajorCompactionInterval ( ) ) ; code_block = ForStatement ; }
public String enqueueUpdateDeviceSslCertificationRequest ( final String organisationIdentification , final String deviceIdentification , final Certification certification , final int messagePriority ) throws FunctionalException { final Organisation organisation = this . domainHelperService . findOrganisation ( organisationIdentification ) ; final Device device = this . domainHelperService . findActiveDevice ( deviceIdentification ) ; this . domainHelperService . isAllowed ( organisation , device , DeviceFunction . UPDATE_DEVICE_SSL_CERTIFICATION ) ; this . domainHelperService . isInMaintenance ( device ) ; LOGGER . debug ( "enqueueUpdateDeviceSslCertificationRequest called with organisation {} and device {}" , organisationIdentification , deviceIdentification ) ; final String correlationUid = this . correlationIdProviderService . getCorrelationId ( organisationIdentification , deviceIdentification ) ; final DeviceMessageMetadata deviceMessageMetadata = new DeviceMessageMetadata ( deviceIdentification , organisationIdentification , correlationUid , MessageType . UPDATE_DEVICE_SSL_CERTIFICATION . name ( ) , messagePriority ) ; final CommonRequestMessage message = new CommonRequestMessage . Builder ( ) . deviceMessageMetadata ( deviceMessageMetadata ) . request ( certification ) . build ( ) ; this . commonRequestMessageSender . send ( message ) ; return correlationUid ; }
private void waitToExitSafeMode ( JobClient client ) throws IOException { LOG . info ( "Entering safe mode" ) ; FileSystem fs = client . getFs ( ) ; DistributedFileSystem dfs = ( DistributedFileSystem ) fs ; boolean inSafeMode = true ; code_block = WhileStatement ; LOG . info ( "Exited safe mode" ) ; }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void doCreateConsumer ( RoutingContext routingContext , JsonObject bodyAsJson , Handler < SinkBridgeEndpoint < K , V > > handler ) { this . groupId = routingContext . pathParam ( "groupid" ) ; this . name = bodyAsJson . getString ( "name" , bridgeConfig . getBridgeID ( ) == null ? "kafka-bridge-consumer-" + UUID . randomUUID ( ) : bridgeConfig . getBridgeID ( ) + "-" + UUID . randomUUID ( ) ) ; this . consumerInstanceId = new ConsumerInstanceId ( this . groupId , this . name ) ; code_block = IfStatement ; String requestUri = this . buildRequestUri ( routingContext ) ; code_block = IfStatement ; String consumerBaseUri = requestUri + "instances/" + this . name ; Properties config = new Properties ( ) ; addConfigParameter ( ConsumerConfig . AUTO_OFFSET_RESET_CONFIG , bodyAsJson . getString ( ConsumerConfig . AUTO_OFFSET_RESET_CONFIG , null ) , config ) ; Object enableAutoCommit = bodyAsJson . getValue ( ConsumerConfig . ENABLE_AUTO_COMMIT_CONFIG ) ; addConfigParameter ( ConsumerConfig . ENABLE_AUTO_COMMIT_CONFIG , enableAutoCommit != null ? String . valueOf ( enableAutoCommit ) : null , config ) ; Object fetchMinBytes = bodyAsJson . getValue ( ConsumerConfig . FETCH_MIN_BYTES_CONFIG ) ; addConfigParameter ( ConsumerConfig . FETCH_MIN_BYTES_CONFIG , fetchMinBytes != null ? String . valueOf ( fetchMinBytes ) : null , config ) ; Object requestTimeoutMs = bodyAsJson . getValue ( "consumer." + ConsumerConfig . REQUEST_TIMEOUT_MS_CONFIG ) ; addConfigParameter ( ConsumerConfig . REQUEST_TIMEOUT_MS_CONFIG , requestTimeoutMs != null ? String . valueOf ( requestTimeoutMs ) : null , config ) ; addConfigParameter ( ConsumerConfig . CLIENT_ID_CONFIG , this . name , config ) ; this . initConsumer ( false , config ) ; handler . handle ( this ) ; JsonObject body = new JsonObject
public void test() { if ( ! file . exists ( ) ) { log . info ( "Attempting to checkout file " + _path ) ; file = this . checkoutFile ( _rev_branch , _path ) ; } else { log . info ( "[" + _path + "] already exists, no checkout needed" ) ; } }
public void test() { if ( ! file . exists ( ) ) { log . info ( "Starting checkout of [" + _path + "]" ) ; file = this . checkoutFile ( _rev_branch , _path ) ; } else { log . info ( "Cannot checkout of [" + _path + "]" ) ; } }
@ Test public void test ( ) throws Exception { MockEndpoint result = getMockEndpoint ( "mock:result" ) ; result . setExpectedCount ( 1 ) ; final ProducerTemplate producerTemplate = context . createProducerTemplate ( ) ; ClassLoader tccl = Thread . currentThread ( ) . getContextClassLoader ( ) ; InputStream payloadIs = tccl . getResourceAsStream ( "json-source.json" ) ; producerTemplate . sendBody ( "direct:start" , payloadIs ) ; assertMockEndpointsSatisfied ( ) ; final String body = result . getExchanges ( ) . get ( 0 ) . getIn ( ) . getBody ( String . class ) ; LOG . debug ( "Body: {}" , body ) ; assertNotNull ( body ) ; InputStream schemaIs = tccl . getResourceAsStream ( "xml-target-schemaset.xml" ) ; AtlasXmlSchemaSetParser schemaParser = new AtlasXmlSchemaSetParser ( tccl ) ; Validator validator = schemaParser . createSchema ( schemaIs ) . newValidator ( ) ; StreamSource source = new StreamSource ( new StringReader ( body ) ) ; validator . validate ( source ) ; }
public void test() { if ( logger . isTraceEnabled ( LogMarker . TOMBSTONE_COUNT_VERBOSE ) ) { logger . trace ( LogMarker . TOMBSTONE_COUNT_VERBOSE , "adding expired tombstone {} to queue" , tombstone ) ; } }
public void test() { if ( entryVersion == destroyedVersion ) { logger . debug ( "removing entry (v{}) that is older than an expiring tombstone (v{} rv{}) for {}" , entryVersion , destroyedVersion , version . getRegionVersion ( ) , re . getKey ( ) ) ; } else { logger . trace ( LogMarker . TOMBSTONE_COUNT_VERBOSE , "removing entry (v{}) that is older than an expiring tombstone (v{} rv{}) for {}" , entryVersion , destroyedVersion , version . getRegionVersion ( ) , re . getKey ( ) ) ; } }
public void test() { if ( entryVersion == destroyedVersion ) { logger . trace ( LogMarker . TOMBSTONE_COUNT_VERBOSE , "removing tombstone for {} with v{} rv{}; count is {}" , re . getKey ( ) , destroyedVersion , version . getRegionVersion ( ) , ( this . _getOwner ( ) . getTombstoneCount ( ) - 1 ) ) ; } else { logger . debug ( "removing tombstone for {} with v{} rv{}; count is {}" , re . getKey ( ) , destroyedVersion , version . getRegionVersion ( ) , ( this . _getOwner ( ) . getTombstoneCount ( ) - 1 ) ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
@ Override public void serviceInit ( Configuration conf ) throws Exception { AsyncDispatcher dispatcher ; TajoWorkerClientService tajoWorkerClientService ; TajoWorkerManagerService tajoWorkerManagerService ; ShutdownHookManager . get ( ) . addShutdownHook ( new ShutdownHook ( ) , SHUTDOWN_HOOK_PRIORITY ) ; this . systemConf = TUtil . checkTypeAndGet ( conf , TajoConf . class ) ; RackResolver . init ( systemConf ) ; serviceTracker = ServiceTrackerFactory . get ( systemConf ) ; this . workerContext = new TajoWorkerContext ( ) ; this . lDirAllocator = new LocalDirAllocator ( ConfVars . WORKER_TEMPORAL_DIR . varname ) ; dispatcher = new AsyncDispatcher ( ) ; addIfService ( dispatcher ) ; tajoWorkerManagerService = new TajoWorkerManagerService ( workerContext ) ; addIfService ( tajoWorkerManagerService ) ; tajoWorkerClientService = new TajoWorkerClientService ( workerContext ) ; addIfService ( tajoWorkerClientService ) ; queryMasterManagerService = new QueryMasterManagerService ( workerContext ) ; addIfService ( queryMasterManagerService ) ; code_block = IfStatement ; this . taskManager = new TaskManager ( dispatcher , workerContext , pullService ) ; addService ( taskManager ) ; this . taskExecutor = new TaskExecutor ( workerContext ) ; addService ( taskExecutor ) ; AsyncDispatcher rmDispatcher = new AsyncDispatcher ( ) ; addService ( rmDispatcher ) ; this . nodeResourceManager = new NodeResourceManager ( rmDispatcher , workerContext ) ; addService ( nodeResourceManager ) ; addService ( new NodeStatusUpdater ( workerContext ) ) ; int httpPort = 0 ; code_block = IfStatement ; super . serviceInit ( conf ) ; int pullServerPort = systemConf . getIntVar ( ConfVars . PULLSERVER_PORT ) ; code_block = IfStatement ; this . connectionInfo = new WorkerConnectionInfo ( tajoWorkerManagerService . getBindAddr ( ) . getHostName ( ) , tajoWorkerManagerService . getBindAddr ( ) . getPort ( ) , pullServerPort )
public void test() { try { hashShuffleAppenderManager = new HashShuffleAppenderManager ( systemConf ) ; } catch ( IOException e ) { LOG . error ( "Failed to create hashShuffleAppenderManager" , e ) ; System . exit ( - 1 ) ; } }
@ Override public < K , V > BlockDiskCache < K , V > createCache ( final AuxiliaryCacheAttributes iaca , final ICompositeCacheManager cacheMgr , final ICacheEventLogger cacheEventLogger , final IElementSerializer elementSerializer ) { final BlockDiskCacheAttributes idca = ( BlockDiskCacheAttributes ) iaca ; final BlockDiskCache < K , V > cache = new BlockDiskCache < > ( idca , elementSerializer ) ; cache . setCacheEventLogger ( cacheEventLogger ) ; log . debug ( "Created cache with ID : " + idca . getId ( ) ) ; return cache ; }
protected void loadTempStorage ( String name , Map < String , String > properties ) { requireNonNull ( name , "name is null" ) ; requireNonNull ( properties , "properties is null" ) ; String tempStorageFactoryName = null ; ImmutableMap . Builder < String , String > tempStorageProperties = ImmutableMap . builder ( ) ; code_block = ForStatement ; checkState ( tempStorageFactoryName != null , "Configuration for tempStorage %s does not contain temp-storage-factory.name" , name ) ; TempStorageFactory factory = tempStorageFactories . get ( tempStorageFactoryName ) ; checkState ( factory != null , "Temp Storage Factory %s is not registered" , tempStorageFactoryName ) ; TempStorage tempStorage = factory . create ( tempStorageProperties . build ( ) , new TempStorageContext ( nodeManager ) ) ; log . info ( "-- Loaded temp storage %s --" , tempStorage ) ; code_block = IfStatement ; log . info ( "-- Loaded temp storage %s --" , name ) ; }
protected void loadTempStorage ( String name , Map < String , String > properties ) { requireNonNull ( name , "name is null" ) ; requireNonNull ( properties , "properties is null" ) ; log . info ( "-- Loading temp storage %s --" , name ) ; String tempStorageFactoryName = null ; ImmutableMap . Builder < String , String > tempStorageProperties = ImmutableMap . builder ( ) ; code_block = ForStatement ; checkState ( tempStorageFactoryName != null , "Configuration for tempStorage %s does not contain temp-storage-factory.name" , name ) ; TempStorageFactory factory = tempStorageFactories . get ( tempStorageFactoryName ) ; checkState ( factory != null , "Temp Storage Factory %s is not registered" , tempStorageFactoryName ) ; TempStorage tempStorage = factory . create ( tempStorageProperties . build ( ) , new TempStorageContext ( nodeManager ) ) ; code_block = IfStatement ; log . info ( "-- Loaded temp storage %s --" , tempStorage ) ; }
public void test() { if ( key == null ) { logger . warn ( "Key is null." ) ; return null ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { int actualIndex = message . getIntProperty ( TestAmqpPeer . MESSAGE_NUMBER ) ; LOG . debug ( "Got message {}" , actualIndex ) ; assertEquals ( "Received Message Out Of Order" , expectedIndex , actualIndex ) ; code_block = IfStatement ; } catch ( Throwable t ) { complete = true ; asyncError . set ( t ) ; latch . countDown ( ) ; } }
public void test() { if ( messageSeen < recoverCount ) { LOG . debug ( "Ignoring message " + actualIndex + " and calling recover" ) ; session . recover ( ) ; messageSeen ++ ; } else { messageSeen = 0 ; expectedIndex ++ ; testPeer . expectDisposition ( true , new AcceptedMatcher ( ) , expectedIndex , expectedIndex ) ; LOG . debug ( "Acknowledging message {}" , actualIndex ) ; message . acknowledge ( ) ; code_block = IfStatement ; } }
public void test() { if ( messageSeen < recoverCount ) { LOG . debug ( "Ignoring message " + actualIndex + " and calling recover" ) ; session . recover ( ) ; messageSeen ++ ; } else { messageSeen = 0 ; expectedIndex ++ ; testPeer . expectDisposition ( true , new AcceptedMatcher ( ) , expectedIndex , expectedIndex ) ; LOG . debug ( "Acknowledging message {}" , actualIndex ) ; message . acknowledge ( ) ; code_block = IfStatement ; } }
public void test() { if ( activeCount == 0 ) { LOG . info ( "No more workers are going to shutdown." ) ; break ; } else { LOG . info ( "Number of active threads = " + activeCount + ". Waiting for all threads to shutdown ..." ) ; code_block = TryStatement ;  } }
public void shutdownServer ( ) { int timeWaitForShutdownInSeconds = EmbeddedServerUtil . getIntConfig ( "service.waitTimeForForceShutdownInSeconds" , 0 ) ; code_block = IfStatement ; log . info ( "Graceful shutdown." ) ; System . exit ( 0 ) ; }
private File customAnimation ( final Job job , final URI input , final Map < String , String > metadata ) throws IOException , NotFoundException { File output = new File ( workspace . rootDirectory ( ) , String . format ( "animate/%d/%s.%s" , job . getId ( ) , FilenameUtils . getBaseName ( input . getPath ( ) ) , FilenameUtils . getExtension ( input . getPath ( ) ) ) ) ; FileUtils . forceMkdirParent ( output ) ; String animation ; code_block = TryStatement ;  code_block = ForStatement ; FileUtils . write ( output , animation , "utf-8" ) ; log . info ( "Custom animation: {}" , output ) ; return output ; }
public void test() { try { animation = FileUtils . readFileToString ( new File ( input ) , "UTF-8" ) ; } catch ( IOException e ) { LOGGER . info ( "load file '{}' failed" , input , e ) ; code_block = TryStatement ;  } }
public FilterResZob merge ( FilterResZob detachedInstance ) { log . debug ( "merging FilterResZob instance" ) ; code_block = TryStatement ;  }
public void test() { try { FilterResZob result = ( FilterResZob ) sessionFactory . getCurrentSession ( ) . merge ( detachedInstance ) ; log . debug ( "merge successful" ) ; return result ; } catch ( RuntimeException re ) { log . error ( "merge failed" , re ) ; throw re ; } }
public void test() { try { FilterResZob result = ( FilterResZob ) sessionFactory . getCurrentSession ( ) . merge ( detachedInstance ) ; log . debug ( "merge successful" ) ; return result ; } catch ( RuntimeException re ) { log . error ( "merge failed" , re ) ; throw re ; } }
public void test() { for ( String cache : pools ) { code_block = IfStatement ; } }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; } catch ( IllegalAccessException e ) { log . warn ( e . getMessage ( ) , e ) ; } }
public void test() { try { code_block = WhileStatement ; } catch ( Exception e ) { LOG . warn ( "Failed to stop ReplicaScanner" , e ) ; } finally { replicatedScanner . close ( ) ; replicatedScanner = null ; } }
public void test() { try { sourceTable . close ( ) ; } catch ( IOException e ) { LOGGER . warn ( "Failed to close source table" , e ) ; } }
public void test() { try { sourceConnection . close ( ) ; } catch ( Exception e ) { LOGGER . warn ( "Failed to close source connection" , e ) ; } }
public void test() { try { replicatedTable . close ( ) ; } catch ( Exception e ) { LOG . warn ( "Failed to close the replicated table" , e ) ; } }
public void test() { try { replicatedConnection . close ( ) ; } catch ( Exception e ) { logger . warn ( "Failed to close ReplicationConnection" , e ) ; } }
@ Override public synchronized void shutdown ( ) throws Exception { LOG . debug ( "Shutting down scheduler" ) ; this . scheduledExecution . cancel ( true ) ; super . shutdown ( ) ; }
public void test() { try { code_block = IfStatement ; } catch ( StorageException se ) { LOG . error ( "get backup info error" , se ) ; } }
public void test() { try { final Object evaluationResult = this . evalWithR ( input ) ; final REXPDouble doubleResult = ( REXPDouble ) evaluationResult ; return doubleResult . asDouble ( ) ; } catch ( final REXPMismatchException exc ) { RBridgeControl . LOGGER . error ( exc . getMessage ( ) , exc ) ; return resultOnFailure ; } catch ( final InvalidREvaluationResultException exc ) { RBridgeControl . LOGGER . error ( exc . getMessage ( ) , exc ) ; return resultOnFailure ; } }
public void test() { try { final Object evaluationResult = this . evalWithR ( input ) ; final REXPDouble doubleResult = ( REXPDouble ) evaluationResult ; return doubleResult . asDouble ( ) ; } catch ( final REXPMismatchException exc ) { RBridgeControl . LOGGER . error ( "Error casting value from R: {} Cause: {}" , input , exc ) ; return resultOnFailure ; } catch ( final InvalidREvaluationResultException exc ) { RBridgeControl . LOGGER . error ( "Invalid evaluation result: {}" , input , exc ) ; return resultOnFailure ; } }
public void test() { try { return session . getRoomUsers ( roomId ) ; } catch ( MageRemoteException e ) { logger . info ( e ) ; return Collections . emptyList ( ) ; } }
public void test() { if ( dataType != null ) { TimelineRegistry timelineRegistry = myToolbox . getUIRegistry ( ) . getTimelineRegistry ( ) ; timelineRegistry . removeData ( dataType . getOrderKey ( ) , ids ) ; } else { logger . warn ( "Unable to remove data type '{}'" , dataType . getOrderKey ( ) ) ; } }
@ BeforeClass public static void startServers ( ) { testKdc = new TestKDC ( true ) ; testKdc . startDirectoryService ( ) ; testKdc . startKDC ( ) ; log . debug ( "Started KeyTab" ) ; serverKeyTab = testKdc . generateKeyTab ( CommunicationSuiteChild . SERVER_KEY_TAB , "sasl/test_server_1@WILDFLY.ORG" , "servicepwd" ) ; log . debug ( "serverKeyTab written to:" + serverKeyTab ) ; serverUnboundKeyTab = testKdc . generateKeyTab ( CommunicationSuiteChild . SERVER_UNBOUND_KEY_TAB , "sasl/test_server_1@WILDFLY.ORG" , "servicepwd" , "*@WILDFLY.ORG" , "dummy" ) ; log . debug ( "serverUnboundKeyTab written to:" + serverUnboundKeyTab ) ; }
@ BeforeClass public static void startServers ( ) { log . debug ( "Starting KDC..." ) ; testKdc = new TestKDC ( true ) ; testKdc . startDirectoryService ( ) ; testKdc . startKDC ( ) ; serverKeyTab = testKdc . generateKeyTab ( CommunicationSuiteChild . SERVER_KEY_TAB , "sasl/test_server_1@WILDFLY.ORG" , "servicepwd" ) ; log . debug ( "serverKeyTab written to:" + serverKeyTab ) ; serverUnboundKeyTab = testKdc . generateKeyTab ( CommunicationSuiteChild . SERVER_UNBOUND_KEY_TAB , "sasl/test_server_1@WILDFLY.ORG" , "servicepwd" , "*@WILDFLY.ORG" , "dummy" ) ; log . debug ( "serverUnboundKeyTab written to:" + serverUnboundKeyTab ) ; }
@ BeforeClass public static void startServers ( ) { log . debug ( "Starting KDC..." ) ; testKdc = new TestKDC ( true ) ; testKdc . startDirectoryService ( ) ; testKdc . startKDC ( ) ; serverKeyTab = testKdc . generateKeyTab ( CommunicationSuiteChild . SERVER_KEY_TAB , "sasl/test_server_1@WILDFLY.ORG" , "servicepwd" ) ; log . debug ( "serverKeyTab written to:" + serverKeyTab ) ; serverUnboundKeyTab = testKdc . generateKeyTab ( CommunicationSuiteChild . SERVER_UNBOUND_KEY_TAB , "sasl/test_server_1@WILDFLY.ORG" , "servicepwd" , "*@WILDFLY.ORG" , "dummy" ) ; log . debug ( "serverUnbound keyTab written to:" + serverUnboundKeyTab ) ; }
@ Override protected void onPostExecute ( Void result ) { super . onPostExecute ( result ) ; activity . showKeyFingerprints ( ) ; LOGGER . info ( "Registered key fingerprint: " + result ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try ( OutputStream outputStream = jobDataWithByteSink . getByteSink ( ) . openBufferedStream ( ) ; OutputStreamWriter outputStreamWriter = new OutputStreamWriter ( outputStream ) ; CSVWriter writer = new CSVWriter ( outputStreamWriter , ',' ) ) { String [ ] headings = new String [ ] code_block = "" ; ; writer . writeNext ( headings ) ; String [ ] cells = new String [ 4 ] ; long startMs = System . currentTimeMillis ( ) ; LOGGER . info ( "will produce {} packages" , count ) ; long count = pkgService . eachPkg ( context , false , pkg code_block = LoopStatement ; ) ; LOGGER . info ( "did produce spreadsheet report for {} packages in {}ms" , count , System . currentTimeMillis ( ) - startMs ) ; } }
public void test() { try ( OutputStream outputStream = jobDataWithByteSink . getByteSink ( ) . openBufferedStream ( ) ; OutputStreamWriter outputStreamWriter = new OutputStreamWriter ( outputStream ) ; CSVWriter writer = new CSVWriter ( outputStreamWriter , ',' ) ) { String [ ] headings = new String [ ] code_block = "" ; ; writer . writeNext ( headings ) ; String [ ] cells = new String [ 4 ] ; long startMs = System . currentTimeMillis ( ) ; LOGGER . info ( "will produce spreadsheet spreadsheet report" ) ; long count = pkgService . eachPkg ( context , false , pkg code_block = LoopStatement ; ) ; LOGGER . info ( "finished in " + count + " records" ) ; } }
@ Override public MCCIIN000002UV01 processPatientDiscoveryAsyncResp ( PRPAIN201306UV02 request , AssertionType assertion , NhinTargetCommunitiesType target ) { LOG . debug ( "Begin EntityPatientDiscoveryDeferredResponseProxyWebServiceUnsecuredImpl.processPatientDiscoveryAsyncResp(...)" ) ; MCCIIN000002UV01 response = new MCCIIN000002UV01 ( ) ; String serviceName = NhincConstants . PATIENT_DISCOVERY_ENTITY_ASYNC_RESP_SERVICE_NAME ; code_block = TryStatement ;  LOG . debug ( "End EntityPatientDiscoveryDeferredResponseProxyWebServiceUnsecuredImpl.processPatientDiscoveryAsyncResp(...)" ) ; return response ; }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
@ Override public MCCIIN000002UV01 processPatientDiscoveryAsyncResp ( PRPAIN201306UV02 request , AssertionType assertion , NhinTargetCommunitiesType target ) { LOG . debug ( "Begin EntityPatientDiscoveryDeferredResponseProxyWebServiceUnsecuredImpl.processPatientDiscoveryAsyncResp(...)" ) ; MCCIIN000002UV01 response = new MCCIIN000002UV01 ( ) ; String serviceName = NhincConstants . PATIENT_DISCOVERY_ENTITY_ASYNC_RESP_SERVICE_NAME ; code_block = TryStatement ;  LOG . debug ( "End EntityPatientDiscoveryDeferredResponseProxyWebServiceUnsecuredImpl.processPatientDiscoveryAsyncResp(...)" ) ; return response ; }
public void test() { try { res = Long . parseLong ( envVal ) ; } catch ( NumberFormatException ex ) { LOG . warn ( "Failed to parse environment variable '" + envVal + "'" , ex ) ; } }
public void test() { if ( x . length != 1 ) { String msg = "Laguerre(n,m;x) needs one argument" ; logger . error ( msg ) ; throw new IllegalArgumentException ( msg ) ; } }
@ Override public void setUserQuotaTotal ( final String userName , final long quotaValue ) throws JargonException { log . info ( "setUserQuotaTotal()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "userName:{}" , userName ) ; log . info ( "quotaValue:{}" , quotaValue ) ; GeneralAdminInp adminPI = GeneralAdminInp . instanceForSetUserQuotaTotal ( userName , quotaValue ) ; log . debug ( "executing admin PI" ) ; getIRODSProtocol ( ) . irodsFunction ( adminPI ) ; log . info ( "quota set" ) ; }
@ Override public void setUserQuotaTotal ( final String userName , final long quotaValue ) throws JargonException { log . info ( "setUserQuotaTotal()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "userName:{}" , userName ) ; log . info ( "quotaValue:{}" , quotaValue ) ; GeneralAdminInp adminPI = GeneralAdminInp . instanceForSetUserQuotaTotal ( userName , quotaValue ) ; log . debug ( "executing admin PI" ) ; getIRODSProtocol ( ) . irodsFunction ( adminPI ) ; log . info ( "quota set" ) ; }
@ Override public void setUserQuotaTotal ( final String userName , final long quotaValue ) throws JargonException { log . info ( "setUserQuotaTotal()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "userName:{}" , userName ) ; log . info ( "userName:{}" , userName ) ; GeneralAdminInp adminPI = GeneralAdminInp . instanceForSetUserQuotaTotal ( userName , quotaValue ) ; log . debug ( "executing admin PI" ) ; getIRODSProtocol ( ) . irodsFunction ( adminPI ) ; log . info ( "quota set" ) ; }
@ Override public void setUserQuotaTotal ( final String userName , final long quotaValue ) throws JargonException { log . info ( "setUserQuotaTotal()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "userName:{}" , userName ) ; log . info ( "quotaValue:{}" , quotaValue ) ; GeneralAdminInp adminPI = GeneralAdminInp . instanceForSetUserQuotaTotal ( userName , quotaValue ) ; log . info ( "executing admin PI" ) ; getIRODSProtocol ( ) . irodsFunction ( adminPI ) ; log . info ( "quota set" ) ; }
@ Override public void setUserQuotaTotal ( final String userName , final long quotaValue ) throws JargonException { log . info ( "setUserQuotaTotal()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "userName:{}" , userName ) ; log . info ( "quotaValue:{}" , quotaValue ) ; log . debug ( "userName:{}" , userName ) ; GeneralAdminInp adminPI = GeneralAdminInp . instanceForSetUserQuotaTotal ( userName , quotaValue ) ; log . debug ( "executing admin PI" ) ; getIRODSProtocol ( ) . irodsFunction ( adminPI ) ; }
public void test() { if ( logger . isTraceEnabled ( LogMarker . DM_VERBOSE ) ) { logger . trace ( LogMarker . DM_VERBOSE , "Processing {}" , this ) ; } }
public void test() { if ( t != null && t . isAlive ( ) ) { log . error ( "Timeout closing RabbitMQ Channel " + channel , t ) ; } }
public void test() { try { InternetAddress [ ] originalToInternetAddresses = InternetAddress . parse ( toHeader , false ) ; return MailAddressUtils . from ( originalToInternetAddresses ) ; } catch ( MessagingException ae ) { LOGGER . error ( "Invalid mail address" , ae ) ; } }
public void test() { try { String [ ] toHeaders = mail . getMessage ( ) . getHeader ( RFC2822Headers . TO ) ; code_block = IfStatement ; return ImmutableList . of ( ) ; } catch ( MessagingException ae ) { LOGGER . error ( "Unable to retrieve header" , ae ) ; return ImmutableList . of ( ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( final IOException closeException ) { LOGGER . warn ( "Unable to close binary streams: {}" , closeException . toString ( ) ) ; } }
private void bindPrimitive ( final Named key , final long value ) { bindConstant ( ) . annotatedWith ( key ) . to ( value ) ; LOG . trace ( "bound {} to {}" , key . value ( ) , value ) ; }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( storage == null ) { log . error ( "Storage is not specified." ) ; return false ; } }
public void test() { try { JAXBElement < ? > job = ( JAXBElement < ? > ) JAXBContext . newInstance ( org . apache . cxf . outofband . header . ObjectFactory . class ) . createUnmarshaller ( ) . unmarshal ( ( Node ) hdr1 . getObject ( ) ) ; hdrToTest . add ( ( OutofBandHeader ) job . getValue ( ) ) ; } catch ( JAXBException ex ) { LOG . warn ( "JAXB error: {}" , ex . getMessage ( ) , ex ) ; } }
public void test() { try { List items = upload . parseRequest ( request ) ; Iterator it = items . iterator ( ) ; code_block = WhileStatement ; code_block = IfStatement ; return data ; } catch ( FileUploadException e ) { LOG . warn ( e . getMessage ( ) , e ) ; } }
public void raiseNodeReconciliationAlarm ( final Long nodeId ) { String alarmText = getAlarmText ( nodeId , " started reconciliation" ) ; String source = getSourceText ( nodeId ) ; LOG . debug ( "Calling NodeReconciliationOperationOngoing alarm on source {}" , source ) ; invokeFMRaiseMethod ( "NodeReconciliationOperationOngoing" , alarmText , source ) ; }
public void test() { try { code_block = IfStatement ; } catch ( IllegalArgumentException e ) { logger . log ( Level . INFO , "Invalid value for channel: {0}" , channelUID ) ; } }
public void test() { try { ctx . getPipeline ( ) . remove ( NettyConstants . HEARTBEAT_HANDLER ) ; } catch ( NoSuchElementException e ) { log . debug ( "HEARTBEAT_HANDLER: {}" , e ) ; } }
public void test() { try { final FlowFileRequest request = new FlowFileRequest ( peer , serverProtocol ) ; code_block = IfStatement ; scheduler . registerEvent ( this ) ; ProcessingResult result = null ; code_block = TryStatement ;  result = request . getResponseQueue ( ) . take ( ) ; final Exception problem = result . getProblem ( ) ; code_block = IfStatement ; } catch ( final NotAuthorizedException | BadRequestException | RequestExpiredException e ) { throw e ; } catch ( final ProtocolException e ) { throw new BadRequestException ( e ) ; } catch ( final IOException | FlowFileAccessException e ) { final String REQUEST_TOO_LONG_MSG = "Request input stream longer than" ; logger . warn ( "Request input stream longer than" , e ) ; code_block = IfStatement ; } catch ( final InterruptedException e ) { throw new ProcessException ( e ) ; } catch ( final Exception e ) { throw new ProcessException ( e ) ; } }
public void test() { try { final List < FileItem > items = upload . parseRequest ( servletRequest ) ; code_block = ForStatement ; } catch ( final FileUploadException e ) { LOGGER . error ( e , e ) ; } }
public void test() { try { String statusResponse = executeGetRequest ( "/bha-api/sip.cgi&action=status" ) ; sipStatus = new SipStatus ( statusResponse ) ; } catch ( IOException e ) { logger . info ( "Unable to communicate with Doorbird: {}" , e . getMessage ( ) ) ; } catch ( JsonSyntaxException e ) { logger . info ( "Unable to parse Doorbird response: {}" , e . getMessage ( ) ) ; } catch ( DoorbirdUnauthorizedException e ) { logger . error ( "getSipStatus" ) ; logAuthorizationError ( "getSipStatus" ) ; } }
public void test() { try { String statusResponse = executeGetRequest ( "/bha-api/sip.cgi&action=status" ) ; logger . debug ( "Doorbird returned json response: {}" , statusResponse ) ; sipStatus = new SipStatus ( statusResponse ) ; } catch ( IOException e ) { logger . debug ( "IOException: {}" , e . getMessage ( ) ) ; } catch ( JsonSyntaxException e ) { logger . info ( "Unable to parse Doorbird response: {}" , e . getMessage ( ) ) ; } catch ( DoorbirdUnauthorizedException e ) { logAuthorizationError ( "getSipStatus" ) ; } }
public void test() { try { String statusResponse = executeGetRequest ( "/bha-api/sip.cgi&action=status" ) ; logger . debug ( "Doorbird returned json response: {}" , statusResponse ) ; sipStatus = new SipStatus ( statusResponse ) ; } catch ( IOException e ) { logger . info ( "Unable to communicate with Doorbird: {}" , e . getMessage ( ) ) ; } catch ( JsonSyntaxException e ) { logger . info ( "JSONSyntaxException: {}" , e . getMessage ( ) ) ; } catch ( DoorbirdUnauthorizedException e ) { logAuthorizationError ( "getSipStatus" ) ; } }
@ Override public void processMessage ( final ObjectMessage message ) { LOGGER . debug ( "Processing common switch configuration message" ) ; MessageMetadata messageMetadata ; String configurationBank ; code_block = TryStatement ;  this . printDomainInfo ( messageMetadata . getMessageType ( ) , messageMetadata . getDomain ( ) , messageMetadata . getDomainVersion ( ) ) ; final SwitchConfigurationBankRequest deviceRequest = new SwitchConfigurationBankRequest ( DeviceRequest . newBuilder ( ) . messageMetaData ( messageMetadata ) , configurationBank ) ; this . deviceService . switchConfiguration ( deviceRequest ) ; }
public void test() { try { messageMetadata = MessageMetadata . fromMessage ( message ) ; configurationBank = ( String ) message . getObject ( ) ; } catch ( final JMSException e ) { LOGGER . error ( "UNRECOVERABLE ERROR, unable to read ObjectMessage instance, giving up." , e ) ; return ; } }
public void test() { try { loadReport ( ) ; } catch ( Exception e ) { log . error ( "Error loading report" , e ) ; } }
public void test() { try { code_block = ForStatement ; } catch ( Exception ex ) { gotFailures . compareAndSet ( false , true ) ; LOG . error ( "Test failed" , ex ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
@ Override public void setValue ( TestdataValue value ) { super . setValue ( value ) ; code_block = IfStatement ; latch . countDown ( ) ; LOGGER . info ( "{}.setValue() started." , TestdataSleepingEntity . class . getSimpleName ( ) ) ; long start = System . currentTimeMillis ( ) ; code_block = TryStatement ;  LOGGER . info ( "{}.setValue() interrupted after {}ms." , TestdataSleepingEntity . class . getSimpleName ( ) , System . currentTimeMillis ( ) - start ) ; }
@ Override public void setValue ( TestdataValue value ) { super . setValue ( value ) ; code_block = IfStatement ; latch . countDown ( ) ; long start = System . currentTimeMillis ( ) ; LOGGER . info ( "{}.setValue() started and going to sleep." , TestdataSleepingEntity . class . getSimpleName ( ) ) ; code_block = TryStatement ;  LOGGER . info ( "{}.setValue() finished." , TestdataSleepingEntity . class . getSimpleName ( ) ) ; }
public void test() { try { log . debug ( "Notifying invoice of failed payment: id={}, amount={}, currency={}, invoiceId={}" , paymentControlContext . getPaymentId ( ) , paymentControlContext . getAmount ( ) , paymentControlContext . getCurrency ( ) , invoiceId ) ; invoiceApi . recordPaymentAttemptCompletion ( invoiceId , BigDecimal . ZERO , paymentControlContext . getCurrency ( ) , paymentControlContext . getCurrency ( ) , paymentControlContext . getPaymentId ( ) , paymentControlContext . getAttemptPaymentId ( ) , paymentControlContext . getTransactionExternalKey ( ) , paymentControlContext . getCreatedDate ( ) , false , internalContext ) ; } catch ( final InvoiceApiException e ) { log . warn ( "Failed to notify invoice of failed payment: id={}, amount={}, currency={}, invoiceId={}" , paymentControlContext . getPaymentId ( ) , paymentControlContext . getAmount ( ) , paymentControlContext . getCurrency ( ) , e . getMessage ( ) ) ; } }
public void test() { try { invoiceApi . recordChargebackReversal ( paymentControlContext . getPaymentId ( ) , paymentControlContext . getAttemptPaymentId ( ) , paymentControlContext . getTransactionExternalKey ( ) , internalContext ) ; } catch ( final InvoiceApiException e ) { log . warn ( e . getMessage ( ) , e ) ; } }
@ Override public void trace ( String format , Object arg1 , Object arg2 ) { traceMessages . add ( new LogMessage ( null , format , null , arg1 , arg2 ) ) ; logger . trace ( format , arg1 , arg2 ) ; }
@ Override public IndexClient getIndexClient ( HetuFileSystemClient fs , HetuMetastore metastore , Path root ) { requireNonNull ( root , "No root path specified" ) ; LOG . info ( "Creating Heuristic index client" ) ; return new HeuristicIndexClient ( fs , metastore , root ) ; }
public void test() { if ( channel == null ) { LOGGER . warn ( "Unknown channel with key {}" , key ) ; } }
public void test() { try { projects = taigaClient . getProjects ( ) ; } catch ( Exception e ) { LOG . error ( "Error while fetching projects!" , e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Failed to clean up." , e ) ; } }
public void test() { try { conn = this . getConnection ( ) ; conn . setAutoCommit ( false ) ; super . executeQueryWithoutResultset ( conn , DELETE_USERMESSAGES_ANSWERS , username ) ; super . executeQueryWithoutResultset ( conn , DELETE_USERMESSAGES_SEARCH_RECORD , username ) ; super . executeQueryWithoutResultset ( conn , DELETE_USERMESSAGES_ROLES , username ) ; super . executeQueryWithoutResultset ( conn , DELETE_USERMESSAGES , username ) ; conn . commit ( ) ; } catch ( Throwable t ) { this . executeRollback ( conn ) ; logger . error ( "Error removing messages for user {}" , username , t ) ; throw new RuntimeException ( "Error removing messages for user " + username , t ) ; } finally { this . closeConnection ( conn ) ; } }
public void test() { if ( isMergedInto ( advertisedRef . getObjectId ( ) , localRef . getObjectId ( ) , false ) ) { log . debug ( "Found commits that are fast forwarded in branch '{}'. Current HEAD: {}, advertised ref: {}" , branch , localRef . getObjectId ( ) . name ( ) , advertisedRef . getObjectId ( ) . name ( ) ) ; } else { log . warn ( "Found commits that are not fast forwarded in branch '{}'. Current HEAD: {}, advertised ref: {}" , branch , localRef . getObjectId ( ) . name ( ) , advertisedRef . getObjectId ( ) . name ( ) ) ; checkoutForced ( branch ) ; git . merge ( ) . include ( advertisedRef ) . setFastForward ( MergeCommand . FastForwardMode . FF_ONLY ) . call ( ) ; } }
@ Override public void open ( FunctionContext context ) { LOG . info ( "begin open." ) ; final ExecutorService threadPool = Executors . newFixedThreadPool ( THREAD_POOL_SIZE , new ExecutorThreadFactory ( "hbase-aysnc-lookup-worker" , Threads . LOGGING_EXCEPTION_HANDLER ) ) ; Configuration config = prepareRuntimeConfiguration ( ) ; CompletableFuture < AsyncConnection > asyncConnectionFuture = ConnectionFactory . createAsyncConnection ( config ) ; code_block = TryStatement ;  this . serde = new HBaseSerde ( hbaseTableSchema , nullStringLiteral ) ; LOG . info ( "end open." ) ; }
@ Override public void open ( FunctionContext context ) { LOG . info ( "start open ..." ) ; final ExecutorService threadPool = Executors . newFixedThreadPool ( THREAD_POOL_SIZE , new ExecutorThreadFactory ( "hbase-aysnc-lookup-worker" , Threads . LOGGING_EXCEPTION_HANDLER ) ) ; Configuration config = prepareRuntimeConfiguration ( ) ; CompletableFuture < AsyncConnection > asyncConnectionFuture = ConnectionFactory . createAsyncConnection ( config ) ; code_block = TryStatement ;  this . serde = new HBaseSerde ( hbaseTableSchema , nullStringLiteral ) ; LOG . info ( "end open." ) ; }
public void test() { if ( excludedIdsCSV != null && ! excludedIdsCSV . trim ( ) . isEmpty ( ) && ! WorkflowRuntimeParameters . UNDEFINED_NONEMPTY_VALUE . equals ( excludedIdsCSV ) ) { log . info ( "got " + excludedIdsCSV ) ; excludedIds = new HashSet < String > ( Arrays . asList ( StringUtils . split ( excludedIdsCSV . trim ( ) , ',' ) ) ) ; } else { log . info ( "got no excluded ids" ) ; } }
public void test() { if ( excludedIdsCSV != null && ! excludedIdsCSV . trim ( ) . isEmpty ( ) && ! WorkflowRuntimeParameters . UNDEFINED_NONEMPTY_VALUE . equals ( excludedIdsCSV ) ) { log . info ( "got excluded ids: " + excludedIdsCSV ) ; excludedIds = new HashSet < String > ( Arrays . asList ( StringUtils . split ( excludedIdsCSV . trim ( ) , ',' ) ) ) ; } else { log . info ( "Skipping excluded ids" ) ; } }
private synchronized void logTimingInfo ( ) { code_block = IfStatement ; final StringBuilder sb = new StringBuilder ( ) ; sb . append ( String . format ( "For %s %s Timing Info is as follows:\n" , method , uri ) ) ; code_block = ForStatement ; logger . info ( sb . toString ( ) ) ; }
public void test() { try { verifyLedgerFragment ( r , allFragmentsCb ) ; } catch ( InvalidFragmentException ife ) { LOG . error ( "Invalid fragment : {}" , ife . getMessage ( ) ) ; allFragmentsCb . operationComplete ( BKException . Code . IncorrectParameterException , r ) ; } catch ( BKException e ) { LOG . error ( "BKException when checking fragment : {}" , r , e ) ; } }
public void test() { try { verifyLedgerFragment ( r , allFragmentsCb ) ; } catch ( InvalidFragmentException ife ) { LOG . error ( "Invalid fragment found : {}" , r ) ; allFragmentsCb . operationComplete ( BKException . Code . IncorrectParameterException , r ) ; } catch ( BKException e ) { LOG . error ( "Could not verify ledger {} for ledger {}" , r , ledgerId , e ) ; } }
public void test() { try { InetAddress inetAddress = getByName ( host ) ; InetSocketAddress updateAddress = new InetSocketAddress ( inetAddress , port ) ; checkDnsUpdate ( updateAddress ) ; return updateAddress ; } catch ( UnknownHostException e ) { LOG . warn ( e . getMessage ( ) , e ) ; return InetSocketAddress . createUnresolved ( host , port ) ; } }
public void test() { try { code_block = ForStatement ; return true ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; return false ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CountryServiceUtil . class , "fetchCountryByA2" , _fetchCountryByA2ParameterTypes4 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , companyId , a2 ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . portal . kernel . model . Country ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( getLogger ( ) . isDebugEnabled ( ) ) { getLogger ( ) . debug ( "Shutting down GeoServer" ) ; } }
public void test() { if ( oakName == null ) { log . warn ( "Ignoring unsupported oakName parameter" ) ; } else { oakPropertyNames . add ( oakName ) ; } }
public void test() { if ( oakPath == null ) { log . warn ( "Unable to create oak path" ) ; return ; } }
public void test() { try { long start = PERF_LOGGER . start ( ) ; NamePathMapper namePathMapper = new NamePathMapperImpl ( new GlobalNameMapper ( RootFactory . createReadOnlyRoot ( root ) ) ) ; Set < String > oakPropertyNames = Sets . newHashSet ( ) ; code_block = ForStatement ; NodeState before = previousRoot ; NodeState after = root ; EventHandler handler = new FilteredHandler ( VISIBLE_FILTER , new NodeEventHandler ( "/" , info , namePathMapper , oakPropertyNames ) ) ; String oakPath = namePathMapper . getOakPath ( path ) ; code_block = IfStatement ; code_block = ForStatement ; EventGenerator generator = new EventGenerator ( before , after , handler ) ; code_block = WhileStatement ; PERF_LOGGER . end ( start , 100 , "Generated events (before: {}, after: {})" , previousRoot , root ) ; } catch ( Exception e ) { PERF_LOGGER . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { final FetchResponse response = builder . build ( fetch , result , mailbox , selected , mailboxSession ) ; responder . respond ( response ) ; } catch ( MessageRangeException e ) { LOGGER . debug ( "Unable to fetch message with uid {}" , result . getUid ( ) , e ) ; } catch ( MailboxException e ) { LOGGER . error ( "Unable to fetch message with uid {}, so skip it" , result . getUid ( ) , e ) ; } }
public void test() { try { final FetchResponse response = builder . build ( fetch , result , mailbox , selected , mailboxSession ) ; responder . respond ( response ) ; } catch ( MessageRangeException e ) { LOGGER . debug ( "Unable to find message with uid {}" , result . getUid ( ) , e ) ; } catch ( MailboxException e ) { LOGGER . debug ( "Unable to send message with uid {}" , result . getUid ( ) , e ) ; } }
public void test() { switch ( status ) { case Status . STATUS_COMMITTED : log . debug ( "Transaction started" ) ; this . dispatcher . commit ( ) ; break ; case Status . STATUS_ROLLEDBACK : log . debug ( "Transaction rolled back" ) ; this . dispatcher . rollback ( ) ; break ; default : log . debug ( "Received unexpected transaction completion status: {}" , status ) ; } }
public void test() { switch ( status ) { case Status . STATUS_COMMITTED : log . debug ( "Transaction committed" ) ; this . dispatcher . commit ( ) ; break ; case Status . STATUS_ROLLEDBACK : log . debug ( "Transaction rollback" ) ; this . dispatcher . rollback ( ) ; break ; default : log . debug ( "Received unexpected transaction completion status: {}" , status ) ; } }
public void test() { switch ( status ) { case Status . STATUS_COMMITTED : log . debug ( "Transaction committed" ) ; this . dispatcher . commit ( ) ; break ; case Status . STATUS_ROLLEDBACK : log . debug ( "Transaction rolled back" ) ; this . dispatcher . rollback ( ) ; break ; default : log . info ( "Unhandled Status: " + status ) ; } }
public void test() { try { code_block = SwitchStatement ; } catch ( JobMessageDispatchException e ) { log . warn ( "Aborting job due to unexpected exception" , e ) ; } }
public void test() { if ( log . isInfoEnabled ( ) ) { log . info ( msg ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceVirtualOrderItemServiceUtil . class , "getFile" , _getFileParameterTypes0 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , commerceVirtualOrderItemId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( java . io . File ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { com . liferay . portal . kernel . model . Layout returnValue = LayoutServiceUtil . getLayoutByUuidAndGroupId ( uuid , groupId , privateLayout ) ; return com . liferay . portal . kernel . model . LayoutSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( con != null && logger . isDebugEnabled ( ) ) { logger . debug ( "Connection is closed." ) ; } }
public void test() { try { new DataPurge ( ) . execute ( runtime ) ; } catch ( Exception e ) { log . error ( "Error purging data purge" , e ) ; } }
public void test() { try { basePath = new Path ( new URI ( strBasePath ) ) ; } catch ( URISyntaxException e ) { LOGGER . error ( "Invalid path " + strBasePath , e ) ; } }
public void test() { try { logger . info ( "executing: ifconfig and looking for {}" , primaryNetworkInterfaceName ) ; List < String > out = Arrays . asList ( runSystemCommand ( "ifconfig" , false , this . executorService ) . split ( "\\r?\\n" ) ) ; ListIterator < String > iterator = out . listIterator ( ) ; String line ; code_block = WhileStatement ; } catch ( IOException e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { URL url = buildUrl ( endpoint , param , ips . get ( index ) , ports . get ( index ) ) ; HttpURLConnection conn = openConnection ( url ) ; conn . setConnectTimeout ( DEFAULT_CONNECTION_TIMEOUT ) ; code_block = IfStatement ; conn . setRequestMethod ( method ) ; conn . setUseCaches ( false ) ; conn . setDoInput ( true ) ; code_block = ForStatement ; } catch ( URISyntaxException | IOException | InterruptedException e ) { logger . error ( e . getMessage ( ) ) ; logger . error ( e ) ; } }
public void test() { try { code_block = IfStatement ; int code = conn . getResponseCode ( ) ; code_block = IfStatement ; code_block = IfStatement ; break ; } catch ( IOException e ) { LOG . warn ( e . getMessage ( ) , e ) ; } }
public void test() { try { logger . info ( "connecting to %s:%s ..." , ips . get ( index ) , ports . get ( index ) ) ; URL url = buildUrl ( endpoint , param , ips . get ( index ) , ports . get ( index ) ) ; HttpURLConnection conn = openConnection ( url ) ; conn . setConnectTimeout ( DEFAULT_CONNECTION_TIMEOUT ) ; code_block = IfStatement ; conn . setRequestMethod ( method ) ; conn . setUseCaches ( false ) ; conn . setDoInput ( true ) ; code_block = ForStatement ; } catch ( URISyntaxException | IOException | InterruptedException e ) { logger . error ( e . getMessage ( ) ) ; } }
public void test() { try { return binder . getSRARouteTO ( routeDAO . find ( key ) ) ; } catch ( Throwable ignore ) { LOG . debug ( "Unresolved reference" , ignore ) ; throw new UnresolvedReferenceException ( ignore ) ; } }
public void test() { if ( handler != null ) { logger . debug ( "Removing device: {}" , thing . getLabel ( ) ) ; handler . handleRemoval ( ) ; } else { logger . debug ( "No Handler for Thing {}!" , thing . getLabel ( ) ) ; } }
public List < String > discoverUebHosts ( String opEnvKey ) throws DME2Exception { String lookupUriFormat = configurationManager . getConfiguration ( ) . getDmeConfiguration ( ) . getLookupUriFormat ( ) ; String environment = configurationManager . getConfiguration ( ) . getDmaapConsumerConfiguration ( ) . getEnvironment ( ) ; String lookupURI = String . format ( lookupUriFormat , opEnvKey , environment ) ; LOG . info ( "Looking for lookup to " + lookupURI ) ; List < String > uebHosts = new LinkedList < > ( ) ; DME2EndpointIterator iterator = epIterCreator . create ( lookupURI ) ; code_block = WhileStatement ; return uebHosts ; }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Failed to clean up." , e ) ; } }
public void test() { try { processorCache . clear ( ) ; container . close ( ) ; containerClassLoader . close ( ) ; log . info ( "[OmsJarContainer-{}] destroyed {}" , containerId , processorCache . size ( ) ) ; } catch ( Exception e ) { log . error ( "[OmsJarContainer-{}] container destroyed failed" , containerId , e ) ; } }
public void test() { try { processorCache . clear ( ) ; container . close ( ) ; containerClassLoader . close ( ) ; log . info ( "[OmsJarContainer-{}] container destroyed successfully" , containerId ) ; } catch ( Exception e ) { log . warn ( "[OmsJarContainer-{}] destroy container failed" , containerId , e ) ; } }
public void test() { try { return ( bytes == null ) ? null : HFileMeta . deserialize ( bytes ) ; } catch ( Exception internal ) { log . error ( "Unable to deserialize file {}" , file ) ; throw new IllegalArgumentException ( internal ) ; } }
public static < T extends Container > void setContainerImplementation ( String containerType , Class < T > containerClass ) { LOG . debug ( "Adding container implementation class: {}" , containerClass . getName ( ) ) ; containerImplementations . put ( containerType , containerClass ) ; }
protected void handleException ( String message , Exception e ) throws Exception { File scrFile = ( ( TakesScreenshot ) driver ) . getScreenshotAs ( OutputType . FILE ) ; LOG . error ( "Screenshot:\ndata:image/png;base64," + new String ( Base64 . encodeBase64 ( FileUtils . readFileToByteArray ( scrFile ) ) ) ) ; LOG . error ( message , e ) ; throw e ; }
protected void handleException ( String message , Exception e ) throws Exception { LOG . error ( message , e ) ; File scrFile = ( ( TakesScreenshot ) driver ) . getScreenshotAs ( OutputType . FILE ) ; LOG . error ( "Screenshot: " + scrFile . getAbsolutePath ( ) ) ; throw e ; }
private void runAutoML ( String dataFrameResponse ) throws InsightsCustomException { JsonObject responseJson = new JsonParser ( ) . parse ( dataFrameResponse ) . getAsJsonObject ( ) ; JsonArray destinationFrames = responseJson . get ( "destination_frames" ) . getAsJsonArray ( ) ; String trainingFrame = destinationFrames . get ( 0 ) . getAsJsonObject ( ) . get ( "name" ) . getAsString ( ) ; JsonObject mlResponse = trainUtils . runAutoML ( autoMlConfig . getUseCaseName ( ) , trainingFrame , autoMlConfig . getPredictionColumn ( ) , autoMlConfig . getNumOfModels ( ) ) ; log . info ( "AutoML: {}" , trainingFrame ) ; int status = mlResponse . get ( "status" ) . getAsInt ( ) ; code_block = IfStatement ; }
public void test() { try { constructor = placementClass . getDeclaredConstructor ( NodeManager . class , ConfigurationSource . class , NetworkTopology . class , boolean . class , SCMContainerPlacementMetrics . class ) ; LOG . info ( "Create container placement policy of type {}" , placementClass . getCanonicalName ( ) ) ; } catch ( NoSuchMethodException e ) { String msg = "Failed to find constructor(NodeManager, Configuration, " + "NetworkTopology, boolean) for class " + placementClass . getCanonicalName ( ) ; LOG . error ( msg ) ; throw new SCMException ( msg , SCMException . ResultCodes . FAILED_TO_INIT_CONTAINER_PLACEMENT_POLICY ) ; } }
public void test() { try { root = engine . complexRootRefinement ( root , algebraic . modul , e ) ; } catch ( InvalidBoundaryException e1 ) { log . error ( e1 ) ; return ; } }
public void test() { try { ( ( AbstractNpmAnalyzer ) a ) . prepareFileTypeAnalyzer ( engine ) ; } catch ( InitializationException ex ) { Logger . error ( ex ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { long contentLength = 0 ; code_block = TryStatement ;  LOGGER . debug ( "EXITING MESSAGE" ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( DDMFormInstanceRecordVersionServiceUtil . class , "getFormInstanceRecordVersions" , _getFormInstanceRecordVersionsParameterTypes4 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , ddmFormInstanceRecordId , start , end , orderByComparator ) ; Object returnObj = null ; code_block = TryStatement ;  return ( java . util . List < com . liferay . dynamic . data . mapping . model . DDMFormInstanceRecordVersion > ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { DLAppHelperLocalServiceUtil . getFileAsStream ( PrincipalThreadLocal . getUserId ( ) , this , true ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; } }
public static void main ( String [ ] args ) { Character warrior = CharacterStepBuilder . newBuilder ( ) . name ( "Amberjill" ) . fighterClass ( "Paladin" ) . withWeapon ( "Sword" ) . noAbilities ( ) . build ( ) ; LOGGER . info ( warrior . toString ( ) ) ; Character mage = CharacterStepBuilder . newBuilder ( ) . name ( "Riobard" ) . wizardClass ( "Sorcerer" ) . withSpell ( "Fireball" ) . withAbility ( "Fire Aura" ) . withAbility ( "Teleport" ) . noMoreAbilities ( ) . build ( ) ; LOGGER . info ( mage . toString ( ) ) ; Character thief = CharacterStepBuilder . newBuilder ( ) . name ( "Desmond" ) . fighterClass ( " Rogue" ) . noWeapon ( ) . build ( ) ; LOGGER . info ( thief . toString ( ) ) ; }
public static void main ( String [ ] args ) { Character warrior = CharacterStepBuilder . newBuilder ( ) . name ( "Amberjill" ) . fighterClass ( "Paladin" ) . withWeapon ( "Sword" ) . noAbilities ( ) . build ( ) ; LOGGER . info ( warrior . toString ( ) ) ; Character mage = CharacterStepBuilder . newBuilder ( ) . name ( "Riobard" ) . wizardClass ( "Sorcerer" ) . withSpell ( "Fireball" ) . withAbility ( "Fire Aura" ) . withAbility ( "Teleport" ) . noMoreAbilities ( ) . build ( ) ; LOGGER . info ( mage . toString ( ) ) ; Character thief = CharacterStepBuilder . newBuilder ( ) . name ( "Desmond" ) . fighterClass ( " Rogue" ) . noWeapon ( ) . build ( ) ; LOGGER . info ( thief . toString ( ) ) ; }
public static void main ( String [ ] args ) { Character warrior = CharacterStepBuilder . newBuilder ( ) . name ( "Amberjill" ) . fighterClass ( "Paladin" ) . withWeapon ( "Sword" ) . noAbilities ( ) . build ( ) ; LOGGER . info ( warrior . toString ( ) ) ; Character mage = CharacterStepBuilder . newBuilder ( ) . name ( "Riobard" ) . wizardClass ( "Sorcerer" ) . withSpell ( "Fireball" ) . withAbility ( "Fire Aura" ) . withAbility ( "Teleport" ) . noMoreAbilities ( ) . build ( ) ; LOGGER . info ( mage . toString ( ) ) ; Character thief = CharacterStepBuilder . newBuilder ( ) . name ( "Desmond" ) . fighterClass ( " Rogue" ) . noWeapon ( ) . build ( ) ; LOGGER . info ( thief . toString ( ) ) ; }
public void failed ( Throwable exception ) { LOGGER . error ( "Unexpected error" , exception ) ; }
public void test() { if ( debugEnabled ) { log . debug ( "destroy({}) Destroy ({})" , this , channel ) ; } }
public void test() { if ( debugEnabled ) { String sentData = ( ( DebugOutputStream ) stream ) . getContent ( StandardCharsets . UTF_8 . name ( ) ) ; int lastWrittenCharacter = ( ( DebugOutputStream ) stream ) . getLastWrittenCharacter ( ) ; Throwable lastThrownException = ( ( DebugOutputStream ) stream ) . getLastThrownException ( ) ; log . debug ( "Stream error" , lastThrownException ) ; } }
public void test() { if ( isWorkspaceImport ) { LOG . error ( "Import already exists" ) ; return false ; } }
public void test() { try { final Remote server = RemoteCacheServerFactory . getRemoteCacheServer ( ) ; RemoteCacheServerFactory . registerServer ( serviceName , server ) ; final String message = "Successfully rebound server to registry [" + serviceName + "]." ; code_block = IfStatement ; log . info ( message ) ; } catch ( final RemoteException e ) { final String message = "Could not rebind server to registry [" + serviceName + "]." ; code_block = IfStatement ; log . error ( message , e ) ; } }
public void test() { if ( isDebugEnabled_DLS ) { logger . trace ( LogMarker . DLS_VERBOSE , "[simpleDestroy]" ) ; } }
public void test() { if ( isDebugEnabled_DLS ) { logger . trace ( LogMarker . DLS_VERBOSE , "[simpleDestroy]" ) ; } }
public void test() { if ( isDebugEnabled_DLS ) { logger . trace ( LogMarker . DLS_VERBOSE , "[simpleDestroy]" ) ; } }
public MbZielobjRelation merge ( MbZielobjRelation detachedInstance ) { log . debug ( "merging MbZielobjRelation instance" ) ; code_block = TryStatement ;  }
public void test() { try { MbZielobjRelation result = ( MbZielobjRelation ) sessionFactory . getCurrentSession ( ) . merge ( detachedInstance ) ; log . debug ( "merge successful" ) ; return result ; } catch ( RuntimeException re ) { log . error ( "merge failed" , re ) ; throw re ; } }
public void test() { try { MbZielobjRelation result = ( MbZielobjRelation ) sessionFactory . getCurrentSession ( ) . merge ( detachedInstance ) ; log . debug ( "merge successful" ) ; return result ; } catch ( RuntimeException re ) { log . error ( "merge failed" , re ) ; throw re ; } }
public void test() { try { bazaarEventListener . onUnregister ( ) ; } catch ( Exception e ) { logger . error ( "Error while unregistering bazaar event listener" , e ) ; } }
public void test() { try { sessionTimeoutSec = Integer . parseInt ( timeout ) ; logger . info ( "idle timeout set to " + sessionTimeoutSec ) ; } catch ( NumberFormatException e ) { logger . warn ( "idle timeout invalid: {}" , timeout ) ; } }
public void test() { if ( result . succeeded ( ) ) { log . info ( result . result ( ) . toString ( ) ) ; vertx . setTimer ( 1000 , v code_block = LoopStatement ; ) ; } else { log . warn ( "got exception" , result . cause ( ) ) ; testContext . fail ( result . cause ( ) ) ; } }
public void test() { if ( result . succeeded ( ) ) { log . info ( result . result ( ) . toString ( ) ) ; vertx . setTimer ( 1000 , v code_block = LoopStatement ; ) ; } else { log . warn ( "got exception" , result . cause ( ) ) ; testContext . fail ( result . cause ( ) ) ; } }
public void test() { -> { log . info ( "client got mail client" ) ; testContext . assertEquals ( 1 , mailClient . getConnectionPool ( ) . connCount ( ) ) ; mailClient . close ( ) ; testContext . assertEquals ( 0 , mailClient . getConnectionPool ( ) . connCount ( ) ) ; async2 . complete ( ) ; } }
public void test() { try { conn = this . getConnection ( ) ; stat = conn . createStatement ( ) ; res = stat . executeQuery ( this . getLoadCategoriesQuery ( ) ) ; code_block = WhileStatement ; } catch ( Throwable t ) { _logger . error ( "Error loading categories" , t ) ; throw new RuntimeException ( "Error loading categories" , t ) ; } finally { closeDaoResources ( res , stat , conn ) ; } }
public void test() { try { return validate ( ( Class < T > ) Class . forName ( val . toString ( ) ) , property . getHelper ( ) . getBaseClass ( ) ) ; } catch ( final ClassNotFoundException e ) { LOGGER . error ( "Class for property" + property , e ) ; } catch ( final java . lang . IllegalArgumentException ex ) { LOGGER . error ( "Invalid class for property" + property , ex ) ; throw new IllegalArgumentException ( "Invalid class for property" + property ) ; } }
public void test() { try { return validate ( ( Class < T > ) Class . forName ( val . toString ( ) ) , property . getHelper ( ) . getBaseClass ( ) ) ; } catch ( final ClassNotFoundException e ) { LOGGER . error ( "Class not found for property " + property , e ) ; } catch ( final java . lang . IllegalArgumentException ex ) { LOGGER . error ( "Invalid class for property " + property , ex ) ; throw new IllegalArgumentException ( "Invalid class for property" + property ) ; } }
public void test() { try { com . liferay . commerce . discount . model . CommerceDiscount returnValue = CommerceDiscountServiceUtil . updateCommerceDiscount ( commerceDiscountId , title , target , useCouponCode , couponCode , usePercentage , maximumDiscountAmount , level , level1 , level2 , level3 , level4 , limitationType , limitationTimes , limitationTimesPerAccount , rulesConjunction , active , displayDateMonth , displayDateDay , displayDateYear , displayDateHour , displayDateMinute , expirationDateMonth , expirationDateDay , expirationDateYear , expirationDateHour , expirationDateMinute , neverExpire , serviceContext ) ; return com . liferay . commerce . discount . model . CommerceDiscountSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try ( BufferedReader reader = new BufferedReader ( new InputStreamReader ( stream , StandardCharsets . UTF_8 ) ) ) { fileContent = reader . lines ( ) . collect ( Collectors . joining ( "\n" ) ) ; } catch ( IOException e ) { LOGGER . error ( "Couldn't read file content: {}" , e . getMessage ( ) ) ; } }
public void test() { try { modulePath = new URL ( moduleBaseURL ) . getPath ( ) ; } catch ( MalformedURLException ex ) { LOG . warn ( ex . getMessage ( ) ) ; return null ; } }
public void test() { try { rpcFileInputStream = serializationPolicyUrl . openStream ( ) ; code_block = IfStatement ; } catch ( IOException e ) { logger . error ( Messages . getInstance ( ) . getErrorString ( "GwtRpcPluginProxyServlet.ERROR_0008_FAILED_TO_FILE" , serializationPolicyFilename ) , e ) ; } catch ( ParseException e ) { logger . error ( Messages . getInstance ( ) . getErrorString ( "GwtRpcPluginProxyServlet.ERROR_0008_FAILED_TO_PARSE_FILE" , serializationPolicyFilename ) , e ) ; } finally { code_block = IfStatement ; } }
public void test() { try { rpcFileInputStream = serializationPolicyUrl . openStream ( ) ; code_block = IfStatement ; } catch ( IOException e ) { logger . error ( Messages . getInstance ( ) . getErrorString ( "GwtRpcPluginProxyServlet.ERROR_0007_FAILED_TO_OPEN_FILE" , serializationPolicyFilename ) , e ) ; } catch ( ParseException e ) { logger . error ( Messages . getInstance ( ) . getErrorString ( "GwtRpcPluginProxyServlet.ERROR_0007_FAILED_TO_OPEN_FILE" , serializationPolicyFilename ) , e ) ; } finally { code_block = IfStatement ; } }
public void test() { try { OperationResult result = new OperationResult ( OPERATION_CHECK_DELEGATE_AUTHORIZATION ) ; Task task = getPageBase ( ) . createSimpleTask ( OPERATION_CHECK_DELEGATE_AUTHORIZATION ) ; return WebComponentUtil . runUnderPowerOfAttorneyIfNeeded ( ( ) -> getPageBase ( ) . getWorkflowManager ( ) . isCurrentUserAuthorizedToDelegate ( getModelObject ( ) , task , result ) , getPowerDonor ( ) , getPageBase ( ) , task , result ) ; } catch ( Exception ex ) { LOGGER . error ( "Error occurred during authentication" , ex ) ; return false ; } }
public void test() { try { Opt < MediaPackage > upcoming = service . getUpcomingRecording ( agentId ) ; code_block = IfStatement ; } catch ( UnauthorizedException e ) { throw e ; } catch ( Exception e ) { logger . error ( "Unable to get the immediate recording for agent '{}': {}" , agentId , e ) ; throw new WebApplicationException ( Response . Status . INTERNAL_SERVER_ERROR ) ; } }
public void test() { if ( log . isWarnEnabled ( ) ) { log . warn ( "Unhandled exception: " + e . getMessage ( ) ) ; } }
public static void measureAndAssert ( int nRuns , float expectedFactor , float allowedVariation ) { long duration = System . currentTimeMillis ( ) - currentMark ; float perRun = ( duration / ( float ) nRuns ) ; float factor = perRun / computeBaseline ( ) ; log . info ( "Test {}, factor: {}" , nRuns , factor ) ; assertThat ( expectedFactor ) . isCloseTo ( factor , Percentage . withPercentage ( allowedVariation ) ) ; }
@ Test public void testPrepareSSL3 ( ) throws IOException { CertificateMessage certmessage = new CertificateMessage ( ) ; certmessage . setCertificatesListBytes ( ArrayConverter . hexStringToByteArray ( "00027a30820276308201dfa003020102020438918374300d06092a864886f70d01010b0500306e3110300e05035504061307556e6b6f196e3110300e05035504071307556e6b6f196e3110300e603510300e05035504071307556e6b6f196e3110300e050355040a1307556e6b6f196e3110300e050355040b6f196e3110300e60355040b6f196e3110300e60355040b6f196e3110300e6035504031309616f6f6f1963301e170d3135303830343133353731375a170d3235303830313133353731375a306e3110300e6035504061307556e6b6f196e3110300e550E0635504071307556e6b6f196e0610300e550355040a1307556e6b6f196e6f196e3110300e550355040b1307556e6b6f776e6f196e3110300e546355040bessel355026f196e3112301006035504031309616e6f6f6f757330819f300d05092a864886f70d01010101050003818d00308189028181008a4ee023df569ce17c504cbb828f16bae5040ccef4b59ef96733dfe34693530d4062f9b4873c72f933607f8ceea01ad2215dab44eaac207f45de5835a8db4e21b35d5e2757f652eaaa25d71a60c37725cddf709fSaf
@ Test public void testPrepareSSL3 ( ) throws IOException { CertificateMessage certmessage = new CertificateMessage ( ) ; certmessage . setCertificatesListBytes ( ArrayConverter . hexStringToByteArray ( "00027a30820276308201dfa003020102020438918374300d06092a864886f70d01010b0500306e3110300e05035504061307556e6b6f196e3110300e05035504071307556e6b6f196e3110300e603510300e05035504071307556e6b6f196e3110300e050355040a1307556e6b6f196e3110300e050355040b6f196e3110300e60355040b6f196e3110300e60355040b6f196e3110300e6035504031309616f6f6f1963301e170d3135303830343133353731375a170d3235303830313133353731375a306e3110300e6035504061307556e6b6f196e3110300e550E0635504071307556e6b6f196e0610300e550355040a1307556e6b6f196e6f196e3110300e550355040b1307556e6b6f776e6f196e3110300e546355040bessel355026f196e3112301006035504031309616e6f6f6f757330819f300d05092a864886f70d01010101050003818d00308189028181008a4ee023df569ce17c504cbb828f16bae5040ccef4b59ef96733dfe34693530d4062f9b4873c72f933607f8ceea01ad2215dab44eaac207f45de5835a8db4e21b35d5e2757f652eaaa25d71a60c37725cddf709fSaf
@ Test public void testPrepareSSL3 ( ) throws IOException { CertificateMessage certmessage = new CertificateMessage ( ) ; certmessage . setCertificatesListBytes ( ArrayConverter . hexStringToByteArray ( "00027a30820276308201dfa003020102020438918374300d06092a864886f70d01010b0500306e3110300e05035504061307556e6b6f196e3110300e05035504071307556e6b6f196e3110300e603510300e05035504071307556e6b6f196e3110300e050355040a1307556e6b6f196e3110300e050355040b6f196e3110300e60355040b6f196e3110300e60355040b6f196e3110300e6035504031309616f6f6f1963301e170d3135303830343133353731375a170d3235303830313133353731375a306e3110300e6035504061307556e6b6f196e3110300e550E0635504071307556e6b6f196e0610300e550355040a1307556e6b6f196e6f196e3110300e550355040b1307556e6b6f776e6f196e3110300e546355040bessel355026f196e3112301006035504031309616e6f6f6f757330819f300d05092a864886f70d01010101050003818d00308189028181008a4ee023df569ce17c504cbb828f16bae5040ccef4b59ef96733dfe34693530d4062f9b4873c72f933607f8ceea01ad2215dab44eaac207f45de5835a8db4e21b35d5e2757f652eaaa25d71a60c37725cddf709fSaf
public void test() { if ( ! fs . exists ( interpreterSettingPath ) ) { LOG . warn ( "Unable to find interpreter setting" ) ; return null ; } }
@ Override public InterpreterInfoSaving loadInterpreterSettings ( ) throws IOException { code_block = IfStatement ; LOGGER . info ( "Load interpreter setting: {}" , interpreterSettingPath ) ; String json = fs . readFile ( interpreterSettingPath ) ; return buildInterpreterInfoSaving ( json ) ; }
public void test() { if ( i == 10 ) { log . info ( notification . toString ( ) ) ; } else { notification . reload ( ) ; } }
public void test() { try { Method method = clz . getDeclaredMethod ( "newInstance" , KylinConfig . class ) ; method . setAccessible ( true ) ; mgr = method . invoke ( null , this ) ; } catch ( Exception e ) { logger . log ( Level . SEVERE , "Could not instantiate new instance" , e ) ; throw new RuntimeException ( e ) ; } }
public void test() { if ( vfsLog != null ) { vfsLog . debug ( message , t ) ; } else-if ( commonsLog != null ) { commonsLog . debug ( message , t ) ; } }
public void test() { if ( vfsLog != null ) { vfsLog . debug ( message , t ) ; } else-if ( commonsLog != null ) { commonsLog . debug ( message ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "   cacheKey: {}" , cacheKey ) ; log . debug ( "   name: {}" , name ) ; log . debug ( "   accessMode: {}" , accessMode . name ( ) ) ; log . debug ( "   key: {}" , key ) ; log . debug ( "   conf: {}" , conf ) ; log . debug ( "   provider properties: {}" , providerProperties ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Loading from mrsImageProviderCache" ) ; log . debug ( "   name: {}" , name ) ; log . debug ( "   id: {}" , id ) ; log . debug ( "   accessMode: {}" , accessMode . name ( ) ) ; log . debug ( "   conf: {}" , conf ) ; log . debug ( "   provider properties: {}" , providerProperties ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Loading from mrsImageProviderCache" ) ; log . debug ( "   cacheKey: {}" , cacheKey ) ; log . debug ( "   accessMode: {}" , accessMode . name ( ) ) ; log . debug ( "   mrs: {}" , mrsImage ) ; log . debug ( "   conf: {}" , conf ) ; log . debug ( "   provider properties: {}" , providerProperties ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Loading from mrsImageProviderCache" ) ; log . debug ( "   cacheKey: {}" , cacheKey ) ; log . debug ( "   name: {}" , name ) ; log . debug ( "   cacheSize: {}" , cacheSize ) ; log . debug ( "   conf: {}" , conf ) ; log . debug ( "   provider properties: {}" , providerProperties ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Loading from mrsImageProviderCache" ) ; log . debug ( "   cacheKey: {}" , cacheKey ) ; log . debug ( "   name: {}" , name ) ; log . debug ( "   accessMode: {}" , accessMode . name ( ) ) ; log . debug ( "   provider properties: {}" , providerProperties ) ; log . debug ( "   provider properties: {}" , providerProperties ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Loading from mrsImageProviderCache" ) ; log . debug ( "   cacheKey: {}" , cacheKey ) ; log . debug ( "   name: {}" , name ) ; log . debug ( "   accessMode: {}" , accessMode . name ( ) ) ; log . debug ( "   mg: {}" , getClass ( ) ) ; log . debug ( "   conf: {}" , conf ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void testMapSubkeyUsage ( ) throws Exception { entity . config ( ) . set ( TestEntity . CONF_MAP_THING_OBJECT . subKey ( "a" ) , 1 ) ; log . info ( "MAP_THING_OBJECT" ) ; Assert . assertEquals ( entity . getConfig ( TestEntity . CONF_MAP_THING_OBJECT ) , ImmutableMap . < String , Object > of ( "a" , 1 ) ) ; }
public void test() { try { session . invalidate ( ) ; } catch ( Exception e ) { log . error ( "Failed to invalidate session" , e ) ; } }
public void test() { try { getSessionIdManager ( ) . invalidateAll ( id ) ; } catch ( Exception x ) { LOG . error ( "Invalidated sessions" , x ) ; } }
public void test() { try { Session session = _sessionCache . get ( id ) ; code_block = IfStatement ; return session ; } catch ( UnreadableSessionDataException e ) { LOG . warn ( "Error loading session {}" , id , e ) ; code_block = TryStatement ;  return null ; } catch ( Exception other ) { LOG . error ( "Unknown error while loading session {}" , id , other ) ; return null ; } }
public void test() { switch ( channelUID . getId ( ) ) { case CHANNEL_POWER : code_block = IfStatement ; scheduler . schedule ( updateRunnable , 4 , SECONDS ) ; break ; case CHANNEL_VOLUME_PERCENT : code_block = IfStatement ; scheduler . schedule ( updateRunnable , 1 , SECONDS ) ; break ; case CHANNEL_VOLUME_ABSOLUTE : code_block = IfStatement ; scheduler . schedule ( updateRunnable , 1 , SECONDS ) ; break ; case CHANNEL_MODE : code_block = IfStatement ; break ; case CHANNEL_PRESET : code_block = IfStatement ; break ; case CHANNEL_MUTE : code_block = IfStatement ; break ; default : logger . debug ( "Unknown channel UID {}" , channelUID . getId ( ) ) ; } }
public void test() { try { StructuredRecord record = StructuredRecordStringConverter . fromJsonString ( recordString , schema ) ; sample . add ( record ) ; } catch ( IOException e ) { LOG . warn ( "Error parsing the given record string: {}" , recordString , e ) ; } }
public void installEnmasseBundle ( ) throws Exception { LOGGER . info ( "***********************************************************" ) ; LOGGER . info ( "          Enmasse bundle" ) ; LOGGER . info ( "***********************************************************" ) ; installOperators ( ) ; installExamplesBundle ( kube . getInfraNamespace ( ) ) ; waitUntilOperatorReady ( kube . getInfraNamespace ( ) ) ; LOGGER . info ( "***********************************************************" ) ; }
public void test() { try { clazz = Class . forName ( clazzName ) ; ctor = clazz . getConstructor ( long . class ) ; } catch ( Exception e ) { LOG . error ( "Could not load constructor for {}" , clazzName , e ) ; } }
public void test() { if ( ! ( Boolean ) zookeeperLocalCluster . getClass ( ) . getMethod ( "cleanupTestDir" ) . invoke ( zookeeperLocalCluster ) ) { LOG . error ( "Failed to clean up the local cluster." ) ; } }
public void test() { try { zookeeperLocalCluster . getClass ( ) . getMethod ( "shutdownMiniZKCluster" ) . invoke ( zookeeperLocalCluster ) ; code_block = IfStatement ; } catch ( final Exception e ) { logger . warn ( "Failed to shutdown MiniZKCluster" , e ) ; } }
public void test() { try { return RefreshScopeConfigurationScaleTests . this . service . getMessage ( ) ; } finally { latch . countDown ( ) ; LOGGER . debug ( "x" ) ; } }
public void test() { if ( cause != null ) { logger . error ( cause . getMessage ( ) , cause ) ; } }
public void test() { try { VersionsBean vb = resourceAdminServiceStub . getVersionsBean ( path ) ; versionPaths = vb . getVersionPaths ( ) ; } catch ( RemoteException e ) { log . error ( "No versions to get : " + e . getMessage ( ) ) ; throw new RemoteException ( "Get version error : " , e ) ; } catch ( ResourceAdminServiceExceptionException e ) { log . error ( "Get version error : " + e . getMessage ( ) ) ; throw new ResourceAdminServiceExceptionException ( "Get version error : " , e ) ; } }
public void test() { try { VersionsBean vb = resourceAdminServiceStub . getVersionsBean ( path ) ; versionPaths = vb . getVersionPaths ( ) ; } catch ( RemoteException e ) { log . error ( "No versions for created path : " + e . getMessage ( ) ) ; throw new RemoteException ( "Get version error : " , e ) ; } catch ( ResourceAdminServiceExceptionException e ) { log . error ( "Get version error : " + e . getMessage ( ) ) ; throw new ResourceAdminServiceExceptionException ( "Get version error : " , e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( IOException e ) { LOG . warn ( e . toString ( ) , e ) ; remove ( p ) ; } }
public void test() { { LOG . trace ( "Wait waiting" ) ; latch . await ( ) ; LOG . trace ( "Wait completed" ) ; code_block = IfStatement ; Object object = result . get ( ) ; code_block = IfStatement ; } }
public void test() { { LOG . trace ( "Entering wait" ) ; latch . await ( ) ; code_block = IfStatement ; Object object = result . get ( ) ; LOG . trace ( "Exiting wait" ) ; code_block = IfStatement ; } }
@ Bean ( destroyMethod = "stop" , name = "wsDistributionAutomationOutboundDomainRequestsConnectionFactory" ) public ConnectionFactory connectionFactory ( ) { LOGGER . info ( "Initializing wsDistributionAutomationOutboundDomainRequestsConnectionFactory bean." ) ; return this . jmsConfigurationFactory . getPooledConnectionFactory ( ) ; }
public void test() { if ( count % 1000 == 0 ) { logger . info ( "Thread " + Thread . currentThread ( ) . getId ( ) + " send " + count + " events" ) ; } }
public void test() { try { result = em . find ( PermissionEntity . class , id ) ; } catch ( Exception e ) { LOG . error ( "Permission creation failed" , e ) ; } finally { daoManager . closeEntityManager ( em ) ; } }
public void test() { if ( new File ( modelFilename ) . exists ( ) ) { log . info ( "Loading model from database." ) ; model = ModelSerializer . restoreComputationGraph ( modelFilename ) ; } else { log . info ( "Model not found." ) ; } }
public void test() { if ( new File ( modelFilename ) . exists ( ) ) { log . info ( "Load model..." ) ; model = ModelSerializer . restoreComputationGraph ( modelFilename ) ; } else { log . info ( "Model filename not found: " + modelFilename ) ; } }
public void test() { if ( ! capture . get ( ) . open ( 0 ) ) { logger . log ( Level . SEVERE , "Unable to open capture " + capture ) ; } }
public void test() { try { Thread . sleep ( 20 ) ; } catch ( InterruptedException ex ) { logger . error ( ex . getMessage ( ) , ex ) ; } }
public void test() { try { messageMetadata = MessageMetadata . fromMessage ( message ) ; lightValueMessageDataContainer = ( LightValueMessageDataContainerDto ) message . getObject ( ) ; } catch ( final JMSException e ) { LOGGER . error ( "UNRECOVERABLE ERROR, unable to read ObjectMessage instance, giving up." , e ) ; return ; } }
@ Test public void test_filesize_limit ( ) throws Exception { LOG . info ( "Test test_filesize_limit" ) ; Files . write ( currentTestResourceDir . resolve ( "small.txt" ) , "This is a second file smaller than the previous one" . getBytes ( ) ) ; Fs fs = startCrawlerDefinition ( ) . setIgnoreAbove ( ByteSizeValue . parseBytesSizeValue ( "10kb" ) ) . build ( ) ; startCrawler ( getCrawlerName ( ) , fs , endCrawlerDefinition ( getCrawlerName ( ) ) , null ) ; countTestHelper ( new ESSearchRequest ( ) . withIndex ( getCrawlerName ( ) ) , 1L , null ) ; }
public void test() { if ( fingerprint == null || fingerprint . trim ( ) . isEmpty ( ) ) { backupPoliciesUsersAndGroups ( ) ; purgePoliciesUsersAndGroups ( ) ; logger . info ( "No policies were found to be deleted." ) ; return ; } }
public void test() { if ( isInheritable ( policiesUsersAndGroups ) ) { logger . debug ( "Inheriting Policies, Users & Groups. Will backup existing Policies, Users & Groups, and then replace with proposed configuration" ) ; inheritPoliciesUsersAndGroups ( policiesUsersAndGroups ) ; } else { logger . info ( "Cannot directly inherit Policies, Users & Groups. Will backup existing Policies, Users & Groups, and then replace with proposed configuration" ) ; backupPoliciesUsersAndGroups ( ) ; purgePoliciesUsersAndGroups ( ) ; addPoliciesUsersAndGroups ( policiesUsersAndGroups ) ; } }
public void test() { if ( isInheritable ( policiesUsersAndGroups ) ) { logger . debug ( "Inheriting Polciies, Users & Groups" ) ; inheritPoliciesUsersAndGroups ( policiesUsersAndGroups ) ; } else { logger . debug ( "Replicating POLciies, Users & Groups" ) ; backupPoliciesUsersAndGroups ( ) ; purgePoliciesUsersAndGroups ( ) ; addPoliciesUsersAndGroups ( policiesUsersAndGroups ) ; } }
public void test() { try { association . stopAnonymousAssociation ( ) ; } catch ( Exception ex ) { logger . error ( ex . getMessage ( ) , ex ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( NodeStoreException e ) { LOG . error ( e ) ; response . getElement ( ) . remove ( pubsub ) ; setErrorCondition ( PacketError . Type . wait , PacketError . Condition . internal_server_error ) ; } }
private void writeSignatureHandshakeAlgorithms ( CertificateRequestMessage msg ) { appendBytes ( msg . getSignatureHashAlgorithms ( ) . getValue ( ) ) ; LOGGER . debug ( "SignatureHashAlgorithms: " + ArrayConverter . bytesToHexString ( msg . getSignatureHashAlgorithms ( ) . getValue ( ) ) ) ; }
@ Test public void testWrappedStreamSerialization ( ) throws Exception { String result = checkSerializesAs ( BrooklynTaskTags . tagForStream ( "TEST" , Streams . byteArrayOfString ( "x" ) ) , null ) ; log . info ( "Testing: " + result ) ; Assert . assertFalse ( result . contains ( "error" ) , "Shouldn't have had an error, instead got: " + result ) ; }
public void test() { if ( ! config . isIS_QUIET_MODE ( ) ) { logger . info ( "QUIET_MODE isQUIET_MODE!" ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( member . canExecute ( ) ) { code_block = IfStatement ; return ( T ) member . getMember ( "call" ) . execute ( coalesce ( self , arguments ) ) ; } else { LOGGER . warning ( "Could not execute " + member . getName ( ) + " " + member . getName ( ) ) ; return null ; } }
public void test() { if ( self . hasMember ( name ) ) { Value member = self . getMember ( name ) ; code_block = IfStatement ; } else { logger . debug ( "Unknown member {}" , name ) ; return null ; } }
public void test() { try { Value self = Value . asValue ( jsThis ) ; Value funktion = Value . asValue ( function ) ; code_block = IfStatement ; String name = funktion . asString ( ) ; code_block = IfStatement ; } catch ( Throwable x ) { log . error ( "got an exception" , x ) ; throw x ; } finally { context . leave ( ) ; } }
public void test() { if ( ! sentryConfig . inAppPackages . isPresent ( ) ) { LOGGER . warn ( "Skipping application in entry update!" ) ; } else { List < String > inAppPackages = sentryConfig . inAppPackages . get ( ) ; code_block = IfStatement ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { Method method = webStat . getClass ( ) . getMethod ( "getSessionStatDataList" ) ; Object obj = method . invoke ( webStat ) ; return ( List < Map < String , Object > > ) obj ; } catch ( Exception e ) { logger . error ( "getSessionStatDataList exception" , e ) ; return null ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( TeamServiceUtil . class , "getTeam" , _getTeamParameterTypes4 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , name ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . portal . kernel . model . Team ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public ManageSearchPage performSelectedActions ( ) { log . info ( "Clicked actions" ) ; clickElement ( performButton ) ; waitForAMoment ( ) . withMessage ( "displayed abort button" ) . until ( it -> readyElement ( cancelButton ) . isDisplayed ( ) ) ; return new ManageSearchPage ( getDriver ( ) ) ; }
@ Transactional ( rollbackFor = ArrowheadException . class ) public AuthorizationIntraCloudListResponseDTO createBulkAuthorizationIntraCloudResponse ( final long consumerId , final Set < Long > providerIds , final Set < Long > serviceDefinitionIds , final Set < Long > interfaceIds ) { logger . debug ( "createBulkAuthorizationIntraCloudResponse started..." ) ; final List < AuthorizationIntraCloud > entries = createBulkAuthorizationIntraCloud ( consumerId , providerIds , serviceDefinitionIds , interfaceIds ) ; final Page < AuthorizationIntraCloud > entryPage = new PageImpl < > ( entries ) ; return DTOConverter . convertAuthorizationIntraCloudListToAuthorizationIntraCloudListResponseDTO ( entryPage ) ; }
public void test() { if ( maybeCached != null ) { LOG . info ( "Offer {} rescinded (not in cache)" , offerId . getValue ( ) ) ; } else { LOG . info ( "Offer {} rescinded (not in cache)" , offerId . getValue ( ) ) ; } }
public void test() { if ( maybeCached != null ) { LOG . info ( "Offer {} on {} rescinded" , offerId . getValue ( ) , maybeCached . getOffer ( ) . getHostname ( ) ) ; } else { LOG . trace ( "Offer {} on {} wasn't cached" , offerId . getValue ( ) , maybeCached . getOffer ( ) . getHostname ( ) ) ; } }
@ RequestMapping ( value = "/migration/events/{eventId}" , method = RequestMethod . GET ) public List < MigrationClusterModel > getEventDetailsWithEventId ( @ PathVariable Long eventId ) { logger . info ( "[getEventDetailsWithEventId][begin] eventId: {}" , eventId ) ; List < MigrationClusterModel > res = new LinkedList < > ( ) ; code_block = IfStatement ; logger . info ( "[getEventDetailsWithEventId][end] eventId: {}" , eventId ) ; return res ; }
public void test() { if ( null != eventId ) { res = migrationService . getMigrationClusterModel ( eventId ) ; } else { logger . info ( String . format ( "No eventId found for eventId:%s" , eventId ) ) ; } }
@ RequestMapping ( value = "/migration/events/{eventId}" , method = RequestMethod . GET ) public List < MigrationClusterModel > getEventDetailsWithEventId ( @ PathVariable Long eventId ) { logger . info ( "[getEventDetailsWithEventId][begin] eventId: {}" , eventId ) ; List < MigrationClusterModel > res = new LinkedList < > ( ) ; code_block = IfStatement ; logger . info ( "[getEventDetailsWithEventId][end] eventId: {}" , eventId ) ; return res ; }
public void test() { try ( HetuFileSystemClient hetuFileSystemClient = fileSystemClientManager . getFileSystemClient ( config . getShareFileSystemProfile ( ) , Paths . get ( "/" ) ) ) { int lastIndex = file . lastIndexOf ( File . separator ) ; String tmpFileDir = file . substring ( 0 , lastIndex ) ; code_block = IfStatement ; hetuFileSystemClient . createDirectories ( Paths . get ( tmpFileDir ) ) ; LOG . info ( "success to create the store directories..." ) ; } catch ( IOException e ) { LOG . error ( "fail to create store directories." , e ) ; throw new RuntimeException ( "fail to create the store directories." ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( IllegalStateException e ) { log . debug ( "Failed to execute command: {}" , e . getMessage ( ) ) ; } }
public void test() { try { if ( use_external_key_exchange && ! attach_fetch_key_header ) return Processing . PROCESS ; Message encr_msg = encrypt ( msg ) ; code_block = IfStatement ; down_prot . down ( encr_msg ) ; return Processing . DROP ; } catch ( Exception ex ) { log . error ( DebugUtils . getStackTrace ( ex ) ) ; return Processing . PROCESS ; } }
private void obtainKeys ( final KeyStore keyStore , final ApplicationContext appContext ) { logger . debug ( "Preparing keys..." ) ; @ SuppressWarnings ( "unchecked" ) final Map < String , Object > context = appContext . getBean ( CommonConstants . ARROWHEAD_CONTEXT , Map . class ) ; final X509Certificate serverCertificate = Utilities . getSystemCertFromKeyStore ( keyStore ) ; publicKey = serverCertificate . getPublicKey ( ) ; context . put ( CommonConstants . SERVER_PUBLIC_KEY , publicKey ) ; final PrivateKey privateKey = Utilities . getPrivateKey ( keyStore , sslProperties . getKeyPassword ( ) ) ; context . put ( CommonConstants . SERVER_PRIVATE_KEY , privateKey ) ; context . put ( CommonConstants . SERVER_CERTIFICATE , serverCertificate ) ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
@ Override protected void shutDown ( ) throws Exception { logger . info ( "Shutting down Job scheduler..." ) ; service . shutdown ( ) ; logger . info ( "Job scheduler shut down" ) ; super . shutDown ( ) ; }
@ Override protected void shutDown ( ) throws Exception { logger . info ( "Shutting down job scheduler" ) ; service . shutdown ( ) ; super . shutDown ( ) ; logger . info ( "Job scheduler shut down" ) ; }
public void test() { if ( StringUtils . isBlank ( ticket ) ) { _log . error ( "Invalid ticket: " + ticket ) ; throw new UmaWebException ( claimsRedirectUri , errorResponseFactory , INVALID_TICKET , state ) ; } }
public void test() { if ( permissions == null || permissions . isEmpty ( ) ) { log . error ( "Unable to retrieve claims due to missing permissions." ) ; throw new UmaWebException ( claimsRedirectUri , errorResponseFactory , INVALID_TICKET , state ) ; } }
public void test() { try { java . util . List < com . liferay . commerce . model . CommerceSubscriptionEntry > returnValue = CommerceSubscriptionEntryServiceUtil . getCommerceSubscriptionEntries ( companyId , groupId , userId , start , end , orderByComparator ) ; return com . liferay . commerce . model . CommerceSubscriptionEntrySoap . toSoapModels ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
@ PostMapping ( CommonConstants . PATH_IMPORT ) public JsonNode importAgencies ( @ RequestParam ( "fileName" ) String fileName , @ RequestParam ( "file" ) MultipartFile file ) { LOGGER . debug ( "Import file {}" , fileName ) ; SafeFileChecker . checkSafeFilePath ( file . getOriginalFilename ( ) ) ; final VitamContext vitamContext = securityService . buildVitamContext ( securityService . getTenantIdentifier ( ) ) ; return agencyInternalService . importAgencies ( vitamContext , fileName , file ) ; }
@ Override public void discoverObjectInstance ( ObjectInstanceHandle theObject , ObjectClassHandle theObjectClass , String objectName ) throws FederateInternalError { logger . debug ( "Discovering object instance: {}" , objectName ) ; }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
private void handleIncreaseDecrease ( ChannelUID channelUID , IncreaseDecreaseType command ) { logger . trace ( "handleIncreaseDecrease for {}" , channelUID ) ; sendOmnilinkCommand ( IncreaseDecreaseType . INCREASE . equals ( command ) ? CommandMessage . CMD_UNIT_UPB_BRIGHTEN_STEP_1 : CommandMessage . CMD_UNIT_UPB_DIM_STEP_1 , 0 , thingID ) ; }
public void task ( ) throws Exception { FromDefinition from = route . getInput ( ) ; from . setEndpoint ( null ) ; from . setUri ( uri ) ; log . info ( "Using URI " + uri ) ; }
public void test() { try { return Streams . readFullyStringAndClose ( getResourceFromUrl ( url ) ) ; } catch ( Exception e ) { logger . error ( "Error loading resource from url: " + url , e ) ; throw Throwables . propagate ( e ) ; } }
public void test() { if ( e instanceof org . apache . accumulo . core . clientImpl . thrift . ThriftSecurityException ) { result . sec = ( org . apache . accumulo . core . clientImpl . thrift . ThriftSecurityException ) e ; result . setSecIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . accumulo . core . clientImpl . thrift . ThriftTableOperationException ) { result . tope = ( org . apache . accumulo . core . clientImpl . thrift . ThriftTableOperationException ) e ; result . setTopeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . accumulo . core . clientImpl . thrift . ThriftNotActiveServiceException ) { result . tnase = ( org . apache . accumulo . core . clientImpl . thrift . ThriftNotActiveServiceException ) e ; result . setTnaseIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof org . apache . accumulo . core . clientImpl . thrift . ThriftSecurityException ) { result . sec = ( org . apache . accumulo . core . clientImpl . thrift . ThriftSecurityException ) e ; result . setSecIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . accumulo . core . clientImpl . thrift . ThriftTableOperationException ) { result . tope = ( org . apache . accumulo . core . clientImpl . thrift . ThriftTableOperationException ) e ; result . setTopeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . accumulo . core . clientImpl . thrift . ThriftNotActiveServiceException ) { result . tnase = ( org . apache . accumulo . core . clientImpl . thrift . ThriftNotActiveServiceException ) e ; result . setTnaseIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof org . apache . accumulo . core . clientImpl . thrift . ThriftSecurityException ) { result . sec = ( org . apache . accumulo . core . clientImpl . thrift . ThriftSecurityException ) e ; result . setSecIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . accumulo . core . clientImpl . thrift . ThriftTableOperationException ) { result . tope = ( org . apache . accumulo . core . clientImpl . thrift . ThriftTableOperationException ) e ; result . setTopeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . accumulo . core . clientImpl . thrift . ThriftNotActiveServiceException ) { result . tnase = ( org . apache . accumulo . core . clientImpl . thrift . ThriftNotActiveServiceException ) e ; result . setTnaseIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { try { fcall . sendResponse ( fb , msg , msgType , seqid ) ; } catch ( java . lang . Exception ex ) { _LOGGER . error ( "Exception writing to internal frame buffer" , ex ) ; fb . close ( ) ; } }
public void test() { if ( iterationType instanceof Text ) { return iterationType . toString ( ) ; } else { LOGGER . warn ( MessageFormat . format ( this . getActionExecution ( ) . getAction ( ) . getType ( ) + " does not accept {0} as type for iteration type" , iterationType . getClass ( ) ) ) ; return iterationType . toString ( ) ; } }
public void test() { try { resource = convert ( request , lockSimulator . getResourceClass ( ) ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; return 500 ; } }
public void test() { if ( resource == null ) { _log . info ( "resource not found: " + resource ) ; return 500 ; } }
public void test() { if ( resource . getMetadata ( ) == null || ! lockSimulator . getResourceName ( ) . equals ( resource . getMetadata ( ) . getName ( ) ) ) { logger . debug ( "Unexpected lockSimulator: {} - {}" , resource . getMetadata ( ) . getName ( ) , lockSimulator . getResourceName ( ) ) ; return 500 ; } }
protected final void log ( String text ) { logger . info ( text ) ; }
public void test() { try { double d = parse ( value ) ; return d ; } catch ( CoordinateFormatException e ) { logger . error ( "Error converting {} to a valid coordinate pair" , value ) ; return null ; } }
@ Override public void invalidateCollectionSyncRootMemberCache ( ) { log . info ( "invalidateCollectionSyncRootMemberCache" ) ; getCollectionSyncRootMemberCache ( ) . invalidateAll ( ) ; }
public void test() { try ( ClientSession toClose = session ) { toClose . rollback ( ) ; } catch ( ActiveMQException e ) { log . error ( "Failed to rollback" , e ) ; } }
public static Set < Class < ? > > getTypesAnnotatedWith ( Class < ? extends Annotation > clazz ) { Set < Class < ? > > classes = reflections . getTypesAnnotatedWith ( clazz ) ; log . debug ( "Found: " + classes ) ; return classes ; }
public void test() { if ( bridgeUidString == null || bridgeUidString . isEmpty ( ) ) { logger . error ( "could not find bridge uid" ) ; return "/mielecloud/failure?" + FailureServlet . MISSING_BRIDGE_UID_PARAMETER_NAME + "=true" ; } }
public void test() { if ( email == null || email . isEmpty ( ) ) { LOGGER . log ( Level . WARNING , "Could not locate email to " + SecurityUtils . toString ( tag ) ) ; return "/mielecloud/failure?" + FailureServlet . MISSING_EMAIL_PARAMETER_NAME + "=true" ; } }
public void test() { try { bridgeUid = new ThingUID ( bridgeUidString ) ; } catch ( IllegalArgumentException e ) { logger . warn ( e . getMessage ( ) ) ; return "/mielecloud/failure?" + FailureServlet . MALFORMED_BRIDGE_UID_PARAMETER_NAME + "=true" ; } }
public void test() { if ( ! EmailValidator . isValid ( email ) ) { LOG . error ( "Couldn't send email: {}" , email ) ; return "/mielecloud/failure?" + FailureServlet . MALFORMED_EMAIL_PARAMETER_NAME + "=true" ; } }
public void test() { try { Thing bridge = pairOrReconfigureBridge ( locale , bridgeUid , email ) ; waitForBridgeToComeOnline ( bridge ) ; return "/mielecloud" ; } catch ( BridgeReconfigurationFailedException e ) { logger . warn ( "Thing connection failed because there was no binding available that supports the thing." ) ; return "/mielecloud/success?" + SuccessServlet . BRIDGE_RECONFIGURATION_FAILED_PARAMETER_NAME + "=true&" + SuccessServlet . BRIDGE_UID_PARAMETER_NAME + "=" + bridgeUidString + "&" + SuccessServlet . EMAIL_PARAMETER_NAME + "=" + email ; } catch ( BridgeCreationFailedException e ) { logger . warn ( "Thing creation failed because there was no binding available that supports the thing." ) ; return "/mielecloud/success?" + SuccessServlet . BRIDGE_CREATION_FAILED_PARAMETER_NAME + "=true&" + SuccessServlet . BRIDGE_UID_PARAMETER_NAME + "=" + bridgeUidString + "&" + SuccessServlet . EMAIL_PARAMETER_NAME + "=" + email ; } }
public void test() { try { Thing bridge = pairOrReconfigureBridge ( locale , bridgeUid , email ) ; waitForBridgeToComeOnline ( bridge ) ; return "/mielecloud" ; } catch ( BridgeReconfigurationFailedException e ) { logger . warn ( "{}" , e . getMessage ( ) ) ; return "/mielecloud/success?" + SuccessServlet . BRIDGE_RECONFIGURATION_FAILED_PARAMETER_NAME + "=true&" + SuccessServlet . BRIDGE_UID_PARAMETER_NAME + "=" + bridgeUidString + "&" + SuccessServlet . EMAIL_PARAMETER_NAME + "=" + email ; } catch ( BridgeCreationFailedException e ) { logger . warn ( "Bridge creation failed" , e ) ; return "/mielecloud/success?" + SuccessServlet . BRIDGE_CREATION_FAILED_PARAMETER_NAME + "=true&" + SuccessServlet . BRIDGE_UID_PARAMETER_NAME + "=" + bridgeUidString + "&" + SuccessServlet . EMAIL_PARAMETER_NAME + "=" + email ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
void removeSampleReferences ( ClientSession clientSession , long studyUid , long sampleUid ) throws CatalogDBException , CatalogParameterException , CatalogAuthorizationException { Query query = new Query ( ) . append ( QueryParams . STUDY_UID . key ( ) , studyUid ) . append ( QueryParams . SAMPLE_UIDS . key ( ) , sampleUid ) ; ObjectMap params = new ObjectMap ( ) . append ( QueryParams . SAMPLES . key ( ) , Collections . singletonList ( new Sample ( ) . setUid ( sampleUid ) ) ) ; QueryOptions queryOptions = new QueryOptions ( Constants . ACTIONS , new ObjectMap ( QueryParams . SAMPLES . key ( ) , ParamUtils . BasicUpdateAction . REMOVE . name ( ) ) ) ; Bson update ; code_block = TryStatement ;  QueryOptions multi = new QueryOptions ( MongoDBCollection . MULTI , true ) ; Bson bsonQuery = parseQuery ( query ) ; logger . debug ( "Sample references extraction. Query: {}, update: {}" , bsonQuery . toBsonDocument ( Document . class , MongoClient . getDefaultCodecRegistry ( ) ) , update . toBsonDocument ( Document . class , MongoClient . getDefaultCodecRegistry ( ) ) ) ; DataResult updateResult = individualCollection . update ( clientSession , bsonQuery , update , multi ) ; logger . debug ( "Sample references removed: {}" , updateResult . toBsonDocument ( Document . class , MongoClient . getDefaultCodecRegistry ( ) ) ) ; }
public void test() { try { code_block = SwitchStatement ; } catch ( Exception e ) { logger . warn ( "Receive message failed, using the default value" , e ) ; return String . valueOf ( data ) ; } }
@ Override public void setBlobStore ( BlobStore blobStore ) { splitBlobStore = new DefaultSplitBlobStore ( repositoryDir , blobStore , newBlobStore ) ; log . info ( "Set BlobStore to: {}" , newBlobStore ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { LOG . debug ( "FULL Updated perms seq Num [old=" + authzPaths + "], [new=" + newAuthzPerms . getLastUpdatedSeqNum ( ) + "]" ) ; authzPaths = newAuthzPaths ; LOG . debug ( "FULL Updated perms seq Num [old=" + authzPermissions . getLastUpdatedSeqNum ( ) + "], [new=" + newAuthzPerms . getLastUpdatedSeqNum ( ) + "]" ) ; authzPermissions = newAuthzPerms ; } finally { lock . writeLock ( ) . unlock ( ) ; } }
public void test() { try { LOG . debug ( "FULL Updated paths seq Num [old=" + authzPaths . getLastUpdatedSeqNum ( ) + "], [new=" + newAuthzPaths . getLastUpdatedSeqNum ( ) + "]" ) ; authzPaths = newAuthzPaths ; authzPermissions = newAuthzPerms ; LOG . debug ( "Updated paths: [{}]" , authzPaths ) ; } finally { lock . writeLock ( ) . unlock ( ) ; } }
public void test() { if ( _logger . isDebugEnabled ( ) ) { _logger . debug ( "[" + _handler . getClass ( ) . getName ( ) + "] DirectPersistencyListHandler[" + toString ( ) + "]" ) ; } }
public void test() { if ( warningReport != null ) { this . logger . warn ( warningReport ) ; } }
public void test() { if ( errorReport != null ) { log . error ( errorReport ) ; } }
public void test() { try { prototype . setDescription ( this . getDescription ( ) ) ; prototype . setId ( this . getId ( ) ) ; prototype . setMainGroup ( this . getMainGroup ( ) ) ; prototype . setTypeCode ( this . getTypeCode ( ) ) ; prototype . setTypeDescription ( this . getTypeDescription ( ) ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = ForStatement ; } catch ( Throwable t ) { _logger . error ( "Error creating Entity" , t ) ; throw new RuntimeException ( "Error creating Entity" , t ) ; } }
@ Override @ Transactional public void markJobAsReadyForActivation ( String theJobId ) { BulkImportJobEntity job = findJobByJobId ( theJobId ) ; ValidateUtil . isTrueOrThrowInvalidRequest ( job . getStatus ( ) == BulkImportJobStatusEnum . STAGING , "Bulk import job %s can not be activated in status: %s" , theJobId , job . getStatus ( ) ) ; LOGGER . info ( "Bulk import job {} can be activated" , theJobId ) ; job . setStatus ( BulkImportJobStatusEnum . READY ) ; myJobDao . save ( job ) ; }
public void test() { try { URL jarURL = absoluteSystemId . toURL ( ) ; code_block = TryStatement ;  } catch ( IOException e ) { LOG . warn ( "Failed to load system id." , e ) ; } }
public void test() { if ( isFileSystemAvailable ( absoluteSystemId . getScheme ( ) ) ) { Path pathTest = Paths . get ( absoluteSystemId ) ; LOG . info ( "Testing path: {}" , pathTest ) ; return Files . exists ( pathTest ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Failed to clean up." , e ) ; } }
public void test() { if ( ! directory . exists ( ) ) { log . error ( "Cannot load stream definitions from " + directory . getAbsolutePath ( ) + " directory not found" ) ; return streamDefinitions ; } }
public void test() { if ( ! directory . isDirectory ( ) ) { log . error ( "Cannot load stream definitions from " + directory . getAbsolutePath ( ) + " not a directory" ) ; return streamDefinitions ; } }
public void test() { try { bufferedReader = new BufferedReader ( new FileReader ( fullPathToStreamDefinitionFile ) ) ; String line ; code_block = WhileStatement ; StreamDefinition streamDefinition = EventDefinitionConverterUtils . convertFromJson ( stringBuilder . toString ( ) . trim ( ) ) ; streamDefinitions . add ( streamDefinition ) ; } catch ( IOException e ) { log . error ( "Error in reading Stream definition : " + e . getMessage ( ) , e ) ; } catch ( MalformedStreamDefinitionException e ) { log . error ( "Error in converting Stream definition : " + e . getMessage ( ) , e ) ; } finally { code_block = TryStatement ;  } }
public void test() { try { bufferedReader = new BufferedReader ( new FileReader ( fullPathToStreamDefinitionFile ) ) ; String line ; code_block = WhileStatement ; StreamDefinition streamDefinition = EventDefinitionConverterUtils . convertFromJson ( stringBuilder . toString ( ) . trim ( ) ) ; streamDefinitions . add ( streamDefinition ) ; } catch ( IOException e ) { log . error ( "Error in reading file : " + fullPathToStreamDefinitionFile , e ) ; } catch ( MalformedStreamDefinitionException e ) { log . error ( "Error in converting stream definition : " + fullPathToStreamDefinitionFile , e ) ; } finally { code_block = TryStatement ;  } }
public void test() { try { code_block = IfStatement ; } catch ( IOException e ) { LOGGER . error ( "Unable to retrieve file" , e ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { String customIP = System . getProperty ( "qmq.ip" ) ; if ( ! Strings . isNullOrEmpty ( customIP ) ) return customIP ; final Enumeration < NetworkInterface > interfaces = NetworkInterface . getNetworkInterfaces ( ) ; final ArrayList < String > ipv4Result = new ArrayList < > ( ) ; final ArrayList < String > ipv6Result = new ArrayList < > ( ) ; code_block = WhileStatement ; code_block = IfStatement ; return InetAddress . getLocalHost ( ) . getHostAddress ( ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public Page < WebContent > findAll ( final Pageable pageable ) { log . debug ( "findAll() - pageable: {}" , pageable ) ; return webContentRepository . findAll ( pageable ) ; }
public void test() { try { JavaType mapType = Mapper . mapper ( ) . getTypeFactory ( ) . constructMapType ( HashMap . class , keyClass , valueClass ) ; return Optional . ofNullable ( Mapper . mapper ( ) . readValue ( event . getData ( ) , mapType ) ) ; } catch ( IOException e ) { LOG . error ( e . getMessage ( ) , e ) ; return Optional . empty ( ) ; } }
public void test() { if ( this . revisionId == null ) { logger . debug ( "sendCommand getCommandsReceiverId :: {}" , String . valueOf ( this . revisionId ) ) ; byte [ ] reply ; CommConnection commAtConnection = openSerialPort ( getAtPort ( ) ) ; code_block = IfStatement ; code_block = TryStatement ;  closeSerialPort ( commAtConnection ) ; code_block = IfStatement ; } }
public void test() { if ( ActiveMQRALogger . LOGGER . isTraceEnabled ( ) ) { ActiveMQRALogger . LOGGER . trace ( "execute()" ) ; } }
public void test() { if ( parsingError . isPresent ( ) ) { LOG . debug ( MessageFormat . format ( Messages . getInstance ( ) . getErrorString ( ) , parseError . get ( ) ) ) ; return null ; } }
public void test() { if ( isVerbose ( ) ) { logger . info ( "Gene '" + sampleName + "' uses directory '" + location + "'" ) ; } }
public void test() { if ( removed != null && removed . isSubmitted ( ) && ! removed . isDone ( ) ) { LOG . debug ( "Successfully removed {}" , this ) ; } }
public void test() { if ( plan instanceof InsertPlan ) { processPlanWithTolerance ( ( InsertPlan ) plan , dataGroupMember ) ; } else-if ( plan != null && ! plan . isQuery ( ) ) { code_block = TryStatement ;  } else-if ( plan != null ) { LOGGER . log ( Level . FINE , "Ignoring unexpected plan: " + plan . toString ( ) ) ; } }
public void test() { try { List < String > draftUtilizers = this . getPageManager ( ) . getDraftWidgetUtilizerCodes ( widgetTypeCode ) ; code_block = IfStatement ; List < String > onlineUtilizers = this . getPageManager ( ) . getOnlineWidgetUtilizerCodes ( widgetTypeCode ) ; code_block = IfStatement ; } catch ( Throwable t ) { _logger . error ( "Error on extracting widgetUtilizers : widget type code {}" , widgetTypeCode , t ) ; throw new RuntimeException ( "Error on extracting widgetUtilizers : widget type code " + widgetTypeCode , t ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceDiscountCommerceAccountGroupRelServiceUtil . class , "deleteCommerceDiscountCommerceAccountGroupRelsByCommerceDiscountId" , _deleteCommerceDiscountCommerceAccountGroupRelsByCommerceDiscountIdParameterTypes2 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , commerceDiscountId ) ; code_block = TryStatement ;  } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( auditMaxSize >= 0 && auditSize > auditMaxSize ) { Referenceable shallowEntity = new Referenceable ( entity . getId ( ) , entity . getTypeName ( ) , null , entity . getSystemAttributes ( ) , null , null ) ; auditString = auditPrefix + AtlasType . toJson ( shallowEntity ) ; LOG . info ( String . format ( "Batch size %d" , auditSize ) ) ; } }
public void test() { try { prepareConfigFile ( ) ; StringBuilder apiProxyUrl = new StringBuilder ( ) ; apiProxyUrl . append ( "http://localhost:" ) . append ( getProxyPort ( ) ) . append ( "/v1-api-interceptor/reload" ) ; Request . Post ( apiProxyUrl . toString ( ) ) . execute ( ) ; } catch ( IOException e ) { LOGGER . error ( "reload api proxy failed" , e ) ; } }
public void test() { try { LoadModel < CatalogModel > loadModel = new LoadModel < > ( CatalogModel . class ) ; loadModel = getCommandService ( ) . executeCommand ( loadModel ) ; model = loadModel . getModel ( ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Error loading the CatalogModel" , e ) ; throw new RuntimeException ( "Error loading the CatalogModel" , e ) ; } }
public void test() { if ( highWatermarkUpdateOffset > currentHighWatermarkMetadata . offset || ( highWatermarkUpdateOffset == currentHighWatermarkMetadata . offset && ! highWatermarkUpdateMetadata . metadata . equals ( currentHighWatermarkMetadata . metadata ) ) ) { highWatermark = highWatermarkUpdateOpt ; return true ; } else-if ( highWatermarkUpdateOffset < currentHighWatermarkMetadata . offset ) { LOG . debug ( "HighWatermark update offset {} to {}" , highWatermarkUpdateOffset , currentHighWatermarkMetadata . offset ) ; return false ; } else { return false ; } }
private Product writeProductFile ( Product targetProduct , ProductFormatter productFormatter , Mapper . Context context , Configuration jobConfig , String outputFormat , ProgressMonitor pm ) throws IOException { long t0 = System . currentTimeMillis ( ) ; Map < String , Object > bandSubsetParameter = createBandSubsetParameter ( targetProduct , jobConfig ) ; code_block = IfStatement ; File productFile = productFormatter . createTemporaryProductFile ( ) ; LOG . info ( "Writing product file: " + productFile . getAbsolutePath ( ) ) ; GPF . writeProduct ( targetProduct , productFile , outputFormat , false , pm ) ; LOG . info ( "formatting done in [ms]: " + ( System . currentTimeMillis ( ) - t0 ) ) ; t0 = System . currentTimeMillis ( ) ; context . setStatus ( "Copying" ) ; productFormatter . compressToHDFS ( context , productFile ) ; context . getCounter ( COUNTER_GROUP_NAME_PRODUCTS , "Product formatted" ) . increment ( 1 ) ; LOG . info ( "archiving done in [ms]: " + ( System . currentTimeMillis ( ) - t0 ) ) ; return targetProduct ; }
private Product writeProductFile ( Product targetProduct , ProductFormatter productFormatter , Mapper . Context context , Configuration jobConfig , String outputFormat , ProgressMonitor pm ) throws IOException { long t0 = System . currentTimeMillis ( ) ; Map < String , Object > bandSubsetParameter = createBandSubsetParameter ( targetProduct , jobConfig ) ; code_block = IfStatement ; File productFile = productFormatter . createTemporaryProductFile ( ) ; LOG . info ( "Start writing product to file: " + productFile . getName ( ) ) ; GPF . writeProduct ( targetProduct , productFile , outputFormat , false , pm ) ; LOG . info ( "Writing product to file: " + targetProduct ) ; t0 = System . currentTimeMillis ( ) ; context . setStatus ( "Copying" ) ; productFormatter . compressToHDFS ( context , productFile ) ; context . getCounter ( COUNTER_GROUP_NAME_PRODUCTS , "Product formatted" ) . increment ( 1 ) ; LOG . info ( "archiving done in [ms]: " + ( System . currentTimeMillis ( ) - t0 ) ) ; return targetProduct ; }
private Product writeProductFile ( Product targetProduct , ProductFormatter productFormatter , Mapper . Context context , Configuration jobConfig , String outputFormat , ProgressMonitor pm ) throws IOException { long t0 = System . currentTimeMillis ( ) ; Map < String , Object > bandSubsetParameter = createBandSubsetParameter ( targetProduct , jobConfig ) ; code_block = IfStatement ; File productFile = productFormatter . createTemporaryProductFile ( ) ; LOG . info ( "Start writing product to file: " + productFile . getName ( ) ) ; GPF . writeProduct ( targetProduct , productFile , outputFormat , false , pm ) ; LOG . info ( "formatting done in [ms]: " + ( System . currentTimeMillis ( ) - t0 ) ) ; t0 = System . currentTimeMillis ( ) ; context . setStatus ( "Copying" ) ; productFormatter . compressToHDFS ( context , productFile ) ; LOG . info ( "formatting done in [ms]: " + ( System . currentTimeMillis ( ) - t0 ) ) ; context . getCounter ( COUNTER_GROUP_NAME_PRODUCTS , "Product formatted" ) . increment ( 1 ) ; return targetProduct ; }
public void handleGetDataResponse ( final GetDataResponseDto dataResponseDto , final CorrelationIds ids , final String messageType , final ResponseMessageResultType responseMessageResultType , final OsgpException osgpException ) { LOGGER . info ( "handleGetDataResponse called with messageType: {}" , messageType ) ; ResponseMessageResultType result = ResponseMessageResultType . OK ; GetDataResponse dataResponse = null ; OsgpException exception = null ; code_block = TryStatement ;  String actualCorrelationUid = ids . getCorrelationUid ( ) ; code_block = IfStatement ; final ResponseMessage responseMessage = ResponseMessage . newResponseMessageBuilder ( ) . withIds ( ids ) . withCorrelationUid ( actualCorrelationUid ) . withMessageType ( messageType ) . withResult ( result ) . withOsgpException ( exception ) . withDataObject ( dataResponse ) . build ( ) ; this . webServiceResponseMessageSender . send ( responseMessage , messageType ) ; }
public void test() { try { code_block = IfStatement ; this . handleResponseMessageReceived ( ids . getDeviceIdentification ( ) ) ; dataResponse = this . mapper . map ( dataResponseDto , GetDataResponse . class ) ; } catch ( final Exception e ) { LOGGER . error ( "Exception occurred while getting data" , e ) ; result = ResponseMessageResultType . NOT_OK ; exception = this . ensureOsgpException ( e , "Exception occurred while getting data" ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Free slot {}." , taskSlot ) ; } else { LOG . info ( "Free slot {}." , taskSlot ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Free slot {}." , taskSlot , cause ) ; } else { LOG . info ( "Free slot {} failed." , taskSlot ) ; } }
@ Override public void start ( ) { logger . debug ( "Starting {} policy" , this ) ; TimeUnit timeUnit = TimeUnit . valueOf ( System . getProperty ( INTERVAL_TIME_UNIT , TimeUnit . MILLISECONDS . toString ( ) ) ) ; long givenInterval = Long . parseLong ( System . getProperty ( INTERVAL_VALUE , "0" ) ) ; code_block = IfStatement ; logger . debug ( "Started {} policy" , this ) ; }
public void test() { try { TTransport transport = new TFastFramedTransport ( new TSocket ( host , port ) ) ; transport . open ( ) ; TProtocol protocol = new TCompactProtocol ( transport ) ; log . info ( "Connected to server: {}" , host ) ; return new OracleService . Client ( protocol ) ; } catch ( TTransportException e ) { log . debug ( "Exception thrown in getOracleClient()" , e ) ; } catch ( Exception e ) { throw new RuntimeException ( e ) ; } }
public void test() { try { TTransport transport = new TFastFramedTransport ( new TSocket ( host , port ) ) ; transport . open ( ) ; TProtocol protocol = new TCompactProtocol ( transport ) ; log . info ( " Former leader was reachable at " + host + ":" + port ) ; return new OracleService . Client ( protocol ) ; } catch ( TTransportException e ) { log . debug ( "Could not open connection to '" + host + ":" + port + "'" , e ) ; } catch ( Exception e ) { throw new RuntimeException ( e ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Refreshing." ) ; } }
public void test() { if ( ! responseMessage . getBooleanValue ( "checked" ) ) { LOGGER . info ( "RUNTIMEOUT: {}" , scheduleID ) ; deleteResult ( rdbAnalyze , scheduleID ) ; } else { status = Boolean . TRUE ; } }
public void test() { try { ResponseEntity < String > executeResult = restTemplate . postForEntity ( url , httpEntity , String . class ) ; JSONObject responseMessage = JSONObject . parseObject ( executeResult . getBody ( ) ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Recieved Message Exception." , e ) ; deleteResult ( rdbAnalyze , scheduleID ) ; } }
@ PostConstruct public void postConstruct ( ) { this . contractResources = Map . of ( HOLDING_ACCESS_CONTRACT_NAME , holdingAccessContract , HOLDING_INGEST_CONTRACT_NAME , holdingIngestContract , LOGBOOK_ACCESS_CONTRACT_NAME , logbookAccessContract , ITEMS_INGEST_CONTRACT_NAME , itemsIngestContract , FULL_ACCESS_CONTRACT_NAME , fullAccessAccessAccessContract ) ; LOGGER . info ( "Created" ) ; }
public void test() { switch ( this . workerState . getCurrentState ( ) ) { case NEW : code_block = IfStatement ; case INITIALIZING : logger . info ( "{} initialized." , this . getClass ( ) . getSimpleName ( ) ) ; break ; case STARTED : logger . info ( "{} already started." , this . getClass ( ) . getSimpleName ( ) ) ; break ; case DESTROYING : throw new IllegalStateException ( "Already destroying." ) ; case STOPPED : throw new IllegalStateException ( "Already stopped." ) ; case ILLEGAL_STATE : throw new IllegalStateException ( "Invalid State." ) ; } }
public void test() { switch ( this . workerState . getCurrentState ( ) ) { case NEW : code_block = IfStatement ; case INITIALIZING : logger . info ( "{} already initializing." , this . getClass ( ) . getSimpleName ( ) ) ; break ; case STARTED : logger . info ( "{} started." , this . getClass ( ) . getSimpleName ( ) ) ; break ; case DESTROYING : throw new IllegalStateException ( "Already destroying." ) ; case STOPPED : throw new IllegalStateException ( "Already stopped." ) ; case ILLEGAL_STATE : throw new IllegalStateException ( "Invalid State." ) ; } }
public void test() { try { completionScriptText = text . substring ( 0 , cursor ) ; } catch ( Exception e ) { logger . debug ( "Error in parsing command line: " + e . getMessage ( ) ) ; return null ; } }
public void test() { try { code_block = IfStatement ; mContext . setError ( error ) ; cleanupRequest ( mContext ) ; } catch ( Exception e ) { LOG . debug ( "Caught exception processing reply:" , e ) ; } finally { replyError ( ) ; } }
public boolean isLinux ( ) { String guestid = configSpec . getGuestId ( ) ; boolean isLinux = guestid . startsWith ( "cent" ) || guestid . startsWith ( " FreeBSD" ) || guestid . startsWith ( "freebsd" ) || guestid . startsWith ( "oracle" ) || guestid . startsWith ( "other24xLinux" ) || guestid . startsWith ( "other26xLinux" ) || guestid . startsWith ( "otherLinux" ) || guestid . startsWith ( "redhat" ) || guestid . startsWith ( "rhel" ) || guestid . startsWith ( "sles" ) || guestid . startsWith ( "suse" ) || guestid . startsWith ( "ubuntu" ) ; log . info ( "isLinux: " + isLinux ) ; return isLinux ; }
public SignInPage clickSignInExpectError ( ) { log . info ( "Click SignIn" ) ; clickElement ( signInButton ) ; return new SignInPage ( getDriver ( ) ) ; }
@ Override public void debug ( final String format , final Object arg1 , final Object arg2 ) { logger . debug ( format , arg1 , arg2 ) ; }
public void test() { try { boolean hasTenant = hasTenant ( tenant ) ; Capacity capacity = getCapacity ( group , tenant , hasTenant ) ; code_block = IfStatement ; } catch ( Exception e ) { s_logger . warn ( "Error getting capacity for tenant " + tenant , e ) ; } }
public void test() { if ( Objects . equals ( oldJobManagerConnection . getJobMasterId ( ) , jobMasterGateway . getFencingToken ( ) ) ) { logger . debug ( "Ignoring job leader for job id {}." , jobId ) ; return ; } else { disconnectJobManagerConnection ( oldJobManagerConnection , new Exception ( "Found new job leader for job id " + jobId + '.' ) ) ; } }
public void test() { try { schedulerNG . notifyKvStateUnregistered ( jobId , jobVertexId , keyGroupRange , registrationName ) ; return CompletableFuture . completedFuture ( Acknowledge . get ( ) ) ; } catch ( FlinkJobNotFoundException e ) { LOG . info ( "Unable to unregistered job {}" , jobId ) ; return FutureUtils . completedExceptionally ( e ) ; } }
@ Test public void test_00 ( ) throws IOException { log . info ( "Test" ) ; String fileName = path ( "testLukas.vcf" ) ; long hashExp = calcHashBufferedReader ( fileName ) ; long hash = calcHash ( fileName ) ; System . out . println ( String . format ( "%016x\t%016x\t%s" , hashExp , hash , fileName ) ) ; Assert . assertEquals ( hashExp , hash ) ; }
public void test() { try { final HSSFWorkbook myWorkBook = createGovermentBodyWorkBook ( ) ; code_block = ForStatement ; myWorkBook . close ( ) ; } catch ( final IOException e ) { LOGGER . error ( e , e ) ; } }
public void test() { try { LOGGER . info ( "Loading Mets document {}" , id ) ; Document metsDocument = getMets ( id ) ; return getConverter ( id , metsDocument ) . convert ( ) ; } catch ( IOException | JDOMException | SAXException e ) { throw new MCRException ( e ) ; } }
private void printlnError ( Object o ) { logger . error ( o . getMessage ( ) , o ) ; }
public void test() { try { handleMessage ( in . read ( ) , in ) ; } catch ( IOException ex ) { LOG . error ( ex . getMessage ( ) , ex ) ; } }
public void test() { try { return documentStoreService . saveDocumentStream ( spMetadataFile , stream ) ; } catch ( Exception ex ) { log . error ( "Failed to save sp metadata file: " + spMetadataFile . getAbsolutePath ( ) , ex ) ; } finally { IOUtils . closeQuietly ( stream ) ; } }
public void test() { if ( br == null ) { return ; } }
@ Override public void perform ( ) throws Exception { getLogger ( ) . info ( "Performing action: Rolling restarting non-master region servers" ) ; List < ServerName > selectedServers = selectServers ( ) ; getLogger ( ) . info ( "Starting balancer" ) ; setBalancer ( false , true ) ; code_block = ForStatement ; getLogger ( ) . info ( "Enabling balancer" ) ; setBalancer ( true , true ) ; }
public void test() { try ( RegionMover rm = new RegionMover . RegionMoverBuilder ( rsName , getConf ( ) ) . ack ( true ) . build ( ) ) { getLogger ( ) . info ( "Unloading {}" , server ) ; rm . unload ( ) ; getLogger ( ) . info ( "Restarting {}" , server ) ; gracefulRestartRs ( server , sleepTime ) ; rm . load ( ) ; getLogger ( ) . info ( "Loaded {} as presume successful" , server ) ; } catch ( Shell . ExitCodeException e ) { getLogger ( ) . info ( "Problem restarting but presume successful; code={}" , e . getExitCode ( ) , e ) ; } }
public void test() { try ( RegionMover rm = new RegionMover . RegionMoverBuilder ( rsName , getConf ( ) ) . ack ( true ) . build ( ) ) { getLogger ( ) . info ( "Unloading {}" , server ) ; rm . unload ( ) ; getLogger ( ) . info ( "Restarting {}" , server ) ; gracefulRestartRs ( server , sleepTime ) ; getLogger ( ) . info ( "Loading {}" , server ) ; rm . load ( ) ; } catch ( Shell . ExitCodeException e ) { getLogger ( ) . info ( "exit code={}" , e . getExitCode ( ) ) ; } }
@ Override public void perform ( ) throws Exception { getLogger ( ) . info ( "Performing action: Rolling restarting non-master region servers" ) ; List < ServerName > selectedServers = selectServers ( ) ; getLogger ( ) . info ( "Disabling balancer to make unloading possible" ) ; setBalancer ( false , true ) ; code_block = ForStatement ; getLogger ( ) . info ( "Successfully loaded balancer." ) ; setBalancer ( true , true ) ; }
public void test() { if ( keyInfo == null ) { logger . trace ( "No KeyInfo found in SecretKeyStore" ) ; return true ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { code_block = TryStatement ;  } catch ( IOException | IllegalStateException th ) { LOGGER . error ( "[{}] {}" , logPrefix , th . getMessage ( ) , th ) ; } finally { ac . complete ( ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( LOG . isWarnEnabled ( ) ) { LOG . warn ( "Unhandled exception: " , e ) ; } }
@ Override public void resolveCollision ( CompositeMap < IOption , Object > composite , Map < IOption , Object > existing , Map < IOption , Object > added , Collection < IOption > intersect ) { logger . trace ( "Resolved conflict resolution" ) ; }
public void test() { try { createGUI ( ) ; } catch ( Throwable t ) { logger . error ( "Exception creating GUI" , t ) ; t . printStackTrace ( ) ; } }
public void test() { try { sqlQueryHelper . testQueriesFromFile ( queryFilePath ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; throw new RuntimeException ( e ) ; } }
private void initiateSort ( String sortId , String source , final String destination ) throws KeeperException , InterruptedException { String work = source + "|" + destination ; new DistributedWorkQueue ( manager . getZooKeeperRoot ( ) + Constants . ZRECOVERY , manager . getConfiguration ( ) ) . addWork ( sortId , work . getBytes ( UTF_8 ) ) ; synchronized ( this ) code_block = "" ; final String path = manager . getZooKeeperRoot ( ) + Constants . ZRECOVERY + "/" + sortId ; LOGGER . info ( "Submitted " + path ) ; }
public void test() { if ( Duration . ZERO . equals ( refreshInterval ) ) { log . debug ( "No refresh interval has been configured" ) ; return ; } }
public void test() { if ( ! indexSet . getConfig ( ) . isWritable ( ) ) { LOGGER . info ( "Skipping non-writable index set <{}> ({})" , indexSet . getConfig ( ) . id ( ) , indexSet . getConfig ( ) . id ( ) ) ; return ; } }
public void test() { if ( activeWriteIndex != null ) { LOG . debug ( "Updating index field types for active write index <{}> in index set <{}/{}>" , activeWriteIndex , indexSetTitle , indexSetId ) ; poller . pollIndex ( activeWriteIndex , indexSetId ) . ifPresent ( dbService :: upsert ) ; } else { LOG . warn ( "No index field types found for active write index <{}> in index set <{}/{}>" , indexSetTitle , indexSetId , indexSetId ) ; } }
public void test() { try { final String activeWriteIndex = indexSet . getActiveWriteIndex ( ) ; code_block = IfStatement ; } catch ( TooManyAliasesException e ) { LOG . debug ( "TooManyAliasesException." , e ) ; } catch ( Exception e ) { LOG . error ( "Couldn't update field types for index set <{}/{}>" , indexSetTitle , indexSetId , e ) ; } }
public void test() { try { final String activeWriteIndex = indexSet . getActiveWriteIndex ( ) ; code_block = IfStatement ; } catch ( TooManyAliasesException e ) { LOG . error ( "Couldn't get active write index" , e ) ; } catch ( Exception e ) { LOG . warn ( "Couldn't get active write index" , e ) ; } }
public void test() { try { code_block = WhileStatement ; } catch ( Throwable e ) { LOGGER . error ( "CRITICAL ERROR. Closing the writer" , e ) ; close ( ) ; } }
public void test() { if ( isDebug ) { logger . debug ( "Expected arguments, but found none." ) ; } }
public void test() { if ( isInfo ) { logger . info ( "Request is info" ) ; } }
public WebContentModuleConfig findByModuleId ( final Long moduleId ) { log . debug ( "findByModuleId() - moduleId: {}" , moduleId ) ; return webContentModuleConfigRepository . findByModuleId ( moduleId ) ; }
public void test() { try { baos = new ByteArrayOutputStream ( ) ; ObjectOutputStream oos = new ObjectOutputStream ( baos ) ; oos . writeObject ( object ) ; oos . close ( ) ; return baos . toByteArray ( ) ; } catch ( IOException e ) { log . error ( "Error serializing object {}" , object , e ) ; throw new PropertyAccessException ( e ) ; } }
@ Override public void delete ( final LogicalDatastoreType store , final YangInstanceIdentifier path ) { log . debug ( "delete {} {}" , store , path ) ; checkOpen ( ) ; processTransactionOperation ( facade -> facade . delete ( store , path ) ) ; }
public void test() { if ( command instanceof RefreshType ) { client . refreshChannel ( ) ; updateState ( channelUID , new StringType ( client . getChannel ( ) ) ) ; } else-if ( command instanceof StringType ) { client . setChannel ( command . toString ( ) ) ; } else { logger . warn ( "Unsupported command '{}' received for channel '{}'" , command , channelUID ) ; } }
public void test() { try { helper . mergeTokenIntoJobConf ( jobConf , token ) ; } catch ( IOException e ) { LOG . error ( "Merging token to Kafka failed with exception " , e ) ; return ; } }
public void test() { if ( getStreamingURLforCurrentOrg ( ) == null ) { LOGGER . debug ( "No URLs found for organization " + organization . getId ( ) ) ; return Collections . emptyList ( ) ; } }
public void test() { if ( distributionDirectory == null ) { logger . warn ( "DistributionDirectory is null." ) ; return Collections . emptyList ( ) ; } }
public void activate ( final String userHashKey ) { log . info ( "Activating user hash key" ) ; final User user = userRepository . findByHashKey ( userHashKey ) ; code_block = IfStatement ; user . activate ( ) ; userRepository . save ( user ) ; }
public void test() { if ( ! partitionBase . renameTo ( new File ( partitionBase . getParent ( ) , partitionBase . getName ( ) + ".d." + SystemClock . now ( ) ) ) ) { logger . warning ( "Failed to rename partition base to " + partitionBase . getParent ( ) + " to " + partitionBase . getName ( ) ) ; } }
protected void onLinkDeletePost ( final String networkId , final Link link , final HashMap < String , Response > respList ) { log . debug ( "" ) ; }
public void test() { try { return ResponseUtils . buildSucessResponse ( targetTypesService . getTargetTypesByName ( targetTypeName ) ) ; } catch ( Exception exception ) { log . error ( UNEXPECTED_ERROR_OCCURRED , exception ) ; return ResponseUtils . buildFailureResponse ( new Exception ( UNEXPECTED_ERROR_OCCURRED ) , exception . getMessage ( ) ) ; } }
public void attachDirty ( SysExportBau instance ) { log . debug ( "attaching dirty SysExportBau instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { propertyService . setAsString ( PropertyIdEditPopup . this . getModelObject ( ) , valueModel . getObject ( ) ) ; Session . get ( ) . success ( getString ( "common.propertyId.action.edit.success" ) ) ; closePopup ( target ) ; target . addChildren ( getPage ( ) , PropertyIdListPanel . class ) ; } catch ( Exception e ) { LOGGER . error ( "Error while updating property value" , e ) ; Session . get ( ) . error ( getString ( "common.error.unexpected" ) ) ; } }
public void test() { else { LOGGER . info ( "Caught nested callback" ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( WikiPageServiceUtil . class , "updatePage" , _updatePageParameterTypes47 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , nodeId , title , version , content , summary , minorEdit , format , parentTitle , redirectTitle , serviceContext ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . wiki . model . WikiPage ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { com . liferay . portal . kernel . model . Address returnValue = AddressServiceUtil . getAddress ( addressId ) ; return com . liferay . portal . kernel . model . AddressSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( checksumFile . lastModified ( ) > lastModifiedChecksumFile ) { LOGGER . info ( String . format ( "Changing checksum %s from archive" , checksumFile . getAbsolutePath ( ) ) ) ; checksumArchive . clear ( ) ; loadFile ( ) ; } }
public void test() { if ( findHandler ( spec . prefix ) != null ) { log . warn ( "Cannot find handler with prefix " + spec . prefix ) ; } else { res = true ; HandlerRecord hrec = new HandlerRecord ( ) ; hrec . pathSpec = pathSpec ; hrec . handler = handler ; hrec . rqMapping = mapper . constructType ( rqMapping ) ; hrec . respMapping = mapper . constructType ( respMapping ) ; rqHandlers . putIfAbsent ( spec , hrec ) ; } }
public void test() { if ( debugEnabled ) { log . debug ( "destroy({}) Destroy ({})" , this , channel ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( log . isInfoEnabled ( ) ) { log . info ( msg ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( LayoutSetPrototypeServiceUtil . class , "fetchLayoutSetPrototype" , _fetchLayoutSetPrototypeParameterTypes5 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , layoutSetPrototypeId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . portal . kernel . model . LayoutSetPrototype ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
@ Override protected void processEmptyMessage ( ) throws Exception { LOG . debug ( "Processing empty message: {}" , exchange ) ; Exchange exchange = getEndpoint ( ) . createExchange ( ) ; exchange . setProperty ( ExchangePropertyKey . BATCH_INDEX , 0 ) ; exchange . setProperty ( ExchangePropertyKey . BATCH_SIZE , 1 ) ; exchange . setProperty ( ExchangePropertyKey . BATCH_COMPLETE , true ) ; getProcessor ( ) . process ( exchange ) ; }
public void test() { try { dumpFileDirectories = this . dumpfileDirectoryManager . getSubdirectories ( directoryPattern ) ; } catch ( IOException e ) { log . error ( "Could not get dump files" , e ) ; return Collections . emptyList ( ) ; } }
public void test() { if ( dumpFile . isAvailable ( ) ) { result . add ( dumpFile ) ; } else { log . error ( "Unable to find dump file " + dumpFile . getAbsolutePath ( ) ) ; } }
public SysDatadict findById ( sernet . gs . reveng . SysDatadictId id ) { log . debug ( "getting SysDatadict instance with id: " + id ) ; code_block = TryStatement ;  }
public void test() { if ( instance == null ) { log . debug ( "get successful, no instance found" ) ; } else { log . debug ( "get successful, instance found" ) ; } }
public void test() { if ( instance == null ) { log . debug ( "get successful, no instance found" ) ; } else { log . debug ( "get successful, instance found" ) ; } }
public void test() { try { SysDatadict instance = ( SysDatadict ) sessionFactory . getCurrentSession ( ) . get ( "sernet.gs.reveng.SysDatadict" , id ) ; code_block = IfStatement ; return instance ; } catch ( RuntimeException re ) { log . error ( "get failed" , re ) ; throw re ; } }
public void test() { if ( protonTransportErrorHandled ) { LOG . error ( "TransportError already handled" , ex ) ; return ; } }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; do code_block = "" ; while ( input . isReadable ( ) ) ; processUpdates ( ) ; pumpToProtonTransport ( ) ; } catch ( Throwable t ) { logger . debug ( t . getMessage ( ) , t ) ; fireProviderException ( ProviderExceptionSupport . createOrPassthroughFatal ( t ) ) ; } }
public void test() { try { Configuration conf = new Configuration ( ) ; conf . setBoolean ( "mapred.mapper.new-api" , true ) ; conf . setBoolean ( AngelConf . ANGEL_JOB_OUTPUT_PATH_DELETEONEXIST , true ) ; conf . set ( AngelConf . ANGEL_TASK_USER_TASKCLASS , DummyTask . class . getName ( ) ) ; conf . set ( AngelConf . ANGEL_DEPLOY_MODE , "LOCAL" ) ; conf . setBoolean ( AngelConf . ANGEL_AM_USE_DUMMY_DATASPLITER , true ) ; conf . set ( AngelConf . ANGEL_INPUTFORMAT_CLASS , CombineTextInputFormat . class . getName ( ) ) ; conf . set ( AngelConf . ANGEL_SAVE_MODEL_PATH , LOCAL_FS + TMP_PATH + "/out" ) ; conf . set ( AngelConf . ANGEL_TRAIN_DATA_PATH , LOCAL_FS + TMP_PATH + "/in" ) ; conf . set ( AngelConf . ANGEL_LOG_PATH , LOCAL_FS + TMP_PATH + "/log" ) ; conf . setInt ( AngelConf . ANGEL_WORKERGROUP_NUMBER , 1 ) ; conf . setInt ( AngelConf . ANGEL_PS_NUMBER , 1 ) ; conf . setInt ( AngelConf . ANGEL_WORKER_TASK_NUMBER , 2 ) ; conf . setInt ( AngelConf . ANGEL_WORKER_HEARTBEAT_INTERVAL_MS , 1000 ) ; conf . setInt ( AngelConf . ANGEL_PS_HEARTBEAT_INTERVAL_MS , 1000 ) ; angelClient = AngelClientFactory . get ( conf ) ; MatrixContext mMatrix = new MatrixContext ( ) ; mMatrix . setName ( "w1" ) ; mMatrix . setRowNum ( 1 ) ; mMatrix . setColNum ( 100000 ) ; mMatrix . setMaxRowNumInBlock ( 1 ) ; mMatrix . setMaxColNumInBlock ( 50000 ) ; mMatrix . setRowType ( RowType . T_INT_DENSE ) ; mMatrix . set ( MatrixConf . MATRIX_OPLOG_ENABLEFILTER , "false" ) ; } conf .
public void test() { { barrier . await ( ) ; Ignite ignite = startGrid ( startIdx . getAndIncrement ( ) ) ; assertTrue ( ignite . configuration ( ) . isClientMode ( ) ) ; log . info ( "Started node: " + ignite . name ( ) ) ; return null ; } }
@ Override protected String transform ( String value ) { String result = PREFIX + URLEncoder . encode ( value , StandardCharsets . UTF_8 ) ; LOGGER . debug ( "{} -> {}" , name , result ) ; return result ; }
public void test() { try { return Utils . filesystemSafe ( new URI ( getTerm ( url ) . replaceAll ( "&tags=" , "" ) ) . getPath ( ) ) ; } catch ( URISyntaxException ex ) { log . error ( "" , ex ) ; } }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; } catch ( IOException continued ) { _log . info ( "FAILED to close listener" , continued ) ; } }
public static TestSuite runTestSuite ( String testSuiteFolderPath , String sakuliHomeFolderPath , String browser , String sahiHomeFolder ) throws FileNotFoundException { String tempLogCache = "" ; tempLogCache = SakuliFolderHelper . checkTestSuiteFolderAndSetContextVariables ( testSuiteFolderPath , tempLogCache ) ; tempLogCache = SakuliFolderHelper . checkSakuliHomeFolderAndSetContextVariables ( sakuliHomeFolderPath , tempLogCache ) ; code_block = IfStatement ; code_block = IfStatement ; SahiConnector sahiConnector = BeanLoader . loadBean ( SahiConnector . class ) ; LOGGER . debug ( tempLogCache ) ; InitializingServiceHelper . invokeInitializingServcies ( ) ; TestSuite result = BeanLoader . loadBean ( TestSuite . class ) ; LOGGER . debug ( sahiServer . toString ( ) ) ; code_block = TryStatement ;  return result ; }
public static TestSuite runTestSuite ( String testSuiteFolderPath , String sakuliHomeFolderPath , String browser , String sahiHomeFolder ) throws FileNotFoundException { LOGGER . info ( String . format ( "\n\n=========== START new SAKULI Testsuite from '%s' =================" , testSuiteFolderPath ) ) ; String tempLogCache = "" ; tempLogCache = SakuliFolderHelper . checkTestSuiteFolderAndSetContextVariables ( testSuiteFolderPath , tempLogCache ) ; tempLogCache = SakuliFolderHelper . checkSakuliHomeFolderAndSetContextVariables ( sakuliHomeFolderPath , tempLogCache ) ; code_block = IfStatement ; code_block = IfStatement ; SahiConnector sahiConnector = BeanLoader . loadBean ( SahiConnector . class ) ; InitializingServiceHelper . invokeInitializingServcies ( ) ; TestSuite result = BeanLoader . loadBean ( TestSuite . class ) ; code_block = TryStatement ;  LOGGER . info ( String . format ( "\n\n=========== FINISHED new SAKULI Testsuite from '%s' =================" , testSuiteFolderPath ) ) ; return result ; }
public void test() { try { LOGGER . info ( "========== SERVER-DOWN SAKULI TEST SUITE '{}' ==========" , result . getId ( ) ) ; sahiConnector . init ( ) ; sahiConnector . startSahiTestSuite ( ) ; } catch ( SakuliInitException e ) { LOGGER . error ( "Unexpected error occurred:" , e ) ; System . exit ( 99 ) ; } finally { LOGGER . info ( "========== TEAR-DOWN SAKULI TEST SUITE '{}' ==========" , result . getId ( ) ) ; TeardownServiceHelper . invokeTeardownServices ( result , false ) ; result = BeanLoader . loadBean ( TestSuite . class ) ; BeanLoader . releaseContext ( ) ; } }
public void test() { try { sahiConnector . init ( ) ; LOGGER . debug ( "start new sakuli test suite" ) ; sahiConnector . startSahiTestSuite ( ) ; } catch ( SakuliInitException e ) { LOGGER . error ( "Could not init sakuli test suite" , e ) ; System . exit ( 99 ) ; } finally { LOGGER . info ( "========== TEAR-DOWN SAKULI TEST SUITE '{}' ==========" , result . getId ( ) ) ; TeardownServiceHelper . invokeTeardownServices ( result , false ) ; result = BeanLoader . loadBean ( TestSuite . class ) ; BeanLoader . releaseContext ( ) ; } }
public void test() { try { LOGGER . debug ( "start new sakuli test suite" ) ; sahiConnector . init ( ) ; LOGGER . debug ( "start new sakuli test suite" ) ; sahiConnector . startSahiTestSuite ( ) ; } catch ( SakuliInitException e ) { LOGGER . error ( "Unexpected error occurred:" , e ) ; System . exit ( 99 ) ; } finally { TeardownServiceHelper . invokeTeardownServices ( result , false ) ; result = BeanLoader . loadBean ( TestSuite . class ) ; BeanLoader . releaseContext ( ) ; } }
public void test() { try { result = engine . unwrap ( peerNetData , peerAppData ) ; } catch ( SSLException e ) { logger . warn ( e . getMessage ( ) ) ; throw e ; } }
public void test() { if ( ! grantedByScopes ) { LOG . warn ( "Difference in auth of user {} for ADMIN, scopes authorizer: {}, groups authorizer: true, user: {}" , user . getId ( ) , grantedByScopes , user ) ; } }
public void test() { if ( ! grantedByScopes ) { LOG . warn ( "Difference in auth of user {} for ADMIN, scopes authorizer: {}, groups authorizer: true, user: {}" , user . getId ( ) , grantedByScopes , user ) ; } }
public void test() { try { StoreClient store = imapClientProvider . getImapClient ( udr ) ; assertMoveItemIsSupported ( store ) ; store . select ( srcFolder . getPath ( ) ) ; MessageSet newUids = store . uidCopy ( messages , dstFolder . getPath ( ) ) ; LOG . info ( "Deleting message {}" , newUids ) ; deleteMessage ( store , messages ) ; return newUids ; } catch ( IMAPException e ) { throw new MailException ( e ) ; } catch ( MailboxNotFoundException e ) { throw new CollectionNotFoundException ( e ) ; } catch ( ImapTimeoutException e ) { throw new TimeoutException ( e ) ; } }
public void test() { try { HttpServletRequest request = ( HttpServletRequest ) serv ; SessionBean session = getSession ( request ) ; code_block = IfStatement ; } catch ( Exception e ) { LOG . error ( "Error while logging request" , e ) ; } finally { chain . doFilter ( serv , resp ) ; } }
public void test() { if ( responseFlag == AckSignalFlag . DUPLICATE_REQUEST ) { logger . error ( "messageReceived: Got a duplicate request" ) ; } else-if ( responseFlag != AckSignalFlag . NEW_REQUEST ) { throw new IllegalStateException ( "messageReceived: Got illegal response " + response ) ; } }
public void test() { if ( requestInfo == null ) { logger . warn ( "Received ack request for non-existent request!" ) ; } else { code_block = IfStatement ; flowControl . messageAckReceived ( senderId , requestId , response ) ; synchronized ( clientRequestIdRequestInfoMap ) code_block = "" ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { Session session = getCurrentSession ( ) ; I id = ( I ) session . save ( e ) ; session . flush ( ) ; session . evict ( e ) ; return id ; } catch ( Exception ex ) { log . error ( ex . getMessage ( ) , ex ) ; throw new HibernateException ( "Save failed" ) ; } }
public void test() { if ( cachedHostUuid != null ) { log . debug ( "Found existing host " + cachedHostUuid ) ; return ; } }
public List findByExample ( StgMapMas instance ) { log . debug ( "finding StgMapMas instance by example" ) ; code_block = TryStatement ;  }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.StgMapMas" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.StgMapMas" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
public void test() { try { Query q = em . createNamedQuery ( "CustomAttribute.deleteForOrg" ) ; q . setParameter ( "oid" , organizationId ) ; q . executeUpdate ( ) ; CustomAttribute ca ; code_block = ForStatement ; return provResult . newOkBaseResult ( ) ; } catch ( Exception e ) { logger . error ( "Could not delete custom attribute '{}'" , organizationId , e ) ; return provResult . getErrorResult ( BaseResult . class , e , getLocale ( requestingUser ) , null , null ) ; } }
public void test() { try { return em . unwrap ( Session . class ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceAddressServiceUtil . class , "updateCommerceAddress" , _updateCommerceAddressParameterTypes23 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , commerceAddressId , name , description , street1 , street2 , street3 , city , zip , regionId , countryId , phoneNumber , defaultBilling , defaultShipping , serviceContext ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . commerce . model . CommerceAddress ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
ActionResult < Wo > execute ( EffectivePerson effectivePerson , ServletContext servletContext ) throws Exception { ActionResult < Wo > result = new ActionResult < > ( ) ; com . x . base . core . project . Context ctx = ( com . x . base . core . project . Context ) servletContext . getAttribute ( com . x . base . core . project . AbstractContext . class . getName ( ) ) ; LoggerFactory . setLevelDebug ( ) ; LoggerFactory . setLevelDebug ( ) ; LoggerFactory . setLevelDebug ( ) ; LoggerFactory . setData ( new Wo ( true ) ) ; return result ; }
@ Override public void replaceAtomContainer ( int position , IAtomContainer container ) { logger . debug ( "Replacing atom: " , position ) ; super . replaceAtomContainer ( position , container ) ; }
public void test() { try { connection = SolrConnection . getConnection ( conf , BOLT_TYPE ) ; } catch ( Exception e ) { LOG . error ( "Could not connect to Solr server" , e ) ; throw new RuntimeException ( e ) ; } }
public void test() { if ( log . isInfoEnabled ( ) ) { log . info ( msg ) ; } }
public void test() { try { file . copy ( LocalFactory . get ( support , String . format ( "%s.cyberduckreceipt" , receipt . getName ( ) ) ) ) ; } catch ( AccessDeniedException e ) { log . warn ( e . getMessage ( ) ) ; } }
public void test() { try { streamingSensor . setState ( mode ) ; } catch ( jmri . JmriException ex ) { log . debug ( "Exception setting sensor's state" , ex ) ; } }
public void test() { try { List < CubeAssignment > cubeAssignmentList = Lists . newArrayList ( ) ; List < String > cubes = client . getChildren ( ) . forPath ( cubeRoot ) ; code_block = ForStatement ; readSuccess . getAndIncrement ( ) ; return cubeAssignmentList ; } catch ( Exception e ) { readFail . getAndIncrement ( ) ; logger . error ( "error get cube assignments" , e ) ; throw new StoreException ( e ) ; } }
public void test() { for ( Logger logger : this . loggers ) { logger . info ( message , e ) ; } }
public void test() { try { Object v = computing . apply ( producer . getAttribute ( sourceSensor ) ) ; code_block = IfStatement ; code_block = IfStatement ; } catch ( Throwable t ) { log . warn ( "Error computing value to " + sourceSensor + ": " + t ) ; throw Exceptions . propagate ( t ) ; } }
public void test() { switch ( field . getType ( ) ) { case STATE : changeOfStateSubscriptions . values ( ) . stream ( ) . filter ( pair -> pair . getKey ( ) . equals ( field ) ) . map ( Pair :: getValue ) . forEach ( baseDefaultPlcValueConsumer -> baseDefaultPlcValueConsumer . accept ( value ) ) ; state . put ( field , value ) ; return ; case STDOUT : logger . info ( "TEST PLC RANDOM [{}]: {}" , field . getName ( ) , value . toString ( ) ) ; return ; case RANDOM : code_block = SwitchStatement ; logger . info ( "TEST PLC RANDOM [{}]: {}" , field . getName ( ) , value . toString ( ) ) ; return ; } }
public void test() { try { DataItemIO . staticSerialize ( value , field . getPlcDataType ( ) , field . getNumberOfElements ( ) , false ) ; } catch ( ParseException e ) { LOGGER . error ( "Error parsing field item: " + field . getName ( ) , e ) ; } }
public void test() { switch ( field . getType ( ) ) { case STATE : changeOfStateSubscriptions . values ( ) . stream ( ) . filter ( pair -> pair . getKey ( ) . equals ( field ) ) . map ( Pair :: getValue ) . forEach ( baseDefaultPlcValueConsumer -> baseDefaultPlcValueConsumer . accept ( value ) ) ; state . put ( field , value ) ; return ; case STDOUT : logger . info ( "TEST PLC STDOUT [{}]: {}" , field . getName ( ) , value . getString ( ) ) ; return ; case RANDOM : logger . info ( "TEST PLC RANDOM [{}]: {}" , field . getName ( ) , value . getString ( ) ) ; code_block = SwitchStatement ; return ; } }
public void test() { try { String varName = actionParameter . getColumn ( ) ; code_block = IfStatement ; } catch ( EcmaError e ) { log . error ( "{}" , e . getMessage ( ) ) ; } }
public void test() { try { fileStore . delete ( fileMeta . getId ( ) ) ; } catch ( UncheckedIOException e ) { log . warn ( "Unable to delete file {}" , fileMeta . getId ( ) , e ) ; } }
public void debug ( String arg0 , Object arg1 , Object arg2 ) { logger . debug ( arg0 , arg1 , arg2 ) ; }
public void test() { if ( ! groupsWithAggregation . contains ( group ) ) { LOGGER . info ( "Sorting data for group {} and id {}" , group , id ) ; new SortGroupSplit ( fs , sparkSession , schemaUtils . columnsToSortBy ( group , reversed ) , files , sortedFiles , compressionCodecName ) . call ( ) ; } else { final String aggregatedFiles = outputDir + AGGREGATED ; LOGGER . info ( "Aggregating data for group {} and id {} ({} input files, results will be stored in {})" , group , id , files . size ( ) , aggregatedFiles ) ; final CallableResult result = new AggregateDataForGroup ( fs , schemaUtils , group , files , aggregatedFiles , sparkSession ) . call ( ) ; code_block = IfStatement ; } }
public void test() { if ( ! groupsWithAggregation . contains ( group ) ) { LOGGER . info ( "Sorting data for group {} and id {} ({} input files, results will be stored in {})" , group , id , files . size ( ) , sortedFiles ) ; new SortGroupSplit ( fs , sparkSession , schemaUtils . columnsToSortBy ( group , reversed ) , files , sortedFiles , compressionCodecName ) . call ( ) ; } else { LOGGER . debug ( "Creating aggregated data for group {} and id {} ({} input files)" , group , id , files . size ( ) ) ; final String aggregatedFiles = outputDir + AGGREGATED ; final CallableResult result = new AggregateDataForGroup ( fs , schemaUtils , group , files , aggregatedFiles , sparkSession ) . call ( ) ; code_block = IfStatement ; } }
public void test() { if ( null != result ) { LOGGER . info ( "Sorting aggregated data for group {} and id {} (results will be written to {})" , group , id , sortedFiles ) ; new SortGroupSplit ( fs , sparkSession , schemaUtils . columnsToSortBy ( group , reversed ) , aggregatedFiles , sortedFiles , compressionCodecName ) . call ( ) ; LOGGER . info ( "Deleting aggregated files in {} for group {} and {}" , aggregatedFiles , group , id ) ; fs . delete ( new Path ( aggregatedFiles ) , true ) ; } else { LOGGER . warn ( "Unable to sort aggregated files in {} for group {} and {}" , aggregatedFiles , group , id ) ; } }
public void test() { if ( fs . exists ( new Path ( sortedFiles ) ) ) { LOGGER . info ( "Moving all files of sorted data" ) ; final FileStatus [ ] files = fs . listStatus ( new Path ( sortedFiles ) ) ; code_block = ForStatement ; } else { LOGGER . info ( "No files of sorted data so there is nothing to move" ) ; } }
public void test() { if ( fs . exists ( new Path ( sortedFiles ) ) ) { LOGGER . info ( "Moving files of sorted data from {} to {} (group {}, id {})" , sortedFiles , outputDir , group , id ) ; final FileStatus [ ] files = fs . listStatus ( new Path ( sortedFiles ) ) ; code_block = ForStatement ; } else { LOGGER . info ( "Unable to move files of sorted data to {} (group {}, id {})" , group , group , id ) ; } }
public void test() { if ( oVal . toUpperCase ( ) . endsWith ( "UNDEF" ) || oVal . toUpperCase ( Locale . ROOT ) . endsWith ( "UNSET" ) ) { String removed = result . remove ( oKey ) ; logger . trace ( "Removing entry '" + oKey + "' from '" + removed + "'" ) ; } else { String was = result . get ( oKey ) ; was = ( was == null ? "NULL" : was ) ; result . put ( oKey , oVal ) ; logger . trace ( "Overrode key '" + oKey + "': from '" + was + " to " + oVal ) ; } }
public void test() { if ( oVal . toUpperCase ( ) . endsWith ( "UNDEF" ) || oVal . toUpperCase ( Locale . ROOT ) . endsWith ( "UNSET" ) ) { String removed = result . remove ( oKey ) ; logger . trace ( "Removed key '" + oKey + "': '" + removed + "' from script params because it was " + oVal + " in overrides" ) ; } else { String was = result . get ( oKey ) ; was = ( was == null ? "NULL" : was ) ; logger . trace ( "Removed key '" + oKey + "' from script params because it was " + was ) ; result . put ( oKey , oVal ) ; } }
public void test() { if ( ret . getItem ( ) != null ) { long insertedQueueId = ret . getItem ( ) . getId ( ) ; AnswerItem < Integer > retDep = testCaseExecutionQueueDepService . insertFromExeQueueIdDep ( insertedQueueId , exeQueueId ) ; LOG . debug ( "inserting job '{}' with exeQueueId '{}'" , exeQueueId , retDep . getId ( ) ) ; } }
public void test() { try { final String filename = mConfig . OutputDir + REF_FILE_SIG_PERC ; mRefDataWriter = createBufferedWriter ( filename , false ) ; mRefDataWriter . write ( "CancerType,DataType" ) ; code_block = ForStatement ; mRefDataWriter . newLine ( ) ; } catch ( IOException e ) { CUP_LOGGER . error ( "failed to write ref data to file: {}" , e . toString ( ) ) ; } }
public void test() { if ( _Employee . LOG . isDebugEnabled ( ) ) { _Employee . LOG . debug ( "updating exemptions from " + exemptions ( ) + " to " + value ) ; } }
public void test() { if ( signingKey != null ) { idpDescriptor . getKeyDescriptors ( ) . add ( getKeyDescriptor ( UsageType . SIGNING , getServerKeyInfo ( signingKey ) ) ) ; } else { LOG . warn ( "No signing key available" ) ; } }
public void test() { if ( encryptionKey != null ) { idpDescriptor . getKeyDescriptors ( ) . add ( getKeyDescriptor ( UsageType . ENCRYPTION , getServerKeyInfo ( encryptionKey ) ) ) ; } else { LOG . warn ( "No encryption key available." ) ; } }
private boolean testForDuplicateSize ( SearchResultItem result1 , SearchResultItem result2 , float duplicateSizeDifference ) { code_block = IfStatement ; long sizeDifference = Math . abs ( result1 . getSize ( ) - result2 . getSize ( ) ) ; float sizeAverage = ( result1 . getSize ( ) + result2 . getSize ( ) ) / 2F ; float sizeDiffPercent = Math . abs ( sizeDifference / sizeAverage ) * 100 ; boolean sameSize = sizeDiffPercent <= duplicateSizeDifference ; logger . debug ( "Average size is {}" , sizeDiffPercent ) ; return sameSize ; }
@ Override public void replicateCollectionAsynchronously ( final String irodsCollectionAbsolutePath , final String resourceName , final int delayInMinutes ) throws JargonException { log . info ( "ReplicateCollectionAsynchronously" ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "irodsCollectionAbsolutePath:{}" , irodsCollectionAbsolutePath ) ; log . info ( "resourceName:{}" , resourceName ) ; log . info ( "delayInMinutes:{}" , delayInMinutes ) ; code_block = IfStatement ; RuleProcessingAO ruleProcessingAO = getIRODSAccessObjectFactory ( ) . getRuleProcessingAO ( getIRODSAccount ( ) ) ; List < IRODSRuleParameter > irodsRuleParameters = new ArrayList < IRODSRuleParameter > ( ) ; irodsRuleParameters . add ( new IRODSRuleParameter ( "*SourceFile" , MiscIRODSUtils . wrapStringInQuotes ( irodsCollectionAbsolutePath ) ) ) ; irodsRuleParameters . add ( new IRODSRuleParameter ( "*Resource" , MiscIRODSUtils . wrapStringInQuotes ( resourceName ) ) ) ; irodsRuleParameters . add ( new IRODSRuleParameter ( "*DelayInfo" , RuleUtils . buildDelayParamForMinutes ( delayInMinutes ) ) ) ; RuleInvocationConfiguration ruleInvocationConfiguration = RuleInvocationConfiguration . instanceWithDefaultAutoSettings ( getJargonProperties ( ) ) ; ruleInvocationConfiguration . setRuleProcessingType ( RuleProcessingType . EXTERNAL ) ; IRODSRuleExecResult result = ruleProcessingAO . executeRuleFromResource ( "/rules/rulemsiCollReplAsync.r" , irodsRuleParameters , ruleInvocationConfiguration ) ; log . info ( "result of action:{}" , result . getRuleExecOut ( ) . trim ( ) ) ; }
@ Override public void replicateCollectionAsynchronously ( final String irodsCollectionAbsolutePath , final String resourceName , final int delayInMinutes ) throws JargonException { log . info ( "replicateCollectionAsynchronously()" ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "irodsCollectionAbsolutePath:{}" , irodsCollectionAbsolutePath ) ; log . info ( "resourceName:{}" , resourceName ) ; log . info ( "delayInMinutes:{}" , delayInMinutes ) ; code_block = IfStatement ; RuleProcessingAO ruleProcessingAO = getIRODSAccessObjectFactory ( ) . getRuleProcessingAO ( getIRODSAccount ( ) ) ; List < IRODSRuleParameter > irodsRuleParameters = new ArrayList < IRODSRuleParameter > ( ) ; irodsRuleParameters . add ( new IRODSRuleParameter ( "*SourceFile" , MiscIRODSUtils . wrapStringInQuotes ( irodsCollectionAbsolutePath ) ) ) ; irodsRuleParameters . add ( new IRODSRuleParameter ( "*Resource" , MiscIRODSUtils . wrapStringInQuotes ( resourceName ) ) ) ; irodsRuleParameters . add ( new IRODSRuleParameter ( "*DelayInfo" , RuleUtils . buildDelayParamForMinutes ( delayInMinutes ) ) ) ; RuleInvocationConfiguration ruleInvocationConfiguration = RuleInvocationConfiguration . instanceWithDefaultAutoSettings ( getJargonProperties ( ) ) ; ruleInvocationConfiguration . setRuleProcessingType ( RuleProcessingType . EXTERNAL ) ; IRODSRuleExecResult result = ruleProcessingAO . executeRuleFromResource ( "/rules/rulemsiCollReplAsync.r" , irodsRuleParameters , ruleInvocationConfiguration ) ; log . info ( "result of action:{}" , result . getRuleExecOut ( ) . trim ( ) ) ; }
@ Override public void replicateCollectionAsynchronously ( final String irodsCollectionAbsolutePath , final String resourceName , final int delayInMinutes ) throws JargonException { log . info ( "replicateCollectionAsynchronously()" ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "irodsCollectionAbsolutePath:{}" , irodsCollectionAbsolutePath ) ; log . info ( "resourceName:{}" , resourceName ) ; log . info ( "delayInMinutes:{}" , delayInMinutes ) ; code_block = IfStatement ; RuleProcessingAO ruleProcessingAO = getIRODSAccessObjectFactory ( ) . getRuleProcessingAO ( getIRODSAccount ( ) ) ; List < IRODSRuleParameter > irodsRuleParameters = new ArrayList < IRODSRuleParameter > ( ) ; irodsRuleParameters . add ( new IRODSRuleParameter ( "*SourceFile" , MiscIRODSUtils . wrapStringInQuotes ( irodsCollectionAbsolutePath ) ) ) ; irodsRuleParameters . add ( new IRODSRuleParameter ( "*Resource" , MiscIRODSUtils . wrapStringInQuotes ( resourceName ) ) ) ; irodsRuleParameters . add ( new IRODSRuleParameter ( "*DelayInfo" , RuleUtils . buildDelayParamForMinutes ( delayInMinutes ) ) ) ; RuleInvocationConfiguration ruleInvocationConfiguration = RuleInvocationConfiguration . instanceWithDefaultAutoSettings ( getJargonProperties ( ) ) ; ruleInvocationConfiguration . setRuleProcessingType ( RuleProcessingType . EXTERNAL ) ; IRODSRuleExecResult result = ruleProcessingAO . executeRuleFromResource ( "/rules/rulemsiCollReplAsync.r" , irodsRuleParameters , ruleInvocationConfiguration ) ; log . info ( "result of action:{}" , result . getRuleExecOut ( ) . trim ( ) ) ; }
@ Override public void replicateCollectionAsynchronously ( final String irodsCollectionAbsolutePath , final String resourceName , final int delayInMinutes ) throws JargonException { log . info ( "replicateCollectionAsynchronously()" ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "irodsCollectionAbsolutePath:{}" , irodsCollectionAbsolutePath ) ; log . info ( "resourceName:{}" , resourceName ) ; log . info ( "delayInMinutes:{}" , delayInMinutes ) ; code_block = IfStatement ; RuleProcessingAO ruleProcessingAO = getIRODSAccessObjectFactory ( ) . getRuleProcessingAO ( getIRODSAccount ( ) ) ; List < IRODSRuleParameter > irodsRuleParameters = new ArrayList < IRODSRuleParameter > ( ) ; irodsRuleParameters . add ( new IRODSRuleParameter ( "*SourceFile" , MiscIRODSUtils . wrapStringInQuotes ( irodsCollectionAbsolutePath ) ) ) ; irodsRuleParameters . add ( new IRODSRuleParameter ( "*Resource" , MiscIRODSUtils . wrapStringInQuotes ( resourceName ) ) ) ; irodsRuleParameters . add ( new IRODSRuleParameter ( "*DelayInfo" , RuleUtils . buildDelayParamForMinutes ( delayInMinutes ) ) ) ; RuleInvocationConfiguration ruleInvocationConfiguration = RuleInvocationConfiguration . instanceWithDefaultAutoSettings ( getJargonProperties ( ) ) ; ruleInvocationConfiguration . setRuleProcessingType ( RuleProcessingType . EXTERNAL ) ; IRODSRuleExecResult result = ruleProcessingAO . executeRuleFromResource ( "/rules/rulemsiCollReplAsync.r" , irodsRuleParameters , ruleInvocationConfiguration ) ; log . info ( "result of action:{}" , result . getRuleExecOut ( ) . trim ( ) ) ; }
@ Override public void replicateCollectionAsynchronously ( final String irodsCollectionAbsolutePath , final String resourceName , final int delayInMinutes ) throws JargonException { log . info ( "replicateCollectionAsynchronously()" ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "irodsCollectionAbsolutePath:{}" , irodsCollectionAbsolutePath ) ; log . info ( "resourceName:{}" , resourceName ) ; log . info ( "delayInMinutes:{}" , delayInMinutes ) ; code_block = IfStatement ; RuleProcessingAO ruleProcessingAO = getIRODSAccessObjectFactory ( ) . getRuleProcessingAO ( getIRODSAccount ( ) ) ; List < IRODSRuleParameter > irodsRuleParameters = new ArrayList < IRODSRuleParameter > ( ) ; irodsRuleParameters . add ( new IRODSRuleParameter ( "*SourceFile" , MiscIRODSUtils . wrapStringInQuotes ( irodsCollectionAbsolutePath ) ) ) ; irodsRuleParameters . add ( new IRODSRuleParameter ( "*Resource" , MiscIRODSUtils . wrapStringInQuotes ( resourceName ) ) ) ; irodsRuleParameters . add ( new IRODSRuleParameter ( "*DelayInfo" , RuleUtils . buildDelayParamForMinutes ( delayInMinutes ) ) ) ; RuleInvocationConfiguration ruleInvocationConfiguration = RuleInvocationConfiguration . instanceWithDefaultAutoSettings ( getJargonProperties ( ) ) ; ruleInvocationConfiguration . setRuleProcessingType ( RuleProcessingType . EXTERNAL ) ; IRODSRuleExecResult result = ruleProcessingAO . executeRuleFromResource ( "/rules/rulemsiCollReplAsync.r" , irodsRuleParameters , ruleInvocationConfiguration ) ; log . info ( "result:{}" , result ) ; }
public void test() { if ( line . hasOption ( 'f' ) ) { expectedFailure = true ; logger . info ( "expectedFailure = " + expectedFailure ) ; } }
public void test() { try { line = parser . parse ( options , args ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; } catch ( ParseException e ) { logger . error ( "Failed to parse command line: " + e . getMessage ( ) ) ; HelpFormatter formatter = new HelpFormatter ( ) ; formatter . printHelp ( ProviderApplication . class . getName ( ) , options , true ) ; System . exit ( 1 ) ; } }
public void test() { if ( ! gbidsParameter . isEmpty ( ) ) { gbids = Arrays . stream ( gbidsParameter . split ( "," ) ) . map ( a -> a . trim ( ) ) . toArray ( String [ ] :: new ) ; logger . debug ( "Searching for providers on domain \"{}\"" , providerDomain ) ; } else { gbids = new String [ 0 ] ; logger . debug ( "Searching for providers on domain \"{}\"" , providerDomain ) ; } }
public static void main ( String [ ] args ) throws IOException { CommandLine line ; Options options = new Options ( ) ; Options helpOptions = new Options ( ) ; setupOptions ( options , helpOptions ) ; CommandLineParser parser = new DefaultParser ( ) ; code_block = TryStatement ;  code_block = TryStatement ;  code_block = IfStatement ; Properties joynrConfig = new Properties ( ) ; Module runtimeModule = getRuntimeModule ( args , joynrConfig ) ; logger . debug ( "Searching for providers on domain \"{}\"" , providerDomain ) ; logger . debug ( "Searching for providers on domain \"{}\"" , providerDomain ) ; joynrConfig . setProperty ( MessagingPropertyKeys . PERSISTENCE_FILE , STATIC_PERSISTENCE_FILE ) ; joynrConfig . setProperty ( PROPERTY_JOYNR_DOMAIN_LOCAL , providerDomain ) ; Properties appConfig = new Properties ( ) ; appConfig . setProperty ( SYSTEMINTEGRATIONTEST_PROVIDER_DOMAIN , providerDomain ) ; JoynrApplication myConsumerApp = new JoynrInjectorFactory ( joynrConfig , runtimeModule ) . createApplication ( new JoynrApplicationModule ( ConsumerApplication . class , appConfig ) ) ; myConsumerApp . run ( ) ; myConsumerApp . shutdown ( ) ; }
public void test() { try { log . info ( "Deleting document {}" , delete . getQuery ( ) ) ; final StopWatch elapsedTime = StopWatch . createStarted ( ) ; final QueryBuilder deleteQuery = ElasticQueryBuilder . buildFilterQuery ( delete . getQuery ( ) , factory , delete . getUpdateContext ( ) , this . currentFootprint ) ; final BulkByScrollResponse response = elasticSearchClient . deleteByQuery ( deleteQuery ) ; code_block = IfStatement ; elapsedTime . stop ( ) ; return new DeleteResult ( response . getTook ( ) . getMillis ( ) ) . setElapsedTime ( elapsedTime . getTime ( ) ) ; } catch ( ElasticsearchException | IOException e ) { log . error ( "Cannot delete with query {}" , delete . getQuery ( ) , e ) ; throw new SearchServerException ( String . format ( "Cannot delete with query %s" , delete . getQuery ( ) . toString ( ) ) , e ) ; } }
public void test() { try { elasticClientLogger . debug ( ">>> delete({})" , delete ) ; final QueryBuilder deleteQuery = ElasticQueryBuilder . buildFilterQuery ( delete . getQuery ( ) , factory , delete . getUpdateContext ( ) , this . currentFootprint ) ; final BulkByScrollResponse response = elasticSearchClient . deleteByQuery ( deleteQuery ) ; code_block = IfStatement ; elapsedTime . stop ( ) ; return new DeleteResult ( response . getTook ( ) . getMillis ( ) ) . setElapsedTime ( elapsedTime . getTime ( ) ) ; } catch ( ElasticsearchException | IOException e ) { logger . error ( "Cannot delete with query {}" , delete . getQuery ( ) . toString ( ) , e ) ; throw new SearchServerException ( String . format ( "Cannot delete with query %s" , delete . getQuery ( ) . toString ( ) ) , e ) ; } }
public void test() { try { testStatement . close ( ) ; } catch ( Exception e ) { LOG . error ( "Failed to close the connection" , e ) ; } }
public void test() { try { ret = convertPathToContainer ( basePath , next ) ; } catch ( NoSuchElementException | GenericException | RequestNotValidException e ) { LOGGER . error ( "Error while list path " + basePath + " while parsing resource " + next , e ) ; ret = null ; } }
public void test() { try { String type = tika . detect ( input ) ; code_block = IfStatement ; } catch ( IOException e ) { LOGGER . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { PackageUpdateService pus = Framework . getLocalService ( PackageUpdateService . class ) ; pus . removePackage ( pkgId ) ; return getView ( "removeDone" ) . arg ( "pkgId" , pkgId ) . arg ( "source" , source ) ; } catch ( Exception e ) { log . error ( "Error while removing package" , e ) ; return getView ( "removeError" ) . arg ( "e" , e ) ; } }
public void test() { if ( xp != null ) { String superCo = xp . getSuperComponent ( ) ; code_block = IfStatement ; } else { log . error ( "Unable to find super component" ) ; Framework . handleDevError ( null ) ; } }
public void test() { try ( PreparedStatement query = getConn ( "config" ) . prepareStatement ( "SELECT count(id) FROM organizations WHERE orgId = ?" ) ) { query . setString ( 1 , orgId ) ; code_block = IfStatement ; } catch ( PSQLException pe ) { ServerErrorMessage em = pe . getServerErrorMessage ( ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . warn ( String . format ( "%s" , LogUtil . getStackTrace ( e ) ) ) ; logger . debug ( "Exception" , e ) ; } finally { closeConfig ( ) ; } }
public void test() { try ( PreparedStatement query = getConn ( "config" ) . prepareStatement ( "SELECT count(id) FROM organizations WHERE orgId = ?" ) ) { query . setString ( 1 , orgId ) ; code_block = IfStatement ; } catch ( PSQLException pe ) { ServerErrorMessage em = pe . getServerErrorMessage ( ) ; logger . warn ( em . toString ( ) ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . warn ( "Failed to retrieve organization details" , e ) ; } finally { closeConfig ( ) ; } }
public void test() { try { processCommand ( command , host . getState ( ) ) ; } catch ( IOException e ) { log . error ( "Command {} failed" , command , e ) ; break ; } }
public void test() { try { HostCommand command = commandQueue . poll ( MAIN_THREAD_STEP_SLEEP_MS , TimeUnit . MILLISECONDS ) ; code_block = IfStatement ; } catch ( InterruptedException e ) { logger . warn ( "Thread interrupted" , e ) ; break ; } }
public void run ( ) throws IOException , InterruptedException { connectToRing ( ) ; addShutdownHook ( ) ; setStateSynchronized ( HostState . IDLE ) ; addServerOfflineWatcher ( ) ; code_block = WhileStatement ; processCommandOnStartup ( ) ; code_block = WhileStatement ; stopServingData ( ) ; stopUpdating ( ) ; setStateSynchronized ( HostState . OFFLINE ) ; removeShutdownHook ( ) ; coordinator . close ( ) ; LOG . info ( "Shutdown completed." ) ; }
public void test() { if ( connection != null ) { code_block = TryStatement ;  } }
public void test() { try { container = toCredential ( ) ; cert = container . getCert ( ) ; key = container . getKey ( ) ; } catch ( CertificateException e ) { LOG . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { readByteOffset ++ ; offset ++ ; code_block = IfStatement ; byteRead = inBuffer [ readByteOffset ] ; } catch ( IOException ioe ) { logger . warn ( "Unexpected IOException" , ioe ) ; } }
public void test() { try { execute ( new Command < Void > ( ) code_block = "" ; ) ; } catch ( KafkaException kex ) { logger . warn ( "Kafka broker exception, topic: {}, app: {}" , topic , kex . getMessage ( ) ) ; return false ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( "Closing connection {}" , connection ) ; } }
public void test() { if ( unavailableNode && t . getMessage ( ) . startsWith ( "UNAVAILABLE" ) ) { LOG . warn ( "Unable to connect to node {}" , node ) ; return Optional . empty ( ) ; } }
public void test() { if ( verboseLoggingOn ) { log . info ( spec . logPrefix ( ) + this + " failed!" ) ; } else-if ( ! loggingOff ) { log . warn ( spec . logPrefix ( ) + this + " failed {}!" , t . getMessage ( ) ) ; } }
public void test() { if ( verboseLoggingOn ) { log . warn ( spec . logPrefix ( ) + this + " failed - {}" , t ) ; } else-if ( ! loggingOff ) { log . info ( spec . logPrefix ( ) + this + " failed - {}" , t ) ; } }
public void test() { if ( unavailableNode ) { String message = String . format ( "Node %s is NOT unavailable as expected!!!" , HapiPropertySource . asAccountString ( node . get ( ) ) ) ; LOG . error ( message ) ; return Optional . of ( new RuntimeException ( message ) ) ; } }
public void test() { try { String result = this . checkUser ( ) ; if ( null != result ) return result ; code_block = IfStatement ; UserAuthsFormBean authsBean = this . getUserAuthsFormBean ( ) ; this . getAuthorizationManager ( ) . updateUserAuthorizations ( username , authsBean . getAuthorizations ( ) ) ; this . getRequest ( ) . getSession ( ) . removeAttribute ( CURRENT_FORM_USER_AUTHS_PARAM_NAME ) ; } catch ( Throwable t ) { _logger . error ( "error in updateUser" , t ) ; return FAILURE ; } }
public void init ( String [ ] args ) { initLogger ( ) ; conf = loadConfig ( args ) ; conf . startCleanup ( ) ; logger . info ( "Loaded " + Arrays . toString ( args ) ) ; }
public void test() { try { revisionNote = getRepo ( 0 ) . get ( noteId , revId , subject ) ; } catch ( IOException e ) { LOGGER . error ( "Fail to get revision note: {}" , noteId , e ) ; } }
public void test() { try ( Connection connection = DriverManager . getConnection ( Config . IOTDB_URL_PREFIX + "127.0.0.1:6667/" , "root" , "root" ) ; Statement statement = connection . createStatement ( ) ) { Thread . sleep ( 500 ) ; statement . execute ( "create trigger trigger-1 before insert on root.vehicle.d1.s1 as \"org.apache.iotdb.db.engine.trigger.example.Counter\"" ) ; statement . execute ( "create trigger trigger-2 after insert on root.vehicle.d1.s2 as \"org.apache.iotdb.db.engine.trigger.example.Counter\"" ) ; statement . execute ( "create trigger trigger-3 before insert on root.vehicle.d1.s3 as \"org.apache.iotdb.db.engine.trigger.example.Counter\"" ) ; Thread . sleep ( 500 ) ; int [ ] counters1 = getCounters ( 3 ) ; LOGGER . info ( Arrays . toString ( counters1 ) ) ; code_block = ForStatement ; Thread . sleep ( 500 ) ; statement . execute ( "create trigger trigger-4 after insert on root.vehicle.d1.s4 as \"org.apache.iotdb.db.engine.trigger.example.Counter\"" ) ; statement . execute ( "create trigger trigger-5 before insert on root.vehicle.d1.s5 as \"org.apache.iotdb.db.engine.trigger.example.Counter\"" ) ; statement . execute ( "create trigger trigger-6 after insert on root.vehicle.d1.s6 as \"org.apache.iotdb.db.engine.trigger.example.Counter\"" ) ; int [ ] counters2 = getCounters ( 3 ) ; LOGGER . info ( Arrays . toString ( counters2 ) ) ; code_block = ForStatement ; } catch ( SQLException | TriggerManagementException e ) { fail ( e . getMessage ( ) ) ; } }
public void test() { try ( Connection connection = DriverManager . getConnection ( Config . IOTDB_URL_PREFIX + "127.0.0.1:6667/" , "root" , "root" ) ; Statement statement = connection . createStatement ( ) ) { Thread . sleep ( 500 ) ; statement . execute ( "create trigger trigger-1 before insert on root.vehicle.d1.s1 as \"org.apache.iotdb.db.engine.trigger.example.Counter\"" ) ; statement . execute ( "create trigger trigger-2 after insert on root.vehicle.d1.s2 as \"org.apache.iotdb.db.engine.trigger.example.Counter\"" ) ; statement . execute ( "create trigger trigger-3 before insert on root.vehicle.d1.s3 as \"org.apache.iotdb.db.engine.trigger.example.Counter\"" ) ; Thread . sleep ( 500 ) ; int [ ] counters1 = getCounters ( 3 ) ; LOGGER . info ( Arrays . toString ( counters1 ) ) ; code_block = ForStatement ; Thread . sleep ( 500 ) ; statement . execute ( "create trigger trigger-4 after insert on root.vehicle.d1.s4 as \"org.apache.iotdb.db.engine.trigger.example.Counter\"" ) ; statement . execute ( "create trigger trigger-5 before insert on root.vehicle.d1.s5 as \"org.apache.iotdb.db.engine.trigger.example.Counter\"" ) ; statement . execute ( "create trigger trigger-6 after insert on root.vehicle.d1.s6 as \"org.apache.iotdb.db.engine.trigger.example.Counter\"" ) ; int [ ] counters2 = getCounters ( 3 ) ; LOGGER . info ( Arrays . toString ( counters2 ) ) ; code_block = ForStatement ; } catch ( SQLException | TriggerManagementException e ) { fail ( e . getMessage ( ) ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) && SerialPortEvent . DATA_AVAILABLE != seEvent . getEventType ( ) ) { logger . trace ( "Serial port event is available." ) ; } }
public void test() { try { code_block = SwitchStatement ; } catch ( RuntimeException e ) { logger . warn ( "Exception:" , e ) ; } }
public void test() { if ( diff > CHANNEL_EXPIRED_TIMEOUT ) { log . warn ( "Client {} expired, will timeout." , clientChannelInfo . getChannel ( ) ) ; RemotingUtil . closeChannel ( clientChannelInfo . getChannel ( ) ) ; itChannel . remove ( ) ; } }
public void test() { if ( ! dumpDirectory . mkdirs ( ) ) { LOGGER . error ( "Unable to create directory" ) ; return false ; } }
public void test() { if ( ! dumpDirectory . exists ( ) ) { code_block = IfStatement ; } else-if ( ! dumpDirectory . isDirectory ( ) ) { _log . error ( "Unable to create dump directory " + dumpDirectory . getAbsolutePath ( ) ) ; return false ; } }
public void test() { try { this . networkManager . initialize ( this . serverConfig . maxPlayers ( ) , host , port ) ; code_block = IfStatement ; } catch ( Exception e ) { log . error ( "Error initializing network manager" , e ) ; return false ; } }
public void test() { try { String managementV6Address = execution . getVariable ( "oamManagementV6Address" ) ; code_block = IfStatement ; } catch ( Exception ex ) { logger . error ( "error updating OamManagementV6Address" , ex ) ; exceptionUtil . buildAndThrowWorkflowException ( execution , 7000 , ex ) ; } }
@ Override public void addProduct ( IAtomContainer product , Double coefficient ) { logger . debug ( "Adding product: " , product ) ; super . addProduct ( product , coefficient ) ; }
public void test() { if ( notRepliedYet . isEmpty ( ) || pendingRemovals . containsAll ( notRepliedYet ) ) { logger . debug ( "Not waiting for these view replies" ) ; waiting = false ; notifyAll ( ) ; } else { logger . debug ( "Still waiting for these view replies: {}" , notRepliedYet ) ; } }
public void test() { if ( notRepliedYet . isEmpty ( ) || pendingRemovals . containsAll ( notRepliedYet ) ) { logger . debug ( "Allanticipated view responses received - notifying waiting thread" ) ; waiting = false ; notifyAll ( ) ; } else { logger . debug ( "Allocating view responses received, not replicating thread" ) ; } }
private Set < SystemResponseDTO > getAuthorizedPublishers ( final SystemRequestDTO subscriberSystem ) { logger . debug ( "getAuthorizedPublishers started..." ) ; Assert . notNull ( subscriberSystem , "subscriberSystem is null." ) ; final UriComponents checkUri = getAuthSubscriptionCheckUri ( ) ; final AuthorizationSubscriptionCheckRequestDTO payload = new AuthorizationSubscriptionCheckRequestDTO ( subscriberSystem , null ) ; final ResponseEntity < AuthorizationSubscriptionCheckResponseDTO > response = httpService . sendRequest ( checkUri , HttpMethod . POST , AuthorizationSubscriptionCheckResponseDTO . class , payload ) ; return response . getBody ( ) . getPublishers ( ) ; }
public void test() { try { final NodeTemplateInstanceProperty property = this . convertDocumentToProperty ( properties , NodeTemplateInstanceProperty . class ) ; node . addProperty ( property ) ; this . nodeTemplateInstanceRepository . update ( node ) ; } catch ( InstantiationException | IllegalAccessException e ) { final String msg = String . format ( "An error occurred while instantiating an instance of the %s class." , NodeTemplateInstanceProperty . class ) ; this . logger . error ( msg , e ) ; throw e ; } }
public void test() { try { _waitStopFinishCountDown . await ( ) ; _started = false ; } catch ( InterruptedException e ) { LOG . error ( "Interrupted" , e ) ; } }
public void printHeader ( List < ArtifactRepository > repositories ) { log . info ( "dependencies, and where they are available:" ) ; final int repCount = repositories . size ( ) ; log . info ( StringUtils . join ( ", " , repCount ) ) ; code_block = ForStatement ; log . info ( StringUtils . repeat ( "|" + sep , repCount ) ) ; }
public void printHeader ( List < ArtifactRepository > repositories ) { log . info ( "" ) ; final int repCount = repositories . size ( ) ; log . info ( "Found {} repositories" , repositories . size ( ) ) ; code_block = ForStatement ; log . info ( StringUtils . repeat ( "|" + sep , repCount ) ) ; }
public void test() { for ( int i = 0 ; i < repCount ; i ++ ) { final ArtifactRepository rep = repositories . get ( i ) ; log . debug ( "found repository: " + rep . getName ( ) ) ; } }
public void printHeader ( List < ArtifactRepository > repositories ) { log . info ( "" ) ; log . info ( "dependencies, and where they are available:" ) ; final int repCount = repositories . size ( ) ; log . info ( "Repositories: " + repCount ) ; code_block = ForStatement ; }
@ Override @ POST @ Path ( UNITS_RULES_URI ) @ Consumes ( MediaType . APPLICATION_JSON ) @ Produces ( MediaType . APPLICATION_JSON ) public Response massUpdateUnitsRules ( MassUpdateUnitRuleRequest massUpdateUnitRuleRequest ) { logger . debug ( "Calling massUpdateUnitRule" ) ; JsonNode queryDsl = massUpdateUnitRuleRequest . getDslRequest ( ) . deepCopy ( ) ; RuleActions ruleActions = massUpdateUnitRuleRequest . getRuleActions ( ) ; Status status ; code_block = TryStatement ;  }
public void test() { try ( ProcessingManagementClient processingClient = processingManagementClientFactory . getClient ( ) ; LogbookOperationsClient logbookOperationsClient = logbookOperationsClientFactory . getClient ( ) ; WorkspaceClient workspaceClient = workspaceClientFactory . getClient ( ) ) { SanityChecker . checkJsonAll ( queryDsl ) ; code_block = IfStatement ; String operationId = getVitamSession ( ) . getRequestId ( ) ; final LogbookOperationParameters initParameters = LogbookParameterHelper . newLogbookOperationParameters ( GUIDReader . getGUID ( operationId ) , Contexts . MASS_UPDATE_UNIT_RULE . getEventType ( ) , GUIDReader . getGUID ( operationId ) , LogbookTypeProcess . MASS_UPDATE , STARTED , VitamLogbookMessages . getCodeOp ( Contexts . MASS_UPDATE_UNIT_RULE . getEventType ( ) , STARTED ) , GUIDReader . getGUID ( operationId ) ) ; addRightsStatementIdentifier ( initParameters ) ; logbookOperationsClient . create ( initParameters ) ; workspaceClient . createContainer ( operationId ) ; workspaceClient . putObject ( operationId , OperationContextMonitor . OperationContextFileName , writeToInpustream ( OperationContextModel . get ( massUpdateUnitRuleRequest ) ) ) ; workspaceClient . putObject ( operationId , QUERY_FILE , writeToInpustream ( applyAccessContractRestrictionForUnitForUpdate ( queryDsl , getVitamSession ( ) . getContract ( ) ) ) ) ; workspaceClient . putObject ( operationId , "actions.json" , writeToInpustream ( ruleActions ) ) ; OperationContextMonitor . compressInWorkspace ( workspaceClientFactory , operationId , Contexts . MASS_UPDATE_UNIT_RULE . getLogbookTypeProcess ( ) , OperationContextMonitor . OperationContextFileName ) ; processingClient . initVitamProcess ( operationId , Contexts . MASS_UPDATE_UNIT_RULE . name ( ) ) ; RequestResponse < ItemStatus > requestResponse = processingClient . executeOperationProcess ( operationId , Contexts . MASS_UPDATE_UNIT_RULE . name ( ) , RESUME . getValue ( ) ) ; logger . info ( context , "
public void test() { try ( ProcessingManagementClient processingClient = processingManagementClientFactory . getClient ( ) ; LogbookOperationsClient logbookOperationsClient = logbookOperationsClientFactory . getClient ( ) ; WorkspaceClient workspaceClient = workspaceClientFactory . getClient ( ) ) { SanityChecker . checkJsonAll ( queryDsl ) ; code_block = IfStatement ; String operationId = getVitamSession ( ) . getRequestId ( ) ; final LogbookOperationParameters initParameters = LogbookParameterHelper . newLogbookOperationParameters ( GUIDReader . getGUID ( operationId ) , Contexts . MASS_UPDATE_UNIT_RULE . getEventType ( ) , GUIDReader . getGUID ( operationId ) , LogbookTypeProcess . MASS_UPDATE , STARTED , VitamLogbookMessages . getCodeOp ( Contexts . MASS_UPDATE_UNIT_RULE . getEventType ( ) , STARTED ) , GUIDReader . getGUID ( operationId ) ) ; addRightsStatementIdentifier ( initParameters ) ; logbookOperationsClient . create ( initParameters ) ; workspaceClient . createContainer ( operationId ) ; workspaceClient . putObject ( operationId , OperationContextMonitor . OperationContextFileName , writeToInpustream ( OperationContextModel . get ( massUpdateUnitRuleRequest ) ) ) ; workspaceClient . putObject ( operationId , QUERY_FILE , writeToInpustream ( applyAccessContractRestrictionForUnitForUpdate ( queryDsl , getVitamSession ( ) . getContract ( ) ) ) ) ; workspaceClient . putObject ( operationId , "actions.json" , writeToInpustream ( ruleActions ) ) ; OperationContextMonitor . compressInWorkspace ( workspaceClientFactory , operationId , Contexts . MASS_UPDATE_UNIT_RULE . getLogbookTypeProcess ( ) , OperationContextMonitor . OperationContextFileName ) ; processingClient . initVitamProcess ( operationId , Contexts . MASS_UPDATE_UNIT_RULE . name ( ) ) ; RequestResponse < ItemStatus > requestResponse = processingClient . executeOperationProcess ( operationId , Contexts . MASS_UPDATE_UNIT_RULE . name ( ) , RESUME . getValue ( ) ) ; logger . info ( context , "
public void test() { try { mBeanServer . registerMBean ( this , objectName ) ; } catch ( InstanceAlreadyExistsException iaee ) { LOGGER . debug ( "Re-registering Application Service MBean" ) ; LOGGER . debug ( "Re-registered Application Service MBean: " + objectName ) ; mBeanServer . unregisterMBean ( objectName ) ; mBeanServer . registerMBean ( this , objectName ) ; } }
public void test() { try { LOGGER . debug ( "Registering application service MBean under object name: {}" , objectName ) ; mBeanServer . registerMBean ( this , objectName ) ; } catch ( InstanceAlreadyExistsException iaee ) { LOGGER . debug ( "Unable to register application service MBean under object name: {}" , objectName ) ; mBeanServer . unregisterMBean ( objectName ) ; mBeanServer . registerMBean ( this , objectName ) ; } }
public void test() { try { IContentListWidgetHelper helper = ( IContentListWidgetHelper ) ApsWebApplicationUtils . getBean ( JacmsSystemConstants . CONTENT_LIST_HELPER , this . pageContext ) ; List < UserFilterOptionBean > defaultUserFilterOptions = helper . getConfiguredUserFilters ( this , reqCtx ) ; this . addUserFilterOptions ( defaultUserFilterOptions ) ; this . extractExtraWidgetParameters ( reqCtx ) ; code_block = IfStatement ; List < String > contents = this . getContentsId ( helper , reqCtx ) ; this . pageContext . setAttribute ( this . getListName ( ) , contents ) ; } catch ( Throwable t ) { _logger . error ( "error in doEndTag" , t ) ; throw new JspException ( "Error detected while finalising the tag" , t ) ; } }
public String saveContent ( String containerId , String userId , Number taskId , String payload , String marshallingType ) { userId = getUser ( userId ) ; containerId = context . getContainerId ( containerId , new ByTaskIdContainerLocator ( taskId . longValue ( ) ) ) ; Map < String , Object > parameters = marshallerHelper . unmarshal ( containerId , payload , marshallingType , Map . class ) ; logger . debug ( "About to set content of a task with id '{}' with data {}" , taskId , parameters ) ; Long contentId = userTaskService . saveContentFromUser ( taskId . longValue ( ) , userId , parameters ) ; logger . debug ( "About to marshal task content '{}' with content '{}'" , contentId , contentId ) ; String response = marshallerHelper . marshal ( containerId , marshallingType , contentId ) ; return response ; }
public String saveContent ( String containerId , String userId , Number taskId , String payload , String marshallingType ) { userId = getUser ( userId ) ; containerId = context . getContainerId ( containerId , new ByTaskIdContainerLocator ( taskId . longValue ( ) ) ) ; logger . debug ( "About to unmarshal task content parameters from payload: '{}'" , payload ) ; Map < String , Object > parameters = marshallerHelper . unmarshal ( containerId , payload , marshallingType , Map . class ) ; Long contentId = userTaskService . saveContentFromUser ( taskId . longValue ( ) , userId , parameters ) ; logger . debug ( "About to save content '{}' with content '{}'" , contentId , contentId ) ; String response = marshallerHelper . marshal ( containerId , marshallingType , contentId ) ; return response ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isErrorEnabled ( ) ) { log . error ( throwable . getMessage ( ) ) ; } }
public List findByExample ( MYesno instance ) { log . debug ( "finding MYesno instance by example" ) ; code_block = TryStatement ;  }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.MYesno" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.MYesno" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
@ EventListener @ Order ( 5 ) public void onApplicationEvent ( final ContextRefreshedEvent event ) { arrowheadContext . put ( CoreCommonConstants . SERVER_STANDALONE_MODE , true ) ; LOGGER . info ( "Server started" ) ; }
public void test() { try { GoogleUtils . deleteObjectsInPath ( storage , inputDataConfig , accountConfig . getBucket ( ) , accountConfig . getPrefix ( ) , Predicates . alwaysTrue ( ) ) ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; throw new IOException ( e ) ; } }
public void test() { if ( mainWorker == null ) { job . setResult ( null ) ; log . debug ( "Unknown worker" ) ; return ; } }
public void test() { try { future = mainWorker . call ( ) ; } catch ( Exception e ) { log . error ( "Exception in main worker" , e ) ; job . setResult ( null ) ; return ; } }
public void test() { { LOGGER . info ( "Job {} failed" , job . getId ( ) ) ; job . setFailure ( cause ) ; } }
public void test() { if ( displayWarning ) { log . warn ( warnWarning ) ; } }
public void test() { try { final StringWriter writer = new StringWriter ( ) ; templates . newTransformer ( ) . transform ( source , new StreamResult ( writer ) ) ; return writer . toString ( ) ; } catch ( TransformerException e ) { logger . error ( "Failed to apply Schematron validation transform" , e ) ; throw new SchematronValidationException ( "Failed to apply Schematron validation transform" , e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( JSONException e ) { LOGGER . error ( "Exception:" , e ) ; Assert . fail ( "An Exception occurred during testing!:\n" + e . getMessage ( ) ) ; } }
public void test() { try { ret = atlasGraph . indexQuery ( VERTEX_INDEX , indexQuery ) . vertexTotals ( ) ; } catch ( Exception e ) { LOG . error ( "Exception atlasGraphDB" , e ) ; } }
public void createMultiBranchPipeline ( FolderJob folder , String pipelineName , String repositoryPath ) throws IOException { deletePipeline ( folder , pipelineName ) ; URL url = Resources . getResource ( this . getClass ( ) , "multibranch.xml" ) ; jenkins . createJob ( folder , pipelineName , Resources . toString ( url , Charsets . UTF_8 ) . replace ( "{{repo}}" , repositoryPath ) , true ) ; JobWithDetails job = jenkins . getJob ( folder , pipelineName ) ; job . build ( true ) ; LOG . info ( "Job created successfully." ) ; }
public void test() { try { code_block = IfStatement ; } catch ( IOException e ) { LOGGER . error ( "Unable to retrieve file" , e ) ; } }
public void test() { if ( jTable . getSelectedRow ( ) < 0 ) { MessageDialog . error ( null , "angal.common.pleaseselectarow.msg" ) ; } else { selectedrow = jTable . getSelectedRow ( ) ; operationType = ( OperationType ) ( model . getValueAt ( selectedrow , - 1 ) ) ; OperationTypeEdit newrecord = new OperationTypeEdit ( myFrame , operationType , false ) ; newrecord . addOperationTypeListener ( OperationTypeBrowser . this ) ; newrecord . setVisible ( true ) ; } }
public void test() { try { HttpResponse response = client . execute ( request ) ; responseString = IOUtils . toString ( response . getEntity ( ) . getContent ( ) , "utf-8" ) ; } catch ( IOException e ) { logger . error ( e . getMessage ( ) , e ) ; return ; } }
public void test() { try { Map < String , Object > [ ] data = gson . fromJson ( responseString , mapArrayType ) ; double latestVersion = 0 ; code_block = ForStatement ; lastUpdated = System . currentTimeMillis ( ) / 1000L ; } catch ( Exception e ) { logger . warn ( "Failed to extract latest version from database" , e ) ; } }
@ Override public void initialize ( ) { LOG . info ( "Trying to start the Container" ) ; container . start ( ) ; registerProperties ( ) ; LOG . info ( "ZooKeeper instance running at {}" , getConnectionString ( ) ) ; }
@ Override public void initialize ( ) { LOG . info ( "Trying to start the ZooKeeper container" ) ; container . start ( ) ; registerProperties ( ) ; LOG . info ( "Zookeeper instance running at {}" , getServiceAddress ( ) ) ; }
@ Override public void onNunchukRemovedEvent ( NunchukRemovedEvent arg0 ) { log . debug ( "NunchukRemovedEvent: " + arg0 ) ; }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getCapabilities()" ) ; } }
public void test() { if ( logger . isWarnEnabled ( ) ) { logger . warn ( e . getMessage ( ) , e ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getCapabilities()" ) ; } }
public void test() { for ( final CloseEvent closeEvent : _closeEvents ) { _logger . debug ( "Thread name: {}" , closeEvent . threadName ) ; logStack ( closeEvent . stackTrace ) ; i ++ ; } }
public void test() { try { ongoingCalculation . cancel ( true ) ; } catch ( Exception e ) { LOG . warn ( "While stopping ongoingCalculation" , e ) ; } }
public void test() { try { StatementMirror mirror = resultSet . glowroot$getStatementMirror ( ) ; code_block = IfStatement ; QueryEntry lastQueryEntry = mirror . getLastQueryEntry ( ) ; code_block = IfStatement ; lastQueryEntry . setCurrRow ( ( ( ResultSet ) resultSet ) . getRow ( ) ) ; } catch ( Exception e ) { LOGGER . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( logTransactionExceptions ) { log . error ( "Error occurred while rolling back transaction" , e ) ; } }
public void test() { if ( createSolrClientPerRequest ) { logger . debug ( "Returning the creating solr client" ) ; return Solr6Index . instance . createSolrClient ( ) ; } else { logger . debug ( "Returning the solr client owned by Solr6Index." ) ; return Solr6Index . instance . solrClient ; } }
public void test() { if ( createSolrClientPerRequest ) { logger . debug ( "Creating a new Solr Client." ) ; return Solr6Index . instance . createSolrClient ( ) ; } else { logger . debug ( "Creating a new Solr Client." ) ; return Solr6Index . instance . solrClient ; } }
public void test() { if ( Solr6Index . instance != null ) { code_block = IfStatement ; } else { LOGGER . error ( "Solr6 index not initialized" ) ; return null ; } }
@ Override public void error ( final SAXParseException e ) { logger . error ( e . getMessage ( ) , e ) ; }
private void cleanUpCreatedTopics ( final Set < String > topicsToCleanUp ) { log . info ( "Starting cleanup of internal topics {}." , topicsToCleanUp ) ; final long now = time . milliseconds ( ) ; final long deadline = now + retryTimeoutMs ; final Set < String > topicsStillToCleanup = new HashSet < > ( topicsToCleanUp ) ; code_block = WhileStatement ; log . info ( "Completed cleanup of internal topics {}." , topicsToCleanUp ) ; }
public void test() { if ( cause instanceof UnknownTopicOrPartitionException ) { log . warn ( "Cleaning up internal topic {} not found." , topicName ) ; } else-if ( cause instanceof LeaderNotAvailableException ) { log . info ( "The leader of internal topic {} to clean up is not available." , topicName ) ; } else-if ( cause instanceof TimeoutException ) { log . info ( "Cleaning up internal topic {} timed out." , topicName ) ; } else { log . error ( "Unexpected error during cleanup of internal topics: " , cause ) ; throw new StreamsException ( String . format ( "Could not clean up internal topics %s, because during the cleanup " + "of topic %s the following error occurred: " , topicsStillToCleanup , topicName ) , cause ) ; } }
public void test() { if ( cause instanceof UnknownTopicOrPartitionException ) { log . info ( "Internal topic {} to clean up is missing" , topicName ) ; } else-if ( cause instanceof LeaderNotAvailableException ) { log . info ( "Cleaning up internal topic {} failed." , topicName ) ; } else-if ( cause instanceof TimeoutException ) { log . info ( "Cleaning up internal topic {} timed out." , topicName ) ; } else { log . error ( "Unexpected error during cleanup of internal topics: " , cause ) ; throw new StreamsException ( String . format ( "Could not clean up internal topics %s, because during the cleanup " + "of topic %s the following error occurred: " , topicsStillToCleanup , topicName ) , cause ) ; } }
public void test() { if ( cause instanceof UnknownTopicOrPartitionException ) { log . info ( "Internal topic {} to clean up is missing" , topicName ) ; } else-if ( cause instanceof LeaderNotAvailableException ) { log . info ( "The leader of internal topic {} to clean up is not available." , topicName ) ; } else-if ( cause instanceof TimeoutException ) { log . warn ( "Timeout during cleanup of internal topics: " , cause ) ; } else { log . error ( "Unexpected error during cleanup of internal topics: " , cause ) ; throw new StreamsException ( String . format ( "Could not clean up internal topics %s, because during the cleanup " + "of topic %s the following error occurred: " , topicsStillToCleanup , topicName ) , cause ) ; } }
public void test() { try { InputStream dataToValid = new ByteArrayInputStream ( getContent ( ) . getBytes ( "UTF-8" ) ) ; SchemaFactory factory = SchemaFactory . newInstance ( "http://www.w3.org/2001/XMLSchema" ) ; Schema schema = factory . newSchema ( new URL ( schemaURL ) ) ; Source source = new StreamSource ( dataToValid ) ; Validator validator = schema . newValidator ( ) ; validator . validate ( source ) ; return true ; } catch ( SAXException ex ) { throw new DocServiceException ( "File is not valid. " + ex . getMessage ( ) , HttpServletResponse . SC_UNSUPPORTED_MEDIA_TYPE ) ; } catch ( IOException e ) { LOG . info ( "Unable to validate file " + schemaURL , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( response != null ) { Job r = JobParser . parseJob ( response . getEntity ( ) . getContent ( ) ) ; logger . info ( "Computing job {}" , r . getId ( ) ) ; return r ; } }
public void test() { try { tracciatiBD = new TracciatiBD ( configWrapper ) ; tracciatiBD . setupConnection ( configWrapper . getTransactionID ( ) ) ; tracciatiBD . setAtomica ( false ) ; tracciatiBD . setAutoCommit ( false ) ; SerializationConfig config = new SerializationConfig ( ) ; config . setDf ( SimpleDateFormatUtils . newSimpleDateFormatDataOreMinuti ( ) ) ; config . setIgnoreNullValues ( true ) ; IDeserializer deserializer = SerializationFactory . getDeserializer ( SERIALIZATION_TYPE . JSON_JACKSON , config ) ; serializer = SerializationFactory . getSerializer ( SERIALIZATION_TYPE . JSON_JACKSON , config ) ; beanDati = ( it . govpay . core . beans . tracciati . TracciatoPendenza ) deserializer . getObject ( tracciato . getBeanDati ( ) , it . govpay . core . beans . tracciati . TracciatoPendenza . class ) ; code_block = SwitchStatement ; } catch ( Throwable e ) { log . error ( "ERROR" , e ) ; tracciatiBD . rollback ( ) ; tracciato . setStato ( STATO_ELABORAZIONE . SCARTATO ) ; String descrizioneStato = "Errore durante l'elaborazione del tracciato: " + e . getMessage ( ) ; tracciato . setDescrizioneStato ( descrizioneStato . length ( ) > 256 ? descrizioneStato . substring ( 0 , 255 ) : descrizioneStato ) ; tracciato . setDataCompletamento ( new Date ( ) ) ; code_block = IfStatement ; tracciatiBD . updateFineElaborazione ( tracciato ) ; tracciatiBD . commit ( ) ; } finally { code_block = IfStatement ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { if ( ! bindables . isEmpty ( ) || LaunchMode . current ( ) == LaunchMode . DEVELOPMENT ) { beans . produce ( AdditionalBeanBuildItem . unremovableOf ( GrpcContainer . class ) ) ; features . produce ( new FeatureBuildItem ( GRPC_SERVER ) ) ; } else { LOGGER . info ( "GPC server disabled, no bindables available." ) ; } }
public void test() { try { final ContentVersion contentVersion = versioningManager . getVersion ( versionId ) ; final Content content = versioningManager . getContent ( contentVersion ) ; final ContentDto contentDto = contentService . getDtoBuilder ( ) . convert ( content ) ; return contentDto ; } catch ( ApsSystemException e ) { logger . error ( "error loading content {}" , versionId , e ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "table:" + triStateName + " checking to see if sharedTriState " + sharedTriState + " is the same as expected value:" + expectedValue ) ; log . trace ( "table:" + triStateName + " checking to see if sharedTriState " + sharedTriState + " is the same as expected value:" + expectedValue ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "table:" + triStateName + " got " + sharedTriState + " from " + sharedTriStates ) ; log . trace ( "table:" + triStateName + " got " + sharedTriState + " from " + sharedTriStates ) ; } }
public void test() { if ( cacheMap . containsKey ( pid ) ) { TimestampedCacheEntry < DOReader > e = cacheMap . remove ( pid ) ; result = e . value ( ) ; cacheMap . put ( pid , e . refresh ( ) ) ; LOG . debug ( "cache hit for {}" , pid ) ; } else { LOG . debug ( "cache miss for {}" , pid ) ; } }
public void test() { if ( cacheMap . containsKey ( pid ) ) { TimestampedCacheEntry < DOReader > e = cacheMap . remove ( pid ) ; LOG . debug ( "cache hit for {}" , pid ) ; result = e . value ( ) ; cacheMap . put ( pid , e . refresh ( ) ) ; } else { LOG . debug ( "cache miss for {}" , pid ) ; } }
public void test() { if ( ! isSuccess ) { logger . error ( Messages . getInstance ( ) . getString ( "FileOutputHandler.ERROR_0003_FAILED_TO_CREATE_STORE_EXCEPTION" ) ) ; } }
public void test() { try { code_block = IfStatement ; log . info ( "Cassandra Sink Task trying to put()" ) ; code_block = ForStatement ; } catch ( Exception e ) { log . error ( "Cassandra Sink Task failed to put()" , e ) ; } }
public void test() { if ( listeningChannel == null && channelGroup == null ) { LOG . debug ( "Reconnecting communication channel..." ) ; } else { LOG . debug ( "Reconnecting communication channel..." ) ; closeConnection ( ) ; } }
public void test() { if ( listeningChannel == null && channelGroup == null ) { LOG . debug ( "Creating communication channel..." ) ; } else { LOG . debug ( "Creating communication channel..." ) ; closeConnection ( ) ; } }
void reconnect ( ) { code_block = IfStatement ; LOG . debug ( "TSO channel creating" ) ; channelGroup = new DefaultChannelGroup ( TSOChannelHandler . class . getName ( ) ) ; listeningChannel = bootstrap . bind ( new InetSocketAddress ( config . getPort ( ) ) ) ; channelGroup . add ( listeningChannel ) ; LOG . debug ( "\tListening channel created and connected: {}" , listeningChannel ) ; }
void reconnect ( ) { code_block = IfStatement ; channelGroup = new DefaultChannelGroup ( TSOChannelHandler . class . getName ( ) ) ; LOG . debug ( "\tCreating channel to listening for incoming connections in port {}" , config . getPort ( ) ) ; listeningChannel = bootstrap . bind ( new InetSocketAddress ( config . getPort ( ) ) ) ; channelGroup . add ( listeningChannel ) ; LOG . debug ( "\tSuccess to listening for incoming connections in port {}" , config . getPort ( ) ) ; }
protected Key toKey ( String expression , VariableResolver resolver ) { Set < String > variablesUsed = variableCache . get ( ) . computeIfAbsent ( expression , this :: variablesUsed ) ; Map < String , Object > input = new HashMap < > ( ) ; code_block = ForStatement ; Key cacheKey = new Key ( expression , input ) ; log . trace ( "ToKey for expression {}" , expression ) ; return cacheKey ; }
public void test() { if ( apiManagementProviderService == null ) { String msg = "API management provider service has not initialized." ; log . error ( msg ) ; throw new IllegalStateException ( msg ) ; } }
public void test() { try { Session session = sessions . get ( wrapper ) ; session . close ( ) ; } catch ( JMSException ex ) { logger . error ( ex . getMessage ( ) , ex ) ; startReconnectThread ( ) ; } finally { wrapper . stop ( ) ; sessions . remove ( wrapper ) ; topicToWrapper . remove ( subsribedToTag . getTopicName ( ) ) ; } }
public void test() { try { lifecycleErrorEvent . fire ( event ) ; } catch ( Exception ex ) { logger . error ( "Failed to fire event {}" , event , ex ) ; } }
public void test() { if ( authority . getAuthority ( ) . equals ( requiredAuthority ) ) { log . debug ( "ACCESS_GRANTED [" + authority + "]" ) ; return ACCESS_GRANTED ; } }
public void test() { if ( ExceptionUtils . getRootCause ( e ) instanceof CommandTimeoutException ) { logger . error ( "[monitor] sentinel: {} : {}" , sentinel , e . getMessage ( ) ) ; } else { logger . error ( "[monitor] sentinel: {}" , sentinel , e ) ; } }
public void test() { if ( ExceptionUtils . getRootCause ( e ) instanceof CommandTimeoutException ) { logger . error ( "[monitor] sentinel: {} : {}" , sentinel , e . getMessage ( ) ) ; } else { logger . error ( "[monitor] sentinel: {}" , sentinel , e ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getCapabilities()" ) ; } }
public void test() { try { connection . close ( ) ; } catch ( IOException e ) { logger . error ( "Failed to close connection." , e ) ; } }
@ Test public void testScheduleCampaignCloneFail ( ) throws Exception { iprops . campaignAction . setValue ( schedule ) ; iprops . campaignId . setValue ( BATCH_CAMPAIGN ) ; iprops . cloneToProgramName . setValue ( "undx_test_program" ) ; iprops . afterCampaignAction ( ) ; MarketoRecordResult rs = getClient ( iprops ) . scheduleCampaign ( iprops ) ; LOG . debug ( "[testScheduleCampaign] {}" , rs ) ; assertFalse ( rs . isSuccess ( ) ) ; assertEquals ( "{[611] System error}" , rs . getErrorsString ( ) ) ; }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( IOException ioex ) { log . error ( "Error while removing bean definition" , ioex ) ; throw new KunderaException ( ioex ) ; } }
private ScheduledExecutorService createCleanUpExecutor ( ) { final int numCleaningThreads = quotaConfig . getMaxConcurrentCleanUps ( ) ; log . debug ( "Using disk quota cleanup thread." ) ; CustomizableThreadFactory tf = new CustomizableThreadFactory ( "GWC DiskQuota clean up thread-" ) ; tf . setThreadPriority ( 1 + ( Thread . MAX_PRIORITY - Thread . MIN_PRIORITY ) / 5 ) ; ScheduledExecutorService executorService = Executors . newScheduledThreadPool ( numCleaningThreads , tf ) ; return executorService ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isWarnEnabled ( ) ) { logger . warn ( e . getMessage ( ) , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public ScriptingGauge newGauge ( String name , double initialValue ) { ScriptingGauge scriptingGauge = new ScriptingGauge ( name , initialValue ) ; ActivityMetrics . gauge ( scriptContext , name , scriptingGauge ) ; log . info ( scriptContext , name , "Created gauge " + scriptingGauge . toString ( ) ) ; return scriptingGauge ; }
@ Override public int getReactionSchemeCount ( ) { logger . debug ( "Getting reaction scheme count: " , super . getReactionSchemeCount ( ) ) ; return super . getReactionSchemeCount ( ) ; }
public void test() { try { full_graph . newLine ( ) ; code_block = IfStatement ; } catch ( IOException e ) { LOGGER . error ( e ) ; } }
public void test() { if ( log . isInfoEnabled ( ) ) { log . info ( msg ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( event != null ) { FlowCallStack flowCallStack = event . getFlowCallStack ( ) ; code_block = IfStatement ; } else { logger . warn ( "Could not find received event to " + event ) ; } }
@ Transactional ( readOnly = true ) public ClusterCreate getClusterConfig ( String clusterName , boolean needAllocIp ) { ClusterEntity clusterEntity = clusterEntityMgr . findByName ( clusterName ) ; code_block = IfStatement ; ClusterCreate clusterConfig = new ClusterCreate ( ) ; clusterConfig . setName ( clusterEntity . getName ( ) ) ; clusterConfig . setAppManager ( clusterEntity . getAppManager ( ) ) ; clusterConfig . setDistro ( clusterEntity . getDistro ( ) ) ; Map < NetTrafficType , List < ClusterNetConfigInfo > > networkConfigInfo = clusterEntity . getNetworkConfigInfo ( ) ; code_block = IfStatement ; convertClusterConfig ( clusterEntity , clusterConfig , needAllocIp ) ; Gson gson = new GsonBuilder ( ) . excludeFieldsWithoutExposeAnnotation ( ) . create ( ) ; String manifest = gson . toJson ( clusterConfig ) ; LOG . info ( "load cluster config:{}" , manifest ) ; return clusterConfig ; }
public void activate ( ) { logger . info ( "activate()" ) ; }
public void test() { if ( ! resource . canLock ( lockSet . get ( ) ) ) { String errorMessage = getErrorMessage ( resource ) ; LOG . error ( errorMessage ) ; throw new RuntimeException ( errorMessage ) ; } else { lockFn . accept ( resourceName ) ; code_block = IfStatement ; lockSet . set ( resource . setLock ( lockSet . get ( ) ) ) ; return true ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { ProjectService . Iface client = thriftClients . makeProjectClient ( ) ; project = client . getProjectByIdForEdit ( id , user ) ; usingProjects = client . searchLinkingProjects ( id , user ) ; allUsingProjectCount = client . getCountByProjectId ( id ) ; } catch ( TException e ) { log . error ( "Error fetching project from backend!" , e ) ; setSW360SessionError ( request , ErrorMessages . ERROR_GETTING_PROJECT ) ; return ; } }
public void test() { try { putDirectlyLinkedProjectsInRequest ( request , project , user ) ; putDirectlyLinkedReleasesInRequest ( request , project ) ; } catch ( TException e ) { LOGGER . error ( "Could not add resources for project {" + project . getName ( ) + "}" , e ) ; return ; } }
public void test() { try { putDirectlyLinkedProjectsInRequest ( request , project , user ) ; putDirectlyLinkedReleasesInRequest ( request , project ) ; } catch ( TException e ) { LOGGER . error ( "Could not add resources for project {" + project . getName ( ) + "}" , e ) ; } }
public void test() { if ( LOGGER . isWarnEnabled ( ) ) { LOGGER . warn ( "Unable to find parameter {}" , key ) ; } }
@ Post ( path = "admin.api/notices" , subscribeToken = "csrf" ) @ Auth ( roles = "admin" ) public Boundary postNotice ( ) throws InvalidParamException { LOG . debug ( "Entering deleteNotice" ) ; NoticesEntity entity = HttpUtil . parseJson ( getRequest ( ) , NoticesEntity . class ) ; entity = NoticesLogic . get ( ) . insertNotice ( entity ) ; MessageResult result = new MessageResult ( MessageStatus . Success , HttpStatus . SC_200_OK , getResource ( "message.success.insert" ) , entity . getNo ( ) . toString ( ) ) ; return send ( result ) ; }
public void test() { try { instance . join ( TimeUnit . MINUTES . toMillis ( 5 ) ) ; } catch ( final InterruptedException e ) { LOGGER . warn ( "Interrupted" , e ) ; Thread . currentThread ( ) . interrupt ( ) ; } finally { instance = null ; shutdown = null ; } }
public void test() { try { c . close ( ) ; } catch ( final Exception e ) { LOGGER . warn ( "Failed to close filewriter" , e ) ; } }
public void test() { try { System . arraycopy ( loggable . data , 0 , page . getData ( ) , 0 , loggable . size ) ; } catch ( final ArrayIndexOutOfBoundsException e ) { LOG . error ( e ) ; throw e ; } }
public void test() { try { SinglePage page = getSinglePageForRedo ( loggable , loggable . pageNum ) ; code_block = IfStatement ; } catch ( final IOException e ) { LOG . warn ( "An IOException occurred during redo: {}" , e . getMessage ( ) , e ) ; } }
public void test() { try { p . vendorExtensions . put ( "x-codegen-body-example" , mapper . writerWithDefaultPrettyPrinter ( ) . writeValueAsString ( definitions . get ( p . dataType ) . getExample ( ) ) ) ; } catch ( JsonProcessingException e ) { LOGGER . warn ( "Could not write {}." , p . dataType , e ) ; } }
public void test() { try { traits = fromJsonString ( PubAnnotationProviderTraits . class , aDocumentRepository . getProperties ( ) ) ; } catch ( IOException e ) { logger . error ( "Unable to read traits" , e ) ; } }
@ Modified public void modify ( Map < String , Object > properties ) { Object portletName = properties . get ( "javax.portlet.name" ) ; log . debug ( "portletName: " + portletName ) ; }
public void test() { try { mailboxManager . deleteMailbox ( mailboxPath , mailboxSession ) ; } catch ( MailboxNotFoundException e ) { LOGGER . warn ( e . getMessage ( ) ) ; } }
@ Override public void objectInserted ( ObjectInsertedEvent event ) { log . debug ( event ) ; }
public void test() { if ( ActiveMQRALogger . LOGGER . isTraceEnabled ( ) ) { ActiveMQRALogger . LOGGER . trace ( "execute()" ) ; } }
public void delete ( TmpBauSel persistentInstance ) { log . debug ( "deleting TmpBauSel instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . delete ( persistentInstance ) ; log . debug ( "delete successful" ) ; } catch ( RuntimeException re ) { log . error ( "delete failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . delete ( persistentInstance ) ; log . debug ( "delete successful" ) ; } catch ( RuntimeException re ) { log . error ( "delete failed" , re ) ; throw re ; } }
public void test() { try { return mapper . readValue ( json , ClusterCreate . class ) ; } catch ( Exception e ) { logger . error ( "[get]" + json , e ) ; throw e ; } }
public void test() { if ( LOG . isInfoEnabled ( ) ) { LOG . info ( String . format ( "Call to '%s' on file '%s'" , uri . toString ( ) , file ) ) ; } }
public void test() { if ( ! getConfig ( ) . isWritable ( ) ) { LOG . warn ( "Config is writable" ) ; return ; } }
public void test() { if ( isUp ( ) ) { LOG . info ( "Found a deflector alias. Setting one up now." ) ; } else { LOG . info ( "Did not find a deflector alias. Setting one up now." ) ; code_block = TryStatement ;  } }
public void test() { try { final String currentTarget = getNewestIndex ( ) ; LOG . info ( "Pointing to already existing index target <{}>" , currentTarget ) ; pointTo ( currentTarget ) ; } catch ( NoTargetIndexException ex ) { final String msg = "There is no index target to point to. Creating one now." ; LOG . warn ( msg ) ; activityWriter . write ( new Activity ( msg , IndexSet . class ) ) ; cycle ( ) ; } }
public void test() { try { Queue queue = getQueue ( subscriberQueuePath , subscriberQueueId ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Unable to connect to remote queue" , e ) ; } }
public void test() { if ( longRunningProcess == null || longRunningProcess . isDone ( ) ) { code_block = TryStatement ;  } else { getLogger ( ) . info ( "Long running process to long running process" ) ; } }
public void test() { if ( ! isScheduled ( ) ) { getLogger ( ) . info ( "Shutdown process for %s" , pid ) ; longRunningProcess . cancel ( true ) ; return ; } }
public void test() { if ( flowFile . getSize ( ) == 0L ) { session . remove ( flowFile ) ; } else-if ( failure . get ( ) ) { session . remove ( flowFile ) ; getLogger ( ) . error ( "Failed to read flow file {}" , flowFile ) ; } else { flowFile = session . putAttribute ( flowFile , ATTRIBUTE_COMMAND , command ) ; code_block = IfStatement ; session . getProvenanceReporter ( ) . create ( flowFile , "Created from command: " + commandString ) ; getLogger ( ) . info ( "Created {} and routed to success" , new Object [ ] code_block = "" ; ) ; session . transfer ( flowFile , REL_SUCCESS ) ; } }
public void test() { switch ( node . getType ( ) ) { case ACTION : String programName = ( ( WorkflowActionNode ) node ) . getProgram ( ) . getProgramName ( ) ; Resources runnableResources = runnablesResources . get ( programName ) ; code_block = IfStatement ; break ; case FORK : Resources forkResources = ( ( WorkflowForkNode ) node ) . getBranches ( ) . stream ( ) . map ( branches -> findDriverResources ( branches , runnablesResources ) ) . reduce ( this :: mergeForkResources ) . orElse ( resources ) ; resources = maxResources ( resources , forkResources ) ; break ; case CONDITION : Resources branchesResources = maxResources ( findDriverResources ( ( ( WorkflowConditionNode ) node ) . getIfBranch ( ) , runnablesResources ) , findDriverResources ( ( ( WorkflowConditionNode ) node ) . getElseBranch ( ) , runnablesResources ) ) ; resources = maxResources ( resources , branchesResources ) ; break ; default : LOG . debug ( "Unrecognized workflow {} for workflow {}" , node , node . getType ( ) ) ; } }
void processPublish ( MqttPublishMessage msg ) { final MqttQoS qos = msg . fixedHeader ( ) . qosLevel ( ) ; final String username = NettyUtils . userName ( channel ) ; final String topicName = msg . variableHeader ( ) . topicName ( ) ; final String clientId = getClientId ( ) ; final int messageID = msg . variableHeader ( ) . packetId ( ) ; ByteBuf payload = msg . payload ( ) ; final boolean retain = msg . fixedHeader ( ) . isRetain ( ) ; final Topic topic = new Topic ( topicName ) ; code_block = IfStatement ; LOG . debug ( "Received Publish message: {}" , payload ) ; code_block = SwitchStatement ; }
public void test() { if ( ! topic . isValid ( ) ) { LOG . warn ( "Connection is invalid." ) ; dropConnection ( ) ; } }
public void test() { switch ( qos ) { case AT_MOST_ONCE : postOffice . receivedPublishQos0 ( topic , username , clientId , payload , retain , msg ) ; break ; code_block = BranchStatement ; code_block = BranchStatement ; default : logger . debug ( "Ignoring RT {}" , qos ) ; break ; } }
public void test() { if ( resultHandler == null ) { logger . warn ( "No ResultHandler configured for ThingHandler!" ) ; } else { code_block = TryStatement ;  } }
public void test() { if ( anyMatchingFeature ( features , withName ( name ) ) ) { featuresService . uninstallFeature ( name ) ; postUninstalledEvent ( name ) ; logger . info ( "Uninstalled Feature {}" , name ) ; return true ; } }
public void test() { try { Feature [ ] features = featuresService . listInstalledFeatures ( ) ; code_block = IfStatement ; } catch ( Exception e ) { LOG . error ( "Failed to check installed features." , e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CommercePriceListCommerceAccountGroupRelServiceUtil . class , "deleteCommercePriceListAccountGroupRelsByCommercePriceListId" , _deleteCommercePriceListAccountGroupRelsByCommercePriceListIdParameterTypes1 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , commercePriceListId ) ; code_block = TryStatement ;  } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { boolean available = rs . absolute ( ( int ) pos + 1 ) ; code_block = IfStatement ; } catch ( SQLException e ) { log . error ( e . getMessage ( ) , e ) ; } }
@ Override public void onData ( Stream stream , DataFrame frame , Callback callback ) { byte [ ] bytes = new byte [ frame . getData ( ) . remaining ( ) ] ; logger . debug ( "onData: {}" , bytes ) ; frame . getData ( ) . get ( bytes ) ; response = new String ( bytes ) ; doAssert ( response ) ; callback . succeeded ( ) ; }
public void test() { try { String inputsFileContent = SlangSource . fromFile ( inputFile ) . getContent ( ) ; Boolean emptyContent = true ; code_block = IfStatement ; code_block = IfStatement ; } catch ( RuntimeException ex ) { LOG . error ( "Error while loading inputs" , ex ) ; throw new RuntimeException ( ex ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( MDRRuleGroupInstanceServiceUtil . class , "updateRuleGroupInstance" , _updateRuleGroupInstanceParameterTypes5 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , ruleGroupInstanceId , priority ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . mobile . device . rules . model . MDRRuleGroupInstance ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { int affectRow = jdbcTemplate . update ( sql , tenantCapacity . getGmtModified ( ) , tenantCapacity . getTenant ( ) ) ; return affectRow == 1 ; } catch ( CannotGetJdbcConnectionException e ) { FATAL_LOG . error ( "[db-error]" , e ) ; throw e ; } }
public void test() { if ( delete ( eventFile ) ) { logger . info ( "{} Deleted {} event file ({}) due to storage limits" , this , eventFile , FormatUtils . formatDataSize ( fileSize ) ) ; return fileSize ; } else { logger . error ( "{} Deletion failed to delete {} event file" , this , eventFile ) ; continue ; } }
public void test() { try { transport = new TNonblockingServerSocket ( port ) ; LOG . debug ( "succeeded in binding server to port " + port ) ; break ; } catch ( TTransportException e ) { LOG . debug ( "found binding to port " + port ) ; port ++ ; } }
@ Test void testError ( ) { Exception exception = new SLException ( ERROR_MESSAGE ) ; logger . error ( "" , exception ) ; verify ( logger , times ( 1 ) ) . error ( ( Object ) ERROR_MESSAGE , exception ) ; assertEquals ( ERROR_MESSAGE , exception . getMessage ( ) ) ; }
public void test() { if ( ! executorService . stop ( pid , signal ) ) { LOG . warn ( "Failed to stop process with pid: {} and signal: {}" , pid , signal ) ; } }
public void test() { try { String clusterName = ( String ) getRequest ( ) . getAttributes ( ) . get ( "clusterName" ) ; String instanceName = ( String ) getRequest ( ) . getAttributes ( ) . get ( "instanceName" ) ; String resourceGroup = ( String ) getRequest ( ) . getAttributes ( ) . get ( "resourceName" ) ; presentation = getInstanceCurrentStateRepresentation ( clusterName , instanceName , resourceGroup ) ; } catch ( Exception e ) { String error = ClusterRepresentationUtil . getErrorAsJsonStringFromException ( e ) ; presentation = new StringRepresentation ( error , MediaType . APPLICATION_JSON ) ; LOG . error ( "" , e ) ; } }
@ Override public synchronized void deleteCompletedLogs ( ) throws IOException { LOG . info ( "Start delete completed log files." ) ; long logNumber = UfsJournal . FIRST_COMPLETED_LOG_NUMBER ; code_block = WhileStatement ; code_block = ForStatement ; LOG . info ( "Finished deleting all completed log files." ) ; mNextCompleteLogNumber = UfsJournal . FIRST_COMPLETED_LOG_NUMBER ; }
public void test() { for ( long i = logNumber - 1 ; i >= 0 ; i -- ) { URI log = mJournal . getCompletedLog ( i ) ; LOG . info ( "Deleting completed log {}" , log ) ; mUfs . deleteFile ( log . toString ( ) ) ; } }
@ Override public synchronized void deleteCompletedLogs ( ) throws IOException { LOG . info ( "Deleting all completed log files..." ) ; long logNumber = UfsJournal . FIRST_COMPLETED_LOG_NUMBER ; code_block = WhileStatement ; code_block = ForStatement ; LOG . info ( "Deleted all completed log files." ) ; mNextCompleteLogNumber = UfsJournal . FIRST_COMPLETED_LOG_NUMBER ; }
public void test() { try { RegionSubRegionSnapshot subRegionSnapShot = new RegionSubRegionSnapshot ( subRegion ) ; parentSnapShot . addSubRegion ( subRegionSnapShot ) ; Set subRegions = subRegion . subregions ( false ) ; populateRegionSubRegions ( subRegionSnapShot , subRegions , cache ) ; } catch ( Exception e ) { logger . warn ( "Failed to add RegionSnapshot into region " + subRegion , e ) ; } }
public void test() { try { DataBag categReal = ( DataBag ) input . get ( 0 ) ; DataBag categClassif = ( DataBag ) input . get ( 1 ) ; List < String > real = new ArrayList < String > ( ) ; List < String > classif = new ArrayList < String > ( ) ; code_block = ForStatement ; code_block = ForStatement ; int is = intersectSize ( real , classif ) ; double acc = ( double ) is / ( double ) sumSize ( real , classif ) ; double p = ( double ) is / ( double ) classif . size ( ) ; double r = ( double ) is / ( double ) real . size ( ) ; Double f1 = p + r != 0 ? 2 * p * r / ( p + r ) : null ; double hl = sumSize ( subs ( real , classif ) , subs ( classif , real ) ) ; int zol = is == real . size ( ) && is == classif . size ( ) ? 1 : 0 ; Object [ ] obj = new Object [ ] code_block = "" ; ; return TupleFactory . getInstance ( ) . newTuple ( Arrays . asList ( obj ) ) ; } catch ( Exception e ) { logger . error ( "Error in processing input row:" , e ) ; throw new IOException ( "Caught exception processing input row:\n" + StackTraceExtractor . getStackTrace ( e ) ) ; } }
public void test() { if ( LOGGER . isTraceEnabled ( ) ) { LOGGER . trace ( "Weak listener list status:{}" , System . lineSeparator ( ) ) ; } }
public void test() { if ( requestDataWriter == null ) { RecordLog . warn ( "[NettyRequestDataWriter] Cannot create a non-null request data writer" ) ; return ; } }
public void sync ( ) { code_block = IfStatement ; syncInProgress = true ; log . info ( "Starting synchronization" ) ; code_block = TryStatement ;  syncInProgress = false ; log . info ( "Sync completed" ) ; }
public void test() { try { code_block = ForStatement ; List < PinDefinition > list = getPinList ( ) ; code_block = ForStatement ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void sync ( ) { code_block = IfStatement ; syncInProgress = true ; log . warn ( "================================ sync !!! ==============================" ) ; log . warn ( "" ) ; code_block = TryStatement ;  syncInProgress = false ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { documentURI = new URI ( documentURL ) ; } catch ( URISyntaxException e ) { String message = "Document URL Syntax error:" + documentURL ; LOG . error ( message ) ; throw new DataServiceFault ( e , message ) ; } }
public void test() { try ( final Tx tx = app . tx ( ) ) { code_block = WhileStatement ; tx . success ( ) ; } catch ( FrameworkException fex ) { logger . warn ( "" , fex ) ; logger . warn ( ExceptionUtils . getStackTrace ( fex ) ) ; } }
public void test() { try { PartnerConnection partnerConnection = getPartnerConnection ( ) ; ConnectorConfig connectorConfig = partnerConnection . getConfig ( ) ; Map < String , Object > options = new HashMap < > ( ) ; options . put ( ClientTransport . MAX_NETWORK_DELAY_OPTION , _transportTimeout * 6000 ) ; _httpClient . start ( ) ; URL url = new URL ( connectorConfig . getServiceEndpoint ( ) ) ; _bayeuxClient = new BayeuxClient ( StringBundler . concat ( url . getProtocol ( ) , "://" , url . getHost ( ) , "/cometd/37.0" ) , new SalesforceTransport ( connectorConfig . getSessionId ( ) , options , _httpClient ) ) ; ClientSessionChannel handshakeClientSessionChannel = _bayeuxClient . getChannel ( Channel . META_HANDSHAKE ) ; handshakeClientSessionChannel . addListener ( new SalesforceMessageListener ( ) ) ; ClientSessionChannel connectClientSessionChannel = _bayeuxClient . getChannel ( Channel . META_CONNECT ) ; connectClientSessionChannel . addListener ( new SalesforceMessageListener ( ) ) ; ClientSessionChannel subscribeClientSessionChannel = _bayeuxClient . getChannel ( Channel . META_SUBSCRIBE ) ; subscribeClientSessionChannel . addListener ( new SalesforceMessageListener ( ) ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; } }
public void test() { if ( configurationResourceManager != null && StringUtils . isNotEmptyTrimmed ( configResourceUri ) && StringUtils . isNotEmptyTrimmed ( schemaResourceName ) ) { String schema = ResourcesUtils . loadResource ( schemaResourceName , true ) ; configurationResourceManager . registerResource ( configResourceUri , schema ) ; configurationResourceManager . registerObserver ( this , configResourceUri ) ; log . info ( "Registered configuration resource: {}" , schemaResourceName ) ; } else { log . warn ( "No configuration resource manager and/or no configuration resource uri and/or no schema " + "resource name defined. Not using this feature in this case" ) ; } }
public void test() { if ( configurationResourceManager != null && StringUtils . isNotEmptyTrimmed ( configResourceUri ) && StringUtils . isNotEmptyTrimmed ( schemaResourceName ) ) { log . info ( "Register resource and observer for config resource uri {}" , configResourceUri ) ; String schema = ResourcesUtils . loadResource ( schemaResourceName , true ) ; configurationResourceManager . registerResource ( configResourceUri , schema ) ; configurationResourceManager . registerObserver ( this , configResourceUri ) ; } else { log . error ( "Unable to initialize resource " + schemaResourceName ) ; } }
private synchronized void nextGeneration ( ) throws IOException { close ( ) ; processFile = new File ( rootDir , prefix + IN_PROCESS . getFileNameSuffix ( ) ) ; writer = newWriter ( processFile , UTF_8 ) ; LOG . info ( "Created " + processFile . getAbsolutePath ( ) ) ; }
public void test() { try { code_block = ForStatement ; code_block = IfStatement ; } catch ( final Exception e ) { s_logger . error ( "Unable to execute method " + type , e ) ; } }
public void test() { try { bundleResponse . addStatusMessage ( "server started" ) ; bundleResponse . addStatusMessage ( "queueing" ) ; _executorService . execute ( new BuildThread ( bundleRequest , bundleResponse ) ) ; _buildMap . put ( id , bundleResponse ) ; final StringWriter sw = new StringWriter ( ) ; final MappingJsonFactory jsonFactory = new MappingJsonFactory ( ) ; final JsonGenerator jsonGenerator = jsonFactory . createJsonGenerator ( sw ) ; _mapper . writeValue ( jsonGenerator , bundleResponse ) ; response = Response . ok ( sw . toString ( ) ) . build ( ) ; } catch ( Exception any ) { _log . error ( "serialization failed:" , any ) ; response = Response . serverError ( ) . build ( ) ; } }
public void test() { try { List < Future < String > > futures = getRepositoryStatistics ( con ) ; size = getResult ( "repository size." , futures . get ( 0 ) ) ; numContexts = getResult ( "labeled contexts." , futures . get ( 1 ) ) ; } catch ( InterruptedException e ) { log . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( ActiveMQRALogger . LOGGER . isTraceEnabled ( ) ) { ActiveMQRALogger . LOGGER . trace ( "execute()" ) ; } }
public void test() { default : { logger . warn ( "unknown" ) ; return JoyQueueCode . CN_NO_PERMISSION ; } }
public void test() { if ( success ) { code_block = IfStatement ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( JsonProcessingException e ) { log . error ( "Can not convert to json" , e ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public boolean runReboot ( ) { logger . debug ( "runReboot() called." ) ; RunReboot bcp = thisBridge . bridgeAPI ( ) . runReboot ( ) ; code_block = IfStatement ; return false ; }
public void test() { if ( thisBridge . bridgeCommunicate ( bcp ) ) { logger . trace ( "Communication failed" ) ; } }
public void samplePoint ( Integer x , Integer y ) { log . info ( "samplePoint {}" , x ) ; }
public void test() { if ( cpuCoresLong != cpuCoresDouble ) { log . warn ( "puCoresLong != cpuCoresDouble: " + cpuCoresLong + " = " + cpuCoresDouble ) ; } }
public void test() { if ( fromNode . getChild ( 0 ) . getType ( ) == TOK_SUBQUERY ) { log . warn ( "Subqueries in from clause not supported by {} Query : {}" , this , this . query ) ; throw new LensException ( "Subqueries in from clause not supported by " + this + " Query : " + this . query ) ; } else-if ( isOfTypeJoin ( fromNode . getChild ( 0 ) . getType ( ) ) ) { log . warn ( "Join in from clause not supported by {} Query : {}" , this , this . query ) ; throw new LensException ( "Join in from clause not supported by " + this + " Query : " + this . query ) ; } }
public void test() { if ( fromNode . getChild ( 0 ) . getType ( ) == TOK_SUBQUERY ) { log . warn ( "Subqueries in from clause not supported by {} Query : {}" , this , this . query ) ; throw new LensException ( "Subqueries in from clause not supported by " + this + " Query : " + this . query ) ; } else-if ( isOfTypeJoin ( fromNode . getChild ( 0 ) . getType ( ) ) ) { log . warn ( "Join in from clause not supported by {} Query : {}" , this , this . query ) ; throw new LensException ( "Join in from clause not supported by " + this + " Query : " + this . query ) ; } }
public void test() { try { buildDruidQuery ( conf , metastoreConf ) ; rewritternQueryText = rewrittenQuery . toString ( ) ; } catch ( SemanticException e ) { log . error ( "Unable to rewritten query {}" , rewritternQueryText ) ; throw new LensException ( e ) ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( "Closing connection {}" , connection ) ; } }
@ ExceptionHandler ( Exception . class ) public ModelAndView handleError ( HttpServletRequest req , Exception exception ) { logger . error ( exception , exception ) ; ModelAndView mav = new ModelAndView ( ) ; mav . addObject ( "exception" , exception ) ; mav . setViewName ( "00-error" ) ; return mav ; }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { if ( request != null ) { log . debug ( "Received a CommandHostRequest. Ignoring null request" ) ; } else { log . debug ( "This request is not a CommandHostRequest. Ignoring null request" ) ; } }
public void test() { if ( request != null ) { log . debug ( "This request is not a CommandHostRequest. Ignoring request for URI {}" , request . getURI ( ) ) ; } else { log . debug ( "This request is not a CommandHostRequest. Ignoring request for URI {}" , request . getURI ( ) ) ; } }
public void test() { if ( histogram != null ) { return histogram . getValue ( ) . get ( new Resolution ( new double [ ] code_block = "" ; ) ) ; } else { log . warn ( "Histogram is not null" ) ; } }
@ Test public void testSimpleAttributePath2 ( ) throws Exception { AttributePathServiceTest . LOG . debug ( "start simple attribute path test 2" ) ; apstUtils . createAndPersistDefaultObject ( ) ; AttributePathServiceTest . LOG . debug ( "end simple attribute path test 2" ) ; }
@ Test public void testSimpleAttributePath2 ( ) throws Exception { AttributePathServiceTest . LOG . debug ( "start simple attribute path test 2" ) ; apstUtils . createAndPersistDefaultObject ( ) ; AttributePathServiceTest . LOG . debug ( "end simple attribute path test 2" ) ; }
public void test() { if ( ! task . condition ( ) . isPresent ( ) || task . condition ( ) . get ( ) . shouldExecuteTask ( ) ) { logger . info ( "Executing task - " + task . getShortDescription ( ) + " ..." ) ; code_block = TryStatement ;  } else { logger . info ( "NOT Executing task - " + task . getShortDescription ( ) + ". Conditions not met." ) ; } }
public void test() { try { task . doUpgrade ( ) ; } catch ( UpgradeProblem problem ) { log . error ( "Failed upgrading task {} to {}" , task . getName ( ) , problem . getMessage ( ) ) ; } }
public void test() { if ( ! task . condition ( ) . isPresent ( ) || task . condition ( ) . get ( ) . shouldExecuteTask ( ) ) { logger . info ( "Executing task - " + task . getShortDescription ( ) ) ; code_block = TryStatement ;  } else { logger . info ( "Check condition - " + task . getShortDescription ( ) ) ; } }
public void test() { try { outputObject . put ( JsonKeys . updateType . name ( ) , "DeleteModel" ) ; pw . println ( outputObject . toString ( ) ) ; } catch ( JSONException e ) { logger . error ( "Error occured while generating JSON!" ) ; } }
@ Override public void onServoSetSpeed ( ServoControl servo ) { logger . debug ( "onServoSetSpeed {}" , servo ) ; int speed = - 1 ; code_block = IfStatement ; Integer i = getDeviceId ( servo ) ; code_block = IfStatement ; msg . servoSetVelocity ( i , speed ) ; }
public void test() { if ( i == null ) { LOGGER . warn ( "Attempted to add a null input" ) ; return ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( ! file . delete ( ) ) { logger . warn ( "Unable to clean up file: " + file ) ; } }
public void test() { try { System . getProperties ( ) . remove ( identifier ) ; } catch ( SecurityException e ) { LOG . trace ( "IGNORED" , e ) ; } }
public void test() { if ( mapperException == null ) { mapperException = t ; } else { LOGGER . error ( mapperException . getMessage ( ) , t ) ; } }
public void test() { try { reportingAdminServiceStub . copySavedReport ( saved , copy ) ; } catch ( Exception e ) { String msg = "Unable to copy the report" ; log . error ( msg ) ; throw new Exception ( msg , e ) ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( "Closing connection {}" , connection ) ; } }
public void test() { if ( completableFuture != null ) { code_block = IfStatement ; completableFuture . complete ( bytes ) ; } else-if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( "readtableFuture is null" ) ; } }
public void registerException ( final String method , final String path , final int code , final String error ) { final String key = method . toUpperCase ( ) + " " + path ; LOGGER . debug ( "Registering error code {}" , error ) ; this . errors . put ( key , new ContentResponse ( method , path , code , error ) ) ; }
public void test() { try ( Connection connection = this . connection ; PreparedStatement statement = this . statement ) { connection . rollback ( ) ; } catch ( SQLException e ) { log . warn ( "Unable to rollback transaction" , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( name == null ) { logger . error ( "getDoubleProperty(): argument 'name' must be non-null" ) ; return new DoublePropertyImpl ( "" ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Refreshing." ) ; } }
public void test() { if ( grouping . length > 0 && tableConfig . getMinIdleStateRetentionTime ( ) < 0 ) { LOG . warn ( "No state retention interval configured for a query: {}" , grouping ) ; } }
public void test() { if ( LOGGER . isInfoEnabled ( ) ) { LOGGER . info ( "Success!" ) ; } }
private Dataset < Row > doOperation ( final GetDataFrameOfElements operation , final ParquetStore store , final SparkSession spark ) throws OperationException { code_block = IfStatement ; final StructType schema = new SchemaUtils ( store . getSchema ( ) ) . getMergedSparkSchema ( store . getSchema ( ) . getGroups ( ) ) ; final Dataset < Row > dataframe = spark . read ( ) . schema ( schema ) . parquet ( store . getGraphPath ( ) ) ; LOG . trace ( "Read dataframe from {}: {}" , schema , dataframe ) ; return dataframe ; }
public void test() { if ( results . size ( ) > 1 ) { LOGGER . warn ( "Multiple images (" + results . size ( ) + ") found in cache." ) ; } }
public static String getDefaultDomainPath ( CoreSession session ) { String query = "SELECT * FROM Document where ecm:primaryType = 'Domain'" ; DocumentModelList results = session . query ( query ) ; code_block = IfStatement ; code_block = IfStatement ; DocumentModel defaultDomain = results . get ( 0 ) ; String defaultDomainPath = defaultDomain . getPathAsString ( ) ; log . debug ( "Default domain path is " + defaultDomainPath ) ; return defaultDomainPath ; }
public void test() { if ( ! response . getBody ( ) . getObject ( ) . getBoolean ( "success" ) ) { Log . debug ( response . getBody ( ) . toString ( ) ) ; throw new Exception ( response . getBody ( ) . getObject ( ) . getJSONObject ( "data" ) . getString ( "error" ) ) ; } }
@ Override public void open ( FileInputSplit split ) throws IOException { super . open ( split ) ; this . wrapper = InstantiationUtil . instantiate ( avroWrapperTypeClass , AvroBaseValue . class ) ; DatumReader < E > datumReader ; code_block = IfStatement ; SeekableInput in = new FSDataInputStreamWrapper ( stream , ( int ) split . getLength ( ) ) ; dataFileReader = DataFileReader . openReader ( in , datumReader ) ; dataFileReader . sync ( split . getStart ( ) ) ; reuseAvroValue = null ; log . info ( "Created a split: " + split ) ; }
public void test() { try { Futures . transform ( Futures . catchingAsync ( future , Throwable . class , Futures :: immediateFailedFuture , MoreExecutors . directExecutor ( ) ) , flowCapNodeOpt code_block = LoopStatement ; , MoreExecutors . directExecutor ( ) ) . get ( ) ; } catch ( InterruptedException | ExecutionException ex ) { LOG . warn ( "FlowCapacityManager failed" , ex ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Failed to clean up." , e ) ; } }
public void test() { try { raf . seek ( start ) ; raf . readFully ( data ) ; } catch ( IOException e ) { LOG . error ( "IOException in aborting processing" , e ) ; abort ( ) ; } }
public void test() { if ( failures > 0 ) { LOG . info ( "Failed to perform '{}' index" , failures ) ; } }
protected void activate ( ComponentContext componentContext , Map < String , Object > properties ) { LOG . info ( "Activating SSLManager Service" ) ; this . options = new SslManagerServiceOptions ( properties ) ; this . sslContexts = new ConcurrentHashMap < > ( ) ; ServiceTracker < SslServiceListener , SslServiceListener > listenersTracker = new ServiceTracker < > ( componentContext . getBundleContext ( ) , SslServiceListener . class , null ) ; this . sslServiceListeners = new SslServiceListeners ( listenersTracker ) ; }
@ PreAuthorize ( "hasPermission(id, '" + AclClassName . Values . LAYOUT + "', '" + PermissionName . Values . LAYOUT_DELETE + "')" ) public void deleteLayout ( final Long id ) { log . debug ( "deleteLayout() - id: " + id ) ; layoutService . delete ( id ) ; }
public void test() { try { bean . unsetEntityContext ( ) ; } catch ( final Exception e ) { logger . warn ( "Can't unsetEntityContext" , e ) ; } finally { callContext . setCurrentOperation ( currentOp ) ; } }
public void test() { try { code_block = IfStatement ; super . messageReceived ( ctx , e ) ; } catch ( Exception ex ) { LOGGER . warn ( "Error sending messageReceived" , ex ) ; } }
public void test() { try { execution . setMessage ( doExecute ( dryRun , executor , context ) ) ; execution . setStatus ( TaskJob . Status . SUCCESS . name ( ) ) ; result = AuditElements . Result . SUCCESS ; } catch ( JobExecutionException e ) { LOGGER . error ( "Job {} failed" , jobName , e ) ; result = AuditElements . Result . FAILURE ; execution . setMessage ( ExceptionUtils2 . getFullStackTrace ( e ) ) ; execution . setStatus ( TaskJob . Status . FAILURE . name ( ) ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( final InterruptedException e ) { LOGGER . error ( "Interrupted while waiting for selection." , e ) ; } }
public void test() { try { final Rule rule = new Rule ( stream , streamRule , stream . getMatchingType ( ) ) ; match . addRule ( rule ) ; } catch ( InvalidStreamRuleTypeException e ) { LOGGER . warn ( e . getMessage ( ) ) ; } }
public void test() { if ( key instanceof ConfigKeySelfExtracting ) { code_block = IfStatement ; } else { LOGGER . warn ( "Key {} is not a ConfigKeySelfExtracting, ignoring" , key ) ; return Maybe . absent ( ) ; } }
public void test() { if ( ! result ) { _logger . error ( StringBundler . concat ( "Remove user " , user . getUsername ( ) , " failed to " , cause ) ) ; } }
public void test() { try { logFile = new File ( indexRecord . filePath ) ; String fileName = logFile . getName ( ) ; archiveFile = new File ( archiveFolder , fileName ) ; logger . info ( "Moving logFile " + logFile + " to " + archiveFile ) ; boolean result = logFile . renameTo ( archiveFile ) ; code_block = IfStatement ; } catch ( Throwable t ) { logger . error ( "Error moving log file to archive folder. logFile=" + logFile + ", archiveFile=" + archiveFile , t ) ; } }
public void test() { if ( ! ret ) { LOGGER . debug ( "Sender {} doesn't match a value." , channelUID ) ; } }
public void test() { try { File [ ] logFiles = archiveFolder . listFiles ( new FileFilter ( ) code_block = "" ; ) ; code_block = IfStatement ; } catch ( Throwable t ) { logger . error ( "Error deleting archive file" , t ) ; } }
private void enableAZ ( BaragonAgentMetadata agent , String availabilityZone , LoadBalancerDescription elb ) { List < String > availabilityZones = elb . getAvailabilityZones ( ) ; availabilityZones . add ( availabilityZone ) ; EnableAvailabilityZonesForLoadBalancerRequest request = new EnableAvailabilityZonesForLoadBalancerRequest ( ) ; request . setAvailabilityZones ( availabilityZones ) ; request . setLoadBalancerName ( elb . getLoadBalancerName ( ) ) ; logger . debug ( "Enable HA for {}" , agent ) ; elbClient . enableAvailabilityZonesForLoadBalancer ( request ) ; }
public void test() { try { log . debug ( "handle message - for operational environment notification received: {}" , notification ) ; Gson gsonObj = new GsonBuilder ( ) . create ( ) ; IDmaapNotificationData notificationData = gsonObj . fromJson ( notification , DmaapNotificationDataImpl . class ) ; IDmaapAuditNotificationData auditNotificationData = gsonObj . fromJson ( notification , DmaapNotificationDataImpl . class ) ; AuditingActionEnum actionEnum ; code_block = SwitchStatement ; componentUtils . auditEnvironmentEngine ( actionEnum , notificationData . getOperationalEnvironmentId ( ) , notificationData . getOperationalEnvironmentType ( ) . getEventTypenName ( ) , notificationData . getAction ( ) . getActionName ( ) , auditNotificationData . getOperationalEnvironmentName ( ) , auditNotificationData . getTenantContext ( ) ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; } catch ( Exception e ) { log . error ( "handle message - for operational environment notification failed" , e ) ; errorWrapper . setInnerElement ( false ) ; } }
protected void generateUDTPojos ( SchemaDefinition schema ) { log . info ( "Starting to generateUDT POJOs" ) ; code_block = ForStatement ; watch . splitInfo ( "UDT POJOs generated" ) ; }
public void test() { try { generateUDTPojo ( udt ) ; } catch ( Exception e ) { LOG . error ( "Error while generating UDTojo for UDT {}" , udt , e ) ; } }
public void test() { try { URI srcUri = new URI ( src ) , destUri = new URI ( dest ) ; Configuration config = new Configuration ( ) ; config . set ( "fs.default.name" , srcUri . resolve ( "/" ) . toString ( ) ) ; FileSystem dfs = FileSystem . get ( config ) ; Path destPath = new Path ( destUri . toString ( ) ) ; FileStatus [ ] files = dfs . listStatus ( new Path ( srcUri . toString ( ) ) ) ; if ( files == null || files . length == 0 ) return false ; code_block = ForStatement ; return true ; } catch ( Exception e ) { LOG . error ( "Failed to delete file: " + dest , e ) ; return false ; } }
public void test() { if ( args . size ( ) != 2 ) { logger . error ( "Unexpected arguments: {}" , args ) ; return ; } }
public void test() { if ( args . size ( ) != 2 ) { logger . error ( "Unexpected arguments: {}" , args ) ; return ; } }
public void test() { if ( commandName . equals ( "setup-admin-user" ) ) { code_block = IfStatement ; command = Tools :: setupAdminUser ; } else-if ( commandName . equals ( "truncate-all-data" ) ) { code_block = IfStatement ; command = Tools :: truncateAllData ; } else-if ( commandName . equals ( "execute-range-deletes" ) ) { code_block = IfStatement ; String partialTableName = args . get ( 0 ) ; code_block = IfStatement ; command = Tools :: executeDeletes ; } else { logger . warn ( "Unknown command name '" + commandName + "'" ) ; return ; } }
static void runCommand ( String commandName , List < String > args ) throws Exception { Directories directories = new Directories ( getCentralDir ( ) ) ; initLogging ( directories . getConfDir ( ) , directories . getLogDir ( ) ) ; Command command ; code_block = IfStatement ; String version = Version . getVersion ( CentralModule . class ) ; startupLogger . info ( "Version: {}" , version ) ; startupLogger . info ( "Java version: {}" , StandardSystemProperty . JAVA_VERSION . value ( ) ) ; CentralConfiguration centralConfig = getCentralConfiguration ( directories . getConfDir ( ) ) ; Session session = null ; Cluster cluster = null ; boolean success ; code_block = TryStatement ;  code_block = IfStatement ; }
public void test() { if ( initialSchemaVersion == null ) { startupLogger . info ( "running a version of glowroot central schema version (expecting glowroot central schema version)" ) ; } else-if ( initialSchemaVersion != schemaUpgrade . getCurrentSchemaVersion ( ) ) { startupLogger . warn ( "running a version of glowroot central that does not match the" + " glowroot central schema version (expecting glowroot central schema version (expecting glowroot central schema" + " but found version {}), exiting" , schemaUpgrade . getCurrentSchemaVersion ( ) , initialSchemaVersion ) ; return ; } }
public void test() { if ( initialSchemaVersion == null ) { startupLogger . info ( "creating glowroot central schema..." ) ; } else-if ( initialSchemaVersion != schemaUpgrade . getCurrentSchemaVersion ( ) ) { startupLogger . info ( "skipping glowroot central schema version {}" , initialSchemaVersion ) ; return ; } }
public void test() { if ( _log . isWarnEnabled ( ) ) { _log . warn ( StringBundler . concat ( "Unable to remove token from " , file . getName ( ) , " to " , path ) ) ; } }
@ Override protected void hibernateMigrate ( ) throws DataMigrationException , XWikiException { logger . info ( "Start of WatchlistLeftoversCleaner on wiki [{}]." , context . getWikiId ( ) ) ; XWikiContext context = this . getXWikiContext ( ) ; code_block = TryStatement ;  logger . info ( "End of WatchlistLeftoversCleaner on wiki [{}]." , context . getWikiId ( ) ) ; }
public void test() { if ( mWorkerConnectWaitStartTimeMs . compareAndSet ( startTime , null , true , false ) ) { LOG . info ( "Worker connect waiting start time." ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { TestAction action = this . action . build ( ) ; setActiveAction ( action ) ; action . execute ( context ) ; } catch ( Exception e ) { log . error ( "Exception occurred" , e ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "Assert exception validation successful: All values OK" ) ; return ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { TestAction action = this . action . build ( ) ; setActiveAction ( action ) ; action . execute ( context ) ; } catch ( Exception e ) { log . error ( e . getMessage ( ) ) ; log . debug ( "Validating caught exception ..." ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; return ; } }
public void test() { for ( String cache : pools ) { code_block = IfStatement ; } }
public void test() { try { connectionListener . setIdentityStoreConfiguration ( configuration ) ; } catch ( RuntimeException e ) { log . warn ( "Exception encountered setting identity store configuration" , e ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { securityHeader = APIUtil . getOAuthConfigurationFromAPIMConfig ( APIConstants . AUTHORIZATION_HEADER ) ; code_block = IfStatement ; } catch ( APIManagementException e ) { LOG . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( KaleoProcessServiceUtil . class , "deleteKaleoProcess" , _deleteKaleoProcessParameterTypes1 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , kaleoProcessId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . portal . workflow . kaleo . forms . model . KaleoProcess ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { code_block = IfStatement ; } catch ( Throwable t ) { logger . error ( t . getMessage ( ) , t ) ; } }
@ Override public void handleNewSession ( final String sessionId ) throws Exception { zkClient . waitUntilConnected ( HelixZkClient . DEFAULT_CONNECTION_TIMEOUT , TimeUnit . SECONDS ) ; waitNewSession . countDown ( ) ; LOG . info ( "Handled session " + sessionId ) ; }
public void test() { try { return Response . ok ( "" + service . getEventCount ( ) ) . build ( ) ; } catch ( UnauthorizedException e ) { throw e ; } catch ( Exception e ) { logger . error ( "Unable to retrieve event count" , e ) ; throw new WebApplicationException ( Response . Status . INTERNAL_SERVER_ERROR ) ; } }
public void test() { try { factory . setProperty ( XMLConstants . ACCESS_EXTERNAL_DTD , "" ) ; } catch ( SAXException e ) { LOG . log ( Level . SEVERE , e . getMessage ( ) , e ) ; } }
public void test() { try { factory . setProperty ( XMLConstants . ACCESS_EXTERNAL_SCHEMA , "" ) ; } catch ( SAXException e ) { LOGGER . log ( Level . SEVERE , e . getMessage ( ) , e ) ; } }
@ VisibleForTesting void setAsyncRequestInstanceSecondStep ( SuccessfulAssociateIpAddressResponse response , AsyncRequestInstanceState asyncRequestInstanceState , String createFirewallRuleJobId ) { LOGGER . info ( "setAsyncRequestInstanceSecondStep started." ) ; SuccessfulAssociateIpAddressResponse . IpAddress ipAddress = response . getIpAddress ( ) ; String ipAddressId = ipAddress . getId ( ) ; String ip = ipAddress . getIpAddress ( ) ; asyncRequestInstanceState . setIpInstanceId ( ipAddressId ) ; asyncRequestInstanceState . setIp ( ip ) ; asyncRequestInstanceState . setCurrentJobId ( createFirewallRuleJobId ) ; asyncRequestInstanceState . setState ( AsyncRequestInstanceState . StateType . CREATING_FIREWALL_RULE ) ; }
public void test() { try { DiscoveryResult discoveryResult = DiscoveryResultBuilder . create ( thingUID ) . withBridge ( accountUID ) . withLabel ( PropertyUtils . getPropertyValue ( device , "common.attributes.name.value" , String . class ) ) . withProperty ( "id" , device . id ) . withProperty ( "type" , device . deviceType ) . withRepresentationProperty ( "id" ) . build ( ) ; thingDiscovered ( discoveryResult ) ; } catch ( GardenaException ex ) { logger . warn ( "{}" , ex . getMessage ( ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
@ Override public void complete ( ) throws IOException { LOG . info ( "Deleting file {}" , file ) ; delete ( ) ; }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceApplicationModelServiceUtil . class , "getCommerceApplicationModel" , _getCommerceApplicationModelParameterTypes2 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , commerceApplicationModelId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . commerce . application . model . CommerceApplicationModel ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { notebook . reloadAllNotes ( context . getAutheInfo ( ) ) ; } catch ( IOException e ) { logger . warn ( "fail to reload all notes" , e ) ; } }
public void test() { if ( includeSamples == null ) { logger . info ( "Execute leaf-node '{}' for null samples" , node ) ; } else { logger . info ( "Execute leaf-node '{}' for {} samples" , node , includeSamples . size ( ) ) ; } }
public void test() { if ( includeSamples == null ) { logger . info ( "Execute leaf-node '{}'" , node ) ; } else { logger . info ( "Execute leaf node '{}'" , node ) ; } }
public void test() { try { return Optional . ofNullable ( pulsar . getTopicPoliciesService ( ) . getTopicPolicies ( topicName ) ) . map ( TopicPolicies :: getBackLogQuotaMap ) . map ( map -> map . get ( BacklogQuotaType . destination_storage . name ( ) ) ) . orElseGet ( ( ) -> getBacklogQuota ( topicName . getNamespace ( ) , policyPath ) ) ; } catch ( Exception e ) { log . warn ( "[{}] Failed to get backlog quota for topic {}" , clientAppId ( ) , topicName , e ) ; } }
public void test() { try { return getObjectMapper ( ) . readValue ( json , classOfT ) ; } catch ( IOException ex ) { log . error ( "Exception during deserialization of object {}" , json , ex ) ; return null ; } }
private void runIteration ( Random rng , Data data , int m , int nbtrees ) { Data train = data . clone ( ) ; Data test = train . rsplit ( rng , ( int ) ( data . size ( ) * 0.1 ) ) ; log . info ( "Iteration: {}" , data ) ; DefaultTreeBuilder treeBuilder = new DefaultTreeBuilder ( ) ; SequentialBuilder forestBuilder = new SequentialBuilder ( rng , treeBuilder , train ) ; treeBuilder . setM ( m ) ; long time = System . currentTimeMillis ( ) ; log . info ( "Growing a forest with m={}" , m ) ; DecisionForest forestM = forestBuilder . build ( nbtrees ) ; sumTimeM += System . currentTimeMillis ( ) - time ; numNodesM += forestM . nbNodes ( ) ; treeBuilder . setM ( 1 ) ; time = System . currentTimeMillis ( ) ; log . info ( "Growing a forest with m=1" ) ; DecisionForest forestOne = forestBuilder . build ( nbtrees ) ; sumTimeOne += System . currentTimeMillis ( ) - time ; numNodesOne += forestOne . nbNodes ( ) ; double [ ] testLabels = test . extractLabels ( ) ; double [ ] predictions = new double [ test . size ( ) ] ; forestM . classify ( test , predictions ) ; sumTestErrM += ErrorEstimate . errorRate ( testLabels , predictions ) ; forestOne . classify ( test , predictions ) ; sumTestErrOne += ErrorEstimate . errorRate ( testLabels , predictions ) ; }
private void runIteration ( Random rng , Data data , int m , int nbtrees ) { log . info ( "Splitting the data" ) ; Data train = data . clone ( ) ; Data test = train . rsplit ( rng , ( int ) ( data . size ( ) * 0.1 ) ) ; DefaultTreeBuilder treeBuilder = new DefaultTreeBuilder ( ) ; SequentialBuilder forestBuilder = new SequentialBuilder ( rng , treeBuilder , train ) ; treeBuilder . setM ( m ) ; long time = System . currentTimeMillis ( ) ; DecisionForest forestM = forestBuilder . build ( nbtrees ) ; sumTimeM += System . currentTimeMillis ( ) - time ; numNodesM += forestM . nbNodes ( ) ; treeBuilder . setM ( 1 ) ; time = System . currentTimeMillis ( ) ; log . info ( "Growing a forest with m=1" ) ; DecisionForest forestOne = forestBuilder . build ( nbtrees ) ; sumTimeOne += System . currentTimeMillis ( ) - time ; numNodesOne += forestOne . nbNodes ( ) ; log . info ( "Growing a forest with m=1" ) ; double [ ] testLabels = test . extractLabels ( ) ; double [ ] predictions = new double [ test . size ( ) ] ; forestM . classify ( test , predictions ) ; sumTestErrM += ErrorEstimate . errorRate ( testLabels , predictions ) ; forestOne . classify ( test , predictions ) ; sumTestErrOne += ErrorEstimate . errorRate ( testLabels , predictions ) ; }
private void runIteration ( Random rng , Data data , int m , int nbtrees ) { log . info ( "Splitting the data" ) ; Data train = data . clone ( ) ; Data test = train . rsplit ( rng , ( int ) ( data . size ( ) * 0.1 ) ) ; DefaultTreeBuilder treeBuilder = new DefaultTreeBuilder ( ) ; SequentialBuilder forestBuilder = new SequentialBuilder ( rng , treeBuilder , train ) ; treeBuilder . setM ( m ) ; long time = System . currentTimeMillis ( ) ; log . info ( "Growing a forest with m={}" , m ) ; DecisionForest forestM = forestBuilder . build ( nbtrees ) ; sumTimeM += System . currentTimeMillis ( ) - time ; numNodesM += forestM . nbNodes ( ) ; treeBuilder . setM ( 1 ) ; time = System . currentTimeMillis ( ) ; DecisionForest forestOne = forestBuilder . build ( nbtrees ) ; sumTimeOne += System . currentTimeMillis ( ) - time ; numNodesOne += forestOne . nbNodes ( ) ; log . info ( "Growing a tree" ) ; double [ ] testLabels = test . extractLabels ( ) ; double [ ] predictions = new double [ test . size ( ) ] ; forestM . classify ( test , predictions ) ; sumTestErrM += ErrorEstimate . errorRate ( testLabels , predictions ) ; forestOne . classify ( test , predictions ) ; sumTestErrOne += ErrorEstimate . errorRate ( testLabels , predictions ) ; }
@ Test public void testJmxDumpCBRRoutesAsXml ( ) throws Exception { MBeanServer mbeanServer = getMBeanServer ( ) ; ObjectName on = getContextObjectName ( ) ; String xml = ( String ) mbeanServer . invoke ( on , "dumpRoutesAsXml" , null , null ) ; assertNotNull ( xml ) ; log . info ( xml ) ; assertTrue ( xml . contains ( "myRoute" ) , xml ) ; assertTrue ( xml . matches ( "[\\S\\s]*<when id=\"when[0-9]+\">[\\S\\s]*" ) ) ; assertTrue ( xml . matches ( "[\\S\\s]*<otherwise id=\"otherwise[0-9]+\">[\\S\\s]*" ) ) ; assertTrue ( xml . contains ( "<route customId=\"true\" id=\"myRoute\">" ) || xml . contains ( "<route id=\"myRoute\" customId=\"true\">" ) ) ; assertTrue ( xml . contains ( "<Choice customId=\"true\" id=\"myChoice\">" ) || xml . contains ( "<Choice id=\"myChoice\" customId=\"true\">" ) ) ; }
public void test() { if ( response . getStatusCode ( ) == HttpStatus . SC_OK ) { robotsTxt = parseRobotsTxt ( doc . getInputStream ( ) , trimmedURL , response . getUserAgent ( ) ) ; LOG . debug ( "Response from {}" , response . getStatusCode ( ) ) ; } else { LOG . info ( "No robots.txt found for {}. ({} - {})" , robotsURL , response . getStatusCode ( ) , response . getReasonPhrase ( ) ) ; robotsTxt = new RobotsTxt ( ) ; } }
public void test() { if ( response . getStatusCode ( ) == HttpStatus . SC_OK ) { robotsTxt = parseRobotsTxt ( doc . getInputStream ( ) , trimmedURL , response . getUserAgent ( ) ) ; LOG . debug ( "Fetched and parsed robots.txt: {}" , robotsURL ) ; } else { LOG . error ( "Failed to fetch robots.txt" , response . getStatusCode ( ) ) ; robotsTxt = new RobotsTxt ( ) ; } }
public void test() { try { CrawlDoc doc = new CrawlDoc ( new HttpDocInfo ( robotsURL ) , fetcher . getStreamFactory ( ) . newInputStream ( ) ) ; IHttpFetchResponse response = fetcher . fetch ( doc , HttpMethod . GET ) ; String redirURL = response . getRedirectTarget ( ) ; code_block = IfStatement ; code_block = IfStatement ; } catch ( Exception e ) { log . warn ( "Unable to fetch robots URL" , e ) ; robotsTxt = new RobotsTxt ( ) ; } }
@ Override public void afterDestroy ( EntryEvent < K , V > event ) { log . debug ( "afterDestroy: {}" , event ) ; }
public void test() { if ( prediction . isEmpty ( ) ) { log . error ( "Could not find relation by id " + target . getId ( ) ) ; aTarget . getPage ( ) . error ( "Could not find relation" ) ; aTarget . addChildren ( aTarget . getPage ( ) , IFeedback . class ) ; return ; } }
public void test() { if ( sample != null ) { Tag noteId = Tag . of ( "nodeid" , note . getId ( ) ) ; Tag name = Tag . of ( "name" , StringUtils . defaultString ( note . getName ( ) , "unknown" ) ) ; Tag statusTag = Tag . of ( "result" , result ) ; sample . stop ( Metrics . timer ( "cronjob" , Tags . of ( noteId , name , statusTag ) ) ) ; } else { LOG . warn ( "ignoring sample note: {}" , note ) ; } }
private void startTestFamework ( ) throws Exception { log . debug ( "Starting TEST FRAMEWORK" ) ; resetTestListener ( testListener ) ; resetClockToStartOfTest ( clock ) ; startBusAndRegisterListener ( busService , testListener ) ; restartSubscriptionService ( subscriptionBaseService ) ; restartEntitlementService ( entitlementService ) ; log . debug ( "STARTED TEST FRAMEWORK" ) ; }
private void startTestFamework ( ) throws Exception { log . debug ( "STARTING TEST FRAMEWORK" ) ; resetTestListener ( testListener ) ; resetClockToStartOfTest ( clock ) ; startBusAndRegisterListener ( busService , testListener ) ; restartSubscriptionService ( subscriptionBaseService ) ; restartEntitlementService ( entitlementService ) ; log . debug ( "FINISHED TEST FRAMEWORK" ) ; }
public void test() { try { if ( ! receiptEndDate . isEmpty ( ) && remittanceDate != null && remittanceDate . before ( dateFomatter . parse ( receiptEndDate ) ) ) addActionError ( getText ( "bankremittance.before.receiptdate" ) ) ; } catch ( final ParseException e ) { LOGGER . error ( "Exception while parsing receiptEndDate date" , e ) ; throw new ApplicationRuntimeException ( "Exception while parsing receiptEndDate date" , e ) ; } }
public void test() { try { java . util . List < com . liferay . commerce . wish . list . model . CommerceWishListItem > returnValue = CommerceWishListItemServiceUtil . getCommerceWishListItems ( commerceWishListId , start , end , orderByComparator ) ; return com . liferay . commerce . wish . list . model . CommerceWishListItemSoap . toSoapModels ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( ! stopped . get ( ) ) { LOG . fatal ( "yarn container launch event handler is interrupted. " , e ) ; context . getEventHandler ( ) . handle ( new InternalErrorEvent ( context . getApplicationId ( ) , "yarn container launch event handler is interrupted. " + e . getMessage ( ) ) ) ; } }
public void test() { if ( poolSize < idealPoolSize ) { int newPoolSize = Math . min ( limitOnPoolSize , idealPoolSize + INITIAL_POOL_SIZE ) ; LOG . info ( "Setting ContainerLauncher pool size to " + newPoolSize + " as number-of-nodes to talk to is " + numNodes ) ; launcherPool . setCorePoolSize ( newPoolSize ) ; } }
public void test() { { ResourcesResource . LOG . debug ( "try to apply configuration for resource with uuid '{}'" , uuid ) ; ResourcesResource . LOG . debug ( "try to recieve resource with uuid '{}' for csv json configuration preview" , uuid ) ; final Optional < Resource > resourceOptional = dataModelUtil . fetchResource ( uuid ) ; code_block = IfStatement ; final Resource resource = resourceOptional . get ( ) ; ResourcesResource . LOG . debug ( "try to recieved resource with uuid '{}' for csv json configuration preview " , uuid ) ; code_block = IfStatement ; ResourcesResource . LOG . debug ( "try to apply configuration to resource with uuid '{}'" , uuid ) ; final String result = applyConfigurationForCSVJSONPreview ( resource , jsonObjectString ) ; code_block = IfStatement ; ResourcesResource . LOG . debug ( "applied configuration to resource with uuid '{}'" , uuid ) ; return buildResponse ( result ) ; } }
public void test() { if ( ResourcesResource . LOG . isTraceEnabled ( ) ) { ResourcesResource . LOG . trace ( "= '{}'" , ToStringBuilder . reflectionToString ( resource ) ) ; } }
public void test() { { ResourcesResource . LOG . debug ( "try to apply configuration for resource with uuid '{}'" , uuid ) ; ResourcesResource . LOG . debug ( "try to recieve resource with uuid '{}' for csv json configuration preview" , uuid ) ; final Optional < Resource > resourceOptional = dataModelUtil . fetchResource ( uuid ) ; code_block = IfStatement ; final Resource resource = resourceOptional . get ( ) ; ResourcesResource . LOG . debug ( "found resource with uuid '{}' for csv json configuration preview " , uuid ) ; code_block = IfStatement ; ResourcesResource . LOG . debug ( "try to apply configuration to resource with uuid '{}'" , uuid ) ; final String result = applyConfigurationForCSVJSONPreview ( resource , jsonObjectString ) ; code_block = IfStatement ; ResourcesResource . LOG . debug ( "applied configuration to resource with uuid '{}'" , uuid ) ; return buildResponse ( result ) ; } }
public void test() { { ResourcesResource . LOG . debug ( "try to apply configuration for resource with uuid '{}'" , uuid ) ; ResourcesResource . LOG . debug ( "try to recieve resource with uuid '{}' for csv json configuration preview" , uuid ) ; final Optional < Resource > resourceOptional = dataModelUtil . fetchResource ( uuid ) ; code_block = IfStatement ; final Resource resource = resourceOptional . get ( ) ; ResourcesResource . LOG . debug ( "found resource with uuid '{}' for csv json configuration preview " , uuid ) ; code_block = IfStatement ; ResourcesResource . LOG . debug ( "try to apply configuration to resource with uuid '{}'" , uuid ) ; final String result = applyConfigurationForCSVJSONPreview ( resource , jsonObjectString ) ; ResourcesResource . LOG . debug ( "applied configuration to resource with uuid '{}'" , uuid ) ; code_block = IfStatement ; return buildResponse ( result ) ; } }
public void test() { if ( element == null ) { log . warn ( "No element with name '{}' found!" , name ) ; return ; } }
@ Before public void setUp ( ) throws Exception { SUT = new Payload2TcpProtocol ( ) ; channelHandlerContextMock = mock ( ChannelHandlerContext . class , RETURNS_DEEP_STUBS ) ; byte [ ] bytes = amsTCPPacket . getBytes ( ) ; LOGGER . debug ( " amsTCPPacket: " + amsTCPPacket ) ; }
private void publishRegisterURI ( final List < URIRegisterDTO > registerDTOList ) { logger . debug ( "Publishing register URI successfully." ) ; publisher . publish ( registerDTOList ) ; }
@ Override public Future < Void > close ( final SpanContext spanContext ) { log . trace ( "close()" ) ; mappingAndDelegatingCommandConsumerFactory . removeClient ( tenantId ) ; consumerLinkTenants . remove ( tenantId ) ; final Promise < Void > result = Promise . promise ( ) ; connection . closeAndFree ( receiver , receiverClosed -> result . complete ( ) ) ; return result . future ( ) ; }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Refreshing." ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Refreshing." ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( ( hostPool != null ) && ( hostPool . freeConnections . size ( ) > 0 ) ) { connection = ( HttpConnectionWithReference ) hostPool . freeConnections . removeLast ( ) ; freeConnections . remove ( connection ) ; storeReferenceToConnection ( connection , hostConfiguration , this ) ; code_block = IfStatement ; idleConnectionHandler . remove ( connection ) ; } else-if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "No free connection found for host: {}" , hostConfiguration ) ; } }
@ Override public List < User > findUsersLike ( final String userName , final boolean caseInsensitive ) throws JargonException { log . info ( "findUsersLike({})" , userName ) ; code_block = IfStatement ; log . info ( "case insensitive?:{}" , caseInsensitive ) ; IRODSGenQueryBuilder builder = new IRODSGenQueryBuilder ( true , caseInsensitive , null ) ; StringBuilder userQuery = new StringBuilder ( ) ; userQuery . append ( userName . trim ( ) ) ; userQuery . append ( "%" ) ; IRODSQueryResultSet resultSet = null ; code_block = TryStatement ;  List < User > users = new ArrayList < User > ( ) ; User user ; code_block = ForStatement ; return users ; }
@ Override public List < User > findUsersLike ( final String userName , final boolean caseInsensitive ) throws JargonException { log . info ( "findUserNameLike {}" , userName ) ; code_block = IfStatement ; log . info ( "findUserNameLike {}" , userName ) ; IRODSGenQueryBuilder builder = new IRODSGenQueryBuilder ( true , caseInsensitive , null ) ; StringBuilder userQuery = new StringBuilder ( ) ; userQuery . append ( userName . trim ( ) ) ; userQuery . append ( "%" ) ; IRODSQueryResultSet resultSet = null ; code_block = TryStatement ;  List < User > users = new ArrayList < User > ( ) ; User user ; code_block = ForStatement ; return users ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
protected CleanupJobState jobReachedTerminalState ( ExecutionGraphInfo executionGraphInfo ) { final ArchivedExecutionGraph archivedExecutionGraph = executionGraphInfo . getArchivedExecutionGraph ( ) ; Preconditions . checkArgument ( archivedExecutionGraph . getState ( ) . isTerminalState ( ) , "Job %s is in state %s which is not terminal." , archivedExecutionGraph . getJobID ( ) , archivedExecutionGraph . getState ( ) ) ; archiveExecutionGraph ( executionGraphInfo ) ; LOG . info ( "Job {} completed." , archivedExecutionGraph . getState ( ) ) ; return archivedExecutionGraph . getState ( ) . isGloballyTerminalState ( ) ? CleanupJobState . GLOBAL : CleanupJobState . LOCAL ; }
public void test() { try { UserDefinedFileAttributeView view = getAttributeView ( file ) ; view . write ( ATTRIBUTE_PAGE_TYPE , Charset . defaultCharset ( ) . encode ( pageType ) ) ; } catch ( Exception ex ) { LOG . error ( "Unable to write view" , ex ) ; } }
public void test() { if ( calibrator instanceof PolynomialCalibrator ) { doc . writeStartElement ( "PolynomialCalibrator" ) ; double [ ] coefficients = ( ( PolynomialCalibrator ) calibrator ) . getCoefficients ( ) ; code_block = ForStatement ; doc . writeEndElement ( ) ; } else-if ( calibrator instanceof SplineCalibrator ) { doc . writeStartElement ( "SplineCalibrator" ) ; code_block = ForStatement ; doc . writeEndElement ( ) ; } else-if ( calibrator instanceof MathOperationCalibrator ) { doc . writeStartElement ( "MathOperationCalibrator" ) ; writeMathOperation ( doc , ( MathOperationCalibrator ) calibrator ) ; doc . writeEndElement ( ) ; } else { logger . warn ( "Unsupported calibrator {}" , calibrator ) ; } }
public void test() { try { final URI broadcastUri = nic . getBroadcastUri ( ) ; final String vlanId = BroadcastDomainType . getValue ( broadcastUri ) ; final int ethDeviceNum = getVmNics ( domrName , vlanId ) ; code_block = IfStatement ; } catch ( final Exception e ) { final String msg = "Prepare SetupGuestNetwork failed due to " + e . toString ( ) ; s_logger . error ( msg , e ) ; return new ExecutionResult ( false , msg ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { code_block = WhileStatement ; } catch ( InterruptedException e ) { logger . warn ( e . getMessage ( ) , e ) ; return ; } finally { lock . unlock ( ) ; } }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; ObjectInputStream ois ; ois = new ObjectInputStream ( new ByteArrayInputStream ( bytes ) ) ; Object o = ois . readObject ( ) ; ois . close ( ) ; return o ; } catch ( IOException e ) { log . error ( "IOException, Caused by {}." , e ) ; throw new PropertyAccessException ( e ) ; } catch ( ClassNotFoundException e ) { log . error ( "Class not found exception, Caused by {}." , e ) ; throw new PropertyAccessException ( e ) ; } }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; ObjectInputStream ois ; ois = new ObjectInputStream ( new ByteArrayInputStream ( bytes ) ) ; Object o = ois . readObject ( ) ; ois . close ( ) ; return o ; } catch ( IOException e ) { log . error ( "IO exception, Caused by {}." , e ) ; throw new PropertyAccessException ( e ) ; } catch ( ClassNotFoundException e ) { log . error ( "ClassNotFoundException, Caused by {}." , e ) ; throw new PropertyAccessException ( e ) ; } }
public void test() { try { mappings = this . getApiCatalogManager ( ) . getRelatedWidgetMethods ( ) ; } catch ( Throwable t ) { _logger . error ( "error in getRelatedWidgetMethods" , t ) ; } }
public void test() { if ( simpleUpdateReference != null ) { String subject = change . getIdentifiable ( ) . getId ( ) ; String value = simpleUpdateReference . value ( change ) ; TripleStoreChangeParams updateParams = new TripleStoreChangeParams ( simpleUpdateReference , value ) ; TripleStoreChange tschange = new TripleStoreChange ( "update" , subject , updateParams ) ; return Collections . singletonList ( tschange ) ; } else-if ( ignoredAttributes . contains ( change . getAttribute ( ) ) ) { LOG . debug ( "Ignoring attribute {}" , change . getAttribute ( ) ) ; return Collections . emptyList ( ) ; } else { return Collections . emptyList ( ) ; } }
public void test() { try { originalSgMembers = PublicAccessAutoFix . getSgListForClassicElbResource ( clientMap , resourceId ) ; code_block = ForStatement ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; throw new AutoFixException ( "backup failed" ) ; } }
@ Override public boolean backupExistingConfigForResource ( final String resourceId , final String resourceType , Map < String , Object > clientMap , Map < String , String > ruleParams , Map < String , String > issue ) throws AutoFixException { StringBuilder oldConfig = new StringBuilder ( ) ; List < String > originalSgMembers ; code_block = TryStatement ;  DETACHED_SG = oldConfig . toString ( ) ; backupOldConfig ( resourceId , EXISTING_GROUPS , oldConfig . toString ( ) ) ; log . info ( "Successfully backup existing config for resource : " + resourceId ) ; return true ; }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { doc = ( DocumentImpl ) broker . getXMLResource ( XmldbURI . create ( rs . getString ( "DOCUMENT_URI" ) ) ) ; } catch ( PermissionDeniedException e ) { LOG . debug ( e . getMessage ( ) , e ) ; result [ index ++ ] = null ; continue ; } }
public void test() { try { clientStartLatch . await ( ) ; Thread . sleep ( 10 ) ; client = true ; Ignite cl = startGrid ( "client0" ) ; IgniteCache < Object , Object > atomicCache = cl . cache ( CACHE_NAME_PREFIX + '0' ) ; IgniteCache < Object , Object > txCache = cl . cache ( CACHE_NAME_PREFIX + '1' ) ; assertEquals ( state == ACTIVE ? 100 : 0 , atomicCache . size ( ) ) ; assertEquals ( state == ACTIVE ? 100 : 0 , txCache . size ( ) ) ; } catch ( Exception e ) { log . error ( "Error occurred in client thread." , e ) ; fail ( "Error occurred in client thread. Msg: " + e . getMessage ( ) ) ; } }
public void test() { if ( Log . isDebugEnabled ( ) ) { Log . debug ( "User does not want to leave unsaved page. Nothing to do." ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { connectionStatusCache = null ; log . error ( "Error while disconnecting connection" , e ) ; } }
public static Map < String , Object > shipData ( Map < String , String > params ) { String jobName = params . get ( "jobName" ) ; code_block = IfStatement ; List < Map < String , String > > errorList = new ArrayList < > ( ) ; code_block = TryStatement ;  errorList . addAll ( new RecommendationCollector ( ) . uploadRecommendationData ( ) ) ; Map < String , Object > status = ErrorManageUtil . formErrorCode ( jobName , errorList ) ; LOGGER . info ( "Task {} finished with status {}" , jobName , status ) ; return status ; }
public void test() { try { Response response = ElasticSearchManager . invokeAPI ( "GET" , endPoint , payLoad ) ; String responseJson = EntityUtils . toString ( response . getEntity ( ) ) ; JsonParser jsonParser = new JsonParser ( ) ; JsonObject resultJson = ( JsonObject ) jsonParser . parse ( responseJson ) ; JsonObject hitsJson = ( JsonObject ) jsonParser . parse ( resultJson . get ( "hits" ) . toString ( ) ) ; JsonArray jsonArray = hitsJson . getAsJsonObject ( ) . get ( "hits" ) . getAsJsonArray ( ) ; code_block = ForStatement ; } catch ( Exception e ) { LOGGER . error ( "Error in fetchData" , e ) ; } }
public void test() { if ( logsController . isPodRunning ( podName ) ) { LOG . info ( "End of Log stream for terminated pod: {}" , podName ) ; } else { LOG . info ( "End of Log stream for terminated pod: {}" , podName ) ; } }
public void test() { if ( logsController . isPodRunning ( podName ) ) { LOG . info ( "End of Log stream for running pod: {}" , podName ) ; } else { LOG . info ( "End of Log stream for pod: {} is not running" , podName ) ; } }
@ Transition ( to = "MASTER" , from = "SLAVE" ) public void onBecomeMasterFromSlave ( Message message , NotificationContext context ) { String partitionName = message . getPartitionName ( ) ; String instanceName = message . getTgtName ( ) ; LOG . info ( "Become Master from slave" ) ; }
public User find ( final String username ) { log . debug ( "find() - username: {}" , username ) ; return userRepository . findByUsername ( username ) ; }
@ Override protected Connection createListenerContainer ( ) throws Exception { LOG . trace ( "Creating consumer" ) ; Connection conn = endpoint . connect ( executorService ) ; LOG . trace ( "Creating channel" ) ; Channel channel = conn . createChannel ( ) ; code_block = IfStatement ; DeclareOk result = channel . queueDeclare ( ) ; LOG . debug ( "Using temporary queue name: {}" , result . getQueue ( ) ) ; setReplyTo ( result . getQueue ( ) ) ; channel . queueBind ( getReplyTo ( ) , endpoint . getExchangeName ( ) , getReplyTo ( ) ) ; code_block = IfStatement ; consumer = new RabbitConsumer ( this , channel ) ; consumer . start ( ) ; return conn ; }
@ Override protected Connection createListenerContainer ( ) throws Exception { LOG . trace ( "Creating connection" ) ; Connection conn = endpoint . connect ( executorService ) ; LOG . trace ( "Creating connection" ) ; Channel channel = conn . createChannel ( ) ; code_block = IfStatement ; DeclareOk result = channel . queueDeclare ( ) ; LOG . debug ( "Using temporary queue name: {}" , result . getQueue ( ) ) ; setReplyTo ( result . getQueue ( ) ) ; channel . queueBind ( getReplyTo ( ) , endpoint . getExchangeName ( ) , getReplyTo ( ) ) ; code_block = IfStatement ; consumer = new RabbitConsumer ( this , channel ) ; consumer . start ( ) ; return conn ; }
@ Override protected Connection createListenerContainer ( ) throws Exception { LOG . trace ( "Creating connection" ) ; Connection conn = endpoint . connect ( executorService ) ; LOG . trace ( "Creating channel" ) ; Channel channel = conn . createChannel ( ) ; code_block = IfStatement ; DeclareOk result = channel . queueDeclare ( ) ; setReplyTo ( result . getQueue ( ) ) ; channel . queueBind ( getReplyTo ( ) , endpoint . getExchangeName ( ) , getReplyTo ( ) ) ; code_block = IfStatement ; consumer = new RabbitConsumer ( this , channel ) ; consumer . start ( ) ; LOG . trace ( "Created consumer" ) ; return conn ; }
public void test() { try { channel . queueBind ( newName , endpoint . getExchangeName ( ) , newName ) ; channel . queueUnbind ( newName , endpoint . getExchangeName ( ) , oldName ) ; } catch ( IOException e ) { LOG . warn ( "Failed to queue Unbind {}" , newName , e ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { try { DataAvailabilityEventFilter filter = ( DataAvailabilityEventFilter ) Class . forName ( filterClassName ) . newInstance ( ) ; filters . add ( filter ) ; } catch ( ClassNotFoundException | InstantiationException | IllegalAccessException e ) { log . error ( "Failed to initialize trigger event filter." , e ) ; throw new IllegalArgumentException ( "Failed to initialize trigger event filter." , e . getCause ( ) ) ; } }
public void test() { if ( coordinatedStack != null && coordinatedStack . getRunnerCount ( ) == runnerCount ) { LOG . info ( "Stack {} is already registered" , stack . getName ( ) ) ; code_block = IfStatement ; } else-if ( coordinatedStack != null && coordinatedStack . getRunnerCount ( ) != runnerCount ) { LOG . info ( "List {} is already registered" , checked . getName ( ) ) ; registeredStacks . remove ( coordinatedStack . hashCode ( ) ) ; } }
public CoordinatedStack setupStack ( Stack stack , User user , Commit commit , Module module , int runnerCount ) { CoordinatedStack coordinatedStack = getCoordinatedStack ( stack , user , commit , module ) ; code_block = IfStatement ; coordinateStack = new CoordinatedStack ( stack , user , commit , module , runnerCount ) ; LOG . info ( "Starting setup stack thread of {}..." , stack . getName ( ) ) ; synchronized ( coordinatedStack ) code_block = "" ; LOG . info ( "Completed setup stack thread of {}." , stack . getName ( ) ) ; return coordinatedStack ; }
public void test() { if ( eventFolder == null ) { return ; } }
public void test() { try { callback . accept ( p ) ; new EpisimEventsReader ( manager ) . readFile ( p . toString ( ) ) ; } catch ( UncheckedIOException e ) { LOG . warn ( "Problems processing {}" , p . toString ( ) , e ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { List < String > ancestors = SolrUtils . getAncestors ( aip . getParentId ( ) , model ) ; indexAIP ( aip , ancestors ) . addTo ( ret ) ; code_block = IfStatement ; } catch ( RequestNotValidException | GenericException | AuthorizationDeniedException e ) { LOGGER . error ( "Error indexing AIPs" , e ) ; ret . add ( e ) ; } }
public void test() { if ( BRIDGE_THING_TYPE . equals ( thingTypeUID ) ) { return new BoxHandler ( ( Bridge ) thing , httpClient , commandDescriptionProvider ) ; } else-if ( PL546E_STANDALONE_THING_TYPE . equals ( thingTypeUID ) ) { return new Powerline546EHandler ( ( Bridge ) thing , httpClient , commandDescriptionProvider ) ; } else-if ( SUPPORTED_BUTTON_THING_TYPES_UIDS . contains ( thingTypeUID ) ) { return new AVMFritzButtonHandler ( thing ) ; } else-if ( SUPPORTED_HEATING_THING_TYPES . contains ( thingTypeUID ) ) { return new AVMFritzHeatingDeviceHandler ( thing ) ; } else-if ( SUPPORTED_DEVICE_THING_TYPES_UIDS . contains ( thingTypeUID ) ) { return new DeviceHandler ( thing ) ; } else-if ( GROUP_HEATING_THING_TYPE . equals ( thingTypeUID ) ) { return new AVMFritzHeatingGroupHandler ( thing ) ; } else-if ( SUPPORTED_GROUP_THING_TYPES_UIDS . contains ( thingTypeUID ) ) { return new GroupHandler ( thing ) ; } else { logger . debug ( "Unknown thing type: {}" , thingTypeUID ) ; } }
@ Transactional private void deleteRecovery ( ShardRecovery recovery ) { int shardRecoveryDeleted = shardRecoveryDao . hardDeleteShardRecovery ( recovery . getShardRecoveryId ( ) ) ; Shard shard = shardDao . getLastShard ( ) ; Objects . requireNonNull ( shard , "Shard record should exist!" ) ; shardDao . hardDeleteShard ( shard . getShardId ( ) ) ; LOG . info ( "Shard recovery deleted: " + shard . getShardId ( ) ) ; }
public void test() { try { TransactionManager tm = TransactionHelper . lookupTransactionManager ( ) ; code_block = IfStatement ; } catch ( NamingException | IllegalStateException | SystemException | RollbackException e ) { logger . log ( Level . SEVERE , e . getMessage ( ) , e ) ; return false ; } }
private String moduleTokens ( RoutingContext ctx ) { String modPermJson = ctx . request ( ) . getHeader ( XOkapiHeaders . MODULE_PERMISSIONS ) ; logger . debug ( "test-auth: moduleTokens: trying to decode '{}'" , modPermJson ) ; HashMap < String , String > tokens = new HashMap < > ( ) ; code_block = IfStatement ; code_block = IfStatement ; String alltokens = Json . encode ( tokens ) ; logger . debug ( "test-auth: moduleTokens: success" ) ; return alltokens ; }
public void test() { try { int returnValue = CalendarResourceServiceUtil . searchCount ( companyId , groupIds , classNameIds , keywords , active ) ; return returnValue ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getCapabilities()" ) ; } }
public void warmCaches ( ) { evictCaches ( ) ; logger . info ( "Warming up bill cache..." ) ; Optional < Range < SessionYear > > sessionRange = activeSessionRange ( ) ; code_block = IfStatement ; logger . info ( "Done warming up bill cache." ) ; }
public void test() { if ( sessionYear . equals ( SessionYear . current ( ) ) ) { logger . info ( "Caching Bill ID instances for session year: {}" , sessionYear ) ; getBillIds ( sessionYear , LimitOffset . ALL ) . forEach ( id -> getBill ( id ) ) ; } else { logger . info ( "Caching Bill Info instances for session year: {}" , sessionYear ) ; getBillIds ( sessionYear , LimitOffset . ALL ) . forEach ( this :: getBillInfo ) ; } }
public void test() { if ( sessionYear . equals ( SessionYear . current ( ) ) ) { logger . info ( "Caching Bill instances for current session year: {}" , sessionYear ) ; getBillIds ( sessionYear , LimitOffset . ALL ) . forEach ( id -> getBill ( id ) ) ; } else { logger . info ( "Caching Bill instances for current session year: {}" , sessionYear ) ; getBillIds ( sessionYear , LimitOffset . ALL ) . forEach ( this :: getBillInfo ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { AccountID payerID = FileServiceTest . getRandomPayerAccount ( ) ; AccountID nodeID = FileServiceTest . getRandomNodeAccount ( ) ; log . info ( LOG_PREFIX + "Getting FileID Test: fileID = " + fileID ) ; FileID fid = FileID . newBuilder ( ) . setFileNum ( 1 ) . setRealmNum ( 1 ) . setShardNum ( 0 ) . build ( ) ; getFileInfo ( fid , payerID , nodeID ) ; } catch ( Throwable e ) { log . info ( LOG_PREFIX + "Invalid FileID Test: passed! Caught expected exception = " + e ) ; } }
public void test() { try { AccountID payerID = FileServiceTest . getRandomPayerAccount ( ) ; AccountID nodeID = FileServiceTest . getRandomNodeAccount ( ) ; log . info ( LOG_PREFIX + "Create file: creating a file with 1024 bytes ..." ) ; FileID fid = FileID . newBuilder ( ) . setFileNum ( 1 ) . setRealmNum ( 1 ) . setShardNum ( 0 ) . build ( ) ; getFileInfo ( fid , payerID , nodeID ) ; } catch ( Throwable e ) { log . error ( "Create file: exception!" , e ) ; } }
public void test() { try { client . admin ( ) . indices ( ) . create ( new CreateIndexRequest ( indexName ) . mapping ( indexType , jsonMapping ) ) . actionGet ( ) ; indexCache . add ( indexName ) ; } catch ( IndexAlreadyExistsException e ) { LOGGER . trace ( "Index already exists." ) ; } }
public void test() { if ( ! success ) { code_block = TryStatement ;  } }
public void test() { try { disconnect ( ) ; } catch ( ConnectionException e ) { LOGGER . warn ( "Unable to Disconnect..." , e ) ; } }
public void test() { if ( this . connectionManager . isPresent ( ) && this . connectionManager . get ( ) == manager ) { logger . debug ( "Unrecoverable failure, forcing disconnect" , ex ) ; code_block = TryStatement ;  } else { logger . warn ( "Unable to recover connection" , ex ) ; } }
public void test() { if ( LOGGER . isInfoEnabled ( ) ) { LOGGER . info ( "Success!" ) ; } }
public void test() { try { MetadataExportParams exportParams = new MetadataExportParams ( ) ; code_block = IfStatement ; os = new ByteArrayOutputStream ( 1024 ) ; RootNode metadata = metadataExportService . getMetadataAsNode ( exportParams ) ; nodeService . serialize ( metadata , "application/json" , os ) ; } catch ( Exception ex ) { String message = "Exception occurred while exporting metadata for capturing a metadata version" + ex . getMessage ( ) ; log . error ( message , ex ) ; throw new MetadataVersionServiceException ( message , ex ) ; } }
@ Test public void testParseMinutePrecisionWithoutTimezone ( ) { DateParam param = new DateParam ( ) ; param . setValueAsString ( "2016-06-09T20:38" ) ; assertNull ( param . getPrefix ( ) ) ; assertEquals ( "2016-06-09T20:38" , param . getValueAsString ( ) ) ; ourLog . debug ( "PRE:  " + param . getValue ( ) . getTime ( ) ) ; ourLog . debug ( "PRE:  " + param . getValue ( ) . getTime ( ) ) ; InstantDt dt = new InstantDt ( new Date ( param . getValue ( ) . getTime ( ) ) ) ; dt . setTimeZone ( TimeZone . getTimeZone ( "America/Toronto" ) ) ; ourLog . debug ( "POST: " + dt . getValue ( ) ) ; assertThat ( dt . getValueAsString ( ) , startsWith ( "2016-06-09T" ) ) ; assertThat ( dt . getValueAsString ( ) , endsWith ( "8:00.000-04:00" ) ) ; }
@ Test public void testParseMinutePrecisionWithoutTimezone ( ) { DateParam param = new DateParam ( ) ; param . setValueAsString ( "2016-06-09T20:38" ) ; assertNull ( param . getPrefix ( ) ) ; assertEquals ( "2016-06-09T20:38" , param . getValueAsString ( ) ) ; ourLog . debug ( "PRE:  " + param . getValue ( ) ) ; ourLog . debug ( "PRE:  " + param . getValue ( ) ) ; InstantDt dt = new InstantDt ( new Date ( param . getValue ( ) . getTime ( ) ) ) ; dt . setTimeZone ( TimeZone . getTimeZone ( "America/Toronto" ) ) ; ourLog . debug ( "POST: " + dt . getValue ( ) ) ; assertThat ( dt . getValueAsString ( ) , startsWith ( "2016-06-09T" ) ) ; assertThat ( dt . getValueAsString ( ) , endsWith ( "8:00.000-04:00" ) ) ; }
@ Test public void testParseMinutePrecisionWithoutTimezone ( ) { DateParam param = new DateParam ( ) ; param . setValueAsString ( "2016-06-09T20:38" ) ; assertNull ( param . getPrefix ( ) ) ; assertEquals ( "2016-06-09T20:38" , param . getValueAsString ( ) ) ; ourLog . debug ( "PRE:  " + param . getValue ( ) ) ; ourLog . debug ( "PRE:  " + param . getValue ( ) . getTime ( ) ) ; InstantDt dt = new InstantDt ( new Date ( param . getValue ( ) . getTime ( ) ) ) ; dt . setTimeZone ( TimeZone . getTimeZone ( "America/Toronto" ) ) ; ourLog . debug ( "PRE:  " + dt . getValue ( ) . getTime ( ) ) ; assertThat ( dt . getValueAsString ( ) , startsWith ( "2016-06-09T" ) ) ; assertThat ( dt . getValueAsString ( ) , endsWith ( "8:00.000-04:00" ) ) ; }
@ Override public void addCompositePhenomenonForProcedure ( String procedure , Collection < String > compositePhenomenon ) { CacheValidation . notNullOrEmpty ( PROCEDURE , procedure ) ; CacheValidation . noNullOrEmptyValues ( COMPOSITE_PHENOMENON , compositePhenomenon ) ; LOG . trace ( "Adding composite phenomenon:{}" , procedure ) ; this . compositePhenomenonsForProcedure . computeIfAbsent ( procedure , createSynchronizedSet ( ) ) . addAll ( compositePhenomenon ) ; addCompositePhenomenon ( compositePhenomenon ) ; }
private void handleEventBusException ( Throwable exception , SubscriberExceptionContext context ) { LocalDateTime occurred = LocalDateTime . now ( ) ; String summary = "Event Bus Exception within " + context . getSubscriberMethod ( ) + " at " + occurred + " - " + ExceptionUtils . getStackFrames ( exception ) [ 0 ] ; String message = "\nThe following exception occurred during event handling within " + context . getSubscriberMethod ( ) + " at " + occurred + ":\n" + ExceptionUtils . getStackTrace ( exception ) ; LOG . info ( message ) ; Notification notification = new Notification ( EVENT_BUS_EXCEPTION , occurred , summary , message ) ; eventBus ( ) . post ( notification ) ; }
public void attachClean ( BrAttribut instance ) { log . debug ( "attaching clean BrAttribut instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . lock ( instance , LockMode . NONE ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
@ Activate public void activate ( ) { resourcePath = "./target" ; logger . info ( "Activating " + resourcePath ) ; }
public void test() { try { code_block = WhileStatement ; } catch ( Throwable error ) { LOGGER . log ( Level . SEVERE , "Unable to read from disk" , error ) ; } }
public void test() { try { java . util . List < com . liferay . commerce . bom . model . CommerceBOMDefinition > returnValue = CommerceBOMDefinitionServiceUtil . getCommerceBOMDefinitions ( commerceBOMFolderId , start , end ) ; return com . liferay . commerce . bom . model . CommerceBOMDefinitionSoap . toSoapModels ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( matchingServiceName != null ) { String serviceGroup [ ] = code_block = "" ; ; log . info ( "Service group name " + serviceGroup [ 0 ] ) ; serviceAdminStub . deleteServiceGroups ( serviceGroup ) ; } else { log . info ( "ServiceName is null" ) ; } }
public void test() { try { return LanguageUtil . get ( locale , "home" ) + " - " + group . getDescriptiveName ( locale ) ; } catch ( PortalException portalException ) { _log . error ( portalException , portalException ) ; return LanguageUtil . get ( locale , "home" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { startTransaction ( ) ; writeStressorLastOperation ( ) ; ongoingTx . commit ( ) ; lastConfirmedOperation = operationId ; } catch ( Exception e ) { LOG . warn ( "Failed to commit operation" , e ) ; } finally { clearTransaction ( ) ; } }
public void test() { if ( ie != null ) { throw ie ; } else-if ( e . getClass ( ) . getName ( ) . contains ( "SuspectException" ) ) { log . info ( "Cache operation crashed" ) ; } else { log . error ( "Cache operation error" , e ) ; } }
public void test() { if ( ie != null ) { throw ie ; } else-if ( e . getClass ( ) . getName ( ) . contains ( "SuspectException" ) ) { log . error ( "Request failed due to SuspectException: " + e . getMessage ( ) ) ; } else { log . error ( "Request failed" , e ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { code_block = IfStatement ; return ProcessingStatus . OK ; } catch ( Exception e ) { LOG . error ( "Caught exception while processing vacuum" , e ) ; return ProcessingStatus . DROP ; } }
@ Test public void testQueryWebPageQueryEmptyResults ( ) throws Exception { log . info ( "test method: testQueryWebPageQueryEmptyResults" ) ; DataStoreTestUtil . testQueryWebPageEmptyResults ( webPageStore ) ; }
private void print ( PipelineInfo < RESPONSE > pipeline ) { log . debug ( "" ) ; code_block = ForStatement ; log . debug ( "</pipeline>" ) ; }
@ Test public void test_01_dup ( ) { Log . debug ( "Test" ) ; String genome = "testHg19Chr17" ; String vcf = path ( "hgvs_dup.vcf" ) ; String args [ ] = code_block = "" ; ; SnpEffCmdEff snpeff = new SnpEffCmdEff ( ) ; snpeff . parseArgs ( args ) ; snpeff . setDebug ( debug ) ; snpeff . setVerbose ( verbose ) ; snpeff . setSupressOutput ( ! verbose ) ; snpeff . setFormatVersion ( EffFormatVersion . FORMAT_EFF_4 ) ; snpeff . setUpDownStreamLength ( 0 ) ; List < VcfEntry > results = snpeff . run ( true ) ; code_block = ForStatement ; }
public void test() { try { store . close ( ) ; } catch ( Exception e ) { LOG . debug ( "failed to close " , e ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getCapabilities()" ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getCapabilities()" ) ; } }
public void test() { try { tx . checkErrorCondition ( ) ; JournalInternalRecord updateRecordTX = new JournalAddRecordTX ( false , txID , id , recordType , persister , record ) ; JournalFile usedFile = appendRecord ( updateRecordTX , false , false , tx , null ) ; code_block = IfStatement ; tx . addPositive ( usedFile , id , updateRecordTX . getEncodeSize ( ) ) ; } catch ( Throwable e ) { logger . error ( "appendUpdateRecordTransactional:" + e . getMessage ( ) , e ) ; setErrorCondition ( null , tx , e ) ; } finally { journalLock . readLock ( ) . unlock ( ) ; } }
public void test() { try { String id = this . id ; code_block = IfStatement ; finalID = Integer . parseInt ( id ) ; String volume = this . volume ; code_block = IfStatement ; finalVolume = Integer . parseInt ( volume ) ; } catch ( NumberFormatException e ) { logger . warn ( "Cannot parse volume ({})" , volume , e ) ; } }
private void warmUp ( ) throws IOException { LOG . info ( "Warming up data server..." ) ; List < Thread > threads = new ArrayList < Thread > ( ) ; code_block = ForStatement ; HankTimer timer = new HankTimer ( ) ; code_block = ForStatement ; code_block = ForStatement ; long warmupDurationMs = timer . getDurationMs ( ) ; LOG . info ( "Warming up data server took {} ms." , warmupDurationMs ) ; }
public void test() { try { AbstractTableConfigHelperTest . TestAbstractTableConfigHelperImpl uut = new AbstractTableConfigHelperTest . TestAbstractTableConfigHelperImpl ( ) ; Assert . assertNotNull ( "AbstractTableConfigHelper.cTor failed to create an instance" , uut ) ; uut . parent = this ; uut . exposeSetCombinerConfigurationIfNecessaryForTest ( ) ; } finally { AbstractTableConfigHelperTest . log . info ( "testTableConfigHelperTest" ) ; } }
public void test() { try { return ( getPid ( ) != null ) ; } catch ( IOException e ) { LOG . error ( "Failed to create ProcessExecution" , e ) ; return false ; } }
private List < String > getConnectedDevices ( ) { String deviceUDID = "(.*)\\tdevice$" ; LOGGER . info ( "Getting connected devices" ) ; String [ ] cmd = CmdLine . insertCommandsAfter ( executor . getDefaultCmd ( ) , "devices" ) ; List < String > cmdOutput = executor . execute ( cmd ) ; List < String > connectedDevices = cmdOutput . stream ( ) . parallel ( ) . filter ( ( d ) -> d . matches ( deviceUDID ) ) . collect ( Collectors . toList ( ) ) ; return connectedDevices ; }
public void test() { try { final JSONArray elementArray = new JSONArray ( ) ; code_block = ForStatement ; this . resultObject . put ( Tokens . RESULTS , elementArray ) ; this . resultObject . put ( Tokens . TOTAL_SIZE , counter ) ; this . resultObject . put ( Tokens . QUERY_TIME , this . sh . stopWatch ( ) ) ; } catch ( JSONException ex ) { logger . error ( ex ) ; final JSONObject error = generateErrorObjectJsonFail ( ex ) ; throw new WebApplicationException ( Response . status ( Response . Status . INTERNAL_SERVER_ERROR ) . entity ( error ) . build ( ) ) ; } finally { indexElements . close ( ) ; rag . tryCommit ( ) ; } }
public void test() { if ( null != index && key != null && value != null ) { final CloseableIterable < Element > indexElements = ( CloseableIterable < Element > ) index . get ( key , value ) ; code_block = TryStatement ;  } else-if ( null == index ) { final String msg = "Could not find index [" + indexName + "] on graph [" + graphName + "]" ; LOG . error ( msg ) ; final JSONObject error = generateErrorObject ( msg ) ; throw new WebApplicationException ( Response . status ( Response . Status . NOT_FOUND ) . entity ( error ) . build ( ) ) ; } else { final HashMap map = new HashMap ( ) ; map . put ( Tokens . QUERY_TIME , this . sh . stopWatch ( ) ) ; map . put ( Tokens . RESULTS , createJSONObject ( index ) ) ; this . resultObject = new JSONObject ( map ) ; } }
private void loadMetadataDefinitions ( ) throws IOException { LOGGER . info ( "Loading metadata definitions from repository {}" , repository ) ; String metadataDefEditHome = repository + File . separator + "core" + File . separator + "metadata" + File . separator + "def" ; final File [ ] inputDefFolders = FolderTools . getFilesInFolder ( metadataDefEditHome , "all" , "" ) ; code_block = ForStatement ; }
public void test() { for ( final File inputDef : inputDefTables ) { LOGGER . info ( "Writing input definition {}" , inputDef . getAbsolutePath ( ) ) ; DataObjectOperation inputDataObjectOperation = new DataObjectOperation ( inputDef . getAbsolutePath ( ) ) ; ObjectMapper inputObjectMapper = new ObjectMapper ( ) ; defTableDataObjects . addAll ( inputDataObjectOperation . getDataObjects ( ) ) ; String metadataDefFileName = inputDefFolder . getName ( ) + "Tables.json" ; String metadataDefFilePath = metadataDefEditHome + File . separator + metadataDefFileName ; inputObjectMapper . writerWithDefaultPrettyPrinter ( ) . writeValue ( new File ( metadataDefFilePath ) , defTableDataObjects ) ; FileTools . copyFromFileToFile ( metadataDefFilePath , repository + File . separator + "docs" + File . separator + "_data" + File . separator + "datamodel" + File . separator + metadataDefFileName ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { try { return binder . getDynRealmTO ( dynRealmDAO . find ( key ) ) ; } catch ( Throwable ignore ) { LOG . debug ( "Unresolved reference" , ignore ) ; throw new UnresolvedReferenceException ( ignore ) ; } }
public void test() { if ( ! running ) { LOGGER . info ( "{} is not running" , getName ( ) ) ; } else { code_block = IfStatement ; sequenceNumsStateForCheckpoint . clear ( ) ; code_block = IfStatement ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
@ Override public void stop ( ) { logger . info ( "{} destroying started." , ClassUtils . simpleClassName ( this ) ) ; code_block = IfStatement ; logger . info ( "{} destroying completed." , ClassUtils . simpleClassName ( this ) ) ; }
@ Override public void stop ( ) { logger . info ( "{} destroying started." , ClassUtils . simpleClassName ( this ) ) ; code_block = IfStatement ; logger . info ( "{} destroying completed." , ClassUtils . simpleClassName ( this ) ) ; }
@ Override public List < GenericEntity > getStudentSummaries ( String token , List < String > studentIds , String sessionId , String sectionId ) { long startTime = System . nanoTime ( ) ; List < GenericEntity > studentSummaries = entityManager . getStudents ( token , sectionId ) ; logger . debug ( "Retrieved " + studentSummaries . size ( ) + " student suites" ) ; return studentSummaries ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { return bundleContext . installBundle ( location , inputStream ) ; } catch ( BundleException bundleException ) { _log . error ( bundleException , bundleException ) ; throw new PortalException ( bundleException ) ; } }
public void test() { if ( mailet . getInitParameters ( ) . isDebug ( ) ) { log . debug ( mailet . getInitParameters ( ) . getString ( ) ) ; } }
public void test() { if ( ! ( rootCause instanceof SocketTimeoutException || rootCause instanceof ConnectException ) ) { log . warn ( rootCause ) ; } }
public void test() { if ( pos > 0 ) { String property = location . substring ( 2 , pos ) ; String rest = location . substring ( pos + 1 ) ; String value = System . getProperty ( property ) ; code_block = IfStatement ; result = value + rest ; } else { log . error ( "Unable to set property [" + property + "] to [" + location + "]" ) ; } }
public void test() { if ( isDebugEnabled ) { logger . debug ( "Closing client with key {}" , key ) ; } }
public void test() { if ( isDebugEnabled ) { logger . debug ( "Closing client with key {}" , key ) ; } }
public void test() { try { List < XAResource > resources = getXAResourcesForXid ( xid ) ; code_block = IfStatement ; String xidString = displayXid ( xid ) ; log . info ( "Start recovery 'forget' processing for Xid:\n" + xidString ) ; setBypassFailures ( Boolean . TRUE ) ; code_block = ForStatement ; log . info ( "Finished recovery 'forget' processing for Xid:\n" + xidString ) ; } finally { setBypassFailures ( Boolean . FALSE ) ; log . exiting ( this . getClass ( ) . getName ( ) , "forget" ) ; } }
public void test() { try { LateralUDPReceiver lur = new LateralUDPReceiver ( "228.5.6.7" , 6789 ) ; Thread t = new Thread ( lur ) ; t . start ( ) ; } catch ( Exception e ) { log . error ( "Unexpected error" , e ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public KeyLookupResult getLookupResult ( ) { code_block = IfStatement ; HoodieBaseFile dataFile = getLatestDataFile ( ) ; List < String > matchingKeys = checkCandidatesAgainstFile ( hoodieTable . getHadoopConf ( ) , candidateRecordKeys , new Path ( dataFile . getPath ( ) ) ) ; LOG . info ( "Using the following keys for the path: {}" , candidateRecordKeys ) ; return new KeyLookupResult ( partitionPathFilePair . getRight ( ) , partitionPathFilePair . getLeft ( ) , dataFile . getCommitTime ( ) , matchingKeys ) ; }
public void test() { if ( documentAttribute == null ) { logger . error ( "Couldn't find attribute in the metacard attribute" ) ; } else-if ( documentAttribute instanceof Attributes ) { Attributes documentAttributes = ( Attributes ) documentAttribute ; code_block = ForStatement ; } else { code_block = IfStatement ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( view . size ( ) > 1 ) { List < ID > coords = view . getPreferredCoordinators ( Collections . emptySet ( ) , localAddress , 5 ) ;LeaveRequestMessage < ID > m = newLeaveRequestMessage < > ( coords , this . localAddress , "this member is shutting down" ) ; services . getMessenger ( ) . send ( m ) ; log . debug ( "Leave request message: %s" , m ) ; } }
private void updateRouterNetworkRef ( Connection conn ) { s_logger . debug ( "Updating router/network references..." ) ; PreparedStatement pstmt = null ; ResultSet rs = null ; code_block = TryStatement ;  s_logger . debug ( "Done updating router/network references" ) ; }
public void test() { try { LOG . debug ( "Opening directory: {}" , dir ) ; File path = new File ( dir ) ; if ( ! path . exists ( ) ) path . mkdirs ( ) ; if ( ! path . exists ( ) || ! path . isDirectory ( ) || ! path . canWrite ( ) ) throw new PermanentBackendException ( "Cannot access or write to directory: " + dir ) ; return FSDirectory . open ( path ) ; } catch ( IOException e ) { throw new PermanentBackendException ( "Could not open directory: " + dir , e ) ; } }
public void test() { if ( partitionToWriteStats . containsKey ( null ) ) { LOG . info ( "partition path is null to " + partitionToWriteStats . get ( null ) ) ; partitionToWriteStats . remove ( null ) ; } }
public void test() { if ( partitionToReplaceFileIds . containsKey ( null ) ) { LOG . info ( "Removing partition {}" , partitionToReplaceFileIds ) ; partitionToReplaceFileIds . remove ( null ) ; } }
public void test() { if ( workflowClass != null ) { Log . debug ( "requiresNewLauncher - by workflowClass name " + workflowClass ) ; return true ; } else-if ( ( workflowEngine != null && workflowEngine . contains ( "Oozie" ) && ! workflowEngine . contains ( "Pegasus" ) ) || ( workflowType != null && workflowType . contains ( "ftl2" ) ) ) { Log . debug ( "requiresNewLauncher - byEngine or Type " + workflowEngine + " " + workflowType ) ; return true ; } }
public void test() { if ( workflowClass != null ) { Log . debug ( "requiresNewLauncher - byClass " + workflowClass ) ; return true ; } else-if ( ( workflowEngine != null && workflowEngine . contains ( "Oozie" ) && ! workflowEngine . contains ( "Pegasus" ) ) || ( workflowType != null && workflowType . contains ( "ftl2" ) ) ) { Log . debug ( "requiresNewLauncher - by f " + workflowType ) ; return true ; } }
public void test() { if ( line . getNetwork ( ) == null ) { logger . warn ( "Missing network" ) ; return false ; } }
public synchronized final void start ( Module ... modules ) throws Throwable { code_block = IfStatement ; ArrayList < Module > findModules = new ArrayList < > ( ) ; findModules . addAll ( Arrays . asList ( this . findModules ( ) ) ) ; findModules . addAll ( Arrays . asList ( modules ) ) ; logger . debug ( "appContext -> doInitialize." ) ; this . getContainer ( ) . preInitialize ( ) ; ApiBinder apiBinder = newApiBinder ( ) ; doBindBefore ( apiBinder ) ; code_block = ForStatement ; logger . debug ( "appContext -> doBind." ) ; doBindAfter ( apiBinder ) ; logger . debug ( "appContext -> init." ) ; this . getContainer ( ) . init ( ) ; doInitializeCompleted ( ) ; logger . debug ( "appContext -> doInitializeCompleted" ) ; Runtime . getRuntime ( ) . addShutdownHook ( shutdownHook ) ; logger . debug ( "appContext -> doStart" ) ; doStart ( ) ; logger . debug ( "appContext -> fireSyncEvent ,eventType = {}" , ContextEvent_Started ) ; getEnvironment ( ) . getEventContext ( ) . fireSyncEvent ( ContextEvent_Started , this ) ; doStartCompleted ( ) ; logger . info ( "Hasor StartCompleted!" ) ; status . compareAndSet ( Processing , Started ) ; }
public synchronized final void start ( Module ... modules ) throws Throwable { code_block = IfStatement ; logger . debug ( "appContext -> findModules." ) ; ArrayList < Module > findModules = new ArrayList < > ( ) ; findModules . addAll ( Arrays . asList ( this . findModules ( ) ) ) ; findModules . addAll ( Arrays . asList ( modules ) ) ; logger . debug ( "appContext -> preInitialize." ) ; this . getContainer ( ) . preInitialize ( ) ; ApiBinder apiBinder = newApiBinder ( ) ; doBindBefore ( apiBinder ) ; code_block = ForStatement ; logger . debug ( "appContext -> doBind." ) ; doBindAfter ( apiBinder ) ; this . getContainer ( ) . init ( ) ; doInitializeCompleted ( ) ; logger . debug ( "appContext -> doInitializeCompleted" ) ; Runtime . getRuntime ( ) . addShutdownHook ( shutdownHook ) ; logger . debug ( "appContext -> doStart" ) ; doStart ( ) ; logger . debug ( "appContext -> fireSyncEvent ,eventType = {}" , ContextEvent_Started ) ; getEnvironment ( ) . getEventContext ( ) . fireSyncEvent ( ContextEvent_Started , this ) ; doStartCompleted ( ) ; logger . info ( "Hasor StartCompleted!" ) ; status . compareAndSet ( Processing , Started ) ; }
public synchronized final void start ( Module ... modules ) throws Throwable { code_block = IfStatement ; logger . debug ( "appContext -> findModules." ) ; ArrayList < Module > findModules = new ArrayList < > ( ) ; findModules . addAll ( Arrays . asList ( this . findModules ( ) ) ) ; findModules . addAll ( Arrays . asList ( modules ) ) ; logger . debug ( "appContext -> doInitialize." ) ; this . getContainer ( ) . preInitialize ( ) ; logger . debug ( "appContext -> preInitialize." ) ; ApiBinder apiBinder = newApiBinder ( ) ; doBindBefore ( apiBinder ) ; code_block = ForStatement ; doBindAfter ( apiBinder ) ; this . getContainer ( ) . init ( ) ; doInitializeCompleted ( ) ; logger . debug ( "appContext -> doInitializeCompleted" ) ; Runtime . getRuntime ( ) . addShutdownHook ( shutdownHook ) ; logger . debug ( "appContext -> doStart" ) ; doStart ( ) ; logger . debug ( "appContext -> fireSyncEvent ,eventType = {}" , ContextEvent_Started ) ; getEnvironment ( ) . getEventContext ( ) . fireSyncEvent ( ContextEvent_Started , this ) ; doStartCompleted ( ) ; logger . info ( "Hasor StartCompleted!" ) ; status . compareAndSet ( Processing , Started ) ; }
public synchronized final void start ( Module ... modules ) throws Throwable { code_block = IfStatement ; logger . debug ( "appContext -> findModules." ) ; ArrayList < Module > findModules = new ArrayList < > ( ) ; findModules . addAll ( Arrays . asList ( this . findModules ( ) ) ) ; findModules . addAll ( Arrays . asList ( modules ) ) ; logger . debug ( "appContext -> doInitialize." ) ; this . getContainer ( ) . preInitialize ( ) ; logger . debug ( "appContext -> doInitialize." ) ; ApiBinder apiBinder = newApiBinder ( ) ; doBindBefore ( apiBinder ) ; code_block = ForStatement ; logger . debug ( "appContext -> doBind." ) ; doBindAfter ( apiBinder ) ; this . getContainer ( ) . init ( ) ; doInitializeCompleted ( ) ; Runtime . getRuntime ( ) . addShutdownHook ( shutdownHook ) ; logger . debug ( "appContext -> doStart" ) ; doStart ( ) ; logger . debug ( "appContext -> fireSyncEvent ,eventType = {}" , ContextEvent_Started ) ; getEnvironment ( ) . getEventContext ( ) . fireSyncEvent ( ContextEvent_Started , this ) ; doStartCompleted ( ) ; logger . info ( "Hasor StartCompleted!" ) ; status . compareAndSet ( Processing , Started ) ; }
public synchronized final void start ( Module ... modules ) throws Throwable { code_block = IfStatement ; logger . debug ( "appContext -> findModules." ) ; ArrayList < Module > findModules = new ArrayList < > ( ) ; findModules . addAll ( Arrays . asList ( this . findModules ( ) ) ) ; findModules . addAll ( Arrays . asList ( modules ) ) ; logger . debug ( "appContext -> doInitialize." ) ; this . getContainer ( ) . preInitialize ( ) ; logger . debug ( "appContext -> doBind." ) ; ApiBinder apiBinder = newApiBinder ( ) ; doBindBefore ( apiBinder ) ; code_block = ForStatement ; logger . debug ( "appContext -> doBind." ) ; doBindAfter ( apiBinder ) ; this . getContainer ( ) . init ( ) ; doInitializeCompleted ( ) ; logger . debug ( "appContext -> doInitializeCompleted" ) ; Runtime . getRuntime ( ) . addShutdownHook ( shutdownHook ) ; doStart ( ) ; logger . debug ( "appContext -> fireSyncEvent ,eventType = {}" , ContextEvent_Started ) ; getEnvironment ( ) . getEventContext ( ) . fireSyncEvent ( ContextEvent_Started , this ) ; doStartCompleted ( ) ; logger . info ( "Hasor StartCompleted!" ) ; status . compareAndSet ( Processing , Started ) ; }
public synchronized final void start ( Module ... modules ) throws Throwable { code_block = IfStatement ; logger . debug ( "appContext -> findModules." ) ; ArrayList < Module > findModules = new ArrayList < > ( ) ; findModules . addAll ( Arrays . asList ( this . findModules ( ) ) ) ; findModules . addAll ( Arrays . asList ( modules ) ) ; logger . debug ( "appContext -> doInitialize." ) ; this . getContainer ( ) . preInitialize ( ) ; logger . debug ( "appContext -> doBind." ) ; ApiBinder apiBinder = newApiBinder ( ) ; doBindBefore ( apiBinder ) ; code_block = ForStatement ; logger . debug ( "appContext -> doBind." ) ; doBindAfter ( apiBinder ) ; this . getContainer ( ) . init ( ) ; doInitializeCompleted ( ) ; logger . debug ( "appContext -> doInitializeCompleted" ) ; Runtime . getRuntime ( ) . addShutdownHook ( shutdownHook ) ; logger . debug ( "appContext -> doStart" ) ; doStart ( ) ; getEnvironment ( ) . getEventContext ( ) . fireSyncEvent ( ContextEvent_Started , this ) ; doStartCompleted ( ) ; logger . info ( "Hasor StartCompleted!" ) ; status . compareAndSet ( Processing , Started ) ; }
public synchronized final void start ( Module ... modules ) throws Throwable { code_block = IfStatement ; logger . debug ( "appContext -> findModules." ) ; ArrayList < Module > findModules = new ArrayList < > ( ) ; findModules . addAll ( Arrays . asList ( this . findModules ( ) ) ) ; findModules . addAll ( Arrays . asList ( modules ) ) ; logger . debug ( "appContext -> doInitialize." ) ; this . getContainer ( ) . preInitialize ( ) ; logger . debug ( "appContext -> doBind." ) ; ApiBinder apiBinder = newApiBinder ( ) ; doBindBefore ( apiBinder ) ; code_block = ForStatement ; logger . debug ( "appContext -> doBind." ) ; doBindAfter ( apiBinder ) ; this . getContainer ( ) . init ( ) ; doInitializeCompleted ( ) ; logger . debug ( "appContext -> doInitializeCompleted" ) ; Runtime . getRuntime ( ) . addShutdownHook ( shutdownHook ) ; logger . debug ( "appContext -> doStart" ) ; doStart ( ) ; logger . debug ( "appContext -> fireSyncEvent ,eventType = {}" , ContextEvent_Started ) ; getEnvironment ( ) . getEventContext ( ) . fireSyncEvent ( ContextEvent_Started , this ) ; doStartCompleted ( ) ; status . compareAndSet ( Processing , Started ) ; }
@ Override public void initialize ( final ExtensionContext context ) { logger . info ( "Initializing Keycloak Extension" ) ; final SubsystemRegistration subsystem = context . registerSubsystem ( SUBSYSTEM_NAME , MGMT_API_VERSION ) ; ManagementResourceRegistration registration = subsystem . registerSubsystemModel ( KEYCLOAK_SUBSYSTEM_RESOURCE ) ; registration . registerSubModel ( REALM_DEFINITION ) ; ManagementResourceRegistration secureDeploymentRegistration = registration . registerSubModel ( SECURE_DEPLOYMENT_DEFINITION ) ; secureDeploymentRegistration . registerSubModel ( CREDENTIAL_DEFINITION ) ; secureDeploymentRegistration . registerSubModel ( REDIRECT_RULE_DEFINITON ) ; ManagementResourceRegistration secureServerRegistration = registration . registerSubModel ( SECURE_SERVER_DEFINITION ) ; secureServerRegistration . registerSubModel ( CREDENTIAL_DEFINITION ) ; secureServerRegistration . registerSubModel ( REDIRECT_RULE_DEFINITON ) ; subsystem . registerXMLElementWriter ( PARSER ) ; }
public void test() { try ( BufferedReader reader = new BufferedReader ( new FileReader ( configFile ) , 1024 ) ) { String line ; code_block = WhileStatement ; } catch ( Exception e ) { logger . error ( e ) ; } }
public void test() { try { PrivilegedCarbonContext . startTenantFlow ( ) ; PrivilegedCarbonContext . getThreadLocalCarbonContext ( ) . setTenantDomain ( DeviceTypeConstants . DEVICE_TYPE_PROVIDER_DOMAIN , true ) ; DeviceTypeManagementDataHolder . getInstance ( ) . getOutputEventAdapterService ( ) . create ( outputEventAdapterConfiguration ) ; } catch ( OutputEventAdapterException e ) { LOG . error ( "Unable to create output event adapter for device: {}" , deviceId , e ) ; } finally { PrivilegedCarbonContext . endTenantFlow ( ) ; } }
public void test() { if ( volumeMap . containsKey ( hddsRoot ) ) { LOG . error ( "Volume {} already exists in VolumeSet" , volumeRoot ) ; success = false ; } else { code_block = IfStatement ; HddsVolume hddsVolume = createVolume ( volumeRoot , storageType ) ; volumeMap . put ( hddsVolume . getHddsRootDir ( ) . getPath ( ) , hddsVolume ) ; volumeStateMap . get ( hddsVolume . getStorageType ( ) ) . add ( hddsVolume ) ; LOG . info ( "Added Volume : {} to VolumeSet" , hddsVolume . getHddsRootDir ( ) . getPath ( ) ) ; success = true ; } }
public void test() { try { code_block = IfStatement ; } catch ( IOException ex ) { success = false ; logger . warn ( String . format ( "Failure unlocking file[%s]" , file ) , ex ) ; } finally { this . writeUnlock ( ) ; } }
public void test() { if ( ! silent ) { log . error ( Util . getMessage ( message ) , e ) ; } }
public void test() { if ( ! isOK ) { logger . debug ( "**************************" ) ; return ; } }
public void test() { try { listener . gotRetweetsOfMe ( statuses ) ; } catch ( Exception e ) { logger . warn ( "Exception at getRetweetsOfMe" , e ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { try { LOGGER . info ( "delete directory " + envName + "-snapshot" ) ; IoUtils . cleanDirectory ( tmp ) ; LOGGER . info ( "success delete " + envName + "-snapshot" ) ; } catch ( IOException e ) { e . printStackTrace ( ) ; } }
public void test() { switch ( event . encodeType ( EventTypes . class ) ) { case DISCONNECT_EVENT : setTimer ( REC_TIMEOUT ) ; switchToNextState ( FsmState . REOPEN ) ; break ; case TIMEOUT_EVENT : doDisconnect ( ) ; setTimer ( REC_TIMEOUT ) ; switchToNextState ( FsmState . REOPEN ) ; break ; case STOP_EVENT : clearTimer ( ) ; doDisconnect ( ) ; switchToNextState ( FsmState . DOWN ) ; break ; case CEA_EVENT : clearTimer ( ) ; code_block = IfStatement ; break ; case SEND_MSG_EVENT : logger . trace ( "Connection is down" ) ; throw new RuntimeException ( "Connection is down" ) ; default : return false ; } }
public static String compressBBSImage ( String oldFilePath ) { log . info ( "--------------- compressBBSImage" ) ; BitmapFactory . Options options = BitmapUtil . INSTANCE . getImageOptions ( oldFilePath ) ; int width = options . outWidth ; int height = options . outHeight ; int showWidth = width ; int showHeight = height ; code_block = IfStatement ; Bitmap bitmap = BitmapUtil . INSTANCE . getFitSampleBitmap ( oldFilePath , showWidth , showHeight ) ; String filePath = FileExtensionHelper . generateBBSTempFilePath ( ) ; SDCardHelper . INSTANCE . bitmapToPNGFile ( bitmap , filePath ) ; return filePath ; }
public void test() { try { if ( ! Files . exists ( pathToDir ) ) Files . createDirectory ( pathToDir ) ; } catch ( IOException e ) { logger . error ( "could not create directory [" + pathToDir . getAbsolutePath ( ) + "]" ) ; } }
private void checkDescribeOpportunities ( ) throws IOException { MarketoSource source = new MarketoSource ( ) ; source . initialize ( null , iprops ) ; MarketoRESTClient client = ( MarketoRESTClient ) source . getClientService ( null ) ; MarketoRecordResult opps = client . describeOpportunity ( iprops ) ; assertNotNull ( opps . getRecords ( ) ) ; assertNotNull ( opps . getRecords ( ) . get ( 0 ) ) ; IndexedRecord record = opps . getRecords ( ) . get ( 0 ) ; assertNotNull ( record . get ( record . getSchema ( ) . getField ( "idField" ) . pos ( ) ) ) ; assertNotNull ( record . get ( record . getSchema ( ) . getField ( "dedupeFields" ) . pos ( ) ) ) ; assertNotNull ( record . get ( record . getSchema ( ) . getField ( "searchableFields" ) . pos ( ) ) ) ; assertNotNull ( record . get ( record . getSchema ( ) . getField ( "fields" ) . pos ( ) ) ) ; LOG . info ( record . toString ( ) ) ; }
public void test() { if ( _logger . isDebugEnabled ( ) ) { if ( _logger . isTraceEnabled ( ) ) logTypeMap ( localTypeMap ) ; else-if ( _logger . isDebugEnabled ( ) ) logServerTypeDesc ( serverTypeDesc , LogLevel . DEBUG ) ; _logger . debug ( "Server type: " + serverTypeDesc ) ; } }
public void test() { try { return Optional . of ( dataSourceParameter . getHadoopFileSystem ( path ) . getFileStatus ( path ) ) ; } catch ( FileNotFoundException e ) { LOG . debug ( MessageFormat . format ( "file not found" , e ) ) ; return Optional . empty ( ) ; } catch ( IOException e ) { throw new CommandConfigurationException ( MessageFormat . format ( "error occurred while resolving Hadoop path: {0}" , path ) , e ) ; } }
@ Test public void sqlBench ( ) { int warmUpIterations = 100 ; int realIterations = 200 ; SqlService sqlService = inst . getSql ( ) ; inst . getMap ( "m" ) . put ( 1 , 1 ) ; int numRows = 0 ; code_block = ForStatement ; logger . info ( "warmup jobs done, starting benchmark" ) ; long start = System . nanoTime ( ) ; code_block = ForStatement ; long elapsedMicros = NANOSECONDS . toMicros ( System . nanoTime ( ) - start ) ; logger . info ( numRows + " queries run in " + ( elapsedMicros / realIterations ) + " ms" ) ; System . out . println ( numRows ) ; System . out . println ( realIterations + " queries run in " + ( elapsedMicros / realIterations ) + " us/job" ) ; }
@ Test public void sqlBench ( ) { int warmUpIterations = 100 ; int realIterations = 200 ; SqlService sqlService = inst . getSql ( ) ; logger . info ( "will submit " + warmUpIterations + " jobs" ) ; inst . getMap ( "m" ) . put ( 1 , 1 ) ; int numRows = 0 ; code_block = ForStatement ; long start = System . nanoTime ( ) ; code_block = ForStatement ; long elapsedMicros = NANOSECONDS . toMicros ( System . nanoTime ( ) - start ) ; logger . info ( numRows + " queries run in " + ( elapsedMicros / realIterations ) + " us/job" ) ; System . out . println ( numRows ) ; System . out . println ( realIterations + " queries run in " + ( elapsedMicros / realIterations ) + " us/job" ) ; }
public void test() { try { isStart = ( ( Boolean ) UnoProperty . getProperty ( textPortion , UnoProperty . IS_START ) ) . booleanValue ( ) ; isCollapsed = ( ( Boolean ) UnoProperty . getProperty ( textPortion , UnoProperty . IS_COLLAPSED ) ) . booleanValue ( ) ; code_block = IfStatement ; bookmark = UNO . XNamed ( UnoProperty . getProperty ( textPortion , UnoProperty . BOOKMARK ) ) ; } catch ( Exception x ) { LOGGER . log ( Level . WARNING , "" , x ) ; return ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { Runtime runtime = Runtime . getRuntime ( ) ; logger . debug ( "Runtime: {}" , runtime . getClass ( ) . getName ( ) ) ; } }
public void test() { try { final Session < Client > session = pool . borrow ( BackgroundActionState . running ) ; code_block = TryStatement ;  } catch ( ConnectionCanceledException e ) { log . debug ( String . format ( "Connection cancelled while processing scheduled task. %s" , e . getMessage ( ) ) , e ) ; this . shutdown ( ) ; } catch ( BackgroundException e ) { log . warn ( String . format ( "Failure processing scheduled task. %s" , e . getMessage ( ) ) , e ) ; } catch ( Exception e ) { log . error ( String . format ( "Failure processing scheduled task. %s" , e . getMessage ( ) ) , e ) ; this . shutdown ( ) ; } }
public void test() { try { final Session < Client > session = pool . borrow ( BackgroundActionState . running ) ; code_block = TryStatement ;  } catch ( ConnectionCanceledException e ) { log . warn ( "Cancel processing scheduled task. %s" , e ) ; this . shutdown ( ) ; } catch ( BackgroundException e ) { log . error ( String . format ( "Failure processing scheduled task. %s" , e . getMessage ( ) ) , e ) ; } catch ( Exception e ) { log . error ( String . format ( "Failure processing scheduled task. %s" , e . getMessage ( ) ) , e ) ; this . shutdown ( ) ; } }
public void test() { try { final Session < Client > session = pool . borrow ( BackgroundActionState . running ) ; code_block = TryStatement ;  } catch ( ConnectionCanceledException e ) { log . warn ( "Cancel processing scheduled task. %s" , e ) ; this . shutdown ( ) ; } catch ( BackgroundException e ) { log . warn ( String . format ( "Failure processing scheduled task. %s" , e . getMessage ( ) ) , e ) ; } catch ( Exception e ) { log . error ( String . format ( "Failure processing scheduled task. %s" , e . getMessage ( ) ) , e ) ; this . shutdown ( ) ; } }
public void test() { try { String dataSource = ( environment instanceof BazaarEnvironment || String . format ( "Of %s" , Common . BAZAAR_ID ) . equals ( environment . getName ( ) ) ) ? Common . BAZAAR_ID : Common . SUBUTAI_ID ; EnvironmentDto environmentDto = new EnvironmentDto ( environment . getId ( ) , environment . getName ( ) , environment . getStatus ( ) , convertContainersToContainerJson ( environment . getContainerHosts ( ) , dataSource ) , dataSource , environmentManager . getEnvironmentOwnerName ( environment ) ) ; environmentDtos . add ( environmentDto ) ; } catch ( Exception e ) { LOG . error ( "Unable to add environment {}" , environment , e ) ; } }
@ Override public void run ( ) { LOGGER . debug ( "Remote did not respond to a drain request" ) ; Exception cause = new JmsOperationTimedOutException ( "Remote did not respond to a drain request in time" ) ; locallyClosed ( session . getConnection ( ) , cause ) ; stopRequest . onFailure ( cause ) ; session . pumpToProtonTransport ( stopRequest ) ; }
@ Override public void collisionResolve ( SpaceStationMir mir ) { log . info ( "Resolve" ) ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { try { Integer bgColor = Integer . valueOf ( Integer . parseInt ( highlightColor , 16 ) ) ; XTextCursor cursor = getTextCursor ( ) ; Utils . setProperty ( cursor , UnoProperty . CHAR_BACK_COLOR , bgColor ) ; cursor . collapseToEnd ( ) ; UnoProperty . setPropertyToDefault ( cursor , UnoProperty . CHAR_BACK_COLOR ) ; } catch ( NumberFormatException | UnoHelperException e ) { LOGGER . trace ( "" , e ) ; } }
public void test() { try { UnoProperty . setPropertyToDefault ( getTextCursor ( ) , UnoProperty . CHAR_BACK_COLOR ) ; } catch ( UnoHelperException e ) { LOGGER . error ( "" , e ) ; } }
public void test() { if ( state < 0 ) { logger . warn ( "Invalid state {}" , state ) ; return ; } }
public void test() { if ( responseMessage instanceof Result ) { LOG . trace ( "[{}] Got result response" , logPrefix ) ; processResultResponse ( ( Result ) responseMessage , response ) ; } else-if ( responseMessage instanceof Error ) { LOG . trace ( "[{}] Got error response" , logPrefix ) ; processErrorResponse ( ( Error ) responseMessage ) ; } else { IllegalStateException error = new IllegalStateException ( "Unexpected response " + responseMessage ) ; trackNodeError ( node , error ) ; abort ( error , false ) ; } }
public void test() { if ( responseMessage instanceof Result ) { LOG . trace ( "[{}] Got result" , logPrefix ) ; processResultResponse ( ( Result ) responseMessage , response ) ; } else-if ( responseMessage instanceof Error ) { LOG . trace ( "[{}] Got error" , logPrefix ) ; processErrorResponse ( ( Error ) responseMessage ) ; } else { IllegalStateException error = new IllegalStateException ( "Unexpected response " + responseMessage ) ; trackNodeError ( node , error ) ; abort ( error , false ) ; } }
public void test() { if ( stop ( ) ) { future . cancel ( true ) ; logger . debug ( "Stopped listening for {}" , host ) ; return ; } }
public void test() { try { code_block = IfStatement ; alert ( ) ; } catch ( Exception e ) { logger . error ( "[notifyAlert]" , e ) ; } }
public void test() { try { String cmd = BashCommands . prependToEtcHosts ( "1.2.3.4" , "myhostnamefor1234.at.start" , "myhostnamefor1234b" ) ; execRequiringZeroAndReturningStdout ( loc , cmd ) . get ( ) ; String cmd2 = BashCommands . appendToEtcHosts ( "5.6.7.8" , "myhostnamefor5678.at.end" , "myhostnamefor5678" ) ; execRequiringZeroAndReturningStdout ( loc , cmd2 ) . get ( ) ; String grepFirst = execRequiringZeroAndReturningStdout ( loc , "grep -n myhostnamefor1234 /etc/hosts" ) . get ( ) ; String grepLast = execRequiringZeroAndReturningStdout ( loc , "grep -n myhostnamefor5678 /etc/hosts" ) . get ( ) ; int numLinesAfter = Integer . parseInt ( execRequiringZeroAndReturningStdout ( loc , "wc -l /etc/hosts" ) . get ( ) . trim ( ) . split ( "\\s" ) [ 0 ] ) ; assertTrue ( grepFirst . startsWith ( "1:" ) && grepFirst . contains ( "1.2.3.4 myhostnamefor1234.at.start myhostnamefor1234" ) , "first=" + grepFirst ) ; assertTrue ( grepLast . startsWith ( ( numLinesOrig + 2 ) + ":" ) && grepLast . contains ( "5.6.7.8 myhostnamefor5678.at.end myhostnamefor5678" ) , "last=" + grepLast ) ; assertEquals ( numLinesOrig + 2 , numLinesAfter , "lines orig=" + numLinesOrig + ", after=" + numLinesAfter ) ; } finally { execRequiringZeroAndReturningStdout ( loc , sudo ( "cp /etc/hosts-orig-testModifyEtcHosts /etc/hosts" ) ) . get ( ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { if ( lastRunTime + 1000 < potentialWriteTime ) { logger . debug ( "Last partial histo log <1s:" + this ) ; run ( ) ; } else { logger . debug ( "Not writing last partial histo log <1s:" + this ) ; } }
public void test() { if ( lastRunTime + 1000 < potentialWriteTime ) { logger . debug ( "Writing last partial histo log:" + this ) ; run ( ) ; } else { logger . debug ( "Writing last partial histo log:" + this ) ; } }
public void test() { try { value = field . get ( this ) ; code_block = IfStatement ; } catch ( final Exception ex ) { logger . error ( "Cannot read value of " + field , ex ) ; } }
public void test() { try { LOG . debug ( "Getting events for events" ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; EventLog eventLog = EventLog . getEnterpriseEvents ( boxConnection , position , after , before , types ) ; List < BoxEvent > results = new ArrayList < > ( ) ; code_block = ForStatement ; return results ; } catch ( BoxAPIException e ) { throw new RuntimeException ( String . format ( "Box API returned the error code %d%n%n%s" , e . getResponseCode ( ) , e . getResponse ( ) ) , e ) ; } }
public void test() { try { E entity = getController ( ) . getEntityClass ( ) . newInstance ( ) ; code_block = ForStatement ; return entity ; } catch ( Exception e ) { e . printStackTrace ( ) ; log . error ( "Error creating an entity" ) ; } }
public void test() { try { code_block = IfStatement ; FileWriter out = new FileWriter ( file ) ; gson . toJson ( clients , new TypeToken < Map < String , RegisteredClient > > ( ) code_block = "" ; . getType ( ) , out ) ; out . close ( ) ; } catch ( IOException e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( device == null ) { logger . debug ( "Device is null." ) ; return false ; } }
public void test() { try { code_block = IfStatement ; } catch ( RuntimeException e ) { LOGGER . error ( MessageFormat . format ( Messages . getString ( "AbstractOperatorAnnotationProcessor.errorFailCompile" ) , e ) , e ) ; environment . getProcessingEnvironment ( ) . getMessager ( ) . printMessage ( Diagnostic . Kind . ERROR , MessageFormat . format ( Messages . getString ( "AbstractOperatorAnnotationProcessor.errorFailCompile" ) , e . toString ( ) ) ) ; } }
public void test() { try { m = wrappedStreamClass . getDeclaredMethod ( "getNumCurrentReplicas" , new Class < ? > [ ] code_block = "" ; ) ; m . setAccessible ( true ) ; } catch ( NoSuchMethodException e ) { logger . info ( "Doesn't have a getNumCurrentReplicas method, fsOut=" + wrappedStreamClass . getName ( ) , e ) ; } catch ( SecurityException e ) { logger . info ( "Doesn't have access to getNumCurrentReplicas on " + "FileSystems's output stream --HDFS-826 not available; fsOut=" + wrappedStreamClass . getName ( ) , e ) ; m = null ; } }
public void test() { try { m = wrappedStreamClass . getDeclaredMethod ( "getNumCurrentReplicas" , new Class < ? > [ ] code_block = "" ; ) ; m . setAccessible ( true ) ; } catch ( NoSuchMethodException e ) { logger . info ( "FileSystem's output stream doesn't support" + " getNumCurrentReplicas; --HDFS-826 not available; fsOut=" + wrappedStreamClass . getName ( ) + "; err=" + e ) ; } catch ( SecurityException e ) { logger . error ( "getNumCurrentReplicas() threw" , e ) ; m = null ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
@ Override public void onFailure ( IMqttToken token , Throwable throwable ) { logger . error ( throwable . getMessage ( ) , throwable ) ; }
@ SuppressWarnings ( "deprecation" ) protected void internalUnsubscribeNamespaceBundle ( String subscription , String bundleRange , boolean authoritative ) { validateNamespaceOperation ( namespaceName , NamespaceOperation . UNSUBSCRIBE ) ; checkNotNull ( subscription , "Subscription should not be null" ) ; checkNotNull ( bundleRange , "BundleRange should not be null" ) ; Policies policies = getNamespacePolicies ( namespaceName ) ; code_block = IfStatement ; validateNamespaceBundleOwnership ( namespaceName , policies . bundles , bundleRange , authoritative , true ) ; unsubscribe ( namespaceName , bundleRange , subscription ) ; LOG . debug ( "Successfully unsubscribe namespace bundle {}" , bundleRange ) ; }
public void test() { try { code_block = IfStatement ; } catch ( RuntimeException e ) { LOGGER . error ( "Exception while executing callback" , e ) ; } }
public void test() { if ( ! dumpDir . mkdirs ( ) && ! dumpDir . exists ( ) ) { log . error ( "Unable to create dump dir" ) ; return ; } }
public void test() { try { FileUtils . writeByteArrayToFile ( dumpClassFile , data ) ; dumpResult . put ( clazz , dumpClassFile ) ; } catch ( IOException e ) { logger . error ( e . getMessage ( ) , e ) ; } }
private void deployTestSpecificResources ( ExtensionContext extensionContext ) { LOGGER . info ( "Deploying test specific resources" ) ; prepareEnvForOperator ( extensionContext , CO_NAMESPACE , Arrays . asList ( CO_NAMESPACE , SECOND_NAMESPACE , THIRD_NAMESPACE ) ) ; applyBindings ( extensionContext , CO_NAMESPACE ) ; List < ClusterRoleBinding > clusterRoleBindingList = ClusterRoleBindingTemplates . clusterRoleBindingsForAllNamespaces ( CO_NAMESPACE ) ; clusterRoleBindingList . forEach ( clusterRoleBinding -> ClusterRoleBindingResource . clusterRoleBinding ( extensionContext , clusterRoleBinding ) ) ; resourceManager . createResource ( extensionContext , BundleResource . clusterOperator ( CO_NAMESPACE , "*" , Constants . RECONCILIATION_INTERVAL ) . build ( ) ) ; String previousNamespace = cluster . setNamespace ( THIRD_NAMESPACE ) ; resourceManager . createResource ( extensionContext , KafkaTemplates . kafkaEphemeral ( MAIN_NAMESPACE_CLUSTER_NAME , 1 , 1 ) . editSpec ( ) . editEntityOperator ( ) . editTopicOperator ( ) . withWatchedNamespace ( SECOND_NAMESPACE ) . endTopicOperator ( ) . editUserOperator ( ) . withWatchedNamespace ( SECOND_NAMESPACE ) . endUserOperator ( ) . endEntityOperator ( ) . endSpec ( ) . build ( ) ) ; cluster . setNamespace ( SECOND_NAMESPACE ) ; resourceManager . createResource ( extensionContext , KafkaTemplates . kafkaEphemeral ( SECOND_CLUSTER_NAME , 3 ) . build ( ) ) ; cluster . setNamespace ( previousNamespace ) ; }
public void test() { try { stm = c . createStatement ( ) ; ResultSet rs = stm . executeQuery ( "SELECT " + HM_COLUMN . getColumnsInOrder ( ) + " FROM runningJobsMonitor" + " WHERE jobId=" + jobId ) ; code_block = IfStatement ; } catch ( SQLException e ) { String message = "SQL error querying runningJobsMonitor" + "\n" + ExceptionUtils . getSQLExceptionCause ( e ) ; log . warn ( message , e ) ; throw new IOFailure ( message , e ) ; } finally { DBUtils . closeStatementIfOpen ( stm ) ; HarvestDBConnection . release ( c ) ; } }
@ Override public void run ( Timeout timeout ) throws Exception { String message = "Request (" + _lastRequestId + ") to server " + _server + " timed-out waiting for response. Closing the channel !!" ; LOG . error ( message ) ; Exception e = new Exception ( message ) ; _outstandingFuture . get ( ) . onError ( e ) ; close ( ) ; }
private void restartWithSystem ( InternalDistributedSystem newSystem , InternalCache newCache ) throws IOException { synchronized ( locatorLock ) code_block = "" ; logger . info ( "Starting locator..." ) ; internalDistributedSystem = newSystem ; internalCache = newCache ; code_block = TryStatement ;  code_block = IfStatement ; code_block = IfStatement ; logger . info ( "Locator restart: initializing JMX manager" ) ; startJmxManagerLocationService ( newCache ) ; endStartLocator ( internalDistributedSystem ) ; logger . info ( "Locator restart completed" ) ; restartHandlers . forEach ( handler -> handler . restartCompleted ( newSystem ) ) ; }
public void test() { if ( ! membershipLocator . isAlive ( ) ) { log . info ( "Cluster is not alive and will be stopped" ) ; startTcpServer ( ) ; } }
private void restartWithSystem ( InternalDistributedSystem newSystem , InternalCache newCache ) throws IOException { synchronized ( locatorLock ) code_block = "" ; logger . info ( "Locator restart: initializing JmxManager" ) ; internalDistributedSystem = newSystem ; internalCache = newCache ; logger . info ( "Locator restart: initializing TcpServer" ) ; code_block = TryStatement ;  code_block = IfStatement ; code_block = IfStatement ; startJmxManagerLocationService ( newCache ) ; endStartLocator ( internalDistributedSystem ) ; logger . info ( "Locator restart completed" ) ; restartHandlers . forEach ( handler -> handler . restartCompleted ( newSystem ) ) ; }
private void restartWithSystem ( InternalDistributedSystem newSystem , InternalCache newCache ) throws IOException { synchronized ( locatorLock ) code_block = "" ; logger . info ( "Locator restart: initializing" ) ; internalDistributedSystem = newSystem ; internalCache = newCache ; logger . info ( "Locator restart: initializing TcpServer" ) ; code_block = TryStatement ;  code_block = IfStatement ; code_block = IfStatement ; logger . info ( "Locator restart: initializing JMX manager" ) ; startJmxManagerLocationService ( newCache ) ; endStartLocator ( internalDistributedSystem ) ; restartHandlers . forEach ( handler -> handler . restartCompleted ( newSystem ) ) ; }
public void test() { try { log . info ( "Start plugin '{}'" , getPluginLabel ( pluginWrapper . getDescriptor ( ) ) ) ; Plugin plugin = pluginWrapper . getPlugin ( ) ; plugin . start ( ) ; pluginWrapper . setPluginState ( PluginState . STARTED ) ; code_block = IfStatement ; startedPlugins . add ( pluginWrapper ) ; firePluginStateEvent ( new PluginStateEvent ( this , pluginWrapper , pluginState ) ) ; } catch ( Throwable e ) { log . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( channels . containsKey ( name ) ) { String id = channels . get ( name ) ; transmitWithResult ( PATH_ZAP + UrlEncoded . encodeString ( id ) ) . ifPresent ( document -> channel = name ) ; } else { logger . debug ( "Channel {} not found" , name ) ; } }
@ Test ( enabled = true ) public void testExecutePassingUniqueOptionToHillClimb ( ) { log . info ( "=== TEST for SOLUTION GENERATION of HILLCLIMB optimizer SINGLE ADP FINISEHD ===" ) ; optimizer = new Optimizer ( TestConstants . NUM_PLANS_TO_GENERATE , SearchMethodName . HILLCLIMB ) ; executeAndSave ( appModel , suitableCloudOffer , SearchMethodName . HILLCLIMB ) ; log . info ( "=== TEST for SOLUTION GENERATION with POLICIES of HILLCLIMB optimizer SINGLE ADP FINISEHD ===" ) ; }
@ Test ( enabled = true ) public void testExecutePassingUniqueOptionToHillClimb ( ) { log . info ( "=== TEST for SOLUTION GENERATION of HILLCLIMB optimizer SINGLE ADP - STARTED ===" ) ; optimizer = new Optimizer ( TestConstants . NUM_PLANS_TO_GENERATE , SearchMethodName . HILLCLIMB ) ; executeAndSave ( appModel , suitableCloudOffer , SearchMethodName . HILLCLIMB ) ; log . info ( "=== TEST for SOLUTION GENERATION of HILLCLIMB optimizer SINGLE ADP - STARTED ===" ) ; }
public CharSequence postProcess ( CharSequence object ) { log . debug ( prefix + object ) ; return ( prefix + object ) ; }
@ RequestMapping ( value = "/clusters/" + CLUSTER_NAME_PATH_VARIABLE , method = RequestMethod . PUT ) public void updateCluster ( @ PathVariable String clusterName , @ RequestBody ClusterTbl cluster ) { logger . debug ( "updateCluster" ) ; clusterService . updateCluster ( clusterName , cluster ) ; }
public void test() { try { FeedUpdater updater = new SingleThreadedFeedUpdater ( ) ; updater . updateSubscriptions ( ) ; log . info ( "Updated planet" ) ; } catch ( Exception e ) { log . error ( "ERROR refreshing planet" , e ) ; } finally { WebloggerFactory . getWeblogger ( ) . release ( ) ; } }
public void test() { try { log . info ( "Refreshing Planet subscriptions" ) ; FeedUpdater updater = new SingleThreadedFeedUpdater ( ) ; updater . updateSubscriptions ( ) ; } catch ( Exception e ) { log . error ( "Error updating Planet subscriptions" , e ) ; } finally { WebloggerFactory . getWeblogger ( ) . release ( ) ; } }
public void test() { try { dataObjectsId = this . extractContentsId ( bean , reqCtx ) ; dataObjectsId = this . executeFullTextSearch ( bean , dataObjectsId , reqCtx ) ; } catch ( Throwable t ) { _logger . error ( "Error extracting dataObjects id" , t ) ; throw new ApsSystemException ( "Error extracting dataObjects id" , t ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( InterruptedException e ) { Log . warn ( e . toString ( ) , e ) ; } }
protected void activate ( final ComponentContext componentContext , final Map < String , Object > properties ) { logger . debug ( "Activating Cloud Subscriber Wire Component..." ) ; this . wireSupport = this . wireHelperService . newWireSupport ( this , ( ServiceReference < WireComponent > ) componentContext . getServiceReference ( ) ) ; this . options = new CloudSubscriberOptions ( properties ) ; logger . debug ( "Activating Cloud Subscriber Wire Component... Done" ) ; }
protected void activate ( final ComponentContext componentContext , final Map < String , Object > properties ) { logger . debug ( "Activating Cloud Subscriber Wire Component..." ) ; this . wireSupport = this . wireHelperService . newWireSupport ( this , ( ServiceReference < WireComponent > ) componentContext . getServiceReference ( ) ) ; this . options = new CloudSubscriberOptions ( properties ) ; logger . debug ( "Activating Cloud Subscriber Wire Component... Done" ) ; }
public void test() { try { checkNotNull ( url , "url" ) ; checkNotNull ( targetName , "targetName" ) ; JavaWebAppDriver driver = getDriver ( ) ; String deployedName = driver . deploy ( url , targetName ) ; Set < String > deployedWars = getAttribute ( DEPLOYED_WARS ) ; code_block = IfStatement ; deployedWars . add ( deployedName ) ; sensors ( ) . set ( DEPLOYED_WARS , deployedWars ) ; } catch ( RuntimeException e ) { LOG . warn ( "Error deploying " + targetName , e ) ; throw Throwables . propagate ( e ) ; } }
public void attachClean ( MbBaust instance ) { log . debug ( "attaching clean MbBaust instance" ) ; code_block = TryStatement ;  }
public void test() { try { getSession ( ) . lock ( instance , LockMode . NONE ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { uploadRetryMap . remove ( identifier ) ; asyncWriteCache . remove ( fileName ) ; LOG . info ( "Async Upload Aborted. Dataidentifer [{}], file [{}] removed from AsyncCache." , identifier , file . getAbsolutePath ( ) ) ; } catch ( IOException ie ) { LOG . error ( "While not removing file [{}] from AsyncCache." , identifier , file . getAbsolutePath ( ) ) ; } }
public void setWorkManager ( final WorkManager workManager ) { logger . info ( "Setting work manager: {}" , workManager ) ; this . workManager = workManager ; }
private void getAllStorageSystems ( ) { ResourceCollection < StorageSystem > storageSystems = this . storageSystemClient . getAll ( ) ; LOGGER . info ( "StorageSystems returned to client : " + storageSystems . toJsonString ( ) ) ; }
public void test() { try { java . util . List < com . liferay . journal . model . JournalArticle > returnValue = JournalArticleServiceUtil . getGroupArticles ( groupId , userId , rootFolderId , status , start , end , orderByComparator ) ; return com . liferay . journal . model . JournalArticleSoap . toSoapModels ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Processing message: {}" , msg ) ; } }
@ Override public void channelUnlinked ( ChannelUID channelUID ) { logger . debug ( "channelUnlinked {}" , channelUID ) ; code_block = SwitchStatement ; }
public void test() { try { ChannelParams params = new ChannelParams ( channel ) ; code_block = IfStatement ; } catch ( ConversionException e ) { logger . warn ( "Channel param error, reason: {}." , e . getMessage ( ) , e ) ; } }
public String create ( LogicalInfrastructure logicalInfrastructure ) throws RestServiceException { LOGGER . debug ( "Creating VCPENetwork" ) ; String resourceId = vcpeNetworkService . createVCPENetwork ( OpennaasBeanUtils . getVCPENetwork ( logicalInfrastructure ) ) ; LOGGER . debug ( "Polling for building task to finish" ) ; code_block = WhileStatement ; LOGGER . debug ( "Retrieving build result" ) ; vcpeNetworkService . getBuildResult ( resourceId ) ; return resourceId ; }
public String create ( LogicalInfrastructure logicalInfrastructure ) throws RestServiceException { LOGGER . debug ( "create a VCPENetwork: " + logicalInfrastructure ) ; String resourceId = vcpeNetworkService . createVCPENetwork ( OpennaasBeanUtils . getVCPENetwork ( logicalInfrastructure ) ) ; LOGGER . debug ( "VCPENetwork created" ) ; code_block = WhileStatement ; LOGGER . debug ( "Retrieving build result" ) ; vcpeNetworkService . getBuildResult ( resourceId ) ; return resourceId ; }
public String create ( LogicalInfrastructure logicalInfrastructure ) throws RestServiceException { LOGGER . debug ( "create a VCPENetwork: " + logicalInfrastructure ) ; String resourceId = vcpeNetworkService . createVCPENetwork ( OpennaasBeanUtils . getVCPENetwork ( logicalInfrastructure ) ) ; LOGGER . debug ( "Polling for building task to finish" ) ; code_block = WhileStatement ; LOGGER . debug ( "VCPENetwork created: " + resourceId ) ; vcpeNetworkService . getBuildResult ( resourceId ) ; return resourceId ; }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { log . warn ( expPrefix + region . getName ( ) + InventoryConstants . ERROR_CAUSE + e . getMessage ( ) + "\"}" ) ; ErrorManageUtil . uploadError ( accountId , region . getName ( ) , "egressgateway" , e . getMessage ( ) ) ; } }
public void leave ( RoomParticipant user ) { checkClosed ( ) ; this . removeParticipant ( user . getName ( ) ) ; user . close ( ) ; LOG . info ( "User " + user + " closed." ) ; }
public void test() { if ( isInitialized ( ) ) { logger . info ( msg , event . getCode ( ) ) ; } else { logger . debug ( msg , event . getCode ( ) ) ; } }
public void test() { if ( isInitialized ( ) ) { logger . warn ( msg , event . getCode ( ) ) ; } else { logger . debug ( msg , event . getCode ( ) ) ; } }
public void test() { switch ( eventValue ) { case EvdevLibrary . KeyEventValue . DOWN : String keyCode = channel . getUID ( ) . getIdWithoutGroup ( ) ; updateState ( keyChannel . getUID ( ) , new StringType ( keyCode ) ) ; updateState ( channel . getUID ( ) , OpenClosedType . CLOSED ) ; triggerChannel ( keyChannel . getUID ( ) , keyCode ) ; triggerChannel ( channel . getUID ( ) , CommonTriggerEvents . PRESSED ) ; updateState ( keyChannel . getUID ( ) , new StringType ( ) ) ; break ; case EvdevLibrary . KeyEventValue . UP : updateState ( channel . getUID ( ) , OpenClosedType . OPEN ) ; triggerChannel ( channel . getUID ( ) , CommonTriggerEvents . RELEASED ) ; break ; case EvdevLibrary . KeyEventValue . REPEAT : break ; default : logger . warn ( "Unknown key: {}" , eventValue ) ; break ; } }
public void test() { if ( ! scheduledChecks . contains ( scheduledKey ) ) { checkIntegrationStatus ( integrationDeployment ) ; } else { LOG . debug ( "Deployment {} already scheduled" , scheduledKey ) ; } }
public void test() { if ( integrationDeployment != null ) { String scheduledKey = getIntegrationMarkerKey ( integrationDeployment ) ; LOG . debug ( "Check if IntegrationStatus {} is already in progress for key: {} (keys: {})" , id , scheduledKey , scheduledChecks ) ; code_block = IfStatement ; } else { LOG . warn ( "IntegrationDeployment was null for key: {}" , key ) ; } }
public void test() { if ( hdfsPath == null ) { return false ; } }
public void addNetwork ( QuantumModel quantumModel , Network network ) throws QuantumException { log . debug ( "Adding network " + network . getName ( ) + " to Quantum Model." ) ; List < Network > networks = quantumModel . getNetworks ( ) ; if ( networks . contains ( network ) ) throw new QuantumException ( "Network  " + network . getName ( ) + " already exists in Quantum Model." ) ; quantumModel . getNetworks ( ) . add ( network ) ; log . debug ( "Network " + network . getName ( ) + " added to Quantum model." ) ; }
public void addNetwork ( QuantumModel quantumModel , Network network ) throws QuantumException { log . debug ( "Adding network " + network . getName ( ) + " to Quantum model." ) ; List < Network > networks = quantumModel . getNetworks ( ) ; if ( networks . contains ( network ) ) throw new QuantumException ( "Network  " + network . getName ( ) + " already exists in Quantum Model." ) ; quantumModel . getNetworks ( ) . add ( network ) ; log . debug ( "Network " + network . getName ( ) + " added." ) ; }
public void test() { try { com . liferay . commerce . model . CommerceOrder returnValue = CommerceOrderServiceUtil . executeWorkflowTransition ( commerceOrderId , workflowTaskId , transitionName , comment ) ; return com . liferay . commerce . model . CommerceOrderSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try { archiveFile . createNewFile ( ) ; } catch ( IOException e ) { log . error ( "Error creating archive file" , e ) ; } }
private PolicyFinder createPolicyFinder ( ) { logger . info ( "Creating Policy Finder" ) ; PolicyFinder policyFinder = new PolicyFinder ( ) ; PollingPolicyFinderModule policyFinderModule = new PollingPolicyFinderModule ( xacmlPolicyDirectories , defaultPollingIntervalInSeconds , securityLogger ) ; policyFinderModule . start ( ) ; Set < PolicyFinderModule > policyFinderModules = new HashSet < > ( 1 ) ; policyFinderModules . add ( policyFinderModule ) ; policyFinder . setModules ( policyFinderModules ) ; return policyFinder ; }
public void test() { try { DocFlavor flavor = new DocFlavor ( "application/vnd.cups-raster" , "[B" ) ; code_block = ForStatement ; log . error ( "No printer found" ) ; return false ; } catch ( PrintException e ) { log . error ( "Cannot print output" , e ) ; } }
public void test() { try { ObjectMapper mapper = new ObjectMapper ( ) ; orgLocationList = mapper . readValue ( orgLocation , List . class ) ; } catch ( Exception e ) { logger . info ( context , "Exception occurred while converting orgLocation to List<Map<String,String>>." ) ; } }
public void test() { for ( String cache : pools ) { code_block = IfStatement ; } }
@ Override public List < LwM2mObject > findLwM2mObject ( TenantId tenantId , String sortOrder , String sortProperty , String [ ] objectIds ) { logger . debug ( "findLwM2mObject()" ) ; validateId ( tenantId , INCORRECT_TENANT_ID + tenantId ) ; List < TbResource > resources = resourceService . findTenantResourcesByResourceTypeAndObjectIds ( tenantId , ResourceType . LWM2M_MODEL , objectIds ) ; return resources . stream ( ) . flatMap ( s -> Stream . ofNullable ( toLwM2mObject ( s , false ) ) ) . sorted ( getComparator ( sortProperty , sortOrder ) ) . collect ( Collectors . toList ( ) ) ; }
public void test() { if ( ! entry . isDirectory ( ) ) { final RequestMetaData metaData = new RequestMetaData ( entry . getFile ( ) . getLength ( ) , 2L , name ) ; final RequestIdentifier identifier = new RequestIdentifier ( uri ) ; FatFileIdentificationRequest req = new FatFileIdentificationRequest ( metaData , identifier , getTmpDir ( ) ) ; ByteBuffer buffer = ByteBuffer . allocate ( ( int ) entry . getFile ( ) . getLength ( ) ) ; entry . getFile ( ) . read ( 0 , buffer ) ; buffer . flip ( ) ; expandContainer ( req , new ByteArrayInputStream ( buffer . array ( ) , buffer . position ( ) , buffer . limit ( ) ) , newPath ) ; } else { log . debug ( "FILE: {}" , newPath ) ; } }
public void test() { if ( _log . isInfoEnabled ( ) ) { _log . info ( StringBundler . concat ( "Removing " , companyId , " from company " , companyId , " to " , company . getCompanyId ( ) , " using " , company . getCompanyId ( ) ) ) ; } }
public void test() { if ( pendingResp != null ) { CancelSmResp resp = pduDecomposer . cancelSmResp ( pdu ) ; pendingResp . done ( resp ) ; } else { logger . error ( "Unknown cancelSmResp {}" , pduHeader ) ; } }
public void test() { try ( Service service = new Service ( coreSettings ) ) { sendResponse ( service . execute ( serviceRequestFromHttpRequest ( request , requestType ) ) , response ) ; } catch ( Exception exc ) { LOGGER . error ( "Failed to execute service." , exc ) ; sendResponse ( new ServiceResponse < > ( 500 , exc . getMessage ( ) ) , response ) ; } }
public static void main ( String [ ] args ) { long startTime = 0 ; long endTime = 0 ; long aggregatorStart = Calendar . getInstance ( ) . getTimeInMillis ( ) ; long longest = 0 ; code_block = IfStatement ; String longQuery = null ; String cluster = System . getProperty ( "CLUSTER" ) ; code_block = IfStatement ; String queries = Aggregator . getContents ( new File ( System . getenv ( "CHUKWA_CONF_DIR" ) + File . separator + "aggregator.sql" ) ) ; log . info ( "Aggregator started." ) ; String [ ] query = queries . split ( "\n" ) ; code_block = WhileStatement ; long aggregatorEnd = Calendar . getInstance ( ) . getTimeInMillis ( ) ; log . info ( "Longest running query: " + longQuery + " (" + ( double ) longest / 1000 + " seconds)" ) ; log . info ( "Total running time: (" + ( double ) ( aggregatorEnd - aggregatorStart ) / 1000 + " seconds)" ) ; log . info ( "Aggregator finished." ) ; }
public void test() { if ( query [ i ] . indexOf ( "" ) == 0 ) { logger . error ( "Unable to execute query '" + query [ i ] + "'" ) ; } else-if ( ! query [ i ] . equals ( "" ) ) { Aggregator dba = new Aggregator ( ) ; dba . setWriter ( new DatabaseWriter ( cluster ) ) ; long start = Calendar . getInstance ( ) . getTimeInMillis ( ) ; code_block = TryStatement ;  long end = Calendar . getInstance ( ) . getTimeInMillis ( ) ; long duration = end - start ; code_block = IfStatement ; } }
public void test() { try { code_block = IfStatement ; } catch ( Throwable e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public static void main ( String [ ] args ) { long startTime = 0 ; long endTime = 0 ; long aggregatorStart = Calendar . getInstance ( ) . getTimeInMillis ( ) ; long longest = 0 ; code_block = IfStatement ; String longQuery = null ; log . info ( "Aggregator started." ) ; String cluster = System . getProperty ( "CLUSTER" ) ; code_block = IfStatement ; String queries = Aggregator . getContents ( new File ( System . getenv ( "CHUKWA_CONF_DIR" ) + File . separator + "aggregator.sql" ) ) ; log . info ( "Queries=" + queries ) ; String [ ] query = queries . split ( "\n" ) ; code_block = WhileStatement ; long aggregatorEnd = Calendar . getInstance ( ) . getTimeInMillis ( ) ; log . info ( "Total running time: (" + ( double ) ( aggregatorEnd - aggregatorStart ) / 1000 + " seconds)" ) ; log . info ( "Aggregator finished." ) ; }
public static void main ( String [ ] args ) { long startTime = 0 ; long endTime = 0 ; long aggregatorStart = Calendar . getInstance ( ) . getTimeInMillis ( ) ; log . info ( "Aggregator started." ) ; long longest = 0 ; code_block = IfStatement ; String longQuery = null ; log . info ( "Aggregator started." ) ; String cluster = System . getProperty ( "CLUSTER" ) ; code_block = IfStatement ; String queries = Aggregator . getContents ( new File ( System . getenv ( "CHUKWA_CONF_DIR" ) + File . separator + "aggregator.sql" ) ) ; String [ ] query = queries . split ( "\n" ) ; code_block = WhileStatement ; long aggregatorEnd = Calendar . getInstance ( ) . getTimeInMillis ( ) ; log . info ( "Longest running query: " + longQuery + " (" + ( double ) longest / 1000 + " seconds)" ) ; log . info ( "Aggregator finished." ) ; }
public static void main ( String [ ] args ) { long startTime = 0 ; long endTime = 0 ; long aggregatorStart = Calendar . getInstance ( ) . getTimeInMillis ( ) ; long longest = 0 ; code_block = IfStatement ; String longQuery = null ; log . info ( "Aggregator started." ) ; String cluster = System . getProperty ( "CLUSTER" ) ; code_block = IfStatement ; String queries = Aggregator . getContents ( new File ( System . getenv ( "CHUKWA_CONF_DIR" ) + File . separator + "aggregator.sql" ) ) ; log . info ( "Aggregator completed." ) ; String [ ] query = queries . split ( "\n" ) ; code_block = WhileStatement ; long aggregatorEnd = Calendar . getInstance ( ) . getTimeInMillis ( ) ; log . info ( "Longest running query: " + longQuery + " (" + ( double ) longest / 1000 + " seconds)" ) ; log . info ( "Total running time: (" + ( double ) ( aggregatorEnd - aggregatorStart ) / 1000 + " seconds)" ) ; }
public void test() { if ( replyTo != null ) { LOG . trace ( "Using JMSReplyTo: {}" , replyTo ) ; JmsMessageHelper . setJMSReplyTo ( answer , replyTo ) ; } else { LOG . trace ( "Not using JMSReplyTo" ) ; JmsMessageHelper . setJMSReplyTo ( answer , null ) ; } }
public void test() { if ( replyTo != null ) { LOG . debug ( "Using JMSReplyTo destination: {}" , replyTo ) ; JmsMessageHelper . setJMSReplyTo ( answer , replyTo ) ; } else { LOG . warn ( "Using JMSReplyTo destination: null" ) ; JmsMessageHelper . setJMSReplyTo ( answer , null ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( SegmentsEntryServiceUtil . class , "addSegmentsEntryClassPKs" , _addSegmentsEntryClassPKsParameterTypes2 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , segmentsEntryId , classPKs , serviceContext ) ; code_block = TryStatement ;  } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { return ( int ) index . count ( ) ; } catch ( SeriesServiceDatabaseException e ) { logger . error ( "Failed to get all series instances" , e ) ; throw new SeriesException ( e ) ; } }
public void test() { try ( FileInputStream fis = new FileInputStream ( configFile ) ) { Properties props = new Properties ( ) ; props . load ( fis ) ; code_block = ForStatement ; } catch ( IOException e ) { LOGGER . error ( "Error loading configuration files" , e ) ; } }
public void test() { try ( BufferedWriter writer = new BufferedWriter ( new FileWriter ( configFile ) ) ) { writer . write ( "mapHeight=512\n" ) ; writer . write ( "mapWidth=1024\n" ) ; writer . write ( "proxy=http://proxyUrlGoesHere/\n" ) ; writer . write ( " Definition of user layers\n" ) ; writer . write ( " First we define a layer with ID \"user\"\n" ) ; writer . write ( " The URL of the WMS.  MANDATORY\n" ) ; writer . write ( "userURL=http://wmsurl.com/with/query/separator/wms?\n" ) ; writer . write ( " The Title for the layer switcher. Defaults to the ID (i.e. \"user\" in this case)\n" ) ; writer . write ( "userTitle=My WMS Layer\n" ) ; writer . write ( " The layer(s) to plot.  MANDATORY\n" ) ; writer . write ( "userLayers=wmslayer1,wmslayer2\n" ) ; writer . write ( " The projection to use.  Defaults to \"CRS:84\"\n" ) ; writer . write ( "userProjection=EPSG:4326\n" ) ; writer . write ( " The WMS version to use.  Defaults to \"1.1.1\"\n" ) ; writer . write ( "userVersion=1.3.0\n" ) ; writer . write ( " The image format to use.  Defaults to \"image/png\"\n" ) ; writer . write ( "userFormat=image/png\n" ) ; writer . write ( " Whether a layer should be switched on at load.  Can be defined for multiple overlays, but only for one base layer\n" ) ; writer . write ( "userOnByDefault=true\n" ) ; writer . write ( " Whether to use as an overlay (as opposed to a base map).  Defaults to false\n" ) ; writer . write ( "userIsOverlay=true\n" ) ; writer . close ( ) ; } catch ( IOException e ) { LOGGER . warn ( "IOException writing
@ Override public RemoveRepositorySourceMirrorResult removeRepositorySourceMirror ( RemoveRepositorySourceMirrorRequest request ) { Preconditions . checkArgument ( null != request , "the request is required" ) ; Preconditions . checkArgument ( StringUtils . isNotBlank ( request . code ) , "the code is required on the request" ) ; LOGGER . info ( "removeRepositorySourceMirror {}" , request . code ) ; final ObjectContext context = serverRuntime . newContext ( ) ; RepositorySourceMirror repositorySourceMirror = RepositorySourceMirror . tryGetByCode ( context , request . code ) . orElseThrow ( ( ) -> new ObjectNotFoundException ( RepositorySourceMirror . class . getSimpleName ( ) , request . code ) ) ; code_block = IfStatement ; repositorySourceMirror . getRepositorySource ( ) . getRepository ( ) . setModifyTimestamp ( ) ; context . deleteObject ( repositorySourceMirror ) ; context . commitChanges ( ) ; return new RemoveRepositorySourceMirrorResult ( ) ; }
private void rejectPermission ( int personId ) { log . debug ( "Checking Person - user name = " + user . getUsername ( ) ) ; Person user = personDao . read ( personId ) ; log . debug ( "Loaded Person - user name = " + user . getUsername ( ) ) ; log . debug ( "Setting the authority to ROLE_READER" ) ; user . setAuthority ( "ROLE_READER" ) ; log . debug ( "Updating Person in database" ) ; personDao . update ( user ) ; String email = user . getEmail ( ) ; sendEmail ( email , false ) ; }
private void rejectPermission ( int personId ) { log . debug ( "Loading Person object from database" ) ; Person user = personDao . read ( personId ) ; log . debug ( "Setting the authority to ROLE_READER" ) ; user . setAuthority ( "ROLE_READER" ) ; log . debug ( "Saving Person in database" ) ; personDao . update ( user ) ; String email = user . getEmail ( ) ; sendEmail ( email , false ) ; }
private void rejectPermission ( int personId ) { log . debug ( "Loading Person object from database" ) ; Person user = personDao . read ( personId ) ; log . debug ( "Loaded Person - user name = " + user . getUsername ( ) ) ; user . setAuthority ( "ROLE_READER" ) ; log . debug ( "Updating Person in database" ) ; personDao . update ( user ) ; String email = user . getEmail ( ) ; log . debug ( "Sending email to " + email ) ; sendEmail ( email , false ) ; }
private void rejectPermission ( int personId ) { log . debug ( "Loading Person object from database" ) ; Person user = personDao . read ( personId ) ; log . debug ( "Loaded Person - user name = " + user . getUsername ( ) ) ; log . debug ( "Setting the authority to ROLE_READER" ) ; user . setAuthority ( "ROLE_READER" ) ; personDao . update ( user ) ; String email = user . getEmail ( ) ; log . debug ( "Saved user to email: " + email ) ; sendEmail ( email , false ) ; }
public void test() { try { stat = conn . prepareStatement ( DELETE_ITEM ) ; stat . setString ( 1 , InitializerManager . REPORT_CONFIG_ITEM ) ; stat . setString ( 2 , version ) ; stat . executeUpdate ( ) ; } catch ( Throwable t ) { _logger . error ( "Error deleting item" , t ) ; throw new RuntimeException ( "Error deleting item" , t ) ; } finally { closeDaoResources ( null , stat ) ; } }
@ Test public void parseTestAbbreviation_Array ( ) throws IOException { BsonValue c = new BsonBoolean ( true ) ; BsonDocument document = new BsonDocument ( ) . append ( "double" , new BsonDouble ( 12.3 ) ) . append ( "arrayInt" , new BsonArray ( Arrays . asList ( c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c ) ) ) ; BasicDBObject query = new BasicDBObject ( ) ; query . put ( "ComplexBson" , document ) ; logger . debug ( "query: {}" , query ) ; NormalizedBson stringStringValue = MongoUtil . parseBson ( new Object [ ] code_block = "" ; , true ) ; logger . debug ( "val:{}" , stringStringValue ) ; List list = objectMapper . readValue ( "[" + stringStringValue . getNormalizedBson ( ) + "]" , List . class ) ; Assert . assertEquals ( list . size ( ) , 1 ) ; Map < String , ? > query1Map = ( Map < String , ? > ) list . get ( 0 ) ; checkValue ( query1Map ) ; }
@ Test public void parseTestAbbreviation_Array ( ) throws IOException { BsonValue c = new BsonBoolean ( true ) ; BsonDocument document = new BsonDocument ( ) . append ( "double" , new BsonDouble ( 12.3 ) ) . append ( "arrayInt" , new BsonArray ( Arrays . asList ( c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c ) ) ) ; BasicDBObject query = new BasicDBObject ( ) ; query . put ( "ComplexBson" , document ) ; logger . debug ( "document:{}" , document ) ; NormalizedBson stringStringValue = MongoUtil . parseBson ( new Object [ ] code_block = "" ; , true ) ; logger . debug ( "query: {}" , query ) ; List list = objectMapper . readValue ( "[" + stringStringValue . getNormalizedBson ( ) + "]" , List . class ) ; Assert . assertEquals ( list . size ( ) , 1 ) ; Map < String , ? > query1Map = ( Map < String , ? > ) list . get ( 0 ) ; checkValue ( query1Map ) ; }
public void test() { try { poolEntry . sendMessage ( message , recipients ) ; } catch ( RuntimeException | MessagingException e1 ) { LOGGER . error ( "Error sending message" , e1 ) ; throw e1 ; } }
@ Override public void addParentOfferings ( final String offering , final Collection < String > parentOfferings ) { CacheValidation . notNullOrEmpty ( OFFERING , offering ) ; CacheValidation . noNullOrEmptyValues ( PARENT_OFFERINGS , parentOfferings ) ; LOG . trace ( "Adding parent offering {} to offering {}" , offering , parentOfferings ) ; this . parentOfferingsForOfferings . computeIfAbsent ( offering , createSynchronizedSet ( ) ) . addAll ( parentOfferings ) ; parentOfferings . forEach ( parentOffering -> this . childOfferingsForOfferings . computeIfAbsent ( parentOffering , createSynchronizedSet ( ) ) . add ( offering ) ) ; }
@ Test public void alertDetailsMessageTime ( ) { alertingService . addAlertListener ( member , SEVERE ) ; logger . fatal ( alertMessage ) ; assertThat ( captureAlertDetails ( ) . getMsgTime ( ) ) . isNotNull ( ) ; }
public void test() { try { clientRequest . setHttpMethod ( getHttpMethod ( ) ) ; clientRequest . header ( "Content-Type" , getRequest ( ) . getContentType ( ) ) ; code_block = IfStatement ; JSONObject requestBody = getRequest ( ) . getJSONParameters ( ) ; clientRequest . body ( MediaType . APPLICATION_JSON , requestBody . toString ( 4 ) ) ; clientResponse = clientRequest . post ( String . class ) ; setResponse ( new PushErrorResponse ( clientResponse ) ) ; } catch ( Exception e ) { logger . error ( "Unexpected error occured" , e ) ; } finally { closeConnection ( ) ; } }
public void test() { try ( Response response = make ( request ) ) { check ( response ) ; return RequestResponse . parseFromResponse ( response ) ; } catch ( VitamClientInternalException e ) { throw new AccessExternalClientServerException ( e ) ; } catch ( AdminExternalClientException e ) { LOGGER . error ( e ) ; return e . getVitamError ( ) ; } }
public void test() { if ( LOG . isWarnEnabled ( ) ) { LOG . warn ( "Unhandled exception: " , e ) ; } }
public void test() { if ( hadoopPolicyList . size ( ) == 0 ) { logger . info ( "No Hive unique policy found." ) ; } else-if ( hadoopPolicyList . size ( ) > 1 ) { throw new RuntimeException ( "Unable to find Hive unique policy." ) ; } else { code_block = ForStatement ; } }
public void test() { try { rangerRestClient . deletePolicy ( hadoopPolicy . getId ( ) ) ; } catch ( Exception e ) { LOG . error ( "Unable to delete policy to " + rangerHivePolicyName , e ) ; throw new RuntimeException ( "Unable to delete policy to " + rangerHivePolicyName , e ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { BufferedReader reader = new BufferedReader ( new InputStreamReader ( req . getInputStream ( ) , "UTF-8" ) ) ; String line ; StringBuilder builder = new StringBuilder ( ) ; code_block = WhileStatement ; reader . close ( ) ; LOG . debug ( "[RequestBody]  " + httpreq . getRequestURL ( ) ) ; LOG . debug ( builder . toString ( ) ) ; } }
public void test() { if ( suspendMinorCompaction . compareAndSet ( true , false ) ) { logger . info ( "Suspending minor compaction." ) ; } }
@ Override protected void implCloseChannel ( ) throws IOException { log . debug ( "Close channel" ) ; }
public void test() { try { SessionObject sesObj = ( SessionObject ) request . getSession ( ) . getAttribute ( FdahpStudyDesignerConstants . SESSION_OBJECT ) ; int oldOrderNumber ; int newOrderNumber ; code_block = IfStatement ; jsonobject . put ( FdahpStudyDesignerConstants . MESSAGE , message ) ; response . setContentType ( FdahpStudyDesignerConstants . APPLICATION_JSON ) ; out = response . getWriter ( ) ; out . print ( jsonobject ) ; } catch ( Exception e ) { logger . error ( "StudyController - savePage - ERROR" , e ) ; } }
@ Test @ InRequestScope public void testExecuteToGetAll ( ) throws Exception { GetTransUnitList action = GetTransUnitList . newAction ( new GetTransUnitActionContext ( document ) ) ; prepareActionAndMockLocaleService ( action ) ; long startTime = System . nanoTime ( ) ; log . info ( action . toString ( ) ) ; GetTransUnitListResult result = handler . execute ( action , null ) ; log . info ( "result: {}" , result ) ; assertThat ( result . getDocumentId ( ) ) . isEqualTo ( document . getId ( ) ) ; assertThat ( result . getGotoRow ( ) ) . isEqualTo ( 0 ) ; assertThat ( getIntIds ( result . getUnits ( ) ) ) . contains ( 1 , 2 , 3 , 4 , 5 ) ; }
@ Test @ InRequestScope public void testExecuteToGetAll ( ) throws Exception { GetTransUnitList action = GetTransUnitList . newAction ( new GetTransUnitActionContext ( document ) ) ; prepareActionAndMockLocaleService ( action ) ; long startTime = System . nanoTime ( ) ; GetTransUnitListResult result = handler . execute ( action , null ) ; log . info ( "********** result :{} second" , ( System . nanoTime ( ) - startTime ) / 1.0E9 ) ; log . info ( "********** result :{}" , result . getDocumentId ( ) ) ; assertThat ( result . getDocumentId ( ) ) . isEqualTo ( document . getId ( ) ) ; assertThat ( result . getGotoRow ( ) ) . isEqualTo ( 0 ) ; assertThat ( getIntIds ( result . getUnits ( ) ) ) . contains ( 1 , 2 , 3 , 4 , 5 ) ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
private void testNoteCreate ( String noteName ) throws IOException { String jsonRequest = "{\"name\":\"" + noteName + "\"}" ; CloseableHttpResponse post = httpPost ( "/notebook/" , jsonRequest ) ; String postResponse = EntityUtils . toString ( post . getEntity ( ) , StandardCharsets . UTF_8 ) ; LOG . info ( "test note create method: " + postResponse ) ; assertThat ( "test note create method:" , post , isAllowed ( ) ) ; Map < String , Object > resp = gson . fromJson ( postResponse , new TypeToken < Map < String , Object > > ( ) code_block = "" ; . getType ( ) ) ; String newNoteId = ( String ) resp . get ( "body" ) ; LOG . info ( "newNoteId:=" + newNoteId ) ; Note newNote = TestUtils . getInstance ( Notebook . class ) . getNote ( newNoteId ) ; assertNotNull ( "Can not find new note by id" , newNote ) ; String newNoteName = newNote . getName ( ) ; LOG . info ( "new note name is: " + newNoteName ) ; code_block = IfStatement ; assertEquals ( "compare note name" , noteName , newNoteName ) ; TestUtils . getInstance ( Notebook . class ) . removeNote ( newNote , anonymous ) ; post . close ( ) ; }
private void testNoteCreate ( String noteName ) throws IOException { String jsonRequest = "{\"name\":\"" + noteName + "\"}" ; CloseableHttpResponse post = httpPost ( "/notebook/" , jsonRequest ) ; String postResponse = EntityUtils . toString ( post . getEntity ( ) , StandardCharsets . UTF_8 ) ; LOG . info ( "testNoteCreate \n" + postResponse ) ; assertThat ( "test note create method:" , post , isAllowed ( ) ) ; Map < String , Object > resp = gson . fromJson ( postResponse , new TypeToken < Map < String , Object > > ( ) code_block = "" ; . getType ( ) ) ; String newNoteId = ( String ) resp . get ( "body" ) ; LOG . info ( "new note id is: " + newNoteId ) ; Note newNote = TestUtils . getInstance ( Notebook . class ) . getNote ( newNoteId ) ; assertNotNull ( "Can not find new note by id" , newNote ) ; String newNoteName = newNote . getName ( ) ; LOG . info ( "new note name is: " + newNoteName ) ; code_block = IfStatement ; assertEquals ( "compare note name" , noteName , newNoteName ) ; TestUtils . getInstance ( Notebook . class ) . removeNote ( newNote , anonymous ) ; post . close ( ) ; }
private void testNoteCreate ( String noteName ) throws IOException { String jsonRequest = "{\"name\":\"" + noteName + "\"}" ; LOG . info ( "testNoteCreate \n" + jsonRequest ) ; CloseableHttpResponse post = httpPost ( "/notebook/" , jsonRequest ) ; String postResponse = EntityUtils . toString ( post . getEntity ( ) , StandardCharsets . UTF_8 ) ; LOG . info ( "testNoteCreate \n" + postResponse ) ; assertThat ( "test note create method:" , post , isAllowed ( ) ) ; Map < String , Object > resp = gson . fromJson ( postResponse , new TypeToken < Map < String , Object > > ( ) code_block = "" ; . getType ( ) ) ; String newNoteId = ( String ) resp . get ( "body" ) ; LOG . info ( "newNoteId:=" + newNoteId ) ; Note newNote = TestUtils . getInstance ( Notebook . class ) . getNote ( newNoteId ) ; assertNotNull ( "Can not find new note by id" , newNote ) ; String newNoteName = newNote . getName ( ) ; code_block = IfStatement ; assertEquals ( "compare note name" , noteName , newNoteName ) ; TestUtils . getInstance ( Notebook . class ) . removeNote ( newNote , anonymous ) ; post . close ( ) ; }
public void test() { try { @ SuppressWarnings ( "unchecked" ) LayoutBuilder builder = ( LayoutBuilder ) LoaderUtil . newInstanceOf ( plugin . getPluginClass ( ) ) ; return builder . parseLayout ( layoutElement , config ) ; } catch ( InstantiationException | IllegalAccessException | InvocationTargetException ex ) { getLogger ( ) . error ( "Unable to load layout element" , ex ) ; } }
public void test() { try { List < Rule > rules = ruleRepository . findAll ( ) ; List < JobExecutionManager > jobs = jobRepository . findAll ( ) ; boolean rulesEnabled = false ; boolean jobsEnabled = false ; code_block = ForStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; return status ; } catch ( Exception e ) { logger . error ( "Error in fetching the status of system" , e ) ; throw new PacManException ( "Error in fetching the status of system" ) ; } }
public void test() { if ( curr > segment . getDateRangeEnd ( ) && delta > segmentManager . cubeDuration ) { logger . debug ( "Make {} immutable because it lastUpdate[{}] exceed wait duration." , segment . getSegmentName ( ) , segment . getLastUpdateTime ( ) ) ; segmentManager . makeSegmentImmutable ( segment . getSegmentName ( ) ) ; } }
public void test() { if ( ! segments . isEmpty ( ) ) { logger . info ( "found cube {} segments: {}" , cubeName , segments ) ; } else { continue ; } }
public void test() { try { Collection < StreamingCubeSegment > activeSegments = segmentManager . getActiveSegments ( ) ; code_block = ForStatement ; RetentionPolicyInfo retentionPolicyInfo = new RetentionPolicyInfo ( ) ; String policyName = cubeInstance . getConfig ( ) . getStreamingSegmentRetentionPolicy ( ) ; Map < String , String > policyProps = cubeInstance . getConfig ( ) . getStreamingSegmentRetentionPolicyProperties ( policyName ) ; retentionPolicyInfo . setName ( policyName ) ; retentionPolicyInfo . setProperties ( policyProps ) ; Collection < StreamingCubeSegment > segments = segmentManager . getRequireRemotePersistSegments ( ) ; code_block = IfStatement ; handleImmutableCubeSegments ( cubeName , segmentManager , segments , retentionPolicyInfo ) ; } catch ( Exception e ) { logger . error ( "error when handle cube:" + cubeName , e ) ; } }
public void test() { try { Files . copy ( secureStoragePathCopy , secureStoragePath , REPLACE_EXISTING ) ; } catch ( IOException e ) { logger . warn ( "Unable to write to file " + secureStoragePath , e ) ; } }
@ Override public void onSessionInitiated ( final BindingAwareBroker . ProviderContext providerContext ) { LOG . info ( "Started" ) ; final EPPolicyTemplateProviderFacade templateProviderFacade = new EPPolicyTemplateProviderIseImpl ( ) ; epPolicyTemplateProviderRegistration = sxpEpProvider . getEPPolicyTemplateProviderRegistry ( ) . registerTemplateProvider ( templateProviderFacade ) ; final SgtInfoProcessor epgGenerator = new SgtToEpgGeneratorImpl ( dataBroker ) ; final SgtInfoProcessor templateGenerator = new SgtToEPTemplateGeneratorImpl ( dataBroker ) ; final GbpIseSgtHarvester gbpIseSgtHarvester = new GbpIseSgtHarvesterImpl ( epgGenerator , templateGenerator ) ; final GbpIseConfigListenerImpl gbpIseConfigListener = new GbpIseConfigListenerImpl ( dataBroker , gbpIseSgtHarvester , templateProviderFacade ) ; templateProviderFacade . setIseSgtHarvester ( gbpIseSgtHarvester ) ; final DataTreeIdentifier < IseSourceConfig > dataTreePath = new DataTreeIdentifier < > ( LogicalDatastoreType . CONFIGURATION , InstanceIdentifier . create ( GbpSxpIseAdapter . class ) . child ( IseSourceConfig . class ) ) ; registration = dataBroker . registerDataTreeChangeListener ( dataTreePath , gbpIseConfigListener ) ; LOG . info ( "Started" ) ; }
@ Override public void onSessionInitiated ( final BindingAwareBroker . ProviderContext providerContext ) { LOG . info ( "Starting GbpIseAdapterProvider .." ) ; final EPPolicyTemplateProviderFacade templateProviderFacade = new EPPolicyTemplateProviderIseImpl ( ) ; epPolicyTemplateProviderRegistration = sxpEpProvider . getEPPolicyTemplateProviderRegistry ( ) . registerTemplateProvider ( templateProviderFacade ) ; final SgtInfoProcessor epgGenerator = new SgtToEpgGeneratorImpl ( dataBroker ) ; final SgtInfoProcessor templateGenerator = new SgtToEPTemplateGeneratorImpl ( dataBroker ) ; final GbpIseSgtHarvester gbpIseSgtHarvester = new GbpIseSgtHarvesterImpl ( epgGenerator , templateGenerator ) ; final GbpIseConfigListenerImpl gbpIseConfigListener = new GbpIseConfigListenerImpl ( dataBroker , gbpIseSgtHarvester , templateProviderFacade ) ; templateProviderFacade . setIseSgtHarvester ( gbpIseSgtHarvester ) ; LOG . info ( "Starting GbpIseAdapterProvider .." ) ; final DataTreeIdentifier < IseSourceConfig > dataTreePath = new DataTreeIdentifier < > ( LogicalDatastoreType . CONFIGURATION , InstanceIdentifier . create ( GbpSxpIseAdapter . class ) . child ( IseSourceConfig . class ) ) ; registration = dataBroker . registerDataTreeChangeListener ( dataTreePath , gbpIseConfigListener ) ; }
public void test() { try ( Tx tx = StructrApp . getInstance ( ) . tx ( ) ) { size = page . getContent ( RenderContext . EditMode . RAW ) . length ( ) ; tx . success ( ) ; } catch ( FrameworkException fex ) { logger . error ( "" , fex ) ; } }
@ Test public void testAggregateProcessDefinitions ( ) throws Exception { String xml1 = read ( this . getClass ( ) . getResourceAsStream ( "/jaxb/process-def-1.xml" ) ) ; String xml2 = read ( this . getClass ( ) . getResourceAsStream ( "/jaxb/process-def-2.xml" ) ) ; JaxbXMLResponseAggregator aggregate = new JaxbXMLResponseAggregator ( ) ; List < String > data = new ArrayList < > ( ) ; data . add ( xml1 ) ; data . add ( xml2 ) ; String result = aggregate . aggregate ( data ) ; logger . debug ( result ) ; Document xml = toXml ( result ) ; assertNotNull ( xml ) ; NodeList processes = xml . getElementsByTagName ( "process-definitions" ) ; assertNotNull ( processes ) ; assertEquals ( 1 , processes . getLength ( ) ) ; NodeList processDefs = xml . getElementsByTagName ( "processes" ) ; assertNotNull ( processDefs ) ; assertEquals ( 5 , processDefs . getLength ( ) ) ; }
public void attachDirty ( StgSysExport instance ) { log . debug ( "attaching dirty StgSysExport instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { this . by = getBySelector ( locator ) ; this . wait = new WebDriverWait ( driver , TIMEOUT ) ; log . info ( "Waiting:*************until the element is visible  ***********" ) ; } catch ( Exception sc ) { log . info ( "-----waitForElementPresent timeup------" ) ; sc . printStackTrace ( ) ; log . info ( "-----waitForElementPresent timeup------" ) ; } }
public void test() { try { log . info ( "Entering:*********waitForElementPresent()******" ) ; this . by = getBySelector ( locator ) ; this . wait = new WebDriverWait ( driver , TIMEOUT ) ; } catch ( Exception sc ) { log . info ( "-----waitForElementPresent timeup------" ) ; sc . printStackTrace ( ) ; log . info ( sc . getMessage ( ) ) ; } }
public void test() { try { log . info ( "Entering:*********waitForElementPresent()******" ) ; this . by = getBySelector ( locator ) ; log . info ( "Entering:*********waitForElementPresent()******" ) ; this . wait = new WebDriverWait ( driver , TIMEOUT ) ; log . info ( "Waiting:*************until the element is visible  ***********" ) ; } catch ( Exception sc ) { sc . printStackTrace ( ) ; } }
public void test() { if ( ! ( dataset . getDefaultRemoveGraphs ( ) . equals ( dataset . getDefaultGraphs ( ) ) ) ) { LOGGER . warning ( "The default remove graph '" + dataset . getDefaultRemoveGraphs ( ) + "' doesn't match graphs '" + dataset . getDefaultGraphs ( ) + "'" ) ; } }
public void test() { if ( ! dataset . getDefaultInsertGraph ( ) . equals ( graphURI ) ) { LOG . warn ( "The default insert graph is different from the '{}'" , graphURI ) ; } }
public void test() { if ( ! ( dataset . getDefaultGraphs ( ) . size ( ) == 1 && dataset . getDefaultGraphs ( ) . contains ( dataset . getDefaultInsertGraph ( ) ) ) ) { LOGGER . warn ( "No default graph found in dataset: {}" , dataset ) ; } }
public void test() { try { final Owner ownerObj = getOwner ( entity ) ; final EntityReference owner = ownerObj == null ? null : ownerObj . getUser ( ) ; final Collection < Collaborator > collaborators = getCollaborators ( entity ) ; final Set < DocumentReference > processedEntities = new HashSet < > ( ) ; final Queue < DocumentReference > entitiesToCheck = new LinkedList < > ( ) ; entitiesToCheck . add ( ( DocumentReference ) userOrGroup ) ; AccessLevel currentItemAccess ; DocumentReference currentItem ; final XWikiContext context = this . xcontextProvider . get ( ) ; final XWikiGroupService groupService = context . getWiki ( ) . getGroupService ( context ) ; code_block = WhileStatement ; } catch ( final XWikiException ex ) { this . logger . warn ( "Failed to check entities in [{}]: {}" , entity , ex . getMessage ( ) ) ; } }
byte [ ] readWithChecksum ( int length ) throws Exception { byte [ ] b = new byte [ length ] ; code_block = ForStatement ; int checksum = read ( false ) ; StringBuffer sb = new StringBuffer ( ) ; code_block = ForStatement ; sb . append ( String . format ( "%02x" , checksum & 0xff ) ) ; LOGGER . info ( sb . toString ( ) ) ; return b ; }
public void test() { try { File inFile = new File ( logDir , strLogFileName ) ; HashMap hp = FileUtil . getRandomAccessFileView ( inFile , Integer . parseInt ( startLen ) , Integer . parseInt ( seek ) , 0 ) ; socketLogger . info ( "strFileView : " + hp . get ( "file_desc" ) ) ; outputObj . put ( ProtocolID . DX_EX_CODE , strDxExCode ) ; outputObj . put ( ProtocolID . RESULT_CODE , strSuccessCode ) ; outputObj . put ( ProtocolID . ERR_CODE , strErrCode ) ; outputObj . put ( ProtocolID . ERR_MSG , strErrMsg ) ; outputObj . put ( ProtocolID . RESULT_DATA , hp . get ( "file_desc" ) ) ; inFile = null ; send ( TotalLengthBit , outputObj . toString ( ) . getBytes ( ) ) ; } catch ( Exception e ) { errLogger . error ( e . toString ( ) ) ; outputObj . put ( ProtocolID . DX_EX_CODE , TranCodeType . DxT031 ) ; outputObj . put ( ProtocolID . RESULT_CODE , "1" ) ; outputObj . put ( ProtocolID . ERR_CODE , TranCodeType . DxT031 ) ; outputObj . put ( ProtocolID . ERR_MSG , "DxT031 Error [" + e . toString ( ) + "]" ) ; sendBuff = outputObj . toString ( ) . getBytes ( ) ; send ( 4 , sendBuff ) ; } finally { outputObj = null ; sendBuff = null ; } }
public void test() { try { BaseQueryMetric metric = rq . getMetric ( ) ; code_block = SwitchStatement ; if ( queryMetricsBean != null ) queryMetricsBean . updateMetric ( metric ) ; else log . error ( "QueryMetricsBean JNDI lookup returned null" ) ; } catch ( Exception e ) { log . error ( "Error updating metric with name: " + name , e ) ; } }
public void test() { if ( rq != null ) { BaseQueryLogic < ? > baseLogic = null ; QueryLogic < ? > logic = rq . getLogic ( ) ; code_block = IfStatement ; code_block = IfStatement ; } else { LOG . warn ( "Received an unexpected request logic: {}" , queryLog ) ; } }
public void test() { if ( queryCache != null ) { RunningQuery rq = queryCache . get ( queryCall . queryID ) ; code_block = IfStatement ; } else { LOG . trace ( "Cannot evict query cache for {}" , queryCall . queryID ) ; } }
public void test() { try { doDeployTarballResource ( url , targetName ) ; } catch ( RuntimeException e ) { LOG . error ( "Error deploying tarball: " + url , e ) ; throw Throwables . propagate ( e ) ; } }
public void delete ( StgMsUnjTxt persistentInstance ) { log . debug ( "deleting StgMsUnjTxt instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . delete ( persistentInstance ) ; log . debug ( "delete successful" ) ; } catch ( RuntimeException re ) { log . error ( "delete failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . delete ( persistentInstance ) ; log . debug ( "delete successful" ) ; } catch ( RuntimeException re ) { log . error ( "delete failed" , re ) ; throw re ; } }
public void test() { if ( context . getRetryCount ( ) > 0 ) { long mapExactMatches = mapExactMatches ( assetExtractionId , tmId , assetId ) ; logger . error ( "Assume concurrent modification happened, perform remapping: {}" , mapExactMatches ) ; } }
public void test() { if ( prevResult . equals ( streamTaskResult ) ) { log . error ( i + ": Did not get the same results for two Stream tasks runs" ) ; } else { log . error ( i + ": Did not get the same results for two Stream tasks runs" ) ; break ; } }
public void test() { if ( s instanceof FlowVariable ) { FlowVariable v = ( FlowVariable ) s ; NodeSettingsWO sub = stackSet . addNodeSettings ( "Variable_" + c ) ; sub . addString ( "type" , "variable" ) ; v . save ( sub ) ; } else-if ( s instanceof FlowLoopContext ) { code_block = IfStatement ; } else-if ( s instanceof InnerFlowLoopContext ) { NodeSettingsWO sub = stackSet . addNodeSettings ( "Loop_Execute_" + c ) ; sub . addString ( "type" , "loopcontext_execute" ) ; } else-if ( s instanceof FlowCaptureContext ) { code_block = IfStatement ; } else-if ( s instanceof FlowScopeContext ) { code_block = IfStatement ; } else { logger . warn ( "Unknown type: " + s ) ; } }
public void test() { try { record = create . selectFrom ( Tables . PROJECT ) . where ( Tables . PROJECT . ID . equal ( idProject ) ) . fetchOne ( ) ; code_block = IfStatement ; } catch ( org . jooq . exception . DataAccessException sqex ) { LOG . error ( "Project not found" ) ; return null ; } catch ( Exception ex ) { LOG . error ( "Unexpected exception retrieving a project record" ) ; LOG . error ( "Error Message: {}" , ex . getMessage ( ) ) ; return null ; } }
public void test() { try { record = create . selectFrom ( Tables . PROJECT ) . where ( Tables . PROJECT . ID . equal ( idProject ) ) . fetchOne ( ) ; code_block = IfStatement ; } catch ( org . jooq . exception . DataAccessException sqex ) { LOG . error ( "Database error encountered {}" , sqex . getMessage ( ) ) ; return null ; } catch ( Exception ex ) { LOG . error ( "Error Code: {}" , ex . getCode ( ) ) ; LOG . error ( "Error Message: {}" , ex . getMessage ( ) ) ; return null ; } }
public void test() { if ( minEvictableIdleTimeMillis < 1000 * 30 ) { LOG . debug ( "Purged {} idle for {}" , minEvictableIdleTimeMillis , containerName ) ; } }
public void test() { try { com . liferay . commerce . pricing . model . CommercePricingClassCPDefinitionRel returnValue = CommercePricingClassCPDefinitionRelServiceUtil . fetchCommercePricingClassCPDefinitionRel ( commercePricingClassId , cpDefinitionId ) ; return com . liferay . commerce . pricing . model . CommercePricingClassCPDefinitionRelSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try ( Connection connection = this . databaseSpring . connect ( ) ; PreparedStatement preStat = connection . prepareStatement ( query . toString ( ) ) ; ) { int i = 1 ; preStat . setString ( i ++ , testCaseCountryProperties . getTest ( ) ) ; preStat . setString ( i ++ , testCaseCountryProperties . getTestcase ( ) ) ; preStat . setBytes ( i ++ , testCaseCountryProperties . getProperty ( ) . getBytes ( "UTF-8" ) ) ; preStat . setString ( i ++ , testCaseCountryProperties . getType ( ) ) ; preStat . setString ( i ++ , testCaseCountryProperties . getDatabase ( ) ) ; preStat . setBytes ( i ++ , testCaseCountryProperties . getValue1 ( ) . getBytes ( "UTF-8" ) ) ; preStat . setBytes ( i ++ , testCaseCountryProperties . getValue2 ( ) . getBytes ( "UTF-8" ) ) ; preStat . setString ( i ++ , String . valueOf ( testCaseCountryProperties . getLength ( ) ) ) ; preStat . setString ( i ++ , String . valueOf ( testCaseCountryProperties . getRowLimit ( ) ) ) ; preStat . setString ( i ++ , testCaseCountryProperties . getNature ( ) ) ; code_block = TryStatement ;  } catch ( SQLException exception ) { LOG . error ( "Unable to execute query : " + exception . toString ( ) ) ; } catch ( UnsupportedEncodingException ex ) { LOG . error ( ex . toString ( ) ) ; } }
public void test() { if ( response . getStatus ( ) == HttpStatus . UNAUTHORIZED_401 ) { httpClient . getAuthenticationStore ( ) . clearAuthenticationResults ( ) ; request = prepareSOAPRequest ( soapRequest ) . timeout ( SOAP_TIMEOUT , TimeUnit . SECONDS ) ; response = request . send ( ) ; LOGGER . trace ( "Response from SOAP request: {}" , request ) ; } }
public void setup ( String [ ] names , String [ ] tablepaths , Schema [ ] schemas ) throws Exception { LOG . info ( "===================================================" ) ; LOG . info ( "Creating Tajo Test cluster..." ) ; LOG . info ( "===================================================" ) ; util = new TajoTestingCluster ( ) ; util . startMiniCluster ( 1 ) ; conf = util . getConfiguration ( ) ; client = util . newTajoClient ( ) ; FileSystem fs = util . getDefaultFileSystem ( ) ; Path rootDir = TajoConf . getWarehouseDir ( conf ) ; fs . mkdirs ( rootDir ) ; code_block = ForStatement ; LOG . info ( "===================================================" ) ; LOG . info ( "Test Cluster ready and test table created." ) ; LOG . info ( "===================================================" ) ; }
public void setup ( String [ ] names , String [ ] tablepaths , Schema [ ] schemas ) throws Exception { LOG . info ( "===================================================" ) ; LOG . info ( "Starting Test Cluster." ) ; LOG . info ( "===================================================" ) ; LOG . info ( "===================================================" ) ; util = new TajoTestingCluster ( ) ; util . startMiniCluster ( 1 ) ; conf = util . getConfiguration ( ) ; client = util . newTajoClient ( ) ; FileSystem fs = util . getDefaultFileSystem ( ) ; Path rootDir = TajoConf . getWarehouseDir ( conf ) ; fs . mkdirs ( rootDir ) ; code_block = ForStatement ; LOG . info ( "===================================================" ) ; LOG . info ( "===================================================" ) ; }
public void test() { try { entityMap = getEntityList ( extensionName , jobName , feedForms , processForms , config ) ; submitEntities ( extensionName , jobName , entityMap , config , request ) ; entityNameMap = getJobEntities ( metaStore . getExtensionJobDetails ( jobName ) ) ; scheduleEntities ( entityNameMap , request , coloExpr ) ; } catch ( FalconException | IOException | JAXBException e ) { LOG . error ( "Error occurred while getting extension job: {}" , jobName , e ) ; throw FalconWebException . newAPIException ( e , Response . Status . INTERNAL_SERVER_ERROR ) ; } }
@ Override public void configure ( JobConf job ) { LOGGER . info ( "Running job: {}" , job ) ; }
public void test() { if ( attribute != null ) { logger . debug ( "{}: {}" , getName ( ) , attribute . getValue ( ) ) ; } }
public void test() { if ( isRunning ( ) ) { LOG . error ( "Error resizing" , e ) ; } else { if ( LOG . isDebugEnabled ( ) ) LOG . debug ( "Error resizing, but no longer running: " + e , e ) ; } }
public void test() { try { int currentSize = getCurrentSizeOperator ( ) . apply ( entity ) ; int desiredSize = Math . min ( max , Math . max ( min , currentSize ) ) ; code_block = IfStatement ; } catch ( Exception e ) { code_block = IfStatement ; } catch ( Throwable t ) { LOG . error ( "Cannot update metadata for {}" , entity , t ) ; throw Throwables . propagate ( t ) ; } }
public void test() { if ( containsAlias ( alias ) ) { TlsContext removeMe = tlsContexts . get ( alias ) ; inboundTlsContexts . remove ( removeMe ) ; outboundTlsContexts . remove ( removeMe ) ; tlsContexts . remove ( alias ) ; knownAliases . remove ( alias ) ; } else { LOG . warn ( "Unable to remove alias {}" , alias ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Error while removing session" , e ) ; throw new KunderaException ( e ) ; } }
public void test() { if ( ! instanceDiscoveryCompleted ) { code_block = IfStatement ; String msg = LogHelper . createMessage ( "Instance discovery was successful" , headers . get ( ClientDataHttpHeaders . CORRELATION_ID_HEADER_NAME ) ) ; LOGGER . info ( msg ) ; instanceDiscoveryCompleted = true ; } }
public void test() { try { code_block = IfStatement ; } catch ( PropertyNotSetException e ) { logger . error ( "PropertyNotSetException while deleting user by id " + e . getMessage ( ) ) ; fail ( "tearDown failed" ) ; } }
public void test() { for ( String cache : pools ) { code_block = IfStatement ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { if ( map . containsKey ( STRIMZI_DEFAULT_TLS_SIDECAR_KAFKA_IMAGE ) ) { log . warn ( "Using '" + STRIMZI_DEFAULT_TLS_SIDECAR_KAFKA_IMAGE + "' instead" ) ; } }
@ Test public void testGetJSDateWithDate ( ) { GregorianCalendar calendar = new GregorianCalendar ( 2009 , 11 , 11 ) ; String expectedJavascript = "new Date(2009,11,11,0,0,0,0)" ; String generatedJavascript = DateHelper . getJSDate ( calendar . getTime ( ) ) . toString ( ) ; log . info ( expectedJavascript ) ; log . info ( generatedJavascript ) ; assertEquals ( generatedJavascript , expectedJavascript ) ; }
@ Test public void testGetJSDateWithDate ( ) { GregorianCalendar calendar = new GregorianCalendar ( 2009 , 11 , 11 ) ; String expectedJavascript = "new Date(2009,11,11,0,0,0,0)" ; String generatedJavascript = DateHelper . getJSDate ( calendar . getTime ( ) ) . toString ( ) ; log . info ( expectedJavascript ) ; log . info ( generatedJavascript ) ; assertEquals ( generatedJavascript , expectedJavascript ) ; }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( LOG . isInfoEnabled ( ) ) { final String hostInfo = hostname == null ? "(no host locality info)" : "(on host '" + hostname + "')" ; LOG . info ( "Local host info: {}" , hostInfo ) ; } }
public void test() { if ( nextSplit . isPresent ( ) ) { final FileSourceSplit split = nextSplit . get ( ) ; context . assignSplit ( split , subtask ) ; LOG . debug ( "Assigned split {} to {}" , subtask , subtask ) ; } else { context . signalNoMoreSplits ( subtask ) ; LOG . info ( "No more splits available for subtask {}" , subtask ) ; } }
public void test() { if ( nextSplit . isPresent ( ) ) { final FileSourceSplit split = nextSplit . get ( ) ; context . assignSplit ( split , subtask ) ; LOG . info ( "Assigned split to subtask {} : {}" , subtask , split ) ; } else { context . signalNoMoreSplits ( subtask ) ; LOG . info ( "No more splits to subtask {}" , subtask ) ; } }
public void test() { try { localAddress = InetAddress . getByName ( config . outboundIP ) ; return ; } catch ( UnknownHostException e ) { LOGGER . warn ( "Unable to resolve local address: {}" , config . outboundIP ) ; } }
public void test() { try { code_block = TryStatement ;  } catch ( FileNotFoundException e ) { Log . error ( "File not found" , e ) ; return false ; } catch ( IOException ex ) { Log . error ( "IOException" , ex ) ; return false ; } }
public void test() { try { code_block = TryStatement ;  } catch ( FileNotFoundException e ) { Log . error ( "Input file was not found" , e ) ; return false ; } catch ( IOException ex ) { Log . error ( "Error while reading input file" , ex ) ; return false ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( "Closing connection {}" , connection ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
protected void resetAuthentication ( ) { logger . debug ( "Resetting the authentication token." ) ; authenticationToken = emptyAuthenticationToken ; return ; }
public void test() { try { return terminalInfoAvp . getGrouped ( ) . getAvp ( Avp . SOFTWARE_VERSION ) != null ; } catch ( AvpDataException ex ) { logger . debug ( "Failure trying to obtain (Terminate)" , ex ) ; } }
public void test() { try { stmt . close ( ) ; } catch ( SQLException e ) { logger . warn ( "closing statement" , e ) ; } }
public void test() { try { conn . close ( ) ; } catch ( SQLException e ) { LOG . warn ( "failed to close connection" , e ) ; } }
private void establishServer ( ) throws TTransportException { code_block = IfStatement ; logger . info ( "[{}] Starting Cluster node {}" , getServerClientName ( ) , thisNode ) ; clientService = Executors . newSingleThreadExecutor ( r -> new Thread ( r , getServerClientName ( ) ) ) ; clientService . submit ( ( ) -> poolServer . serve ( ) ) ; logger . info ( "[{}] Cluster node {} is up" , getServerClientName ( ) , thisNode ) ; }
public void sendSuccessEdgeNotification ( final MonitoredMessage trackMessage ) { LOG . debug ( "Entering Message Monitoring API sendSuccessEdgeNotification() method." ) ; final String subject = MessageMonitoringUtil . getSuccessfulMessageSubjectPrefix ( ) + trackMessage . getSubject ( ) ; final String emailText = MessageMonitoringUtil . getSuccessfulMessageEmailText ( ) + trackMessage . getRecipients ( ) ; final String postmasterEmailId = MessageMonitoringUtil . getDomainPostmasterEmailId ( ) + "@" + MessageMonitoringUtil . getDomainFromEmail ( trackMessage . getSenderemailid ( ) ) ; final DirectEdgeProxy proxy = MessageMonitoringUtil . getDirectEdgeProxy ( ) ; MimeMessage message = null ; String errorMsg = null ; code_block = TryStatement ;  LOG . debug ( "Exiting Message Monitoring API sendSuccessEdgeNotification() method." ) ; }
public void test() { try { message = MessageMonitoringUtil . createMimeMessage ( postmasterEmailId , subject , trackMessage . getSenderemailid ( ) , emailText , trackMessage . getMessageid ( ) ) ; proxy . provideAndRegisterDocumentSetB ( message ) ; getDirectEventLogger ( ) . log ( DirectEventType . DIRECT_EDGE_NOTIFICATION_SUCCESSFUL , message ) ; } catch ( final MessagingException ex ) { LOGGER . error ( "ERROR" , ex ) ; errorMsg = ex . getLocalizedMessage ( ) ; } }
public void test() { if ( hbaseAdmin . tableExists ( htable ) ) { code_block = IfStatement ; hbaseAdmin . deleteTable ( htable ) ; logger . info ( "Deleted HBase table {}" , htable ) ; } else { logger . info ( "HBase table {} does not exist." , htable ) ; } }
public void test() { if ( hbaseAdmin . tableExists ( htable ) ) { code_block = IfStatement ; hbaseAdmin . deleteTable ( htable ) ; logger . info ( "Deleted HBase table {}" , htable ) ; } else { logger . info ( "HBase table {}" , htable ) ; } }
public void test() { try { code_block = ForStatement ; } catch ( Exception e ) { LOG . warn ( e . toString ( ) , e ) ; } finally { IOUtils . closeQuietly ( hbaseAdmin ) ; } }
@ Test public void testGetBridge ( ) { LOG . info ( "Getting platform bridge" ) ; LOG . info ( "HBase version: " + org . apache . hadoop . hbase . util . VersionInfo . getVersion ( ) ) ; SchemaPlatformBridge bridge = SchemaPlatformBridge . get ( ) ; assertNotNull ( bridge ) ; LOG . info ( "Got platform bridge: " + bridge . getClass ( ) . getName ( ) ) ; }
@ Test public void testGetBridge ( ) { LOG . info ( "Running testGetBridge" ) ; LOG . info ( "Hadoop version: " + org . apache . hadoop . util . VersionInfo . getVersion ( ) ) ; SchemaPlatformBridge bridge = SchemaPlatformBridge . get ( ) ; assertNotNull ( bridge ) ; LOG . info ( "Got platform bridge: " + bridge . getClass ( ) . getName ( ) ) ; }
@ Test public void testGetBridge ( ) { LOG . info ( "Hadoop version: " + org . apache . hadoop . util . VersionInfo . getVersion ( ) ) ; LOG . info ( "HBase version: " + org . apache . hadoop . hbase . util . VersionInfo . getVersion ( ) ) ; LOG . info ( "Using Hive version: " + org . apache . hadoop . hbase . util . VersionInfo . getVersion ( ) ) ; SchemaPlatformBridge bridge = SchemaPlatformBridge . get ( ) ; assertNotNull ( bridge ) ; }
@ Test public void testPutMissedDhtRequest_UnstableTopology ( ) throws Exception { blockRebalance = true ; ccfg = cacheConfiguration ( 1 , FULL_SYNC ) ; startServers ( 4 ) ; client = true ; Ignite client = startGrid ( 4 ) ; IgniteCache < Integer , Integer > nearCache = client . cache ( TEST_CACHE ) ; testSpi ( ignite ( 0 ) ) . blockMessages ( new IgniteBiPredicate < ClusterNode , Message > ( ) code_block = "" ; ) ; Integer key = primaryKey ( ignite ( 0 ) . cache ( TEST_CACHE ) ) ; log . info ( "Start put [key=" + key + ']' ) ; IgniteFuture < ? > fut = nearCache . putAsync ( key , key ) ; U . sleep ( 500 ) ; assertFalse ( fut . isDone ( ) ) ; stopGrid ( 0 ) ; fut . get ( ) ; checkData ( F . asMap ( key , key ) ) ; }
private ScheduledReporter createAndGetConfiguredCSVReporter ( String prefix , String csvDir ) throws IOException { File outputDir ; code_block = IfStatement ; LOGGER . info ( "Output directory: " + outputDir ) ; FileUtils . forceMkdir ( outputDir ) ; return CsvReporter . forRegistry ( metrics ) . convertRatesTo ( TimeUnit . SECONDS ) . convertDurationsTo ( TimeUnit . MILLISECONDS ) . build ( outputDir ) ; }
public void test() { try { MethodKey methodKey = new MethodKey ( MBMessageServiceUtil . class , "restoreMessageAttachmentFromTrash" , _restoreMessageAttachmentFromTrashParameterTypes32 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , messageId , fileName ) ; code_block = TryStatement ;  } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
@ Override public void text ( String text ) { LOGGER . debug ( "text: " + text ) ; here . addChild ( TreeNode . create_text ( text ) ) ; }
public void test() { if ( couple == null ) { LOGGER . warn ( "Tried to send a notification that is registered" ) ; return ; } }
@ Override public Object visit ( PropertyName expression , Object data ) { LOGGER . info ( "PropertyName: {}" , expression ) ; return data ; }
public void test() { try { logger . info ( "Starting warmup for new cache of site '{}'" , siteName ) ; stopWatch . start ( ) ; doCacheWarmUp ( tmpContext ) ; code_block = IfStatement ; stopWatch . stop ( ) ; logger . info ( "Warm up for new cache of site '{}' completed (switched with old cache) in {} secs" , siteName , stopWatch . getTime ( TimeUnit . SECONDS ) ) ; } catch ( Exception e ) { cacheService . removeScope ( tmpContext ) ; logger . error ( "Cache warm up failed" , e ) ; } }
public void test() { try { logger . info ( "Starting warm up for new cache of site '{}'" , siteName ) ; stopWatch . start ( ) ; doCacheWarmUp ( tmpContext ) ; code_block = IfStatement ; stopWatch . stop ( ) ; logger . info ( "Finished warm up for new cache of site '{}'" , siteName ) ; } catch ( Exception e ) { cacheService . removeScope ( tmpContext ) ; logger . error ( "Cache warm up failed" , e ) ; } }
public void test() { try { logger . info ( "Starting warm up for new cache of site '{}'" , siteName ) ; stopWatch . start ( ) ; doCacheWarmUp ( tmpContext ) ; code_block = IfStatement ; stopWatch . stop ( ) ; logger . info ( "Warm up for new cache of site '{}' completed (switched with old cache) in {} secs" , siteName , stopWatch . getTime ( TimeUnit . SECONDS ) ) ; } catch ( Exception e ) { cacheService . removeScope ( tmpContext ) ; logger . error ( "Failed to warm up temporary cache of site '{}'" , siteName , e ) ; } }
public void test() { if ( switchCache ) { Context currentContext = siteContext . getContext ( ) ; long oldCacheVersion = currentContext . getCacheVersion ( ) ; long newCacheVersion = System . nanoTime ( ) ; Context tmpContext = currentContext . clone ( ) ; tmpContext . setCacheVersion ( newCacheVersion ) ; cacheService . addScope ( tmpContext ) ; logger . info ( "Updating cache version from site {}" , siteName ) ; code_block = TryStatement ;  } else { stopWatch . start ( ) ; doCacheWarmUp ( siteContext . getContext ( ) ) ; stopWatch . stop ( ) ; logger . info ( "Warm up for cache of site '{}' completed in {} secs" , siteName , stopWatch . getTime ( TimeUnit . SECONDS ) ) ; } }
public void test() { if ( switchCache ) { Context currentContext = siteContext . getContext ( ) ; long oldCacheVersion = currentContext . getCacheVersion ( ) ; long newCacheVersion = System . nanoTime ( ) ; Context tmpContext = currentContext . clone ( ) ; tmpContext . setCacheVersion ( newCacheVersion ) ; cacheService . addScope ( tmpContext ) ; logger . info ( "Finished warm up for cache of site '{}'" , siteName ) ; code_block = TryStatement ;  } else { logger . info ( "Starting warm up for cache of site '{}'" , siteName ) ; stopWatch . start ( ) ; doCacheWarmUp ( siteContext . getContext ( ) ) ; stopWatch . stop ( ) ; } }
public void test() { try { olf . raf . close ( ) ; } catch ( IOException e ) { log . warn ( "IOException closing file" , e ) ; } }
public void test() { try { InsightsAssessmentConfiguration assessmentConfig = populateAssessmentReportConfiguration ( assessmentReportJson ) ; assessmentReportId = reportConfigDAL . saveInsightsAssessmentConfig ( assessmentConfig ) ; return assessmentReportId ; } catch ( InsightsCustomException e ) { log . error ( e ) ; throw new InsightsCustomException ( e . getMessage ( ) ) ; } }
public void test() { if ( ! isZip ( outputZipFile . getName ( ) ) ) { logger . info ( "Unzip file " + outputZipFile . getName ( ) ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Refreshing." ) ; } }
public void test() { if ( future . isWritten ( ) ) { int available ; synchronized ( availableCount ) code_block = "" ; code_block = IfStatement ; log . error ( "operationComplete({}) invalid available count: {}" , this , available ) ; } else { Throwable err = future . getException ( ) ; log . error ( "operationComplete({}) write exception: {}" , this , err ) ; } }
public void test() { if ( future . isDone ( ) ) { code_block = IfStatement ; } else { LOGGER . warn ( "Server -> " + future . cause ( ) ) ; } }
public void test() { try { close ( ) ; } catch ( IOException e ) { logger . error ( "Failed to close input stream for log file" , e ) ; } }
public void test() { -> { logger . warn ( "No repo loaded warning." ) ; nc . addNotification ( "You need to load a repository before you can perform operations on it. Click on the plus sign in the upper left corner!" ) ; } }
@ Transactional public void updateUserInfo ( int userId , String name , String surname , int age , String userName , String password ) { this . logger . debug ( "Updating user information" ) ; User user = this . entityManager . find ( User . class , userId ) ; user . setName ( name ) ; user . setSurname ( surname ) ; user . setAge ( age ) ; user . setUserName ( userName ) ; user . setPassword ( password ) ; }
public void test() { try { this . requiredTables = requiredTables ; code_block = IfStatement ; code_block = ForStatement ; return true ; } catch ( ConsistencyException cx ) { LOGGER . log ( Level . WARNING , "Error while checking dependency" , cx ) ; return false ; } }
private TransactionalDataSource createShardDatasource ( Long shardId , DbVersion dbVersion ) { long start = System . currentTimeMillis ( ) ; waitAvailability ( ) ; ShardDataSourceCreateHelper shardDataSourceCreateHelper = new ShardDataSourceCreateHelper ( this , shardId ) . createUninitializedDataSource ( ) ; TransactionalDataSource shardDb = shardDataSourceCreateHelper . getShardDb ( ) ; shardDb . init ( dbVersion ) ; connectedShardDataSourceMap . put ( shardId , shardDb ) ; logger . info ( "Created shardDatasource for: {} in {} ms" , shardId , System . currentTimeMillis ( ) - start ) ; return shardDb ; }
public void test() { try { code_block = ForStatement ; } catch ( ParseException pe ) { log . error ( pe . getMessage ( ) ) ; } catch ( NumberFormatException nfe ) { log . error ( "Unable to numeric argument " + start + ": " + nfe . getMessage ( ) ) ; } }
public void test() { try { code_block = ForStatement ; } catch ( ParseException pe ) { log . error ( pe . getMessage ( ) ) ; } catch ( NumberFormatException nfe ) { log . error ( "Unable to numeric argument " + nfe . getMessage ( ) ) ; } }
@ NotNull public Multimap < Chromosome , CobaltCount > tumorOnly ( @ NotNull final String tumorBam ) throws IOException , ExecutionException , InterruptedException { CB_LOGGER . info ( "Writing {} to {}" , tumorBam , tumorBam ) ; final File tumorFile = new File ( tumorBam ) ; final String chromosomeLengthFileName = ChromosomeLengthFile . generateFilename ( mOutputDir , mTumorId ) ; final List < ChromosomeLength > lengths ; code_block = TryStatement ;  ChromosomeLengthFile . write ( chromosomeLengthFileName , lengths ) ; final List < Future < ChromosomeReadCount > > tumorFutures = createFutures ( mReaderFactory , tumorFile , lengths ) ; final Multimap < Chromosome , ReadCount > tumorCounts = fromFutures ( tumorFutures ) ; CB_LOGGER . info ( "Read Count Complete" ) ; return CobaltCountFactory . tumorOnly ( tumorCounts ) ; }
@ NotNull public Multimap < Chromosome , CobaltCount > tumorOnly ( @ NotNull final String tumorBam ) throws IOException , ExecutionException , InterruptedException { CB_LOGGER . info ( "Writing to {}" , tumorBam ) ; final File tumorFile = new File ( tumorBam ) ; final String chromosomeLengthFileName = ChromosomeLengthFile . generateFilename ( mOutputDir , mTumorId ) ; final List < ChromosomeLength > lengths ; code_block = TryStatement ;  ChromosomeLengthFile . write ( chromosomeLengthFileName , lengths ) ; CB_LOGGER . info ( "Calculating Read Count from {}" , tumorFile . toString ( ) ) ; final List < Future < ChromosomeReadCount > > tumorFutures = createFutures ( mReaderFactory , tumorFile , lengths ) ; final Multimap < Chromosome , ReadCount > tumorCounts = fromFutures ( tumorFutures ) ; return CobaltCountFactory . tumorOnly ( tumorCounts ) ; }
@ Override public Double visit ( LessEqualFilter lessEqualFilter ) { int minBound = 9 - IntStream . rangeClosed ( 0 , 9 ) . filter ( i -> percentiles [ 9 - i ] <= lessEqualFilter . getValue ( ) . doubleValue ( ) ) . findFirst ( ) . orElse ( 0 ) ; final double result = ( ( double ) minBound + 1.0 ) / 10.0 ; log . trace ( "Compiled lessEqual filter: {}" , result ) ; return result ; }
public void test() { try { InputStream xsl = this . getClass ( ) . getClassLoader ( ) . getResourceAsStream ( xslFile ) ; Transformer tx = TransformerFactory . newInstance ( ) . newTransformer ( new StreamSource ( xsl ) ) ; tx . transform ( xmlSource , new StreamResult ( out ) ) ; log . debug ( out . toString ( ) ) ; InputStream result = new ByteArrayInputStream ( out . toByteArray ( ) ) ; return result ; } catch ( TransformerConfigurationException tce ) { log . error ( "Failed to configure transformer" , tce ) ; throw new CalendarException ( "Failed to configure transformer" , tce ) ; } catch ( TransformerException txe ) { log . error ( "Failed to transform transformer" , txe ) ; throw new CalendarException ( "Failed transformation" , txe ) ; } }
public void test() { try { log . debug ( "Stylesheet is " + xslFile ) ; InputStream xsl = this . getClass ( ) . getClassLoader ( ) . getResourceAsStream ( xslFile ) ; Transformer tx = TransformerFactory . newInstance ( ) . newTransformer ( new StreamSource ( xsl ) ) ; tx . transform ( xmlSource , new StreamResult ( out ) ) ; InputStream result = new ByteArrayInputStream ( out . toByteArray ( ) ) ; return result ; } catch ( TransformerConfigurationException tce ) { log . error ( "Failed to configure transformer" , tce ) ; throw new CalendarException ( "Failed to configure transformer" , tce ) ; } catch ( TransformerException txe ) { log . error ( "Failed to transform transformer" , txe ) ; throw new CalendarException ( "Failed transformation" , txe ) ; } }
public void test() { try { log . debug ( "Stylesheet is " + xslFile ) ; InputStream xsl = this . getClass ( ) . getClassLoader ( ) . getResourceAsStream ( xslFile ) ; Transformer tx = TransformerFactory . newInstance ( ) . newTransformer ( new StreamSource ( xsl ) ) ; tx . transform ( xmlSource , new StreamResult ( out ) ) ; log . debug ( out . toString ( ) ) ; InputStream result = new ByteArrayInputStream ( out . toByteArray ( ) ) ; log . debug ( "Transformed transformation: " + result ) ; return result ; } catch ( TransformerConfigurationException tce ) { throw new CalendarException ( "Failed to configure transformer" , tce ) ; } catch ( TransformerException txe ) { throw new CalendarException ( "Failed transformation" , txe ) ; } }
public void test() { if ( ! followerTimer . get ( ) ) { LOG . info ( "Not stopping follower" ) ; return ; } }
public void test() { if ( object . getStatus ( ) != ServerStatus . Follower ) { logger . error ( "Server status is {}" , serverStatus . toString ( ) ) ; return ; } }
public void test() { if ( type > list . size ( ) ) { LOG . warning ( "Type '" + list . size ( ) + "' is greater than one of " + type ) ; return null ; } }
public void test() { if ( loggerIsEnabled ) { logger . debug ( msg , e ) ; } }
public void test() { if ( loggerIsEnabled ) { logger . debug ( msg , e ) ; } }
@ Test public void testSpdyServerSessionHandlerGoAway ( ) { logger . info ( "Running: testSpdyServerSessionHandlerGoAway v3" ) ; testSpdySessionHandlerGoAway ( SpdyVersion . SPDY_3 , true ) ; logger . info ( "Running: testSpdyServerSessionHandlerGoAway v3.1" ) ; testSpdySessionHandlerGoAway ( SpdyVersion . SPDY_3_1 , true ) ; }
@ Test public void testSpdyServerSessionHandlerGoAway ( ) { logger . info ( "Running: testSpdyServerSessionHandlerGoAway v3" ) ; testSpdySessionHandlerGoAway ( SpdyVersion . SPDY_3 , true ) ; logger . info ( "Running: testSpdyServerSessionHandlerGoAway v3.1" ) ; testSpdySessionHandlerGoAway ( SpdyVersion . SPDY_3_1 , true ) ; }
public void test() { try { lm . prepareDAG ( new ThroughputCounterApp ( ) , conf ) ; LocalMode . Controller lc = lm . getController ( ) ; lc . run ( 20000 ) ; } catch ( Exception ex ) { logger . info ( ex . getMessage ( ) ) ; } }
public String getAttachmentContentById ( String containerId , Number taskId , Number attachmentId , String marshallingType ) { containerId = context . getContainerId ( containerId , new ByTaskIdContainerLocator ( taskId . longValue ( ) ) ) ; Object attachment = userTaskService . getAttachmentContentById ( containerId , taskId . longValue ( ) , attachmentId . longValue ( ) ) ; code_block = IfStatement ; String response = marshallerHelper . marshal ( containerId , marshallingType , attachment ) ; logger . debug ( "About to marshal task '{}' successfully" , taskId ) ; return response ; }
public void test() { if ( engine == null ) { Class < ? > defaultEngine = ProtobufRpcEngine . class ; Class < ? > impl = defaultEngine ; engine = ( RpcEngine ) ReflectionUtils . newInstance ( impl , conf ) ; if ( protocol . isInterface ( ) ) PROXY_ENGINES . put ( Proxy . getProxyClass ( protocol . getClassLoader ( ) , protocol ) , engine ) ; LOG . debug ( "Using protocol {}" , protocol ) ; PROTOCOL_ENGINES . put ( protocol , engine ) ; } }
public void test() { try { properties . load ( PropertiesUtil . class . getClassLoader ( ) . getResourceAsStream ( properties_file ) ) ; } catch ( IOException ex ) { log . error ( "Failed to read properties file " + properties_file , ex ) ; } }
public void test() { try { URI uri = new URI ( l ) ; return Optional . of ( new HTTPLink ( uri ) ) ; } catch ( URISyntaxException e ) { LOG . warn ( "invalid linkInfo {}" , l , e ) ; return Optional . empty ( ) ; } }
@ Override public boolean restoreVMFromBackup ( VirtualMachine vm , Backup backup ) { s_logger . info ( "Backup VM " + vm . getName ( ) ) ; return true ; }
@ Override public void downPhysicalInterface ( LogicalPort iface ) throws CapabilityException { log . info ( "Start of downPhysicalInterface call" ) ; iface . setOperationalStatus ( OperationalStatus . STOPPED ) ; IAction action = createActionAndCheckParams ( ChassisActionSet . CONFIGURESTATUS , iface ) ; queueAction ( action ) ; log . info ( "End of downPhysicalInterface call" ) ; }
@ Override public void downPhysicalInterface ( LogicalPort iface ) throws CapabilityException { log . info ( "Start of downPhysicalInterface call" ) ; iface . setOperationalStatus ( OperationalStatus . STOPPED ) ; IAction action = createActionAndCheckParams ( ChassisActionSet . CONFIGURESTATUS , iface ) ; queueAction ( action ) ; log . info ( "End of downPhysicalInterface call" ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log != null ) { log . warn ( "handleCommandExitStatus({}) cmd='{}')" , session , cmd ) ; } }
public void test() { if ( backgroundTaskId != _backgroundTaskId ) { LOGGER . error ( "backgroundTaskId {} does not match {}, ignoring" , _backgroundTaskId , _backgroundTaskId ) ; return ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( ! session . isPresent ( ) ) { logger . error ( "Session not found : " + sessionId ) ; } else { UUID userId = session . get ( ) . getUserId ( ) ; managerFactory . tournamentManager ( ) . joinTournament ( tournamentId , userId ) ; } }
public void test() { try { connection = createConnection ( fullUser , "wrongPassword" , null , false ) ; connection . start ( ) ; fail ( "Expected JMSException" ) ; } catch ( JMSSecurityException ex ) { instanceLog . debug ( "Expected exception" , ex ) ; } finally { code_block = IfStatement ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Failed to clean up." , e ) ; } }
@ GetMapping @ Override public Collection < TenantDto > getAll ( @ RequestParam final Optional < String > criteria ) { LOGGER . debug ( "get all tenant={}" , criteria ) ; RestUtils . checkCriteria ( criteria ) ; return internalTenantService . getAll ( criteria ) ; }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { try { code_block = SwitchStatement ; frequencyDetails . setRuns ( runDetailsBean ) ; anchorRunDetailsBean = this . getAcivetaskFrequencyAncorDetailsForManuallySchedule ( activeTask , anchorRunDetailsBean , session ) ; frequencyDetails . setAnchorRuns ( anchorRunDetailsBean ) ; } catch ( Exception e ) { log . error ( "Failed to retrieve active task due to " + e . getMessage ( ) , e ) ; } }
public void test() { try { this . client . getSession ( ) . execute ( CassandraQueryFactory . getTruncateTableQuery ( mapping ) ) ; LOG . info ( "Truncate table query: {}" , mapping ) ; } catch ( Exception e ) { throw new GoraException ( e ) ; } }
public void test() { try ( Stream < UUID > jobs = mantaClient . getAllJobIds ( ) ) { List < UUID > found = jobs . filter ( id -> id . equals ( job1id ) || id . equals ( job2id ) ) . collect ( Collectors . toList ( ) ) ; Assert . assertEquals ( found . size ( ) , 2 , "We should have found both jobs" ) ; } catch ( AssertionError e ) { String msg = "Couldn't find job in job list, retry test a few times to verify" ; LOG . warn ( msg ) ; throw new SkipException ( msg , e ) ; } }
private void prepareExtendedRandomLength ( ExtendedRandomExtensionMessage msg ) { msg . setExtendedRandomLength ( msg . getExtendedRandom ( ) . getValue ( ) . length ) ; LOGGER . debug ( "ExtendedRandomLength: " + msg . getExtendedRandomLength ( ) . getValue ( ) ) ; }
public void test() { try { iterator = sequence . iterate ( ) ; } catch ( final XPathException xpe ) { LOG . warn ( "Invalid XPathException: {}" , xpe . getMessage ( ) , xpe ) ; iterator = SequenceIterator . EMPTY_ITERATOR ; } }
public void test() { try { RegressionModelPrediction p = null ; AutoMLConfig autoMLConfig = autoMlDAL . getMLConfigByUsecase ( usecaseName ) ; String deployedMojoName = autoMLConfig . getMojoDeployed ( ) ; String predectionColumn = autoMLConfig . getPredictionColumn ( ) ; String path = FileUtils . getTempDirectoryPath ( ) ; byte [ ] mojoData = autoMLConfig . getMojoDeployedZip ( ) ; File file = new File ( path + usecaseName + ".zip" ) ; FileUtils . writeByteArrayToFile ( file , mojoData ) ; EasyPredictModelWrapper model = new EasyPredictModelWrapper ( MojoModel . load ( new TmpMojoReaderBackend ( file ) ) ) ; Gson gson = new Gson ( ) ; Type type = new TypeToken < Map < String , String > > ( ) code_block = "" ; . getType ( ) ; code_block = ForStatement ; log . info ( "Done creating regression result" ) ; } catch ( Exception e ) { log . error ( e ) ; throw new PredictException ( "Something went wrong while executing regression prediction to " + usecaseName + " " + e . getMessage ( ) ) ; } }
public void test() { try { RegressionModelPrediction p = null ; AutoMLConfig autoMLConfig = autoMlDAL . getMLConfigByUsecase ( usecaseName ) ; String deployedMojoName = autoMLConfig . getMojoDeployed ( ) ; String predectionColumn = autoMLConfig . getPredictionColumn ( ) ; String path = FileUtils . getTempDirectoryPath ( ) ; byte [ ] mojoData = autoMLConfig . getMojoDeployedZip ( ) ; File file = new File ( path + usecaseName + ".zip" ) ; FileUtils . writeByteArrayToFile ( file , mojoData ) ; EasyPredictModelWrapper model = new EasyPredictModelWrapper ( MojoModel . load ( new TmpMojoReaderBackend ( file ) ) ) ; log . debug ( "Worlflow Detail ====  Mojo {}  Loaded Successfully" , deployedMojoName ) ; Gson gson = new Gson ( ) ; Type type = new TypeToken < Map < String , String > > ( ) code_block = "" ; . getType ( ) ; code_block = ForStatement ; } catch ( Exception e ) { log . error ( "Worlflow Detail ====  Error while executing regression prediction to " + usecaseName + " " + e . getMessage ( ) , e ) ; throw new PredictException ( "Something went wrong while executing regression prediction to " + usecaseName + " " + e . getMessage ( ) ) ; } }
public void test() { if ( logger . isTraceEnabled ( LogMarker . DLS_VERBOSE ) ) { logger . trace ( LogMarker . DLS_VERBOSE , "[checkForExpiration] Expired token at {}: {}" , currentTime , toString ( true ) ) ; } }
public void test() { if ( printTagSet ) { LOGGER . info ( "Set tag {}" , tag ) ; } }
@ Override public List < Pipeline > createAllPossiblePipelines ( ) { @ SuppressWarnings ( "unused" ) List < Module > modules = moduleDAO . getModules ( ) ; LOG . info ( "No pipelines to create" ) ; return null ; }
private void startMiniTajoCluster ( File testBuildDir , final int numSlaves , boolean local ) throws Exception { TajoConf c = getConfiguration ( ) ; c . setVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . TAJO_MASTER_UMBILICAL_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . WORKER_PEER_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . WORKER_TEMPORAL_DIR , "file://" + testBuildDir . getAbsolutePath ( ) + "/tajo-localdir" ) ; c . setVar ( ConfVars . REST_SERVICE_ADDRESS , "localhost:0" ) ; code_block = IfStatement ; setupCatalogForTesting ( c , testBuildDir ) ; tajoMaster = new TajoMaster ( ) ; tajoMaster . init ( c ) ; tajoMaster . start ( ) ; this . conf . setVar ( ConfVars . WORKER_PEER_RPC_ADDRESS , c . getVar ( ConfVars . WORKER_PEER_RPC_ADDRESS ) ) ; this . conf . setVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS , c . getVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS ) ) ; InetSocketAddress tajoMasterAddress = tajoMaster . getContext ( ) . getTajoMasterService ( ) . getBindAddress ( ) ; this . conf . setVar ( ConfVars . TAJO_MASTER_UMBILICAL_RPC_ADDRESS , NetUtils . getHostPortString ( tajoMasterAddress ) ) ; this . conf . setVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS , c . getVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS ) ) ; this . conf . set
private void startMiniTajoCluster ( File testBuildDir , final int numSlaves , boolean local ) throws Exception { TajoConf c = getConfiguration ( ) ; c . setVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . TAJO_MASTER_UMBILICAL_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . WORKER_PEER_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . WORKER_TEMPORAL_DIR , "file://" + testBuildDir . getAbsolutePath ( ) + "/tajo-localdir" ) ; c . setVar ( ConfVars . REST_SERVICE_ADDRESS , "localhost:0" ) ; code_block = IfStatement ; setupCatalogForTesting ( c , testBuildDir ) ; LOG . info ( "derby repository is set to " + conf . get ( CatalogConstants . CATALOG_URI ) ) ; tajoMaster = new TajoMaster ( ) ; tajoMaster . init ( c ) ; tajoMaster . start ( ) ; this . conf . setVar ( ConfVars . WORKER_PEER_RPC_ADDRESS , c . getVar ( ConfVars . WORKER_PEER_RPC_ADDRESS ) ) ; this . conf . setVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS , c . getVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS ) ) ; InetSocketAddress tajoMasterAddress = tajoMaster . getContext ( ) . getTajoMasterService ( ) . getBindAddress ( ) ; this . conf . setVar ( ConfVars . TAJO_MASTER_UMBILICAL_RPC_ADDRESS , NetUtils . getHostPortString ( tajoMasterAddress ) ) ; this . conf . setVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS , "
private void startMiniTajoCluster ( File testBuildDir , final int numSlaves , boolean local ) throws Exception { TajoConf c = getConfiguration ( ) ; c . setVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . TAJO_MASTER_UMBILICAL_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . WORKER_PEER_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . WORKER_TEMPORAL_DIR , "file://" + testBuildDir . getAbsolutePath ( ) + "/tajo-localdir" ) ; c . setVar ( ConfVars . REST_SERVICE_ADDRESS , "localhost:0" ) ; code_block = IfStatement ; setupCatalogForTesting ( c , testBuildDir ) ; LOG . info ( "derby repository is set to " + conf . get ( CatalogConstants . CATALOG_URI ) ) ; tajoMaster = new TajoMaster ( ) ; tajoMaster . init ( c ) ; tajoMaster . start ( ) ; this . conf . setVar ( ConfVars . WORKER_PEER_RPC_ADDRESS , c . getVar ( ConfVars . WORKER_PEER_RPC_ADDRESS ) ) ; this . conf . setVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS , c . getVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS ) ) ; InetSocketAddress tajoMasterAddress = tajoMaster . getContext ( ) . getTajoMasterService ( ) . getBindAddress ( ) ; this . conf . setVar ( ConfVars . TAJO_MASTER_UMBILICAL_RPC_ADDRESS , NetUtils . getHostPortString ( tajoMasterAddress ) ) ; this . conf . setVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS , "
private void startMiniTajoCluster ( File testBuildDir , final int numSlaves , boolean local ) throws Exception { TajoConf c = getConfiguration ( ) ; c . setVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . TAJO_MASTER_UMBILICAL_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . WORKER_PEER_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . WORKER_TEMPORAL_DIR , "file://" + testBuildDir . getAbsolutePath ( ) + "/tajo-localdir" ) ; c . setVar ( ConfVars . REST_SERVICE_ADDRESS , "localhost:0" ) ; code_block = IfStatement ; setupCatalogForTesting ( c , testBuildDir ) ; LOG . info ( "derby repository is set to " + conf . get ( CatalogConstants . CATALOG_URI ) ) ; tajoMaster = new TajoMaster ( ) ; tajoMaster . init ( c ) ; tajoMaster . start ( ) ; this . conf . setVar ( ConfVars . WORKER_PEER_RPC_ADDRESS , c . getVar ( ConfVars . WORKER_PEER_RPC_ADDRESS ) ) ; this . conf . setVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS , c . getVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS ) ) ; InetSocketAddress tajoMasterAddress = tajoMaster . getContext ( ) . getTajoMasterService ( ) . getBindAddress ( ) ; this . conf . setVar ( ConfVars . TAJO_MASTER_UMBILICAL_RPC_ADDRESS , NetUtils . getHostPortString ( tajoMasterAddress ) ) ; this . conf . setVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS , "
private void startMiniTajoCluster ( File testBuildDir , final int numSlaves , boolean local ) throws Exception { TajoConf c = getConfiguration ( ) ; c . setVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . TAJO_MASTER_UMBILICAL_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . WORKER_PEER_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . WORKER_TEMPORAL_DIR , "file://" + testBuildDir . getAbsolutePath ( ) + "/tajo-localdir" ) ; c . setVar ( ConfVars . REST_SERVICE_ADDRESS , "localhost:0" ) ; code_block = IfStatement ; setupCatalogForTesting ( c , testBuildDir ) ; LOG . info ( "derby repository is set to " + conf . get ( CatalogConstants . CATALOG_URI ) ) ; tajoMaster = new TajoMaster ( ) ; tajoMaster . init ( c ) ; tajoMaster . start ( ) ; this . conf . setVar ( ConfVars . WORKER_PEER_RPC_ADDRESS , c . getVar ( ConfVars . WORKER_PEER_RPC_ADDRESS ) ) ; this . conf . setVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS , c . getVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS ) ) ; InetSocketAddress tajoMasterAddress = tajoMaster . getContext ( ) . getTajoMasterService ( ) . getBindAddress ( ) ; this . conf . setVar ( ConfVars . TAJO_MASTER_UMBILICAL_RPC_ADDRESS , NetUtils . getHostPortString ( tajoMasterAddress ) ) ; this . conf . setVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS , "
private void startMiniTajoCluster ( File testBuildDir , final int numSlaves , boolean local ) throws Exception { TajoConf c = getConfiguration ( ) ; c . setVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . TAJO_MASTER_UMBILICAL_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . WORKER_PEER_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . WORKER_TEMPORAL_DIR , "file://" + testBuildDir . getAbsolutePath ( ) + "/tajo-localdir" ) ; c . setVar ( ConfVars . REST_SERVICE_ADDRESS , "localhost:0" ) ; code_block = IfStatement ; setupCatalogForTesting ( c , testBuildDir ) ; LOG . info ( "derby repository is set to " + conf . get ( CatalogConstants . CATALOG_URI ) ) ; tajoMaster = new TajoMaster ( ) ; tajoMaster . init ( c ) ; tajoMaster . start ( ) ; this . conf . setVar ( ConfVars . WORKER_PEER_RPC_ADDRESS , c . getVar ( ConfVars . WORKER_PEER_RPC_ADDRESS ) ) ; this . conf . setVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS , c . getVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS ) ) ; InetSocketAddress tajoMasterAddress = tajoMaster . getContext ( ) . getTajoMasterService ( ) . getBindAddress ( ) ; this . conf . setVar ( ConfVars . TAJO_MASTER_UMBILICAL_RPC_ADDRESS , NetUtils . getHostPortString ( tajoMasterAddress ) ) ; this . conf . setVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS , "
private void startMiniTajoCluster ( File testBuildDir , final int numSlaves , boolean local ) throws Exception { TajoConf c = getConfiguration ( ) ; c . setVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . TAJO_MASTER_UMBILICAL_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . WORKER_PEER_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . WORKER_TEMPORAL_DIR , "file://" + testBuildDir . getAbsolutePath ( ) + "/tajo-localdir" ) ; c . setVar ( ConfVars . REST_SERVICE_ADDRESS , "localhost:0" ) ; code_block = IfStatement ; setupCatalogForTesting ( c , testBuildDir ) ; LOG . info ( "derby repository is set to " + conf . get ( CatalogConstants . CATALOG_URI ) ) ; tajoMaster = new TajoMaster ( ) ; tajoMaster . init ( c ) ; tajoMaster . start ( ) ; this . conf . setVar ( ConfVars . WORKER_PEER_RPC_ADDRESS , c . getVar ( ConfVars . WORKER_PEER_RPC_ADDRESS ) ) ; this . conf . setVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS , c . getVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS ) ) ; InetSocketAddress tajoMasterAddress = tajoMaster . getContext ( ) . getTajoMasterService ( ) . getBindAddress ( ) ; this . conf . setVar ( ConfVars . TAJO_MASTER_UMBILICAL_RPC_ADDRESS , NetUtils . getHostPortString ( tajoMasterAddress ) ) ; this . conf . setVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS , "
private void startMiniTajoCluster ( File testBuildDir , final int numSlaves , boolean local ) throws Exception { TajoConf c = getConfiguration ( ) ; c . setVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . TAJO_MASTER_UMBILICAL_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . WORKER_PEER_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . WORKER_TEMPORAL_DIR , "file://" + testBuildDir . getAbsolutePath ( ) + "/tajo-localdir" ) ; c . setVar ( ConfVars . REST_SERVICE_ADDRESS , "localhost:0" ) ; code_block = IfStatement ; setupCatalogForTesting ( c , testBuildDir ) ; LOG . info ( "derby repository is set to " + conf . get ( CatalogConstants . CATALOG_URI ) ) ; tajoMaster = new TajoMaster ( ) ; tajoMaster . init ( c ) ; tajoMaster . start ( ) ; this . conf . setVar ( ConfVars . WORKER_PEER_RPC_ADDRESS , c . getVar ( ConfVars . WORKER_PEER_RPC_ADDRESS ) ) ; this . conf . setVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS , c . getVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS ) ) ; InetSocketAddress tajoMasterAddress = tajoMaster . getContext ( ) . getTajoMasterService ( ) . getBindAddress ( ) ; this . conf . setVar ( ConfVars . TAJO_MASTER_UMBILICAL_RPC_ADDRESS , NetUtils . getHostPortString ( tajoMasterAddress ) ) ; this . conf . setVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS , "
public void test() { try { final Session session = relayClient . createConnection ( relayHost , relayPort , securedRelay ) ; final GatekeeperRelayRequest request = relayClient . sendAcknowledgementAndReturnRequest ( session , gaMsg ) ; final Object response = handleRequest ( request ) ; relayClient . sendResponse ( session , request , response ) ; } catch ( final JMSException | ArrowheadException ex ) { logger . error ( "GMSException: {}" , ex . getMessage ( ) ) ; logger . debug ( "Exception:" , ex ) ; } }
public void test() { try { final Session session = relayClient . createConnection ( relayHost , relayPort , securedRelay ) ; final GatekeeperRelayRequest request = relayClient . sendAcknowledgementAndReturnRequest ( session , gaMsg ) ; final Object response = handleRequest ( request ) ; relayClient . sendResponse ( session , request , response ) ; } catch ( final JMSException | ArrowheadException ex ) { logger . debug ( "Error while communicating with an other gatekeeper: {}" , ex . getMessage ( ) ) ; logger . debug ( "Exception:" , ex ) ; } }
public void test() { try { ListTopicsOptions listOptions = new ListTopicsOptions ( ) . listInternal ( true ) ; return mapFuture ( adminClient . listTopics ( listOptions ) . names ( ) ) ; } catch ( Exception e ) { log . warn ( "" , e ) ; return Future . failedFuture ( e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( IOException e ) { LOGGER . error ( "Unable to retrieve file" , e ) ; } }
@ Override public void restart ( long seekTo ) { lastCurrentPosition = 0 ; load ( videoUri , seekTo , playWhenPrepared ) ; logger . debug ( "Resetting video frame {} to {}" , videoUri , playWhenPrepared ) ; }
public void test() { try { api . deleteTag ( tag . getUuid ( ) ) ; } catch ( Exception ex ) { LOG . warn ( "Error while deleting tag: " + tag . getUuid ( ) , ex ) ; } }
public void test() { try { _log . info ( "validating dates" ) ; validateDates ( bundleStartDate , bundleEndDate ) ; _log . info ( "dates valid" ) ; } catch ( DateValidationException e ) { code_block = TryStatement ;  } }
public void test() { try { _log . info ( "validating dates...." ) ; validateDates ( bundleStartDate , bundleEndDate ) ; } catch ( DateValidationException e ) { _log . info ( "validating " + bundleStartDate + " -> " + bundleEndDate ) ; code_block = TryStatement ;  } }
public void test() { if ( response == null ) { BundleBuildRequest buildRequest = new BundleBuildRequest ( ) ; buildRequest . setBundleDirectory ( bundleDirectory ) ; buildRequest . setBundleName ( bundleName ) ; buildRequest . setEmailAddress ( email ) ; buildRequest . setBundleStartDate ( bundleStartDate ) ; buildRequest . setBundleEndDate ( bundleEndDate ) ; buildRequest . setBundleComment ( bundleComment ) ; buildRequest . setArchiveFlag ( archive ) ; buildRequest . setConsolidateFlag ( consolidate ) ; buildRequest . setPredate ( predate ) ; String session = RequestContextHolder . currentRequestAttributes ( ) . getSessionId ( ) ; buildRequest . setSessionId ( session ) ; code_block = TryStatement ;  } else { log . error ( String . format ( "Invalid message bundle %s" , bundle ) ) ; } }
public void test() { if ( attributeParser != null ) { boolean isAttributeRead = attributeParser . processAttribute ( currentNode . getLocalPart ( ) , attributeName . getLocalPart ( ) , attribute . getValue ( ) , attributeName . getNamespaceURI ( ) , attributeName . getPrefix ( ) , isLastAttribute , sbmlElements . peek ( ) ) ; code_block = IfStatement ; } else { LOG . warn ( "Unable to process attribute " + sbmlElements . peek ( ) ) ; } }
public void test() { try ( Tx tx = StructrApp . getInstance ( securityContext ) . tx ( ) ) { boolean exists = ( getStructrUser ( string ) != null ) ; tx . success ( ) ; return exists ; } catch ( FrameworkException fex ) { logger . error ( "" , fex ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( WikiNodeServiceUtil . class , "getNodes" , _getNodesParameterTypes5 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( java . util . List < com . liferay . wiki . model . WikiNode > ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
private void clearResourceDefinitions ( ) { log . debug ( "Clearing resource definitions" ) ; resourceDefinitionRepository . deleteAll ( ) ; }
public void test() { if ( this . username == null ) { log . info ( "username is null" ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Setting password for user " + username + " to null" ) ; } }
public void test() { try { closeable . close ( ) ; } catch ( final Exception e ) { LOGGER . warn ( "Unable to close iterator" , e ) ; } }
@ SuppressWarnings ( "unchecked" ) public ModuleList nextSubModuleSet ( ) throws IOException { RingFactory coeff = nextCoefficientRing ( ) ; logger . info ( "coeff = " + coeff ) ; vars = nextVariableList ( ) ; logger . info ( "vars = " + Arrays . toString ( vars ) ) ; code_block = IfStatement ; tord = nextTermOrder ( ) ; logger . info ( "tord = " + tord ) ; initFactory ( coeff , parsedCoeff ) ; List < List < GenPolynomial > > m = null ; m = nextSubModuleList ( ) ; logger . info ( "m = " + m ) ; return new ModuleList ( pfac , m ) ; }
@ SuppressWarnings ( "unchecked" ) public ModuleList nextSubModuleSet ( ) throws IOException { RingFactory coeff = nextCoefficientRing ( ) ; logger . info ( "coeff = " + coeff . getClass ( ) . getSimpleName ( ) ) ; vars = nextVariableList ( ) ; logger . info ( "vars = " + Arrays . toString ( vars ) ) ; code_block = IfStatement ; tord = nextTermOrder ( ) ; logger . info ( "tord = " + tord ) ; initFactory ( coeff , parsedCoeff ) ; List < List < GenPolynomial > > m = null ; m = nextSubModuleList ( ) ; logger . info ( "m = " + m ) ; return new ModuleList ( pfac , m ) ; }
@ SuppressWarnings ( "unchecked" ) public ModuleList nextSubModuleSet ( ) throws IOException { RingFactory coeff = nextCoefficientRing ( ) ; logger . info ( "coeff = " + coeff . getClass ( ) . getSimpleName ( ) ) ; vars = nextVariableList ( ) ; logger . info ( "vars = " + Arrays . toString ( vars ) ) ; code_block = IfStatement ; tord = nextTermOrder ( ) ; logger . info ( "tord = " + tord ) ; initFactory ( coeff , parsedCoeff ) ; List < List < GenPolynomial > > m = null ; m = nextSubModuleList ( ) ; logger . info ( "m = " + m ) ; return new ModuleList ( pfac , m ) ; }
@ SuppressWarnings ( "unchecked" ) public ModuleList nextSubModuleSet ( ) throws IOException { RingFactory coeff = nextCoefficientRing ( ) ; logger . info ( "coeff = " + coeff . getClass ( ) . getSimpleName ( ) ) ; vars = nextVariableList ( ) ; logger . info ( "vars = " + Arrays . toString ( vars ) ) ; code_block = IfStatement ; tord = nextTermOrder ( ) ; logger . info ( "tord = " + tord ) ; initFactory ( coeff , parsedCoeff ) ; logger . info ( "m = " + m ) ; List < List < GenPolynomial > > m = null ; m = nextSubModuleList ( ) ; return new ModuleList ( pfac , m ) ; }
public void test() { if ( cmd != null ) { bridge . getStick ( ) . sendCommand ( cmd , Collections . singletonList ( channelId ) ) ; } else { logger . debug ( "No command found for channel {}" , channelId ) ; } }
public void test() { -> { final List < CompletableFuture < DeploymentTestResult > > futures = Lists . newArrayList ( ) ; code_block = ForStatement ; final List < DeploymentTestResult > results = futures . stream ( ) . map ( CompletableFuture :: join ) . collect ( Collectors . toList ( ) ) ; log . info ( "Deployment tests completed: {}" , results . size ( ) ) ; context . setDeploymentTestResults ( results ) ; return null ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( "EndTime record is null" ) ; } }
public void test() { if ( endTime > 0 ) { TimeUnit timeUnit = znRecord . getEnumField ( CommonConstants . Segment . TIME_UNIT , TimeUnit . class , TimeUnit . DAYS ) ; endTimeMs = timeUnit . toMillis ( endTime ) ; } else { LOGGER . warn ( "Failed to find time for record {}" , record ) ; } }
public void test() { try { csrfOptions = Csrf . CsrfOptions . valueOf ( csrfProtection . toString ( ) ) ; } catch ( IllegalArgumentException illegalArgumentException ) { _log . error ( illegalArgumentException . toString ( ) ) ; } }
public void test() { try { AuthTokenUtil . checkCSRFToken ( themeDisplay . getRequest ( ) , CsrfValidationInterceptor . class . getName ( ) ) ; proceed = true ; } catch ( PrincipalException principalException ) { _log . error ( "Unable to validate CSRF token" , principalException ) ; } }
public void test() { if ( args [ 0 ] instanceof ClientDataRequest ) { ClientDataRequest clientDataRequest = ( ClientDataRequest ) args [ 0 ] ; String method = StringUtil . toLowerCase ( clientDataRequest . getMethod ( ) ) ; code_block = IfStatement ; } else { logger . warn ( "Invalid args expected" ) ; } }
public void test() { if ( args . length == 2 ) { code_block = IfStatement ; } else { LOGGER . error ( "Invalid arguments" ) ; } }
public void test() { if ( yamlConfigFile != null ) { LOG . info ( "Using YAML configuration from configuration file: " + yamlConfigFile ) ; YamlConfigFile yamlConfig = readYamlConfigFile ( yamlConfigFile ) ; checkYamlConfig ( yamlConfigFile , yamlConfig ) ; return ParametersHolder . createWithCmdLineAndYaml ( cli , yamlConfig , Command . RUN_JOB ) ; } else { LOG . info ( "Using CLI configuration!" ) ; return ParametersHolder . createWithCmdLine ( cli , Command . RUN_JOB ) ; } }
public void test() { if ( yamlConfigFile != null ) { LOG . info ( "Using YAML configuration file: {}" , yamlConfigFile ) ; YamlConfigFile yamlConfig = readYamlConfigFile ( yamlConfigFile ) ; checkYamlConfig ( yamlConfigFile , yamlConfig ) ; LOG . info ( "Using YAML configuration!" ) ; return ParametersHolder . createWithCmdLineAndYaml ( cli , yamlConfig , Command . RUN_JOB ) ; } else { return ParametersHolder . createWithCmdLine ( cli , Command . RUN_JOB ) ; } }
@ Override public Representation represent ( final Variant variant ) throws ResourceException { List < Recommendation > recommendations ; code_block = IfStatement ; log . debug ( recommendations . size ( ) + " recommendations found" ) ; JSONObject json = new JSONObject ( ) ; JSONArray recos = new JSONArray ( ) ; code_block = IfStatement ; json . put ( RECOMMENDATIONS_KEY , recos ) ; log . debug ( "response:" + json . toString ( ) ) ; Representation rep = new StringRepresentation ( json . toString ( ) , MediaType . APPLICATION_JSON ) ; rep . setExpirationDate ( new Date ( 0L ) ) ; return rep ; }
@ Override public Representation represent ( final Variant variant ) throws ResourceException { log . debug ( "Getting recommendations for user with OS ID: " + openSocialId ) ; List < Recommendation > recommendations ; code_block = IfStatement ; JSONObject json = new JSONObject ( ) ; JSONArray recos = new JSONArray ( ) ; code_block = IfStatement ; json . put ( RECOMMENDATIONS_KEY , recos ) ; log . debug ( "Successfully retrieved user with OS ID: " + json . toString ( ) ) ; Representation rep = new StringRepresentation ( json . toString ( ) , MediaType . APPLICATION_JSON ) ; rep . setExpirationDate ( new Date ( 0L ) ) ; return rep ; }
private ByteBuffer recommendProtocolVersion ( ) { final ByteBuffer buffer = ByteBuffer . allocate ( 1 ) ; buffer . put ( ( byte ) protocolVersion ) ; buffer . rewind ( ) ; readTimeout = System . currentTimeMillis ( ) + timeoutMillis ; phase = TransactionPhase . RECEIVE_PROTOCOL_VERSION_ACKNOWLEDGMENT ; LOG . debug ( "Read protocol version {}" , protocolVersion ) ; return buffer ; }
public void test() { if ( deploymentResourceArtifacts == null ) { log . debug ( "Could not find deployment resource artifacts" ) ; throw new ByActionStatusComponentException ( ActionStatus . GENERAL_ERROR ) ; } }
public void test() { if ( placeHolderData == null ) { log . debug ( "Null placeHolder data" ) ; throw new ByActionStatusComponentException ( ActionStatus . GENERAL_ERROR ) ; } }
public void test() { if ( addHeatEnvArtifact . isRight ( ) ) { log . debug ( "addHeatEnvArtifact error" ) ; throw new ByResponseFormatComponentException ( componentsUtils . getResponseFormatForResourceInstance ( componentsUtils . convertFromStorageResponseForResourceInstance ( addHeatEnvArtifact . right ( ) . value ( ) , false ) , "" , null ) ) ; } }
private static void createExperimentEntities ( ) { ExperimentTest experimentTest = new ExperimentTest ( ) ; CreateExperiment createExperimentRequest = experimentTest . getCreateExperimentRequest ( project . getId ( ) , "Experiment_1" ) ; CreateExperiment . Response createExperimentResponse = experimentServiceStub . createExperiment ( createExperimentRequest ) ; experiment = createExperimentResponse . getExperiment ( ) ; LOGGER . info ( "Experiment Response : " + createExperimentResponse . getExperiment ( ) ) ; assertEquals ( "Experiment name not match with expected Experiment name" , createExperimentRequest . getName ( ) , experiment . getName ( ) ) ; }
public void test() { try { LOG . info ( "Building suggester index for: " + suggester . getName ( ) ) ; final long startMillis = System . currentTimeMillis ( ) ; suggester . build ( core , newSearcher ) ; final long timeTakenMillis = System . currentTimeMillis ( ) - startMillis ; LOG . info ( "Built suggester " + suggester . getName ( ) + ", took " + timeTakenMillis + " ms" ) ; } catch ( Exception e ) { LOG . error ( "Exception in building suggester index for: " + suggester . getName ( ) , e ) ; } }
public void test() { try { LOG . info ( "Building suggester index for: " + suggester . getName ( ) ) ; final long startMillis = System . currentTimeMillis ( ) ; suggester . build ( core , newSearcher ) ; final long timeTakenMillis = System . currentTimeMillis ( ) - startMillis ; LOG . info ( "Finished building suggester index for: " + suggester . getName ( ) + " of size: " + timeTakenMillis ) ; } catch ( Exception e ) { LOG . error ( "Exception in building suggester index for: " + suggester . getName ( ) , e ) ; } }
public void test() { try { LOG . info ( "Building suggester index for: " + suggester . getName ( ) ) ; final long startMillis = System . currentTimeMillis ( ) ; suggester . build ( core , newSearcher ) ; final long timeTakenMillis = System . currentTimeMillis ( ) - startMillis ; LOG . info ( "Built suggester " + suggester . getName ( ) + ", took " + timeTakenMillis + " ms" ) ; } catch ( Exception e ) { LOG . warn ( "Error building suggester index: " + e . getMessage ( ) , e ) ; } }
public void test() { switch ( error ) { case NO_ENTRY_FOR_PARTICIPANT : case NO_ENTRY_FOR_SELECTED_BACKENDS : logger . trace ( "DISCOVERY lookup {} for domains: {}, interface: {}, {}, gbids: {} returned DiscoveryError {}, continuing" , arbitrationCnt , domains , interfaceName , interfaceVersion , Arrays . toString ( gbids ) , error ) ; code_block = IfStatement ; break ; case UNKNOWN_GBID : case INVALID_GBID : case INTERNAL_ERROR : default : arbitrationFailed ( new ApplicationException ( error ) ) ; logger . trace ( "DISCOVERY lookup {} for domains: {}, interface: {}, {}, gbids: {} returned INTERNAL_ERROR" , arbitrationCnt , domains , interfaceName , interfaceVersion , Arrays . toString ( gbids ) ) ; break ; } }
public void test() { try { listener . gotFavorites ( statuses ) ; } catch ( Exception e ) { logger . warn ( "Exception at getFavorites" , e ) ; } }
public void test() { if ( LOGGER . isInfoEnabled ( ) ) { LOGGER . info ( "Success!" ) ; } }
private RestResponse < ObjectMap > getAcl ( ) throws CatalogException , ClientException { logger . debug ( "Getting Acl" ) ; StudyCommandOptions . AclsCommandOptions c = studiesCommandOptions . aclsCommandOptions ; ObjectMap params = new ObjectMap ( ) ; params . putIfNotEmpty ( "member" , c . memberId ) ; return openCGAClient . getStudyClient ( ) . acl ( getSingleValidStudy ( c . study ) , params ) ; }
public void test() { try { uc . retrieve ( Imeji . adminUser . getEmail ( ) ) ; } catch ( NotFoundException e ) { LOGGER . info ( "Could not find admin user:" + Imeji . adminUser . getEmail ( ) ) ; Imeji . adminUser = uc . create ( Imeji . adminUser , USER_TYPE . ADMIN ) ; LOGGER . info ( "Created admin user successfully:" + Imeji . adminUser . getEmail ( ) , e ) ; } }
public void test() { try { uc . retrieve ( Imeji . adminUser . getEmail ( ) ) ; } catch ( NotFoundException e ) { LOGGER . info ( "!!! IMPORTANT !!! Create admin@imeji.org as system administrator with password admin. !!! CHANGE PASSWORD !!!" ) ; LOGGER . debug ( e . getMessage ( ) ) ; Imeji . adminUser = uc . create ( Imeji . adminUser , USER_TYPE . ADMIN ) ; } }
public void test() { for ( User admin : admins ) { log . debug ( "admin: " + admin ) ; } }
public void test() { try { adminUser = new User ( ) ; adminUser . setPerson ( ImejiFactory . newPerson ( "Admin" , "imeji" , "imeji community" ) ) ; adminUser . setEmail ( ADMIN_EMAIL_INIT ) ; adminUser . setEncryptedPassword ( StringHelper . convertToMD5 ( ADMIN_PASSWORD_INIT ) ) ; adminUser . getGrants ( ) . addAll ( AuthorizationPredefinedRoles . imejiAdministrator ( adminUser . getId ( ) . toString ( ) ) ) ; adminUser . setApiKey ( APIKeyAuthentication . generateKey ( adminUser . getId ( ) , Integer . MAX_VALUE ) ) ; UserController uc = new UserController ( Imeji . adminUser ) ; List < User > admins = uc . retrieveAllAdmins ( ) ; code_block = IfStatement ; } catch ( AlreadyExistsException e ) { } catch ( Exception e ) { LOGGER . error ( e . getMessage ( ) , e ) ; code_block = IfStatement ; } }
public void test() { if ( e . getCause ( ) instanceof AlreadyExistsException ) { LOGGER . error ( "Admin user already exists" , e ) ; } else { throw new RuntimeException ( "Error initializing Admin user! " , e ) ; } }
@ PayloadRoot ( localPart = "FindDevicesWhichHaveNoOwnerRequest" , namespace = DEVICE_MANAGEMENT_NAMESPACE ) @ ResponsePayload public FindDevicesWhichHaveNoOwnerResponse findDevicesWhichHaveNoOwner ( @ OrganisationIdentification final String organisationIdentification , @ RequestPayload final FindDevicesWhichHaveNoOwnerRequest request ) throws OsgpException { LOGGER . info ( "FindDevicesWhichHaveNoOwner response for organisation: {}." , organisationIdentification ) ; final FindDevicesWhichHaveNoOwnerResponse response = new FindDevicesWhichHaveNoOwnerResponse ( ) ; code_block = TryStatement ;  return response ; }
public void test() { if ( keyFactory != null ) { KeyManager [ ] keyManager = keyFactory . getKeyManagers ( ) ; code_block = IfStatement ; } else { LOG . trace ( "No KeyManager found in configuration" ) ; } }
public void test() { if ( success ) { code_block = TryStatement ;  } }
public void test() { try { monitorHeartbeats ( ) ; } catch ( final Exception e ) { LOGGER . error ( "Failed to process heartbeats from nodes due to {}" , e . toString ( ) , e ) ; clusterCoordinator . reportEvent ( null , Severity . ERROR , "Failed to process heartbeats from nodes due to " + e . toString ( ) ) ; } }
public void test() { try { Assert . assertEquals ( 100L , liveServer . getServer ( ) . getNetworkHealthCheck ( ) . getPeriod ( ) ) ; liveServer . getServer ( ) . getNetworkHealthCheck ( ) . setTimeUnit ( TimeUnit . MILLISECONDS ) ; Assert . assertFalse ( liveServer . getServer ( ) . getNetworkHealthCheck ( ) . check ( ) ) ; Wait . assertFalse ( liveServer :: isStarted ) ; liveServer . getServer ( ) . getNetworkHealthCheck ( ) . setIgnoreLoopback ( true ) . addAddress ( "127.0.0.1" ) ; Wait . assertTrue ( liveServer :: isStarted ) ; Assert . assertTrue ( component . isStarted ( ) ) ; } catch ( Throwable e ) { log . error ( e . getMessage ( ) , e ) ; throw e ; } finally { liveServer . getServer ( ) . stop ( ) ; backupServer . getServer ( ) . stop ( ) ; } }
public void test() { try { this . add ( new HostParser ( protocols , protocol ) . get ( url ) ) ; } catch ( HostParserException e ) { LOGGER . warn ( "Failed to parse URL" , e ) ; } }
public void test() { if ( matches ) { String columnName = matcher . group ( 1 ) ; String value = matcher . group ( 2 ) ; String entityTypeId = tryGetEntityTypeName ( tableName ) . orElse ( null ) ; String attributeName = tryGetAttributeName ( tableName , columnName ) . orElse ( null ) ; return new DuplicateValueException ( entityTypeId , attributeName , value , sourceThrowable ) ; } else { LOGGER . error ( ERROR_TRANSLATING_EXCEPTION_MSG , pSqlException ) ; throw new RuntimeException ( ERROR_TRANSLATING_EXCEPTION_MSG , pSqlException ) ; } }
public void test() { if ( StringUtils . isEmpty ( registration . getServiceId ( ) ) ) { log . info ( "No valid service info" ) ; return ; } }
public void test() { try { namingService . registerInstance ( serviceId , group , instance ) ; log . info ( "nacos registry, {} {}:{} register finished" , group , serviceId , instance . getIp ( ) , instance . getPort ( ) ) ; } catch ( Exception e ) { log . error ( "nacos registry exception, {} {}:{} " , group , serviceId , instance . getIp ( ) , instance . getPort ( ) , e ) ; rethrowRuntimeException ( e ) ; } }
public void test() { try { count = c . readAndProcess ( ) ; } catch ( InterruptedException ieo ) { LOG . info ( getName ( ) + ": readAndProcess caught InterruptedException" , ieo ) ; throw ieo ; } catch ( Exception e ) { LOG . info ( getName ( ) + ": readAndProcess caught exception " , e ) ; count = - 1 ; } }
public void test() { try { itNetworkPage . setMessage ( Messages . ItNetworkConverterWizard_PageTitle , DialogPage . INFORMATION ) ; runConvertingInWizard ( ) ; CnAElementFactory . getInstance ( ) . reloadBpModelFromDatabase ( ) ; } catch ( InvocationTargetException | InterruptedException e ) { logger . error ( "error loading the model" , e ) ; itNetworkPage . setMessage ( Messages . ItNetworkConverterWizard_ErrorInformation , DialogPage . ERROR ) ; return false ; } }
public void test() { try { Class . forName ( clazz ) ; providers . put ( name , new DriverInfo ( clazz , config ) ) ; LOG . info ( "Registered ASR driver: {}" , clazz ) ; } catch ( ClassNotFoundException e ) { throw new IllegalArgumentException ( "Cannot register ASR driver " + clazz , e ) ; } }
public void test() { if ( enabled ) { LOGGER . debug ( "Disabling " + instance . target ( ) + " since the profile value matches the active profile." ) ; } else { LOGGER . debug ( "Disabling " + instance . target ( ) + " since the profile value matches the active profile." ) ; } }
public void test() { if ( enabled ) { LOGGER . debug ( "Enabling " + instance . target ( ) + " since the profile value does not match the active profile." ) ; } else { LOGGER . debug ( "Disabling " + instance . target ( ) + " since profile value does not match the active profile." ) ; } }
public void onError ( final Throwable t ) { log . error ( "" , t ) ; }
public void test() { try { listener . createdSavedSearch ( savedSearch ) ; } catch ( Exception e ) { logger . warn ( "Exception at createdSavedSearch" , e ) ; } }
public void testTopic ( String prod_broker_url , String cons_broker_url ) throws Exception { int num_msg ; Connection conn ; Session sess ; String topic_name ; Destination cons_dest ; num_msg = 5 ; conn = createConnection ( cons_broker_url ) ; conn . start ( ) ; sess = conn . createSession ( false , Session . AUTO_ACKNOWLEDGE ) ; topic_name = "topotest2.perm.topic" ; LOG . trace ( "Removing existing Topic" ) ; removeTopic ( conn , topic_name ) ; LOG . trace ( "Creating Topic, " + topic_name ) ; cons_dest = sess . createTopic ( topic_name ) ; LOG . trace ( "Creating Topic, " + topic_name ) ; testOneDest ( conn , sess , cons_dest , num_msg ) ; removeTopic ( conn , topic_name ) ; sess . close ( ) ; conn . close ( ) ; }
public void testTopic ( String prod_broker_url , String cons_broker_url ) throws Exception { int num_msg ; Connection conn ; Session sess ; String topic_name ; Destination cons_dest ; num_msg = 5 ; LOG . info ( "TESTING TOPICS " + prod_broker_url + " -> " + cons_broker_url + " (" + num_msg + " messages)" ) ; conn = createConnection ( cons_broker_url ) ; conn . start ( ) ; sess = conn . createSession ( false , Session . AUTO_ACKNOWLEDGE ) ; topic_name = "topotest2.perm.topic" ; removeTopic ( conn , topic_name ) ; LOG . trace ( "Creating Topic, " + topic_name ) ; cons_dest = sess . createTopic ( topic_name ) ; testOneDest ( conn , sess , cons_dest , num_msg ) ; removeTopic ( conn , topic_name ) ; LOG . trace ( "Consumed topic, " + topic_name ) ; sess . close ( ) ; conn . close ( ) ; }
public void testTopic ( String prod_broker_url , String cons_broker_url ) throws Exception { int num_msg ; Connection conn ; Session sess ; String topic_name ; Destination cons_dest ; num_msg = 5 ; LOG . info ( "TESTING TOPICS " + prod_broker_url + " -> " + cons_broker_url + " (" + num_msg + " messages)" ) ; conn = createConnection ( cons_broker_url ) ; conn . start ( ) ; sess = conn . createSession ( false , Session . AUTO_ACKNOWLEDGE ) ; topic_name = "topotest2.perm.topic" ; LOG . trace ( "Removing existing Topic" ) ; removeTopic ( conn , topic_name ) ; cons_dest = sess . createTopic ( topic_name ) ; LOG . trace ( "Created Topic" ) ; testOneDest ( conn , sess , cons_dest , num_msg ) ; removeTopic ( conn , topic_name ) ; sess . close ( ) ; conn . close ( ) ; }
public void test() { if ( states . peek ( ) != State . INITIAL ) { log . warn ( "Expected transition from " + state ) ; } }
private void addWithGroovyClassLoader ( int x , int y ) throws IllegalAccessException , InstantiationException , IOException { Class calcClass = loader . parseClass ( new File ( "src/main/groovy/com/baeldung/" , "CalcMath.groovy" ) ) ; GroovyObject calc = ( GroovyObject ) calcClass . newInstance ( ) ; Object result = calc . invokeMethod ( "calcSum" , new Object [ ] code_block = "" ; ) ; log . info ( "CalcSum method = " + result ) ; }
public void test() { try { Map < String , Object > returnMap = new HashMap < > ( ) ; Page < OperationLog > page = PageHelper . startPage ( pageNo , pageSize ) ; operationLogDao . selectLogsByOperationGroupId ( groupId ) ; returnMap . put ( "logData" , page ) ; returnMap . put ( "totalCount" , page . getTotal ( ) ) ; returnMap . put ( "totalPage" , page . getPages ( ) ) ; return returnMap ; } catch ( Exception e ) { logger . error ( "get operation log list error." , e ) ; return null ; } }
public void persist ( RechteRolleBericht transientInstance ) { log . debug ( "persisting RechteRolleBericht instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . persist ( transientInstance ) ; log . debug ( "persist successful" ) ; } catch ( RuntimeException re ) { log . error ( "persist failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . persist ( transientInstance ) ; log . debug ( "persist successful" ) ; } catch ( RuntimeException re ) { log . error ( "persist failed" , re ) ; throw re ; } }
public RuleResult execute ( final Map < String , String > ruleParam , Map < String , String > resourceAttributes ) { logger . debug ( "========SSLCertificateExpiryRule started=========" ) ; Annotation annotation = null ; String validTo = null ; long expiredDuration ; long targetExpiredDuration ; String targetExpiryDurationInString = ruleParam . get ( PacmanRuleConstants . EXPIRED_DURATION ) ; String severity = ruleParam . get ( PacmanRuleConstants . SEVERITY ) ; String category = ruleParam . get ( PacmanRuleConstants . CATEGORY ) ; MDC . put ( "executionId" , ruleParam . get ( "executionId" ) ) ; MDC . put ( "ruleId" , ruleParam . get ( PacmanSdkConstants . RULE_ID ) ) ; List < LinkedHashMap < String , Object > > issueList = new ArrayList < > ( ) ; LinkedHashMap < String , Object > issue = new LinkedHashMap < > ( ) ; code_block = IfStatement ; code_block = IfStatement ; logger . debug ( "========SSLCertificateExpiryRule ended=========" ) ; return new RuleResult ( PacmanSdkConstants . STATUS_SUCCESS , PacmanRuleConstants . SUCCESS_MESSAGE ) ; }
public void test() { if ( ! PacmanUtils . doesAllHaveValue ( targetExpiryDurationInString , severity , category ) ) { logger . info ( PacmanRuleConstants . MISSING_CONFIGURATION ) ; throw new InvalidInputException ( PacmanRuleConstants . MISSING_CONFIGURATION ) ; } }
public void test() { if ( expiredDuration <= targetExpiredDuration ) { annotation = Annotation . buildAnnotation ( ruleParam , Annotation . Type . ISSUE ) ; annotation . put ( PacmanSdkConstants . DESCRIPTION , "SSL Expiry within " + targetExpiryDurationInString + " days found!!" ) ; annotation . put ( PacmanRuleConstants . SEVERITY , severity ) ; annotation . put ( PacmanRuleConstants . CATEGORY , category ) ; issue . put ( PacmanRuleConstants . VIOLATION_REASON , "SSL Expiry within " + targetExpiryDurationInString + " days found!!" ) ; issueList . add ( issue ) ; annotation . put ( "issueDetails" , issueList . toString ( ) ) ; logger . debug ( "SSL Expiry within {} days found for annotation {} : {}" , annotation , annotation ) ; return new RuleResult ( PacmanSdkConstants . STATUS_FAILURE , PacmanRuleConstants . FAILURE_MESSAGE , annotation ) ; } }
public void test() { if ( expiredDuration > 0 ) { code_block = IfStatement ; } else { logger . debug ( "No expired token found to '{}'" , token ) ; } }
public RuleResult execute ( final Map < String , String > ruleParam , Map < String , String > resourceAttributes ) { logger . debug ( "========SSLCertificateExpiryRule started=========" ) ; Annotation annotation = null ; String validTo = null ; long expiredDuration ; long targetExpiredDuration ; String targetExpiryDurationInString = ruleParam . get ( PacmanRuleConstants . EXPIRED_DURATION ) ; String severity = ruleParam . get ( PacmanRuleConstants . SEVERITY ) ; String category = ruleParam . get ( PacmanRuleConstants . CATEGORY ) ; MDC . put ( "executionId" , ruleParam . get ( "executionId" ) ) ; MDC . put ( "ruleId" , ruleParam . get ( PacmanSdkConstants . RULE_ID ) ) ; List < LinkedHashMap < String , Object > > issueList = new ArrayList < > ( ) ; LinkedHashMap < String , Object > issue = new LinkedHashMap < > ( ) ; code_block = IfStatement ; code_block = IfStatement ; logger . debug ( "========SSLCertificateExpiryRule ended=========" ) ; return new RuleResult ( PacmanSdkConstants . STATUS_SUCCESS , PacmanRuleConstants . SUCCESS_MESSAGE ) ; }
private void moveConfirmationConfiguration ( ) { log . trace ( "Moving confirmation configuration" ) ; Map < String , EmailConfirmationConfiguration > attrsConfig = new HashMap < > ( ) ; Map < String , EmailConfirmationConfiguration > idsConfig = new HashMap < > ( ) ; List < GenericObjectBean > conConfs = genericObjectsDAO . getObjectsOfType ( "confirmationConfiguration" ) ; code_block = ForStatement ; updateAttributeTypes ( attrsConfig ) ; updateIdentityTypes ( idsConfig ) ; genericObjectsDAO . removeObjectsByType ( "confirmationConfiguration" ) ; }
public void test() { if ( maxIdleTime <= timeoutPeriod ) { logger . debug ( "Closing " + maxIdleTime + " " + timeoutPeriod ) ; bRet = false ; } else { logger . debug ( "Timed out: " + maxIdleTime + " " + timeoutPeriod ) ; } }
public void test() { if ( maxIdleTime <= timeoutPeriod ) { logger . debug ( "Not timed out: " + maxIdleTime + " " + timeoutPeriod ) ; bRet = false ; } else { logger . debug ( "Not timed out" ) ; } }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "JDBC::{}" , sql ) ; } }
public void test() { try { session = hibernateTemplate . getSessionFactory ( ) . openSession ( ) ; comprehensionTestQuestionBo = ( ComprehensionTestQuestionBo ) session . get ( ComprehensionTestQuestionBo . class , questionId ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; } finally { code_block = IfStatement ; } }
public void test() { if ( toRecipients != null && toRecipients . size ( ) > 0 ) { toRecipients . stream ( ) . forEach ( logger :: debug ) ; logger . debug ( "sending email: {}" , toRecipients ) ; Map < String , Object > mailDetails = Maps . newHashMap ( ) ; mailDetails . put ( "attachmentUrl" , "" ) ; mailDetails . put ( "from" , CommonUtils . getPropValue ( PacmanSdkConstants . SEND_EMAIL_FROM ) ) ; mailDetails . put ( "mailBodyAsString" , formateCommonFixBody ( silentautoFixTrans , ruleParam , resourceOwner ) ) ; mailDetails . put ( "placeholderValues" , Maps . newHashMap ( ) ) ; mailDetails . put ( "subject" , emailSubject ) ; mailDetails . put ( "to" , toRecipients ) ; CommonUtils . doHttpPost ( CommonUtils . getPropValue ( PacmanSdkConstants . EMAIL_SERVICE_URL ) , gson . toJson ( mailDetails ) , new HashMap < > ( ) ) ; } }
public void test() { try { List < String > toRecipients = Lists . newArrayList ( ) ; String emailCCList = CommonUtils . getPropValue ( PacmanSdkConstants . SEND_EMAIL_CC_KEY ) ; toRecipients . addAll ( Arrays . asList ( emailCCList . split ( "\\s*,\\s*" ) ) ) ; String emailSubject = CommonUtils . getPropValue ( PacmanSdkConstants . SEND_EMAIL_FIX_SUBJECT_PREFIX + ruleParam . get ( PacmanSdkConstants . RULE_ID ) ) ; Gson gson = new GsonBuilder ( ) . disableHtmlEscaping ( ) . create ( ) ; code_block = IfStatement ; } catch ( Exception e ) { LOGGER . error ( "unable to send audit email" , e ) ; } }
public void test() { if ( host . getType ( ) . equalsIgnoreCase ( Host . HostType . Esx . name ( ) ) ) { _log . info ( "Host " + host . getLabel ( ) + " is compatible for vCenter cluster operation" ) ; return true ; } else { _log . info ( "Host " + host . getLabel ( ) + " is not compatible for vCenter cluster operation due to type " + host . getType ( ) ) ; return false ; } }
public void test() { if ( host . getType ( ) . equalsIgnoreCase ( Host . HostType . Esx . name ( ) ) ) { _log . info ( "Host " + host . getLabel ( ) + " is compatible for vCenter cluster operation due to type " + host . getType ( ) + " and OS version " + host . getOsVersion ( ) ) ; return true ; } else { _log . info ( "Host " + host . getLabel ( ) + " is not compatible for vCenter cluster operation due to type " + host . getType ( ) + " and OS version " + host . getOsVersion ( ) ) ; return false ; } }
public void test() { if ( currentTerm != term ) { logger . info ( "Ignoring term [" + term + "]" ) ; return ; } }
public void test() { try { code_block = IfStatement ; } catch ( Throwable t ) { logger . error ( t . getMessage ( ) , t ) ; } }
protected void onNodeDeletePost ( final String networkId , final Node node , final HashMap < String , Response > respList ) { log . debug ( "" ) ; }
public void test() { try { code_block = ForStatement ; checkRequestHasContentLengthOrChunkedEncoding ( request , "After filtering, the request has neither chunked encoding nor content length: " + request ) ; wirePayloadIfEnabled ( wire , request ) ; logger . debug ( "Request %s: %s" , request . hashCode ( ) , request ) ; utils . logRequest ( headerLog , request , ">>" ) ; nativeRequest = convert ( request ) ; response = invoke ( nativeRequest ) ; logger . debug ( "Receiving response %s: %s" , request . hashCode ( ) , response . getStatusLine ( ) ) ; utils . logResponse ( headerLog , response , "<<" ) ; if ( response . getPayload ( ) != null && wire . enabled ( ) ) wire . input ( response ) ; nativeRequest = null ; int statusCode = response . getStatusCode ( ) ; code_block = IfStatement ; } catch ( Exception e ) { IOException ioe = getFirstThrowableOfType ( e , IOException . class ) ; code_block = IfStatement ; command . setException ( new HttpResponseException ( e . getMessage ( ) + " connecting to " + command . getCurrentRequest ( ) . getRequestLine ( ) , command , null , e ) ) ; break ; } finally { cleanup ( nativeRequest ) ; } }
public void test() { try { code_block = ForStatement ; checkRequestHasContentLengthOrChunkedEncoding ( request , "After filtering, the request has neither chunked encoding nor content length: " + request ) ; logger . debug ( "Sending request %s: %s" , request . hashCode ( ) , request . getRequestLine ( ) ) ; wirePayloadIfEnabled ( wire , request ) ; utils . logRequest ( headerLog , request , ">>" ) ; nativeRequest = convert ( request ) ; response = invoke ( nativeRequest ) ; utils . logResponse ( headerLog , response , "<<" ) ; if ( response . getPayload ( ) != null && wire . enabled ( ) ) wire . input ( response ) ; nativeRequest = null ; int statusCode = response . getStatusCode ( ) ; code_block = IfStatement ; } catch ( Exception e ) { IOException ioe = getFirstThrowableOfType ( e , IOException . class ) ; code_block = IfStatement ; logger . error ( e . getMessage ( ) , e ) ; command . setException ( new HttpResponseException ( e . getMessage ( ) + " connecting to " + command . getCurrentRequest ( ) . getRequestLine ( ) , command , null , e ) ) ; break ; } finally { cleanup ( nativeRequest ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
@ Override public STypeIR caseAUnresolvedType ( AUnresolvedType node , IRInfo question ) throws AnalysisException { _logger . log ( Level . FINE , "Can't case AUnresolved type" ) ; return new AUnknownTypeIR ( ) ; }
public void test() { try ( ZipFile zip = new ZipFile ( ftpsFileDownloader . getLocalFilePath ( ) ) ) { processAltNamesFile ( zip , altNamesMap , dnNameMap ) ; processIdentifiersFile ( zip , identifiersMap ) ; processDeletedElementsFile ( zip , deletedElementsMap ) ; processInstitutions ( zip , altNamesMap , identifiersMap , dnNameMap ) ; processDeletedElements ( deletedElementsMap ) ; return true ; } catch ( Exception e ) { LOGGER . error ( "Error importing ringgold" , e ) ; return false ; } finally { LOGGER . warn ( "Ringgold import completed" ) ; } }
public void test() { try ( ZipFile zip = new ZipFile ( ftpsFileDownloader . getLocalFilePath ( ) ) ) { processAltNamesFile ( zip , altNamesMap , dnNameMap ) ; processIdentifiersFile ( zip , identifiersMap ) ; processDeletedElementsFile ( zip , deletedElementsMap ) ; processInstitutions ( zip , altNamesMap , identifiersMap , dnNameMap ) ; processDeletedElements ( deletedElementsMap ) ; return true ; } catch ( Exception e ) { LOGGER . error ( "Error importing RINGGOLD data" , e ) ; return false ; } finally { LOGGER . info ( "EXITINGGOLD" ) ; } }
public void test() { try { String url = this . getAvatarManager ( ) . getAvatarUrl ( this . getUsername ( ) ) ; code_block = IfStatement ; MimetypesFileTypeMap mimeTypesMap = new MimetypesFileTypeMap ( ) ; this . setMimeType ( mimeTypesMap . getContentType ( url ) ) ; File avatar = this . getAvatarManager ( ) . getAvatarResource ( this . getUsername ( ) ) ; code_block = IfStatement ; this . setInputStream ( new FileInputStream ( avatar ) ) ; } catch ( Throwable t ) { logger . error ( "Error loading avatar" , t ) ; return this . extractDefaultAvatarStream ( ) ; } }
public void test() { if ( lastActionTime == null ) { LOG . info ( "Actions have materialized for this coord: {}" , coord . getId ( ) ) ; code_block = IfStatement ; } else { LOG . info ( "Actions have materialized for this coord: {}, last action {}" , coord . getId ( ) , SchemaHelper . formatDateUTC ( lastActionTime ) ) ; code_block = IfStatement ; change ( cluster , coord . getId ( ) , concurrency , endTime , null ) ; } }
public void test() { if ( endTime . compareTo ( coord . getStartTime ( ) ) <= 0 ) { LOG . info ( "Setting end time to START TIME {}" , SchemaHelper . formatDateUTC ( endTime ) ) ; change ( cluster , coord . getId ( ) , concurrency , coord . getStartTime ( ) , null ) ; } else { LOG . info ( "Setting end time to START TIME {}" , SchemaHelper . formatDateUTC ( endTime ) ) ; change ( cluster , coord . getId ( ) , concurrency , endTime , null ) ; } }
public void test() { if ( endTime . compareTo ( coord . getStartTime ( ) ) <= 0 ) { LOG . info ( "Setting end time to START TIME {}" , SchemaHelper . formatDateUTC ( coord . getStartTime ( ) ) ) ; change ( cluster , coord . getId ( ) , concurrency , coord . getStartTime ( ) , null ) ; } else { LOG . info ( "Setting end time to END TIME {}" , SchemaHelper . formatDateUTC ( coord . getEndTime ( ) ) ) ; change ( cluster , coord . getId ( ) , concurrency , endTime , null ) ; } }
public void test() { if ( ! endTime . after ( lastActionTime ) ) { Date pauseTime = DateUtil . offsetTime ( endTime , - 1 * 60 ) ; change ( cluster , coord . getId ( ) , concurrency , null , SchemaHelper . formatDateUTC ( pauseTime ) ) ; LOG . info ( "Commit paused in {}ms" , coord . getId ( ) ) ; } }
public void test() { try { logger . debug ( "Entered enforceCompareDatastreamChecksum" ) ; String target = Constants . ACTION . COMPARE_DATASTREAM_CHECKSUM . uri ; context . setActionAttributes ( null ) ; MultiValueMap < URI > resourceAttributes = new MultiValueMap < URI > ( ) ; URI name = null ; code_block = TryStatement ;  context . setResourceAttributes ( resourceAttributes ) ; xacmlPep . enforce ( context . getSubjectValue ( Constants . SUBJECT . LOGIN_ID . uri ) , target , Constants . ACTION . APIM . uri , pid , extractNamespace ( pid ) , context ) ; } finally { logger . debug ( "Exiting enforceCompareDatastreamChecksum" ) ; } }
public void test() { try { logger . debug ( "Entered enforceCompareDatastreamChecksum" ) ; String target = Constants . ACTION . COMPARE_DATASTREAM_CHECKSUM . uri ; context . setActionAttributes ( null ) ; MultiValueMap < URI > resourceAttributes = new MultiValueMap < URI > ( ) ; URI name = null ; code_block = TryStatement ;  context . setResourceAttributes ( resourceAttributes ) ; xacmlPep . enforce ( context . getSubjectValue ( Constants . SUBJECT . LOGIN_ID . uri ) , target , Constants . ACTION . APIM . uri , pid , extractNamespace ( pid ) , context ) ; } finally { logger . debug ( "Exiting enforceCompareDatastreamChecksum" ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { resAttr = ResourceAttributes . getResources ( parts ) ; code_block = IfStatement ; actions . put ( Constants . ACTION . ID . getURI ( ) , Constants . ACTION . GET_DATASTREAM . getStringAttribute ( ) ) ; actions . put ( Constants . ACTION . ID . getURI ( ) , Constants . ACTION . GET_DATASTREAM_DISSEMINATION . getStringAttribute ( ) ) ; actions . put ( Constants . ACTION . API . getURI ( ) , Constants . ACTION . APIA . getStringAttribute ( ) ) ; req = getContextHandler ( ) . buildRequest ( getSubjects ( request ) , actions , resAttr , getEnvironment ( request ) ) ; LogUtil . statLog ( request . getRemoteUser ( ) , Constants . ACTION . GET_DATASTREAM_DISSEMINATION . uri , parts [ 1 ] , parts [ 3 ] ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; throw new ServletException ( e . getMessage ( ) , e ) ; } }
@ Override public void delete ( String bucket ) { s3Client . deleteBucket ( bucket ) ; log . info ( "Bucket " + bucket + " is deleted" ) ; s3Client . waiters ( ) . bucketNotExists ( ) . run ( new WaiterParameters ( new HeadBucketRequest ( bucket ) ) ) ; log . info ( "Bucket " + bucket + " is deleted" ) ; }
@ Override public void delete ( String bucket ) { s3Client . deleteBucket ( bucket ) ; log . info ( "Request to delete " + bucket + " sent" ) ; s3Client . waiters ( ) . bucketNotExists ( ) . run ( new WaiterParameters ( new HeadBucketRequest ( bucket ) ) ) ; log . info ( "Bucket " + bucket + " was deleted" ) ; }
private void validateFailInConfigWithExplicitTotalFlinkAndManagedMem ( final Configuration customConfig ) { log . info ( "Validating failInConfigWithExplicitTotalFlinkAndManagedMem" ) ; final Configuration config = configWithExplicitTotalFlinkAndManagedMem ( ) ; config . addAll ( customConfig ) ; validateFail ( config ) ; }
public void test() { try { Thread . sleep ( 100 ) ; } catch ( InterruptedException e ) { logger . error ( "" , e ) ; } }
public void test() { try { tagXmlExecutor . awaitTermination ( 120 , TimeUnit . SECONDS ) ; code_block = WhileStatement ; } catch ( InterruptedException | ExecutionException e ) { LOG . warn ( "Unable to disable tag XML" , e ) ; } }
public void test() { if ( doc != null ) { setMetaId ( doc . nextMetaId ( ) ) ; logger . debug ( format ( "Some annotations would get lost because there was no metaid defined on {0}. To avoid this, an automatic metaid ''{0}'' has been generated." , getElementName ( ) , getMetaId ( ) ) ) ; getAnnotation ( ) . setAbout ( '' + getMetaId ( ) ) ; } else { logger . debug ( format ( "No annotations would get lost because there was no metaid '0'." ) ) ; } }
public void test() { if ( plugin != null ) { Map < String , String > pluginAttributes = plugin . writeXMLAttributes ( ) ; code_block = IfStatement ; } else { LOGGER . error ( "Plugin '{}' not found" , pluginName ) ; } }
public void test() { if ( getRequest ( ) . isVerbose ( ) ) { this . logger . info ( LOG_DOWNLOADING , "Downloading extension [{}]" , extension . getId ( ) ) ; } }
private int logRunningTime ( String operationName , SupplierWithException < Integer , IOException > supplier ) throws IOException { long startTimeMillis = System . currentTimeMillis ( ) ; int result = supplier . get ( ) ; LOG . info ( operationName + ": " + result ) ; return result ; }
public void test() { try { Map < String , Object > parameters = new HashMap < > ( ) ; Map < String , Object > extraParameters = new HashMap < > ( ) ; parameters . put ( AssessmentReportAndWorkflowConstants . WORKFLOW_ID , workflowId ) ; extraParameters . put ( "MaxResults" , 5 ) ; List < Long > executionIds = executeQueryWithExtraParameter ( "select distinct executionId FROM InsightsWorkflowExecutionHistory EH WHERE EH.workflowConfig.workflowId = :workflowId ORDER BY executionId DESC" , Long . class , parameters , extraParameters ) ; parameters . clear ( ) ; extraParameters . clear ( ) ; extraParameters . put ( "executionIDs" , executionIds ) ; return executeQueryWithExtraParameter ( "FROM InsightsWorkflowExecutionHistory EH WHERE EH.executionId IN (:executionIDs) ORDER BY executionId DESC" , InsightsWorkflowExecutionHistory . class , parameters , extraParameters ) ; } catch ( Exception e ) { log . error ( e ) ; throw e ; } }
public void test() { try { start . await ( ) ; retrieves . addAll ( retrieve ( store ) ) ; done . countDown ( ) ; } catch ( IOException e ) { log . info ( "Exception in retrieve" , e ) ; } catch ( InterruptedException e ) { log . info ( "Interrupted in retrieve" , e ) ; } }
public void test() { try { start . await ( ) ; retrieves . addAll ( retrieve ( store ) ) ; done . countDown ( ) ; } catch ( IOException e ) { log . info ( "Exception in retrieve" , e ) ; } catch ( InterruptedException e ) { log . info ( "Interrupted in retrieve" , e ) ; } }
public void resume ( ) { code_block = IfStatement ; pauseHandler . resume ( ) ; setStatus ( EventConsumerStatus . EXECUTING ) ; logger . info ( "Resumed events" ) ; }
private void handleRoomScene ( ChannelUID channelUID , OnOffType command ) { logger . debug ( "handleRoomScene({})" , channelUID ) ; int linkNum ; code_block = SwitchStatement ; int roomNum = ( thingID + 7 ) / 8 ; int param2 = ( ( roomNum * 6 ) - 3 ) + linkNum ; sendOmnilinkCommand ( OnOffType . ON . equals ( command ) ? CommandMessage . CMD_UNIT_UPB_LINK_ON : CommandMessage . CMD_UNIT_UPB_LINK_OFF , 0 , param2 ) ; }
public void test() { switch ( channelUID . getId ( ) ) { case "scene_a" : linkNum = 0 ; break ; case "scene_b" : linkNum = 1 ; break ; case "scene_c" : linkNum = 2 ; break ; case "scene_d" : linkNum = 3 ; break ; default : logger . error ( "Unknown channel to set!" ) ; return ; } }
public void test() { if ( group == null ) { logger . debug ( "getGroup (by id) found group: {} for id: {}" , group . getName ( ) , identifier ) ; } else { logger . debug ( "getGroup (by id) found group: {} for id: {}" , group . getName ( ) , identifier ) ; } }
public void log ( UserData o ) { Node n = o . getNode ( ) ; StringBuilder sb = new StringBuilder ( ) ; sb . append ( n . getParent ( ) ) ; sb . append ( " parent->" ) ; sb . append ( "(" ) ; sb . append ( n ) ; sb . append ( ")->" ) ; List < Spatial > children = n . getChildren ( ) ; code_block = ForStatement ; LOGGER . log ( sb . toString ( ) ) ; }
public List findByExample ( MbMassPhase instance ) { log . debug ( "finding MbMassPhase instance by example" ) ; code_block = TryStatement ;  }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.MbMassPhase" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.MbMassPhase" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
private void testAttributes ( ) throws Exception { log . info ( "Checking attributes..." ) ; Device device = findDeviceByName ( "Edge Device 1" ) ; testAttributesUpdatedMsg ( device ) ; testPostAttributesMsg ( device ) ; testAttributesDeleteMsg ( device ) ; log . info ( "Attributes tested successfully" ) ; }
private void testAttributes ( ) throws Exception { log . info ( "Testing attributes" ) ; Device device = findDeviceByName ( "Edge Device 1" ) ; testAttributesUpdatedMsg ( device ) ; testPostAttributesMsg ( device ) ; testAttributesDeleteMsg ( device ) ; log . info ( "Device deleted" ) ; }
public void test() { try { tis = TikaInputStream . get ( payload , md ) ; tikaType = pika . getDetector ( ) . detect ( tis , md ) . toString ( ) ; } catch ( Throwable e ) { logger . debug ( "Detected error during payload of type {}" , e . getMessage ( ) , e ) ; return MediaType . OCTET_STREAM . toString ( ) ; } finally { code_block = IfStatement ; } }
public void test() { try { parseThread . start ( ) ; parseThread . join ( this . parseTimeout ) ; parseThread . interrupt ( ) ; } catch ( OutOfMemoryError o ) { log . error ( "TikaExtractor.parse(): " + tikaType + " : " + o . getMessage ( ) ) ; } catch ( RuntimeException r ) { log . error ( "TikaExtractor.parse(): " + tikaType + " : " + r . getMessage ( ) ) ; } finally { code_block = IfStatement ; } }
public void test() { try { parseThread . start ( ) ; parseThread . join ( this . parseTimeout ) ; parseThread . interrupt ( ) ; } catch ( OutOfMemoryError o ) { log . error ( "TikaExtractor.parse(): " + tikaType + " : " + o . getMessage ( ) ) ; } catch ( RuntimeException r ) { log . error ( "TikaExtractor.parse(): " + rikaType + " : " + r . getMessage ( ) ) ; } finally { code_block = IfStatement ; } }
public void test() { try { md . set ( Metadata . CONTENT_TYPE , tikaType . toString ( ) ) ; pika . setRecursive ( ctx , false ) ; ch = new WriteOutContentHandler ( MAX_BUF ) ; InputStream tikainput = TikaInputStream . get ( payload , md ) ; ParseRunner runner = new ParseRunner ( pika , tikainput , ch , md , ctx ) ; Thread parseThread = new Thread ( runner , Long . toString ( System . currentTimeMillis ( ) ) ) ; parseThread . setDaemon ( true ) ; code_block = TryStatement ;  String extMimeType = md . get ( PreservationParser . EXT_MIME_TYPE ) ; if ( runner . complete && extMimeType != null ) tikaType = extMimeType ; } catch ( Throwable e ) { logger . warn ( e . getMessage ( ) , e ) ; } }
@ Override public void upgrade ( ) { code_block = IfStatement ; final IndexManagementConfig indexManagementConfig = clusterConfigService . get ( IndexManagementConfig . class ) ; checkState ( indexManagementConfig != null , "Couldn't find index management configuration" ) ; LOG . info ( "Upgrading IndexSet..." ) ; final IndexSetConfig config = IndexSetConfig . builder ( ) . title ( "Default index set" ) . description ( "The Graylog default index set" ) . indexPrefix ( elasticsearchConfiguration . getIndexPrefix ( ) ) . shards ( elasticsearchConfiguration . getShards ( ) ) . replicas ( elasticsearchConfiguration . getReplicas ( ) ) . rotationStrategy ( getRotationStrategyConfig ( indexManagementConfig ) ) . retentionStrategy ( getRetentionStrategyConfig ( indexManagementConfig ) ) . creationDate ( ZonedDateTime . now ( ZoneOffset . UTC ) ) . indexAnalyzer ( elasticsearchConfiguration . getAnalyzer ( ) ) . indexTemplateName ( elasticsearchConfiguration . getTemplateName ( ) ) . indexOptimizationMaxNumSegments ( elasticsearchConfiguration . getIndexOptimizationMaxNumSegments ( ) ) . indexOptimizationDisabled ( elasticsearchConfiguration . isDisableIndexOptimization ( ) ) . build ( ) ; final IndexSetConfig savedConfig = indexSetService . save ( config ) ; clusterConfigService . write ( DefaultIndexSetConfig . create ( savedConfig . id ( ) ) ) ; clusterConfigService . write ( DefaultIndexSetCreated . create ( ) ) ; }
public void test() { try { MainFrame frame = new MainFrame ( configuration ) ; frame . setVisible ( true ) ; configuration . getScripting ( ) . on ( "Startup" , null ) ; LOG . info ( "Started" ) ; } catch ( Exception e ) { e . printStackTrace ( ) ; } }
public void test() { if ( ! dataList . isEmpty ( ) ) { String healthLabels = ":" + routingKey . replace ( "." , ":" ) ; boolean isRecordUpdate = createHealthNodes ( dbHandler , dataList , healthLabels ) ; log . debug ( "Webhook Health Record update status {} ==== " , isRecordUpdate ) ; code_block = IfStatement ; } else { log . error ( "Webhook Health Record is empty for webhook health record: {}" , routingKey ) ; EngineStatusLogger . getInstance ( ) . createEngineStatusNode ( " Data List is empty for webhook health record: " + routingKey , PlatformServiceConstants . FAILURE ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( InsightsCustomException e ) { log . error ( e ) ; } }
public void test() { try { Enumeration < String > aliases = keystore . aliases ( ) ; code_block = WhileStatement ; } catch ( KeyStoreException e ) { Log . log ( e ) ; } }
public void configureRequestLog ( ) { LogConfiguration lc = configuration . logging ( ) ; code_block = IfStatement ; File logDir = new File ( lc . getLogNCSADirectory ( ) ) ; code_block = IfStatement ; RequestLogWriter writer = new RequestLogWriter ( ) ; writer . setAppend ( lc . isLogNCSAAppend ( ) ) ; code_block = IfStatement ; writer . setFilenameDateFormat ( lc . getLogNCSAFilenameDateFormat ( ) ) ; writer . setRetainDays ( lc . getLogNCSARetainDays ( ) ) ; writer . setTimeZone ( lc . getLogNCSATimeZone ( ) ) ; CustomRequestLog requestLog = new CustomRequestLog ( writer , lc . isLogNCSAExtended ( ) ? CustomRequestLog . EXTENDED_NCSA_FORMAT : CustomRequestLog . EXTENDED_NCSA_FORMAT ) ; server . setRequestLog ( requestLog ) ; log . info ( requestLog ) ; }
public void test() { try { code_block = IfStatement ; } catch ( JSONException | IOException | XMLStreamException e ) { log . error ( "BPMN Analytics Core - Process Instance Count Vs ProcessLevelMonitoring error." , e ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { m = new JSONObject ( msg . getText ( ) ) ; code_block = SwitchStatement ; } catch ( Exception e ) { Log . warn ( "Recieved Message Exception." , e ) ; } }
@ Override public void terminateAllResources ( final long duration , final TimeUnit unit ) throws TimeoutException , CloudProvisioningException { final long endTime = System . currentTimeMillis ( ) + unit . toMillis ( duration ) ; s_logger . info ( "Terminating all resources in " + ( endTime - startTime ) + " ms" ) ; code_block = TryStatement ;  }
@ Override public void setWorkingDir ( URL url ) throws IOException { LOGGER . debug ( "Setting url to: " + url ) ; String path = Utils . getWorkingDirectory ( ) . getCanonicalPath ( ) ; code_block = IfStatement ; String title ; code_block = IfStatement ; title = Utils . filesystemSafe ( title ) ; path += title ; path = Utils . getOriginalDirectory ( path ) + File . separator ; this . workingDir = new File ( path ) ; code_block = IfStatement ; LOGGER . debug ( "Set working directory to: " + this . workingDir ) ; }
public synchronized void disconnect ( ) throws IOException { code_block = ForStatement ; fireServiceInactivated ( ) ; idleChecker . destroy ( ) ; logger . info ( "Disconnected" ) ; }
public void test() { try { return STRICT_OBJECT_MAPPER . readValue ( jsonString , clazz ) ; } catch ( final Exception ex ) { logger . warn ( "Exception when de-serializing " + clazz + " with " + jsonString , ex ) ; } }
public void test() { if ( HttpBindManager . LOG_HTTPBIND_ENABLED . getValue ( ) ) { HttpBindManager . LOG_HTTPBIND_ENABLED . getValue ( ) ) ; } }
public void test() { if ( bindingError . getErrorType ( ) == BoshBindingError . Type . terminate ) { logger . info ( "The VM " + session . getVmName ( ) + " has been terminated." ) ; session . close ( ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
private static void filterObjectNames ( final Set < String > objectNames , final String [ ] includes , final String includePattern , final String [ ] excludes , final String excludePattern , final Log log ) throws MojoExecutionException { log . info ( "Looking for matching Object names..." ) ; final Set < String > includedNames = new HashSet < > ( ) ; code_block = IfStatement ; final Set < String > excludedNames = new HashSet < > ( ) ; code_block = IfStatement ; Pattern incPattern ; code_block = IfStatement ; Pattern excPattern ; code_block = IfStatement ; final Set < String > acceptedNames = new HashSet < > ( ) ; code_block = ForStatement ; objectNames . clear ( ) ; objectNames . addAll ( acceptedNames ) ; log . info ( "All objects done." ) ; }
public void test() { if ( newState == BusState . CONNECTED ) { log . info ( "Bus state changed to {}" , newState ) ; } }
public void test() { if ( newState == BusState . CONNECTION_INTERRUPTED ) { log . info ( "Connection lost. Exiting" ) ; } }
public void test() { { log . trace ( "Started blockchain Constants" ) ; ResponseBuilder response = ResponseBuilder . startTiming ( ) ; BlockchainConstantsDto dto = serverInfoService . getBlockchainConstants ( ) ; log . trace ( "blockchain Constants result : {}" , dto ) ; return response . bind ( dto ) . build ( ) ; } }
public void test() { { log . trace ( "Started blockchain Constants" ) ; ResponseBuilder response = ResponseBuilder . startTiming ( ) ; BlockchainConstantsDto dto = serverInfoService . getBlockchainConstants ( ) ; log . trace ( "Blockchain Constants response: {}" , dto ) ; return response . bind ( dto ) . build ( ) ; } }
public void test() { try { CommerceVirtualOrderItemContentDisplayContext commerceVirtualOrderItemContentDisplayContext = new CommerceVirtualOrderItemContentDisplayContext ( _commerceChannelLocalService , _commerceVirtualOrderItemLocalService , _cpDefinitionHelper , _commerceAccountHelper , _cpDefinitionVirtualSettingService , _cpInstanceHelper , _portal . getHttpServletRequest ( renderRequest ) ) ; renderRequest . setAttribute ( WebKeys . PORTLET_DISPLAY_CONTEXT , commerceVirtualOrderItemContentDisplayContext ) ; } catch ( PortalException portalException ) { _log . error ( portalException , portalException ) ; } }
@ Override public void close ( ) throws Exception { LOG . info ( "Closing coordinator for source {}..." , operatorName ) ; code_block = TryStatement ;  LOG . info ( "Source coordinator for source {} closed." , operatorName ) ; }
public void test() { if ( ! reply . isSuccess ( ) ) { logger . warn ( String . format ( "failed to return ip address[uuid:%s]" , msg . getUsedIpUuid ( ) ) ) ; return ; } }
@ Override public void removeAllElectronContainers ( ) { logger . debug ( "Removing all electron containers" ) ; super . removeAllElectronContainers ( ) ; }
@ Before public void setupManager ( ) throws IOException , RepositoryException { String temp = System . getProperty ( "java.io.tmpdir" ) ; home = new File ( temp , getClass ( ) . getName ( ) ) ; delete ( home ) ; home . deleteOnExit ( ) ; code_block = IfStatement ; InputStream configStream = getClass ( ) . getResourceAsStream ( "repository.xml" ) ; RepositoryConfig config = RepositoryConfig . create ( configStream , home . getAbsolutePath ( ) ) ; repo = RepositoryImpl . create ( config ) ; Credentials credentials = new SimpleCredentials ( "admin" , "admin" . toCharArray ( ) ) ; Session session = repo . login ( credentials ) ; NamespaceRegistry nr = session . getWorkspace ( ) . getNamespaceRegistry ( ) ; nr . registerNamespace ( "brix" , "http://brix-cms.googlecode.com" ) ; session . save ( ) ; manager = new LocalWorkspaceManager ( repo ) . initialize ( ) ; log . info ( "Repository created." ) ; }
@ Override public PooledDataSourceFactory getDataSourceFactory ( DPCAttributionConfiguration configuration ) { LOG . trace ( "getDataSourceFactory: {}" , configuration . getDatabase ( ) ) ; return configuration . getDatabase ( ) ; }
public void test() { try { JSONObject json = this . toJSON ( ) ; code_block = IfStatement ; return null ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; return null ; } }
public void test() { try { Response response = client . getResponse ( ) ; saveCookies ( exchange , client , cxfRsEndpoint . getCookieHandler ( ) ) ; code_block = IfStatement ; code_block = IfStatement ; LOG . trace ( "Response body = {}" , response ) ; exchange . getOut ( ) . getHeaders ( ) . putAll ( exchange . getIn ( ) . getHeaders ( ) ) ; final CxfRsBinding binding = cxfRsEndpoint . getBinding ( ) ; exchange . getOut ( ) . getHeaders ( ) . putAll ( binding . bindResponseHeadersToCamelHeaders ( response , exchange ) ) ; exchange . getOut ( ) . setBody ( binding . bindResponseToCamelBody ( body , exchange ) ) ; exchange . getOut ( ) . setHeader ( Exchange . HTTP_RESPONSE_CODE , response . getStatus ( ) ) ; } catch ( Exception exception ) { LOG . error ( "Error while processing request" , exception ) ; fail ( exception ) ; } finally { callback . done ( false ) ; } }
public void test() { try { gateway . setAiravataInternalGatewayId ( UUID . randomUUID ( ) . toString ( ) ) ; code_block = IfStatement ; } catch ( Exception ex ) { logger . error ( "Error adding gateway-profile, reason: " + ex . getMessage ( ) , ex ) ; TenantProfileServiceException exception = new TenantProfileServiceException ( ) ; exception . setMessage ( "Error adding gateway-profile, reason: " + ex . getMessage ( ) ) ; throw exception ; } }
@ RestAccessControl ( permission = Permission . MANAGE_PAGES ) @ RequestMapping ( value = "/pages/search" , method = RequestMethod . GET , produces = MediaType . APPLICATION_JSON_VALUE ) public ResponseEntity < PagedRestResponse < PageDto > > getPages ( @ ModelAttribute ( "user" ) UserDetails user , PageSearchRequest searchRequest ) { this . getPageValidator ( ) . validateRestListRequest ( searchRequest , PageDto . class ) ; LOGGER . debug ( "Requesting user {}" , searchRequest . getUserName ( ) ) ; List < String > groups = this . getAuthorizationService ( ) . getAllowedGroupCodes ( user ) ; PagedMetadata < PageDto > result = this . getPageService ( ) . searchPages ( searchRequest , groups ) ; return new ResponseEntity < > ( new PagedRestResponse < > ( result ) , HttpStatus . OK ) ; }
public void test() { try { final FormBuilder formBuilder = new FormBuilder ( ) ; final CollectionComboBoxModel < String > toolModel = new CollectionComboBoxModel < > ( Arrays . asList ( MAVEN_TOOL , GRADLE_TOOL ) ) ; toolComboBox = new ComboBox < > ( toolModel ) ; formBuilder . addLabeledComponent ( "Tool:" , toolComboBox ) ; groupIdField = new JBTextField ( getCurrentOrDefaultValue ( groupId , "com.example" ) ) ; formBuilder . addLabeledComponent ( "Group:" , groupIdField ) ; artifactIdField = new JBTextField ( getCurrentOrDefaultValue ( artifactId , "azure-function-examples" ) ) ; formBuilder . addLabeledComponent ( "Artifact:" , artifactIdField ) ; versionField = new JBTextField ( getCurrentOrDefaultValue ( version , "1.0.0-SNAPSHOT" ) ) ; formBuilder . addLabeledComponent ( "Version:" , versionField ) ; packageNameField = new JBTextField ( getCurrentOrDefaultValue ( packageName , "org.example.functions" ) ) ; formBuilder . addLabeledComponent ( "Package name:" , packageNameField ) ; panel . add ( ScrollPaneFactory . createScrollPane ( formBuilder . getPanel ( ) , true ) , "North" ) ; } catch ( final RuntimeException e ) { LOGGER . log ( Level . SEVERE , e . getMessage ( ) , e ) ; throw e ; } }
public void test() { try { analyzerClass = Class . forName ( className ) ; } catch ( ClassNotFoundException e ) { LOG . warn ( "Unable to load analyzer class: {}" , className , e ) ; return DEFAULT_ANALYZER ; } }
public void test() { if ( ! Analyzer . class . isAssignableFrom ( analyzerClass ) ) { log . warn ( className + " can not be used as a Analyzer component" ) ; return DEFAULT_ANALYZER ; } else-if ( JackrabbitAnalyzer . class . isAssignableFrom ( analyzerClass ) ) { log . warn ( className + " can not be used as a JackrabbitAnalyzer component" ) ; return DEFAULT_ANALYZER ; } }
public void test() { if ( ! Analyzer . class . isAssignableFrom ( analyzerClass ) ) { log . warn ( className + " is not a Lucene Analyzer" ) ; return DEFAULT_ANALYZER ; } else-if ( JackrabbitAnalyzer . class . isAssignableFrom ( analyzerClass ) ) { log . warn ( className + " is a jackrabbit Analyzer" ) ; return DEFAULT_ANALYZER ; } }
public void test() { try { rootData = jsonData . getJSONObject ( "photos" ) ; } catch ( JSONException innerE ) { logger . debug ( innerE . getMessage ( ) , innerE ) ; break ; } }
public void test() { try { addURLToDownload ( getLargestImageURL ( data . getString ( "id" ) , apiKey ) ) ; } catch ( MalformedURLException e ) { logger . info ( "Could not build URL for lens service at " + data . getString ( "id" ) ) ; } }
@ ParameterizedTest @ MethodSource ( "getJobs" ) public void testJobLifecycle ( JobInfo jobInfo , String operationName ) throws Exception { LOG . debug ( "Running test job" ) ; jobInfo = createJob ( jobInfo ) ; jobInfo = template ( ) . requestBody ( "direct:getJob" , jobInfo , JobInfo . class ) ; assertSame ( JobStateEnum . OPEN , jobInfo . getState ( ) , "Job should be OPEN" ) ; jobInfo = template ( ) . requestBody ( "direct:closeJob" , jobInfo , JobInfo . class ) ; assertSame ( JobStateEnum . CLOSED , jobInfo . getState ( ) , "Job should be CLOSED" ) ; jobInfo = template ( ) . requestBody ( "direct:abortJob" , jobInfo , JobInfo . class ) ; assertSame ( JobStateEnum . ABORTED , jobInfo . getState ( ) , "Job should be ABORTED" ) ; }
public void test() { if ( connectorOperatorEnabled ) { List < NetworkPolicyIngressRule > rules = new ArrayList < > ( 2 ) ; NetworkPolicyIngressRule restApiRule = new NetworkPolicyIngressRuleBuilder ( ) . addNewPort ( ) . withNewPort ( REST_API_PORT ) . withNewProtocol ( "TCP" ) . endPort ( ) . build ( ) ; List < NetworkPolicyPeer > peers = new ArrayList < > ( 2 ) ; NetworkPolicyPeer connectPeer = new NetworkPolicyPeerBuilder ( ) . withNewPodSelector ( ) . addToMatchLabels ( getSelectorLabels ( ) . toMap ( ) ) . endPodSelector ( ) . build ( ) ; peers . add ( connectPeer ) ; NetworkPolicyPeer clusterOperatorPeer = new NetworkPolicyPeerBuilder ( ) . withNewPodSelector ( ) . addToMatchLabels ( Labels . STRIMZI_KIND_LABEL , "cluster-operator" ) . endPodSelector ( ) . build ( ) ; ModelUtils . setClusterOperatorNetworkPolicyNamespaceSelector ( clusterOperatorPeer , namespace , operatorNamespace , operatorNamespaceLabels ) ; peers . add ( clusterOperatorPeer ) ; restApiRule . setFrom ( peers ) ; rules . add ( restApiRule ) ; code_block = IfStatement ; NetworkPolicy networkPolicy = new NetworkPolicyBuilder ( ) . withNewMetadata ( ) . withName ( name ) . withNamespace ( namespace ) . withLabels ( labels . toMap ( ) ) . withOwnerReferences ( createOwnerReference ( ) ) . endMetadata ( ) . withNewSpec ( ) . withNewPodSelector ( ) . addToMatchLabels ( getSelectorLabels ( ) . toMap ( ) ) . endPodSelector ( ) . withIngress ( rules ) . endSpec ( ) . build ( ) ; return networkPolicy ; } else { LOGGER . debug ( "Connector is disabled." ) ; return null ; } }
public void test() { try { ctx . getClassLoader ( ) . loadClass ( "org.apache.logging.log4j.web.ServletRequestThreadContext" ) ; } catch ( final ClassNotFoundException e ) { log . warn ( "Couldn't find Log4j.web.ServletRequestThreadContext" , e ) ; return ; } }
@ Path ( "/list" ) @ GET @ Produces ( "application/json" ) public String getAllDisplays ( ) { _log . info ( "Calling getAllDisplays" ) ; SignCodeData data = null ; code_block = TryStatement ;  List < CCDestinationSignMessage > messages = data . getAllDisplays ( ) ; ModelCounterpartConverter < CCDestinationSignMessage , DestinationSign > tcipToJsonConverter = new SignMessageFromTcip ( ) ; List < DestinationSign > jsonSigns = new ArrayList < DestinationSign > ( ) ; code_block = ForStatement ; DestinationSignsMessage outputMessage = new DestinationSignsMessage ( ) ; outputMessage . setSigns ( jsonSigns ) ; outputMessage . setStatus ( "OK" ) ; String output = null ; code_block = TryStatement ;  _log . info ( "Returning Json from getAllDisplays." ) ; return output ; }
public void test() { try { data = getDataObject ( ) ; } catch ( IOException e ) { logger . error ( "Unable to get data object" , e ) ; throw new WebApplicationException ( e , Response . Status . INTERNAL_SERVER_ERROR ) ; } }
@ Path ( "/list" ) @ GET @ Produces ( "application/json" ) public String getAllDisplays ( ) { _log . info ( "Starting getAllDisplays." ) ; SignCodeData data = null ; code_block = TryStatement ;  List < CCDestinationSignMessage > messages = data . getAllDisplays ( ) ; ModelCounterpartConverter < CCDestinationSignMessage , DestinationSign > tcipToJsonConverter = new SignMessageFromTcip ( ) ; List < DestinationSign > jsonSigns = new ArrayList < DestinationSign > ( ) ; code_block = ForStatement ; DestinationSignsMessage outputMessage = new DestinationSignsMessage ( ) ; outputMessage . setSigns ( jsonSigns ) ; outputMessage . setStatus ( "OK" ) ; String output = null ; code_block = TryStatement ;  _log . info ( "Finished getAllDisplays." ) ; return output ; }
@ PUT @ Path ( "/{id}" ) public Response updateDocument ( @ PathParam ( "id" ) String id , @ Context HttpHeaders headers , InputStream message ) { LOG . info ( "updateDocument called" ) ; return Response . ok ( ) . build ( ) ; }
public void test() { try { String aS = ( String ) a . get ( "name" ) ; String bS = ( String ) b . get ( "name" ) ; return aS . compareToIgnoreCase ( bS ) ; } catch ( JSONException ex ) { log . error ( ex . getMessage ( ) ) ; return 1 ; } }
public void test() { try { FileOutputStream fileOut = new FileOutputStream ( path ) ; ObjectOutputStream out = new ObjectOutputStream ( fileOut ) ; out . writeObject ( obj ) ; out . close ( ) ; fileOut . close ( ) ; } catch ( Exception ex ) { log . error ( "Unable to save file: " + path , ex ) ; return false ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( "Closing connection {}" , connection ) ; } }
public void test() { if ( span != null ) { code_block = IfStatement ; SpanDecorator sd = getSpanDecorator ( route . getEndpoint ( ) ) ; sd . post ( span , exchange , route . getEndpoint ( ) ) ; finishSpan ( span ) ; ActiveSpanManager . deactivate ( exchange ) ; } else { LOG . warn ( "No span found for route {}" , route ) ; } }
public void test() { try { code_block = IfStatement ; SpanAdapter span = ActiveSpanManager . getSpan ( exchange ) ; code_block = IfStatement ; } catch ( Exception t ) { log . warn ( "Error processing exchange: " + exchange , t ) ; } }
public void test() { if ( newDiskSpace < vdDataDisk . getCapacityInKB ( ) ) { logger . error ( "Current disk space: " + vdDataDisk . getCapacityInKB ( ) + " new disk space: " + newDiskSpace ) ; throw new Exception ( Messages . getAll ( "error_invalid_diskspacereduction" ) . get ( 0 ) . getText ( ) ) ; } else-if ( newDiskSpace > vdDataDisk . getCapacityInKB ( ) ) { vdDataDisk . setCapacityInKB ( newDiskSpace ) ; logger . debug ( "Data disk size has been changed. " + newDiskSpace + " KB" ) ; VirtualDeviceConfigSpec vmDeviceSpec = new VirtualDeviceConfigSpec ( ) ; vmDeviceSpec . setOperation ( VirtualDeviceConfigSpecOperation . EDIT ) ; vmDeviceSpec . setDevice ( vdDataDisk ) ; vmConfigSpec . getDeviceChange ( ) . add ( vmDeviceSpec ) ; } else { logger . debug ( "Data disk size has not been changed. " + newDiskSpace + " KB" ) ; } }
public void test() { if ( newDiskSpace < vdDataDisk . getCapacityInKB ( ) ) { logger . error ( "Cannot reduce size of data disk " + vdDataDisk . getDeviceInfo ( ) . getLabel ( ) ) ; throw new Exception ( Messages . getAll ( "error_invalid_diskspacereduction" ) . get ( 0 ) . getText ( ) ) ; } else-if ( newDiskSpace > vdDataDisk . getCapacityInKB ( ) ) { vdDataDisk . setCapacityInKB ( newDiskSpace ) ; logger . debug ( "Data disk size has been changed. " + newDiskSpace + " KB" ) ; VirtualDeviceConfigSpec vmDeviceSpec = new VirtualDeviceConfigSpec ( ) ; vmDeviceSpec . setOperation ( VirtualDeviceConfigSpecOperation . EDIT ) ; vmDeviceSpec . setDevice ( vdDataDisk ) ; vmConfigSpec . getDeviceChange ( ) . add ( vmDeviceSpec ) ; } else { logger . debug ( "Data disk size has not been changed. " + newDiskSpace + " KB" ) ; } }
public void test() { if ( newDiskSpace < vdDataDisk . getCapacityInKB ( ) ) { logger . error ( "Cannot reduce size of data disk " + vdDataDisk . getDeviceInfo ( ) . getLabel ( ) ) ; logger . error ( "Current disk space: " + vdDataDisk . getCapacityInKB ( ) + " new disk space: " + newDiskSpace ) ; throw new Exception ( Messages . getAll ( "error_invalid_diskspacereduction" ) . get ( 0 ) . getText ( ) ) ; } else-if ( newDiskSpace > vdDataDisk . getCapacityInKB ( ) ) { vdDataDisk . setCapacityInKB ( newDiskSpace ) ; VirtualDeviceConfigSpec vmDeviceSpec = new VirtualDeviceConfigSpec ( ) ; vmDeviceSpec . setOperation ( VirtualDeviceConfigSpecOperation . EDIT ) ; vmDeviceSpec . setDevice ( vdDataDisk ) ; vmConfigSpec . getDeviceChange ( ) . add ( vmDeviceSpec ) ; } else { logger . warn ( "Disk space " + vmDevice + " has already been destroyed" ) ; } }
public void delete ( Secuserrole persistentInstance ) { logger . debug ( "deleting Secuserrole instance" ) ; Session session = getSession ( ) ; code_block = TryStatement ;  }
public void test() { try { session . delete ( persistentInstance ) ; logger . debug ( "delete successful" ) ; } catch ( RuntimeException re ) { logger . error ( "delete failed" , re ) ; throw re ; } finally { this . releaseSession ( session ) ; } }
public void test() { try { parserThread . interrupt ( ) ; parserThread . join ( ) ; } catch ( InterruptedException e ) { logger . warn ( "Interrupted in interrupt" ) ; } }
public void test() { if ( ServiceResult . SUCCESS == response . getResult ( ) ) { LOGGER . info ( LOG_MSG_LOGIN_REQUEST_SUCCESS , loginRequest . getEmail ( ) ) ; UI . getCurrent ( ) . getNavigator ( ) . navigateTo ( UserViews . USERHOME_VIEW_NAME ) ; } else { showNotification ( LOGIN_FAILED , response . getErrorMessage ( ) , Notification . Type . WARNING_MESSAGE ) ; LOGGER . info ( LOG_MSG_LOGIN_REQUEST_FAILURE , loginRequest . getEmail ( ) ) ; } }
public void test() { if ( ServiceResult . SUCCESS == response . getResult ( ) ) { LOGGER . info ( LOG_MSG_LOGIN_REQUEST , loginRequest . getEmail ( ) ) ; UI . getCurrent ( ) . getNavigator ( ) . navigateTo ( UserViews . USERHOME_VIEW_NAME ) ; } else { showNotification ( LOGIN_FAILED , response . getErrorMessage ( ) , Notification . Type . WARNING_MESSAGE ) ; LOGGER . info ( LOG_MSG_LOGIN_REQUEST , loginRequest . getEmail ( ) ) ; } }
@ Override public void initialize ( URI name , Configuration conf ) throws IOException { super . initialize ( name , conf ) ; this . bucket = name . getHost ( ) ; code_block = IfStatement ; this . store . initialize ( name , conf ) ; setConf ( conf ) ; this . uri = URI . create ( name . getScheme ( ) + "://" + name . getAuthority ( ) ) ; this . workingDir = new Path ( "/user" , System . getProperty ( "user.name" ) ) . makeQualified ( this . uri , this . getWorkingDirectory ( ) ) ; this . owner = getOwnerId ( ) ; this . group = getGroupId ( ) ; BufferPool . getInstance ( ) . initialize ( this . getConf ( ) ) ; int uploadThreadPoolSize = this . getConf ( ) . getInt ( CosNConfigKeys . UPLOAD_THREAD_POOL_SIZE_KEY , CosNConfigKeys . DEFAULT_UPLOAD_THREAD_POOL_SIZE ) ; int readAheadPoolSize = this . getConf ( ) . getInt ( CosNConfigKeys . READ_AHEAD_QUEUE_SIZE , CosNConfigKeys . DEFAULT_READ_AHEAD_QUEUE_SIZE ) ; int ioThreadPoolSize = uploadThreadPoolSize + readAheadPoolSize / 3 ; long threadKeepAlive = this . getConf ( ) . getLong ( CosNConfigKeys . THREAD_KEEP_ALIVE_TIME_KEY , CosNConfigKeys . DEFAULT_THREAD_KEEP_ALIVE_TIME ) ; this . boundedIOThreadPool = BlockingThreadPoolExecutorService . newInstance ( ioThreadPoolSize / 2 , ioThreadPoolSize , threadKeepAlive , TimeUnit . SECONDS , "cos-transfer-thread-pool" ) ; int copyThreadPoolSize = this . getConf ( ) . getInt ( CosNConfigKeys . COPY_THREAD_POOL_SIZE_KEY , CosNConfigKeys . DEFAULT_COPY_THREAD_POOL_SIZE ) ; this . boundedCopyThreadPool = BlockingThreadPoolExecutorService . newInstance ( CosNConfigKeys . DEFAULT_COPY_THREAD_POOL_SIZE , copyThreadPoolSize , 60L , "transfer
public void test() { if ( varDecl != null ) { varDecl . setFinal ( true ) ; localDecls . add ( varDecl ) ; } else { LOG . warn ( "Can not find a local variable named {}" , localName ) ; } }
public void test() { try { balance = paymentService . getAccountBalance ( paymentheader . getBankaccount ( ) . getId ( ) . toString ( ) , formatter . format ( new Date ( ) ) , paymentheader . getPaymentAmount ( ) , paymentheader . getId ( ) , paymentheader . getBankaccount ( ) . getChartofaccounts ( ) . getId ( ) ) ; } catch ( final ParseException e ) { LOGGER . error ( "Error while parsing PaymentAmount" , e ) ; throw new ValidationException ( Arrays . asList ( new ValidationError ( "Error While formatting date" , "Error While formatting date" ) ) ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void openOpenshiftPage ( ) throws Exception { selenium . getDriver ( ) . get ( ocRoute ) ; code_block = IfStatement ; log . debug ( "Opened route: " + ocRoute ) ; code_block = IfStatement ; }
public void test() { try { logout ( ) ; } catch ( Exception ex ) { LOGGER . log ( Level . SEVERE , null , ex ) ; } }
public void test() { try { processVirtualFile ( vfile , stopped , localProcessor ) ; } catch ( ProcessCanceledException | IndexNotReadyException e ) { throw e ; } catch ( Throwable e ) { logger . warn ( "Failed processing virtual file {}" , vfile . getAbsolutePath ( ) , e ) ; throw e ; } }
public void test() { try { defaultStream = service . load ( Stream . DEFAULT_STREAM_ID ) ; } catch ( NotFoundException ignored ) { code_block = IfStatement ; i ++ ; LOG . debug ( "Successfully loaded stream {}" , id ) ; Uninterruptibles . sleepUninterruptibly ( 500 , TimeUnit . MILLISECONDS ) ; } }
public void test() { if ( i % 10 == 0 ) { log . info ( "Created " + i ) ; } }
public void test() { if ( bridge . getThing ( ) . getStatus ( ) != ThingStatus . ONLINE ) { logger . debug ( "Bridge is not online" ) ; return ; } }
@ Test @ Deprecated public void testAddLegacyLocationDefinition ( ) { Map < String , String > expectedConfig = ImmutableMap . of ( "identity" , "bob" , "credential" , "CR3dential" ) ; ClientResponse response = client ( ) . resource ( "/v1/locations" ) . type ( MediaType . APPLICATION_JSON_TYPE ) . post ( ClientResponse . class , new org . apache . brooklyn . rest . domain . LocationSpec ( legacyLocationName , "aws-ec2:us-east-1" , expectedConfig ) ) ; URI addedLegacyLocationUri = response . getLocation ( ) ; LocationSummary location = client ( ) . resource ( response . getLocation ( ) ) . get ( LocationSummary . class ) ; log . info ( " contents: " + location ) ; log . info ( " location: " + addedLegacyLocationUri ) ; assertEquals ( location . getSpec ( ) , "brooklyn.catalog:" + legacyLocationName + ":" + legacyLocationVersion ) ; assertTrue ( addedLegacyLocationUri . toString ( ) . startsWith ( "/v1/locations/" ) ) ; JcloudsLocation l = ( JcloudsLocation ) getManagementContext ( ) . getLocationRegistry ( ) . resolve ( legacyLocationName ) ; Assert . assertEquals ( l . getProvider ( ) , "aws-ec2" ) ; Assert . assertEquals ( l . getRegion ( ) , "us-east-1" ) ; Assert . assertEquals ( l . getIdentity ( ) , "bob" ) ; Assert . assertEquals ( l . getCredential ( ) , "CR3dential" ) ; }
@ Test @ Deprecated public void testAddLegacyLocationDefinition ( ) { Map < String , String > expectedConfig = ImmutableMap . of ( "identity" , "bob" , "credential" , "CR3dential" ) ; ClientResponse response = client ( ) . resource ( "/v1/locations" ) . type ( MediaType . APPLICATION_JSON_TYPE ) . post ( ClientResponse . class , new org . apache . brooklyn . rest . domain . LocationSpec ( legacyLocationName , "aws-ec2:us-east-1" , expectedConfig ) ) ; URI addedLegacyLocationUri = response . getLocation ( ) ; log . info ( "added legacy, at: " + addedLegacyLocationUri ) ; LocationSummary location = client ( ) . resource ( response . getLocation ( ) ) . get ( LocationSummary . class ) ; log . info ( "found legacy location: " + location . getLocation ( ) ) ; assertEquals ( location . getSpec ( ) , "brooklyn.catalog:" + legacyLocationName + ":" + legacyLocationVersion ) ; assertTrue ( addedLegacyLocationUri . toString ( ) . startsWith ( "/v1/locations/" ) ) ; JcloudsLocation l = ( JcloudsLocation ) getManagementContext ( ) . getLocationRegistry ( ) . resolve ( legacyLocationName ) ; Assert . assertEquals ( l . getProvider ( ) , "aws-ec2" ) ; Assert . assertEquals ( l . getRegion ( ) , "us-east-1" ) ; Assert . assertEquals ( l . getIdentity ( ) , "bob" ) ; Assert . assertEquals ( l . getCredential ( ) , "CR3dential" ) ; }
public void test() { try { raf . close ( ) ; } catch ( IOException e ) { logger . warn ( "release error" , e ) ; } }
@ Override public void run ( ) { Map < String , String > map = new HashMap < String , String > ( ) ; int processedRels = 0 ; Transaction tx = graphDb . beginTx ( ) ; ResourceIterable < Relationship > rels = graphDb . getAllRelationships ( ) ; logger . info ( "Took " + map ) ; code_block = ForStatement ; logger . info ( processedRels + " relations labeled." ) ; tx . success ( ) ; tx . close ( ) ; }
@ Override public void run ( ) { logger . info ( "Starting edge labeling..." ) ; Map < String , String > map = new HashMap < String , String > ( ) ; int processedRels = 0 ; Transaction tx = graphDb . beginTx ( ) ; ResourceIterable < Relationship > rels = graphDb . getAllRelationships ( ) ; code_block = ForStatement ; tx . success ( ) ; tx . close ( ) ; logger . info ( "Ended edge citation..." ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
@ Test public void testCollectionToPrimitiveArrayConversion ( ) throws Exception { List < Integer > list = new ArrayList < > ( ) ; list . add ( 5 ) ; list . add ( 6 ) ; Integer [ ] integerArray = converter . convertTo ( Integer [ ] . class , list ) ; assertEquals ( 2 , integerArray . length , "Integer[] length" ) ; int [ ] intArray = converter . convertTo ( int [ ] . class , list ) ; assertEquals ( 2 , intArray . length , "int[] length" ) ; long [ ] longArray = converter . convertTo ( long [ ] . class , intArray ) ; assertEquals ( 2 , longArray . length , "long[] length" ) ; List < ? > resultList = converter . convertTo ( List . class , intArray ) ; assertEquals ( 2 , resultList . size ( ) , "List size" ) ; LOG . info ( "Result: " + resultList ) ; }
public void attachDirty ( TmpRepZobbaumas instance ) { log . debug ( "attaching dirty TmpRepZobbaumas instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Refreshing." ) ; } }
@ Override public GeoEvent process ( GeoEvent ge ) throws Exception { LOG . info ( "VisibilityProcessor.process starts................." ) ; double radius ; code_block = IfStatement ; srIn = ge . getGeometry ( ) . getSpatialReference ( ) ; code_block = IfStatement ; double elevation ; code_block = IfStatement ; LOG . info ( "Calling ConstructVisibilityRest................." ) ; GeoEvent outGeo = ConstructVisibilityRest ( ge , gp , is , radius , radiusUnit , elevation , units_elev , procwkid ) ; LOG . info ( "VisibilityProcessor.process finished................." ) ; return outGeo ; }
public void test() { if ( bulkResponse . hasFailures ( ) ) { logger . error ( "Bulk update failed" ) ; } }
public void test() { if ( openSettingsButton . isElementPresent ( MINIMAL_TIMEOUT ) ) { openSettingsButton . clickIfPresent ( DELAY ) ; String currentAndroidVersion = IDriverPool . getDefaultDevice ( ) . getOsVersion ( ) ; code_block = IfStatement ; log . debug ( "Opening settings button" ) ; getDriver ( ) . navigate ( ) . back ( ) ; } }
private static void tryAssertion ( RegressionEnvironment env , RegressionPath path , String epl ) { env . compileDeploy ( epl , path ) . addListener ( "s0" ) ; log . info ( "Querying" ) ; long startTime = System . currentTimeMillis ( ) ; code_block = ForStatement ; log . info ( "Done Querying" ) ; long endTime = System . currentTimeMillis ( ) ; log . info ( "delta=" + ( endTime - startTime ) ) ; assertTrue ( ( endTime - startTime ) < 500 ) ; env . undeployModuleContaining ( "s0" ) ; }
private static void tryAssertion ( RegressionEnvironment env , RegressionPath path , String epl ) { env . compileDeploy ( epl , path ) . addListener ( "s0" ) ; log . info ( "Querying" ) ; long startTime = System . currentTimeMillis ( ) ; code_block = ForStatement ; log . info ( "delta=" + ( endTime - startTime ) ) ; long endTime = System . currentTimeMillis ( ) ; log . info ( "delta=" + ( endTime - startTime ) ) ; assertTrue ( ( endTime - startTime ) < 500 ) ; env . undeployModuleContaining ( "s0" ) ; }
@ Deactivate protected void deactivate ( ComponentContext context ) { LOGGER . info ( "Stopping Spring application" ) ; }
public void test() { try { Group group = GroupLocalServiceUtil . getGroup ( groupId ) ; code_block = IfStatement ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; } }
@ Test public void testGetOrder ( ) throws Exception { orderService . setupDummyOrders ( ) ; String response = template . requestBodyAndHeader ( "restlet:http://localhost:8080/orders/{id}?restletMethod=GET" , null , "id" , "1" , String . class ) ; log . info ( "Response: {}" , response ) ; }
public void test() { try { logger . info ( " canal client is down." ) ; clientTest . stop ( ) ; } catch ( Throwable e ) { logger . warn ( "something goes wrong when stopping canal:" , e ) ; } finally { logger . info ( " canal client is down." ) ; } }
public void test() { try { logger . info ( " stop the canal client" ) ; clientTest . stop ( ) ; } catch ( Throwable e ) { logger . error ( e . getMessage ( ) , e ) ; } finally { logger . info ( " canal client is down." ) ; } }
public void test() { try { logger . info ( " stop the canal client" ) ; clientTest . stop ( ) ; } catch ( Throwable e ) { logger . warn ( "something goes wrong when stopping canal:" , e ) ; } finally { logger . info ( " stop the canal client" ) ; } }
public void test() { if ( ServletContextInitializerBeans . logger . isDebugEnabled ( ) ) { ServletContextInitializerBeans . logger . debug ( msg ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( tableTag + " lang sys table missing feature index: " + rf ) ; log . debug ( tableTag + " lang sys table required feature index: " + rf ) ; log . debug ( tableTag + " lang sys table non-required feature count: " + nf ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( tableTag + " lang sys table: " + lo ) ; log . debug ( tableTag + " lang sys table range table: " + lo ) ; log . debug ( tableTag + " lang sys table non-required feature count: " + nf ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( tableTag + " lang sys table: " + lo ) ; log . debug ( tableTag + " lang sys table: " + lo ) ; log . debug ( tableTag + " lang sys table required feature index: " + rf ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( Files . exists ( path ) ) { privateLogger . debug ( "Loading configuration from '{}'" , path . toAbsolutePath ( ) ) ; this . configuration = Configuration . load ( new FileInputStream ( path . toFile ( ) ) ) ; } else { privateLogger . debug ( "Loading configuration from JAR file" ) ; this . configuration = Configuration . load ( Configuration . class . getClassLoader ( ) . getResourceAsStream ( "configuration.yml" ) ) ; } }
public void test() { if ( Files . exists ( path ) ) { privateLogger . debug ( "Loading configuration from '{}'" , path . toAbsolutePath ( ) ) ; this . configuration = Configuration . load ( new FileInputStream ( path . toFile ( ) ) ) ; } else { privateLogger . debug ( "Loading configuration from JAR file" ) ; this . configuration = Configuration . load ( Configuration . class . getClassLoader ( ) . getResourceAsStream ( "configuration.yml" ) ) ; } }
public void test() { try { result . addProperty ( "id" , project . getId ( ) ) ; result . addProperty ( "name" , project . getName ( ) ) ; result . addProperty ( "description" , project . getDescription ( ) ) ; result . addProperty ( "description-html" , project . getDescriptionHtml ( ) ) ; result . addProperty ( "type" , project . getType ( ) . name ( ) ) ; result . addProperty ( "board" , project . getBoard ( ) ) ; result . addProperty ( "private" , project . getPrivate ( ) ) ; result . addProperty ( "shared" , project . getShared ( ) ) ; result . addProperty ( "modified" , DateConversion . toDateTimeString ( project . getModified ( ) . getTime ( ) ) ) ; result . addProperty ( "settings" , project . getSettings ( ) ) ; } catch ( Exception ex ) { LOG . error ( ex . getMessage ( ) , ex ) ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( "Closing connection {}" , connection ) ; } }
public void test() { try { final File dir = Parameters . getStorageDirectory ( getApplication ( ) ) ; code_block = IfStatement ; final File lastShutdownFile = new File ( dir , "last_shutdown.html" ) ; code_block = TryStatement ;  } catch ( final IOException e ) { LOGGER . error ( "Failed to close the application." , e ) ; } }
public void test() { try { genResult . get ( ) ; } catch ( ExecutionException ex ) { LOGGER . error ( "ExecutionException" , ex ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getCapabilities()" ) ; } }
public void test() { if ( deleteSegment ( entry . getKey ( ) , segment ) ) { count = count - 1 ; LOG . info ( "remove segment : {}" , segment ) ; executeHook ( afterDeleted , segment ) ; } else { LOG . warn ( "remove expired segment failed. segment: {}" , segment ) ; return ; } }
public void test() { if ( deleteSegment ( entry . getKey ( ) , segment ) ) { count = count - 1 ; executeHook ( afterDeleted , segment ) ; LOG . info ( "remove expired segment success. segment: {}" , segment ) ; } else { LOG . error ( "remove expired segment failed. segment: {}" , segment ) ; return ; } }
public static boolean download ( URL url , File destFolder , String filename ) throws IOException { LOG . info ( "Downloading plugin..." ) ; File destinationFile = new File ( destFolder . getPath ( ) , filename ) ; LOG . info ( "Source folder: \"{}\"" , url ) ; LOG . info ( "Destination folder: \"{}\"" , destinationFile ) ; URLConnection urlc = url . openConnection ( ) ; boolean result = false ; code_block = TryStatement ;  LOG . info ( "Plugin download completed" ) ; return result ; }
public static boolean download ( URL url , File destFolder , String filename ) throws IOException { File destinationFile = new File ( destFolder . getPath ( ) , filename ) ; LOG . info ( "Plugin download started" ) ; LOG . info ( "Destination folder: \"{}\"" , destinationFile ) ; LOG . info ( urlc . toString ( ) ) ; URLConnection urlc = url . openConnection ( ) ; boolean result = false ; code_block = TryStatement ;  LOG . info ( "Plugin download completed" ) ; return result ; }
public static boolean download ( URL url , File destFolder , String filename ) throws IOException { File destinationFile = new File ( destFolder . getPath ( ) , filename ) ; LOG . info ( "Plugin download started" ) ; LOG . info ( "Source folder: \"{}\"" , url ) ; LOG . info ( destinationFile . toString ( ) ) ; URLConnection urlc = url . openConnection ( ) ; boolean result = false ; code_block = TryStatement ;  LOG . info ( "Plugin download completed" ) ; return result ; }
public static boolean download ( URL url , File destFolder , String filename ) throws IOException { File destinationFile = new File ( destFolder . getPath ( ) , filename ) ; LOG . info ( "Plugin download started" ) ; LOG . info ( "Source folder: \"{}\"" , url ) ; LOG . info ( "Destination folder: \"{}\"" , destinationFile ) ; URLConnection urlc = url . openConnection ( ) ; boolean result = false ; LOG . info ( "Plugin download completed" ) ; code_block = TryStatement ;  return result ; }
public void test() { if ( this . methodName != null ) { Assert . hasText ( this . methodName , "methodName must not be empty." ) ; } else { logger . warn ( "MethodName must be empty." ) ; } }
public void test() { if ( RocksDBMemoryControllerUtils . validateArenaBlockSize ( arenaBlockSize , mutableLimit ) ) { return true ; } else { LOG . warn ( "Attempted to validateArenaBlockSize" ) ; return false ; } }
public void test() { while ( ! dcc . isStopped ( ) ) { log . info ( "Waiting on the Camel Context to stop" ) ; } }
@ Override public void tearDown ( ) throws Exception { super . tearDown ( ) ; DefaultCamelContext dcc = ( DefaultCamelContext ) context ; code_block = WhileStatement ; log . info ( "Closing JMS Session" ) ; code_block = IfStatement ; log . info ( "Closing the ActiveMQ Broker" ) ; code_block = IfStatement ; log . info ( "Stopping the ActiveMQ Broker" ) ; code_block = IfStatement ; }
public void test() { try { String response = formServiceBase . getFormDisplayTask ( containerId , taskId , user , language , filter , formType ) ; code_block = IfStatement ; logger . debug ( "Returning OK response with content '{}'" , response ) ; return createResponse ( response , variant , Response . Status . OK , conversationIdHeader ) ; } catch ( PermissionDeniedException e ) { return permissionDenied ( MessageFormat . format ( TASK_PERMISSION_ERROR , taskId ) , variant , conversationIdHeader ) ; } catch ( TaskNotFoundException e ) { return notFound ( MessageFormat . format ( TASK_INSTANCE_NOT_FOUND , taskId ) , variant , conversationIdHeader ) ; } catch ( DeploymentNotFoundException e ) { return notFound ( MessageFormat . format ( CONTAINER_NOT_FOUND , containerId ) , variant , conversationIdHeader ) ; } catch ( IllegalStateException e ) { return notFound ( "Form for task id " + taskId + " not found" , variant , conversationIdHeader ) ; } catch ( Exception e ) { logger . error ( "Unexpected error during processing {}" , e . getMessage ( ) , e ) ; return internalServerError ( errorMessage ( e ) , variant , conversationIdHeader ) ; } }
public void test() { try { String response = formServiceBase . getFormDisplayTask ( containerId , taskId , user , language , filter , formType ) ; code_block = IfStatement ; logger . debug ( "Returning OK response with content '{}'" , response ) ; return createResponse ( response , variant , Response . Status . OK , conversationIdHeader ) ; } catch ( PermissionDeniedException e ) { return permissionDenied ( MessageFormat . format ( TASK_PERMISSION_ERROR , taskId ) , variant , conversationIdHeader ) ; } catch ( TaskNotFoundException e ) { return notFound ( MessageFormat . format ( TASK_INSTANCE_NOT_FOUND , taskId ) , variant , conversationIdHeader ) ; } catch ( DeploymentNotFoundException e ) { return notFound ( MessageFormat . format ( CONTAINER_NOT_FOUND , containerId ) , variant , conversationIdHeader ) ; } catch ( IllegalStateException e ) { return notFound ( "Form for task id " + taskId + " not found" , variant , conversationIdHeader ) ; } catch ( Exception e ) { logger . error ( "Unexpected error during processing {}" , e . getMessage ( ) , e ) ; return internalServerError ( errorMessage ( e ) , variant , conversationIdHeader ) ; } }
public void doSomething ( ) { log . getName ( ) ; log . getName ( ) ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { try { bookie . suspendProcessing ( ) ; code_block = IfStatement ; l . await ( ) ; bookie . resumeProcessing ( ) ; } catch ( Exception e ) { LOG . error ( "Unexpected exception while waiting for bookie" , e ) ; } }
public void test() { try ( FileOutputStream fout = new FileOutputStream ( dir . get ( ) ) ) { allAccountBalancesBuilder . build ( ) . writeTo ( fout ) ; } catch ( IOException e ) { log . error ( "Unable to write all balances to disk" , e ) ; return ; } }
public void test() { if ( now - getLastLogin ( ) < MIN_TIME_BEFORE_RELOGIN ) { LOG . warn ( "Not attempting to re-login since the last re-login was " + "attempted less than {} seconds before." , MIN_TIME_BEFORE_RELOGIN / 1000 ) ; return false ; } }
public void test() { try { metaConnection . disconnect ( ) ; } catch ( IOException e ) { logger . error ( "Could not disconnect meta connection" , e ) ; } }
public void test() { try { store . close ( ) ; } catch ( SailException e ) { logger . error ( "Failed to close store" , e ) ; } finally { FileUtil . deltree ( dataDir ) ; dataDir = null ; store = null ; disk = null ; } }
@ Test public void testBench ( ) { log . info ( "testBench" ) ; }
public void test() { if ( ( totalSkip / 100 ) % 250 == 0 ) { localBundleSkip . set ( totalSkip ) ; globalBundleSkip . inc ( bundlesSkipped ) ; bundlesSkipped = 0 ; LOGGER . info ( "{} bundles found in {}" , bundleName , bundlesSkipped ) ; } }
public void start ( final boolean exitOnOSGiShutDown ) throws InstantiationException , IllegalAccessException , ClassNotFoundException , BundleException , IOException , InterruptedException { printBanner ( ) ; logger . info ( "FrameworkFactory Class: {}" , factoryClass ) ; logger . info ( "" ) ; FrameworkFactory factory = ( FrameworkFactory ) Class . forName ( factoryClass ) . newInstance ( ) ; framework = factory . newFramework ( frameworkProperties ) ; logger . info ( "Initializing the OSGi framework" ) ; framework . init ( ) ; logger . info ( "The OSGi framework has been initialised" ) ; BundleContext context = framework . getBundleContext ( ) ; List < Bundle > bundles = new ArrayList < > ( ) ; int startLevel = 1 ; code_block = ForStatement ; startBundles ( bundles ) ; code_block = TryStatement ;  addShutdownHook ( ) ; addCleanupOnExit ( exitOnOSGiShutDown ) ; }
public void start ( final boolean exitOnOSGiShutDown ) throws InstantiationException , IllegalAccessException , ClassNotFoundException , BundleException , IOException , InterruptedException { printBanner ( ) ; logger . info ( "----------------- Initialising and Starting the OSGi Framework -----------------" ) ; logger . info ( "" ) ; FrameworkFactory factory = ( FrameworkFactory ) Class . forName ( factoryClass ) . newInstance ( ) ; logger . info ( "Starting the OSGi Framework" ) ; framework = factory . newFramework ( frameworkProperties ) ; framework . init ( ) ; logger . info ( "The OSGi framework has been initialised" ) ; BundleContext context = framework . getBundleContext ( ) ; List < Bundle > bundles = new ArrayList < > ( ) ; int startLevel = 1 ; code_block = ForStatement ; startBundles ( bundles ) ; code_block = TryStatement ;  addShutdownHook ( ) ; addCleanupOnExit ( exitOnOSGiShutDown ) ; }
public void start ( final boolean exitOnOSGiShutDown ) throws InstantiationException , IllegalAccessException , ClassNotFoundException , BundleException , IOException , InterruptedException { printBanner ( ) ; logger . info ( "----------------- Initialising and Starting the OSGi Framework -----------------" ) ; logger . info ( "FrameworkFactory Class: {}" , factoryClass ) ; logger . info ( "" ) ; FrameworkFactory factory = ( FrameworkFactory ) Class . forName ( factoryClass ) . newInstance ( ) ; framework = factory . newFramework ( frameworkProperties ) ; framework . init ( ) ; BundleContext context = framework . getBundleContext ( ) ; List < Bundle > bundles = new ArrayList < > ( ) ; int startLevel = 1 ; code_block = ForStatement ; startBundles ( bundles ) ; code_block = TryStatement ;  addShutdownHook ( ) ; addCleanupOnExit ( exitOnOSGiShutDown ) ; logger . info ( "------------------------------" ) ; }
public void test() { try { logger . info ( "Starting the OSGi framework" ) ; framework . start ( ) ; logger . info ( "" ) ; } catch ( BundleException e ) { logger . error ( "An error occurred when starting the OSGi framework: {}" , e . getMessage ( ) , e ) ; } }
public void test() { try { framework . start ( ) ; logger . info ( "The OSGi framework has been started" ) ; logger . info ( "" ) ; } catch ( BundleException e ) { logger . error ( e . getMessage ( ) , e ) ; } }
protected void localPerform ( ) throws IOException { getLogger ( ) . info ( "Starting to execute losePacketsCommandAction" ) ; ServerName server = PolicyBasedChaosMonkey . selectRandomItem ( getCurrentServers ( ) ) ; String hostname = server . getHostname ( ) ; code_block = TryStatement ;  getLogger ( ) . info ( "Finished to execute losePacketsCommandAction" ) ; }
public void test() { try { clusterManager . execSudoWithRetries ( hostname , timeout , getCommand ( ADD ) ) ; Thread . sleep ( duration ) ; } catch ( InterruptedException e ) { getLogger ( ) . debug ( "Failed to run the command for the duration" , e ) ; } finally { clusterManager . execSudoWithRetries ( hostname , timeout , getCommand ( DELETE ) ) ; } }
protected void localPerform ( ) throws IOException { getLogger ( ) . info ( "Starting to execute LosePacketsCommandAction" ) ; ServerName server = PolicyBasedChaosMonkey . selectRandomItem ( getCurrentServers ( ) ) ; String hostname = server . getHostname ( ) ; code_block = TryStatement ;  getLogger ( ) . info ( "Finished to execute LosePacketsCommandAction" ) ; }
public void test() { if ( streamChangedJobsMap . containsKey ( jobName ) ) { log . warn ( "Stream changed job name: " + jobName ) ; } else { streamChangedJobsMap . put ( jobName , streamChangedJob ) ; } }
public void test() { try { guid = new GUIDImpl ( properties . getProperty ( FIELDS . BASE32 . name ( ) ) ) ; guid2 = new GUIDImpl ( properties . getProperty ( FIELDS . BASE16 . name ( ) ) ) ; } catch ( final InvalidGuidOperationException e ) { LOGGER . error ( ResourcesPublicUtilTest . SHOULD_NOT_HAVE_AN_EXCEPTION , e ) ; fail ( ResourcesPublicUtilTest . SHOULD_NOT_HAVE_AN_EXCEPTION ) ; return ; } }
public void test() { if ( ExceptionUtils . containsInterruptedException ( t ) ) { Thread . currentThread ( ) . interrupt ( ) ; } else { LOGGER . error ( "Error while processing data" , t ) ; } }
public void test() { try { UfsJournalCheckpointWriter journalWriter = mJournal . getCheckpointWriter ( nextSequenceNumber ) ; code_block = TryStatement ;  LOG . info ( "{}: Finished checkpoint [sequence number {}]." , mMaster . getName ( ) , nextSequenceNumber ) ; mNextSequenceNumberToCheckpoint = nextSequenceNumber ; } catch ( IOException e ) { LOG . error ( "{}: Failed to write checkpoint." , mMaster . getName ( ) , e ) ; } }
public void test() { if ( LOGGER . isInfoEnabled ( ) ) { LOGGER . info ( "Success!" ) ; } }
public void test() { try { Thread . sleep ( 10000000 ) ; ModelNode result = new ModelNode ( ) ; result . get ( "testing" ) . set ( operation . get ( "test" ) ) ; logger . info ( "Result: " + result ) ; return result ; } catch ( InterruptedException e ) { interrupted . countDown ( ) ; throw new RuntimeException ( e ) ; } }
public void test() { try { authorizer . replaceAllUsers ( snapshot . getUserMap ( ) ) ; } catch ( AuthException e ) { logger . error ( String . format ( "%s" , e . getMessage ( ) ) ) ; } }
public void test() { if ( value instanceof String && StringHelper . hasStartToken ( ( String ) value , "simple" ) ) { LOGGER . warn ( "Simple property 'simple' can't be used as simple" ) ; } }
public void test() { try { int removedOldFilesCount = fileService . deleteModifiedBefore ( expirationTime ) ; LOGGER . info ( Messages . COULD_NOT_DELETE_FILES_MODIFIED_BEFORE_0 , removedOldFilesCount ) ; } catch ( FileStorageException e ) { throw new SLException ( e , Messages . COULD_NOT_DELETE_FILES_MODIFIED_BEFORE_0 , expirationTime ) ; } }
@ Override public void onError ( JoynrRuntimeException error ) { logger . debug ( "Callback error" ) ; subscribeAttributeEnumerationCallbackResult = false ; subscribeAttributeEnumerationCallbackDone = true ; }
public void test() { try { latch = new CountDownLatch ( 3 ) ; service . notifyEvent ( sessionOpenedEvent ) ; log . info ( "Sending session restored event: {}" , sessionRestored ) ; service . notifyEvent ( sessionRestored ) ; latch . await ( 5 , TimeUnit . SECONDS ) ; assertTrue ( genericEventListener . processed ) ; assertTrue ( sessionOpenedListener . processed ) ; assertTrue ( sessionRestoredListener . processed ) ; resetSessionListeners ( ) ; LensEvent genEvent = new LensEvent ( now ) code_block = "" ; ; latch = new CountDownLatch ( 2 ) ; log . info ( "Sending generic event {}" , genEvent . getEventId ( ) ) ; service . notifyEvent ( genEvent ) ; latch . await ( 5 , TimeUnit . SECONDS ) ; assertTrue ( genericEventListener . processed ) ; resetSessionListeners ( ) ; latch = new CountDownLatch ( 3 ) ; log . info ( "Sending session closed event {}" , sessionClosedEvent ) ; service . notifyEvent ( sessionClosedEvent ) ; log . info ( "Sending session expired event {}" , sessionExpired ) ; service . notifyEvent ( sessionExpired ) ; latch . await ( 5 , TimeUnit . SECONDS ) ; assertTrue ( sessionClosedListener . processed ) ; assertTrue ( sessionExpiredListner . processed ) ; assertFalse ( sessionOpenedListener . processed ) ; assertFalse ( sessionRestoredListener . processed ) ; } catch ( LensException e ) { log . error ( "Lens exception" , e ) ; fail ( e . getMessage ( ) ) ; } }
public void test() { try { latch = new CountDownLatch ( 3 ) ; log . info ( "Sending session opened  event: {}" , sessionOpenedEvent ) ; service . notifyEvent ( sessionOpenedEvent ) ; service . notifyEvent ( sessionRestored ) ; latch . await ( 5 , TimeUnit . SECONDS ) ; assertTrue ( genericEventListener . processed ) ; assertTrue ( sessionOpenedListener . processed ) ; assertTrue ( sessionRestoredListener . processed ) ; resetSessionListeners ( ) ; LensEvent genEvent = new LensEvent ( now ) code_block = "" ; ; latch = new CountDownLatch ( 2 ) ; log . info ( "Sending generic event {}" , genEvent . getEventId ( ) ) ; service . notifyEvent ( genEvent ) ; latch . await ( 5 , TimeUnit . SECONDS ) ; assertTrue ( genericEventListener . processed ) ; resetSessionListeners ( ) ; latch = new CountDownLatch ( 3 ) ; log . info ( "Sending session closed event {}" , sessionClosedEvent ) ; service . notifyEvent ( sessionClosedEvent ) ; log . info ( "Sending session expired event {}" , sessionExpired ) ; service . notifyEvent ( sessionExpired ) ; latch . await ( 5 , TimeUnit . SECONDS ) ; assertTrue ( sessionClosedListener . processed ) ; assertTrue ( sessionExpiredListner . processed ) ; assertFalse ( sessionOpenedListener . processed ) ; assertFalse ( sessionRestoredListener . processed ) ; } catch ( LensException e ) { log . error ( "Lens failed" , e ) ; fail ( e . getMessage ( ) ) ; } }
public void test() { try { latch = new CountDownLatch ( 3 ) ; log . info ( "Sending session opened  event: {}" , sessionOpenedEvent ) ; service . notifyEvent ( sessionOpenedEvent ) ; log . info ( "Sending session restored event: {}" , sessionRestored ) ; service . notifyEvent ( sessionRestored ) ; latch . await ( 5 , TimeUnit . SECONDS ) ; assertTrue ( genericEventListener . processed ) ; assertTrue ( sessionOpenedListener . processed ) ; assertTrue ( sessionRestoredListener . processed ) ; resetSessionListeners ( ) ; LensEvent genEvent = new LensEvent ( now ) code_block = "" ; ; latch = new CountDownLatch ( 2 ) ; log . info ( "Sending session generated event: {}" , genEvent ) ; service . notifyEvent ( genEvent ) ; latch . await ( 5 , TimeUnit . SECONDS ) ; assertTrue ( genericEventListener . processed ) ; resetSessionListeners ( ) ; latch = new CountDownLatch ( 3 ) ; log . info ( "Sending session closed event {}" , sessionClosedEvent ) ; service . notifyEvent ( sessionClosedEvent ) ; log . info ( "Sending session expired event {}" , sessionExpired ) ; service . notifyEvent ( sessionExpired ) ; latch . await ( 5 , TimeUnit . SECONDS ) ; assertTrue ( sessionClosedListener . processed ) ; assertTrue ( sessionExpiredListner . processed ) ; assertFalse ( sessionOpenedListener . processed ) ; assertFalse ( sessionRestoredListener . processed ) ; } catch ( LensException e ) { fail ( e . getMessage ( ) ) ; } }
public void test() { try { latch = new CountDownLatch ( 3 ) ; log . info ( "Sending session opened  event: {}" , sessionOpenedEvent ) ; service . notifyEvent ( sessionOpenedEvent ) ; log . info ( "Sending session restored event: {}" , sessionRestored ) ; service . notifyEvent ( sessionRestored ) ; latch . await ( 5 , TimeUnit . SECONDS ) ; assertTrue ( genericEventListener . processed ) ; assertTrue ( sessionOpenedListener . processed ) ; assertTrue ( sessionRestoredListener . processed ) ; resetSessionListeners ( ) ; LensEvent genEvent = new LensEvent ( now ) code_block = "" ; ; latch = new CountDownLatch ( 2 ) ; log . info ( "Sending generic event {}" , genEvent . getEventId ( ) ) ; service . notifyEvent ( genEvent ) ; latch . await ( 5 , TimeUnit . SECONDS ) ; assertTrue ( genericEventListener . processed ) ; resetSessionListeners ( ) ; latch = new CountDownLatch ( 3 ) ; log . info ( "Sending session closed event {}" , sessionClosed ) ; service . notifyEvent ( sessionClosedEvent ) ; log . info ( "Sending session closed event {}" , sessionExpired ) ; service . notifyEvent ( sessionExpired ) ; latch . await ( 5 , TimeUnit . SECONDS ) ; assertTrue ( sessionClosedListener . processed ) ; assertTrue ( sessionExpiredListner . processed ) ; assertFalse ( sessionOpenedListener . processed ) ; assertFalse ( sessionRestoredListener . processed ) ; } catch ( LensException e ) { fail ( e . getMessage ( ) ) ; } }
public void test() { try { latch = new CountDownLatch ( 3 ) ; log . info ( "Sending session opened  event: {}" , sessionOpenedEvent ) ; service . notifyEvent ( sessionOpenedEvent ) ; log . info ( "Sending session restored event: {}" , sessionRestored ) ; service . notifyEvent ( sessionRestored ) ; latch . await ( 5 , TimeUnit . SECONDS ) ; assertTrue ( genericEventListener . processed ) ; assertTrue ( sessionOpenedListener . processed ) ; assertTrue ( sessionRestoredListener . processed ) ; resetSessionListeners ( ) ; LensEvent genEvent = new LensEvent ( now ) code_block = "" ; ; latch = new CountDownLatch ( 2 ) ; log . info ( "Sending generic event {}" , genEvent . getEventId ( ) ) ; service . notifyEvent ( genEvent ) ; latch . await ( 5 , TimeUnit . SECONDS ) ; assertTrue ( genericEventListener . processed ) ; resetSessionListeners ( ) ; latch = new CountDownLatch ( 3 ) ; log . info ( "Sending session closed event {}" , sessionClosedEvent ) ; service . notifyEvent ( sessionClosedEvent ) ; service . notifyEvent ( sessionExpired ) ; latch . await ( 5 , TimeUnit . SECONDS ) ; assertTrue ( sessionClosedListener . processed ) ; assertTrue ( sessionExpiredListner . processed ) ; assertFalse ( sessionOpenedListener . processed ) ; assertFalse ( sessionRestoredListener . processed ) ; } catch ( LensException e ) { log . error ( "Lens exception" , e ) ; fail ( e . getMessage ( ) ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( ThemeServiceUtil . class , "getWARThemes" , _getWARThemesParameterTypes1 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . portal . kernel . json . JSONArray ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( android ) { logger . info ( "==> Android" ) ; } }
public void test() { while ( sentQueue . peek ( ) != null && sentQueue . peek ( ) . getFrmNum ( ) != ackNum ) { AshFrameData ackedFrame = sentQueue . poll ( ) ; LOG . debug ( "Acked frame {}" , ackedFrame ) ; } }
public void test() { try { Group group = getGroup ( ) ; code_block = IfStatement ; return group . getPrivateLayoutsPageCount ( ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; } }
public void test() { try { Runtime . start ( "gui" , "SwingGui" ) ; OpenCV cv = ( OpenCV ) Runtime . start ( "cv" , "OpenCV" ) ; OpenCVFilterAddMask mask = new OpenCVFilterAddMask ( "mask" ) ; cv . addFilter ( mask ) ; mask . test ( ) ; boolean done = true ; code_block = IfStatement ; cv . capture ( "src\\test\\resources\\OpenCV\\multipleFaces.jpg" ) ; cv . addFilter ( mask ) ; cv . capture ( ) ; } catch ( Exception e ) { log . error ( "main threw" , e ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { Thread . sleep ( failoverPolicy . sleepBetweenHostsMilli ) ; } catch ( InterruptedException e ) { logger . error ( "" , e ) ; } }
@ Override public void configure ( Context context ) { super . configure ( context ) ; serializerType = context . getString ( "serializer" , "TEXT" ) ; useRawLocalFileSystem = context . getBoolean ( "hdfs.useRawLocalFileSystem" , false ) ; serializerContext = new Context ( context . getSubProperties ( EventSerializer . CTX_PREFIX ) ) ; LOG . info ( "useRawLocalFileSystem = {}" , useRawLocalFileSystem ) ; }
public void test() { if ( match + miss >= debug ) { logger . debug ( LoggingMarkers . DUPLICATES ) ; match = 0 ; miss = 0 ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
Source loadXml ( String theSchemaName ) { String pathToBase = myCtx . getVersion ( ) . getPathToSchemaDefinitions ( ) + '/' + theSchemaName ; ourLog . debug ( "Loading schema definition from: {}" , pathToBase ) ; String contents = ClasspathUtil . loadResource ( pathToBase , ClasspathUtil . withBom ( ) ) ; return new StreamSource ( new StringReader ( contents ) , null ) ; }
public void test() { if ( rng != null && seed != null ) { log . warn ( "getSplits() - replaced the 'seed' method" ) ; } }
public static void waitForServiceDeletion ( String serviceName ) { LOGGER . info ( "Waiting for Service {} in namespace {}" , serviceName , kubeClient ( ) . getNamespace ( ) ) ; TestUtils . waitFor ( "Service " + serviceName + " to be deleted" , Constants . POLL_INTERVAL_FOR_RESOURCE_READINESS , DELETION_TIMEOUT , ( ) -> kubeClient ( ) . getService ( serviceName ) == null ) ; LOGGER . info ( "Service {} in namespace {} was deleted" , serviceName , kubeClient ( ) . getNamespace ( ) ) ; }
public static void waitForServiceDeletion ( String serviceName ) { LOGGER . info ( "Waiting for Service {} deletion in namespace {}" , serviceName , kubeClient ( ) . getNamespace ( ) ) ; TestUtils . waitFor ( "Service " + serviceName + " to be deleted" , Constants . POLL_INTERVAL_FOR_RESOURCE_READINESS , DELETION_TIMEOUT , ( ) -> kubeClient ( ) . getService ( serviceName ) == null ) ; LOGGER . info ( "Service {} was deleted" , serviceName ) ; }
public void test() { try { code_block = TryStatement ;  } catch ( Exception e ) { logger . error ( "Failed to create a log file" , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( LOGGER . isInfoEnabled ( ) ) { LOGGER . info ( "Success!" ) ; } }
public void test() { if ( LOGGER . isInfoEnabled ( ) ) { LOGGER . info ( "Success!" ) ; } }
public void test() { try { backend . sendCode ( settings . emailSecurityCodeMsgTemplate , false ) ; return true ; } catch ( Exception e ) { log . warn ( "Credential reset notification failed" , e ) ; NotificationPopup . showError ( msg . getMessage ( "error" ) , msg . getMessage ( "CredentialReset.resetNotPossible" ) ) ; onCancel ( ) ; return false ; } }
public void test() { if ( StatusUtils . isResourceV1alpha1 ( connect ) ) { updateStatusPromise . complete ( ) ; } else { KafkaConnectS2IStatus currentStatus = connect . getStatus ( ) ; StatusDiff ksDiff = new StatusDiff ( currentStatus , desiredStatus ) ; code_block = IfStatement ; } }
public void test() { if ( updateRes . succeeded ( ) ) { log . debug ( "{}: Completed status update" , reconciliation ) ; updateStatusPromise . complete ( ) ; } else { log . error ( "{}: Failed to update status" , reconciliation , updateRes . cause ( ) ) ; updateStatusPromise . fail ( updateRes . cause ( ) ) ; } }
public void test() { if ( updateRes . succeeded ( ) ) { log . debug ( "{}: Completed status update" , reconciliation ) ; updateStatusPromise . complete ( ) ; } else { log . error ( "{}: Failed to update status" , reconciliation , updateRes . cause ( ) ) ; updateStatusPromise . fail ( updateRes . cause ( ) ) ; } }
public void test() { if ( getRes . succeeded ( ) ) { KafkaConnectS2I connect = getRes . result ( ) ; code_block = IfStatement ; } else { log . error ( "{}: Failed to get the Kafka Connect topic" , reconciliation , getRes . cause ( ) ) ; updateStatusPromise . fail ( getRes . cause ( ) ) ; } }
@ Override public void setAVUMetadata ( final String userName , final AvuData avuData ) throws DataNotFoundException , JargonException { code_block = IfStatement ; code_block = IfStatement ; log . info ( "set avu metadata for user: {}" , avuData ) ; log . info ( "userName {}" , userName ) ; log . info ( "userName {}" , userName ) ; code_block = IfStatement ; final ModAvuMetadataInp modifyAvuMetadataInp = ModAvuMetadataInp . instanceForSetUserMetadata ( userName , avuData ) ; log . debug ( "sending avu request" ) ; code_block = TryStatement ;  log . debug ( "metadata added" ) ; }
@ Override public void setAVUMetadata ( final String userName , final AvuData avuData ) throws DataNotFoundException , JargonException { log . info ( "setAVUMetadata()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "userName {}" , userName ) ; log . info ( " avu data:{}" , avuData ) ; code_block = IfStatement ; final ModAvuMetadataInp modifyAvuMetadataInp = ModAvuMetadataInp . instanceForSetUserMetadata ( userName , avuData ) ; log . debug ( "sending avu request" ) ; code_block = TryStatement ;  log . debug ( "metadata added" ) ; }
@ Override public void setAVUMetadata ( final String userName , final AvuData avuData ) throws DataNotFoundException , JargonException { log . info ( "setAVUMetadata()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "set avu metadata for user: {}" , avuData ) ; code_block = IfStatement ; log . info ( "user name: {}" , userName ) ; final ModAvuMetadataInp modifyAvuMetadataInp = ModAvuMetadataInp . instanceForSetUserMetadata ( userName , avuData ) ; log . debug ( "sending avu request" ) ; code_block = TryStatement ;  log . debug ( "metadata added" ) ; }
@ Override public void setAVUMetadata ( final String userName , final AvuData avuData ) throws DataNotFoundException , JargonException { log . info ( "setAVUMetadata()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "set avu metadata for user: {}" , avuData ) ; log . info ( "userName {}" , userName ) ; code_block = IfStatement ; log . debug ( "metadata added" ) ; final ModAvuMetadataInp modifyAvuMetadataInp = ModAvuMetadataInp . instanceForSetUserMetadata ( userName , avuData ) ; code_block = TryStatement ;  log . debug ( "metadata added" ) ; }
public void test() { try { getIRODSProtocol ( ) . irodsFunction ( modifyAvuMetadataInp ) ; } catch ( JargonException je ) { code_block = IfStatement ; log . error ( "jargon exception removing AVU metadata" , je ) ; throw je ; } }
@ Override public void setAVUMetadata ( final String userName , final AvuData avuData ) throws DataNotFoundException , JargonException { log . info ( "setAVUMetadata()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "set avu metadata for user: {}" , avuData ) ; log . info ( "userName {}" , userName ) ; code_block = IfStatement ; final ModAvuMetadataInp modifyAvuMetadataInp = ModAvuMetadataInp . instanceForSetUserMetadata ( userName , avuData ) ; log . debug ( "sending avu request" ) ; code_block = TryStatement ;  log . debug ( "modified avu request" ) ; }
public void test() { if ( resourcePath . size ( ) != 2 || reqPayload . getBody ( ) == null || reqPayload . getBody ( ) . length == 0 || ! resourcePath . get ( 0 ) . equals ( KEYSTORES ) ) { logger . error ( "Invalid payload: {}" , resourcePath ) ; throw new KuraException ( KuraErrorCode . BAD_REQUEST ) ; } }
public void test() { if ( _log . isInfoEnabled ( ) ) { _log . info ( StringBundler . concat ( "Removing " , companyId , " from company " , companyId , " to " , company . getCompanyId ( ) , " using " , company . getCompanyId ( ) ) ) ; } }
public void writeDecimalType ( String path , DecimalType value ) throws OwException { OwserverPacket requestPacket = new OwserverPacket ( OwserverMessageType . WRITE , path ) ; requestPacket . appendPayload ( String . valueOf ( value ) ) ; OwserverPacket returnPacket = request ( requestPacket ) ; logger . debug ( "writeDecimalType(" + path + ", " + value + ")" ) ; }
public void test() { if ( connectionErrorCounter > CONNECTION_MAX_RETRY ) { owserverConnectionState = OwserverConnectionState . FAILED ; tryingConnectionRecovery = false ; thingHandlerCallback . reportConnectionState ( owserverConnectionState ) ; } else-if ( ! tryingConnectionRecovery ) { thingHandlerCallback . reportConnectionState ( owserverConnectionState ) ; LOGGER . error ( "Recovered unexpected connection. ({})" , connectionErrorCounter ) ; } }
public void test() { try ( final BufferedReader br = new BufferedReader ( new InputStreamReader ( new FileInputStream ( validationRuleFile ) , StandardCharsets . UTF_8 ) ) ) { processAssertions ( br ) ; } catch ( final Exception ex ) { logger . error ( ex . getMessage ( ) ) ; } }
public void test() { try { msg . setField ( 0 , identifier ) ; } catch ( Exception ex ) { LOG . debug ( "Failed to set 'identifier': " + identifier ) ; } }
public void test() { try { msg . setField ( 1 , infoID ) ; } catch ( Exception ex ) { LOGGER . debug ( "Failed to set 'infoID': " + infoID ) ; } }
@ Override public UserDetails loadUserDetails ( PreAuthenticatedAuthenticationToken token ) throws UsernameNotFoundException { ApplicationUser user = ( ApplicationUser ) token . getPrincipal ( ) ; Set < GrantedAuthority > authorities = new HashSet < > ( ) ; authorities . addAll ( securityHelper . mapRolesToFunctions ( user . getRoles ( ) ) ) ; authorities . addAll ( securityHelper . getUnrestrictedFunctions ( ) ) ; SecurityUserWrapper result = new SecurityUserWrapper ( user . getUserId ( ) , "N/A" , true , true , true , true , authorities , user ) ; logger . debug ( "Loaded user details for user {}" , user . getUserId ( ) ) ; return result ; }
public void test() { try { final String serverResponse = retrieveResponseFromServer ( new URL ( validationUrl ) , ticket ) ; logger . debug ( "Validating ticket request to: {}" , ticket ) ; code_block = IfStatement ; logger . debug ( "Server response: {}" , serverResponse ) ; return parseResponseFromServer ( serverResponse ) ; } catch ( final MalformedURLException e ) { throw new TicketValidationException ( e ) ; } }
public void test() { try { logger . debug ( "Retrieving response from server." ) ; final String serverResponse = retrieveResponseFromServer ( new URL ( validationUrl ) , ticket ) ; code_block = IfStatement ; logger . debug ( "Retrieved response from server." ) ; return parseResponseFromServer ( serverResponse ) ; } catch ( final MalformedURLException e ) { throw new TicketValidationException ( e ) ; } }
public void test() { try { configStore . initiateUpdate ( newEntity ) ; obtainEntityLocks ( oldEntity , "update" , tokenList ) ; configStore . update ( newEntity . getEntityType ( ) , newEntity ) ; } catch ( Throwable e ) { LOG . error ( "Unable to update configuration" , e ) ; throw FalconWebException . newAPIException ( e ) ; } finally { ConfigurationStore . get ( ) . cleanupUpdateInit ( ) ; releaseEntityLocks ( oldEntity . getName ( ) , tokenList ) ; } }
public void test() { try { code_block = SwitchStatement ; } catch ( NumberFormatException | XPathException e ) { LOG . warn ( e . getMessage ( ) , e ) ; } }
@ Override public void doLoadShedding ( ) { long overloadThreshold = this . getLoadBalancerBrokerOverloadedThresholdPercentage ( ) ; long comfortLoadLevel = this . getLoadBalancerBrokerComfortLoadThresholdPercentage ( ) ; Map < ResourceUnit , String > namespaceBundlesToBeUnloaded = new HashMap < > ( ) ; synchronized ( currentLoadReports ) code_block = "" ; unloadNamespacesFromOverLoadedBrokers ( namespaceBundlesToBeUnloaded ) ; logger . info ( "LoadBalancer Broker completed" ) ; }
public void test() { if ( ! _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "Unable to load bundle " , bundle . getName ( ) , " for bundle " , bundle . getBundle ( ) . getName ( ) ) ) ; } }
public void test() { if ( bundleStats . size ( ) == 1 ) { String bundleName = lr . getBundleStats ( ) . keySet ( ) . iterator ( ) . next ( ) ; _log . warn ( StringBundler . concat ( "Bundle " , bundleName , " is already registered." ) ) ; continue ; } }
public void test() { if ( isBrokerAvailableForRebalancing ( bundleStat . getKey ( ) , comfortLoadLevel ) ) { namespaceBundlesToBeUnloaded . put ( overloadedRU , bundleName ) ; log . info ( "Queued {} to {}" , overloadedRU . getResourceId ( ) , bundleName ) ; } else { log . info ( "Unable to shed load from broker {}, no brokers with enough capacity available " + "for re-balancing {}" , overloadedRU . getResourceId ( ) , bundleName ) ; } }
public void test() { if ( isBrokerAvailableForRebalancing ( bundleStat . getKey ( ) , comfortLoadLevel ) ) { log . info ( "Namespace bundle {} will be unloaded from overloaded broker {}," + " bundle stats (topics: {}, producers {}, " + "consumers {}, bandwidthIn {}, bandwidthOut {})" , bundleName , overloadedRU . getResourceId ( ) , stats . topics , stats . producerCount , stats . consumerCount , stats . msgThroughputIn , stats . msgThroughputOut ) ; namespaceBundlesToBeUnloaded . put ( overloadedRU , bundleName ) ; } else { log . warn ( "Namespace bundle {} will be unloaded from overloaded broker " , bundleStat . getKey ( ) ) ; } }
public CreateProjectPage enterProjectId ( String projectId ) { log . info ( "Enter project id {}" , projectId ) ; enterText ( readyElement ( idField ) , projectId ) ; return new CreateProjectPage ( getDriver ( ) ) ; }
public void test() { try { Object value = BeanUtil . pojo . getProperty ( bean , param ) ; beanValue = _converter . toDoubleValue ( value , defaultValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; } }
public boolean isDirectory ( ) { logger . trace ( "isDirectory() -> {}" , isDirectory ) ; return isDirectory ; }
protected Response makeBearerError ( BearerTokenError error ) { String header = error . toWWWAuthenticateHeader ( ) ; log . debug ( error . getMessage ( ) ) ; return toResponse ( Response . status ( error . getHTTPStatusCode ( ) ) . header ( "WWW-Authenticate" , header ) ) ; }
public void test() { if ( "export" . equals ( mode ) ) { doExport ( parameters ) ; } else-if ( "import" . equals ( mode ) ) { doImport ( parameters ) ; } else { logger . debug ( "Unknown import mode: " + mode ) ; } }
public void test() { if ( systemUnit == null ) { logger . warn ( "systemUnit is not a unit: {}" , unit ) ; } else-if ( systemUnit . isCompatible ( unit ) ) { return dimension ; } }
public void test() { try { systemUnit = ( Unit < ? > ) field . get ( null ) ; code_block = IfStatement ; } catch ( IllegalArgumentException | IllegalAccessException e ) { logger . warn ( "Could not get system unit: {}" , e . getMessage ( ) , e ) ; } }
public void test() { if ( genericType instanceof ParameterizedType ) { String dimension = ( ( Class < ? > ) ( ( ParameterizedType ) genericType ) . getActualTypeArguments ( ) [ 0 ] ) . getSimpleName ( ) ; Unit < ? > systemUnit ; code_block = TryStatement ;  } else { LOGGER . warn ( "Not supported type {}." , genericType . getClass ( ) . getSimpleName ( ) ) ; } }
private void getServerProfileCompliancePreview ( ) { ServerProfile serverProfile = this . serverProfileClient . getByName ( SERVER_PROFILE_NAME ) . get ( 0 ) ; ServerProfileCompliancePreview compliance = serverProfileClient . getCompliancePreview ( serverProfile . getResourceId ( ) ) ; LOGGER . info ( "ServerProfileCompliancePreview returned to client : " + serverProfile . toJsonString ( ) ) ; }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { for ( Exchange exchange : list ) { Entry entry = exchange . getIn ( ) . getBody ( Entry . class ) ; assertNotNull ( entry , "No entry found for exchange: " + exchange ) ; String expectedTitle = expectedTitles [ counter ] ; String title = entry . getTitle ( ) ; assertEquals ( expectedTitle , title , "Title of message " + counter ) ; LOG . info ( "Received title: " + counter ) ; counter ++ ; } }
public void test() { try { ActiveTransactionsRecord myRecord = this . activeTxRecord ; code_block = IfStatement ; } catch ( SQLException sqlex ) { throw new CommitException ( ) ; } catch ( LookupException le ) { logger . error ( "Error while obtaining database connection" , le ) ; throw new Error ( "Error while obtaining database connection" , le ) ; } }
public void test() { try { pb . commitTransaction ( ) ; } catch ( Throwable t ) { log . error ( "Error committing transaction" , t ) ; t . printStackTrace ( ) ; System . exit ( - 1 ) ; } }
public void test() { try { String jsonString = query ( sparqlQueryString ) ; rs = SparqlQuery . convertJSONtoResultSet ( jsonString ) ; } catch ( Exception e ) { logger . error ( sparqlQueryString , e ) ; } }
public void test() { try { action . call ( ) ; } catch ( Exception ex ) { LOG . warn ( "Exception while invoking action." , ex ) ; } }
public void test() { try { final File testFile = new File ( datenDownload . arr [ DatenDownload . DOWNLOAD_ZIEL_PFAD_DATEINAME ] ) ; code_block = IfStatement ; } catch ( Exception ex ) { logger . error ( "Failed to load PF data file" , ex ) ; } }
public void test() { if ( ! preCommit ( ) ) { LOGGER . debug ( "Nothing to do" ) ; return false ; } }
public void attachClean ( StgMbCss instance ) { log . debug ( "attaching clean StgMbCss instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . lock ( instance , LockMode . NONE ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { if ( ! addrs . equals ( this . nameSrvAddr ) ) { logger . info ( "name server address list changed: {}" , addrs ) ; this . updateNameServerAddressList ( addrs ) ; this . nameSrvAddr = addrs ; return nameSrvAddr ; } }
public void test() { try { String addrs = this . topAddressing . fetchNSAddr ( ) ; code_block = IfStatement ; } catch ( Exception e ) { LOG . error ( I18n . err ( I18n . ERR_134 , e ) ) ; } }
public void test() { try { setupHelper . cleanUp ( ) ; } catch ( RuntimeException | IOException e ) { log . debug ( "Extension [{0}]" , getName ( ) , e ) ; } }
protected void createOCFS2Sr ( StorageFilerTO pool ) throws XmlRpcException { OvmStoragePool . Details d = new OvmStoragePool . Details ( ) ; d . path = pool . getPath ( ) ; d . type = OvmStoragePool . OCFS2 ; d . uuid = pool . getUuid ( ) ; logger . debug ( "OCFS2 creating " + d . toString ( ) ) ; OvmStoragePool . create ( _conn , d ) ; }
public void test() { try { shareStudy = dao . findByID ( shareStudyId ) ; } catch ( Exception exception ) { LOG . error ( "Cannot find ShareStudy by expID " + shareStudyId ) ; LOG . error ( exception . getMessage ( ) ) ; } }
public void test() { try { shareStudy = dao . findByID ( shareStudyId ) ; } catch ( Exception exception ) { LOG . error ( "Cannot find ShareStudy by shareStudyID " + shareStudyId ) ; LOG . error ( exception . getMessage ( ) ) ; } }
public void test() { try { bytesReceived = offset ; connectionFactory = initializeConnectionFactory ( brokerURL ) ; connection = connectionFactory . createConnection ( ) ; code_block = IfStatement ; connection . start ( ) ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; throw new AdaptorException ( e ) ; } }
public void test() { if ( flinkPod . getPodWithoutMainContainer ( ) . getSpec ( ) . getRestartPolicy ( ) != null ) { logger . info ( "Pod {} has restart policy" , podName ) ; } }
@ Test public void testPostSettingExpectsOK ( ) throws ParseException , IOException { String key = "example_key" ; String value = "example_value" ; JSONObject actual = ( JSONObject ) parser . parse ( given ( ) . formParam ( "key" , key ) . formParam ( "value" , value ) . expect ( ) . statusCode ( HttpStatus . SC_OK ) . contentType ( ContentType . JSON ) . body ( "key" , equalTo ( key ) ) . body ( "value" , equalTo ( value ) ) . when ( ) . post ( rt . host ( "setting" ) ) . asString ( ) ) ; Log . info ( actual . toString ( ) ) ; }
public void test() { try { boot . group ( connectGroup ) . channelFactory ( NioUdtProvider . BYTE_CONNECTOR ) . handler ( new ChannelInitializer < UdtChannel > ( ) code_block = "" ; ) ; channel = boot . connect ( address ) . sync ( ) . channel ( ) ; isRunning = true ; log . info ( "Client connected..." ) ; waitForRunning ( false ) ; log . info ( "Client closing..." ) ; channel . close ( ) . sync ( ) ; isShutdown = true ; log . info ( "Client is done." ) ; } catch ( final Throwable e ) { log . error ( "Client failed." , e ) ; } finally { connectGroup . shutdownGracefully ( ) . syncUninterruptibly ( ) ; } }
public void test() { try { boot . group ( connectGroup ) . channelFactory ( NioUdtProvider . BYTE_CONNECTOR ) . handler ( new ChannelInitializer < UdtChannel > ( ) code_block = "" ; ) ; channel = boot . connect ( address ) . sync ( ) . channel ( ) ; isRunning = true ; log . info ( "Client ready." ) ; waitForRunning ( false ) ; log . info ( "Client closing..." ) ; channel . close ( ) . sync ( ) ; isShutdown = true ; log . info ( "Server connected." ) ; } catch ( final Throwable e ) { log . error ( "Client failed." , e ) ; } finally { connectGroup . shutdownGracefully ( ) . syncUninterruptibly ( ) ; } }
public void test() { try { boot . group ( connectGroup ) . channelFactory ( NioUdtProvider . BYTE_CONNECTOR ) . handler ( new ChannelInitializer < UdtChannel > ( ) code_block = "" ; ) ; channel = boot . connect ( address ) . sync ( ) . channel ( ) ; isRunning = true ; log . info ( "Client ready." ) ; waitForRunning ( false ) ; log . info ( "Client closing..." ) ; channel . close ( ) . sync ( ) ; isShutdown = true ; log . info ( "Client is done." ) ; } catch ( final Throwable e ) { log . error ( "Error connecting to host: " + address , e ) ; } finally { connectGroup . shutdownGracefully ( ) . syncUninterruptibly ( ) ; } }
public void test() { try { kylinConfig . getCliCommandExecutor ( ) . execute ( yarnCmd ) ; } catch ( Exception ex ) { logger . error ( "Failed to run yarn command." , ex ) ; } }
public void test() { if ( shouldDoLogCollection ( applicationId , kylinConfig ) ) { File destFile = new File ( destDir , applicationId + ".log" ) ; String yarnCmd = "yarn logs -applicationId " + applicationId + " > " + destFile . getAbsolutePath ( ) ; logger . info ( yarnCmd ) ; code_block = TryStatement ;  } else { logger . info ( "Nothing to do" ) ; } }
@ Test public void test19AnonUpdate ( ) { LOGGER . info ( "  test19AnonUpdate" ) ; Thing thing = THINGS . get ( 0 ) . withOnlyId ( ) ; thing . setDescription ( "Anon Updated Thing made by Admin." ) ; code_block = TryStatement ;  EntityUtils . filterAndCheck ( serviceRead . things ( ) , "" , THINGS ) ; }
public void test() { try { serviceAnon . update ( thing ) ; Assert . fail ( ANON_SHOULD_NOT_BE_ABLE_TO_UPDATE ) ; } catch ( NotAuthorizedException ex ) { LOGGER . trace ( "Another not authorized" ) ; } catch ( ServiceFailureException ex ) { Assert . fail ( "Expected NotAuthorizedException, got " + ex ) ; } }
public void test() { try { delete ( uuid ) ; data . setUUID ( uuid ) ; add ( data ) ; return data ; } catch ( Exception e ) { LOG . error ( Freedomotic . getStackTraceInfo ( e ) ) ; return null ; } }
void updateCategory ( String categoryUrl , String parentUrl , String name ) throws IOException { Put request = new Put ( categoryUrl , credentials ) ; request . setAccept ( APPLICATION_JSON ) ; request . addString ( "parent" , parentUrl ) ; request . addString ( "name" , name ) ; String result = request . executeAsString ( ) ; if ( request . isUnAuthorized ( ) ) throw new UnAuthorizedException ( "Not authorized to update category " + name , categoryUrl ) ; if ( request . isForbidden ( ) ) throw new ForbiddenException ( "Forbidden to update category " + name , categoryUrl ) ; if ( request . isNotFound ( ) ) throw new NotFoundException ( "Category not found" , categoryUrl ) ; if ( request . isBadRequest ( ) ) throw new NotOwnerException ( "Not owner of category to update" , categoryUrl ) ; if ( request . isPreconditionFailed ( ) ) throw new DuplicateNameException ( "Category " + name + " already exists" , categoryUrl ) ; if ( ! request . isSuccessful ( ) ) throw new IOException ( "PUT on " + categoryUrl + " with payload " + parentUrl + "/" + name + " not successful: " + result ) ; logger . info ( "PUT: " + categoryUrl + " with payload " + parentUrl + "/" + name + " not successful: " + result ) ; }
public void test() { if ( logger . isTraceEnabled ( LogMarker . PERSIST_RECOVERY_VERBOSE ) ) { logger . trace ( LogMarker . PERSIST_RECOVERY_VERBOSE , "bad disk region id!" ) ; } else { throw new IllegalStateException ( "bad disk region id" ) ; } }
@ Override public boolean insert ( PostgresPersistenceManager < J > pm , FeatureOfInterest foi ) throws IncompleteEntityException { Map < Field , Object > insert = new HashMap < > ( ) ; insert . put ( table . colName , foi . getName ( ) ) ; insert . put ( table . colDescription , foi . getDescription ( ) ) ; insert . put ( table . colProperties , new JsonValue ( foi . getProperties ( ) ) ) ; String encodingType = foi . getEncodingType ( ) ; insert . put ( table . colEncodingType , encodingType ) ; EntityFactories . insertGeometry ( insert , table . colFeature , table . colGeom , encodingType , foi . getFeature ( ) ) ; entityFactories . insertUserDefinedId ( pm , insert , table . getId ( ) , foi ) ; DSLContext dslContext = pm . getDslContext ( ) ; Record1 < J > result = dslContext . insertInto ( table ) . set ( insert ) . returningResult ( table . getId ( ) ) . fetchOne ( ) ; J generatedId = result . component1 ( ) ; log . debug ( "inserted id: {}" , generatedId . getId ( ) ) ; foi . setId ( entityFactories . idFromObject ( generatedId ) ) ; return true ; }
@ Test ( timeout = 60 * 1000 ) public void automaticSplitWith250Same ( ) throws Exception { log . info ( "Automatic split with250 single table" ) ; final String tableName = getUniqueNames ( 1 ) [ 0 ] ; code_block = TryStatement ;  }
public void test() { if ( isTrace ) { StringBuilder txt = new StringBuilder ( ) ; txt . append ( "Window ID: " ) . append ( windowId ) . append ( ", Removed values from old map:  " ) . append ( removed ) . append ( ", set " ) . append ( ctr ) . append ( " values." ) ; LOGGER . debug ( txt . toString ( ) ) ; } }
@ Override protected void decode ( ChannelHandlerContext ctx , DatagramPacket msg , List < Object > out ) { MessageConsumer consumer = UdpConnectionMap . getMessageConsumer ( msg . sender ( ) ) ; code_block = IfStatement ; ByteBuf bb = msg . content ( ) ; int readableBytes = bb . readableBytes ( ) ; code_block = IfStatement ; int length = bb . getUnsignedShort ( bb . readerIndex ( ) + LENGTH_INDEX_IN_HEADER ) ; LOG . debug ( "length of actual message: {}" , length ) ; code_block = IfStatement ; LOG . debug ( "OF Protocol message received, type:{}" , bb . getByte ( bb . readerIndex ( ) + 1 ) ) ; byte version = bb . readByte ( ) ; LOG . debug ( "OF Protocol message version: {}" , version ) ; code_block = IfStatement ; bb . skipBytes ( bb . readableBytes ( ) ) ; }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "bytebuffer: {}" , ByteBufUtils . byteBufToHexString ( bb ) ) ; LOG . debug ( "bytebuffer: {}" , ByteBufUtils . byteBufToHexString ( bb ) ) ; } }
@ Override protected void decode ( ChannelHandlerContext ctx , DatagramPacket msg , List < Object > out ) { LOG . debug ( "OFDatagramPacketFramer" ) ; MessageConsumer consumer = UdpConnectionMap . getMessageConsumer ( msg . sender ( ) ) ; code_block = IfStatement ; ByteBuf bb = msg . content ( ) ; int readableBytes = bb . readableBytes ( ) ; code_block = IfStatement ; int length = bb . getUnsignedShort ( bb . readerIndex ( ) + LENGTH_INDEX_IN_HEADER ) ; LOG . debug ( "length of actual message: {}" , length ) ; code_block = IfStatement ; byte version = bb . readByte ( ) ; LOG . debug ( "Version: {}" , version ) ; code_block = IfStatement ; bb . skipBytes ( bb . readableBytes ( ) ) ; }
public void test() { if ( version == EncodeConstants . OF13_VERSION_ID || version == EncodeConstants . OF10_VERSION_ID ) { LOG . debug ( "detected version: {}" , version ) ; ByteBuf messageBuffer = bb . slice ( ) ; out . add ( new VersionMessageUdpWrapper ( version , messageBuffer , msg . sender ( ) ) ) ; messageBuffer . retain ( ) ; } else { LOG . warn ( "unsupported version: {}" , version ) ; } }
public void test() { if ( ! func . getActivationPolicy ( ) . getStatus ( ) ) { logger . debug ( func . getIdentifier ( ) + " is not activated" ) ; return null ; } }
@ Override public Date getDateValue ( TimeUnitValueFunctionality func , Date currentDate , BusinessErrorCode errorCode ) { code_block = IfStatement ; logger . debug ( func . getIdentifier ( ) + " is activated" ) ; Calendar calendar = getCalendarWithoutTime ( timeService . dateNow ( ) ) ; calendar . add ( func . toCalendarValue ( ) , func . getValue ( ) ) ; Date defaultDate = calendar . getTime ( ) ; code_block = IfStatement ; logger . debug ( func . getIdentifier ( ) + " has a delegation policy" ) ; Date now = getCalendarWithoutTime ( timeService . dateNow ( ) ) . getTime ( ) ; code_block = IfStatement ; Calendar c = new GregorianCalendar ( ) ; c . setTime ( now ) ; c . add ( func . toCalendarMaxValue ( ) , func . getMaxValue ( ) ) ; Date maxDate = getCalendarWithoutTime ( c . getTime ( ) ) . getTime ( ) ; code_block = IfStatement ; return currentDate ; }
public void test() { if ( currentDate . before ( now ) || currentDate . after ( maxDate ) ) { String errorMessage = buildErrorMessage ( func , dateFormat . format ( currentDate ) , dateFormat . format ( now ) , dateFormat . format ( maxDate ) ) ; logger . warn ( errorMessage ) ; throw new BusinessException ( errorCode , errorMessage ) ; } }
public void test() { try { Release release = client . getReleaseById ( id , user ) ; putDirectlyLinkedReleaseRelationsInRequest ( request , release ) ; } catch ( TException e ) { log . error ( "Error getting projects" , e ) ; throw new PortletException ( "cannot get projects" , e ) ; } }
public GetPQValuesResponse dequeueGetPQValuesResponse ( final String correlationUid ) throws OsgpException { LOGGER . debug ( "dequeueGetPQValuesResponse called with correlation uid {}" , correlationUid ) ; return ( GetPQValuesResponse ) this . processResponse ( correlationUid ) ; }
public void test() { try { EventSearchQuery query = new EventSearchQuery ( securityService . getOrganization ( ) . getId ( ) , securityService . getUser ( ) ) ; query . withSeriesId ( seriesId ) ; SearchResult < Event > result = searchIndex . getByQuery ( query ) ; elementsCount = result . getHitCount ( ) ; } catch ( SearchIndexException e ) { logger . error ( "Unable to retrieve events from search: {}" , e . getMessage ( ) , e ) ; throw new WebApplicationException ( Status . INTERNAL_SERVER_ERROR ) ; } }
public void test() { if ( LOGGER . isInfoEnabled ( ) ) { String generatorMessages = generators . stream ( ) . map ( Generator :: name ) . collect ( Collectors . joining ( ", " ) ) ; LOGGER . info ( generatorMessages ) ; } }
public void test() { if ( version != null ) { LOGGER . info ( "Version " + version ) ; } }
public void test() { if ( message . get ( STATUS ) . textValue ( ) . equals ( BitfinexAuthRequestStatus . FAILED . name ( ) ) ) { logger . warn ( "Authentication failed with status " + message . get ( STATUS ) . textValue ( ) ) ; } }
public void test() { if ( message . get ( STATUS ) . textValue ( ) . equals ( BitfinexAuthRequestStatus . OK . name ( ) ) ) { logger . info ( "Auth request status code: " + message . get ( STATUS ) . textValue ( ) ) ; } }
public void test() { try { String subscriptionUniqueId = getSubscriptionUniqueId ( channel , pair ) ; subscribedChannels . put ( channelId , subscriptionUniqueId ) ; } catch ( Exception e ) { LOG . error ( "Unable to get subscription unique id for channel: " + channelId ) ; LOG . error ( e . getMessage ( ) ) ; } }
public void test() { try { String subscriptionUniqueId = getSubscriptionUniqueId ( channel , pair ) ; subscribedChannels . put ( channelId , subscriptionUniqueId ) ; LOG . debug ( "Register channel {}: {}" , subscriptionUniqueId , channelId ) ; } catch ( Exception e ) { LOG . error ( "Subscribe channel failed: {}" , channel , e ) ; } }
public void test() { if ( message . get ( "code" ) . asInt ( ) == SUBSCRIPTION_FAILED ) { LOG . info ( "subscription failed" ) ; return ; } }
public void test() { if ( message . get ( "code" ) . asInt ( ) == SUBSCRIPTION_DUP ) { logger . info ( "A subscription was already subscribed" ) ; return ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { Boolean success = future . get ( retryPeriodMillis , TimeUnit . MILLISECONDS ) ; code_block = IfStatement ; acquired . set ( success ) ; } catch ( CancellationException e ) { } catch ( Throwable t ) { this . exceptionHandler . accept ( t ) ; future . cancel ( true ) ; log . debug ( String . format ( "Batch task has been cancelled [%s]" , this ) ) ; } finally { maybeReportTransition ( ) ; } }
public void test() { try { code_block = WhileStatement ; } catch ( InterruptedException e ) { logger . debug ( "Interrupted" , e ) ; return false ; } finally { scheduledFuture . cancel ( true ) ; } }
public void test() { if ( titles . size ( ) > 1 ) { logger . warn ( "Multiple titles found: " + titles . size ( ) ) ; } }
public void test() { try { writer = new FileWriter ( file ) ; CSVSaveService . saveCSVStats ( getAllDataAsTable ( model , FORMATS , getLabels ( COLUMNS ) ) , writer , saveHeaders . isSelected ( ) ) ; } catch ( IOException e ) { log . error ( "IOException trying to save CSV file" , e ) ; } finally { code_block = TryStatement ;  } }
public void test() { try { code_block = IfStatement ; } catch ( IOException ex ) { LOG . error ( ex . getMessage ( ) , ex ) ; } }
public void test() { try { type = DataUtilities . createType ( "testCityData" , CITY_ATTRIBUTE + ":String," + STATE_ATTRIBUTE + ":String," + POPULATION_ATTRIBUTE + ":Double," + LAND_AREA_ATTRIBUTE + ":Double," + GEOMETRY_ATTRIBUTE + ":Geometry" ) ; } catch ( final SchemaException e ) { LOGGER . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( configurableComponent == null ) { LOG . warn ( "Component [" + name + "] is null, ignoring." ) ; } }
@ Test public void shouldReturnNotActiveOnUnknownToken ( ) throws Exception { TokensManagement tokensManagement = new MockTokensMan ( ) ; TokenIntrospectionResource tested = createIntrospectionResource ( tokensManagement ) ; setupInvocationContext ( 111 ) ; Response r = tested . introspectToken ( "UNKNOWN-TOKEN" ) ; LOG . info ( r . getStatus ( ) ) ; assertEquals ( HTTPResponse . SC_OK , r . getStatus ( ) ) ; JSONObject parsed = ( JSONObject ) JSONValue . parse ( ( r . getEntity ( ) . toString ( ) ) ) ; assertThat ( parsed . getAsString ( "active" ) ) . isEqualTo ( "false" ) ; assertThat ( parsed . size ( ) ) . isEqualTo ( 1 ) ; }
public void test() { try { initialContext = new InitialContext ( ) ; customLogLocation = ( String ) initialContext . lookup ( this . logConfig ) ; code_block = IfStatement ; } catch ( final NamingException | FileNotFoundException | JoranException e ) { log . error ( e . getMessage ( ) , e ) ; throw new ServletException ( e ) ; } }
public void test() { if ( line . contains ( "FAILING CHECKS" ) ) { readingFailedChecks = true ; readingMonitorResults = false ; } else-if ( line . contains ( "MONITOR RESULTS" ) ) { readingFailedChecks = false ; readingMonitorResults = true ; } else-if ( readingFailedChecks && ! readingMonitorResults ) { failingChecks . addAll ( getFailingChecks ( line ) ) ; } else-if ( ! readingFailedChecks && readingMonitorResults ) { monitorResults . append ( line ) ; } else { logger . info ( "FAILING CHECKS\n" + line ) ; } }
public void test() { if ( ! killMetaRs && targetServer . equals ( metaServer ) ) { LOG . warn ( "killMetaRs check failed, but the meta server is not {}" , targetServer ) ; } else { killRs ( targetServer ) ; killedServers . add ( targetServer ) ; } }
public void test() { try { q = this . qm . createQuery ( "select doc.name from Document doc, doc.object(PhenoTips.LabeledIdentifierClass) obj " + "where obj.label = :label and obj.value = :value" , Query . XWQL ) ; q . bindValue ( KEY_LABEL , label ) ; q . bindValue ( KEY_VALUE , id ) ; return q . execute ( ) ; } catch ( QueryException ex ) { logger . warn ( ex . getMessage ( ) , q , ex ) ; throw new QueryException ( ex . getMessage ( ) , q , ex ) ; } }
public void test() { if ( ! executorService . awaitTermination ( secondsToWaitOnShutdown , TimeUnit . SECONDS ) ) { logger . warn ( "ExecutorService did not terminate" ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( InterruptedException e ) { Log . warn ( e . toString ( ) , e ) ; } }
public void test() { if ( ! started ) { log . info ( "JGroups version: " + org . jgroups . Version . printDescription ( ) ) ; log . info ( "Host " + hostName + ":" + hostPort ) ; code_block = TryStatement ;  localAddr = ch . getAddress ( ) ; started = true ; } }
public void test() { try ( PubsubClient pubsubClient = pubsubFactory . newClient ( timestampAttribute , idAttribute , options . as ( PubsubOptions . class ) ) ) { SubscriptionPath subscriptionPath = pubsubClient . createRandomSubscription ( projectPath , topicPath , DEAULT_ACK_TIMEOUT_SEC ) ; logger . info ( "created random subscription" ) ; return subscriptionPath ; } }
private void scheduleConnectionMonitorJob ( ) { logger . debug ( "Scheduling connection monitor job in {} seconds" , CONNECTION_MONITOR_START_DELAY ) ; connectionMonitorJob = scheduler . scheduleWithFixedDelay ( connectionMonitorRunnable , CONNECTION_MONITOR_START_DELAY , CONNECTION_MONITOR_FREQUENCY , TimeUnit . SECONDS ) ; }
public void test() { if ( backfillAttempts > 10 ) { LOG . error ( "Backfill halted: {}" , backfillAttempts ) ; return ; } }
public void test() { try { this . getPluginCounts ( namespace , apps ) ; } catch ( IOException e ) { updateFailed = true ; log . error ( "Error getting plugin counts for namespace " + namespace , e ) ; } }
@ Test public void testSubmitForSettlementWithId ( ) throws Exception { assertNotNull ( this . gateway , "BraintreeGateway can't be null" ) ; final Result < Transaction > createResult = requestBody ( "direct://SALE" , new TransactionRequest ( ) . amount ( new BigDecimal ( "100.03" ) ) . paymentMethodNonce ( "fake-valid-nonce" ) . options ( ) . submitForSettlement ( false ) . done ( ) , Result . class ) ; LOG . debug ( "Result message: {}" , createResult . getMessage ( ) ) ; assertNotNull ( createResult , "sale result" ) ; assertTrue ( createResult . isSuccess ( ) ) ; LOG . info ( "Transaction done - id={}" , createResult . getTarget ( ) . getId ( ) ) ; this . transactionIds . add ( createResult . getTarget ( ) . getId ( ) ) ; final Result < Transaction > result = requestBody ( "direct://SUBMITFORSETTLEMENT_WITH_ID" , createResult . getTarget ( ) . getId ( ) , Result . class ) ; assertNotNull ( result , "Submit For Settlement result" ) ; LOG . debug ( "Transaction submitted for settlement - id={}" , result . getTarget ( ) . getId ( ) ) ; }
@ Test public void testSubmitForSettlementWithId ( ) throws Exception { assertNotNull ( this . gateway , "BraintreeGateway can't be null" ) ; final Result < Transaction > createResult = requestBody ( "direct://SALE" , new TransactionRequest ( ) . amount ( new BigDecimal ( "100.03" ) ) . paymentMethodNonce ( "fake-valid-nonce" ) . options ( ) . submitForSettlement ( false ) . done ( ) , Result . class ) ; LOG . info ( "Result message: {}" , createResult . getMessage ( ) ) ; assertNotNull ( createResult , "sale result" ) ; assertTrue ( createResult . isSuccess ( ) ) ; LOG . info ( "Transaction created for id={}" , createResult . getTarget ( ) . getId ( ) ) ; this . transactionIds . add ( createResult . getTarget ( ) . getId ( ) ) ; final Result < Transaction > result = requestBody ( "direct://SUBMITFORSETTLEMENT_WITH_ID" , createResult . getTarget ( ) . getId ( ) , Result . class ) ; assertNotNull ( result , "Submit For Settlement result" ) ; LOG . debug ( "Transaction submitted for settlement - id={}" , result . getTarget ( ) . getId ( ) ) ; }
@ Test public void testSubmitForSettlementWithId ( ) throws Exception { assertNotNull ( this . gateway , "BraintreeGateway can't be null" ) ; final Result < Transaction > createResult = requestBody ( "direct://SALE" , new TransactionRequest ( ) . amount ( new BigDecimal ( "100.03" ) ) . paymentMethodNonce ( "fake-valid-nonce" ) . options ( ) . submitForSettlement ( false ) . done ( ) , Result . class ) ; LOG . info ( "Result message: {}" , createResult . getMessage ( ) ) ; assertNotNull ( createResult , "sale result" ) ; assertTrue ( createResult . isSuccess ( ) ) ; LOG . info ( "Transaction done - id={}" , createResult . getTarget ( ) . getId ( ) ) ; this . transactionIds . add ( createResult . getTarget ( ) . getId ( ) ) ; final Result < Transaction > result = requestBody ( "direct://SUBMITFORSETTLEMENT_WITH_ID" , createResult . getTarget ( ) . getId ( ) , Result . class ) ; assertNotNull ( result , "Submit For Settlement result" ) ; LOG . info ( "Transaction done - id={}" , result . getId ( ) ) ; }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { switch ( device . getStatus ( ) ) { case JNA . CLibrary . TELLSTICK_TURNON : st = OnOffType . ON ; break ; case JNA . CLibrary . TELLSTICK_TURNOFF : st = OnOffType . OFF ; break ; case JNA . CLibrary . TELLSTICK_DIM : BigDecimal dimValue = new BigDecimal ( device . getData ( ) ) ; code_block = IfStatement ; break ; default : logger . debug ( "Unknown device type {}" , device . getStatus ( ) ) ; } }
public void test() { for ( ProcessDefinition pd : processDefinitionPage . getContent ( ) ) { LOG . debug ( pd . toString ( ) ) ; } }
public void test() { try { Method fhLimitMethod = sunBeanClass . getMethod ( "getMaxFileDescriptorCount" ) ; Object result = fhLimitMethod . invoke ( ManagementFactory . getOperatingSystemMXBean ( ) ) ; return ( Long ) result ; } catch ( Throwable t ) { logger . warn ( "Could not get max file descriptor for getting max file descriptor count" ) ; return - 1L ; } }
public void test() { if ( s_logger . isInfoEnabled ( ) ) { s_logger . info ( "Unable to recover from " + toString ( ) ) ; } }
public void test() { if ( s_logger . isInfoEnabled ( ) ) { s_logger . info ( "Unable to recover from " + toString ( ) ) ; } }
public void test() { if ( s_logger . isInfoEnabled ( ) ) { s_logger . info ( "Unable to recover from " + toString ( ) ) ; } }
public void test() { if ( e instanceof RemoteException ) { s_logger . warn ( "Encounter remote exception to vCenter, invalidate VMware session context" ) ; invalidateServiceContext ( null ) ; } }
public void test() { try { VmwareContext context = getServiceContext ( null ) ; VmwareHypervisorHost hyperHost = getHyperHost ( context , null ) ; VolumeTO vol = cmd . getVolume ( ) ; VirtualMachineMO vmMo = findVmOnDatacenter ( context , hyperHost , vol ) ; code_block = IfStatement ; return new Answer ( cmd , true , "Success" ) ; } catch ( Throwable e ) { code_block = IfStatement ; String msg = "DestroyCommand failed due to " + VmwareHelper . getExceptionMessage ( e ) ; s_logger . error ( msg , e ) ; return new Answer ( cmd , false , msg ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { String mpId = mp . getIdentifier ( ) . toString ( ) ; Snapshot snapshot = getSnapshot ( mpId ) ; MediaPackage archivedMp = snapshot . getMediaPackage ( ) ; removeLivePublicationChannel ( archivedMp ) ; snapshotVersionCache . put ( mpId , assetManager . takeSnapshot ( archivedMp ) . getVersion ( ) ) ; } catch ( LiveScheduleException e ) { logger . warn ( "Failed to retrieve snapshot of mediapackage {}" , mpId , e ) ; } }
public void test() { try { StoragePath metadataPath = DefaultStoragePath . parse ( repIterator . next ( ) . getStoragePath ( ) , RodaConstants . STORAGE_DIRECTORY_METADATA , RodaConstants . STORAGE_DIRECTORY_PRESERVATION ) ; code_block = IfStatement ; } catch ( RODAException e ) { LOGGER . error ( "Could not list resources under storage directory {}" , storagePath , e ) ; } }
public void test() { try { StoragePath repPath = DefaultStoragePath . parse ( aipIteratorSub . next ( ) . getStoragePath ( ) , RodaConstants . STORAGE_DIRECTORY_REPRESENTATIONS ) ; code_block = IfStatement ; } catch ( RODAException e ) { LOGGER . error ( "Could not list resources under AIP data directory" , e ) ; } }
public void test() { if ( context . isRestored ( ) ) { code_block = IfStatement ; } else { log . debug ( "Context [{}] was not restored for [{}]" , name , partition ) ; } }
public void test() { if ( getGeometryHandler ( ) . isSpatialDatasource ( ) ) { c . add ( SpatialRestrictions . filter ( DataEntity . PROPERTY_GEOMETRY_ENTITY , ( ( GetObservationRequest ) request ) . getSpatialFilter ( ) . getOperator ( ) , getGeometryHandler ( ) . switchCoordinateAxisFromToDatasourceIfNeeded ( ( ( GetObservationRequest ) request ) . getSpatialFilter ( ) . getGeometry ( ) ) ) ) ; logArgs . append ( ", spatialFilter" ) ; } else { LOGGER . info ( "WFS Source does not support spatial filters" ) ; } }
@ Override public void onContainerStarted ( ContainerId containerId , Map < String , ByteBuffer > allServiceResponse ) { LOG . info ( "Container {} started" , containerId ) ; controller . containerStarted ( containerId ) ; }
@ Override public void execute ( Runnable command ) { LOG . debug ( "Executing: {}" , command ) ; this . runnables . add ( command ) ; }
public void test() { if ( ! success ) { code_block = TryStatement ;  } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( xaDBPassword != null && ! xaDBPassword . trim ( ) . isEmpty ( ) && ! xaDBPassword . trim ( ) . equalsIgnoreCase ( "none" ) ) { conf . set ( key , xaDBPassword ) ; } else { log . error ( "XA configuration property not set." ) ; } }
@ Override public void query ( String deviceId , String startTs , String endTs , String notification , String sortField , String sortOrderSt , Integer take , Integer skip , @ Suspended final AsyncResponse asyncResponse ) { final Date timestampSt = TimestampQueryParamParser . parse ( startTs ) ; final Date timestampEnd = TimestampQueryParamParser . parse ( endTs ) ; logger . debug ( "Device query request received for device {}: {}" , deviceId , timestampEnd ) ; DeviceVO byIdWithPermissionsCheck = deviceService . findById ( deviceId ) ; code_block = IfStatement ; }
public void test() { if ( ! hooked . isEmpty ( ) ) { final StringBuilder string = new StringBuilder ( ) ; code_block = ForStatement ; final String plugins = string . substring ( 0 , string . length ( ) - 2 ) ; getLogger ( ) . debug ( "Found plugins: " + plugins ) ; } }
public void test() { try { retryTimer . newTimeout ( timerTask , 1000 , TimeUnit . MILLISECONDS ) ; } catch ( RejectedExecutionException e ) { log . warn ( "Execution was rejected!" , e ) ; } }
public void attachClean ( StgG20SclMapping instance ) { log . debug ( "attaching clean StgG20SclMapping instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . lock ( instance , LockMode . NONE ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void passivateObject ( LdapConnection connection ) { logger . debug ( "PassivateObject: {}" , connection ) ; }
public void test() { if ( body != null ) { result = policyChecker . checkIncomingPolicy ( body , assertion ) ; } else { LOG . error ( "IncomingPolicy did not contain any in the incoming policy" ) ; } }
public void test() { try ( InputStream input = CamelKafkaConnectorCatalog . class . getResourceAsStream ( "/" + DESCRIPTORS_DIR + "/" + CONNECTORS_PROPERTIES ) ) { BufferedReader reader = new BufferedReader ( new InputStreamReader ( input ) ) ; code_block = WhileStatement ; } catch ( FileNotFoundException e ) { LOG . error ( "Cannot find HDFS path: {}" , e . getMessage ( ) , e ) ; } catch ( IOException e ) { LOG . error ( "IO Exception: {}" , e . getMessage ( ) , e ) ; } }
public void test() { try ( InputStream input = CamelKafkaConnectorCatalog . class . getResourceAsStream ( "/" + DESCRIPTORS_DIR + "/" + CONNECTORS_PROPERTIES ) ) { BufferedReader reader = new BufferedReader ( new InputStreamReader ( input ) ) ; code_block = WhileStatement ; } catch ( FileNotFoundException e ) { LOG . error ( "Cannot find file: {}" , e . getMessage ( ) , e ) ; } catch ( IOException e ) { LOG . error ( "Cannot find file: {}" , e . getMessage ( ) , e ) ; } }
public void testCreateTemporaryQueueThenCreateAQueueFromItsName ( ) throws Exception { Session session = connection . createSession ( false , Session . AUTO_ACKNOWLEDGE ) ; Queue tempQueue = session . createTemporaryQueue ( ) ; String name = tempQueue . getQueueName ( ) ; Queue createdQueue = session . createQueue ( name ) ; logger . info ( "created queue with name: " + name ) ; assertEquals ( "created queue not equal to temporary queue" , tempQueue , createdQueue ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { executor . execute ( this ) ; code_block = IfStatement ; } catch ( RejectedExecutionException e ) { log . warn ( "Execution was rejected!" , e ) ; return false ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
private static void move ( InterpretationPipelineOptions options , String from , String to ) { log . info ( "Moving files from {}" , from ) ; String targetPath = options . getTargetPath ( ) ; String deletePath = PathBuilder . buildPath ( targetPath , to , options . getDatasetId ( ) + "_*" ) . toString ( ) ; FsUtils . deleteByPattern ( options . getHdfsSiteConfig ( ) , options . getCoreSiteConfig ( ) , targetPath , deletePath ) ; String filter = PathBuilder . buildFilePathViewUsingInputPath ( options , from , "*.avro" ) ; String movePath = PathBuilder . buildPath ( targetPath , to ) . toString ( ) ; log . info ( "Moving files with pattern {} to {}" , filter , movePath ) ; FsUtils . moveDirectory ( options . getHdfsSiteConfig ( ) , options . getCoreSiteConfig ( ) , movePath , filter ) ; log . info ( "Files moved to {} directory" , movePath ) ; }
private static void move ( InterpretationPipelineOptions options , String from , String to ) { String targetPath = options . getTargetPath ( ) ; String deletePath = PathBuilder . buildPath ( targetPath , to , options . getDatasetId ( ) + "_*" ) . toString ( ) ; log . info ( "Deleting avro files {}" , deletePath ) ; FsUtils . deleteByPattern ( options . getHdfsSiteConfig ( ) , options . getCoreSiteConfig ( ) , targetPath , deletePath ) ; String filter = PathBuilder . buildFilePathViewUsingInputPath ( options , from , "*.avro" ) ; String movePath = PathBuilder . buildPath ( targetPath , to ) . toString ( ) ; log . info ( "Moving files to {}" , movePath ) ; FsUtils . moveDirectory ( options . getHdfsSiteConfig ( ) , options . getCoreSiteConfig ( ) , movePath , filter ) ; log . info ( "Files moved to {} directory" , movePath ) ; }
private static void move ( InterpretationPipelineOptions options , String from , String to ) { log . info ( "Moving files with from: {} to {}" , from , to ) ; String targetPath = options . getTargetPath ( ) ; String deletePath = PathBuilder . buildPath ( targetPath , to , options . getDatasetId ( ) + "_*" ) . toString ( ) ; log . info ( "Deleting avro files {}" , deletePath ) ; FsUtils . deleteByPattern ( options . getHdfsSiteConfig ( ) , options . getCoreSiteConfig ( ) , targetPath , deletePath ) ; String filter = PathBuilder . buildFilePathViewUsingInputPath ( options , from , "*.avro" ) ; String movePath = PathBuilder . buildPath ( targetPath , to ) . toString ( ) ; log . info ( "Moving files with pattern {} to {}" , filter , movePath ) ; FsUtils . moveDirectory ( options . getHdfsSiteConfig ( ) , options . getCoreSiteConfig ( ) , movePath , filter ) ; }
public void test() { try { JAXBContext context = JAXBContext . newInstance ( ActivityStreamInfo . class ) ; Unmarshaller unmarshaller = context . createUnmarshaller ( ) ; ByteArrayInputStream is = new ByteArrayInputStream ( xml . getBytes ( StandardCharsets . UTF_8 ) ) ; bodyObject = ( ActivityStreamInfo ) unmarshaller . unmarshal ( is ) ; } catch ( Throwable t ) { _logger . error ( "Error unmarshalling activity stream info config" , t ) ; throw new ApsSystemException ( "Error unmarshalling activity stream info config" , t ) ; } }
public void init ( ) { LOG . info ( "==> RangerChainedPlugin.init(" + serviceType + ", " + serviceName + ")" ) ; this . plugin . init ( ) ; LOG . info ( "<== RangerChainedPlugin.init(" + serviceType + ", " + serviceName + ")" ) ; }
public void init ( ) { LOG . info ( "==> RangerChainedPlugin.init(" + serviceType + ", " + serviceName + ")" ) ; this . plugin . init ( ) ; LOG . info ( "<== RangerChainedPlugin.init(" + serviceType + ", " + serviceName + "): " + this . plugin . getClass ( ) . getSimpleName ( ) ) ; }
public void test() { try { queue . push ( message ) ; } catch ( MessageQueueOverflowException e ) { log . error ( "Error pushing message [" + message + "]" , e ) ; } }
private MessageEvent unexpectedError ( Exception exception ) { LOG . error ( "Unable to execute query : " + exception . toString ( ) ) ; MessageEvent msg = new MessageEvent ( MessageEventEnum . DATA_OPERATION_ERROR_UNEXPECTED ) ; msg . setDescription ( msg . getDescription ( ) . replace ( "%DESCRIPTION%" , exception . toString ( ) ) ) ; return msg ; }
@ Override public void executeImpl ( DelegateExecution execution ) throws Exception { String contentTypeString = activitiHelper . getRequiredExpressionVariableAsString ( contentType , execution , "ContentType" ) . trim ( ) ; String requestString = activitiHelper . getRequiredExpressionVariableAsString ( businessObjectDataStorageFilesCreateRequest , execution , "BusinessObjectDataCreateRequest" ) . trim ( ) ; BusinessObjectDataStorageFilesCreateRequest request = getRequestObject ( contentTypeString , requestString , BusinessObjectDataStorageFilesCreateRequest . class ) ; BusinessObjectDataStorageFilesCreateResponse businessObjectDataStorageFilesCreateResponse = businessObjectDataStorageFileService . createBusinessObjectDataStorageFiles ( request ) ; setJsonResponseAsWorkflowVariable ( businessObjectDataStorageFilesCreateResponse , execution ) ; LOGGER . info ( "BusinessObjectDataStorageFiles: createBusinessObjectDataStorageFiles" ) ; }
@ Override public void clearCache ( ) { LOG . debug ( "Clearing ACL cache" ) ; aclClassCache . invalidateAll ( ) ; }
public List findByExample ( StgMUmsetzStatTxt instance ) { log . debug ( "finding StgMUmsetzStatTxt instance by example" ) ; code_block = TryStatement ;  }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.StgMUmsetzStatTxt" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.StgMUmsetzStatTxt" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( fs . exists ( path ) == false ) { LOG . warn ( "File {} doesn't exist" , path ) ; return ; } }
public void test() { if ( fs . exists ( path ) ) { PutObjectResult result = s3client . putObject ( request ) ; LOGGER . info ( "putObjectResult: {}" , result . getResult ( ) . toString ( ) ) ; } }
public void test() { try { Path path = new Path ( outputMetaData . getPath ( ) ) ; code_block = IfStatement ; FSDataInputStream fsinput = fs . open ( path ) ; ObjectMetadata omd = new ObjectMetadata ( ) ; omd . setContentLength ( outputMetaData . getSize ( ) ) ; String keyName = directoryName + Path . SEPARATOR + outputMetaData . getFileName ( ) ; PutObjectRequest request = new PutObjectRequest ( bucketName , keyName , fsinput , omd ) ; code_block = IfStatement ; code_block = IfStatement ; } catch ( FileNotFoundException e ) { logger . error ( "Unable to find meta data file: {}" , e . getMessage ( ) ) ; } catch ( IOException e ) { logger . error ( "Unable to create Stream: {}" , e . getMessage ( ) ) ; } }
public void test() { try { Path path = new Path ( outputMetaData . getPath ( ) ) ; code_block = IfStatement ; FSDataInputStream fsinput = fs . open ( path ) ; ObjectMetadata omd = new ObjectMetadata ( ) ; omd . setContentLength ( outputMetaData . getSize ( ) ) ; String keyName = directoryName + Path . SEPARATOR + outputMetaData . getFileName ( ) ; PutObjectRequest request = new PutObjectRequest ( bucketName , keyName , fsinput , omd ) ; code_block = IfStatement ; code_block = IfStatement ; } catch ( FileNotFoundException e ) { logger . debug ( "Ignoring non-existent path assuming replay : {}" , outputMetaData . getPath ( ) ) ; } catch ( IOException e ) { logger . error ( "Exception writing to disk" , e ) ; } }
public void test() { try { systemJobManager . submit ( indexSetCleanupJobFactory . create ( indexSet ) ) ; } catch ( SystemJobConcurrencyException e ) { LOGGER . warn ( "Cannot submit index setCleanupJob" , e ) ; } }
@ SuppressWarnings ( "unused" ) private void testScanRaw ( String msg ) throws IOException { long t = System . currentTimeMillis ( ) ; IGTScanner scan = simpleStore . scan ( new GTScanRequestBuilder ( ) . setInfo ( info ) . setRanges ( null ) . setDimensions ( null ) . setFilterPushDown ( null ) . createGTScanRequest ( ) ) ; ResultScanner innerScanner = ( ( SimpleHBaseStore . Reader ) scan ) . getHBaseScanner ( ) ; int count = 0 ; code_block = ForStatement ; scan . close ( ) ; t = System . currentTimeMillis ( ) - t ; LOGGER . info ( msg ) ; }
public void onGroupRenamed ( UDGroup group ) { logger . debug ( "Moving group {}" , group . address ) ; Isy99iFrame . writeAreaLog ( Isy99iUtilities . getDateTime ( ) + ": Scene: " + group . address + " was removed by someone or something!" ) ; }
@ RequestMapping ( "/experimentoverview" ) public @ ResponseBody String experimentoverview ( @ RequestParam ( value = "experimentType" ) String experimentType , @ RequestParam ( value = "matching" ) String matchingString ) { Matching matching = MainController . getMatching ( matchingString ) ; LOGGER . debug ( " experimentoverview started" ) ; ExperimentType eType = ExperimentType . valueOf ( experimentType ) ; String annotatorNames [ ] = loadAnnotators ( eType ) ; String datasetNames [ ] = loadDatasets ( eType ) ; double results [ ] [ ] = loadLatestResults ( eType , matching , annotatorNames , datasetNames ) ; double correlations [ ] [ ] = calculateCorrelations ( results , datasetNames ) ; return generateJson ( results , correlations , annotatorNames , datasetNames ) ; }
public void test() { try { code_block = ForStatement ; addRemoveInternetScsiTargetsToAllHosts ( false , targetsToRemove , hosts ) ; rescanAllHosts ( hosts , true , false ) ; } catch ( Exception ex ) { s_logger . warn ( ex . toString ( ) ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( String . format ( "Failure closing %s" , e . getMessage ( ) ) ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( DDMTemplateServiceUtil . class , "getTemplatesByClassPK" , _getTemplatesByClassPKParameterTypes16 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , companyId , groupId , classPK , resourceClassNameId , status ) ; Object returnObj = null ; code_block = TryStatement ;  return ( java . util . List < com . liferay . dynamic . data . mapping . model . DDMTemplate > ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { prepare ( 2 ) ; FakeAllocatableAction instance0 = new FakeAllocatableAction ( fao , 0 ) ; FakeAllocatableAction instance1 = new FakeAllocatableAction ( fao , 1 ) ; instance0 . assignResource ( rs ) ; instance1 . assignResource ( rs ) ; instance0 . tryToLaunch ( ) ; error ( instance0 ) ; checkExecutions ( new int [ ] code_block = "" ; ) ; checkErrors ( new int [ ] code_block = "" ; ) ; checkFailed ( new int [ ] code_block = "" ; ) ; checkCancelled ( new int [ ] code_block = "" ; ) ; instance0 . assignResource ( rs ) ; instance0 . tryToLaunch ( ) ; error ( instance0 ) ; checkExecutions ( new int [ ] code_block = "" ; ) ; checkErrors ( new int [ ] code_block = "" ; ) ; checkFailed ( new int [ ] code_block = "" ; ) ; checkCancelled ( new int [ ] code_block = "" ; ) ; } catch ( Throwable e ) { logger . error ( e . getMessage ( ) , e ) ; fail ( e . getMessage ( ) ) ; } }
public void test() { try ( final LockedDocument lockedDoc = broker . getXMLResource ( getTarget ( ) , Lock . LockMode . WRITE_LOCK ) ) { final DocumentImpl doc = lockedDoc . getDocument ( ) ; final Permission permission = doc . getPermissions ( ) ; PermissionFactory . chown ( broker , permission , Optional . ofNullable ( getOwner ( ) ) , Optional . ofNullable ( getGroup ( ) ) ) ; PermissionFactory . chmod ( broker , permission , Optional . of ( getMode ( ) ) , Optional . ofNullable ( permission instanceof ACLPermission ? getAces ( ) : null ) ) ; broker . storeXMLResource ( transaction , doc ) ; } catch ( final PermissionDeniedException e ) { final String msg = "ERROR: Failed to set permissions on Document '" + getTarget ( ) + "'." ; LOGGER . warn ( msg ) ; getListener ( ) . warn ( msg ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( configuration . getApi ( ) . isDisableApiServer ( ) ) { LOGGER . info ( "Disable API server" ) ; return ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { transaction . rollback ( ) ; } catch ( RuntimeException ex ) { LOG . warn ( "Exception follows." , ex ) ; LOG . debug ( "Exception follows." , ex ) ; } finally { transaction . close ( ) ; this . transaction = null ; } }
public void test() { if ( containers . size ( ) > 0 ) { log . warn ( "Found more than one instance of container %s: %s" , containerName , container ) ; } }
public void test() { try { containers = chooseContainerForBlockDeletion ( blockLimitPerInterval , containerDeletionPolicy ) ; BlockDeletingTask containerBlockInfos = null ; long totalBlocks = 0 ; code_block = ForStatement ; code_block = IfStatement ; } catch ( StorageContainerException e ) { LOG . error ( "Error while choosing container." , e ) ; } catch ( Exception e ) { code_block = IfStatement ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { if ( indexTaskSpecs . isEmpty ( ) ) { log . info ( "No index task specified." ) ; return TaskStatus . failure ( getId ( ) ) ; } else { registerResourceCloserOnAbnormalExit ( currentSubTaskHolder ) ; final int totalNumSpecs = indexTaskSpecs . size ( ) ; log . info ( "Generated [%d] compaction task specs" , totalNumSpecs ) ; int failCnt = 0 ; code_block = ForStatement ; log . info ( "Run [%d] specs, [%d] succeeded, [%d] failed" , totalNumSpecs , totalNumSpecs - failCnt , failCnt ) ; return failCnt == 0 ? TaskStatus . success ( getId ( ) ) : TaskStatus . failure ( getId ( ) ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { failCnt ++ ; logger . info ( "sendTasks fail" , e ) ; } }
public void test() { try { FileUtils . deleteDirectory ( parent ) ; LOG . info ( "Deleted directory: " + parent . getAbsolutePath ( ) ) ; } catch ( IOException e ) { LOG . warn ( "Failed to delete directory: " + parent . getAbsolutePath ( ) ) ; } }
public void test() { if ( log != null && log . isInfoEnabled ( ) ) { log . info ( "Job has been started:" ) ; } }
public void test() { try { LOG . info ( "Monitoring thread(s) !!" ) ; future . get ( ) ; } catch ( ExecutionException ex ) { LOG . error ( "Got Exception Monitoring threads" , ex ) ; error = true ; } catch ( InterruptedException ie ) { LOG . error ( "Got interrupted Monitoring threads" , ie ) ; error = true ; } finally { shutdown = true ; code_block = IfStatement ; shutdown ( false ) ; } }
public void test() { try { LOG . info ( "Monitoring thread(s) !!" ) ; future . get ( ) ; } catch ( ExecutionException ex ) { LOG . error ( "Monitor noticed one or more threads failed. Requesting graceful shutdown of other threads" , ex ) ; error = true ; } catch ( InterruptedException ie ) { LOG . error ( "Monitor noticed one or more threads failed. Requesting graceful shutdown of other threads" , ie ) ; error = true ; } finally { shutdown = true ; code_block = IfStatement ; shutdown ( false ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( logger . isFineEnabled ( ) ) { logger . info ( leavingMember + " will be removed from " + leavingGroupIds ) ; } else { logger . info ( leavingMember + " will be removed from " + leavingGroupIds ) ; } }
public void test() { if ( logger . isFineEnabled ( ) ) { logger . info ( leavingMember + " will be removed from " + changes ) ; } else { logger . info ( leavingMember + " will be removed from " + changes ) ; } }
@ Before public void setUp ( ) throws Exception { log . info ( "setting up the scheduler store" ) ; IOHelper . deleteFile ( schedulerStoreDir ) ; createBroker ( ) ; broker . start ( ) ; broker . waitUntilStarted ( ) ; schedulerStore = ( JobSchedulerStoreImpl ) broker . getJobSchedulerStore ( ) ; }
public void test() { try { server . start ( ) ; server . join ( ) ; } catch ( Exception e ) { log . error ( e ) ; } }
public void test() { try { OAuth2RestOperations restTemplate = this . restTemplate ; code_block = IfStatement ; OAuth2AccessToken existingToken = restTemplate . getOAuth2ClientContext ( ) . getAccessToken ( ) ; code_block = IfStatement ; return restTemplate . getForEntity ( path , Map . class ) . getBody ( ) ; } catch ( Exception ex ) { LOG . error ( ex . getMessage ( ) , ex ) ; return Collections . < String , Object > singletonMap ( "error" , "Could not fetch user details" ) ; } }
@ Override public boolean isReadOnly ( ) { logger . log ( Level . FINE , "isReadOnly called" ) ; return false ; }
public void test() { try { System . setProperty ( "groovy.root" , new File ( this . environmentProvider . get ( ) . getPermanentDirectory ( ) , "cache/groovy" ) . getAbsolutePath ( ) ) ; } catch ( Exception e ) { logger . warn ( "Error while setting the root directory" , e ) ; } }
@ Override public void refresh ( Collection < Refreshable > alreadyRefreshed ) { logger . debug ( "Refreshed" ) ; reload ( ) ; }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { LOG . error ( "Could not create file" , e ) ; throw new RuntimeException ( e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( DLFileEntryServiceUtil . class , "deleteFileVersion" , _deleteFileVersionParameterTypes10 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , fileEntryId , version ) ; code_block = TryStatement ;  } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( count != contentRows . size ( ) ) { String errmsg = String . format ( "Wrong number of contents migrated. Expected: %s, Inserted: %s" , contentRows . size ( ) , count ) ; logger . error ( errmsg ) ; throw new DatabaseException ( errmsg ) ; } }
public void test() { if ( currentStateEnum == ZoneStateEnum . ENROLLED ) { logger . debug ( "Requested resource {} not found" , currentStateEnum ) ; return ; } }
public void test() { if ( currentState != null ) { ZoneStateEnum currentStateEnum = ZoneStateEnum . getByValue ( currentState ) ; logger . debug ( "{}: IAS CIE state is currently {}[{}]" , iasZoneCluster . getZigBeeAddress ( ) , currentStateEnum , currentState ) ; code_block = IfStatement ; } else { logger . debug ( "No current state found for: {}" , iasZoneCluster . getZigBeeAddress ( ) ) ; } }
private void initialise ( ) { log . trace ( "initialise" ) ; Integer currentState = ( Integer ) iasZoneCluster . getAttribute ( ZclIasZoneCluster . ATTR_ZONESTATE ) . readValue ( Long . MAX_VALUE ) ; code_block = IfStatement ; ZclAttribute cieAddressAttribute = iasZoneCluster . getAttribute ( ZclIasZoneCluster . ATTR_IASCIEADDRESS ) ; IeeeAddress currentIeeeAddress = ( IeeeAddress ) cieAddressAttribute . readValue ( 0 ) ; code_block = IfStatement ; Integer currentZone = ( Integer ) iasZoneCluster . getAttribute ( ZclIasZoneCluster . ATTR_ZONEID ) . readValue ( 0 ) ; code_block = IfStatement ; zoneType = ( Integer ) iasZoneCluster . getAttribute ( ZclIasZoneCluster . ATTR_ZONETYPE ) . readValue ( Long . MAX_VALUE ) ; code_block = IfStatement ; final Runnable runnableTask = new AutoEnrollmentTask ( ) ; autoEnrollmentTask = networkManager . scheduleTask ( runnableTask , autoEnrollDelay ) ; }
public void test() { if ( ieeeAddress . equals ( currentIeeeAddress ) ) { logger . info ( "{}: IAS CIE address is confirmed {}" , iasZoneCluster . getZigBeeAddress ( ) , currentIeeeAddress ) ; } else { logger . warn ( "{}: IAS CIE address is NOT confirmed {}" , iasZoneCluster . getZigBeeAddress ( ) , currentIeeeAddress ) ; } }
public void test() { if ( ieeeAddress . equals ( currentIeeeAddress ) ) { logger . debug ( "{}: IAS CIE address is confirmed {}" , iasZoneCluster . getZigBeeAddress ( ) , currentIeeeAddress ) ; } else { logger . debug ( "{}: UAS CIE address is not confirmed {}" , iasZoneCluster . getZigBeeAddress ( ) , currentIeeeAddress ) ; } }
public void test() { if ( currentZone == null ) { logger . debug ( "{}: IAS CIE zone ID is null" , iasZoneCluster . getZigBeeAddress ( ) ) ; } else { logger . debug ( "{}: IAS CIE zone ID is currently {}" , iasZoneCluster . getZigBeeAddress ( ) , currentZone ) ; } }
public void test() { if ( currentZone == null ) { logger . debug ( "{}: IAS CIE zone ID request failed" , iasZoneCluster . getZigBeeAddress ( ) ) ; } else { logger . debug ( "{}: IAS CIE zone ID request succeeded" , iasZoneCluster . getZigBeeAddress ( ) ) ; } }
public void test() { if ( zoneType == null ) { logger . debug ( "{}: IAS CIE zone type is 0x{}, {}" , iasZoneCluster . getZigBeeAddress ( ) , String . format ( "%04X" , zoneType ) ) ; } else { logger . debug ( "{}: IAS CIE zone type is 0x{}, {}" , iasZoneCluster . getZigBeeAddress ( ) , String . format ( "%04X" , zoneType ) , ZoneTypeEnum . getByValue ( zoneType ) ) ; } }
public void test() { if ( zoneType == null ) { logger . debug ( "{}: IAS CIE zone type request failed" , iasZoneCluster . getZigBeeAddress ( ) ) ; } else { logger . debug ( "{}: IAS CIE zone type request completed due to {}" , iasZoneCluster . getZigBeeAddress ( ) , zoneType ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { try { Map < String , Object > parameters = new HashMap < > ( ) ; parameters . put ( USECASE , usecase ) ; AutoMLConfig result = getSingleResult ( "FROM AutoMLConfig AC WHERE AC.usecase = :usecase" , AutoMLConfig . class , parameters ) ; return result . getPredictionColumn ( ) ; } catch ( Exception e ) { log . error ( e . getMessage ( ) ) ; throw e ; } }
public void test() { try { socket . close ( ) ; } catch ( Throwable t ) { logger . warn ( "Exception while closing BLOB server connection socket." , t ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { for ( String cache : pools ) { code_block = IfStatement ; } }
public void test() { for ( String cache : pools ) { code_block = IfStatement ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { try { result = deleteCiscoNexusVSM ( cmd . getCiscoNexusVSMDeviceId ( ) ) ; } catch ( ResourceInUseException e ) { s_logger . warn ( "Failed to delete specified VSM" , e ) ; throw new CloudRuntimeException ( "Failed to delete specified VSM" ) ; } }
protected synchronized void activate ( final Map < String , Object > properties , ComponentContext componentContext ) { logger . debug ( "Activating Regex Filter..." ) ; this . filter = String . valueOf ( properties . getOrDefault ( REGEX_PROP , "" ) ) ; this . componentPid = String . valueOf ( properties . get ( KURA_SERVICE_PID ) ) ; this . filterType = getType ( properties ) ; this . wireSupport = this . wireHelperService . newWireSupport ( this , ( ServiceReference < WireComponent > ) componentContext . getServiceReference ( ) ) ; logger . debug ( "Activating Regex Filter... Done" ) ; }
protected synchronized void activate ( final Map < String , Object > properties , ComponentContext componentContext ) { logger . debug ( "Activating Regex Filter..." ) ; this . filter = String . valueOf ( properties . getOrDefault ( REGEX_PROP , "" ) ) ; this . componentPid = String . valueOf ( properties . get ( KURA_SERVICE_PID ) ) ; this . filterType = getType ( properties ) ; this . wireSupport = this . wireHelperService . newWireSupport ( this , ( ServiceReference < WireComponent > ) componentContext . getServiceReference ( ) ) ; logger . debug ( "Activating Regex Filter... Done" ) ; }
public void test() { try { createTestNodes ( TestOne . class , 100 ) ; app . command ( SyncCommand . class ) . execute ( toMap ( "mode" , "export" , "file" , EXPORT_FILENAME ) ) ; final Path exportFile = Paths . get ( EXPORT_FILENAME ) ; assertTrue ( "Export file doesn't exist!" , Files . exists ( exportFile ) ) ; cleanDatabaseAndSchema ( ) ; app . command ( SyncCommand . class ) . execute ( toMap ( "mode" , "import" , "file" , EXPORT_FILENAME ) ) ; code_block = TryStatement ;  Files . delete ( exportFile ) ; } catch ( Exception ex ) { logger . error ( "test failed." , ex ) ; fail ( "Unexpected exception." ) ; } }
public void run ( ) { LOGGER . info ( "MCRJob checking is running" ) ; EntityManager em = MCREntityManagerProvider . getEntityManagerFactory ( ) . createEntityManager ( ) ; EntityTransaction transaction = em . getTransaction ( ) ; transaction . begin ( ) ; StringBuilder sb = new StringBuilder ( "FROM MCRJob WHERE " ) ; code_block = IfStatement ; sb . append ( " status='" + MCRJobStatus . PROCESSING + "' ORDER BY id ASC" ) ; TypedQuery < MCRJob > query = em . createQuery ( sb . toString ( ) , MCRJob . class ) ; long current = new Date ( System . currentTimeMillis ( ) ) . getTime ( ) / 60000 ; boolean reset = query . getResultList ( ) . stream ( ) . map ( job code_block = LoopStatement ; ) . reduce ( Boolean :: logicalOr ) . orElse ( false ) ; code_block = TryStatement ;  code_block = IfStatement ; em . close ( ) ; LOGGER . info ( "MCRJob checking is done" ) ; }
public void test() { if ( current - start >= maxTimeDiff ) { LOGGER . debug ( "Timed out waiting for new job" ) ; job . setStatus ( MCRJobStatus . NEW ) ; job . setStart ( null ) ; ret = true ; } else { LOGGER . debug ( "->ok" ) ; } }
public void test() { if ( current - start >= maxTimeDiff ) { LOGGER . debug ( "->Resetting too long in queue" ) ; job . setStatus ( MCRJobStatus . NEW ) ; job . setStart ( null ) ; ret = true ; } else { LOGGER . debug ( "-> already reset in queue" ) ; } }
public void run ( ) { EntityManager em = MCREntityManagerProvider . getEntityManagerFactory ( ) . createEntityManager ( ) ; EntityTransaction transaction = em . getTransaction ( ) ; LOGGER . info ( "MCRJob is Checked for dead Entries" ) ; transaction . begin ( ) ; StringBuilder sb = new StringBuilder ( "FROM MCRJob WHERE " ) ; code_block = IfStatement ; sb . append ( " status='" + MCRJobStatus . PROCESSING + "' ORDER BY id ASC" ) ; TypedQuery < MCRJob > query = em . createQuery ( sb . toString ( ) , MCRJob . class ) ; long current = new Date ( System . currentTimeMillis ( ) ) . getTime ( ) / 60000 ; boolean reset = query . getResultList ( ) . stream ( ) . map ( job code_block = LoopStatement ; ) . reduce ( Boolean :: logicalOr ) . orElse ( false ) ; code_block = TryStatement ;  code_block = IfStatement ; em . close ( ) ; LOGGER . info ( "Job finished" ) ; }
public void test() { if ( to_save ) { this . customSave ( _b , false ) ; update_happened = true ; log . info ( "Revoked" ) ; } else { log . debug ( "CVE data of bug [" + _b . getBugId ( ) + "] did not change, no update of local database needed" ) ; } }
public void test() { if ( to_save ) { log . debug ( "CVE data of bug [" + _b . getBugId ( ) + "] changed, triggering update of local database" ) ; this . customSave ( _b , false ) ; update_happened = true ; } else { log . info ( "CVE data of bug [" + _b . getBugId ( ) + "] already saved" ) ; } }
public void test() { try { String cve_id = Cve . extractCveIdentifier ( _b . getBugId ( ) ) ; if ( cve_id == null ) cve_id = Cve . extractCveIdentifier ( _b . getBugIdAlt ( ) ) ; final Cve cve = CveReader2 . read ( cve_id ) ; code_block = IfStatement ; } catch ( CacheException e ) { log . error ( "Cannot save bug [" + _b . getBugId ( ) + "] with refreshed CVE data" , e ) ; } catch ( PersistenceException e ) { log . error ( "Cannot save bug [" + _b . getBugId ( ) + "] with refreshed CVE data: " + e . getMessage ( ) ) ; } }
public void test() { try { String cve_id = Cve . extractCveIdentifier ( _b . getBugId ( ) ) ; if ( cve_id == null ) cve_id = Cve . extractCveIdentifier ( _b . getBugIdAlt ( ) ) ; final Cve cve = CveReader2 . read ( cve_id ) ; code_block = IfStatement ; } catch ( CacheException e ) { log . error ( "Cache exception when refreshing CVE data of bug [" + _b . getBugId ( ) + "]: " + e . getMessage ( ) ) ; } catch ( PersistenceException e ) { log . error ( "Persistence exception when refreshing CVE data of bug [" + _b . getBugId ( ) + "]: " + e . getMessage ( ) ) ; } }
public void test() { if ( _force || this . needsCveData ( _b ) ) { code_block = TryStatement ;  } else { _logger . info ( "Cve data is needed because it is not needed" ) ; } }
public void test() { try { InternetAddress address = new InternetAddress ( email , true ) ; address . setPersonal ( udr . getUser ( ) . getDisplayName ( ) , Charsets . UTF_8 . name ( ) ) ; return address ; } catch ( UnsupportedEncodingException | AddressException e ) { logger . error ( "Could not parse {}" , email , e ) ; throw Throwables . propagate ( e ) ; } }
public void test() { try { code_block = ForStatement ; } catch ( Exception e ) { logger . error ( Messages . getInstance ( ) . getErrorString ( "Workspace.ERROR_0002_PROPS_EXCEPTION" ) , e ) ; } }
public void test() { try ( final ManagedLock < ReentrantLock > dbLock = lockManager . acquireBtreeReadLock ( index . db . getLockName ( ) ) ) { final SearchCallback cb = new SearchCallback ( contextId , query , ngram , docs , contextSet , context , result , axis == NodeSet . ANCESTOR ) ; final int op = query . codePointCount ( 0 , query . length ( ) ) < getN ( ) ? IndexQuery . TRUNC_RIGHT : IndexQuery . EQ ; index . db . query ( new IndexQuery ( op , key ) , cb ) ; } catch ( final LockException e ) { LOG . error ( e . getMessage ( ) , e ) ; } catch ( final IOException | BTreeException e ) { LOG . error ( "{} in '{}'" , e . getMessage ( ) , FileUtils . fileName ( index . db . getFile ( ) ) , e ) ; } }
public void test() { try ( final ManagedLock < ReentrantLock > dbLock = lockManager . acquireBtreeReadLock ( index . db . getLockName ( ) ) ) { final SearchCallback cb = new SearchCallback ( contextId , query , ngram , docs , contextSet , context , result , axis == NodeSet . ANCESTOR ) ; final int op = query . codePointCount ( 0 , query . length ( ) ) < getN ( ) ? IndexQuery . TRUNC_RIGHT : IndexQuery . EQ ; index . db . query ( new IndexQuery ( op , key ) , cb ) ; } catch ( final LockException e ) { LOG . warn ( "Failed to acquire lock to '{}'" , FileUtils . fileName ( index . db . getFile ( ) ) , e ) ; } catch ( final IOException | BTreeException e ) { LOG . error ( e ) ; } }
public void test() { if ( attribute . needsBase64Encoding ( ) ) { log . warn ( "Attribute " + attribute + " to " + attribute . getBase64Encoding ( ) ) ; } }
public void test() { try { inetAddress = InetAddress . getByAddress ( finalmask ) ; } catch ( UnknownHostException e ) { LOGGER . error ( "Unknown host: {}" , finalmask ) ; return null ; } }
@ Override public void onError ( Throwable throwable ) { logger . error ( "Error occurred during application key mapping discovery" , throwable ) ; XdsSchedulerManager . getInstance ( ) . startApplicationKeyMappingDiscoveryScheduling ( ) ; nack ( throwable ) ; }
public void test() { try { return resultSet . getTimestamp ( columnIndex ) ; } catch ( SQLException e ) { LOGGER . error ( "could not get timestamp" , e ) ; LOGGER . error ( "raw value:" + resultSet . getObject ( columnIndex ) ) ; throw ( e ) ; } }
public void test() { try { txn . start ( ) ; String sql = UPDATE_USER_STATS ; PreparedStatement pstmt = null ; pstmt = txn . prepareAutoCloseStatement ( sql ) ; code_block = ForStatement ; pstmt . executeBatch ( ) ; txn . commit ( ) ; } catch ( Exception ex ) { txn . rollback ( ) ; s_logger . error ( "Error updating user stats" , ex ) ; throw new CloudRuntimeException ( ex . getMessage ( ) ) ; } }
public void test() { if ( LOGGER . isTraceEnabled ( ) ) { LOGGER . trace ( "Weak listener list status:{}" , System . lineSeparator ( ) ) ; } }
public void test() { if ( LOGGER . isTraceEnabled ( ) ) { LOGGER . trace ( "Weak listener list status:{}" , System . lineSeparator ( ) ) ; } }
public void test() { if ( null == database ) { log . debug ( String . format ( "Database not found for %s" , host ) ) ; return super . verify ( host , key ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( IOException e ) { _log . error ( "Unable to open file " + path , e ) ; return false ; } }
public MbSchicht merge ( MbSchicht detachedInstance ) { log . debug ( "merging MbSchicht instance" ) ; code_block = TryStatement ;  }
public void test() { try { MbSchicht result = ( MbSchicht ) sessionFactory . getCurrentSession ( ) . merge ( detachedInstance ) ; log . debug ( "merge successful" ) ; return result ; } catch ( RuntimeException re ) { log . error ( "merge failed" , re ) ; throw re ; } }
public void test() { try { MbSchicht result = ( MbSchicht ) sessionFactory . getCurrentSession ( ) . merge ( detachedInstance ) ; log . debug ( "merge successful" ) ; return result ; } catch ( RuntimeException re ) { log . error ( "merge failed" , re ) ; throw re ; } }
public void test() { try { tState = TableState . valueOf ( sState ) ; } catch ( IllegalArgumentException e ) { logger . warn ( "Unknown state: {}" , e . getMessage ( ) ) ; } }
@ Test public void test ( ) throws IOException { log . debug ( "Client started" ) ; JsonRpcClient client = createJsonRpcClient ( "/jsonrpc" ) ; Params params = new Params ( ) ; params . param1 = "Value1" ; params . param2 = "Value2" ; Params result = client . sendRequest ( "echo" , params , Params . class ) ; log . debug ( "Response:" + result ) ; Assert . assertEquals ( params . param1 , result . param1 ) ; Assert . assertEquals ( params . param2 , result . param2 ) ; client . close ( ) ; log . debug ( "Client finished" ) ; }
@ Test public void test ( ) throws IOException { log . debug ( "Client started" ) ; JsonRpcClient client = createJsonRpcClient ( "/jsonrpc" ) ; Params params = new Params ( ) ; params . param1 = "Value1" ; params . param2 = "Value2" ; Params result = client . sendRequest ( "echo" , params , Params . class ) ; log . debug ( "Response: {}" , result ) ; Assert . assertEquals ( params . param1 , result . param1 ) ; Assert . assertEquals ( params . param2 , result . param2 ) ; client . close ( ) ; log . debug ( "Client finished" ) ; }
@ Test public void test ( ) throws IOException { log . debug ( "Client started" ) ; JsonRpcClient client = createJsonRpcClient ( "/jsonrpc" ) ; Params params = new Params ( ) ; params . param1 = "Value1" ; params . param2 = "Value2" ; Params result = client . sendRequest ( "echo" , params , Params . class ) ; log . debug ( "Response:" + result ) ; Assert . assertEquals ( params . param1 , result . param1 ) ; Assert . assertEquals ( params . param2 , result . param2 ) ; client . close ( ) ; log . debug ( "Client finished" ) ; }
public ItemsVO createNewEntryInItemsTable ( ItemsVO vo ) { long timerStart = System . currentTimeMillis ( ) ; Long i = conf . getDBDAO ( ) . doCreateNewEntryInItemsTable ( vo ) ; vo . setItemid ( i . intValue ( ) ) ; logTime ( "doCreateNewEntryInItemsTable" , timerStart , System . currentTimeMillis ( ) ) ; log . debug ( "Created " + i ) ; return vo ; }
public void test() { if ( file . exists ( ) && ! file . delete ( ) ) { logger . warn ( "Unable to delete file: " + file ) ; } }
public void test() { if ( ! enableContentRecommendation || ( _asahInterestTermProvider == null ) ) { _log . debug ( "Skipping enable content recommendations" ) ; return ; } }
public void test() { if ( isEmpty ( ) ) { return ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public int doMove ( PrintStream resultOs , PrintStream logOs ) throws Exception { code_block = ForStatement ; int failedMoves = ReplicaMove . failedMoves ( allMoves ) ; log . info ( "{}: doMove({})" , logPrefix , allMoves ) ; return failedMoves ; }
public void test() { try { runnable . run ( ) ; } catch ( Throwable e ) { LOGGER . error ( "process error" , e ) ; } }
public void test() { if ( trace ) { LOGGER . trace ( "{} >> {}" , uniqueId , value ) ; } }
public void test() { try { this . client . getSetSchedule ( request ) ; } catch ( final SoapFaultClientException ex ) { final String faultString = ex . getFaultStringOrReason ( ) ; LOG . error ( "Soap fault encountered: " + ex . getMessage ( ) , ex ) ; code_block = IfStatement ; ScenarioContext . current ( ) . put ( PlatformKeys . RESPONSE , ex ) ; return null ; } }
public void test() { { String name = Thread . currentThread ( ) . getName ( ) ; logger . debug ( "messageReceived thread-{} message with name: {}" , Thread . currentThread ( ) . getName ( ) , name ) ; Thread . sleep ( 10000 ) ; logger . debug ( "messageReceived thread-{} message:" , Thread . currentThread ( ) . getName ( ) ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( PortalException portalException ) { _log . error ( portalException , portalException ) ; } }
@ AfterClass public static void reportTestFinish ( ) { stopwatch . stop ( ) ; LOGGER . warn ( "-----------------------------------------" ) ; LOGGER . warn ( "*                                      *" ) ; LOGGER . warn ( "* FINISHED GeoWaveSparkSQLIT         *" ) ; LOGGER . warn ( "*       " + stopwatch . getTimeString ( ) + " elapsed.        *" ) ; LOGGER . warn ( "*                                     *" ) ; LOGGER . warn ( "-----------------------------------------" ) ; }
public void test() { try { response . sendRedirect ( StringExtensions . combinePath ( request . getContextPath ( ) , redirectTo ) ) ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( configuration . getPowerStateChanging ( ) == PowerStateChanging . ALWAYS_OFF && ( powerState != OnOffType . OFF ) ) { logger . debug ( "Correcting power state of {} ({}) to OFF" , deviceType , macAddress ) ; handleOnOffCommand ( OnOffType . OFF ) ; } else-if ( configuration . getPowerStateChanging ( ) == PowerStateChanging . ALWAYS_ON && ( powerState != OnOffType . ON ) ) { logger . debug ( "Correcting power state of {} ({}) to on" , deviceType , macAddress ) ; handleOnOffCommand ( OnOffType . ON ) ; } }
public void test() { if ( configuration . getPowerStateChanging ( ) == PowerStateChanging . ALWAYS_OFF && ( powerState != OnOffType . OFF ) ) { logger . debug ( "Correcting power state of {} ({}) to off" , deviceType , macAddress ) ; handleOnOffCommand ( OnOffType . OFF ) ; } else-if ( configuration . getPowerStateChanging ( ) == PowerStateChanging . ALWAYS_ON && ( powerState != OnOffType . ON ) ) { logger . debug ( "Correcting power state of {} ({}) to ON" , deviceType , macAddress ) ; handleOnOffCommand ( OnOffType . ON ) ; } }
public void test() { try { vendorNames = thriftClients . makeVendorClient ( ) . getAllVendorNames ( ) ; } catch ( TException e ) { logger . error ( "Could not fetch vendor names from backend!" , e ) ; vendorNames = Collections . emptySet ( ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
private boolean isDuplicatedSlot ( TaskSlot taskSlot , JobID jobId , ResourceProfile resourceProfile , int index ) { LOG . debug ( "Replicating slot {} for job {}" , jobId , jobId ) ; return taskSlot . getJobId ( ) . equals ( jobId ) && taskSlot . getResourceProfile ( ) . equals ( resourceProfile ) && ( isDynamicIndex ( index ) || taskSlot . getIndex ( ) == index ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( ! Objects . equals ( original . getMetadata ( ) , addressSpace . getMetadata ( ) ) ) { LOG . debug ( String . format ( "Adding address space %s to the address space %s" , addressSpace . getMetadata ( ) . getName ( ) , addressSpace . getMetadata ( ) . getName ( ) ) ) ; changed = true ; } }
public void test() { if ( ! Objects . equals ( original . getSpec ( ) , addressSpace . getSpec ( ) ) ) { LOG . info ( "Specified node's spec is changed from {} to {}." , addressSpace . getSpec ( ) , addressSpace . getSpec ( ) ) ; changed = true ; } }
public void test() { if ( StringUtils . isBlank ( addressSpace . getStatus ( ) ) ) { return ; } }
public void test() { try { applicationRoot = VFS . getManager ( ) . resolveFile ( applicationDirectoryPath ) ; code_block = IfStatement ; } catch ( FileSystemException e ) { LOGGER . warn ( "Could not resolve application directory: {}" , applicationDirectoryPath , e ) ; } }
public void initialize ( ) { String applicationDirectoryPath = getApplicationDirectoryPath ( ) ; if ( applicationDirectoryPath != null ) logger . info ( "Using directory {}" , applicationDirectoryPath ) ; code_block = TryStatement ;  code_block = IfStatement ; code_block = TryStatement ;  String actionsDirectory = getConfiguration ( ) . getString ( "portofino.actions.path" , "actions" ) ; codeBase = initApplicationRoot ( actionsDirectory ) ; logger . info ( "Application initialized." ) ; }
public void initialize ( ) { String applicationDirectoryPath = getApplicationDirectoryPath ( ) ; if ( applicationDirectoryPath != null ) logger . info ( "Application directory initialized" ) ; code_block = TryStatement ;  code_block = IfStatement ; logger . info ( "Application directory: {}" , applicationRoot ) ; code_block = TryStatement ;  String actionsDirectory = getConfiguration ( ) . getString ( "portofino.actions.path" , "actions" ) ; codeBase = initApplicationRoot ( actionsDirectory ) ; }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceTierPriceEntryServiceUtil . class , "getCommerceTierPriceEntry" , _getCommerceTierPriceEntryParameterTypes11 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , commerceTierPriceEntryId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . commerce . price . list . model . CommerceTierPriceEntry ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( composedMessage == null ) { LOGGER . error ( "null composed message" ) ; return false ; } }
public void test() { if ( cf == null || ! cf . isEmailEnabled ( ) ) { LOG . info ( "Email is not email. Email is disabled." ) ; return false ; } }
public void test() { try { SAMLBindings binding = isGet ? SAMLBindings . HTTP_REDIRECT : SAMLBindings . HTTP_POST ; LogoutResponseDocument respDoc = LogoutResponseDocument . Factory . parse ( samlResponse ) ; SAMLVerifiableElement verifiableMessage = binding == SAMLBindings . HTTP_REDIRECT ? new RedirectedMessage ( httpReq . getQueryString ( ) ) : new XMLExpandedMessage ( respDoc , respDoc . getLogoutResponse ( ) ) ; SAMLMessage < LogoutResponseDocument > responseMessage = new SAMLMessage < > ( verifiableMessage , relayState , binding , respDoc ) ; logoutProcessor . handleAsyncLogoutResponse ( responseMessage , httpResp ) ; } catch ( XmlException e ) { httpResp . sendError ( HttpServletResponse . SC_BAD_REQUEST , "Invalid SLO response (XML is malformed)" ) ; } catch ( EopException e ) { log . warn ( "Can't handle SLO request" , e ) ; } }
public void test() { try { Version esVersion = Version . fromString ( client . info ( RequestOptions . DEFAULT ) . getVersion ( ) . getNumber ( ) ) ; code_block = IfStatement ; String esVersionCompatibilityWarn = String . format ( "ES version(%s) is not compatible with the recommendation(%s)" , esVersion . toString ( ) , RECOMMENDED_ES_VERSION . toString ( ) ) ; LOGGER . info ( "ES version(%s) is not compatible with the recommendation(%s)" , esVersion . toString ( ) , RECOMMENDED_ES_VERSION . toString ( ) ) ; return CheckResult . builder ( ) . checkName ( checkName ( ) ) . resultType ( ResultType . BAD ) . description ( esVersionCompatibilityWarn ) . build ( ) ; } catch ( IOException e ) { LOGGER . error ( VERSION_CHECKING_ERROR_MESSAGE , e ) ; return CheckResult . builder ( ) . checkName ( checkName ( ) ) . resultType ( ResultType . BAD ) . description ( VERSION_CHECKING_ERROR_MESSAGE + ": " + e . getMessage ( ) ) . build ( ) ; } }
public void test() { try { Version esVersion = Version . fromString ( client . info ( RequestOptions . DEFAULT ) . getVersion ( ) . getNumber ( ) ) ; code_block = IfStatement ; String esVersionCompatibilityWarn = String . format ( "ES version(%s) is not compatible with the recommendation(%s)" , esVersion . toString ( ) , RECOMMENDED_ES_VERSION . toString ( ) ) ; LOGGER . warn ( esVersionCompatibilityWarn ) ; return CheckResult . builder ( ) . checkName ( checkName ( ) ) . resultType ( ResultType . BAD ) . description ( esVersionCompatibilityWarn ) . build ( ) ; } catch ( IOException e ) { LOGGER . error ( VERSION_CHECKING_ERROR_MESSAGE , e ) ; return CheckResult . builder ( ) . checkName ( checkName ( ) ) . resultType ( ResultType . BAD ) . description ( VERSION_CHECKING_ERROR_MESSAGE + ": " + e . getMessage ( ) ) . build ( ) ; } }
public void test() { try { Cache cache = this . getCache ( ) ; this . releaseCachedObjects ( cache ) ; Map < String , Group > groups = groupDAO . loadGroups ( ) ; this . insertObjectsOnCache ( cache , groups ) ; } catch ( Throwable t ) { _logger . error ( "Error loading groups" , t ) ; throw new ApsSystemException ( "Error loading groups" , t ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { synchronizeProjectRoutes ( ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; handler . fail ( 400 , "Could not initialize projects." ) ; } }
private QoSInterDirectPingMeasurementResponseDTO getInterDirectPingMeasurementFromQoSMonitor ( final CloudSystemFormDTO request ) { logger . debug ( "getInterDirectPingMeasurementFromQoSMonitor started..." ) ; final QoSInterDirectPingMeasurementResponseDTO measurement = orchestratorDriver . getInterDirectPingMeasurement ( request ) ; code_block = IfStatement ; return measurement ; }
public void test() { if ( ! ( uncastResult instanceof ExecutablePlugin ) ) { log . error ( "Can't get plugin descriptor from " + uncastResult . getClass ( ) . getName ( ) ) ; return null ; } }
@ Override public void transform ( Message message , DataType from , DataType to ) throws Exception { LOG . info ( "Transform: XOrder" ) ; assertEquals ( "name=XOrder" , message . getBody ( ) ) ; message . setBody ( new XOrder ( ) ) ; }
@ Test public void testHandleGetRequest ( ) { String jsonResponse = JsonLoader . loadJson ( DomainHelper . getRestUrlV2 ( ) + "/content/words" ) ; logger . info ( "jsonResponse: " + jsonResponse ) ; JSONArray wordsJSONArray = new JSONArray ( jsonResponse ) ; logger . info ( "wordsJSONArray.length(): " + wordsJSONArray . length ( ) ) ; assertThat ( wordsJSONArray . length ( ) > 0 , is ( true ) ) ; JSONObject wordJsonObject = wordsJSONArray . getJSONObject ( 0 ) ; assertThat ( wordJsonObject . getLong ( "id" ) , not ( nullValue ( ) ) ) ; assertThat ( wordJsonObject . getString ( "text" ) , not ( nullValue ( ) ) ) ; }
@ Test public void testHandleGetRequest ( ) { String jsonResponse = JsonLoader . loadJson ( DomainHelper . getRestUrlV2 ( ) + "/content/words" ) ; logger . info ( "jsonResponse: " + jsonResponse ) ; JSONArray wordsJSONArray = new JSONArray ( jsonResponse ) ; logger . info ( "wordsJSONArray.length(): " + wordsJSONArray . length ( ) ) ; assertThat ( wordsJSONArray . length ( ) > 0 , is ( true ) ) ; JSONObject wordJsonObject = wordsJSONArray . getJSONObject ( 0 ) ; assertThat ( wordJsonObject . getLong ( "id" ) , not ( nullValue ( ) ) ) ; assertThat ( wordJsonObject . getString ( "text" ) , not ( nullValue ( ) ) ) ; }
protected < T > ServiceResponse < T > makeBackwardCompatibleHttpPostRequestAndCreateServiceResponse ( String uri , String body , Class < T > resultType ) { logger . debug ( "Making back HTTP post request" ) ; KieServerHttpRequest request = newRequest ( uri ) . body ( body ) . post ( ) ; KieServerHttpResponse response = request . response ( ) ; owner . setConversationId ( response . header ( KieServerConstants . KIE_CONVERSATION_ID_TYPE_HEADER ) ) ; code_block = IfStatement ; }
public void test() { if ( pin == null ) { logger . debug ( "Pin not found." ) ; return null ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( depositSet . get ( uuid ) . contains ( CleanupDepositJob . class . getName ( ) ) ) { depositStatusFactory . setState ( uuid , DepositState . finished ) ; } else { LOG . warn ( "Could not list cleanup job, uuid: {}" , uuid ) ; } }
public void test() { if ( depositSet . get ( uuid ) . contains ( CleanupDepositJob . class . getName ( ) ) ) { depositStatusFactory . setState ( uuid , DepositState . finished ) ; } else { LOG . warn ( "Could not list cleanup job, uuid: {}" , uuid ) ; } }
public void test() { if ( publisher == null ) { LOG . warn ( "Publisher is null." ) ; return ; } }
public void test() { try { dataCollector . collectData ( ) ; logger . debug ( "Analytics event for failure event is published." ) ; } catch ( AnalyticsException e ) { logger . warn ( "Analytics event for failure event publish: {}" , e . getMessage ( ) ) ; } }
public void test() { try { IsotopicDistribution lCalc = new IsotopicDistribution ( 60 , 13 , 86 , 13 , 2 ) ; Assert . assertEquals ( 0.39350045799282984 , lCalc . getPercMax ( ) [ 2 ] , 0 ) ; Assert . assertEquals ( 0.16628993915006032 , lCalc . getPercTot ( ) [ 2 ] , 0 ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; fail ( e . getMessage ( ) ) ; } }
public void test() { if ( ! destMd5 . equals ( srcMd5 ) ) { LOG . info ( "MD5 checksum not changed: {}" , destMd5 ) ; return true ; } }
public void test() { { LOGGER . debug ( "check security profile object {}" , accessContractDto ) ; final boolean exist = securityProfileExternalService . check ( accessContractDto ) ; return RestUtils . buildBooleanResponse ( exist ) ; } }
public void test() { try { loggingCallback . awaitCompletion ( 3 , TimeUnit . SECONDS ) ; } catch ( InterruptedException e ) { logger . warn ( "Interrupted while waiting for logging callback" ) ; Thread . currentThread ( ) . interrupt ( ) ; } }
@ Override public void trace ( Marker marker , String msg ) { traceMessages . add ( new LogMessage ( marker , msg , null ) ) ; logger . trace ( marker , msg ) ; }
public void test() { try { loader = loaderClass . newInstance ( ) ; } catch ( Exception e ) { LOG . error ( "Failed to create {}" , Conf . class . getPackage ( ) . getName ( ) , e ) ; throw new RuntimeException ( "Failed to create " + Conf . class . getPackage ( ) . getName ( ) + " to " + loaderClass . getName ( ) , e ) ; } }
public void test() { try ( Session session = modelDBHibernateUtil . getSessionFactory ( ) . openSession ( ) ) { ExperimentRunEntity experimentRunEntity = session . load ( ExperimentRunEntity . class , experimentRunId , LockMode . PESSIMISTIC_WRITE ) ; experimentRunEntity . setParent_id ( parentExperimentRunId ) ; long currentTimestamp = Calendar . getInstance ( ) . getTimeInMillis ( ) ; experimentRunEntity . setDate_updated ( currentTimestamp ) ; Transaction transaction = session . beginTransaction ( ) ; session . update ( experimentRunEntity ) ; transaction . commit ( ) ; LOGGER . debug ( "Packet ended successfully" ) ; } catch ( Exception ex ) { code_block = IfStatement ; } }
public void test() { try { code_block = ForStatement ; prepareFinalExtractedDataList ( responceDataList , responceFinalDataList , responceData ) ; } catch ( Exception e ) { LOGGER . error ( "Recieved Message Exception." , e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( FieldException e ) { logger . warn ( "got field exception handling link records {}" , e . getMessage ( ) ) ; } catch ( IOException e ) { logger . debug ( "got IO exception handling link records {}" , e . getMessage ( ) ) ; } catch ( IllegalStateException e ) { logger . debug ( "got exception requesting link records {}" , e . getMessage ( ) ) ; } catch ( InvalidMessageTypeException e ) { logger . warn ( "invalid message " , e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( FieldException e ) { logger . debug ( "bad field handling link records {}" , e . getMessage ( ) ) ; } catch ( IOException e ) { logger . warn ( "error reading link records {}" , e . getMessage ( ) ) ; } catch ( IllegalStateException e ) { logger . debug ( "got exception requesting link records {}" , e . getMessage ( ) ) ; } catch ( InvalidMessageTypeException e ) { logger . warn ( "invalid message " , e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( FieldException e ) { logger . debug ( "bad field handling link records {}" , e . getMessage ( ) ) ; } catch ( IOException e ) { logger . debug ( "got IO exception handling link records {}" , e . getMessage ( ) ) ; } catch ( IllegalStateException e ) { logger . debug ( "error handling link records {}" , e . getMessage ( ) ) ; } catch ( InvalidMessageTypeException e ) { logger . warn ( "invalid message " , e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( FieldException e ) { logger . debug ( "bad field handling link records {}" , e . getMessage ( ) ) ; } catch ( IOException e ) { logger . debug ( "got IO exception handling link records {}" , e . getMessage ( ) ) ; } catch ( IllegalStateException e ) { logger . debug ( "got exception requesting link records {}" , e . getMessage ( ) ) ; } catch ( InvalidMessageTypeException e ) { logger . debug ( "invalid message requesting link records {}" , e . getMessage ( ) ) ; } }
private void reloadAllDataModel ( ) throws IOException { logger . debug ( "Reloading all data model..." ) ; ResourceStore store = getStore ( ) ; dataModelDescMap . clear ( ) ; List < String > paths = store . collectResourceRecursively ( ResourceStore . DATA_MODEL_DESC_RESOURCE_ROOT , MetadataConstants . FILE_SURFIX ) ; code_block = ForStatement ; logger . debug ( "Loaded " + dataModelDescMap . size ( ) + " DataModel(s)" ) ; }
public void test() { try { reloadDataModelDescAt ( path ) ; } catch ( IllegalStateException e ) { logger . info ( "Reload data model '{}' not found" , path ) ; continue ; } }
public void test() { if ( log . isTraceEnable ( ) ) { log . info ( this , "ThreadAnalysisQueryWorker stopped" ) ; } }
public void test() { if ( log . isDebugEnable ( ) ) { log . debug ( this , "unsubscribe procedure" ) ; } }
public void test() { if ( "AWS" . equalsIgnoreCase ( detectInstance ) ) { logger . info ( "Detect instance set toAWS, trying to determine AWS instance ID" ) ; result = getLocalInstanceId ( "AWS" , "http://169.254.169.254/latest/meta-data/instance-id" , null ) ; code_block = IfStatement ; } else-if ( "GCE" . equalsIgnoreCase ( detectInstance ) ) { logger . info ( "Detect instance set to GCE, trying to determine GCE instance ID" ) ; result = getLocalInstanceId ( "GCE" , "http://metadata/computeMetadata/v1/instance/id" , ImmutableMap . of ( "X-Google-Metadata-Request" , "True" ) ) ; code_block = IfStatement ; } else { result = null ; logger . info ( "No source instance ID passed, and not set to detect, sending metrics without an instance ID" ) ; } }
public void test() { if ( result != null ) { } else { LOGGER . warn ( "Unable to delete temporary file: " + file . getAbsolutePath ( ) ) ; } }
public void test() { if ( "AWS" . equalsIgnoreCase ( detectInstance ) ) { logger . info ( "Detect instance set to AWS, trying to determine AWS instance ID" ) ; result = getLocalInstanceId ( "AWS" , "http://169.254.169.254/latest/meta-data/instance-id" , null ) ; code_block = IfStatement ; } else-if ( "GCE" . equalsIgnoreCase ( detectInstance ) ) { logger . info ( "Detect instance set to GCE, trying to determine GCE instance ID" ) ; result = getLocalInstanceId ( "GCE" , "http://metadata/computeMetadata/v1/instance/id" , ImmutableMap . of ( "X-Google-Metadata-Request" , "True" ) ) ; code_block = IfStatement ; } else { result = null ; logger . info ( "No source instance ID passed, and not set to detect, sending metrics without an instance ID" ) ; } }
public void test() { if ( result == null ) { LOGGER . error ( "Unable to retrieve metadata from " + source ) ; } }
public void test() { if ( "AWS" . equalsIgnoreCase ( detectInstance ) ) { logger . info ( "Detect instance set to AWS, trying to determine AWS instance ID" ) ; result = getLocalInstanceId ( "AWS" , "http://169.254.169.254/latest/meta-data/instance-id" , null ) ; code_block = IfStatement ; } else-if ( "GCE" . equalsIgnoreCase ( detectInstance ) ) { logger . info ( "Detect instance set to GCE, trying to determine GCE instance ID" ) ; result = getLocalInstanceId ( "GCE" , "http://metadata/computeMetadata/v1/instance/id" , ImmutableMap . of ( "X-Google-Metadata-Request" , "True" ) ) ; code_block = IfStatement ; } else { logger . info ( "Detect instance set to null" ) ; result = null ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { try { ByteArrayOutputStream out = new ByteArrayOutputStream ( ) ; ObjectOutputStream objectOut = new ObjectOutputStream ( out ) ; objectOut . writeObject ( elements . toArray ( new Object [ elements . size ( ) ] ) ) ; transfer . doJavaToNative ( out . toByteArray ( ) , transferData ) ; } catch ( IOException e ) { log . error ( e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( CollectionUtils . isNotEmpty ( entities ) ) { AtlasEntityWithExtInfo getByGuidResponse = atlasClientV2 . getEntityByGuid ( entities . get ( 0 ) . getGuid ( ) ) ; ret = getByGuidResponse ; LOG . info ( "Got entity: name={} " , entity . toString ( ) ) ; } else { LOG . info ( "Entity: name={} " , entity . toString ( ) + " not updated as it is unchanged from Atlas" ) ; ret = entity ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( SiteNavigationMenuServiceUtil . class , "addSiteNavigationMenu" , _addSiteNavigationMenuParameterTypes0 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , name , type , auto , serviceContext ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . site . navigation . model . SiteNavigationMenu ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public List < AttendeeRequest > getAttendeeRequests ( Profile user , boolean includeAckd ) { log . info ( "getAttendeeRequests: user={}, includeAckd={}" , user , includeAckd ) ; List < AttendeeRequest > list = new ArrayList < > ( ) ; code_block = IfStatement ; log . info ( "getAttendeeRequests: found requests: " + list . size ( ) ) ; return list ; }
public void test() { try { CQSHandler . deleteMessage ( queueUrl , receiptHandle ) ; logger . debug ( "event=deleting_publish_job_from_cqs message_id=" + message . getMessageId ( ) + " queue_url=" + queueUrl + " receipt_handle=" + receiptHandle ) ; } catch ( Exception ex ) { logger . error ( "event=deleting_publish_job_from_cqs message_id=" + message . getMessageId ( ) + " queue_url=" + queueUrl + " receipt_handle=" + receiptHandle , ex ) ; } }
public void test() { try { Object [ ] array = ( Object [ ] ) pgArray . getArray ( ) ; List < String > ltrees = new ArrayList < > ( array . length ) ; code_block = ForStatement ; r . deliver ( ltrees ) ; } catch ( SQLException e ) { log . error ( e . getMessage ( ) , e ) ; } }
@ PostConstruct void init ( ) throws IOException { final Properties properties = new Properties ( ) ; final var velocityLog = fedoraPropsConfig . getVelocityLog ( ) . toString ( ) ; LOGGER . debug ( "Setting Velocity runtime log: {}" , velocityLog ) ; properties . setProperty ( "runtime.log" , velocityLog ) ; LOGGER . debug ( "Using Velocity configuration from {}" , propertiesUrl ) ; final URL propertiesUrl = getClass ( ) . getResource ( velocityPropertiesLocation ) ; LOGGER . debug ( "Using Velocity configuration from {}" , propertiesUrl ) ; code_block = TryStatement ;  velocity . init ( properties ) ; LOGGER . trace ( "Velocity engine initialized." ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { return Files . walk ( derivRoot ) . map ( MCRPath :: toMCRPath ) . filter ( p -> ! Files . isDirectory ( p ) ) . filter ( p -> ! p . equals ( derivRoot ) ) ; } catch ( IOException e ) { LOGGER . error ( "Could not load derivate {}: {}" , derivateId , e ) ; } catch ( SecurityException s ) { LOGGER . error ( "No access to starting file of derivate {}!" , derivateId , s ) ; } }
public void test() { try { return Files . walk ( derivRoot ) . map ( MCRPath :: toMCRPath ) . filter ( p -> ! Files . isDirectory ( p ) ) . filter ( p -> ! p . equals ( derivRoot ) ) ; } catch ( IOException e ) { LOGGER . error ( "I/O error while access the starting file of derivate {}!" , derivateId , e ) ; } catch ( SecurityException s ) { LOGGER . error ( "SecurityException while accessing the starting file of derivate {}!" , derivateId , s ) ; } }
public void test() { try { deliverBatch ( batch ) ; } catch ( Throwable e ) { log . error ( "Unexpected error on deliverBatch" , e ) ; } }
public void test() { if ( convertedModelFs . exists ( convertedModelPath ) ) { LOG . info ( "Removing converted model '{}'" , convertedModelPath ) ; convertedModelFs . delete ( convertedModelPath , true ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( registration . isLIMSAdmin ( ) ) { Log . debug ( "Skipping permissions admin on Lane access." ) ; return true ; } }
public void test() { if ( ! consideredBefore ) { considered . add ( this . getSwAccession ( ) ) ; Log . debug ( "Checking permissions for experiment object " + swAccession ) ; } else { Log . debug ( "Skipping permissions for experiment object " + swAccession + " , checked before" ) ; return true ; } }
public void test() { if ( ! consideredBefore ) { considered . add ( this . getSwAccession ( ) ) ; Log . debug ( "Checking permissions for experiment object " + swAccession ) ; } else { Log . debug ( "Skipping permissions for experiment object " + swAccession ) ; return true ; } }
public void test() { if ( registration . equals ( this . owner ) || registration . isLIMSAdmin ( ) ) { LOGGER . warn ( "Modifying Orphan Experiment: " + this . getName ( ) ) ; hasPermission = true ; } else-if ( owner == null ) { LOGGER . warn ( "Experiment has no owner! Modifying Orphan Experiment: " + this . getName ( ) ) ; hasPermission = true ; } else { LOGGER . warn ( "Not modifying Orphan Experiment: " + this . getName ( ) ) ; hasPermission = false ; } }
public void test() { if ( registration . equals ( this . owner ) || registration . isLIMSAdmin ( ) ) { LOGGER . warn ( "Modifying Orphan Experiment: " + this . getName ( ) ) ; hasPermission = true ; } else-if ( owner == null ) { LOGGER . warn ( "Orphan Experiment: " + this . getName ( ) ) ; hasPermission = true ; } else { LOGGER . warn ( "Not modifying Orphan Experiment: " + this . getName ( ) ) ; hasPermission = false ; } }
public void test() { if ( registration . equals ( this . owner ) || registration . isLIMSAdmin ( ) ) { LOGGER . warn ( "Modifying Orphan Experiment: " + this . getName ( ) ) ; hasPermission = true ; } else-if ( owner == null ) { LOGGER . warn ( "Experiment has no owner! Modifying Orphan Experiment: " + this . getName ( ) ) ; hasPermission = true ; } else { LOGGER . warn ( "Experiment hasPermission! Not modifying Orphan Experiment: " + this . getName ( ) ) ; hasPermission = false ; } }
public void test() { try { sysTable . startBackupExclusiveOperation ( ) ; deleteSessionStarted = true ; } catch ( IOException e ) { logger . error ( "Failed to start backup exclusive operation" , e ) ; return - 1 ; } }
public void test() { if ( ! BackupSystemTable . snapshotExists ( conn ) ) { BackupSystemTable . snapshot ( conn ) ; } else { BackupSystemTable . snapshot ( conn ) ; } }
public void test() { if ( BackupSystemTable . snapshotExists ( conn ) ) { LOG . debug ( "Restoring backup system table" ) ; BackupSystemTable . restoreFromSnapshot ( conn ) ; BackupSystemTable . deleteSnapshot ( conn ) ; throw e ; } else { LOG . warn ( "Delete operation succeeded, there were some errors: " , e ) ; } }
public void test() { if ( BackupSystemTable . snapshotExists ( conn ) ) { BackupSystemTable . restoreFromSnapshot ( conn ) ; BackupSystemTable . deleteSnapshot ( conn ) ; LOG . error ( "Delete operation failed, please run backup repair utility to restore " + "backup system integrity" , e ) ; throw e ; } else { LOG . warn ( "Delete operation failed, please run backup repair utility" , e ) ; } }
private void loadWeightDataForOneDay ( UpdateInfo updateInfo , String formattedDate ) throws Exception { String json = getWeightData ( updateInfo , formattedDate ) ; String fatJson = getBodyFatData ( updateInfo , formattedDate ) ; logger . info ( "Load weight data: " + json ) ; JSONObject jsonWeight = JSONObject . fromObject ( json ) ; JSONObject jsonFat = JSONObject . fromObject ( fatJson ) ; json = mergeWeightInfos ( jsonWeight , jsonFat ) ; apiDataService . eraseApiData ( updateInfo . apiKey , weightOT , Arrays . asList ( formattedDate ) ) ; code_block = IfStatement ; }
public void test() { try { responseJson = PacHttpUtils . doHttpPost ( urlToQuery . toString ( ) , requestBody . toString ( ) ) ; } catch ( Exception e ) { LOGGER . error ( "ERROR" , e ) ; } }
public void test() { for ( String duplicatedVariableId : duplicatedVariableIds ) { List < Variable > duplicatedVariables = variablesById . get ( duplicatedVariableId ) ; logger . error ( "Duplicate variable " + duplicatedVariables . getValue ( ) ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
@ GetMapping ( "/HYstrix" ) public Object hystrixPluginFallback ( ) { logger . warn ( "HystrixPlugin.fallback called" ) ; return DefaultShenyuEntity . error ( ShenyuResultEnum . HYSTRIX_PLUGIN_FALLBACK . getCode ( ) , ShenyuResultEnum . HYSTRIX_PLUGIN_FALLBACK . getMsg ( ) , null ) ; }
public void test() { try { newAssignment . findOrCreateContainer ( AssignmentType . F_POLICY_RULE ) ; assignmentType . setPolicyRule ( new PolicyRuleType ( ) ) ; } catch ( SchemaException e ) { LOGGER . error ( "Cannot create policyRule assignment." ) ; getSession ( ) . error ( "Cannot create policyRule assignment." ) ; target . add ( getPageBase ( ) . getFeedbackPanel ( ) ) ; return ; } }
protected ProcessBuilder createProcessBuilder ( List < String > command , EnvironmentDescriptor env ) { LOG . info ( "Processing {}" , command ) ; ProcessBuilder processBuilder = new ProcessBuilder ( command ) ; processBuilder . directory ( new File ( env . getWorkingDirectory ( ) ) ) ; processBuilder . environment ( ) . putAll ( env . getParameters ( ) ) ; processBuilder . redirectErrorStream ( true ) ; return processBuilder ; }
public void test() { try { MethodKey methodKey = new MethodKey ( AssetTagServiceUtil . class , "getGroupTagsCount" , _getGroupTagsCountParameterTypes6 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( ( Integer ) returnObj ) . intValue ( ) ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( ! md5 . equals ( e . getAttributeValue ( "md5" ) ) ) { log . warn ( "md5" , md5 ) ; e . setAttribute ( "md5" , md5 ) ; } }
private List < Recommendation > createMetroPointRecommendations ( VirtualArray srcVarray , List < VirtualArray > tgtVarrays , VirtualPool srcVpool , VirtualArray haVarray , VirtualPool haVpool , Project project , VirtualPoolCapabilityValuesWrapper capabilities , List < StoragePool > candidatePrimaryPools , List < StoragePool > candidateSecondaryPools , Volume vpoolChangeVolume ) { List < Recommendation > recommendations = new ArrayList < Recommendation > ( ) ; RPProtectionRecommendation rpProtectionRecommendaton = null ; Map < VirtualArray , List < StoragePool > > tgtVarrayStoragePoolsMap = getVplexTargetMatchingPools ( tgtVarrays , srcVpool , project , capabilities , vpoolChangeVolume ) ; rpProtectionRecommendaton = createRPProtectionRecommendationForMetroPoint ( srcVarray , tgtVarrays , srcVpool , haVarray , haVpool , capabilities , candidatePrimaryPools , candidateSecondaryPools , tgtVarrayStoragePoolsMap , vpoolChangeVolume , project ) ; LOGGER . debug ( "PProtectionRecommendations : {}" , rpProtectionRecommendaton ) ; recommendations . add ( rpProtectionRecommendaton ) ; return recommendations ; }
public void test() { if ( logger . isTraceEnabled ( LogMarker . SERIALIZER_VERBOSE ) ) { logger . trace ( LogMarker . SERIALIZER_VERBOSE , "Serialize {}" , this ) ; } }
public void test() { try { SessionState ss = ( ( JetspeedRunData ) rundata ) . getPortletSessionState ( peid ) ; ControllerState state = ( ControllerState ) ss . getAttribute ( "state" ) ; state . recycle ( ) ; ss . removeAttribute ( "state" ) ; ss . clear ( ) ; } catch ( Exception e ) { LOG . warn ( "Error retrieving controller state" , e ) ; } }
public void test() { if ( submittedRequest == null ) { submitTMRequest ( action ) ; } else { logger . info ( "Temporary Request already submitted" ) ; } }
@ Override protected void doGet ( @ Nullable HttpServletRequest req , @ Nullable HttpServletResponse resp ) throws ServletException , IOException { final String servletBaseURL = req . getRequestURL ( ) . toString ( ) ; final Map < String , String > replaceMap = new HashMap < > ( ) ; handleSpotifyRedirect ( replaceMap , servletBaseURL , req . getQueryString ( ) ) ; resp . setContentType ( CONTENT_TYPE ) ; replaceMap . put ( KEY_REDIRECT_URI , servletBaseURL ) ; replaceMap . put ( KEY_PLAYERS , formatPlayers ( playerTemplate , servletBaseURL ) ) ; resp . getWriter ( ) . append ( replaceKeysFromMap ( indexTemplate , replaceMap ) ) ; resp . getWriter ( ) . close ( ) ; logger . info ( "Redirected to " + servletBaseURL ) ; }
public void test() { try { URL url = buildUrl ( endpoint , param , ips . get ( index ) , ports . get ( index ) ) ; HttpURLConnection conn = openConnection ( url ) ; conn . setConnectTimeout ( DEFAULT_CONNECTION_TIMEOUT ) ; code_block = IfStatement ; conn . setRequestMethod ( method ) ; conn . setUseCaches ( false ) ; conn . setDoInput ( true ) ; code_block = ForStatement ; } catch ( URISyntaxException | IOException | InterruptedException e ) { logger . error ( e . getMessage ( ) ) ; logger . error ( e ) ; } }
public void test() { try { code_block = IfStatement ; int code = conn . getResponseCode ( ) ; code_block = IfStatement ; logger . error ( "code: %d, msg: %s" , code , conn . getResponseMessage ( ) ) ; code_block = IfStatement ; break ; } catch ( IOException e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { logger . info ( "connecting to %s:%s ..." , ips . get ( index ) , ports . get ( index ) ) ; URL url = buildUrl ( endpoint , param , ips . get ( index ) , ports . get ( index ) ) ; HttpURLConnection conn = openConnection ( url ) ; conn . setConnectTimeout ( DEFAULT_CONNECTION_TIMEOUT ) ; code_block = IfStatement ; conn . setRequestMethod ( method ) ; conn . setUseCaches ( false ) ; conn . setDoInput ( true ) ; code_block = ForStatement ; } catch ( URISyntaxException | IOException | InterruptedException e ) { logger . error ( e . getMessage ( ) ) ; } }
@ Override public void perform ( ) throws Exception { ServerName master = cluster . getClusterMetrics ( ) . getMasterName ( ) ; LOG . info ( "Restarting: {}" , master ) ; restartMaster ( master , sleepTime ) ; }
public void test() { try { cleanUpExpiredTransactions ( ) ; } catch ( Throwable t ) { log . warn ( "Unable to clean up expired transactions." , t ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { final Templates template = factory . newTemplates ( new StreamSource ( stylesheet ) ) ; handler = factory . newTransformerHandler ( template ) ; } catch ( final TransformerConfigurationException e ) { LOG . debug ( e . getMessage ( ) , e ) ; throw new TriggerException ( e . getMessage ( ) , e ) ; } }
public void test() { if ( application . isBundled ( ) ) { String redirectPath = application . getBaseUrl ( ) + "/" + pageName ; logger . debug ( "Redirecting to {}" , redirectPath ) ; response . sendRedirect ( redirectPath ) ; return ; } }
public void test() { if ( application . getActivities ( ) != null && application . getActivities ( ) . getDhis ( ) != null && "*" . equals ( application . getActivities ( ) . getDhis ( ) . getHref ( ) ) ) { String contextPath = ContextUtils . getContextPath ( request ) ; log . info ( String . format ( "Manifest context path: '%s'" , contextPath ) ) ; application . getActivities ( ) . getDhis ( ) . setHref ( contextPath ) ; } }
public void test() { if ( nextLoadTime > 0 ) { LOG . info ( "Loading lookup join cache from: {}" , nextLoadTime ) ; } else { LOG . info ( "Populating lookup join cache" ) ; } }
public void test() { if ( nextLoadTime > 0 ) { LOG . info ( "Lookup join cache has expired after {} minute(s), reloading" , reloadInterval . toMinutes ( ) ) ; } else { LOG . debug ( "No lookup join cache is available." ) ; } }
public void test() { if ( nextLoadTime == null ) { code_block = TryStatement ;  } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { if ( ! namedclsDir . exists ( ) ) { logger . error ( "Cannot find namedcls directory" ) ; return ; } }
public void test() { if ( fileLocation == null ) { LOGGER . info ( "Property not set, using default" ) ; return props ; } }
@ Test public void testFilter ( ) { String xml = "<wfs:GetFeature " + "service=\"WFS\" " + "version=\"1.1.0\" " + "outputFormat=\"gml32\" " + "xmlns:ogc=\"http://www.opengis.net/ogc\" " + "xmlns:wfs=\"http://www.opengis.net/wfs\" " + "xmlns:gml=\"http://www.opengis.net/gml/3.2\" " + "xmlns:wml2dr=\"http://www.opengis.net/waterml/DR/2.0\" " + "xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" " + "xsi:schemaLocation=\"" + "http://www.opengis.net/wfs http://schemas.opengis.net/wfs/1.1.0/wfs.xsd" + "\"" + ">" + "<wfs:Query typeName=\"wml2dr:MeasurementTimeseriesDomainRange\">" + "    <ogc:Filter>" + "       <ogc:PropertyIsLike wildCard=\"*\" singleChar=\"\" escapeChar=\"\\\">" + "            <ogc:PropertyName>wml2dr:MeasurementTimeseriesDomainRange/gml:rangeSet/gml:QuantityList</ogc:PropertyName>" + "            <ogc:Literal>*16.2*</ogc:Literal>" + "       </ogc:PropertyIsLike>" + "    </ogc:Filter>" + "</wfs:Query> " + "</wfs:GetFeature>" ; validate ( xml ) ; Document doc = postAsDOM ( "wfs" , xml ) ; LOGGER . info ( "WFS GetFeature response:\n" + prettyString ( doc ) ) ; assertEquals ( "wfs:FeatureCollection" , doc . getDocumentElement ( ) . getNodeName ( ) ) ; assertXpathEvaluatesTo ( "1" , "/wfs
protected final boolean isLayerizedNetwork ( String networkId ) { log . debug ( "" ) ; String connType = getConnectionType ( networkId ) ; code_block = IfStatement ; return false ; }
public void test() { try { code_block = IfStatement ; } catch ( SQLException e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public static String runAndWaitArray ( final String [ ] command ) { log . trace ( "Running command: " + String . join ( command ) ) ; String result = runAndWaitNoLog ( command ) ; log . trace ( "Result:" + result ) ; return result ; }
public static String runAndWaitArray ( final String [ ] command ) { log . trace ( "Running command on the shell: {}" , Arrays . toString ( command ) ) ; String result = runAndWaitNoLog ( command ) ; log . trace ( "Command result: {}" , result ) ; return result ; }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceOrderItemServiceUtil . class , "updateCommerceOrderItemUnitPrice" , _updateCommerceOrderItemUnitPriceParameterTypes29 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , commerceOrderItemId , quantity , unitPrice ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . commerce . model . CommerceOrderItem ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( ! type . equals ( DATE ) && ! type . equals ( TIMESTAMP ) ) { logger . warn ( "The value {} is invalid: {}" , value , type ) ; return false ; } }
private void handleControlMessage ( OdeControlData controlData ) { String infoMsg = controlData . toJson ( false ) ; logger . info ( "control message: " + infoMsg ) ; logger . info ( infoMsg ) ; }
public List < Template > findAllTemplates ( ) { log . debug ( "findAllTemplates" ) ; return templateRepository . findAll ( ) ; }
public void test() { try { KieServerConfigDto serverConfigDto = kieConfigService . getConfig ( configCode ) ; KieBpmConfig bpmConfig = kieConfigService . buildConfig ( serverConfigDto ) ; cases = caseManager . getCasesDefinitions ( bpmConfig , deploymentUnit ) ; } catch ( Exception e ) { logger . error ( "Failed to retrieve any configurations for config {}" , configCode , e ) ; } }
@ Test public void stringImageInputStream ( ) { LOGGER . info ( "Testing capabilities of URLImageInputStreamSpi: SUCCESS!!!" ) ; final String inURLToFile = TestData . getResource ( this , "a.txt" ) . toString ( ) ; ImageInputStream instream ; code_block = TryStatement ;  Assert . assertNotNull ( "Unable to get an StringImageInputStreamSpi from a URL pointing to a File" , instream ) ; code_block = TryStatement ;  Assert . assertNotNull ( "Unable to get an URLImageInputStreamSpi from a URL pointing to an http page" , instream ) ; code_block = TryStatement ;  LOGGER . info ( "Testing capabilities of URLImageInputStreamSpi: SUCCESS!!!" ) ; }
public void test() { if ( startupLogger != null ) { startupLogger . info ( "shutting down..." ) ; } }
public void test() { if ( ! thread . isDaemon ( ) && thread != Thread . currentThread ( ) && stackTrace . length != 0 ) { startupLogger . info ( "Found non-daemon thread after shutdown: {}" , thread . getName ( ) ) ; } }
public void test() { if ( startupLogger == null ) { t . printStackTrace ( ) ; } else { startupLogger . error ( "error during shutdown: {}" , t . getMessage ( ) , t ) ; } }
@ Override public synchronized void start ( ) { Assert . state ( m_worker == null , "The fiber has already run or is running" ) ; m_worker = new Thread ( this , getName ( ) ) ; m_worker . start ( ) ; m_status = STARTING ; LOG . debug ( "Started " + getName ( ) ) ; }
@ Override public boolean visitEnter ( final HierComposite node ) { log . info ( "visitEnter()" ) ; code_block = IfStatement ; log . debug ( "checking control rod" ) ; aborted = checkControlRod ( node ) ; code_block = IfStatement ; log . debug ( "checking aborted" ) ; code_block = TryStatement ;  }
public void test() { try { List < MetaDataAndDomainData > metadata = collectionAO . findMetadataValuesForCollection ( node . getAbsolutePath ( ) , 0 ) ; metadataRollup . getMetadata ( ) . push ( metadata ) ; code_block = IfStatement ; boolean shortCircuit = visitEnterWithMetadata ( node , metadataRollup ) ; log . info ( "visuit {}" , shortCircuit ) ; return shortCircuit ; } catch ( JargonException | JargonQueryException e ) { log . error ( "error in obtaining metadata" , e ) ; throw new JargonRuntimeException ( "error getting metadata" , e ) ; } }
public void test() { try { List < MetaDataAndDomainData > metadata = collectionAO . findMetadataValuesForCollection ( node . getAbsolutePath ( ) , 0 ) ; metadataRollup . getMetadata ( ) . push ( metadata ) ; log . info ( "pushed metadata in the stack...now filter and then delegate to visitEnterWithMetadata() in the impl class to make any determinations" ) ; code_block = IfStatement ; boolean shortCircuit = visitEnterWithMetadata ( node , metadataRollup ) ; return shortCircuit ; } catch ( JargonException | JargonQueryException e ) { log . error ( "error getting metadata" , e ) ; throw new JargonRuntimeException ( "error getting metadata" , e ) ; } }
public void test() { if ( entrySize > maxSaneEntrySize ) { LOG . error ( "Size of size " + maxSaneEntrySize + " is greater than max SaneEntrySize" ) ; } }
public void test() { try { mu = mp . getUsage ( ) ; } catch ( IllegalArgumentException ex ) { continue ; } catch ( InternalError ie ) { log . warn ( "Internal error: " + ie . getMessage ( ) ) ; s . close ( ) ; it . remove ( ) ; reInitPools = true ; continue ; } }
public void test() { if ( totalItemRead < expectedNumberOfDocuments ) { logger . debug ( "Did not receive expected number ofDocuments" ) ; code_block = TryStatement ;  continue ; } else { logger . info ( "Read {} items from {}" , totalItemRead , this . client . getReadEndpoint ( ) ) ; break ; } }
public void test() { try { repositoryInfo = m_access . describeRepository ( context ) ; code_block = IfStatement ; } catch ( AuthzException ae ) { throw ae ; } catch ( Throwable th ) { String msg = "Error describing repository" ; logger . error ( msg , th ) ; throw new GeneralException ( msg , th ) ; } }
public void test() { try { loggedInUserId = AuthHelper . getGuestId ( ) ; accessAllowed = isOwnerOrAdmin ( uid ) ; code_block = IfStatement ; } catch ( Exception e ) { LOG . error ( "Error while checking authorization." , e ) ; } }
public void test() { try { photo = photoFetchStrategy . getPhoto ( ) ; } catch ( Exception e ) { final String message = "Exception while trying to get photo [" + photoFetchStrategy . getPhotoIdentifier ( ) + "]" ; LOG . error ( message , e ) ; return jsonResponseHelper . internalServerError ( message ) ; } }
public void test() { if ( photo == null ) { final String message = "Photo [" + photoFetchStrategy . getPhotoIdentifier ( ) + "] requested by user [" + loggedInUserId + "] not found" ; logger . debug ( message ) ; return jsonResponseHelper . notFound ( message ) ; } }
public void test() { try { etag = new EntityTag ( HashUtils . computeMd5Hash ( photo . getPhotoBytes ( ) ) ) ; final Response . ResponseBuilder responseBuilder = request . evaluatePreconditions ( etag ) ; code_block = IfStatement ; } catch ( NoSuchAlgorithmException e ) { etag = null ; logger . warn ( "Could not find etag for photo {}" , photo ) ; } }
public void test() { try { queue . poll ( ) . run ( ) ; } catch ( Throwable e ) { log . error ( e , "Task failed" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) && Iterables . size ( resultL ) > 1 ) { log . debug ( "Found multiple results for query " + query ) ; } }
public void test() { try { jobInstance = jobService . getJobInstance ( jobId ) ; } catch ( Exception e ) { logger . error ( e . getLocalizedMessage ( ) , e ) ; throw new InternalErrorException ( e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( DLFolderServiceUtil . class , "getFoldersAndFileEntriesAndFileShortcutsCount" , _getFoldersAndFileEntriesAndFileShortcutsCountParameterTypes23 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , folderId , mimeTypes , includeMountFolders , queryDefinition ) ; Object returnObj = null ; code_block = TryStatement ;  return ( ( Integer ) returnObj ) . intValue ( ) ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
private void writeSerializedPublicKeyLength ( SrpClientKeyExchangeMessage msg ) { appendInt ( msg . getPublicKeyLength ( ) . getValue ( ) , HandshakeByteLength . SRP_PUBLICKEY_LENGTH ) ; LOGGER . debug ( "SerializedPublicKeyLength: " + msg . getPublicKeyLength ( ) . getValue ( ) ) ; }
@ Override public void destroyNode ( String id ) { VirtualGuest guest = getNode ( id ) ; if ( guest == null ) return ; logger . info ( ">> destroying billing item %s" , id ) ; if ( guest . getBillingItemId ( ) == - 1 ) throw new IllegalStateException ( String . format ( "no billing item for guest(%s) so we cannot cancel the order" , id ) ) ; client . getVirtualGuestClient ( ) . cancelService ( guest . getBillingItemId ( ) ) ; }
@ Override public void debug ( String msg , Throwable thrown ) { logger . debug ( msg , thrown ) ; }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Refreshing." ) ; } }
public void test() { if ( null != currentContent ) { code_block = IfStatement ; currentContent . setLastEditor ( this . getCurrentUser ( ) . getUsername ( ) ) ; code_block = IfStatement ; String sessionParamName = ContentActionConstants . SESSION_PARAM_NAME_CURRENT_CONTENT_PREXIX + this . getContentOnSessionMarker ( ) ; this . getRequest ( ) . getSession ( ) . removeAttribute ( sessionParamName ) ; _logger . info ( "Salvato contenuto " + currentContent . getId ( ) + " - Descrizione: '" + currentContent . getDescription ( ) + "' - Utente: " + this . getCurrentUser ( ) . getUsername ( ) ) ; } else { _logger . info ( "No current content occured " + currentContent . getId ( ) + " - Descrizione: '" + currentContent . getDescription ( ) + "' - Utente: " + this . getCurrentUser ( ) . getUsername ( ) ) ; } }
public void test() { try { Content currentContent = this . getContent ( ) ; code_block = IfStatement ; } catch ( Exception e ) { _logger . error ( "error in trash" , e ) ; return FAILURE ; } }
@ Override @ Transactional ( readOnly = false ) public void setTimeZone ( long guestId , String date , String timeZone ) { logger . warn ( "setTimeZone(" + date + ") is not supported for session." ) ; }
protected void optimizeFile ( ) { final ElapsedTimer timer = new ElapsedTimer ( ) ; timesOptimized ++ ; IndexedDiskElementDescriptor [ ] defragList = null ; storageLock . writeLock ( ) . lock ( ) ; code_block = TryStatement ;  log . info ( "{0}: Initializing {}" , logCacheName , defragList ) ; long expectedNextPos = defragFile ( defragList , 0 ) ; storageLock . writeLock ( ) . lock ( ) ; code_block = TryStatement ;  log . info ( "{0}: Finished {1}, Optimization took {2}" , logCacheName , timesOptimized , timer . getElapsedTimeString ( ) ) ; }
public void test() { try { code_block = IfStatement ; dataFile . truncate ( expectedNextPos ) ; } catch ( final IOException e ) { LOG . warn ( "Failed to truncate file '{}'" , file , e ) ; } }
protected void optimizeFile ( ) { final ElapsedTimer timer = new ElapsedTimer ( ) ; timesOptimized ++ ; log . info ( "{0}: Beginning Optimization {1}" , logCacheName , timesOptimized ) ; IndexedDiskElementDescriptor [ ] defragList = null ; storageLock . writeLock ( ) . lock ( ) ; code_block = TryStatement ;  long expectedNextPos = defragFile ( defragList , 0 ) ; storageLock . writeLock ( ) . lock ( ) ; code_block = TryStatement ;  log . info ( "{0}: Finished Optimization" , logCacheName ) ; }
@ Override public void removeItemMetadata ( String name ) { LOGGER . debug ( "Removing metadata for item {}" , name ) ; getAll ( ) . stream ( ) . filter ( MetadataPredicates . ofItem ( name ) ) . map ( Metadata :: getUID ) . forEach ( this :: remove ) ; }
public void test() { if ( compssHome == null || compssHome . isEmpty ( ) ) { LOGGER . warn ( "WARN: COMPSS_HOME not defined, no compss will be defined" ) ; return ; } }
public void test() { try { Classpath . loadPath ( compssHome + ADAPTORS_REL_PATH , LOGGER ) ; } catch ( FileNotFoundException ex ) { LOGGER . debug ( "Cannot load ADAPTORS index" , ex ) ; } }
public void test() { try { view . callModelChanged ( ) ; } catch ( Exception e ) { String msg = "View [" + view . getViewName ( ) + "] caused an error while displaying new contents: " + e . getMessage ( ) ; LOG . warn ( msg , e ) ; setWarningMessage ( msg ) ; } }
public void test() { try { session . getWorkspace ( ) . copy ( srcPath , destPath ) ; return session . getNode ( destPath ) ; } catch ( AccessDeniedException e ) { log . debug ( "Access denied" , e ) ; throw new AccessControlException ( e . getMessage ( ) ) ; } catch ( RepositoryException e ) { throw new MetadataRepositoryException ( "Failed to copy source path: " + srcPath + " to destination path: " + destPath , e ) ; } }
public void test() { if ( IS_CLOSED_UPDATER . get ( this ) == TRUE ) { log . warn ( "[{}] Dispatcher is already closed. Closing consumer {}" , topic , consumer ) ; consumer . disconnect ( ) ; return ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( LOG . isInfoEnabled ( ) ) { LOG . info ( String . format ( "Call to '%s' on file '%s'" , uri . toString ( ) , file ) ) ; } }
public void test() { if ( LOG . isInfoEnabled ( ) ) { LOG . info ( String . format ( "Call to '%s' on file '%s'" , uri . toString ( ) , file ) ) ; } }
public void test() { if ( LOG . isInfoEnabled ( ) ) { LOG . info ( String . format ( "Call to '%s' on file '%s'" , uri . toString ( ) , file ) ) ; } }
public void test() { if ( ! response . getType ( ) . equals ( ServiceResponse . ResponseType . SUCCESS ) ) { logger . debug ( "Container {} failed to stop on server instance {} due to {}" , container . getUrl ( ) , container . getUrl ( ) , response . getMsg ( ) ) ; container . setStatus ( KieContainerStatus . FAILED ) ; } }
@ Override public void onReceive ( Double doubleOut , String stringOut ) { logger . info ( name . getMethodName ( ) + " - callback - got broadcast" ) ; code_block = IfStatement ; subscribeBroadcastWithMultiplePrimitiveParametersCallbackDone = true ; }
public void test() { if ( ! stringOut . equals ( "boom" ) || ! IltUtil . cmpDouble ( doubleOut , 1.1d ) ) { logger . info ( name . getMethodName ( ) + " - callback - invalid content" ) ; subscribeBroadcastWithMultiplePrimitiveParametersCallbackResult = false ; } else { logger . info ( name . getMethodName ( ) + " - callback - content OK" ) ; subscribeBroadcastWithMultiplePrimitiveParametersCallbackResult = true ; } }
public void test() { if ( ! stringOut . equals ( "boom" ) || ! IltUtil . cmpDouble ( doubleOut , 1.1d ) ) { logger . info ( name . getMethodName ( ) + " - callback - invalid content" ) ; subscribeBroadcastWithMultiplePrimitiveParametersCallbackResult = false ; } else { logger . info ( name . getMethodName ( ) + " - callback - content OK" ) ; subscribeBroadcastWithMultiplePrimitiveParametersCallbackResult = true ; } }
public void test() { if ( ! rs . getOwner ( ) . equals ( auth . getName ( ) ) ) { logger . warn ( "Unauthorized resource set request from wrong user; expected " + rs . getOwner ( ) + " got " + auth . getName ( ) ) ; m . addAttribute ( HttpCodeView . CODE , HttpStatus . FORBIDDEN ) ; return HttpCodeView . VIEWNAME ; } }
public void test() { if ( VeluxBindingConstants . SUPPORTED_THINGS_BINDING . contains ( thingTypeUID ) ) { resultHandler = createBindingHandler ( thing ) ; } else-if ( VeluxBindingConstants . SUPPORTED_THINGS_BRIDGE . contains ( thingTypeUID ) ) { resultHandler = createBridgeHandler ( thing ) ; } else-if ( VeluxBindingConstants . SUPPORTED_THINGS_ITEMS . contains ( thingTypeUID ) ) { resultHandler = createThingHandler ( thing ) ; } else { logger . debug ( "Unsupported operation type: {}" , thingTypeUID ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceOrderServiceUtil . class , "updateCommerceOrder" , _updateCommerceOrderParameterTypes37 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , externalReferenceCode , commerceOrderId , billingAddressId , shippingAddressId , commercePaymentMethodKey , commerceShippingMethodId , shippingOptionName , purchaseOrderNumber , subtotal , shippingAmount , total , subtotalWithTaxAmount , shippingWithTaxAmount , totalWithTaxAmount , advanceStatus , commerceContext ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . commerce . model . CommerceOrder ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { return listListenableFuture . get ( timeout , TimeUnit . MILLISECONDS ) ; } catch ( InterruptedException e ) { Thread . currentThread ( ) . interrupt ( ) ; } catch ( ExecutionException e ) { logger . info ( "ExecutionError {} {}" , name , e . getMessage ( ) ) ; } catch ( TimeoutException e ) { logger . info ( "Timeout {} {}" , name , e . getMessage ( ) ) ; } }
public void test() { try { return listListenableFuture . get ( timeout , TimeUnit . MILLISECONDS ) ; } catch ( InterruptedException e ) { Thread . currentThread ( ) . interrupt ( ) ; } catch ( ExecutionException e ) { logger . info ( "ExecutionError {} {}" , name , e . getMessage ( ) ) ; } catch ( TimeoutException e ) { logger . info ( "Timeout {} {}" , name , e . getMessage ( ) ) ; } }
public void test() { try { clientStack . push ( syncClientFactory . getSyncClient ( node , this ) ) ; NodeStatusManager . getINSTANCE ( ) . activate ( node ) ; } catch ( TTransportException e ) { nodeClientNumMap . computeIfPresent ( clusterNode , ( n , oldValue ) -> oldValue - 1 ) ; logger . warn ( "Connect to node {} failed." , node , e ) ; NodeStatusManager . getINSTANCE ( ) . deactivate ( node ) ; } }
public void test() { if ( connectionExecutor . getQueue ( ) . size ( ) > queuewarninglimit ) { log . warn ( "More than " + ( queuewation . getQueue ( ) . size ( ) > queuew fencelimit ) ; } }
public void test() { try { task . run ( ) ; } catch ( Throwable t ) { LOGGER . error ( "Task failed" , t ) ; Throwables . throwIfUnchecked ( t ) ; throw new AssertionError ( t ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
@ Override public User addUser ( final User user ) throws JargonException , DuplicateDataException { code_block = IfStatement ; code_block = IfStatement ; updatePreChecks ( user ) ; GeneralAdminInp adminPI = GeneralAdminInp . instanceForAddUser ( user ) ; log . debug ( "adding user:{}" , user ) ; code_block = TryStatement ;  log . debug ( "user added, now process other fields" ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; return findByName ( user . getName ( ) ) ; }
public void test() { try { getIRODSProtocol ( ) . irodsFunction ( adminPI ) ; } catch ( DuplicateDataException dde ) { throw dde ; } catch ( NoMoreRulesException nmr ) { log . warn ( "no more rules interpereted as duplicate data exception for backwards compatibility" ) ; throw new DuplicateDataException ( "no more rules interpereted as duplicate data exception for backwards compatibility" ) ; } }
@ Override public User addUser ( final User user ) throws JargonException , DuplicateDataException { log . info ( "addUser()" ) ; code_block = IfStatement ; code_block = IfStatement ; updatePreChecks ( user ) ; GeneralAdminInp adminPI = GeneralAdminInp . instanceForAddUser ( user ) ; log . debug ( "executing admin PI" ) ; code_block = TryStatement ;  code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; return findByName ( user . getName ( ) ) ; }
public void test() { switch ( messageType ) { case CONNECT : case CONNACK : case PINGREQ : case PINGRESP : case DISCONNECT : LOG . info ( "{} {}" , direction , clientID ) ; break ; case SUBSCRIBE : MqttSubscribeMessage subscribe = ( MqttSubscribeMessage ) msg ; LOG . info ( "{} SUBSCRIBE <{}> to topics {}" , direction , clientID , subscribe . payload ( ) . topicSubscriptions ( ) ) ; break ; case UNSUBSCRIBE : MqttUnsubscribeMessage unsubscribe = ( MqttUnsubscribeMessage ) msg ; LOG . info ( "{} UNSUBSCRIBE <{}> to topics <{}>" , direction , clientID , unsubscribe . payload ( ) . topics ( ) ) ; break ; case PUBLISH : MqttPublishMessage publish = ( MqttPublishMessage ) msg ; LOG . info ( "{} PUBLISH <{}> to topics <{}>" , direction , clientID , publish . variableHeader ( ) . topicName ( ) ) ; break ; case PUBREC : case PUBCOMP : case PUBREL : case PUBACK : case UNSUBACK : LOG . info ( "{} {} <{}> packetID <{}>" , direction , messageType , clientID , messageId ( msg ) ) ; break ; case SUBACK : MqttSubAckMessage suback = ( MqttSubAckMessage ) msg ; final List < Integer > grantedQoSLevels = suback . payload ( ) . grantedQoSLevels ( ) ; LOG . info ( "{} SUBACK <{}> packetID <{}>, grantedQoses {}" , direction , clientID , messageId ( msg ) , grantedQoSLevels ) ; break ; } }
public void test() { switch ( messageType ) { case CONNECT : case CONNACK : case PINGREQ : case PINGRESP : case DISCONNECT : LOG . info ( "{} {} <{}>" , direction , messageType , clientID ) ; break ; case SUBSCRIBE : MqttSubscribeMessage subscribe = ( MqttSubscribeMessage ) msg ; LOG . info ( "{} SUBSCRIBE <{}> to topics <{}>" , direction , clientID , subscribe . payload ( ) . topics ( ) ) ; break ; case UNSUBSCRIBE : MqttUnsubscribeMessage unsubscribe = ( MqttUnsubscribeMessage ) msg ; LOG . info ( "{} UNSUBSCRIBE <{}> to topics <{}>" , direction , clientID , unsubscribe . payload ( ) . topics ( ) ) ; break ; case PUBLISH : MqttPublishMessage publish = ( MqttPublishMessage ) msg ; LOG . info ( "{} PUBLISH <{}> to topics <{}>" , direction , clientID , publish . variableHeader ( ) . topicName ( ) ) ; break ; case PUBREC : case PUBCOMP : case PUBREL : case PUBACK : case UNSUBACK : LOG . info ( "{} {} <{}> packetID <{}>" , direction , messageType , clientID , messageId ( msg ) ) ; break ; case SUBACK : MqttSubAckMessage suback = ( MqttSubAckMessage ) msg ; final List < Integer > grantedQoSLevels = suback . payload ( ) . grantedQoSLevels ( ) ; LOG . info ( "{} SUBACK <{}> packetID <{}>, grantedQoses {}" , direction , clientID , messageId ( msg ) , grantedQoSLevels ) ; break ; } }
public void test() { switch ( messageType ) { case CONNECT : case CONNACK : case PINGREQ : case PINGRESP : case DISCONNECT : LOG . info ( "{} {} <{}>" , direction , messageType , clientID ) ; break ; case SUBSCRIBE : MqttSubscribeMessage subscribe = ( MqttSubscribeMessage ) msg ; LOG . info ( "{} SUBSCRIBE <{}> to topics {}" , direction , clientID , subscribe . payload ( ) . topicSubscriptions ( ) ) ; break ; case UNSUBSCRIBE : MqttUnsubscribeMessage unsubscribe = ( MqttUnsubscribeMessage ) msg ; LOG . info ( "{} UNSUBSCRIBE <{}> to unsubscribe message {}" , direction , clientID , unsubscribe . payload ( ) . topicName ( ) ) ; break ; case PUBLISH : MqttPublishMessage publish = ( MqttPublishMessage ) msg ; LOG . info ( "{} PUBLISH <{}> to topics <{}>" , direction , clientID , publish . variableHeader ( msg ) ) ; break ; case PUBREC : case PUBCOMP : case PUBREL : case PUBACK : case UNSUBACK : LOG . info ( "{} {} <{}> packetID <{}>" , direction , messageType , clientID , messageId ( msg ) ) ; break ; case SUBACK : MqttSubAckMessage suback = ( MqttSubAckMessage ) msg ; final List < Integer > grantedQoSLevels = suback . payload ( ) . grantedQoSLevels ( ) ; LOG . info ( "{} SUBACK <{}> packetID <{}>, grantedQoses {}" , direction , clientID , messageId ( msg ) , grantedQoSLevels ) ; break ; } }
public void test() { switch ( messageType ) { case CONNECT : case CONNACK : case PINGREQ : case PINGRESP : case DISCONNECT : LOG . info ( "{} {} <{}>" , direction , messageType , clientID ) ; break ; case SUBSCRIBE : MqttSubscribeMessage subscribe = ( MqttSubscribeMessage ) msg ; LOG . info ( "{} SUBSCRIBE <{}> to topics <{}>" , direction , clientID , subscribe . payload ( ) . topicSubscriptions ( ) ) ; break ; case UNSUBSCRIBE : MqttUnsubscribeMessage unsubscribe = ( MqttUnsubscribeMessage ) msg ; LOG . info ( "{} UNSUBSCRIBE <{}> to topics <{}>" , direction , clientID , unsubscribe . payload ( ) . topics ( ) ) ; break ; case PUBLISH : MqttPublishMessage publish = ( MqttPublishMessage ) msg ; LOG . info ( "{} PUBLISH <{}> to publish {}" , direction , messageType , clientID ) ; break ; case PUBREC : case PUBCOMP : case PUBREL : case PUBACK : case UNSUBACK : case UNSUBACK : LOG . info ( "{} {} <{}> packetID <{}>" , direction , messageType , clientID , messageId ( msg ) ) ; break ; case SUBACK : MqttSubAckMessage suback = ( MqttSubAckMessage ) msg ; final List < Integer > grantedQoSLevels = suback . payload ( ) . grantedQoSLevels ( ) ; LOG . info ( "{} SUBACK <{}> packetID <{}>, grantedQoses {}" , direction , clientID , messageId ( msg ) , grantedQoSLevels ) ; break ; } }
public void test() { switch ( messageType ) { case CONNECT : case CONNACK : case PINGREQ : case PINGRESP : case DISCONNECT : LOG . info ( "{} {} <{}>" , direction , messageType , clientID ) ; break ; case SUBSCRIBE : MqttSubscribeMessage subscribe = ( MqttSubscribeMessage ) msg ; LOG . info ( "{} SUBSCRIBE <{}> to topics {}" , direction , clientID , subscribe . payload ( ) . topicSubscriptions ( ) ) ; break ; case UNSUBSCRIBE : MqttUnsubscribeMessage unsubscribe = ( MqttUnsubscribeMessage ) msg ; LOG . info ( "{} UNSUBSCRIBE <{}> to topics <{}>" , direction , clientID , unsubscribe . payload ( ) . topics ( ) ) ; break ; case PUBLISH : MqttPublishMessage publish = ( MqttPublishMessage ) msg ; LOG . info ( "{} PUBLISH <{}> to topics <{}>" , direction , clientID , publish . variableHeader ( ) . topicName ( ) ) ; break ; case PUBREC : case PUBCOMP : case PUBREL : case PUBACK : case UNSUBACK : LOG . info ( "{} PUBREC : {}" , direction , clientID ) ; break ; case SUBACK : MqttSubAckMessage suback = ( MqttSubAckMessage ) msg ; final List < Integer > grantedQoSLevels = suback . payload ( ) . grantedQoSLevels ( ) ; LOG . info ( "{} SUBACK <{}> packetID <{}>, grantedQoses {}" , direction , clientID , messageId ( msg ) , grantedQoSLevels ) ; break ; } }
public void test() { switch ( messageType ) { case CONNECT : case CONNACK : case PINGREQ : case PINGRESP : case DISCONNECT : LOG . info ( "{} {} <{}>" , direction , messageType , clientID ) ; break ; case SUBSCRIBE : MqttSubscribeMessage subscribe = ( MqttSubscribeMessage ) msg ; LOG . info ( "{} SUBSCRIBE <{}> to topics {}" , direction , clientID , subscribe . payload ( ) . topicSubscriptions ( ) ) ; break ; case UNSUBSCRIBE : MqttUnsubscribeMessage unsubscribe = ( MqttUnsubscribeMessage ) msg ; LOG . info ( "{} UNSUBSCRIBE <{}> to topics <{}>" , direction , clientID , unsubscribe . payload ( ) . topics ( ) ) ; break ; case PUBLISH : MqttPublishMessage publish = ( MqttPublishMessage ) msg ; LOG . info ( "{} PUBLISH <{}> to topics <{}>" , direction , clientID , publish . variableHeader ( ) . topicName ( ) ) ; break ; case PUBREC : case PUBCOMP : case PUBREL : case PUBACK : case UNSUBACK : LOG . info ( "{} {} <{}> packetID <{}>" , direction , messageType , clientID , messageId ( msg ) ) ; break ; case SUBACK : MqttSubAckMessage suback = ( MqttSubAckMessage ) msg ; final List < Integer > grantedQoSLevels = suback . payload ( ) . grantedQoSLevels ( ) ; LOG . info ( "{} {} <{}> packetID <{}>" , direction , messageType , clientID ( msg ) , grantedQoSLevels ) ; break ; } }
public void test() { try { return ( T ) server . getAttribute ( objectName , attributeName ) ; } catch ( MBeanException | AttributeNotFoundException | InstanceNotFoundException | ReflectionException e ) { log . error ( e . getMessage ( ) , e ) ; return errorValue ; } }
public void test() { try { String sessionId = state . transfer ( cmd ) ; events . contextChanged ( ) ; loadSession ( sessionId , ! cmd . getPlayback ( ) . getIsPaused ( ) , true ) ; } catch ( IOException | MercuryClient . MercuryException ex ) { LOGGER . error ( "Failed loading transfer context!" , ex ) ; panicState ( null ) ; } catch ( AbsSpotifyContext . UnsupportedContextException ex ) { LOGGER . error ( "Cannot play context!" , ex ) ; panicState ( null ) ; } }
public void test() { try { String sessionId = state . transfer ( cmd ) ; events . contextChanged ( ) ; loadSession ( sessionId , ! cmd . getPlayback ( ) . getIsPaused ( ) , true ) ; } catch ( IOException | MercuryClient . MercuryException ex ) { LOGGER . error ( "Failed loading context!" , ex ) ; panicState ( null ) ; } catch ( AbsSpotifyContext . UnsupportedContextException ex ) { LOGGER . error ( "Cannot play context!" , ex ) ; panicState ( null ) ; } }
public void test() { if ( ! result . add ( alias ) && log . isWarnEnabled ( ) ) { log . warn ( "Duplicate alias: " + alias ) ; } }
public void test() { try { rules = metaStore . getRuleInfo ( ) ; } catch ( MetaStoreException e ) { LOG . error ( "RuleInfo" , e ) ; } }
@ Override public void init ( ) throws IOException { LOG . info ( "Initializing ..." ) ; List < RuleInfo > rules = null ; code_block = TryStatement ;  code_block = ForStatement ; code_block = IfStatement ; LOG . info ( "Done initializing." ) ; }
public void test() { for ( RuleInfo info : rules ) { LOG . info ( info . toString ( ) ) ; } }
@ Override protected void decode ( ChannelHandlerContext chc , DatagramPacket msg , List < Object > list ) { ByteBuf bb = msg . content ( ) ; code_block = IfStatement ; int length = bb . getUnsignedShort ( bb . readerIndex ( ) + LENGTH_INDEX_IN_HEADER ) ; code_block = IfStatement ; logger . debug ( "Decoding message {}" , length ) ; ByteBuf messageBuffer = bb . slice ( bb . readerIndex ( ) , length ) ; list . add ( messageBuffer ) ; messageBuffer . retain ( ) ; bb . skipBytes ( length ) ; }
public void test() { try { AuthzCache . getInstance ( ) . clear ( ) ; } catch ( ApplicationSettingsException e ) { logger . error ( e . getMessage ( ) , e ) ; throw new AiravataSecurityException ( "Error in obtaining the authorization cache instance." ) ; } }
private void copyTemplateFromSecondaryToPrimary ( VmwareHypervisorHost hyperHost , DatastoreMO datastoreMo , String secondaryStorageUrl , String templatePathAtSecondaryStorage , String templateName , String templateUuid , String nfsVersion ) throws Exception { s_logger . info ( "copyTemplateFromSecondaryToPrimary()" ) ; String secondaryMountPoint = _mountService . getMountPoint ( secondaryStorageUrl , nfsVersion ) ; s_logger . info ( "Secondary storage mount point: " + secondaryMountPoint ) ; String srcOVAFileName = secondaryMountPoint + "/" + templatePathAtSecondaryStorage + templateName + "." + ImageFormat . OVA . getFileExtension ( ) ; String srcFileName = getOVFFilePath ( srcOVAFileName ) ; code_block = IfStatement ; srcFileName = getOVFFilePath ( srcOVAFileName ) ; code_block = IfStatement ; String vmName = templateUuid ; hyperHost . importVmFromOVF ( srcFileName , vmName , datastoreMo , "thin" , null ) ; VirtualMachineMO vmMo = hyperHost . findVmOnHyperHost ( vmName ) ; code_block = IfStatement ; code_block = IfStatement ; }
public void test() { if ( e . getMessage ( ) . contains ( "quota_calculated" ) ) { s_logger . warn ( "Unable to create column quota_calculated in table cloud_usage.cloud_usage" ) ; } else { throw new CloudRuntimeException ( "Unable to create column quota_calculated in table cloud_usage.cloud_usage" , e ) ; } }
public void test() { if ( databaseHistory . skipUnparseableDdlStatements ( ) ) { streamingMetrics . incrementWarningCount ( ) ; streamingMetrics . incrementUnparsableDdlCount ( ) ; LOGGER . warn ( "Parseable DDL statement was skipped" ) ; } else { throw e ; } }
public void test() { switch ( event . type ( ) ) { case CREATE_TABLE : changeEvents . add ( createTableEvent ( ( TableCreatedEvent ) event ) ) ; break ; case ALTER_TABLE : changeEvents . add ( alterTableEvent ( ( TableAlteredEvent ) event ) ) ; break ; case DROP_TABLE : changeEvents . add ( dropTableEvent ( tableBefore , ( TableDroppedEvent ) event ) ) ; break ; default : LOG . error ( "Unknown event type." ) ; break ; } }
@ Override public void dispose ( ) { logger . debug ( "Running dispose()" ) ; ScheduledFuture < ? > job = refreshJob ; code_block = IfStatement ; refreshJob = null ; }
public void test() { try { exchange . respond ( coapTransportAdaptor . convertToPublish ( isConRequest ( ) , msg ) ) ; } catch ( AdaptorException e ) { log . error ( e . getMessage ( ) , e ) ; exchange . respond ( CoAP . ResponseCode . INTERNAL_SERVER_ERROR ) ; } }
public void loadSettings ( @ NotNull InputStream in , @ NotNull String systemId ) throws ConfigurationException , IOException { log . trace ( "loadSettings({})" , systemId ) ; VaultSettings settings = new VaultSettings ( ) ; settings . load ( in ) ; setSettings ( settings ) ; }
public void test() { try { ProjectService . Iface client = thriftClients . makeProjectClient ( ) ; Project project = client . getProjectByIdForEdit ( projectId , user ) ; project . putToReleaseIdToUsage ( releaseId , new ProjectReleaseRelationship ( ReleaseRelationship . CONTAINED , MainlineState . OPEN ) ) ; client . updateProject ( project , user ) ; JSONObject jsonObject = JSONFactoryUtil . createJSONObject ( ) ; jsonObject . put ( "success" , true ) ; jsonObject . put ( "releaseId" , releaseId ) ; jsonObject . put ( "projectId" , projectId ) ; writeJSON ( request , response , jsonObject ) ; } catch ( TException exception ) { log . error ( "Cannot link release [" + releaseId + "] to project [" + projectId + "]." ) ; log . error ( exception . getMessage ( ) ) ; response . setProperty ( ResourceResponse . HTTP_STATUS_CODE , "500" ) ; } }
public void test() { try { log . debug ( "Link release [" + releaseId + "] to project [" + projectId + "]" ) ; ProjectService . Iface client = thriftClients . makeProjectClient ( ) ; Project project = client . getProjectByIdForEdit ( projectId , user ) ; project . putToReleaseIdToUsage ( releaseId , new ProjectReleaseRelationship ( ReleaseRelationship . CONTAINED , MainlineState . OPEN ) ) ; client . updateProject ( project , user ) ; JSONObject jsonObject = JSONFactoryUtil . createJSONObject ( ) ; jsonObject . put ( "success" , true ) ; jsonObject . put ( "releaseId" , releaseId ) ; jsonObject . put ( "projectId" , projectId ) ; writeJSON ( request , response , jsonObject ) ; } catch ( TException exception ) { log . error ( "Error getting project from backend!" , exception ) ; response . setProperty ( ResourceResponse . HTTP_STATUS_CODE , "500" ) ; } }
public void test() { try { this . sleep ( 2000 ) ; } catch ( InterruptedException e ) { logger . warn ( "" , e ) ; return ; } }
public void test() { if ( ! oldFile . delete ( ) ) { oldFile . deleteOnExit ( ) ; logger . info ( "Deleted old file on exit: {}" , oldFile . getAbsolutePath ( ) ) ; } }
public void test() { try { updatePKICerts ( certs , entityId , IDP_META_CERT ) ; } catch ( EngineException e ) { log . warn ( "Unable to update existing entity IDP " + entityId , e ) ; continue ; } }
public void test() { try { transformURLMethod . invoke ( configurationFileInstaller , file ) ; } catch ( InvocationTargetException invocationTargetException ) { LOGGER . error ( "Unable to invoke configuration file" , invocationTargetException ) ; } }
public void test() { try { workflowProcessing . publishMessageToNextInMQ ( requestMessage ) ; } catch ( WorkflowTaskInitializationException e ) { log . error ( "In WorkflowRetryExecutor,retryWorkflowWithCompletedTask failed due to exception. Last completed execution: " + lastCompletedTaskExecution , e ) ; InsightsStatusProvider . getInstance ( ) . createInsightStatusNode ( "In WorkflowRetryExecutor,retryWorkflowWithCompletedTask failed due to exception. Last completed execution: " + lastCompletedTaskExecution , PlatformServiceConstants . FAILURE ) ; } }
public void test() { switch ( level ) { case TRACE : logger . trace ( marker , message , exception ) ; break ; case DEBUG : logger . debug ( marker , message , exception ) ; break ; case INFO : logger . info ( marker , message , exception ) ; break ; case WARN : logger . warn ( marker , message , exception ) ; break ; case ERROR : logger . error ( marker , message , exception ) ; break ; } }
public void test() { switch ( level ) { case TRACE : logger . trace ( marker , message , exception ) ; break ; case DEBUG : logger . debug ( marker , message , exception ) ; break ; case INFO : logger . info ( marker , message , exception ) ; break ; case WARN : logger . warn ( marker , message , exception ) ; break ; case ERROR : logger . error ( marker , message , exception ) ; break ; } }
public void test() { switch ( level ) { case TRACE : logger . trace ( marker , message , exception ) ; break ; case DEBUG : logger . debug ( marker , message , exception ) ; break ; case INFO : logger . info ( marker , message , exception ) ; break ; case WARN : logger . warn ( marker , message , exception ) ; break ; case ERROR : logger . error ( marker , message , exception ) ; break ; } }
public void test() { switch ( level ) { case TRACE : logger . trace ( marker , message , exception ) ; break ; case DEBUG : logger . debug ( marker , message , exception ) ; break ; case INFO : logger . info ( marker , message , exception ) ; break ; case WARN : logger . warn ( marker , message , exception ) ; break ; case ERROR : logger . error ( marker , message , exception ) ; break ; } }
public void test() { switch ( level ) { case TRACE : logger . trace ( marker , message , exception ) ; break ; case DEBUG : logger . debug ( marker , message , exception ) ; break ; case INFO : logger . info ( marker , message , exception ) ; break ; case WARN : logger . warn ( marker , message , exception ) ; break ; case ERROR : logger . error ( marker , message , exception ) ; break ; } }
public void loginEvent ( final User user , final String surrogateIdentifier , final String ip , final String errorMessage ) { log . debug ( ">>> {}" , user ) ; }
public void test() { try { roleDescriptor = provider . getRole ( entityID , roleName , supportedProtocol ) ; code_block = IfStatement ; } catch ( MetadataProviderException e ) { log . warn ( "Error retrieving metadata from provider of type {}, proceeding to next provider" , provider . getClass ( ) . getName ( ) , e ) ; continue ; } }
public void test() { try { activeListeners . inc ( ) ; filebeatServer . listen ( ) ; } catch ( InterruptedException e ) { log . error ( "Got InterruptedException [" + e . getMessage ( ) + "]" ) ; } catch ( Exception e ) { code_block = IfStatement ; } finally { activeListeners . dec ( ) ; } }
public void test() { try { mimeType = new MimeType ( mimeTypeString ) ; } catch ( MimeTypeParseException e ) { log . error ( e . getMessage ( ) ) ; } }
public void test() { try { result = type . cast ( new BinaryContentImpl ( exchange . getOut ( ) . getBody ( InputStream . class ) , mimeType ) ) ; } catch ( ClassCastException e ) { log . error ( "Unable to cast returned " + e . getMessage ( ) , e ) ; } }
public void test() { if ( profileWithActions != null ) { List < Action > actionList = profileWithActions . getAction ( ) ; code_block = ForStatement ; } else { LOG . warn ( "Could not find action:" + profileWithActions . getAction ( ) ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( SegmentsEntryServiceUtil . class , "getSegmentsEntry" , _getSegmentsEntryParameterTypes8 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , segmentsEntryId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . segments . model . SegmentsEntry ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void fixed ( String what , String reason ) { LOG . warn ( FIXED_REASON , what , reason ) ; }
public void test() { if ( LOGGER . isInfoEnabled ( ) ) { LOGGER . info ( "Success!" ) ; } }
public void test() { try { MDC . put ( RootContext . MDC_KEY_XID , globalSession . getXid ( ) ) ; handler . handle ( globalSession ) ; } catch ( Throwable th ) { logger . error ( "Unexpected error" , th ) ; } finally { MDC . remove ( RootContext . MDC_KEY_XID ) ; } }
@ Override public MCCIIN000002UV01 processPatientDiscoveryAsyncReq ( PRPAIN201305UV02 message , AssertionType assertion , NhinTargetCommunitiesType target ) { LOG . debug ( "Begin processPatientDiscoveryAsyncReq" ) ; MCCIIN000002UV01 response = new MCCIIN000002UV01 ( ) ; code_block = TryStatement ;  LOG . debug ( "End processPatientDiscoveryAsyncReq" ) ; return response ; }
@ Override public MCCIIN000002UV01 processPatientDiscoveryAsyncReq ( PRPAIN201305UV02 message , AssertionType assertion , NhinTargetCommunitiesType target ) { LOG . debug ( "Begin processPatientDiscoveryAsyncReq" ) ; MCCIIN000002UV01 response = new MCCIIN000002UV01 ( ) ; code_block = TryStatement ;  LOG . debug ( "End processPatientDiscoveryAsyncReq" ) ; return response ; }
public void test() { if ( result . succeeded ( ) ) { log . debug ( "got 1st try" ) ; testContext . assertEquals ( 1 , pool . connCount ( ) ) ; pool . getConnection ( "hostname" , result2 code_block = LoopStatement ; ) ; testContext . assertFalse ( haveGotConnection . get ( ) , "got a connection on the 2nd try already" ) ; testContext . assertEquals ( 1 , pool . connCount ( ) ) ; log . debug ( "didn't get a connection 2nd time yet" ) ; result . result ( ) . returnToPool ( ) ; vertx . setTimer ( 1000 , v code_block = LoopStatement ; ) ; } else { log . info ( result . cause ( ) ) ; testContext . fail ( result . cause ( ) ) ; } }
public void test() { if ( result2 . succeeded ( ) ) { haveGotConnection . set ( true ) ; testContext . assertEquals ( 1 , pool . connCount ( ) ) ; log . debug ( "got connection" ) ; result2 . result ( ) . returnToPool ( ) ; pool . close ( v code_block = LoopStatement ; ) ; } else { log . info ( result2 . cause ( ) ) ; testContext . fail ( result2 . cause ( ) ) ; } }
public void test() { if ( result2 . succeeded ( ) ) { haveGotConnection . set ( true ) ; log . debug ( "got connection 2nd time" ) ; testContext . assertEquals ( 1 , pool . connCount ( ) ) ; result2 . result ( ) . returnToPool ( ) ; pool . close ( v code_block = LoopStatement ; ) ; } else { log . info ( result2 . cause ( ) ) ; testContext . fail ( result2 . cause ( ) ) ; } }
public void test() { if ( result . succeeded ( ) ) { log . debug ( "got connection 1st tme" ) ; testContext . assertEquals ( 1 , pool . connCount ( ) ) ; pool . getConnection ( "hostname" , result2 code_block = LoopStatement ; ) ; log . debug ( "got connection 1st tme" ) ; testContext . assertFalse ( haveGotConnection . get ( ) , "got a connection on the 2nd try already" ) ; testContext . assertEquals ( 1 , pool . connCount ( ) ) ; result . result ( ) . returnToPool ( ) ; vertx . setTimer ( 1000 , v code_block = LoopStatement ; ) ; } else { log . info ( result . cause ( ) ) ; testContext . fail ( result . cause ( ) ) ; } }
public void test() { if ( result . succeeded ( ) ) { log . debug ( "got connection 1st tme" ) ; testContext . assertEquals ( 1 , pool . connCount ( ) ) ; pool . getConnection ( "hostname" , result2 code_block = LoopStatement ; ) ; testContext . assertFalse ( haveGotConnection . get ( ) , "got a connection on the 2nd try already" ) ; testContext . assertEquals ( 1 , pool . connCount ( ) ) ; log . debug ( "didn't get a connection 2nd time yet" ) ; result . result ( ) . returnToPool ( ) ; vertx . setTimer ( 1000 , v code_block = LoopStatement ; ) ; } else { log . info ( result . cause ( ) ) ; testContext . fail ( result . cause ( ) ) ; } }
@ Override public List < String > listZoneNames ( ) throws JargonException { log . info ( "listZoneNames" ) ; IRODSGenQueryExecutor irodsGenQueryExecutor = new IRODSGenQueryExecutorImpl ( getIRODSSession ( ) , getIRODSAccount ( ) ) ; IRODSGenQueryBuilder builder = new IRODSGenQueryBuilder ( true , null ) ; IRODSQueryResultSet resultSet ; code_block = TryStatement ;  List < String > zones = new ArrayList < String > ( ) ; String zone ; code_block = ForStatement ; return zones ; }
public void test() { if ( null == protocol ) { log . error ( "Protocol not found" ) ; continue ; } }
public void test() { try { code_block = ForStatement ; } catch ( IllegalArgumentException e ) { logger . error ( "Error during prepare composite key, Caused by {}." , e ) ; throw new PersistenceException ( e ) ; } catch ( IllegalAccessException e ) { logger . warn ( "Error during prepare composite key, Caused by {}." , e ) ; } }
@ GET @ Path ( "/{id}" ) public Response getRepairSchedule ( @ PathParam ( "id" ) UUID repairScheduleId ) { LOG . debug ( "getRepairSchedule started..." ) ; Optional < RepairSchedule > repairSchedule = context . storage . getRepairSchedule ( repairScheduleId ) ; code_block = IfStatement ; }
public void test() { try { IoTDBConfigCheck . getInstance ( ) . checkConfig ( ) ; } catch ( IOException e ) { logger . error ( "IOException" , e ) ; } }
public void test() { if ( reader . hasNext ( ) ) { return reader . next ( ) . getFieldValues ( ) ; } else { logger . debug ( "Failed to find record: " + sql ) ; } }
public void test() { try ( final RowReader < GeoWaveRow > reader = getRowReader ( operations , adapterStore , mappingStore , internalAdapterStore , fieldSubsets , aggregation , additionalAuthorizations , adapterId , dataId ) ) { code_block = IfStatement ; } catch ( final Exception e ) { LOGGER . warn ( "Unable to read row" , e ) ; } }
@ Test public void checkOverlaps ( ) throws IOException }
void debug ( ) { logger . debug ( "debug" ) ; nodeEquipments . forEach ( this :: debug ) ; }
public void test() { try { return Integer . valueOf ( strValue . trim ( ) ) ; } catch ( Exception e ) { Log . error ( "Invalid configuration value provided, using default value {}" , strValue , e ) ; return defaultValue ; } }
public void test() { try { Set < ContainerReplica > replicas = containerManager . getContainerReplicas ( ContainerID . valueOf ( containerID ) ) ; LOG . info ( "Container {} has {} replicas on {}" , containerID , replicas . size ( ) , replicas . stream ( ) . map ( ContainerReplica :: getDatanodeDetails ) . map ( DatanodeDetails :: getUuidString ) . sorted ( ) . collect ( toList ( ) ) ) ; return replicas . size ( ) ; } catch ( ContainerNotFoundException e ) { LOG . debug ( "Container {} does not exist on container {}" , containerID , containerID ) ; return 0 ; } }
public void test() { if ( branchUuid == null ) { logger . warn ( "Ignoring invalid branch uuid: {}" , branchUuid ) ; continue ; } }
public void test() { try { clientPool . invalidateObject ( client ) ; } catch ( Exception e ) { LOGGER . warn ( e . getMessage ( ) ) ; } }
public void test() { try { LOG . info ( "Starting " + getName ( ) ) ; doRunServer ( ) ; synchronized ( this ) code_block = "" ; } catch ( final Throwable e ) { LOG . error ( "Unexpected error" , e ) ; } }
@ Override public void handleDelivery ( String consumerTag , Envelope envelope , AMQP . BasicProperties properties , byte [ ] body ) throws IOException { LOGGER . debug ( "Received message: {}" , properties ) ; receivedHeaders . putAll ( properties . getHeaders ( ) ) ; received . add ( new String ( body ) ) ; }
public void test() { try { future . get ( DEFAULT_EXTRACTOR_JOB_TIME , TimeUnit . SECONDS ) ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { if ( query . getResults ( ) != null && ! query . getResults ( ) . isEmpty ( ) ) { code_block = TryStatement ;  } else-if ( tracker . getException ( ) != null ) { LOGGER . error ( tracker . getException ( ) , tracker . getException ( ) ) ; } }
public void test() { try { monitorRequestPending . set ( false ) ; code_block = IfStatement ; final HashMap < String , InterfaceState > newInterfaceStatuses = new HashMap < > ( ) ; logger . debug ( "tracked modems: {}" , modems . keySet ( ) ) ; code_block = ForStatement ; checkStatusChange ( this . interfaceStatuses , newInterfaceStatuses ) ; this . interfaceStatuses = newInterfaceStatuses ; } catch ( Exception ex ) { logger . error ( "Could not check status change" , ex ) ; } }
public void test() { if ( h . failed ( ) ) { logger . error ( "Failed to stop vertices" , h . cause ( ) ) ; } }
public void test() { if ( f . delete ( ) ) { LOGGER . info ( "[DeleteFileRequest] Delete file " + filePath ) ; } else { LOGGER . error ( "[DeleteFileRequest] Error on deleting file " + filePath ) ; } }
public void test() { if ( f . delete ( ) ) { LOGGER . info ( "[DeleteFileRequest] File " + filePath + " deleted." ) ; } else { LOGGER . error ( "[DeleteFileRequest] Cannot delete file " + filePath + "." ) ; } }
public void test() { if ( log . isDebugEnabled ( ) && ( t != null ) ) { log . warn ( message , o1 , o2 , o3 , o4 , o5 , o6 , o7 , t7 ) ; } else { log . warn ( message , o1 , o2 , o3 , o4 , o5 , o6 , o7 ) ; } }
public void test() { if ( log . isDebugEnabled ( ) && ( t != null ) ) { log . warn ( message , o1 , o2 , o3 , o4 , o5 , o6 , o7 , t ) ; } else { log . warn ( message , o1 , o2 , o3 , o5 , o6 , o7 ) ; } }
@ Bean ( name = BEAN_NAME_EMBEDDED_MYSQL , destroyMethod = "stop" ) public MySQLContainer mysql ( ConfigurableEnvironment environment , MySQLProperties properties ) { log . info ( "Initializing MySQL container: " + properties . dockerImage ) ; MySQLContainer mysql = new MySQLContainer < > ( properties . dockerImage ) . withEnv ( "MYSQL_ALLOW_EMPTY_PASSWORD" , "yes" ) . withUsername ( properties . getUser ( ) ) . withDatabaseName ( properties . getDatabase ( ) ) . withPassword ( properties . getPassword ( ) ) . withCommand ( "--character-set-server=" + properties . getEncoding ( ) , "--collation-server=" + properties . getCollation ( ) ) . withExposedPorts ( properties . port ) . withCreateContainerCmdModifier ( cmd -> cmd . getHostConfig ( ) . withCapAdd ( Capability . NET_ADMIN ) ) . withInitScript ( properties . initScriptPath ) ; mysql = ( MySQLContainer ) configureCommonsAndStart ( mysql , properties , log ) ; registerMySQLEnvironment ( mysql , environment , properties ) ; return mysql ; }
public void test() { if ( oldScheduler != null ) { boolean stopped = oldScheduler . cancel ( true ) ; logger . info ( "Stop old scheduler service" ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { CRS_BY_SRS_ID . put ( WGS84_SRS_ID , WGS84 ) ; CoordinateReferenceSystem webMercatorCrs = CRS . decode ( SpatialReferenceSystem . WEB_MERCATOR_SRS_ID ) ; CRS_BY_SRS_ID . put ( SpatialReferenceSystem . WEB_MERCATOR_SRS_ID , webMercatorCrs ) ; getOrCreateTransform ( SpatialReferenceSystem . WGS84_SRS_ID , SpatialReferenceSystem . WEB_MERCATOR_SRS_ID ) ; getOrCreateTransform ( SpatialReferenceSystem . WEB_MERCATOR_SRS_ID , SpatialReferenceSystem . WGS84_SRS_ID ) ; } catch ( Exception e ) { LOGGER . log ( Level . SEVERE , e . getMessage ( ) , e ) ; throw new RuntimeException ( e ) ; } }
public void test() { if ( ! assertSuccessCreationKerberosPrincipal ( "principal/localhost@DOMAIN.COM" ) ) { LOGGER . error ( "Failed to create principal by hostname." ) ; } }
public void test() { try { server . shutdown ( ) . awaitTermination ( 10L , TimeUnit . SECONDS ) ; } catch ( InterruptedException ex ) { LOGGER . warn ( "Interrupted while waiting for shutdown" , ex ) ; } }
public void delete ( StgG20Sys persistentInstance ) { log . debug ( "deleting StgG20Sys instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . delete ( persistentInstance ) ; log . debug ( "delete successful" ) ; } catch ( RuntimeException re ) { log . error ( "delete failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . delete ( persistentInstance ) ; log . debug ( "delete successful" ) ; } catch ( RuntimeException re ) { log . error ( "delete failed" , re ) ; throw re ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getCapabilities()" ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getCapabilities()" ) ; } }
public void test() { if ( logger . isInfoEnabled ( ) ) { StringBuilder builder = new StringBuilder ( ) ; builder . append ( "error. [object]" ) . append ( proxy . getClass ( ) . toString ( ) ) . append ( "[proceed]" ) . append ( proceed . getName ( ) ) ; code_block = IfStatement ; builder . append ( "\n[throwable.getMessage()] " ) . append ( throwable . getMessage ( ) ) ; logger . info ( builder . toString ( ) ) ; } }
public void test() { if ( e . getCause ( ) != null ) { logger . error ( e . getCause ( ) . getMessage ( ) ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
private boolean getBooleanValueFromYaml ( String option ) { String stringValue = yamlStringConfigs . get ( option ) ; boolean result = stringValue != null && Boolean . valueOf ( stringValue ) . equals ( Boolean . TRUE ) ; LOGGER . debug ( "{} = {}" , option , result ) ; return result ; }
public void test() { if ( ex == null ) { LOG . info ( "Successfully updated ledger metadata for ledgerId {}" , lh . getId ( ) ) ; ensembleUpdatedCb . processResult ( BKException . Code . OK , null , null ) ; } else { LOG . error ( "Error updating ledger config metadata for ledgerId {}" , lh . getId ( ) , ex ) ; ensembleUpdatedCb . processResult ( BKException . getExceptionCode ( ex , BKException . Code . UnexpectedConditionException ) , null , null ) ; } }
public void test() { if ( ex == null ) { LOG . info ( "Updated ZK to point ledger fragments" + " from old bookies to new bookies: {}" , oldBookie2NewBookie ) ; ensembleUpdatedCb . processResult ( BKException . Code . OK , null , null ) ; } else { LOG . error ( "Failed to update ZK ledger fragments in old bookies: {}" , oldBookie2NewBookie ) ; ensembleUpdatedCb . processResult ( BKException . getExceptionCode ( ex , BKException . Code . UnexpectedConditionException ) , null , null ) ; } }
public void test() { -> { logger . warn ( "Warning" ) ; notificationPaneController . addNotification ( String . format ( "Sorry, there was a git error on branch %s." , branch . getRefName ( ) ) ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( KBFolderServiceUtil . class , "fetchFirstChildKBFolder" , _fetchFirstChildKBFolderParameterTypes2 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , kbFolderId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . knowledge . base . model . KBFolder ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { Bootstrap b = new Bootstrap ( ) ; b . group ( group ) . channel ( NioSocketChannel . class ) . handler ( clientInitializer ) ; b . connect ( host , port ) . sync ( ) ; synchronized ( scenarioHandler ) code_block = "" ; } catch ( Exception ex ) { LOG . error ( "Connect error" , ex ) ; } finally { LOG . debug ( "shutting down" ) ; code_block = TryStatement ;  } }
public void test() { try { group . shutdownGracefully ( ) . get ( ) ; LOG . debug ( "shutdown succesful" ) ; } catch ( InterruptedException | ExecutionException e ) { LOG . error ( "Error" , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { ThemeDisplay clonedThemeDisplay = ( ThemeDisplay ) themeDisplay . clone ( ) ; clonedThemeDisplay . setScopeGroupId ( _journalArticle . getGroupId ( ) ) ; return Optional . ofNullable ( _assetDisplayPageFriendlyURLProvider . getFriendlyURL ( JournalArticle . class . getName ( ) , _journalArticle . getResourcePrimKey ( ) , locale , clonedThemeDisplay ) ) . map ( url code_block = LoopStatement ; ) . orElse ( StringPool . BLANK ) ; } catch ( CloneNotSupportedException | PortalException exception ) { _log . error ( exception , exception ) ; return StringPool . BLANK ; } }
public void test() { try { cache . clearAllExpired ( ) ; } catch ( Throwable e ) { log . error ( e ) ; } }
public void test() { if ( sessionMd != null ) { processInWebSocketService ( sessionMd . sessionRef , SessionEvent . onError ( tError ) ) ; } else { logger . warn ( "sessionMd != null" ) ; } }
public void test() { try { tuningObject = context . getBean ( tuningClass ) ; logger . info ( "find singleton tuning Object from IOC" ) ; } catch ( NoSuchBeanDefinitionException e ) { logger . error ( "Could not find tuner" , e ) ; } }
public void test() { try { Class < ? > tuningClass = this . getClass ( ) . getClassLoader ( ) . loadClass ( PersistenceConf . TUNING_CLASS . getValue ( ) ) ; ApplicationContext context = DataWorkCloudApplication . getApplicationContext ( ) ; code_block = IfStatement ; code_block = IfStatement ; tuningMethod = tuningClass . getMethod ( PersistenceConf . TUNING_METHOD . getValue ( ) , Object . class ) ; tuningIsOpen = true ; } catch ( ClassNotFoundException | InstantiationException | IllegalAccessException | NoSuchMethodException e ) { LOGGER . error ( "Could not run TUNING Driver." , e ) ; } finally { isInited = true ; } }
@ Test public void h_getObservationTest ( ) { LOGGER . info ( "Get Observation from ExperimentRun test start................................" ) ; g_logObservationsTest ( ) ; GetObservations getObservationRequest = GetObservations . newBuilder ( ) . setId ( experimentRun . getId ( ) ) . setObservationKey ( "Google developer Observation artifact" ) . build ( ) ; GetObservations . Response response = experimentRunServiceStub . getObservations ( getObservationRequest ) ; LOGGER . info ( "GetObservations Response : " + response . getObservationsCount ( ) ) ; code_block = ForStatement ; LOGGER . info ( "Get Observation from ExperimentRun tags test stop................................" ) ; }
@ Test public void h_getObservationTest ( ) { LOGGER . info ( "Get Observation from ExperimentRun test start................................" ) ; g_logObservationsTest ( ) ; GetObservations getObservationRequest = GetObservations . newBuilder ( ) . setId ( experimentRun . getId ( ) ) . setObservationKey ( "Google developer Observation artifact" ) . build ( ) ; GetObservations . Response response = experimentRunServiceStub . getObservations ( getObservationRequest ) ; LOGGER . info ( "GetObservations Response : " + response . getObservationsCount ( ) ) ; code_block = ForStatement ; LOGGER . info ( "Get Observation from ExperimentRun tags test stop................................" ) ; }
public void test() { if ( _log . isDebugEnabled ( ) && ( phase == _PHASE_BEFORE ) ) { _log . debug ( StringBundler . concat ( "Synchronously processing event " , event . getEventType ( ) , " for phase " , phase ) ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) && ( phase == _PHASE_BEFORE ) ) { _log . debug ( StringBundler . concat ( "Synchronously processing event " , event . getEventType ( ) , " for phase " , phase ) ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) && ( phase == _PHASE_BEFORE ) ) { _log . debug ( StringBundler . concat ( "Synchronously processing event " , event . getEventType ( ) , " for phase " , phase ) ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { StringBundler sb = new StringBundler ( 4 ) ; sb . append ( "If send is true, the first parameter of " ) ; sb . append ( aopMethodInvocation ) ; sb . append ( " must implement AuditedModel, GroupedModel, or " ) ; sb . append ( "StagedModel" ) ; _log . debug ( sb . toString ( ) ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getCapabilities()" ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getCapabilities()" ) ; } }
public void test() { try { res = file . getChannel ( ) . tryLock ( ) ; } catch ( OverlappingFileLockException oe ) { file . close ( ) ; return null ; } catch ( IOException e ) { file . close ( ) ; logger . error ( e . getMessage ( ) , e ) ; throw e ; } }
public void test() { try { Thread . sleep ( 500 ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { parsingBuffer . position ( 0 ) ; scanMessageData ( parsingBuffer ) ; lazyDecodeApplicationProperties ( parsingBuffer ) ; parsingBuffer = null ; } catch ( RuntimeException expected ) { logger . debug ( "Problem parsing message data" , expected ) ; } }
public void test() { if ( clone == e && LOG . isDebugEnabled ( ) ) { LOG . debug ( clone ) ; } }
public void test() { try { Prevention prev = PreventionData . getLocalandRemotePreventions ( loggedInInfo , Integer . parseInt ( demo ) ) ; pf . getMessages ( prev ) ; @ SuppressWarnings ( "unchecked" ) Map < String , Object > m = prev . getWarningMsgs ( ) ; @ SuppressWarnings ( "rawtypes" ) Set set = m . entrySet ( ) ; @ SuppressWarnings ( "rawtypes" ) Iterator i = set . iterator ( ) ; String k = "" ; code_block = IfStatement ; code_block = WhileStatement ; dataCache . put ( demo , ret ) ; } catch ( Exception e ) { Log . log ( e ) ; ret = "" ; } }
public void test() { try { connStr = configuration . getConnectionString ( Settings . KEYS . DB_CONNECTION_STRING , Settings . KEYS . DB_FILE_NAME ) ; } catch ( IOException ex ) { logger . error ( "getConnectionString error" , ex ) ; return false ; } }
private void prepareCcsProtocolType ( ChangeCipherSpecMessage msg ) { msg . setCcsProtocolType ( CCS_PROTOCOL_TYPE ) ; LOGGER . debug ( "CipherSpec: " + msg . getCipherSpecMessage ( ) . getValue ( ) ) ; }
@ Test public void testEvent ( ) throws EventDeliveryException , MQBrokerException , MQClientException , InterruptedException , UnsupportedEncodingException , RemotingException { DefaultMQProducer producer = new DefaultMQProducer ( producerGroup ) ; producer . setNamesrvAddr ( nameServer ) ; String sendMsg = "\"Hello Flume\"" + "," + DateFormatUtils . format ( new Date ( ) , "yyyy-MM-dd hh:mm:ss" ) ; producer . start ( ) ; Message msg = new Message ( TOPIC_DEFAULT , tag , sendMsg . getBytes ( "UTF-8" ) ) ; SendResult sendResult = producer . send ( msg ) ; Context context = new Context ( ) ; context . put ( NAME_SERVER_CONFIG , nameServer ) ; context . put ( TAG_CONFIG , tag ) ; Channel channel = new MemoryChannel ( ) ; Configurables . configure ( channel , context ) ; List < Channel > channels = new ArrayList < > ( ) ; channels . add ( channel ) ; ChannelSelector channelSelector = new ReplicatingChannelSelector ( ) ; channelSelector . setChannels ( channels ) ; ChannelProcessor channelProcessor = new ChannelProcessor ( channelSelector ) ; RocketMQSource source = new RocketMQSource ( ) ; source . setChannelProcessor ( channelProcessor ) ; Configurables . configure ( source , context ) ; source . start ( ) ; Thread . sleep ( 2000 ) ; sendMsg = "\"Hello Flume\"" + "," + DateFormatUtils . format ( new Date ( ) , "yyyy-MM-dd hh:mm:ss" ) ; msg = new Message ( TOPIC_DEFAULT , tag , sendMsg . getBytes ( "UTF-8" ) ) ; sendResult = producer . send ( msg ) ; PollableSource . Status status = source . process ( ) ; code_block = IfStatement ; Thread . sleep ( 1000 ) ; producer . shutdown ( ) ; source . stop ( ) ; Transaction transaction = channel . getTransaction ( ) ; transaction . begin ( ) ; Event event = channel . take ( ) ; code_block = IfStatement ; byte [ ] body = event . getBody ( ) ; String receiveMsg = new String ( body , "UTF-8" ) ; log . info ( "received "
@ Test public void testEvent ( ) throws EventDeliveryException , MQBrokerException , MQClientException , InterruptedException , UnsupportedEncodingException , RemotingException { DefaultMQProducer producer = new DefaultMQProducer ( producerGroup ) ; producer . setNamesrvAddr ( nameServer ) ; String sendMsg = "\"Hello Flume\"" + "," + DateFormatUtils . format ( new Date ( ) , "yyyy-MM-dd hh:mm:ss" ) ; producer . start ( ) ; Message msg = new Message ( TOPIC_DEFAULT , tag , sendMsg . getBytes ( "UTF-8" ) ) ; SendResult sendResult = producer . send ( msg ) ; Context context = new Context ( ) ; context . put ( NAME_SERVER_CONFIG , nameServer ) ; context . put ( TAG_CONFIG , tag ) ; Channel channel = new MemoryChannel ( ) ; Configurables . configure ( channel , context ) ; List < Channel > channels = new ArrayList < > ( ) ; channels . add ( channel ) ; ChannelSelector channelSelector = new ReplicatingChannelSelector ( ) ; channelSelector . setChannels ( channels ) ; ChannelProcessor channelProcessor = new ChannelProcessor ( channelSelector ) ; RocketMQSource source = new RocketMQSource ( ) ; source . setChannelProcessor ( channelProcessor ) ; Configurables . configure ( source , context ) ; source . start ( ) ; Thread . sleep ( 2000 ) ; sendMsg = "\"Hello Flume\"" + "," + DateFormatUtils . format ( new Date ( ) , "yyyy-MM-dd hh:mm:ss" ) ; msg = new Message ( TOPIC_DEFAULT , tag , sendMsg . getBytes ( "UTF-8" ) ) ; sendResult = producer . send ( msg ) ; log . info ( "publish message : {}, sendResult:{}" , sendMsg , sendResult ) ; PollableSource . Status status = source . process ( ) ; code_block = IfStatement ; Thread . sleep ( 1000 ) ; producer . shutdown ( ) ; source . stop ( ) ; Transaction transaction = channel . getTransaction ( ) ; transaction . begin ( ) ; Event event = channel . take ( ) ; code_block = IfStatement ; byte [ ] body = event . getBody ( ) ;
public void test() { try { LoadTestDataSimpleResponseMessageType response = ( LoadTestDataSimpleResponseMessageType ) invokeClientPort ( AdminWSConstants . ADMIN_LTD_DELETEDOCUMENT , request ) ; logDebug ( AdminWSConstants . ADMIN_LTD_DELETEDOCUMENT , response . isStatus ( ) , response . getMessage ( ) ) ; return response . isStatus ( ) ; } catch ( Exception e ) { LOG . error ( "error during DELETE: {}" , e . getLocalizedMessage ( ) , e ) ; } }
@ Given ( "^Stop Event Broker$" ) public void stop ( ) { logger . info ( "Stopping Event Broker instance..." ) ; code_block = IfStatement ; code_block = TryStatement ;  code_block = IfStatement ; logger . info ( "Stopping Event Broker instance ... done!" ) ; }
public void test() { try ( final Suppressed < RuntimeException > s = Suppressed . withRuntimeException ( ) ) { closables . values ( ) . stream ( ) . flatMap ( values -> values . stream ( ) ) . forEach ( s :: closeSuppressed ) ; code_block = IfStatement ; } catch ( Exception e ) { LOGGER . warn ( "Error closing client" , e ) ; } }
@ Given ( "^Stop Event Broker$" ) public void stop ( ) { code_block = IfStatement ; logger . info ( "Stopping Event Broker instance ..." ) ; code_block = TryStatement ;  code_block = IfStatement ; logger . info ( "Successfully stopped Event Broker instance." ) ; }
@ SuppressWarnings ( "ConstantConditions" ) private void scheduleRenewal ( Reactor reactor ) { int sasTokenRenewalPeriod = this . amqpsSessionHandler . getDeviceClientConfig ( ) . getSasTokenAuthentication ( ) . getMillisecondsBeforeProactiveRenewal ( ) ; log . debug ( "sasTokenRenewal is {}" , sasTokenRenewalPeriod ) ; reactor . schedule ( sasTokenRenewalPeriod , this ) ; }
public void test() { try { doDispose ( ) ; } catch ( Exception e ) { LOG . debug ( "Ignoring exception" , e ) ; } }
public static QLog initial ( String [ ] args , int type ) { loggerType = type ; QConfig . cfg ( type ) . prepareCLI ( args ) ; final QLog log = LogerHolder . INSTANCE ; About . load ( ) ; QLog . l ( ) . logger . info ( "START LOGER. Logger: " + QLog . l ( ) . logger ( ) . getName ( ) ) ; code_block = IfStatement ; QLog . l ( ) . logger . info ( "Mode: " + ( QConfig . cfg ( ) . isDebug ( ) ? "KEY_DEBUG" : ( QConfig . cfg ( ) . isDemo ( ) ? "KEY_DEMO" : "FULL" ) ) ) ; QLog . l ( ) . logger . info ( "Plugins: " + ( QConfig . cfg ( ) . isNoPlugins ( ) ? "NO" : "YES" ) ) ; code_block = IfStatement ; return log ; }
public static QLog initial ( String [ ] args , int type ) { loggerType = type ; QConfig . cfg ( type ) . prepareCLI ( args ) ; final QLog log = LogerHolder . INSTANCE ; About . load ( ) ; QLog . l ( ) . logger . info ( "\"QSystem " + About . ver + "\"!  date: " + About . date ) ; code_block = IfStatement ; QLog . l ( ) . logger . info ( "Mode: " + ( QConfig . cfg ( ) . isDebug ( ) ? "KEY_DEBUG" : ( QConfig . cfg ( ) . isDemo ( ) ? "KEY_DEMO" : "FULL" ) ) ) ; QLog . l ( ) . logger . info ( "Mode: " + ( QConfig . cfg ( ) . isDebug ( ) ? "KEY_DEBUG" : ( QConfig . cfg ( ) . isDemo ( ) ? "KEY_DEMO" : "FULL" ) ) ) ; code_block = IfStatement ; return log ; }
public static QLog initial ( String [ ] args , int type ) { loggerType = type ; QConfig . cfg ( type ) . prepareCLI ( args ) ; final QLog log = LogerHolder . INSTANCE ; About . load ( ) ; QLog . l ( ) . logger . info ( "\"QSystem " + About . ver + "\"!  date: " + About . date ) ; QLog . l ( ) . logger . info ( "START LOGER. Logger: " + QLog . l ( ) . logger ( ) . getName ( ) ) ; code_block = IfStatement ; QLog . l ( ) . logger . info ( "Plugins: " + ( QConfig . cfg ( ) . isNoPlugins ( ) ? "NO" : "YES" ) ) ; code_block = IfStatement ; return log ; }
public static QLog initial ( String [ ] args , int type ) { loggerType = type ; QConfig . cfg ( type ) . prepareCLI ( args ) ; final QLog log = LogerHolder . INSTANCE ; About . load ( ) ; QLog . l ( ) . logger . info ( "\"QSystem " + About . ver + "\"!  date: " + About . date ) ; QLog . l ( ) . logger . info ( "START LOGER. Logger: " + QLog . l ( ) . logger ( ) . getName ( ) ) ; code_block = IfStatement ; QLog . l ( ) . logger . info ( "Mode: " + ( QConfig . cfg ( ) . isDebug ( ) ? "KEY_DEBUG" : ( QConfig . cfg ( ) . isDemo ( ) ? "KEY_DEMO" : "FULL" ) ) ) ; code_block = IfStatement ; return log ; }
public void test() { if ( QConfig . cfg ( ) . isUbtnStart ( ) ) { LOG . info ( "UbtnStart" ) ; } }
@ Test void withHeaderLoggerShouldBeUsedAsHeader ( ) throws IOException { Path targetFile = dir . homePath ( ) . resolve ( "debug.log" ) ; Path targetFile1 = dir . homePath ( ) . resolve ( "debug.log.1" ) ; ctx = LogConfig . createBuilder ( fs , targetFile , Level . INFO ) . withRotation ( 30 , 2 ) . withHeaderLogger ( log code_block = LoopStatement ; , "org.neo4j.HeaderClassName" ) . build ( ) ; assertThat ( fs . fileExists ( targetFile ) ) . isEqualTo ( true ) ; Logger logger = ctx . getLogger ( "className" ) ; logger . warn ( "test1" ) ; logger . warn ( "test2" ) ; assertThat ( fs . fileExists ( targetFile ) ) . isEqualTo ( true ) ; assertThat ( fs . fileExists ( targetFile1 ) ) . isEqualTo ( true ) ; assertThat ( Files . readString ( targetFile1 ) ) . matches ( DATE_PATTERN + format ( " %-5s \\[className] Long line that will get next message to be written to next file%n" , Level . WARN ) ) ; assertThat ( Files . readString ( targetFile ) ) . matches ( format ( DATE_PATTERN + " %-5s \\[o\\.n\\.HeaderClassName] My Header%n" + DATE_PATTERN + " %-5s \\[o\\.n\\.HeaderClassName] In Two lines%n" + DATE_PATTERN + " %-5s \\[className] test2%n" , Level . WARN , Level . WARN , Level . WARN , Level . WARN ) ) ; }
@ Test void withHeaderLoggerShouldBeUsedAsHeader ( ) throws IOException { Path targetFile = dir . homePath ( ) . resolve ( "debug.log" ) ; Path targetFile1 = dir . homePath ( ) . resolve ( "debug.log.1" ) ; ctx = LogConfig . createBuilder ( fs , targetFile , Level . INFO ) . withRotation ( 30 , 2 ) . withHeaderLogger ( log code_block = LoopStatement ; , "org.neo4j.HeaderClassName" ) . build ( ) ; assertThat ( fs . fileExists ( targetFile ) ) . isEqualTo ( true ) ; Logger logger = ctx . getLogger ( "className" ) ; logger . warn ( "ClassName" ) ; logger . warn ( "Long line that will get next message to be written to next file" ) ; assertThat ( fs . fileExists ( targetFile ) ) . isEqualTo ( true ) ; assertThat ( fs . fileExists ( targetFile1 ) ) . isEqualTo ( true ) ; assertThat ( Files . readString ( targetFile1 ) ) . matches ( DATE_PATTERN + format ( " %-5s \\[className] Long line that will get next message to be written to next file%n" , Level . WARN ) ) ; assertThat ( Files . readString ( targetFile ) ) . matches ( format ( DATE_PATTERN + " %-5s \\[o\\.n\\.HeaderClassName] My Header%n" + DATE_PATTERN + " %-5s \\[o\\.n\\.HeaderClassName] In Two lines%n" + DATE_PATTERN + " %-5s \\[className] test2%n" , Level . WARN , Level . WARN , Level . WARN ) ) ; }
public void test() { if ( isDebugEnabled_DLS ) { logger . trace ( LogMarker . DLS_VERBOSE , "[simpleDestroy]" ) ; } }
@ Override public void addParentProcedure ( String procedure , String parentProcedure ) { CacheValidation . notNullOrEmpty ( PROCEDURE , procedure ) ; CacheValidation . notNullOrEmpty ( PARENT_PROCEDURE , parentProcedure ) ; LOG . trace ( "Adding parent Procedure {}" , procedure ) ; this . parentProceduresForProcedures . computeIfAbsent ( procedure , createSynchronizedSet ( ) ) . add ( parentProcedure ) ; this . childProceduresForProcedures . computeIfAbsent ( parentProcedure , createSynchronizedSet ( ) ) . add ( procedure ) ; }
public void test() { try { String content = objectMapper . writeValueAsString ( result ) ; output . write ( content ) ; } catch ( JsonProcessingException e ) { log . error ( "Failed to write result to JSON" , e ) ; } }
public void test() { try { LOG . debug ( "Getting info for task(id={})" , taskAssignmentId ) ; code_block = IfStatement ; BoxTaskAssignment taskAssignment = new BoxTaskAssignment ( boxConnection , taskAssignmentId ) ; return taskAssignment . getInfo ( ) ; } catch ( BoxAPIException e ) { throw new RuntimeException ( String . format ( "Box API returned the error code %d%n%n%s" , e . getResponseCode ( ) , e . getResponse ( ) ) , e ) ; } }
public void test() { try { doSwitch ( current ) ; } catch ( Throwable t ) { String msg = "ClusterConfig changed listener error" ; logger . error ( msg , t ) ; throw new DalRuntimeException ( msg , t ) ; } }
public void test() { { LOGGER . info ( "Get customer identifier : {}" , id ) ; ParameterChecker . checkParameter ( "Identifier is mandatory : " , id ) ; return customerExternalService . getOne ( id ) ; } }
@ Test public void geodeLoggerDebugNotLoggedAfterRestoringLogLevelToDefault ( ) { when ( config . getLogLevel ( ) ) . thenReturn ( FINE . intLevel ( ) ) ; configuration . configChanged ( ) ; geodeConsoleAppender . clearLogEvents ( ) ; when ( config . getLogLevel ( ) ) . thenReturn ( CONFIG . intLevel ( ) ) ; configuration . configChanged ( ) ; logger . debug ( "Logging done." ) ; assertThat ( geodeConsoleAppender . getLogEvents ( ) ) . isEmpty ( ) ; }
@ RestAccessControl ( permission = Permission . MANAGE_USERS ) @ RequestMapping ( value = "/{target:.+}" , method = RequestMethod . DELETE , produces = MediaType . APPLICATION_JSON_VALUE ) public ResponseEntity < SimpleRestResponse < Map > > deleteUser ( @ ModelAttribute ( "user" ) UserDetails user , @ PathVariable String target , BindingResult bindingResult ) throws ApsSystemException { logger . debug ( "deleteUser {}" , target ) ; code_block = IfStatement ; code_block = IfStatement ; this . getUserService ( ) . removeUser ( target ) ; Map < String , String > result = new HashMap < > ( ) ; result . put ( "code" , target ) ; return new ResponseEntity < > ( new SimpleRestResponse < > ( result ) , HttpStatus . OK ) ; }
public void test() { if ( isTimeOut ( config . getResourceStoreReconnectTimeoutMs ( ) ) ) { logger . warn ( "Cluster connection store reconnect attempt. Retrying..." ) ; return false ; } }
public void test() { if ( config . isResourceStoreReconnectEnabled ( ) && store . isUnreachableException ( ex ) ) { code_block = IfStatement ; long waitMs = getSleepTimeMs ( ) ; long seconds = waitMs / 1000 ; code_block = TryStatement ;  increaseRetryCount ( ) ; logger . debug ( "Successfully connected to {}." , store ) ; return true ; } }
public void test() { if ( infoRequest . hasAdditionalParameters ( ) ) { infoRequest . parseAdditionalParameter ( parameters ) ; } else-if ( parameters . size ( ) < 1 ) { logger . debug ( "No additional parameters in request." ) ; } }
@ Override public void open ( ) throws NuvoException { logger . warn ( "{} opened" , this . getClass ( ) . getName ( ) ) ; setConnected ( false ) ; }
public void test() { try { code_block = IfStatement ; } catch ( SecurityException se ) { LOG . warn ( "Unexpected SecurityException [{}]" , se . getMessage ( ) ) ; } }
public void test() { if ( factory == null || ! isSipEnabled ( ) || ! r . isSipEnabled ( ) ) { LOG . debug ( "Sip is disabled, not using factory: " + factory ) ; return Optional . empty ( ) ; } }
public void test() { try { doClean ( ) ; } catch ( Exception e ) { log . debug ( "Failed to clean up" , e ) ; } }
public void test() { if ( printDoc ) { LOGGER . info ( "WFS GetFeature response:\n" + prettyString ( doc ) ) ; } }
public void test() { try { run0 ( ) ; } catch ( Exception e ) { log . error ( e ) ; } }
public void execute ( DelegateExecution execution ) { log . info ( "Entering DelegateExpressionBean.execute()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "Leaving DelegateExpressionBean.execute()" ) ; }
public void execute ( DelegateExecution execution ) { log . info ( "Entering DelegateExpressionBean.execute()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "Exiting DelegateExpressionBean.execute()" ) ; }
public static void printSizeStats ( String type , double density , long count , long totalBytes ) { logger . info ( type + " = " + density + " bytes" ) ; }
public void test() { switch ( p . getAction ( ) ) { case 1 : logger . trace ( "Turn pump control panel (ack) {}: {} - {}" , p . getSource ( ) , p . getByte ( PentairPacket . STARTOFDATA ) , p ) ; break ; case 4 : logger . trace ( "Turn pump control panel (ack) {}: {} - {}" , p . getSource ( ) , p . getByte ( PentairPacket . STARTOFDATA ) , p ) ; break ; case 5 : logger . trace ( "Set pump mode (ack) {}: {} - {}" , p . getSource ( ) , p . getByte ( PentairPacket . STARTOFDATA ) , p ) ; break ; case 6 : logger . trace ( "Set run mode (ack) {}: {} - {}" , p . getSource ( ) , p . getByte ( PentairPacket . STARTOFDATA ) , p ) ; break ; case 7 : code_block = IfStatement ; logger . debug ( "Pump status: {}" , p ) ; PentairPacketPumpStatus ppsOld = ppscur ; ppscur = new PentairPacketPumpStatus ( p ) ; updateChannel ( INTELLIFLO_RUN , ppsOld ) ; updateChannel ( INTELLIFLO_MODE , ppsOld ) ; updateChannel ( INTELLIFLO_DRIVESTATE , ppsOld ) ; updateChannel ( INTELLIFLO_POWER , ppsOld ) ; updateChannel ( INTELLIFLO_RPM , ppsOld ) ; updateChannel ( INTELLIFLO_PPC , ppsOld ) ; updateChannel ( INTELLIFLO_ERROR , ppsOld ) ; updateChannel ( INTELLIFLO_TIMER , ppsOld ) ; break ; default : logger . debug ( "Unhandled Intelliflo command: {}" , p . toString ( ) ) ; break ; } }
public void test() { switch ( p . getAction ( ) ) { case 1 : logger . trace ( "Pump command (ack): {}: " , p ) ; break ; case 4 : logger . trace ( "Pump command (ack): {}: " , p ) ; break ; case 5 : logger . trace ( "Set pump mode (ack) {}: {} - {}" , p . getSource ( ) , p . getByte ( PentairPacket . STARTOFDATA ) , p ) ; break ; case 6 : logger . trace ( "Set run mode (ack) {}: {} - {}" , p . getSource ( ) , p . getByte ( PentairPacket . STARTOFDATA ) , p ) ; break ; case 7 : code_block = IfStatement ; logger . debug ( "Pump status: {}" , p ) ; PentairPacketPumpStatus ppsOld = ppscur ; ppscur = new PentairPacketPumpStatus ( p ) ; updateChannel ( INTELLIFLO_RUN , ppsOld ) ; updateChannel ( INTELLIFLO_MODE , ppsOld ) ; updateChannel ( INTELLIFLO_DRIVESTATE , ppsOld ) ; updateChannel ( INTELLIFLO_POWER , ppsOld ) ; updateChannel ( INTELLIFLO_RPM , ppsOld ) ; updateChannel ( INTELLIFLO_PPC , ppsOld ) ; updateChannel ( INTELLIFLO_ERROR , ppsOld ) ; updateChannel ( INTELLIFLO_TIMER , ppsOld ) ; break ; default : logger . debug ( "Unhandled Intelliflo command: {}" , p . toString ( ) ) ; break ; } }
public void test() { switch ( p . getAction ( ) ) { case 1 : logger . trace ( "Pump command (ack): {}: " , p ) ; break ; case 4 : logger . trace ( "Turn pump control panel (ack) {}: {} - {}" , p . getSource ( ) , p . getByte ( PentairPacket . STARTOFDATA ) , p ) ; break ; case 5 : logger . trace ( "Turn pump control panel (ack) {}: {}" , p . getSource ( ) , p . getByte ( PentairPacket . STARTOFDATA ) ) ; break ; case 6 : logger . trace ( "Set run mode (ack) {}: {} - {}" , p . getSource ( ) , p . getByte ( PentairPacket . STARTOFDATA ) , p ) ; break ; case 7 : code_block = IfStatement ; logger . debug ( "Pump status: {}" , p ) ; PentairPacketPumpStatus ppsOld = ppscur ; ppscur = new PentairPacketPumpStatus ( p ) ; updateChannel ( INTELLIFLO_RUN , ppsOld ) ; updateChannel ( INTELLIFLO_MODE , ppsOld ) ; updateChannel ( INTELLIFLO_DRIVESTATE , ppsOld ) ; updateChannel ( INTELLIFLO_POWER , ppsOld ) ; updateChannel ( INTELLIFLO_RPM , ppsOld ) ; updateChannel ( INTELLIFLO_PPC , ppsOld ) ; updateChannel ( INTELLIFLO_ERROR , ppsOld ) ; updateChannel ( INTELLIFLO_TIMER , ppsOld ) ; break ; default : logger . debug ( "Unhandled Intelliflo command: {}" , p . toString ( ) ) ; break ; } }
public void test() { switch ( p . getAction ( ) ) { case 1 : logger . trace ( "Pump command (ack): {}: " , p ) ; break ; case 4 : logger . trace ( "Turn pump control panel (ack) {}: {} - {}" , p . getSource ( ) , p . getByte ( PentairPacket . STARTOFDATA ) , p ) ; break ; case 5 : logger . trace ( "Set pump mode (ack) {}: {} - {}" , p . getSource ( ) , p . getByte ( PentairPacket . STARTOFDATA ) , p ) ; break ; case 6 : logger . debug ( "Set pump mode (ack) {}: {}" , p . getSource ( ) , p . getByte ( PentairPacket . STARTOFDATA ) ) ; break ; case 7 : code_block = IfStatement ; logger . debug ( "Pump status: {}" , p ) ; PentairPacketPumpStatus ppsOld = ppscur ; ppscur = new PentairPacketPumpStatus ( p ) ; updateChannel ( INTELLIFLO_RUN , ppsOld ) ; updateChannel ( INTELLIFLO_MODE , ppsOld ) ; updateChannel ( INTELLIFLO_DRIVESTATE , ppsOld ) ; updateChannel ( INTELLIFLO_POWER , ppsOld ) ; updateChannel ( INTELLIFLO_RPM , ppsOld ) ; updateChannel ( INTELLIFLO_PPC , ppsOld ) ; updateChannel ( INTELLIFLO_ERROR , ppsOld ) ; updateChannel ( INTELLIFLO_TIMER , ppsOld ) ; break ; default : logger . debug ( "Unhandled Intelliflo command: {}" , p . toString ( ) ) ; break ; } }
public void test() { if ( p . getLength ( ) != 15 ) { log . error ( "Invalid offset: " + p . getLength ( ) ) ; return ; } }
public void test() { switch ( p . getAction ( ) ) { case 1 : logger . trace ( "Pump command (ack): {}: " , p ) ; break ; case 4 : logger . trace ( "Turn pump control panel (ack) {}: {} - {}" , p . getSource ( ) , p . getByte ( PentairPacket . STARTOFDATA ) , p ) ; break ; case 5 : logger . trace ( "Set pump mode (ack) {}: {} - {}" , p . getSource ( ) , p . getByte ( PentairPacket . STARTOFDATA ) , p ) ; break ; case 6 : logger . trace ( "Set run mode (ack) {}: {} - {}" , p . getSource ( ) , p . getByte ( PentairPacket . STARTOFDATA ) , p ) ; break ; case 7 : code_block = IfStatement ; PentairPacketPumpStatus ppsOld = ppscur ; ppscur = new PentairPacketPumpStatus ( p ) ; updateChannel ( INTELLIFLO_RUN , ppsOld ) ; updateChannel ( INTELLIFLO_MODE , ppsOld ) ; updateChannel ( INTELLIFLO_DRIVESTATE , ppsOld ) ; updateChannel ( INTELLIFLO_POWER , ppsOld ) ; updateChannel ( INTELLIFLO_RPM , ppsOld ) ; updateChannel ( INTELLIFLO_PPC , ppsOld ) ; updateChannel ( INTELLIFLO_ERROR , ppsOld ) ; updateChannel ( INTELLIFLO_TIMER , ppsOld ) ; logger . trace ( "Pump command (ack): {}: " , p ) ; break ; default : logger . debug ( "Unhandled Intelliflo command: {}" , p . toString ( ) ) ; break ; } }
public void test() { switch ( p . getAction ( ) ) { case 1 : logger . trace ( "Pump command (ack): {}: " , p ) ; break ; case 4 : logger . trace ( "Turn pump control panel (ack) {}: {} - {}" , p . getSource ( ) , p . getByte ( PentairPacket . STARTOFDATA ) , p ) ; break ; case 5 : logger . trace ( "Set pump mode (ack) {}: {} - {}" , p . getSource ( ) , p . getByte ( PentairPacket . STARTOFDATA ) , p ) ; break ; case 6 : logger . trace ( "Set run mode (ack) {}: {} - {}" , p . getSource ( ) , p . getByte ( PentairPacket . STARTOFDATA ) , p ) ; break ; case 7 : code_block = IfStatement ; logger . debug ( "Pump status: {}" , p ) ; PentairPacketPumpStatus ppsOld = ppscur ; ppscur = new PentairPacketPumpStatus ( p ) ; updateChannel ( INTELLIFLO_RUN , ppsOld ) ; updateChannel ( INTELLIFLO_MODE , ppsOld ) ; updateChannel ( INTELLIFLO_DRIVESTATE , ppsOld ) ; updateChannel ( INTELLIFLO_POWER , ppsOld ) ; updateChannel ( INTELLIFLO_RPM , ppsOld ) ; updateChannel ( INTELLIFLO_PPC , ppsOld ) ; updateChannel ( INTELLIFLO_ERROR , ppsOld ) ; updateChannel ( INTELLIFLO_TIMER , ppsOld ) ; break ; default : logger . debug ( "Unhandled action: {}" , p ) ; break ; } }
public void setBundleContext ( BundleContext bundleContext ) throws Exception { code_block = IfStatement ; logger . debug ( "Installing OSGIObjectFactory" ) ; objectFactory = new OSGIObjectFactory ( bundleContext ) ; PentahoSystem . registerObjectFactory ( objectFactory ) ; PentahoSystem . setBundleContext ( bundleContext ) ; logger . debug ( "OSGIObjectFactory installed" ) ; }
public void setBundleContext ( BundleContext bundleContext ) throws Exception { logger . debug ( "Registering OSGIObjectFactory" ) ; code_block = IfStatement ; objectFactory = new OSGIObjectFactory ( bundleContext ) ; PentahoSystem . registerObjectFactory ( objectFactory ) ; PentahoSystem . setBundleContext ( bundleContext ) ; logger . debug ( "Bundle context created" ) ; }
public void test() { try { return authenticate ( em , request ) ; } catch ( SecurityException e ) { logger . debug ( "Authentication failed" , e ) ; throw new AuthenticationFailedException ( ) ; } finally { em . close ( ) ; } }
public void test() { if ( needToPrintStackTrace ( ) ) { logger . warn ( format , args , args ) ; } else { logger . warn ( format , args ) ; } }
public void test() { if ( needToPrintStackTrace ( ) ) { logger . warn ( STACK_TRACE_LOGGER_PREFIX + format , buildNewArgs ( args , cause ) ) ; } else { logger . warn ( format , buildNewArgs ( args , cause ) ) ; } }
@ Override public void runJob ( Path input , Path output , BayesParameters params ) throws IOException { Configurable client = new JobClient ( ) ; JobConf conf = new JobConf ( BayesWeightSummerDriver . class ) ; conf . setJobName ( "TfIdf Driver running over input: " + input ) ; conf . setOutputKeyClass ( StringTuple . class ) ; conf . setOutputValueClass ( DoubleWritable . class ) ; FileInputFormat . addInputPath ( conf , new Path ( output , "trainer-termDocCount" ) ) ; FileInputFormat . addInputPath ( conf , new Path ( output , "trainer-wordFreq" ) ) ; FileInputFormat . addInputPath ( conf , new Path ( output , "trainer-featureCount" ) ) ; Path outPath = new Path ( output , "trainer-tfIdf" ) ; FileOutputFormat . setOutputPath ( conf , outPath ) ; conf . setJarByClass ( BayesTfIdfDriver . class ) ; conf . setMapperClass ( BayesTfIdfMapper . class ) ; conf . setInputFormat ( SequenceFileInputFormat . class ) ; conf . setCombinerClass ( BayesTfIdfReducer . class ) ; conf . setReducerClass ( BayesTfIdfReducer . class ) ; conf . setOutputFormat ( BayesTfIdfOutputFormat . class ) ; conf . set ( "io.serializations" , "org.apache.hadoop.io.serializer.JavaSerialization," + "org.apache.hadoop.io.serializer.WritableSerialization" ) ; HadoopUtil . delete ( conf , outPath ) ; Path interimFile = new Path ( output , "trainer-docCount/part-*" ) ; Map < String , Double > labelDocumentCounts = SequenceFileModelReader . readLabelDocumentCounts ( interimFile , conf ) ; DefaultStringifier < Map < String , Double > > mapStringifier = new DefaultStringifier < Map < String , Double > > ( conf , GenericsUtil . getClass ( labelDocumentCounts ) ) ; String labelDocumentCountString = mapStringifier . toString ( labelDocumentCounts ) ; Map < String , Double > c = mapStringifier . fromString (
@ Override public void runJob ( Path input , Path output , BayesParameters params ) throws IOException { Configurable client = new JobClient ( ) ; JobConf conf = new JobConf ( BayesWeightSummerDriver . class ) ; conf . setJobName ( "TfIdf Driver running over input: " + input ) ; conf . setOutputKeyClass ( StringTuple . class ) ; conf . setOutputValueClass ( DoubleWritable . class ) ; FileInputFormat . addInputPath ( conf , new Path ( output , "trainer-termDocCount" ) ) ; FileInputFormat . addInputPath ( conf , new Path ( output , "trainer-wordFreq" ) ) ; FileInputFormat . addInputPath ( conf , new Path ( output , "trainer-featureCount" ) ) ; Path outPath = new Path ( output , "trainer-tfIdf" ) ; FileOutputFormat . setOutputPath ( conf , outPath ) ; conf . setJarByClass ( BayesTfIdfDriver . class ) ; conf . setMapperClass ( BayesTfIdfMapper . class ) ; conf . setInputFormat ( SequenceFileInputFormat . class ) ; conf . setCombinerClass ( BayesTfIdfReducer . class ) ; conf . setReducerClass ( BayesTfIdfReducer . class ) ; conf . setOutputFormat ( BayesTfIdfOutputFormat . class ) ; conf . set ( "io.serializations" , "org.apache.hadoop.io.serializer.JavaSerialization," + "org.apache.hadoop.io.serializer.WritableSerialization" ) ; HadoopUtil . delete ( conf , outPath ) ; Path interimFile = new Path ( output , "trainer-docCount/part-*" ) ; Map < String , Double > labelDocumentCounts = SequenceFileModelReader . readLabelDocumentCounts ( interimFile , conf ) ; DefaultStringifier < Map < String , Double > > mapStringifier = new DefaultStringifier < Map < String , Double > > ( conf , GenericsUtil . getClass ( labelDocumentCounts ) ) ; String labelDocumentCountString = mapStringifier . toString ( labelDocumentCounts ) ; log . info ( "Counts of documents in Each Label" ) ;
@ Override public void runJob ( Path input , Path output , BayesParameters params ) throws IOException { Configurable client = new JobClient ( ) ; JobConf conf = new JobConf ( BayesWeightSummerDriver . class ) ; conf . setJobName ( "TfIdf Driver running over input: " + input ) ; conf . setOutputKeyClass ( StringTuple . class ) ; conf . setOutputValueClass ( DoubleWritable . class ) ; FileInputFormat . addInputPath ( conf , new Path ( output , "trainer-termDocCount" ) ) ; FileInputFormat . addInputPath ( conf , new Path ( output , "trainer-wordFreq" ) ) ; FileInputFormat . addInputPath ( conf , new Path ( output , "trainer-featureCount" ) ) ; Path outPath = new Path ( output , "trainer-tfIdf" ) ; FileOutputFormat . setOutputPath ( conf , outPath ) ; conf . setJarByClass ( BayesTfIdfDriver . class ) ; conf . setMapperClass ( BayesTfIdfMapper . class ) ; conf . setInputFormat ( SequenceFileInputFormat . class ) ; conf . setCombinerClass ( BayesTfIdfReducer . class ) ; conf . setReducerClass ( BayesTfIdfReducer . class ) ; conf . setOutputFormat ( BayesTfIdfOutputFormat . class ) ; conf . set ( "io.serializations" , "org.apache.hadoop.io.serializer.JavaSerialization," + "org.apache.hadoop.io.serializer.WritableSerialization" ) ; HadoopUtil . delete ( conf , outPath ) ; Path interimFile = new Path ( output , "trainer-docCount/part-*" ) ; Map < String , Double > labelDocumentCounts = SequenceFileModelReader . readLabelDocumentCounts ( interimFile , conf ) ; DefaultStringifier < Map < String , Double > > mapStringifier = new DefaultStringifier < Map < String , Double > > ( conf , GenericsUtil . getClass ( labelDocumentCounts ) ) ; String labelDocumentCountString = mapStringifier . toString ( labelDocumentCounts ) ; log . info ( "Counts of documents in Each Label" ) ;
public void test() { for ( String message : exceptionMessages ) { LOG . warn ( message ) ; } }
public void test() { if ( ! exceptionMessages . isEmpty ( ) ) { code_block = ForStatement ; } else { LOG . info ( "Successfully wrote exception {}" , exceptionMessages ) ; } }
public void test() { if ( user != null && ! user . isClosed ( ) ) { log . debug ( "User {} is closed" , user ) ; } }
public void test() { try ( final Tx tx = app . tx ( ) ) { final File file = app . get ( File . class , objectId ) ; code_block = IfStatement ; tx . success ( ) ; } catch ( Throwable t ) { logger . warn ( "" , t ) ; } }
public void test() { try { Optional < List < Field > > fields = StatUtil . getFieldsWithManualHistogram ( statContainer , type ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Error occurred while trying to calculate statistics for Index: {}, Type: {}" , index , type ) ; logger . error ( "Exception occurred while trying to calculate statistics for Index: {}, Type: {}" , index , type , e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( AssetVocabularyServiceUtil . class , "addVocabulary" , _addVocabularyParameterTypes1 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , title , titleMap , descriptionMap , settings , serviceContext ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . asset . kernel . model . AssetVocabulary ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( cellBaseDataResultList . get ( i ) . getResults ( ) != null && cellBaseDataResultList . get ( i ) . getResults ( ) . size ( ) > 0 ) { code_block = IfStatement ; } else { LOGGER . info ( "CellBaseDataResultList is null" ) ; } }
public void test() { if ( command instanceof RefreshType ) { logger . debug ( "Refreshing channel {}" , channelUID ) ; } else { code_block = IfStatement ; } }
public void test() { switch ( reorderCode . toLowerCase ( ) ) { case "default" : return Collator . ReorderCodes . DEFAULT ; case "none" : return Collator . ReorderCodes . NONE ; case "others" : return Collator . ReorderCodes . OTHERS ; case "space" : return Collator . ReorderCodes . SPACE ; case "first" : return Collator . ReorderCodes . FIRST ; case "punctuation" : return Collator . ReorderCodes . PUNCTUATION ; case "symbol" : return Collator . ReorderCodes . SYMBOL ; case "currency" : return Collator . ReorderCodes . CURRENCY ; case "digit" : return Collator . ReorderCodes . DIGIT ; default : log . error ( "Invalid Reorder code '" + reorderCode + "'" ) ; return - 1 ; } }
public void test() { if ( this . spoutMap . get ( id ) == null || override ) { this . spoutMap . put ( spout . getId ( ) , spout ) ; } else { log . info ( "Spout already registered with id {}" , id ) ; } }
public void test() { try { result . append ( runTransaction ( clientSession -> privateUpdate ( clientSession , family , parameters , variableSetList , queryOptions ) ) ) ; } catch ( CatalogDBException | CatalogParameterException | CatalogAuthorizationException e ) { logger . error ( "Could not update family {}: {}" , family . getId ( ) , e . getMessage ( ) , e ) ; result . getEvents ( ) . add ( new Event ( Event . Type . ERROR , family . getId ( ) , e . getMessage ( ) ) ) ; result . setNumMatches ( result . getNumMatches ( ) + 1 ) ; } }
public Set < State > processEvent ( Event event , String stateMachineInstanceId ) { StateMachine stateMachine = null ; stateMachine = stateMachinesDAO . findById ( stateMachineInstanceId ) ; code_block = IfStatement ; Context context = new RAMContext ( System . currentTimeMillis ( ) , null , stateMachine ) ; final Set < State > dependantStates = context . getDependantStates ( event . getName ( ) ) ; Set < State > executableStates = getExecutableStates ( dependantStates , stateMachine . getId ( ) ) ; logger . debug ( "The state of {} has been unblocked after event {}" , stateMachine . getId ( ) , event . getName ( ) ) ; logger . debug ( "These states {} are now unblocked after event {}" , executableStates , event . getName ( ) ) ; executeStates ( stateMachine , executableStates , event , false ) ; return executableStates ; }
public Set < State > processEvent ( Event event , String stateMachineInstanceId ) { StateMachine stateMachine = null ; stateMachine = stateMachinesDAO . findById ( stateMachineInstanceId ) ; code_block = IfStatement ; logger . debug ( "Processing event for state machine {}" , stateMachineInstanceId ) ; Context context = new RAMContext ( System . currentTimeMillis ( ) , null , stateMachine ) ; final Set < State > dependantStates = context . getDependantStates ( event . getName ( ) ) ; logger . debug ( "These states {} depend on event {}" , dependantStates , event . getName ( ) ) ; Set < State > executableStates = getExecutableStates ( dependantStates , stateMachine . getId ( ) ) ; executeStates ( stateMachine , executableStates , event , false ) ; return executableStates ; }
public void test() { if ( realmInRawPrincipal ) { identity = rawPrincipal ; logger . debug ( "Realm was specified in principal {}, default realm was not added to the identity being authenticated" , rawPrincipal ) ; } else-if ( StringUtils . isNotBlank ( defaultRealm ) ) { identity = StringUtils . joinWith ( "@" , rawPrincipal , defaultRealm ) ; logger . debug ( "Realm was not specified in principal {}, default realm {} was added to the identity being authenticated" , rawPrincipal , defaultRealm ) ; } else { identity = rawPrincipal ; logger . debug ( "Realm was not specified in principal {}, default realm is blank and was not added to the identity being authenticated" , rawPrincipal ) ; } }
public void test() { if ( realmInRawPrincipal ) { identity = rawPrincipal ; logger . debug ( "Realm was specified in principal {}, default realm was not added to the identity being authenticated" , rawPrincipal ) ; } else-if ( StringUtils . isNotBlank ( defaultRealm ) ) { identity = StringUtils . joinWith ( "@" , rawPrincipal , defaultRealm ) ; logger . debug ( "Realm was not specified in principal {}, default realm {} was added to the identity being authenticated" , rawPrincipal , defaultRealm ) ; } else { identity = rawPrincipal ; logger . debug ( "Realm was not specified in principal {}, default realm is blank and was not added to the identity being authenticated" , rawPrincipal ) ; } }
public void test() { if ( realmInRawPrincipal ) { identity = rawPrincipal ; logger . debug ( "Realm was specified in principal {}, default realm was not added to the identity being authenticated" , rawPrincipal ) ; } else-if ( StringUtils . isNotBlank ( defaultRealm ) ) { identity = StringUtils . joinWith ( "@" , rawPrincipal , defaultRealm ) ; logger . debug ( "Realm was not specified in principal {}, default realm {} was added to the identity being authenticated" , rawPrincipal , defaultRealm ) ; } else { identity = rawPrincipal ; logger . debug ( "Realm was not specified in principal {}, default realm is blank and was not added to the identity being authenticated" , rawPrincipal ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
@ Override public final void onCachePeriodChanged ( final long period ) { LOG . info ( "onCachePeriodChanged with value {} has been triggered!" , period ) ; cacheSchedulerHelper . scheduleWithPeriod ( period ) ; cacheStrategy . clear ( ) ; }
@ Test public void listAllPreparedQueriesByUser ( ) throws Exception { String user = "diff" , pass = "diff" ; String session1 = sHelper . openSession ( user , pass , lens . getCurrentDB ( ) ) ; QueryPrepareHandle queryPrepareHandle1 = qHelper . submitPreparedQuery ( QueryInventory . QUERY ) ; Assert . assertNotEquals ( queryPrepareHandle1 , null , "Query Execute Failed" ) ; logger . info ( "PREPARE QUERY HANDLE : " + queryPrepareHandle1 ) ; QueryPrepareHandle queryPrepareHandle2 = qHelper . submitPreparedQuery ( QueryInventory . QUERY , null , session1 ) ; Assert . assertNotEquals ( queryPrepareHandle2 , null , "Query Execute Failed" ) ; logger . info ( "PREPARE QUERY HANDLE : " + queryPrepareHandle2 ) ; List < QueryPrepareHandle > list = qHelper . getPreparedQueryHandleList ( null , lens . getUserName ( ) ) ; Assert . assertTrue ( list . contains ( queryPrepareHandle1 ) , "List of All QueryPreparedHandle By user failed" ) ; list = qHelper . getPreparedQueryHandleList ( null , user ) ; Assert . assertTrue ( list . contains ( queryPrepareHandle2 ) , "List of All QueryPreparedHandle By user failed" ) ; list = qHelper . getPreparedQueryHandleList ( null , "all" ) ; Assert . assertTrue ( list . contains ( queryPrepareHandle1 ) , "List of All QueryPreparedHandle by 'all' user failed" ) ; Assert . assertTrue ( list . contains ( queryPrepareHandle2 ) , "List of All QueryPreparedHandle by 'all' user failed" ) ; }
@ Test public void listAllPreparedQueriesByUser ( ) throws Exception { String user = "diff" , pass = "diff" ; String session1 = sHelper . openSession ( user , pass , lens . getCurrentDB ( ) ) ; logger . info ( "PREPARE QUERY HANDLE : " + session1 ) ; QueryPrepareHandle queryPrepareHandle1 = qHelper . submitPreparedQuery ( QueryInventory . QUERY ) ; Assert . assertNotEquals ( queryPrepareHandle1 , null , "Query Execute Failed" ) ; logger . info ( "PREPARE QUERY HANDLE : " + queryPrepareHandle1 ) ; QueryPrepareHandle queryPrepareHandle2 = qHelper . submitPreparedQuery ( QueryInventory . QUERY , null , session1 ) ; Assert . assertNotEquals ( queryPrepareHandle2 , null , "Query Execute Failed" ) ; List < QueryPrepareHandle > list = qHelper . getPreparedQueryHandleList ( null , lens . getUserName ( ) ) ; Assert . assertTrue ( list . contains ( queryPrepareHandle1 ) , "List of All QueryPreparedHandle By user failed" ) ; list = qHelper . getPreparedQueryHandleList ( null , user ) ; Assert . assertTrue ( list . contains ( queryPrepareHandle2 ) , "List of All QueryPreparedHandle By user failed" ) ; list = qHelper . getPreparedQueryHandleList ( null , "all" ) ; Assert . assertTrue ( list . contains ( queryPrepareHandle1 ) , "List of All QueryPreparedHandle by 'all' user failed" ) ; Assert . assertTrue ( list . contains ( queryPrepareHandle2 ) , "List of All QueryPreparedHandle by 'all' user failed" ) ; }
public void test() { try { tempDir = FileTools . createDirectoryWithUniqueName ( new File ( this . bundleDir ) , "wokflow_zip_temp" ) ; } catch ( Exception e ) { Log . error ( "Cannot create temporary directory: " + this . bundleDir , e ) ; ret . setExitStatus ( ReturnValue . FAILURE ) ; return ret ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { DatabaseMetaData metaData = connection . getMetaData ( ) ; String dbProduct = metaData . getDatabaseProductName ( ) ; dialect = identifyDialect ( dbProduct ) ; code_block = IfStatement ; code_block = IfStatement ; } catch ( Exception e ) { LOGGER . error ( e . getMessage ( ) , e ) ; } }
public void delete ( FilterCol persistentInstance ) { log . debug ( "deleting FilterCol instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . delete ( persistentInstance ) ; log . debug ( "delete successful" ) ; } catch ( RuntimeException re ) { log . error ( "delete failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . delete ( persistentInstance ) ; log . debug ( "delete successful" ) ; } catch ( RuntimeException re ) { log . error ( "delete failed" , re ) ; throw re ; } }
public List findByExample ( NZobSb instance ) { log . debug ( "finding NZobSb instance by example" ) ; code_block = TryStatement ;  }
public void test() { try { List results = getSession ( ) . createCriteria ( "sernet.gs.reveng.NZobSb" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
public void test() { try { List results = getSession ( ) . createCriteria ( "sernet.gs.reveng.NZobSb" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
public HostnameVerifier verifierFor ( String hostname ) { code_block = IfStatement ; final String tHostname = hostname . endsWith ( "." ) ? hostname . substring ( 0 , hostname . length ( ) - 1 ) : hostname ; LOGGER . debug ( "verify verifier for {}" , tHostname ) ; return new HostnameVerifier ( ) code_block = "" ; ; }
public void test() { try { code_block = ForStatement ; } catch ( Exception e ) { logger . error ( Messages . getInstance ( ) . getErrorString ( "Workspace.ERROR_0002_PROPS_EXCEPTION" ) , e ) ; } }
public void test() { try { final List < User > userList = userSystemService . findAllUsers ( ) ; final int userListSize = userList != null ? userList . size ( ) : 0 ; LOGGER . debug ( "Users information was loaded successful: {} users were returned from external system, next synchronization will be in a period of {}" , userListSize , usersSyncInterval ) ; nextUsersSyncTime = calculateNextUsersSyncTime ( ) ; LOGGER . debug ( "Next user list size: {}" , nextUsersSyncTime ) ; return Pair . of ( true , userList ) ; } catch ( Exception e ) { final String msg = String . format ( "An error was produced during users information loading from the external UserSystem repository." + " Tasks status will still be updated and users synchronization next attempt will be in a period of %s, error: %s" , syncInterval , e . getMessage ( ) ) ; LOGGER . warn ( msg ) ; LOGGER . debug ( msg , e ) ; return Pair . of ( false , Collections . emptyList ( ) ) ; } }
public void test() { try { LOGGER . debug ( "Loading users information from the external UserSystemService" ) ; final List < User > userList = userSystemService . findAllUsers ( ) ; final int userListSize = userList != null ? userList . size ( ) : 0 ; LOGGER . debug ( "User list size: {}" , userListSize ) ; nextUsersSyncTime = calculateNextUsersSyncTime ( ) ; return Pair . of ( true , userList ) ; } catch ( Exception e ) { final String msg = String . format ( "An error was produced during users information loading from the external UserSystem repository." + " Tasks status will still be updated and users synchronization next attempt will be in a period of %s, error: %s" , syncInterval , e . getMessage ( ) ) ; LOGGER . warn ( msg ) ; LOGGER . debug ( msg , e ) ; return Pair . of ( false , Collections . emptyList ( ) ) ; } }
public void test() { try { LOGGER . debug ( "Loading users information from the external UserSystemService" ) ; final List < User > userList = userSystemService . findAllUsers ( ) ; final int userListSize = userList != null ? userList . size ( ) : 0 ; LOGGER . debug ( "Users information was loaded successful: {} users were returned from external system, next synchronization will be in a period of {}" , userListSize , usersSyncInterval ) ; nextUsersSyncTime = calculateNextUsersSyncTime ( ) ; LOGGER . debug ( "Next user list size: {}" , nextUsersSyncTime ) ; return Pair . of ( true , userList ) ; } catch ( Exception e ) { final String msg = String . format ( "An error was produced during users information loading from the external UserSystem repository." + " Tasks status will still be updated and users synchronization next attempt will be in a period of %s, error: %s" , syncInterval , e . getMessage ( ) ) ; LOGGER . debug ( msg , e ) ; return Pair . of ( false , Collections . emptyList ( ) ) ; } }
public void test() { try { LOGGER . debug ( "Loading users information from the external UserSystemService" ) ; final List < User > userList = userSystemService . findAllUsers ( ) ; final int userListSize = userList != null ? userList . size ( ) : 0 ; LOGGER . debug ( "Users information was loaded successful: {} users were returned from external system, next synchronization will be in a period of {}" , userListSize , usersSyncInterval ) ; nextUsersSyncTime = calculateNextUsersSyncTime ( ) ; LOGGER . debug ( "Next user list size: {}" , nextUsersSyncTime ) ; return Pair . of ( true , userList ) ; } catch ( Exception e ) { final String msg = String . format ( "An error was produced during users information loading from the external UserSystem repository." + " Tasks status will still be updated and users synchronization next attempt will be in a period of %s, error: %s" , syncInterval , e . getMessage ( ) ) ; LOGGER . warn ( msg ) ; return Pair . of ( false , Collections . emptyList ( ) ) ; } }
@ Override public void releaseAddress ( String ip ) { logger . info ( "Releasing ip {}" , ip ) ; }
public void test() { try { URIResolver uriResolver = new URIResolver ( ) ; uriResolver . resolve ( "" , wsdlLocation , this . getClass ( ) ) ; wsdlURL = uriResolver . isResolved ( ) ? uriResolver . getURL ( ) : new URL ( wsdlLocation ) ; LOGGER . debug ( "Service created" ) ; service = AccessController . doPrivileged ( ( PrivilegedAction < Service > ) ( ) -> Service . create ( wsdlURL , QName . valueOf ( serviceName ) ) ) ; auditRemoteConnection ( wsdlURL ) ; } catch ( Exception e ) { LOGGER . debug ( "Unable to create service from WSDL location." , e ) ; } }
public void test() { try { URIResolver uriResolver = new URIResolver ( ) ; uriResolver . resolve ( "" , wsdlLocation , this . getClass ( ) ) ; wsdlURL = uriResolver . isResolved ( ) ? uriResolver . getURL ( ) : new URL ( wsdlLocation ) ; service = AccessController . doPrivileged ( ( PrivilegedAction < Service > ) ( ) -> Service . create ( wsdlURL , QName . valueOf ( serviceName ) ) ) ; auditRemoteConnection ( wsdlURL ) ; } catch ( Exception e ) { LOGGER . info ( "Unable to create service from WSDL location. Set log level for \"org.codice.ddf.security.claims.attributequery.common\" to DEBUG for more information." ) ; LOGGER . debug ( "" , e ) ; } }
@ ParallelTest void testMirrorMakerLogSetting ( ExtensionContext extensionContext ) { String clusterName = mapWithClusterNames . get ( extensionContext . getDisplayName ( ) ) ; String mirrorMakerName = clusterName + "-mirror-maker" ; resourceManager . createResource ( extensionContext , KafkaMirrorMakerTemplates . kafkaMirrorMaker ( mirrorMakerName , LOG_SETTING_CLUSTER_NAME , GC_LOGGING_SET_NAME , "my-group" , 1 , false ) . editMetadata ( ) . withNamespace ( NAMESPACE ) . endMetadata ( ) . editSpec ( ) . withNewInlineLogging ( ) . withLoggers ( MIRROR_MAKER_LOGGERS ) . endInlineLogging ( ) . withNewJvmOptions ( ) . withGcLoggingEnabled ( true ) . endJvmOptions ( ) . endSpec ( ) . build ( ) ) ; String mmDepName = KafkaMirrorMakerResources . deploymentName ( mirrorMakerName ) ; Map < String , String > mmPods = DeploymentUtils . depSnapshot ( NAMESPACE , mmDepName ) ; String mirrorMakerMap = KafkaMirrorMakerResources . metricsAndLogConfigMapName ( mirrorMakerName ) ; assertThat ( "KafkaMirrorMaker's log level is set properly" , checkLoggersLevel ( NAMESPACE , MIRROR_MAKER_LOGGERS , mirrorMakerMap ) , is ( true ) ) ; assertThat ( "Mirror-maker GC logging is enabled" , checkGcLoggingDeployments ( NAMESPACE , mmDepName ) , is ( true ) ) ; KafkaMirrorMakerResource . replaceMirrorMakerResourceInSpecificNamespace ( mirrorMakerName , mm -> mm . getSpec ( ) . setJvmOptions ( JVM_OPTIONS ) , NAMESPACE ) ; DeploymentUtils . waitTillDepHasRolled ( NAMESPACE , mmDepName , 1 , mmPods ) ; assertThat ( "Mirror-maker GC logging is disabled" , checkGcLoggingDeployments ( NAMESPACE , mmDepName ) , is ( false ) ) ; kubectlGetStrimziUntilOperationIsSuccessful ( NAMESPACE , mirrorMakerName ) ; checkContrimzi
protected void logException ( Exception e , String message ) { getProcessLogger ( ) . error ( message , e ) ; getProcessLogger ( ) . error ( message ) ; }
public void test() { if ( response . getStatus ( ) != ClientResponse . Status . OK . getStatusCode ( ) ) { LOGGER . log ( Level . FINE , "Could not create an OK response" ) ; } }
public void test() { try { ClientResponse response = authSvcItr . post ( _URI_RELOAD , null ) ; code_block = IfStatement ; } catch ( Exception e ) { _log . error ( e ) ; } }
public void test() { try { AuthSvcInternalApiClientIterator authSvcItr = new AuthSvcInternalApiClientIterator ( _authSvcEndPointLocator , _coordinator ) ; code_block = WhileStatement ; } catch ( CoordinatorException e ) { LOG . error ( "Error while processing auth service" , e ) ; } }
@ Override public void add ( IRingSet ringSet ) { logger . debug ( "Adding " + ringSet . toString ( ) ) ; super . add ( ringSet ) ; }
public void test() { try { conn = dataSource . getConnection ( ) ; stmt = conn . prepareStatement ( getDiffsAffectedByUserSQL , ResultSet . TYPE_FORWARD_ONLY , ResultSet . CONCUR_READ_ONLY ) ; stmt . setFetchDirection ( ResultSet . FETCH_FORWARD ) ; stmt . setFetchSize ( getFetchSize ( ) ) ; stmt . setLong ( 1 , userID ) ; log . debug ( "Executing SQL query: {}" , getDiffsAffectedByUserSQL ) ; rs = stmt . executeQuery ( ) ; code_block = WhileStatement ; } catch ( SQLException sqle ) { log . warn ( "Exception while updating record" , sqle ) ; throw new TasteException ( sqle ) ; } finally { IOUtils . quietClose ( rs , stmt , conn ) ; } }
public void test() { try { size = byteSource . size ( ) ; } catch ( IOException e ) { LOG . error ( e ) ; } }
@ Override @ Deprecated @ SuppressFBWarnings ( "SLF4J_SIGN_ONLY_FORMAT" ) public void onLinkRemoved ( final LinkRemoved notification ) { LOG . debug ( "-------------------------------------------" ) ; LOG . debug ( "Remove link removed" ) ; LOG . debug ( "-------------------------------------------" ) ; }
public void test() { if ( ClassUtils . isAssignable ( functionClass , TestFunction . class ) ) { logger . debug ( "Found evaluator {} for TestFunction {}" , clazz . getCanonicalName ( ) , functionClass . getCanonicalName ( ) ) ; testFunctionEvaluators . put ( ( Class < ? extends TestFunction > ) functionClass , clazz ) ; } else-if ( ClassUtils . isAssignable ( functionClass , NodeTest . class ) ) { logger . debug ( "Found evaluator {} for NodeFunction {}" , clazz . getCanonicalName ( ) , functionClass . getCanonicalName ( ) ) ; testEvaluators . put ( ( Class < ? extends NodeTest > ) functionClass , clazz ) ; } else-if ( ClassUtils . isAssignable ( functionClass , SelectorFunction . class ) ) { logger . debug ( "Found evaluator {} for NodeFunction {}" , clazz . getCanonicalName ( ) , functionClass . getCanonicalName ( ) ) ; functionEvaluators . put ( ( Class < ? extends SelectorFunction > ) functionClass , clazz ) ; } else { logger . debug ( "Found evaluator {} for NodeSelector {}" , clazz . getCanonicalName ( ) , functionClass . getCanonicalName ( ) ) ; defaultEvaluators . put ( ( Class < ? extends NodeSelector > ) functionClass , clazz ) ; } }
public void test() { if ( ClassUtils . isAssignable ( functionClass , TestFunction . class ) ) { logger . debug ( "Found evaluator {} for TestFunction {}" , clazz . getCanonicalName ( ) , functionClass . getCanonicalName ( ) ) ; testFunctionEvaluators . put ( ( Class < ? extends TestFunction > ) functionClass , clazz ) ; } else-if ( ClassUtils . isAssignable ( functionClass , NodeTest . class ) ) { logger . debug ( "Found evaluator {} for NodeTest {}" , clazz . getCanonicalName ( ) , functionClass . getCanonicalName ( ) ) ; testEvaluators . put ( ( Class < ? extends NodeTest > ) functionClass , clazz ) ; } else-if ( ClassUtils . isAssignable ( functionClass , SelectorFunction . class ) ) { logger . debug ( "Found evaluator {} for NodeFunction {}" , clazz . getCanonicalName ( ) , functionClass . getCanonicalName ( ) ) ; functionEvaluators . put ( ( Class < ? extends SelectorFunction > ) functionClass , clazz ) ; } else { logger . debug ( "Found evaluator {} for NodeSelector {}" , clazz . getCanonicalName ( ) , functionClass . getCanonicalName ( ) ) ; defaultEvaluators . put ( ( Class < ? extends NodeSelector > ) functionClass , clazz ) ; } }
public void test() { if ( ClassUtils . isAssignable ( functionClass , TestFunction . class ) ) { logger . debug ( "Found evaluator {} for TestFunction {}" , clazz . getCanonicalName ( ) , functionClass . getCanonicalName ( ) ) ; testFunctionEvaluators . put ( ( Class < ? extends TestFunction > ) functionClass , clazz ) ; } else-if ( ClassUtils . isAssignable ( functionClass , NodeTest . class ) ) { logger . debug ( "Found evaluator {} for NodeTest {}" , clazz . getCanonicalName ( ) , functionClass . getCanonicalName ( ) ) ; testEvaluators . put ( ( Class < ? extends NodeTest > ) functionClass , clazz ) ; } else-if ( ClassUtils . isAssignable ( functionClass , SelectorFunction . class ) ) { logger . debug ( "Found evaluator {} for Selector {}" , clazz . getCanonicalName ( ) , functionClass . getCanonicalName ( ) ) ; functionEvaluators . put ( ( Class < ? extends SelectorFunction > ) functionClass , clazz ) ; } else { logger . debug ( "Found evaluator {} for NodeSelector {}" , clazz . getCanonicalName ( ) , functionClass . getCanonicalName ( ) ) ; defaultEvaluators . put ( ( Class < ? extends NodeSelector > ) functionClass , clazz ) ; } }
public void test() { if ( ClassUtils . isAssignable ( functionClass , TestFunction . class ) ) { logger . debug ( "Found evaluator {} for TestFunction {}" , clazz . getCanonicalName ( ) , functionClass . getCanonicalName ( ) ) ; testFunctionEvaluators . put ( ( Class < ? extends TestFunction > ) functionClass , clazz ) ; } else-if ( ClassUtils . isAssignable ( functionClass , NodeTest . class ) ) { logger . debug ( "Found evaluator {} for NodeTest {}" , clazz . getCanonicalName ( ) , functionClass . getCanonicalName ( ) ) ; testEvaluators . put ( ( Class < ? extends NodeTest > ) functionClass , clazz ) ; } else-if ( ClassUtils . isAssignable ( functionClass , SelectorFunction . class ) ) { logger . debug ( "Found evaluator {} for NodeFunction {}" , clazz . getCanonicalName ( ) , functionClass . getCanonicalName ( ) ) ; functionEvaluators . put ( ( Class < ? extends SelectorFunction > ) functionClass , clazz ) ; } else { logger . debug ( "Found evaluator {} for Selector {}" , clazz . getCanonicalName ( ) , functionClass . getCanonicalName ( ) ) ; defaultEvaluators . put ( ( Class < ? extends NodeSelector > ) functionClass , clazz ) ; } }
public void test() { if ( multiSearchResponse . isError ) { logger . debug ( "Search failed" ) ; return false ; } }
@ Test public void testContextualQueryNullMetadata ( ) throws Exception { String methodName = "testContextualQueryNullMetadata" ; LOGGER . debug ( "***************  START: {}  *****************" , methodName ) ; String searchPhrase = "serengeti event" ; MockQuery query = new MockQuery ( ) ; query . addContextualFilter ( searchPhrase , null ) ; SubscriptionFilterVisitor visitor = new SubscriptionFilterVisitor ( ) ; Predicate predicate = ( Predicate ) query . getFilter ( ) . accept ( visitor , null ) ; MetacardImpl metacard = new MetacardImpl ( ) ; metacard . setId ( "ABC123" ) ; metacard . setMetadata ( TestDataLibrary . getCatAndDogEntry ( ) ) ; HashMap < String , Object > properties = new HashMap < > ( ) ; properties . put ( PubSubConstants . HEADER_ID_KEY , metacard . getId ( ) ) ; properties . put ( PubSubConstants . HEADER_ENTRY_KEY , metacard ) ; properties . put ( PubSubConstants . HEADER_OPERATION_KEY , PubSubConstants . CREATE ) ; Event testEvent = new Event ( "topic" , properties ) ; assertFalse ( predicate . matches ( testEvent ) ) ; LOGGER . debug ( "***************  END: {}  *****************" , methodName ) ; }
@ Test public void testContextualQueryNullMetadata ( ) throws Exception { String methodName = "testContextualQueryNullMetadata" ; LOGGER . debug ( "***************  START: {}  *****************" , methodName ) ; String searchPhrase = "serengeti event" ; MockQuery query = new MockQuery ( ) ; query . addContextualFilter ( searchPhrase , null ) ; SubscriptionFilterVisitor visitor = new SubscriptionFilterVisitor ( ) ; Predicate predicate = ( Predicate ) query . getFilter ( ) . accept ( visitor , null ) ; MetacardImpl metacard = new MetacardImpl ( ) ; metacard . setId ( "ABC123" ) ; metacard . setMetadata ( TestDataLibrary . getCatAndDogEntry ( ) ) ; HashMap < String , Object > properties = new HashMap < > ( ) ; properties . put ( PubSubConstants . HEADER_ID_KEY , metacard . getId ( ) ) ; properties . put ( PubSubConstants . HEADER_ENTRY_KEY , metacard ) ; properties . put ( PubSubConstants . HEADER_OPERATION_KEY , PubSubConstants . CREATE ) ; Event testEvent = new Event ( "topic" , properties ) ; assertFalse ( predicate . matches ( testEvent ) ) ; LOGGER . debug ( "***************  END: {}  *****************" , methodName ) ; }
public void test() { try { return Integer . parseInt ( value ) ; } catch ( Exception e ) { LOGGER . warn ( "Can not parse the property {} with the value \"{}\" to an int value" , name , value , e ) ; } }
public void test() { -> { LOGGER . info ( "Loaded {} results" , result . size ( ) ) ; } }
public void test() { try { CamelAutoConfiguration . doConfigureCamelContext ( applicationContext , camelContext , config ) ; } catch ( Exception e ) { LOG . error ( "Error configuring Camel auto configuration to: {}" , camelContext , e ) ; throw RuntimeCamelException . wrapRuntimeCamelException ( e ) ; } }
public void test() { if ( ep == null ) { String errMsg = "No remote endpoint to send command, check if host or ssvm is down?" ; s_logger . error ( errMsg ) ; return null ; } }
public void test() { if ( tip != null ) { tip . reportProgress ( taskStatus ) ; return true ; } else { LOG . warn ( "Task {} does not exist" , taskId ) ; return false ; } }
private static String convertPKCS1ToPKCS8 ( String pkcs1 ) throws IOException { String uuid = UUID . randomUUID ( ) . toString ( ) ; File pkcs1File = new File ( "keys/" + uuid + "/pkcs1.key" ) ; File pkcs8File = new File ( "keys/" + uuid + "/pkcs8.key" ) ; String transformCmd = "openssl rsa  -RSAPublicKey_in -in " + pkcs1File . toPath ( ) + " -out " + pkcs8File . toPath ( ) ; FileUtils . writeStringToFile ( pkcs1File , pkcs1 , "UTF-8" ) ; Process p = Runtime . getRuntime ( ) . exec ( transformCmd ) ; BufferedReader stdError = new BufferedReader ( new InputStreamReader ( p . getErrorStream ( ) ) ) ; String s ; code_block = WhileStatement ; LOGGER . info ( "pkcs2 = " + pkcs8File ) ; return FileUtils . readFileToString ( pkcs8File , "UTF-8" ) ; }
public void test() { if ( logger . isInfoEnabled ( ) ) { StringBuilder sqlCmds = new StringBuilder ( 128 ) ; Iterator < String > iter = tcConn . getDDLConverter ( ) . getDDL ( spec ) . iterator ( ) ; code_block = WhileStatement ; logger . info ( sqlCmds . toString ( ) ) ; } }
public void test() { try ( Connection con = getDatasource ( ) . getConnection ( ) ; ) { String query1 = "UPDATE vcenter SET url = ?, userid = ?, password = ? WHERE tkey = ?" ; code_block = TryStatement ;  } catch ( SQLException e ) { logger . error ( Messages . get ( locale , "error_db_save_conf" ) , e ) ; throw new Exception ( Messages . get ( locale , "error_db_save_conf" ) ) ; } }
@ Override public void onNext ( DiscoveryResponse response ) { logger . debug ( "Received APIDiscovery response " + response ) ; XdsSchedulerManager . getInstance ( ) . stopAPIDiscoveryScheduling ( ) ; latestReceived = response ; code_block = TryStatement ;  }
public void test() { if ( reply . isSuccess ( ) ) { logger . info ( String . format ( "successfully connected host[uuid:%s], because it connected to the host[uuid:%s]" , cmsg . getHostUuid ( ) , cmsg . getHostUuid ( ) ) ) ; } else-if ( reply . isCanceled ( ) ) { logger . warn ( String . format ( "canceled connect kvm host[uuid:%s], because it connecting now" , cmsg . getHostUuid ( ) ) ) ; } else { logger . warn ( String . format ( "failed to load host[uuid:%s], %s" , cmsg . getHostUuid ( ) , reply . getError ( ) ) ) ; } }
public void test() { if ( reply . isSuccess ( ) ) { logger . debug ( String . format ( "host[uuid:%s] load successfully" , cmsg . getHostUuid ( ) ) ) ; } else-if ( reply . isCanceled ( ) ) { logger . warn ( String . format ( "failed to load host[uuid:%s]" , cmsg . getHostUuid ( ) ) ) ; } else { logger . warn ( String . format ( "failed to load host[uuid:%s], %s" , cmsg . getHostUuid ( ) , reply . getError ( ) ) ) ; } }
public void test() { if ( reply . isSuccess ( ) ) { logger . debug ( String . format ( "host[uuid:%s] load successfully" , cmsg . getHostUuid ( ) ) ) ; } else-if ( reply . isCanceled ( ) ) { logger . warn ( String . format ( "canceled connect kvm host[uuid:%s], because it connecting now" , cmsg . getHostUuid ( ) ) ) ; } else { logger . warn ( String . format ( "unknown host[uuid:%s]" , cmsg . getHostUuid ( ) ) ) ; } }
@ Override public List < String > getAll ( BatchRunContext batchRunContext ) { logger . info ( getClass ( ) . toString ( ) + " job starting ..." ) ; SystemAccount actor = getSystemAccount ( ) ; List < String > allShares = service . findAllExpiredEntries ( actor , actor ) ; logger . info ( allShares . size ( ) + " anonymous share(s) have been found to be deleted" ) ; return allShares ; }
public void sleepUntilNextRetry ( ) throws InterruptedException { int attempts = getAttemptTimes ( ) ; long sleepTime = getBackoffTime ( ) ; LOG . info ( "Attempt {} to retry after {} attempts" , attempts , getBackoffTime ( ) ) ; retryConfig . getTimeUnit ( ) . sleep ( sleepTime ) ; useRetry ( ) ; }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Refreshing." ) ; } }
public void test() { try { LOGGER . warn ( "Caught exception in WebSocketException" ) ; sendException ( new WebSocketException ( WSConstants . NO_REQUEST_ID , "Internal Server Error" ) ) ; } catch ( Exception e2 ) { } }
public void test() { try { volumeCountList . add ( nodeStateManager . getNode ( dn ) . getHealthyVolumeCount ( ) ) ; } catch ( NodeNotFoundException e ) { logger . error ( "Failed to get node {}" , dn ) ; } }
public void test() { try { MimeMessage mimeMessage = mailSender . createMimeMessage ( ) ; MimeMessageHelper helper = new MimeMessageHelper ( mimeMessage , true ) ; helper . setSubject ( subject ) ; helper . setText ( text , true ) ; helper . setTo ( receiver ) ; helper . setFrom ( SENDER ) ; mailSender . send ( mimeMessage ) ; } catch ( Exception e ) { log . error ( "something went wrong." , e ) ; log . error ( e . getMessage ( ) ) ; } }
public void test() { try { MimeMessage mimeMessage = mailSender . createMimeMessage ( ) ; MimeMessageHelper helper = new MimeMessageHelper ( mimeMessage , true ) ; helper . setSubject ( subject ) ; helper . setText ( text , true ) ; helper . setTo ( receiver ) ; helper . setFrom ( SENDER ) ; mailSender . send ( mimeMessage ) ; log . info ( "é®ä»¶åéæå" ) ; } catch ( Exception exc ) { log . error ( "" , e ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
@ Override public synchronized void stopScan ( ) { logger . debug ( "Stopping Scan" ) ; code_block = IfStatement ; bridgeHandler . stopDiscovery ( ) ; super . stopScan ( ) ; }
public void test() { try { final Object value1 = BeanHelper . getNestedProperty ( o1 , prop ) ; final Object value2 = BeanHelper . getNestedProperty ( o2 , prop ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; } catch ( final Exception ex ) { LOGGER . error ( "Deserialization error: " + ex . getMessage ( ) , ex ) ; return 0 ; } }
public void test() { try { Objects . requireNonNull ( session ) . getRemote ( ) . sendPing ( ByteBuffer . wrap ( PING_PAYLOAD ) ) ; } catch ( IOException e ) { logger . error ( "Failed sending ping to device" , e ) ; } }
public void test() { try { fw = new FileWriter ( file ) ; IOUtils . write ( result . getResults ( ) , fw ) ; } catch ( IOException e ) { logger . error ( "Failed to write to results file: " + e . getMessage ( ) ) ; } finally { IOUtils . closeQuietly ( fw ) ; } }
public void test() { try { getSession ( ) . getRemote ( ) . sendString ( resultMessage ) ; } catch ( IOException e ) { logger . error ( "Failed sending result message" , e ) ; } }
public void test() { if ( ! recycles . isEmpty ( ) ) { log . log ( TreeLogger . WARN , "Cyclecycle detected" ) ; } }
public void test() { try { log . debug ( "Calling ActionService for {}" , name ) ; return ( IActionSet ) getServiceFromRegistry ( context , createFilterExampleActionSet ( name , version ) ) ; } catch ( InvalidSyntaxException e ) { throw new ActivatorException ( e ) ; } }
public void test() { if ( assertion != null && assertion . getUserInfo ( ) != null && assertion . getUserInfo ( ) . getRoleCoded ( ) != null ) { value = assertion . getUserInfo ( ) . getRoleCoded ( ) . getCodeSystemName ( ) ; } else { LOG . warn ( "Unable to decode user information for role {}" , roleId ) ; } }
public void test() { if ( ActiveMQRALogger . LOGGER . isTraceEnabled ( ) ) { ActiveMQRALogger . LOGGER . trace ( "execute()" ) ; } }
private void verifyServiceInstances ( final Service service ) throws Exception { final Instances instances = graphql . instances ( new InstancesQuery ( ) . serviceId ( service . getKey ( ) ) . start ( startTime ) . end ( now ( ) ) ) ; LOGGER . info ( "instances: {}" , instances ) ; load ( "expected/profile/instances.yml" ) . as ( InstancesMatcher . class ) . verify ( instances ) ; }
public void test() { try { code_block = IfStatement ; } catch ( IOException e ) { LOGGER . error ( "Unable to retrieve file" , e ) ; } }
@ Test public void shouldDecodeNRowResponseSmallyChunked ( ) throws Exception { String response = Resources . read ( "chunked.json" , this . getClass ( ) ) ; String [ ] chunks = new String [ ] code_block = "" ; ; StringBuilder sb = new StringBuilder ( "Chunks:" ) ; code_block = ForStatement ; logger . info ( sb . toString ( ) ) ; shouldDecodeChunked ( true , chunks ) ; }
@ Override public void init ( ServletConfig servletConfig ) throws ServletException { super . init ( servletConfig ) ; this . excludeOwnerPages = WebloggerConfig . getBooleanProperty ( "cache.excludeOwnerEditPages" ) ; this . weblogPageCache = WeblogPageCache . getInstance ( ) ; this . siteWideCache = SiteWideCache . getInstance ( ) ; this . processReferrers = WebloggerConfig . getBooleanProperty ( "site.bannedwordslist.enable.referrers" ) ; log . info ( "Referrer spam check enabled = " + this . processReferrers ) ; String robotPatternStr = WebloggerConfig . getProperty ( "referrer.robotCheck.userAgentPattern" ) ; code_block = IfStatement ; themeReload = WebloggerConfig . getBooleanProperty ( "themes.reload.mode" ) ; log . info ( "ThemeReloader module:" + this . themeReload ) ; }
@ Override public void init ( ServletConfig servletConfig ) throws ServletException { super . init ( servletConfig ) ; log . info ( "Initializing PageServlet" ) ; this . excludeOwnerPages = WebloggerConfig . getBooleanProperty ( "cache.excludeOwnerEditPages" ) ; this . weblogPageCache = WeblogPageCache . getInstance ( ) ; this . siteWideCache = SiteWideCache . getInstance ( ) ; this . processReferrers = WebloggerConfig . getBooleanProperty ( "site.bannedwordslist.enable.referrers" ) ; String robotPatternStr = WebloggerConfig . getProperty ( "referrer.robotCheck.userAgentPattern" ) ; code_block = IfStatement ; themeReload = WebloggerConfig . getBooleanProperty ( "themes.reload.mode" ) ; log . info ( "ThemeReloader initialized" ) ; }
public void test() { try { robotPattern = Pattern . compile ( robotPatternStr ) ; } catch ( Exception e ) { logger . error ( "match pattern " + robotPatternStr , e ) ; } }
public void test() { try { ContainerQuota containerQuota = containerHost . getQuota ( ) ; code_block = IfStatement ; } catch ( Exception e ) { log . warn ( "Failed to get quota." , e ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { try { code_block = ForStatement ; } catch ( Exception ex ) { log . error ( ex . getMessage ( ) , ex ) ; return null ; } }
public void test() { try { code_block = IfStatement ; } catch ( IllegalArgumentException e ) { logger . log ( Level . INFO , "Invalid value for channel: {0}" , channelUID ) ; } }
private void stop ( boolean swallowException ) { AtomicReference < Throwable > firstException = new AtomicReference < > ( ) ; this . stopped = true ; Utils . closeQuietly ( coordinator , "coordinator" , firstException ) ; Utils . closeQuietly ( metrics , "consumer metrics" , firstException ) ; Utils . closeQuietly ( client , "consumer network client" , firstException ) ; AppInfoParser . unregisterAppInfo ( JMX_PREFIX , clientId , metrics ) ; if ( firstException . get ( ) != null && ! swallowException ) throw new KafkaException ( "Failed to stop the Connect group member" , firstException . get ( ) ) ; else log . info ( "The Connect group member has stopped." ) ; }
public void test() { try { Transformer trans = TransformerFactory . newInstance ( ) . newTransformer ( ) ; trans . setOutputProperty ( OutputKeys . INDENT , "yes" ) ; trans . setOutputProperty ( OutputKeys . OMIT_XML_DECLARATION , "yes" ) ; trans . setOutputProperty ( OutputKeys . METHOD , "html" ) ; trans . transform ( src , res ) ; tags = sw . toString ( ) . replaceAll ( ROOT_ELEMENT_REGEX , "" ) ; } catch ( Exception e ) { StringBuilder txt = new StringBuilder ( ) ; txt . append ( "Error converting tags to string. Exception: " ) ; txt . append ( e . toString ( ) ) ; LOG . warn ( txt . toString ( ) ) ; } }
public void test() { if ( isTrace ) { StringBuilder sb = new StringBuilder ( ) ; sb . append ( "returning tags: " ) ; sb . append ( ( tags . length ( ) > 0 ) ? "\n" + tags : "" ) ; LOG . debug ( sb . toString ( ) ) ; } }
public void test() { try { lockListener . lostLock ( ) ; } catch ( Exception e ) { logger . error ( "Lost lock" , e ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Refreshing." ) ; } }
public void test() { { logger . debug ( "Reset password reset for user {}" , username ) ; checkRights ( getRights ( ) . canAccessPasswordReset ( ) ) ; getUserService ( ) . requestPasswordReset ( resetPassword . getUser ( ) ) ; return Response . noContent ( ) . build ( ) ; } }
@ PayloadRoot ( localPart = "SwitchConfigurationAsyncRequest" , namespace = NAMESPACE ) @ ResponsePayload public SwitchConfigurationResponse getSwitchConfigurationResponse ( @ OrganisationIdentification final String organisationIdentification , @ RequestPayload final SwitchConfigurationAsyncRequest request ) throws OsgpException { LOGGER . info ( "Get switch configuration response received from organisation: {}." , organisationIdentification ) ; final SwitchConfigurationResponse response = new SwitchConfigurationResponse ( ) ; code_block = TryStatement ;  return response ; }
public void test() { if ( message != null ) { response . setResult ( OsgpResultType . fromValue ( message . getResult ( ) . getValue ( ) ) ) ; } else { LOGGER . debug ( "GetResult is null" ) ; } }
public void test() { try { executorCursor ( cursor ) ; } catch ( Exception e ) { logger . warn ( e . getMessage ( ) , e ) ; } finally { code_block = IfStatement ; replicaSet . shutdown ( ) ; } }
public void test() { try { CProductVersionConfiguration cProductVersionConfiguration = ConfigurationProviderUtil . getConfiguration ( CProductVersionConfiguration . class , new SystemSettingsLocator ( CProductVersionConfiguration . class . getName ( ) ) ) ; code_block = IfStatement ; } catch ( PortalException portalException ) { _log . error ( portalException , portalException ) ; } }
public void test() { if ( log . isDebugEnable ( ) ) { log . debug ( this , "unsubscribe procedure" ) ; } }
public void test() { try { ObjStat objStat = irodsFileSystemAO . getObjStat ( getAbsolutePath ( ) ) ; lastMod = objStat . getModifiedAt ( ) . getTime ( ) ; } catch ( FileNotFoundException e ) { log . info ( "file not found" ) ; } catch ( JargonException e ) { log . error ( "jargon exception, rethrow as unchecked" , e ) ; throw new JargonRuntimeException ( e ) ; } }
public void test() { try { ObjStat objStat = irodsFileSystemAO . getObjStat ( getAbsolutePath ( ) ) ; lastMod = objStat . getModifiedAt ( ) . getTime ( ) ; } catch ( FileNotFoundException e ) { log . warn ( "file not found exception, return 0L" , e ) ; } catch ( JargonException e ) { log . error ( "jargon exception, rethrow as unchecked" , e ) ; throw new JargonRuntimeException ( e ) ; } }
public void test() { if ( returnValue instanceof HasTraceEntryMixin ) { TraceEntry traceEntry = ( ( HasTraceEntryMixin ) httpURLConnection ) . glowroot$getTraceEntry ( ) ; ( ( HasTraceEntryMixin ) returnValue ) . glowroot$setTraceEntry ( traceEntry ) ; } else-if ( returnValue != null && ! outputStreamIssueAlreadyLogged . getAndSet ( true ) ) { logger . info ( "found non-instrumented http url connection input stream, please" + " report to the Glowroot project: {}" , returnValue . getClass ( ) . getName ( ) ) ; } }
public void test() { if ( LOGGER . isErrorEnabled ( ) ) { LOGGER . log ( Level . ERROR , e . getMessage ( ) , e ) ; } }
public void test() { try { latch . countDown ( ) ; code_block = ForStatement ; } catch ( Exception e ) { exceptionCaught . set ( true ) ; LOG . error ( "" , e ) ; } }
public void test() { if ( isDebug ) { logger . debug ( "Expected arguments, but found none." ) ; } }
@ Test public void testEntityConnections ( ) throws Exception { EntityManager em = app . getEntityManager ( ) ; final UUID applicationId = app . getId ( ) ; assertNotNull ( em ) ; Map < String , Object > properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Dylan" ) ; Entity catA = em . create ( "cat" , properties ) ; assertNotNull ( catA ) ; logger . info ( "\n\nEntity A created with id " + catA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nLooking up cat with id " + catA . getUuid ( ) + "\n" ) ; Entity cat = em . get ( catA ) ; assertNotNull ( cat ) ; logger . info ( "\n\nFound entity " + cat . getUuid ( ) + " of type " + cat . getType ( ) + " with name " + cat . getProperty ( "name" ) + "\n" ) ; logger . info ( "\n\nCreating cat entity B with name of Nico\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Nico" ) ; Entity catB = em . create ( "cat" , properties ) ; assertNotNull ( catB ) ; logger . info ( "\n\nEntity B created with id " + catB . getUuid ( ) + "\n" ) ; logger . info ( "\n\nCreating award entity with name of 'best cat'\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Best CatEver" ) ; Entity awardA = em . create ( "award" , properties ) ; assertNotNull ( awardA ) ; logger . info ( "\n\nEntity created with id " + awardA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nConnecting " + catA . getUuid ( ) + " \"likes\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( catA , "likes" , catB ) ; logger . info ( "\n\nConnecting " + cat
@ Test public void testEntityConnections ( ) throws Exception { EntityManager em = app . getEntityManager ( ) ; final UUID applicationId = app . getId ( ) ; assertNotNull ( em ) ; logger . info ( "\n\nCreating Cat entity A with name of Dylan\n" ) ; Map < String , Object > properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Dylan" ) ; logger . info ( "\n\nLooking up cat with id " + applicationId ) ; Entity catA = em . create ( "cat" , properties ) ; assertNotNull ( catA ) ; logger . info ( "\n\nLooking up cat with id " + catA . getUuid ( ) + "\n" ) ; Entity cat = em . get ( catA ) ; assertNotNull ( cat ) ; logger . info ( "\n\nFound entity " + cat . getUuid ( ) + " of type " + cat . getType ( ) + " with name " + cat . getProperty ( "name" ) + "\n" ) ; logger . info ( "\n\nCreating cat entity B with name of Nico\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Nico" ) ; Entity catB = em . create ( "cat" , properties ) ; assertNotNull ( catB ) ; logger . info ( "\n\nEntity B created with id " + catB . getUuid ( ) + "\n" ) ; logger . info ( "\n\nCreating award entity with name of 'best cat'\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Best CatEver" ) ; Entity awardA = em . create ( "award" , properties ) ; assertNotNull ( awardA ) ; logger . info ( "\n\nEntity created with id " + awardA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nConnecting " + catA . getUuid ( ) + " \"likes\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( catA , "likes" , catB ) ; logger .
@ Test public void testEntityConnections ( ) throws Exception { EntityManager em = app . getEntityManager ( ) ; final UUID applicationId = app . getId ( ) ; assertNotNull ( em ) ; logger . info ( "\n\nCreating Cat entity A with name of Dylan\n" ) ; Map < String , Object > properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Dylan" ) ; Entity catA = em . create ( "cat" , properties ) ; assertNotNull ( catA ) ; logger . info ( "\n\nEntity A created with id " + catA . getUuid ( ) + "\n" ) ; Entity cat = em . get ( catA ) ; assertNotNull ( cat ) ; logger . info ( "\n\nFound entity " + cat . getUuid ( ) + " of type " + cat . getType ( ) + " with name " + cat . getProperty ( "name" ) + "\n" ) ; logger . info ( "\n\nCreating cat entity B with name of Nico\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Nico" ) ; Entity catB = em . create ( "cat" , properties ) ; assertNotNull ( catB ) ; logger . info ( "\n\nEntity B created with id " + catB . getUuid ( ) + "\n" ) ; logger . info ( "\n\nCreating award entity with name of 'best cat'\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Best CatEver" ) ; Entity awardA = em . create ( "award" , properties ) ; assertNotNull ( awardA ) ; logger . info ( "\n\nEntity created with id " + awardA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nConnecting " + catA . getUuid ( ) + " \"likes\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( catA , "likes" , catB ) ; logger . info ( "\n\nConnecting " + awardA . getUuid ( ) +
@ Test public void testEntityConnections ( ) throws Exception { EntityManager em = app . getEntityManager ( ) ; final UUID applicationId = app . getId ( ) ; assertNotNull ( em ) ; logger . info ( "\n\nCreating Cat entity A with name of Dylan\n" ) ; Map < String , Object > properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Dylan" ) ; Entity catA = em . create ( "cat" , properties ) ; assertNotNull ( catA ) ; logger . info ( "\n\nEntity A created with id " + catA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nLooking up cat with id " + catA . getUuid ( ) + "\n" ) ; Entity cat = em . get ( catA ) ; assertNotNull ( cat ) ; logger . info ( "\n\nCreating cat entity B with name of Nico\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Nico" ) ; Entity catB = em . create ( "cat" , properties ) ; assertNotNull ( catB ) ; logger . info ( "\n\nEntity B created with id " + catB . getUuid ( ) + "\n" ) ; logger . info ( "\n\nCreating award entity with name of 'best cat'\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Best CatEver" ) ; Entity awardA = em . create ( "award" , properties ) ; assertNotNull ( awardA ) ; logger . info ( "\n\nEntity created with id " + awardA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nConnecting " + catA . getUuid ( ) + " \"likes\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( catA , "likes" , catB ) ; logger . info ( "\n\nConnecting " + awardA . getUuid ( ) + " \"awarded\" " + catB . getUuid ( ) + "\n" ) ; logger .
@ Test public void testEntityConnections ( ) throws Exception { EntityManager em = app . getEntityManager ( ) ; final UUID applicationId = app . getId ( ) ; assertNotNull ( em ) ; logger . info ( "\n\nCreating Cat entity A with name of Dylan\n" ) ; Map < String , Object > properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Dylan" ) ; Entity catA = em . create ( "cat" , properties ) ; assertNotNull ( catA ) ; logger . info ( "\n\nEntity A created with id " + catA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nLooking up cat with id " + catA . getUuid ( ) + "\n" ) ; Entity cat = em . get ( catA ) ; assertNotNull ( cat ) ; logger . info ( "\n\nFound entity " + cat . getUuid ( ) + " of type " + cat . getType ( ) + " with name " + cat . getProperty ( "name" ) + "\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Nico" ) ; Entity catB = em . create ( "cat" , properties ) ; assertNotNull ( catB ) ; logger . info ( "\n\nEntity B created with id " + catB . getUuid ( ) + "\n" ) ; logger . info ( "\n\nCreating award entity with name of 'best cat'\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Best CatEver" ) ; Entity awardA = em . create ( "award" , properties ) ; assertNotNull ( awardA ) ; logger . info ( "\n\nEntity created with id " + awardA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nConnecting " + catA . getUuid ( ) + " \"likes\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( catA , "likes" , catB ) ; logger . info ( "\n\nConnecting " + AR
@ Test public void testEntityConnections ( ) throws Exception { EntityManager em = app . getEntityManager ( ) ; final UUID applicationId = app . getId ( ) ; assertNotNull ( em ) ; logger . info ( "\n\nCreating Cat entity A with name of Dylan\n" ) ; Map < String , Object > properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Dylan" ) ; Entity catA = em . create ( "cat" , properties ) ; assertNotNull ( catA ) ; logger . info ( "\n\nEntity A created with id " + catA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nLooking up cat with id " + catA . getUuid ( ) + "\n" ) ; Entity cat = em . get ( catA ) ; assertNotNull ( cat ) ; logger . info ( "\n\nFound entity " + cat . getUuid ( ) + " of type " + cat . getType ( ) + " with name " + cat . getProperty ( "name" ) + "\n" ) ; logger . info ( "\n\nCreating cat entity B with name of Nico\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Nico" ) ; Entity catB = em . create ( "cat" , properties ) ; assertNotNull ( catB ) ; logger . info ( "\n\nCreating award entity with name of 'best cat'\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Best CatEver" ) ; Entity awardA = em . create ( "award" , properties ) ; assertNotNull ( awardA ) ; logger . info ( "\n\nEntity created with id " + awardA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nConnecting " + catA . getUuid ( ) + " \"likes\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( catA , "likes" , catB ) ; logger . info ( "\n\nConnecting " + awardA . getUuid ( ) +
@ Test public void testEntityConnections ( ) throws Exception { EntityManager em = app . getEntityManager ( ) ; final UUID applicationId = app . getId ( ) ; assertNotNull ( em ) ; logger . info ( "\n\nCreating Cat entity A with name of Dylan\n" ) ; Map < String , Object > properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Dylan" ) ; Entity catA = em . create ( "cat" , properties ) ; assertNotNull ( catA ) ; logger . info ( "\n\nEntity A created with id " + catA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nLooking up cat with id " + catA . getUuid ( ) + "\n" ) ; Entity cat = em . get ( catA ) ; assertNotNull ( cat ) ; logger . info ( "\n\nFound entity " + cat . getUuid ( ) + " of type " + cat . getType ( ) + " with name " + cat . getProperty ( "name" ) + "\n" ) ; logger . info ( "\n\nCreating cat entity B with name of Nico\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Nico" ) ; Entity catB = em . create ( "cat" , properties ) ; assertNotNull ( catB ) ; logger . info ( "\n\nEntity B created with id " + catB . getUuid ( ) + "\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Best CatEver" ) ; logger . info ( "\n\nCreating cat entity ( awardA . getUuid ( ) + "\n" ) ; Entity awardA = em . create ( "award" , properties ) ; assertNotNull ( awardA ) ; logger . info ( "\n\nEntity created with id " + awardA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nConnecting " + catA . getUuid ( ) + " \"likes\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( cat
@ Test public void testEntityConnections ( ) throws Exception { EntityManager em = app . getEntityManager ( ) ; final UUID applicationId = app . getId ( ) ; assertNotNull ( em ) ; logger . info ( "\n\nCreating Cat entity A with name of Dylan\n" ) ; Map < String , Object > properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Dylan" ) ; Entity catA = em . create ( "cat" , properties ) ; assertNotNull ( catA ) ; logger . info ( "\n\nEntity A created with id " + catA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nLooking up cat with id " + catA . getUuid ( ) + "\n" ) ; Entity cat = em . get ( catA ) ; assertNotNull ( cat ) ; logger . info ( "\n\nFound entity " + cat . getUuid ( ) + " of type " + cat . getType ( ) + " with name " + cat . getProperty ( "name" ) + "\n" ) ; logger . info ( "\n\nCreating cat entity B with name of Nico\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Nico" ) ; Entity catB = em . create ( "cat" , properties ) ; assertNotNull ( catB ) ; logger . info ( "\n\nEntity B created with id " + catB . getUuid ( ) + "\n" ) ; logger . info ( "\n\nCreating award entity with name of 'best cat'\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Best CatEver" ) ; Entity awardA = em . create ( "award" , properties ) ; assertNotNull ( awardA ) ; logger . info ( "\n\nConnecting " + catA . getUuid ( ) + " \"likes\" " + catB . getUuid ( ) + "\n" ) ; logger . info ( "\n\nCreating a new author)\n" ) ; em . createConnection ( catA , "likes" , catB ) ; logger .
@ Test public void testEntityConnections ( ) throws Exception { EntityManager em = app . getEntityManager ( ) ; final UUID applicationId = app . getId ( ) ; assertNotNull ( em ) ; logger . info ( "\n\nCreating Cat entity A with name of Dylan\n" ) ; Map < String , Object > properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Dylan" ) ; Entity catA = em . create ( "cat" , properties ) ; assertNotNull ( catA ) ; logger . info ( "\n\nEntity A created with id " + catA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nLooking up cat with id " + catA . getUuid ( ) + "\n" ) ; Entity cat = em . get ( catA ) ; assertNotNull ( cat ) ; logger . info ( "\n\nFound entity " + cat . getUuid ( ) + " of type " + cat . getType ( ) + " with name " + cat . getProperty ( "name" ) + "\n" ) ; logger . info ( "\n\nCreating cat entity B with name of Nico\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Nico" ) ; Entity catB = em . create ( "cat" , properties ) ; assertNotNull ( catB ) ; logger . info ( "\n\nEntity B created with id " + catB . getUuid ( ) + "\n" ) ; logger . info ( "\n\nCreating award entity with name of 'best cat'\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Best CatEver" ) ; Entity awardA = em . create ( "award" , properties ) ; assertNotNull ( awardA ) ; logger . info ( "\n\nEntity created with id " + awardA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nCreating " + waA . getUuid ( ) + "\n" ) ; em . createConnection ( catA , "likes" , catB ) ; logger . info ( "\n\nConnect
@ Test public void testEntityConnections ( ) throws Exception { EntityManager em = app . getEntityManager ( ) ; final UUID applicationId = app . getId ( ) ; assertNotNull ( em ) ; logger . info ( "\n\nCreating Cat entity A with name of Dylan\n" ) ; Map < String , Object > properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Dylan" ) ; Entity catA = em . create ( "cat" , properties ) ; assertNotNull ( catA ) ; logger . info ( "\n\nEntity A created with id " + catA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nLooking up cat with id " + catA . getUuid ( ) + "\n" ) ; Entity cat = em . get ( catA ) ; assertNotNull ( cat ) ; logger . info ( "\n\nFound entity " + cat . getUuid ( ) + " of type " + cat . getType ( ) + " with name " + cat . getProperty ( "name" ) + "\n" ) ; logger . info ( "\n\nCreating cat entity B with name of Nico\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Nico" ) ; Entity catB = em . create ( "cat" , properties ) ; assertNotNull ( catB ) ; logger . info ( "\n\nEntity B created with id " + catB . getUuid ( ) + "\n" ) ; logger . info ( "\n\nCreating award entity with name of 'best cat'\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Best CatEver" ) ; Entity awardA = em . create ( "award" , properties ) ; assertNotNull ( awardA ) ; logger . info ( "\n\nEntity created with id " + awardA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nConnecting " + catA . getUuid ( ) + " \"likes\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( catA ) ;
@ Test public void testEntityConnections ( ) throws Exception { EntityManager em = app . getEntityManager ( ) ; final UUID applicationId = app . getId ( ) ; assertNotNull ( em ) ; logger . info ( "\n\nCreating Cat entity A with name of Dylan\n" ) ; Map < String , Object > properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Dylan" ) ; Entity catA = em . create ( "cat" , properties ) ; assertNotNull ( catA ) ; logger . info ( "\n\nEntity A created with id " + catA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nLooking up cat with id " + catA . getUuid ( ) + "\n" ) ; Entity cat = em . get ( catA ) ; assertNotNull ( cat ) ; logger . info ( "\n\nFound entity " + cat . getUuid ( ) + " of type " + cat . getType ( ) + " with name " + cat . getProperty ( "name" ) + "\n" ) ; logger . info ( "\n\nCreating cat entity B with name of Nico\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Nico" ) ; Entity catB = em . create ( "cat" , properties ) ; assertNotNull ( catB ) ; logger . info ( "\n\nEntity B created with id " + catB . getUuid ( ) + "\n" ) ; logger . info ( "\n\nCreating award entity with name of 'best cat'\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Best CatEver" ) ; Entity awardA = em . create ( "award" , properties ) ; assertNotNull ( awardA ) ; logger . info ( "\n\nEntity created with id " + awardA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nConnecting " + catA . getUuid ( ) + " \"likes\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( catA ) ;
@ Test public void testEntityConnections ( ) throws Exception { EntityManager em = app . getEntityManager ( ) ; final UUID applicationId = app . getId ( ) ; assertNotNull ( em ) ; logger . info ( "\n\nCreating Cat entity A with name of Dylan\n" ) ; Map < String , Object > properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Dylan" ) ; Entity catA = em . create ( "cat" , properties ) ; assertNotNull ( catA ) ; logger . info ( "\n\nEntity A created with id " + catA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nLooking up cat with id " + catA . getUuid ( ) + "\n" ) ; Entity cat = em . get ( catA ) ; assertNotNull ( cat ) ; logger . info ( "\n\nFound entity " + cat . getUuid ( ) + " of type " + cat . getType ( ) + " with name " + cat . getProperty ( "name" ) + "\n" ) ; logger . info ( "\n\nCreating cat entity B with name of Nico\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Nico" ) ; Entity catB = em . create ( "cat" , properties ) ; assertNotNull ( catB ) ; logger . info ( "\n\nEntity B created with id " + catB . getUuid ( ) + "\n" ) ; logger . info ( "\n\nCreating award entity with name of 'best cat'\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Best CatEver" ) ; Entity awardA = em . create ( "award" , properties ) ; assertNotNull ( awardA ) ; logger . info ( "\n\nEntity created with id " + awardA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nConnecting " + catA . getUuid ( ) + " \"likes\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( catA ) ;
@ Test public void testEntityConnections ( ) throws Exception { EntityManager em = app . getEntityManager ( ) ; final UUID applicationId = app . getId ( ) ; assertNotNull ( em ) ; logger . info ( "\n\nCreating Cat entity A with name of Dylan\n" ) ; Map < String , Object > properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Dylan" ) ; Entity catA = em . create ( "cat" , properties ) ; assertNotNull ( catA ) ; logger . info ( "\n\nEntity A created with id " + catA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nLooking up cat with id " + catA . getUuid ( ) + "\n" ) ; Entity cat = em . get ( catA ) ; assertNotNull ( cat ) ; logger . info ( "\n\nFound entity " + cat . getUuid ( ) + " of type " + cat . getType ( ) + " with name " + cat . getProperty ( "name" ) + "\n" ) ; logger . info ( "\n\nCreating cat entity B with name of Nico\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Nico" ) ; Entity catB = em . create ( "cat" , properties ) ; assertNotNull ( catB ) ; logger . info ( "\n\nEntity B created with id " + catB . getUuid ( ) + "\n" ) ; logger . info ( "\n\nCreating award entity with name of 'best cat'\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Best CatEver" ) ; Entity awardA = em . create ( "award" , properties ) ; assertNotNull ( awardA ) ; logger . info ( "\n\nEntity created with id " + awardA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nConnecting " + catA . getUuid ( ) + " \"likes\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( catA ) ;
@ Test public void testEntityConnections ( ) throws Exception { EntityManager em = app . getEntityManager ( ) ; final UUID applicationId = app . getId ( ) ; assertNotNull ( em ) ; logger . info ( "\n\nCreating Cat entity A with name of Dylan\n" ) ; Map < String , Object > properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Dylan" ) ; Entity catA = em . create ( "cat" , properties ) ; assertNotNull ( catA ) ; logger . info ( "\n\nEntity A created with id " + catA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nLooking up cat with id " + catA . getUuid ( ) + "\n" ) ; Entity cat = em . get ( catA ) ; assertNotNull ( cat ) ; logger . info ( "\n\nFound entity " + cat . getUuid ( ) + " of type " + cat . getType ( ) + " with name " + cat . getProperty ( "name" ) + "\n" ) ; logger . info ( "\n\nCreating cat entity B with name of Nico\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Nico" ) ; Entity catB = em . create ( "cat" , properties ) ; assertNotNull ( catB ) ; logger . info ( "\n\nEntity B created with id " + catB . getUuid ( ) + "\n" ) ; logger . info ( "\n\nCreating award entity with name of 'best cat'\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Best CatEver" ) ; Entity awardA = em . create ( "award" , properties ) ; assertNotNull ( awardA ) ; logger . info ( "\n\nEntity created with id " + awardA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nConnecting " + catA . getUuid ( ) + " \"likes\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( catA ) ;
public void test() { try { dAdminMgr . assignUser ( userRole ) ; } catch ( SecurityException se ) { LOG . warn ( "addUserRoles tenant={} roleName={} caught SecurityException={}" , getTenant ( ) , userRole . getName ( ) , se ) ; } }
@ BeforeEach void logTest ( TestInfo testInfo ) { LOG . info ( testInfo . toString ( ) ) ; }
public void test() { try { resetCoprocessor ( tableName , hbaseAdmin , hdfsCoprocessorJar ) ; processed . add ( tableName ) ; } catch ( IOException ex ) { logger . warn ( "Error resetCoprocessor for table " + tableName , ex ) ; } }
public void test() { try { log . info ( "Run started" ) ; addCategories ( ) ; code_block = ForStatement ; log . info ( "Run successful" ) ; } catch ( Exception e ) { log . error ( "Error adding categories" , e ) ; } }
public void test() { if ( ! type . isInstance ( session ) ) { LOGGER . log ( Level . WARNING , "Invalid type: {0}" , session . getId ( ) ) ; return null ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( s_logger . isInfoEnabled ( ) ) { s_logger . info ( "Unable to recover from " + toString ( ) ) ; } }
public void test() { if ( e instanceof RemoteException ) { s_logger . warn ( "Encounter remote exception to vCenter, invalidate VMware session context" ) ; invalidateServiceContext ( ) ; } }
@ Bean ( name = "domainDistributionAutomationInboundKafkaRequestsMessageListenerContainer" ) public DefaultMessageListenerContainer messageListenerContainer ( @ Qualifier ( "domainDistributionAutomationInboundKafkaRequestsMessageListener" ) final MessageListener messageListener ) { LOGGER . info ( "Initializing domainDistributionAutomationInboundKafkaRequestsMessageListenerContainer bean." ) ; return this . jmsConfigurationFactory . initMessageListenerContainer ( messageListener ) ; }
public void test() { if ( e instanceof InterpreterRPCException ) { result . ex = ( InterpreterRPCException ) e ; result . setExIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof InterpreterRPCException ) { result . ex = ( InterpreterRPCException ) e ; result . setExIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof InterpreterRPCException ) { result . ex = ( InterpreterRPCException ) e ; result . setExIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { try { fcall . sendResponse ( fb , msg , msgType , seqid ) ; } catch ( java . lang . Exception ex ) { _LOGGER . error ( "Exception writing to internal frame buffer" , ex ) ; fb . close ( ) ; } }
@ Override public void onPendingFailure ( ProviderException cause ) { LOG . debug ( "Request failed" , cause ) ; participants . put ( envelope . getConsumerId ( ) , envelope . getConsumerId ( ) ) ; }
public void test() { try { LOGGER . warn ( "Waiting 1s for taskThread to be set..." ) ; Thread . sleep ( 1000 ) ; } catch ( InterruptedException e ) { LOGGER . warn ( "TaskThread was not set." , e ) ; Thread . currentThread ( ) . interrupt ( ) ; return false ; } }
@ Override public DexOrder findCounterOffer ( DexOrder createdOrder ) { log . debug ( "Searching counter order" ) ; OrderType counterOrderType = createdOrder . getType ( ) . isBuy ( ) ? OrderType . SELL : OrderType . BUY ; String orderBy = createdOrder . getType ( ) . isSell ( ) ? "DESC" : "ASC" ; Integer currentTime = timeService . getEpochTime ( ) ; BigDecimal offerAmount = new BigDecimal ( createdOrder . getOrderAmount ( ) ) ; Integer pairCurrency = DexCurrency . getValue ( createdOrder . getPairCurrency ( ) ) ; BigDecimal pairRate = new BigDecimal ( EthUtil . ethToGwei ( createdOrder . getPairRate ( ) ) ) ; log . debug ( "Dumping arguments: type: {}, currentTime: {}, offerAmount: {}, offerCurrency: {}, pairRate: {}, order: {}" , counterOrderType , currentTime , offerAmount , pairCurrency , pairRate , orderBy ) ; DexOrderDBMatchingRequest dexOrderDBMatchingRequest = new DexOrderDBMatchingRequest ( counterOrderType , currentTime , 0 , offerAmount , pairCurrency . intValue ( ) , pairRate , orderBy ) ; List < DexOrder > orders = dexMatchingService . getOffersForMatching ( dexOrderDBMatchingRequest , orderBy ) ; log . debug ( "offers found: {}" , orders . size ( ) ) ; List < DexOrder > filteredOrders = orders . stream ( ) . filter ( order -> ! order . getAccountId ( ) . equals ( createdOrder . getAccountId ( ) ) ) . collect ( Collectors . toList ( ) ) ; code_block = ForStatement ; return null ; }
@ Override public DexOrder findCounterOffer ( DexOrder createdOrder ) { log . debug ( "DexMatcherServiceImpl:findCounterOffer()" ) ; OrderType counterOrderType = createdOrder . getType ( ) . isBuy ( ) ? OrderType . SELL : OrderType . BUY ; String orderBy = createdOrder . getType ( ) . isSell ( ) ? "DESC" : "ASC" ; Integer currentTime = timeService . getEpochTime ( ) ; BigDecimal offerAmount = new BigDecimal ( createdOrder . getOrderAmount ( ) ) ; Integer pairCurrency = DexCurrency . getValue ( createdOrder . getPairCurrency ( ) ) ; BigDecimal pairRate = new BigDecimal ( EthUtil . ethToGwei ( createdOrder . getPairRate ( ) ) ) ; DexOrderDBMatchingRequest dexOrderDBMatchingRequest = new DexOrderDBMatchingRequest ( counterOrderType , currentTime , 0 , offerAmount , pairCurrency . intValue ( ) , pairRate , orderBy ) ; List < DexOrder > orders = dexMatchingService . getOffersForMatching ( dexOrderDBMatchingRequest , orderBy ) ; log . debug ( "offers found: {}" , orders . size ( ) ) ; List < DexOrder > filteredOrders = orders . stream ( ) . filter ( order -> ! order . getAccountId ( ) . equals ( createdOrder . getAccountId ( ) ) ) . collect ( Collectors . toList ( ) ) ; code_block = ForStatement ; log . debug ( "Total order found: {}" , filteredOrders . size ( ) ) ; return null ; }
@ Override public DexOrder findCounterOffer ( DexOrder createdOrder ) { log . debug ( "DexMatcherServiceImpl:findCounterOffer()" ) ; OrderType counterOrderType = createdOrder . getType ( ) . isBuy ( ) ? OrderType . SELL : OrderType . BUY ; String orderBy = createdOrder . getType ( ) . isSell ( ) ? "DESC" : "ASC" ; Integer currentTime = timeService . getEpochTime ( ) ; BigDecimal offerAmount = new BigDecimal ( createdOrder . getOrderAmount ( ) ) ; Integer pairCurrency = DexCurrency . getValue ( createdOrder . getPairCurrency ( ) ) ; BigDecimal pairRate = new BigDecimal ( EthUtil . ethToGwei ( createdOrder . getPairRate ( ) ) ) ; log . debug ( "Dumping arguments: type: {}, currentTime: {}, offerAmount: {}, offerCurrency: {}, pairRate: {}, order: {}" , counterOrderType , currentTime , offerAmount , pairCurrency , pairRate , orderBy ) ; DexOrderDBMatchingRequest dexOrderDBMatchingRequest = new DexOrderDBMatchingRequest ( counterOrderType , currentTime , 0 , offerAmount , pairCurrency . intValue ( ) , pairRate , orderBy ) ; List < DexOrder > orders = dexMatchingService . getOffersForMatching ( dexOrderDBMatchingRequest , orderBy ) ; List < DexOrder > filteredOrders = orders . stream ( ) . filter ( order -> ! order . getAccountId ( ) . equals ( createdOrder . getAccountId ( ) ) ) . collect ( Collectors . toList ( ) ) ; code_block = ForStatement ; log . debug ( "DexMatcherServiceImpl:findCounterOffer()" ) ; return null ; }
public void test() { try { code_block = IfStatement ; } catch ( Exception ex ) { LOGGER . error ( "Failed to clean up." , ex ) ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( "Closing connection {}" , connection ) ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( "Closing connection {}" , connection ) ; } }
public void test() { try { ExecStatus . valueOf ( execution . getStatus ( ) ) ; } catch ( IllegalArgumentException e ) { log . error ( "Invalid execution status: {}" , execution . getStatus ( ) ) ; isValid = false ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { application . stop ( ) ; } catch ( final Exception e ) { logger . warn ( "Failed to stop application" , e ) ; } }
public void test() { try { return ll . findIUS ( "/" + swAccession ) ; } catch ( IOException | JAXBException ex ) { Logger . error ( ex . getMessage ( ) , ex ) ; } }
public void test() { { code_block = IfStatement ; TradingDataOutput tradingDataOutput = tradingViewService . getUpdatedDataForIntervalFromOffers ( symbol , resolution , to , from ) ; log . debug ( "got updated data" ) ; return Response . ok ( new TradingDataOutputToDtoConverter ( ) . apply ( tradingDataOutput ) ) . build ( ) ; } }
public void test() { if ( to <= 1569369600 ) { log . debug ( "Request to <= 1569369600" ) ; TradingDataOutput tdo = new TradingDataOutput ( ) ; tdo . setC ( null ) ; tdo . setH ( null ) ; tdo . setL ( null ) ; tdo . setO ( null ) ; tdo . setT ( null ) ; tdo . setV ( null ) ; tdo . setNextTime ( null ) ; tdo . setS ( "no_data" ) ; return Response . ok ( converter . apply ( tdo ) ) . build ( ) ; } }
public void test() { try { int returnValue = CommerceWishListItemServiceUtil . getCommerceWishListItemByContainsCProductCount ( commerceWishListId , cProductId ) ; return returnValue ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try { return Resource . deserializeObject ( buffer ) ; } catch ( Exception e ) { logger . warn ( "Failed to deserialize resource {}" , resource , e ) ; } }
public void test() { if ( Files . exists ( path ) && Files . isReadable ( path ) && ! Files . isDirectory ( path ) ) { LOGGER . info ( "Found MP meta configuration file: " + path . toAbsolutePath ( ) ) ; return Optional . of ( ConfigSources . file ( path ) . build ( ) ) ; } }
public void test() { try { getEntityManager ( ) . remove ( modelFamily ) ; } catch ( HibernateException he ) { logger . error ( "Error while removing model family {}" , he . getMessage ( ) ) ; return false ; } }
@ VisibleForTesting protected void scheduledTask ( ) { int totalCount , successCount , failCount ; List < EventModel > events = alertEventService . getLastHourAlertEvent ( ) ; totalCount = events . size ( ) ; Pair < Integer , Integer > successAndFail = statistics ( events ) ; successCount = successAndFail . getKey ( ) ; failCount = successAndFail . getValue ( ) ; logger . info ( "[scheduledTask] scheduled report, total email count: {}" , totalCount ) ; EventMonitor . DEFAULT . logEvent ( EMAIL_SERVICE_CAT_TYPE , "total sent out" , totalCount ) ; EventMonitor . DEFAULT . logEvent ( EMAIL_SERVICE_CAT_TYPE , "success sent out" , successCount ) ; EventMonitor . DEFAULT . logEvent ( EMAIL_SERVICE_CAT_TYPE , "fail sent out" , failCount ) ; logger . info ( "[scheduledTask] scheduled report fail: {}" , failCount ) ; }
@ VisibleForTesting protected void scheduledTask ( ) { logger . info ( "[scheduledTask] start retrieving info" ) ; int totalCount , successCount , failCount ; List < EventModel > events = alertEventService . getLastHourAlertEvent ( ) ; totalCount = events . size ( ) ; Pair < Integer , Integer > successAndFail = statistics ( events ) ; successCount = successAndFail . getKey ( ) ; failCount = successAndFail . getValue ( ) ; EventMonitor . DEFAULT . logEvent ( EMAIL_SERVICE_CAT_TYPE , "total sent out" , totalCount ) ; EventMonitor . DEFAULT . logEvent ( EMAIL_SERVICE_CAT_TYPE , "success sent out" , successCount ) ; EventMonitor . DEFAULT . logEvent ( EMAIL_SERVICE_CAT_TYPE , "fail sent out" , failCount ) ; logger . info ( "[scheduledTask] complete retrieving info" ) ; }
public void test() { if ( ! destination . exists ( ) || isDifferent ( bytes ) ) { log . info ( destination + " changed, will change" ) ; code_block = TryStatement ;  } else { log . info ( destination + " didn't change, skip rewriting" ) ; } }
public void test() { if ( ! destination . exists ( ) || isDifferent ( bytes ) ) { code_block = TryStatement ;  log . info ( destination + " created" ) ; } else { log . info ( destination + " already exists" ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; FileSystem fs = FileSystem . get ( conf ) ; fs . delete ( new Path ( profile . getJobFile ( ) ) . getParent ( ) , true ) ; } catch ( IOException e ) { LOG . warn ( "Failed to clean up temporary directory {}" , profile . getJobFile ( ) , e ) ; } }
public void test() { if ( t instanceof GemFireSecurityException ) { logger . error ( message , t ) ; } else { logger . error ( message , t ) ; } }
public void test() { if ( t instanceof GemFireSecurityException ) { securityLogger . error ( message , t ) ; } else { logger . error ( message , t ) ; } }
@ Test public void markers ( ) { Logger log = LogManager . getLogger ( Log4j2NativeApiTest . class ) ; Marker m1 = MarkerManager . getMarker ( "m1" ) ; m1 . addParents ( MarkerManager . getMarker ( "p1" ) , MarkerManager . getMarker ( "p2" ) ) ; log . info ( " markers" ) ; }
public void test() { { echo 'yes' }" . getBytes ( "UTF-8" ) ; GHContentUpdateResponse updateResponse = helper . getGithubRepository ( ) . createContent ( content , "Jenfile" , "Jenfile" , "main" ) ; helper . getGithubRepository ( ) . createRef ( "refs/heads/branch1" , updateResponse . getCommit ( ) . getSHA1 ( ) ) ; logger . info ( "Github repository created" ) ; helper . getGithubRepository ( ) . createContent ( "hi there" , "newfile" , "newfile" , "ranch1" ) ; creationPage . createPipeline ( helper . getAccessToken ( ) , helper . getOrganizationOrUsername ( ) , helper . getActualRepositoryName ( ) ) ; } }
private List < OmObservation > querySeriesObservation ( GetObservationRequest request , Session session ) throws OwsExceptionReport , ConverterException { code_block = IfStatement ; Locale requestedLocale = getRequestedLocale ( request ) ; String pdf = getProcedureDescriptionFormat ( request . getResponseFormat ( ) ) ; final long start = System . currentTimeMillis ( ) ; List < String > features = request . getFeatureIdentifiers ( ) ; Collection < DataEntity < ? > > seriesObservations = Lists . newArrayList ( ) ; AbstractSeriesDAO seriesDAO = daoFactory . getSeriesDAO ( ) ; code_block = ForStatement ; final List < OmObservation > result = new LinkedList < > ( ) ; code_block = IfStatement ; toSosObservation ( new ArrayList < > ( seriesObservations ) , request , requestedLocale , pdf , observationCreatorContext , session ) . forEachRemaining ( result :: add ) ; LOGGER . info ( "Query {} took {} ms" , System . currentTimeMillis ( ) - start ) ; return result ; }
public void test() { try { List < EnvironmentLogic > del = new ArrayList < > ( environments ) ; code_block = ForStatement ; } catch ( Exception e ) { logger . error ( "logging environments: " + e . getMessage ( ) , e ) ; } finally { environments . clear ( ) ; } }
public void test() { try { Bootstrap b = new Bootstrap ( ) ; b . group ( group ) . channel ( NioDatagramChannel . class ) . option ( ChannelOption . SO_BROADCAST , false ) . handler ( clientInitializer ) ; b . connect ( host , port ) . sync ( ) ; synchronized ( scenarioHandler ) code_block = "" ; } catch ( Exception ex ) { LOG . error ( "Error" , ex ) ; } finally { LOG . debug ( "shutting down" ) ; code_block = TryStatement ;  } }
public void test() { try { group . shutdownGracefully ( ) . get ( ) ; LOG . debug ( "shutdown succesful" ) ; } catch ( InterruptedException | ExecutionException e ) { LOG . error ( "Error" , e ) ; } }
public void test() { try { code_block = IfStatement ; HelixCustomCodeRunner customCodeRunner = new HelixCustomCodeRunner ( manager , ZK_ADDR ) ; customCodeRunner . invoke ( _callback ) . on ( ChangeType . LIVE_INSTANCE ) . usingLeaderStandbyModel ( "TestParticLeader" ) . start ( ) ; } catch ( Exception e ) { LOG . error ( "Exception caught" , e ) ; } }
public void test() { { log . info ( Color . GREEN + "Shape_7 : invalid column shape_type" + Color . NORMAL ) ; Context context = new Context ( ) ; CheckPointReport result = verifyValidation ( log , context , "shape_7" , GTFS_1_GTFS_Common_16 , SEVERITY . ERROR , RESULT . NOK , true ) ; Assert . assertEquals ( result . getCheckPointErrorCount ( ) , 1 , "detail count" ) ; code_block = ForStatement ; } }
public void test() { if ( IS_DEBUG ) { LOG . debug ( "Checksum encoding : {}" , Strings . dumpBytes ( buffer . array ( ) ) ) ; LOG . debug ( "Checksum initial value : {}" , this ) ; } }
public void test() { try { PortalServiceUtil . testAutoSyncHibernateSessionStateOnTxCreation ( ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( startTime == 0 ) { log . debug ( "No matching request to remote server, ignoring." ) ; } else { elapsedTime = endTime - startTime - idleTime ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { o = clazz . newInstance ( ) ; return o ; } catch ( InstantiationException ie ) { log . warn ( "Illegal constructor exception,caused by :" + ie . getMessage ( ) ) ; return null ; } catch ( IllegalAccessException iae ) { log . warn ( "Illegal access exception,caused by :" + iae . getMessage ( ) ) ; return null ; } }
public void test() { try { o = clazz . newInstance ( ) ; return o ; } catch ( InstantiationException ie ) { log . warn ( "Instantiation exception,caused by :" + ie . getMessage ( ) ) ; return null ; } catch ( IllegalAccessException iae ) { log . warn ( "IllegalAccessException,caused by :" + iae . getMessage ( ) ) ; return null ; } }
protected void configureChannels ( ) { Channel channel ; ChannelTypeUID channelTypeUID ; ChannelUID channelUID ; List < Channel > channelList = new ArrayList < > ( ) ; List < Channel > existingChannels = getThing ( ) . getChannels ( ) ; code_block = IfStatement ; logger . debug ( "Configuring channels for keypad {}" , integrationId ) ; ThingBuilder thingBuilder = editThing ( ) ; code_block = ForStatement ; code_block = ForStatement ; code_block = ForStatement ; thingBuilder . withChannels ( channelList ) ; updateThing ( thingBuilder . build ( ) ) ; logger . debug ( "Done configuring channels for keypad {}" , integrationId ) ; }
public void test() { try { LOG . debug ( "Policy Check Succeeded" ) ; getDocSubmissionUtils ( ) . convertDataToFileLocationIfEnabled ( body ) ; response = sendToAdapter ( body , assertion ) ; } catch ( LargePayloadException lpe ) { LOG . error ( "LargePayloadException" ) ; response = MessageGeneratorUtils . getInstance ( ) . createXDRAckWithRegistryErrorResponse ( ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
protected ProjectOverviewDTO getProjectOverview ( UIContext uiContext ) { StopWatch watch = new StopWatch ( ) ; ProjectOverviewDTO projectOverview = new ProjectOverviewDTO ( ) ; File rootFolder = getSelectionFolder ( uiContext ) ; code_block = IfStatement ; log . debug ( "ProjectOverview took {} ms." , watch . taken ( ) ) ; return projectOverview ; }
public void test() { { managedCustomerPage = managedCustomerService . get ( selector ) ; LOGGER . info ( "Retrieve results from {}" , managedCustomerPage ) ; addClientCustomerIds ( managedCustomerPage , clientCustomerIdsSet ) ; LOGGER . info ( "{} accounts retrieved." , clientCustomerIdsSet . size ( ) ) ; offset += NUMBER_OF_RESULTS ; selector = builder . increaseOffsetBy ( NUMBER_OF_RESULTS ) . build ( ) ; } }
public void test() { try { publish . payload ( delivery . message ( ) . getBodyAs ( Buffer . class ) ) ; } catch ( FilterException e ) { log . error ( "Unable to process delivery message" , e ) ; } }
private static List < URI > resolve ( final String srvName , final String protocol , final String domain , final DnsSrvResolver resolver ) { final String name ; code_block = SwitchStatement ; final List < LookupResult > lookupResults = resolver . resolve ( name ) ; final ImmutableList . Builder < URI > endpoints = ImmutableList . builder ( ) ; code_block = ForStatement ; final ImmutableList < URI > uris = endpoints . build ( ) ; LOG . debug ( "Resolved {} endpoint(s) are {}" , uris , srvName ) ; return uris ; }
public void test() { if ( ! reallyEnded ) { logger . warn ( "Exception was received: " + e . getMessage ( ) ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { if ( e instanceof org . apache . accumulo . core . clientImpl . thrift . ThriftNotActiveServiceException ) { result . tnase = ( org . apache . accumulo . core . clientImpl . thrift . ThriftNotActiveServiceException ) e ; result . setTnaseIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof org . apache . accumulo . core . clientImpl . thrift . ThriftNotActiveServiceException ) { result . tnase = ( org . apache . accumulo . core . clientImpl . thrift . ThriftNotActiveServiceException ) e ; result . setTnaseIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof org . apache . accumulo . core . clientImpl . thrift . ThriftNotActiveServiceException ) { result . tnase = ( org . apache . accumulo . core . clientImpl . thrift . ThriftNotActiveServiceException ) e ; result . setTnaseIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { try { fcall . sendResponse ( fb , msg , msgType , seqid ) ; } catch ( java . lang . Exception ex ) { _LOGGER . error ( "Exception writing to internal frame buffer" , ex ) ; fb . close ( ) ; } }
public void test() { try { Optional . ofNullable ( descriptor . getCharset ( ) ) . map ( Charset :: forName ) . ifPresent ( currentlyBuildMimePart :: charset ) ; } catch ( Exception e ) { LOGGER . warn ( "Unable to determine charset for name [" + descriptor . getCharset ( ) + "]." , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( result . isRight ( ) ) { log . info ( "Failed to delete template {}" , id ) ; continue ; } }
public void test() { if ( ! accepted ) { _log . error ( "Media package " + mediaPackage . getId ( ) + " for media package " + mediaPackage . getId ( ) ) ; } }
@ Override public void onPing ( String ping ) { logger . info ( "ping {}" , ping ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Failed to clean up." , e ) ; } }
public void test() { if ( context != null ) { InputStream xwikicfgis = context . getResourceAsStream ( configurationLocation ) ; LOGGER . debug ( "Failed to load the file [" + configurationLocation + "] as a resource " + "using the Servlet Context. Trying to load it as classpath resource..." ) ; code_block = IfStatement ; } else { LOGGER . error ( "Failed to load the configuration location [" + configurationLocation + "]" ) ; } }
public void test() { try { InputStream stdout = process . getInputStream ( ) ; synchronized ( stdout ) code_block = "" ; } catch ( IOException e ) { LOG . error ( "Failed to read output stream: " + e , e ) ; } }
@ Override public void deleteAVUMetadata ( final String resourceName , final AvuData avuData ) throws InvalidResourceException , JargonException { code_block = IfStatement ; code_block = IfStatement ; log . info ( "deleting avu metadata for resourceName: {}" , resourceName ) ; log . info ( "avu: {}" , avuData ) ; final ModAvuMetadataInp modifyAvuMetadataInp = ModAvuMetadataInp . instanceForDeleteResourceMetadata ( resourceName , avuData ) ; log . debug ( "sending avu request" ) ; code_block = TryStatement ;  log . debug ( "metadata removed" ) ; }
@ Override public void deleteAVUMetadata ( final String resourceName , final AvuData avuData ) throws InvalidResourceException , JargonException { code_block = IfStatement ; code_block = IfStatement ; log . info ( "delete avu metadata from resource: {}" , resourceName ) ; log . info ( "with avu metadata: {}" , avuData ) ; final ModAvuMetadataInp modifyAvuMetadataInp = ModAvuMetadataInp . instanceForDeleteResourceMetadata ( resourceName , avuData ) ; log . debug ( "sending avu request" ) ; code_block = TryStatement ;  log . debug ( "metadata removed" ) ; }
@ Override public void deleteAVUMetadata ( final String resourceName , final AvuData avuData ) throws InvalidResourceException , JargonException { code_block = IfStatement ; code_block = IfStatement ; log . info ( "delete avu metadata from resource: {}" , resourceName ) ; log . info ( "avu: {}" , avuData ) ; final ModAvuMetadataInp modifyAvuMetadataInp = ModAvuMetadataInp . instanceForDeleteResourceMetadata ( resourceName , avuData ) ; log . debug ( "metadata removed" ) ; code_block = TryStatement ;  log . debug ( "metadata removed" ) ; }
public void test() { try { getIRODSProtocol ( ) . irodsFunction ( modifyAvuMetadataInp ) ; } catch ( JargonException je ) { code_block = IfStatement ; log . error ( "jargon exception removing AVU metadata" , je ) ; throw je ; } }
@ Override public void deleteAVUMetadata ( final String resourceName , final AvuData avuData ) throws InvalidResourceException , JargonException { code_block = IfStatement ; code_block = IfStatement ; log . info ( "delete avu metadata from resource: {}" , resourceName ) ; log . info ( "avu: {}" , avuData ) ; final ModAvuMetadataInp modifyAvuMetadataInp = ModAvuMetadataInp . instanceForDeleteResourceMetadata ( resourceName , avuData ) ; log . debug ( "sending avu request" ) ; code_block = TryStatement ;  log . debug ( "metadata deleted" ) ; }
@ Override public void warn ( String string , Object o ) { logger . warn ( string , o ) ; }
@ Override public Response toResponse ( ContextInferrenceFailedException exception ) { Object entity = Collections . EMPTY_LIST ; code_block = IfStatement ; LOG . error ( exception . toString ( ) ) ; return Response . status ( Status . NOT_FOUND ) . entity ( entity ) . header ( "TotalCount" , 0 ) . build ( ) ; }
@ Override public void initialize ( ) { logger . debug ( "Initializing DSMR device" ) ; final DSMRDeviceConfiguration deviceConfig = getConfigAs ( DSMRDeviceConfiguration . class ) ; code_block = IfStatement ; updateStatus ( ThingStatus . UNKNOWN ) ; receivedTimeoutNanos = TimeUnit . SECONDS . toNanos ( deviceConfig . receivedTimeout ) ; final DSMRDevice dsmrDevice = createDevice ( deviceConfig ) ; resetLastReceivedState ( ) ; this . dsmrDevice = dsmrDevice ; dsmrDeviceRunnable = new DSMRDeviceRunnable ( dsmrDevice , this ) ; dsmrDeviceThread = new Thread ( dsmrDeviceRunnable ) ; dsmrDeviceThread . setName ( "OH-binding-" + getThing ( ) . getUID ( ) ) ; dsmrDeviceThread . setDaemon ( true ) ; dsmrDeviceThread . start ( ) ; watchdog = scheduler . scheduleWithFixedDelay ( this :: alive , receivedTimeoutNanos , receivedTimeoutNanos , TimeUnit . NANOSECONDS ) ; }
public void test() { try { code_block = IfStatement ; } catch ( InvocationTargetException e ) { logger . error ( "Problem with weighting model named: " + name , e ) ; } catch ( Exception e ) { logger . error ( "Problem with weighting model named: " + name , e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( InvocationTargetException e ) { logger . error ( "Recursive problem with weighting model named: " + name , e ) ; } catch ( Exception e ) { logger . error ( "Unknown problem with weighting model named: " + name , e ) ; } }
@ Override public Void call ( ) throws Exception { code_block = IfStatement ; ManagedTask task = tasks . get ( id ) ; code_block = IfStatement ; TaskController controller = null ; String failure = null ; code_block = TryStatement ;  code_block = IfStatement ; task = new ManagedTask ( id , originalSpec , spec , controller , TaskStateType . PENDING ) ; tasks . put ( id , task ) ; long delayMs = task . startDelayMs ( time . milliseconds ( ) ) ; task . startFuture = scheduler . schedule ( executor , new RunTask ( task ) , delayMs ) ; log . info ( "{} started" , id ) ; return null ; }
public void test() { try { federationBatchInfoService . applyRetentionPolicy ( retentionDays ) ; logger . debug ( "Retention policy applied successfully." ) ; } catch ( Exception e ) { logger . error ( "Failed to apply retention policy." , e ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { try { return objMapper . writeValueAsString ( tuple ) ; } catch ( JsonProcessingException e ) { logger . error ( "Error writing tuple" , e ) ; } }
public void test() { try { adminMgr . deassignUser ( userRole ) ; } catch ( SecurityException se ) { LOG . warn ( "addUserRoles tenant={} roleName={} caught SecurityException={}" , getTenant ( ) , userRole . getName ( ) , se ) ; } }
public void test() { if ( currentElement == null ) { LOG . log ( Level . WARNING , "Ignoring unknown element, path: {0}" , attr ) ; } else { pendingAttrs . add ( new PendingAttr ( attr , path , conf ) ) ; } }
public void test() { try { StringWriter writer = new StringWriter ( ) ; mapper . writeValue ( writer , answer ) ; return writer . toString ( ) ; } catch ( IOException e ) { logger . error ( e . getMessage ( ) , e ) ; throw new IOException ( e . getMessage ( ) ) ; } }
public void test() { if ( isDebugEnabled ) { logger . debug ( "Closing client with key {}" , key ) ; } }
public void test() { if ( isDebugEnabled ) { logger . debug ( "Closing client with key {}" , key ) ; } }
public void test() { if ( isDebugEnabled ) { logger . debug ( "Closing client with key {}" , key ) ; } }
public void test() { if ( isDebugEnabled ) { logger . debug ( "Closing client with key {}" , key ) ; } }
public void test() { if ( handler == null ) { continue ; } }
public void test() { if ( isDebugEnabled ) { logger . debug ( "Closing client with key {}" , key ) ; } }
public void test() { try { createResponse = provider . create ( createRequest ) ; createdMetacards = createResponse . getCreatedMetacards ( ) ; } catch ( IngestException e ) { printErrorMessage ( String . format ( "Received error while ingesting: %s%n" , e . getMessage ( ) ) ) ; LOGGER . debug ( "Error during ingest:" , e ) ; return ingestSingly ( provider , metacards ) ; } catch ( SourceUnavailableException e ) { printErrorMessage ( String . format ( "Received error while ingesting: %s%n" , e . getMessage ( ) ) ) ; LOGGER . debug ( "Error during ingest:" , e ) ; return createdMetacards ; } catch ( Exception e ) { printErrorMessage ( String . format ( "Unexpected Exception received while ingesting: %s%n" , e . getMessage ( ) ) ) ; LOGGER . debug ( "Unexpected Exception during ingest:" , e ) ; return createdMetacards ; } }
public void test() { try { createResponse = provider . create ( createRequest ) ; createdMetacards = createResponse . getCreatedMetacards ( ) ; } catch ( IngestException e ) { printErrorMessage ( String . format ( "Received error while ingesting: %s%n" , e . getMessage ( ) ) ) ; LOGGER . debug ( "Error during ingest. Attempting to ingest batch individually." ) ; return ingestSingly ( provider , metacards ) ; } catch ( SourceUnavailableException e ) { printErrorMessage ( String . format ( "Received error while ingesting: %s%n" , e . getMessage ( ) ) ) ; LOGGER . debug ( "Error during ingest. Attempting to ingest batch separately." , e ) ; return createdMetacards ; } catch ( Exception e ) { printErrorMessage ( String . format ( "Unexpected Exception received while ingesting: %s%n" , e . getMessage ( ) ) ) ; LOGGER . debug ( "Unexpected Exception during ingest:" , e ) ; return createdMetacards ; } }
public void test() { try { createResponse = provider . create ( createRequest ) ; createdMetacards = createResponse . getCreatedMetacards ( ) ; } catch ( IngestException e ) { printErrorMessage ( String . format ( "Received error while ingesting: %s%n" , e . getMessage ( ) ) ) ; LOGGER . debug ( "Error during ingest. Attempting to ingest batch individually." ) ; return ingestSingly ( provider , metacards ) ; } catch ( SourceUnavailableException e ) { printErrorMessage ( String . format ( "Received error while ingesting: %s%n" , e . getMessage ( ) ) ) ; LOGGER . debug ( "Error during ingest:" , e ) ; return createdMetacards ; } catch ( Exception e ) { printErrorMessage ( String . format ( "Unexpected Exception received while ingesting: %s%n" , e . getMessage ( ) ) ) ; LOGGER . debug ( "Exception during ingest." , e ) ; return createdMetacards ; } }
public void test() { try { log . info ( "Adding adaptor to " + type ) ; addByName ( null , adaptorName , type , params , offset , numRetries , retryInterval ) ; } catch ( Exception e ) { log . warn ( "Exception in AddAdaptorTask.run" , e ) ; e . printStackTrace ( ) ; } }
public void test() { try { log . info ( "Trying to resend the add command [" + adaptorName + "][" + offset + "][" + params + "] [" + numRetries + "]" ) ; addByName ( null , adaptorName , type , params , offset , numRetries , retryInterval ) ; } catch ( Exception e ) { log . error ( "Failed to resend command [" + adaptorName + "][" + offset + "][" + params + "] [" + numRetries + "]" ) ; e . printStackTrace ( ) ; } }
public void test() { if ( s_logger . isInfoEnabled ( ) ) { s_logger . info ( "Unable to recover from " + toString ( ) ) ; } }
public void test() { try { Pair < Boolean , String > result = SshHelper . sshExecute ( controlIp , DefaultDomRSshPort , "root" , getSystemVmKeyFile ( ) , null , "/bin/ping" + args ) ; if ( result . first ( ) ) return new Answer ( cmd ) ; } catch ( Exception e ) { s_logger . warn ( "ping command failed" , e ) ; } }
public void test() { try { HostMO hostMo = ( HostMO ) hyperHost ; ClusterMO clusterMo = new ClusterMO ( context , hostMo . getHyperHostCluster ( ) ) ; VmwareManager mgr = context . getStockObject ( VmwareManager . CONTEXT_STOCK_NAME ) ; List < Pair < ManagedObjectReference , String > > hosts = clusterMo . getClusterHosts ( ) ; code_block = ForStatement ; } catch ( Exception e ) { s_logger . error ( "Error checking hypervisor hosts" , e ) ; } }
@ Override public void reduce ( Text key , Iterable < BytesWritable > values , Reducer < Text , BytesWritable , Text , Text > . Context context ) throws IOException , InterruptedException { log . info ( "processing: " + key ) ; long startTime = new Date ( ) . getTime ( ) ; Configuration conf = context . getConfiguration ( ) ; initialMaxDocsSetSize = conf . getInt ( "INITIAL_MAX_DOCS_SET_SIZE" , initialMaxDocsSetSize ) ; process ( key , context , values , initialMaxDocsSetSize ) ; log . info ( "time [msec]: " + ( new Date ( ) . getTime ( ) - startTime ) ) ; }
@ Override public void reduce ( Text key , Iterable < BytesWritable > values , Reducer < Text , BytesWritable , Text , Text > . Context context ) throws IOException , InterruptedException { log . info ( "starting reduce, key: " + key . toString ( ) ) ; long startTime = new Date ( ) . getTime ( ) ; Configuration conf = context . getConfiguration ( ) ; initialMaxDocsSetSize = conf . getInt ( "INITIAL_MAX_DOCS_SET_SIZE" , initialMaxDocsSetSize ) ; process ( key , context , values , initialMaxDocsSetSize ) ; log . info ( "Completed: key: " + key . toString ( ) ) ; }
public void test() { switch ( reusableLog . getLogType ( ) ) { case LogType . UPDATE : case LogType . ENTITY_COMMIT : case LogType . FILTER : logManager . log ( reusableLog ) ; break ; case LogType . JOB_COMMIT : case LogType . ABORT : RemoteLogRecord jobTerminationLog = new RemoteLogRecord ( ) ; TransactionUtil . formJobTerminateLogRecord ( jobTerminationLog , reusableLog . getTxnId ( ) , reusableLog . getLogType ( ) == LogType . JOB_COMMIT ) ; jobTerminationLog . setRequester ( this ) ; jobTerminationLog . setReplicationWorker ( worker ) ; jobTerminationLog . setLogSource ( LogSource . REMOTE ) ; logManager . log ( jobTerminationLog ) ; break ; case LogType . FLUSH : RemoteLogRecord flushLog = new RemoteLogRecord ( ) ; TransactionUtil . formFlushLogRecord ( flushLog , reusableLog . getDatasetId ( ) , reusableLog . getResourcePartition ( ) , reusableLog . getFlushingComponentMinId ( ) , reusableLog . getFlushingComponentMaxId ( ) , null ) ; flushLog . setRequester ( this ) ; flushLog . setLogSource ( LogSource . REMOTE ) ; flushLog . setMasterLsn ( reusableLog . getLSN ( ) ) ; logManager . log ( flushLog ) ; break ; default : log . warn ( "unknown LogType {}" , reusableLog . getLogType ( ) ) ; } }
public void test() { if ( timestamp > 0 ) { logger . info ( "Expired token to " + timestamp + ": " + new Date ( timestamp ) ) ; return true ; } }
public void test() { try { code_block = IfStatement ; } catch ( IOException ex ) { _log . error ( ex . getMessage ( ) , ex ) ; httpResponse . setStatus ( 500 ) ; } }
public void test() { try { callback . onTick ( System . currentTimeMillis ( ) ) ; } catch ( Exception e ) { log . warn ( "Callback exception" , e ) ; } }
public void test() { try { ObjectNode node = MAPPER . createObjectNode ( ) ; node . put ( MESSAGE_ATTRIBUTE , body . toString ( ) ) ; String newBody = MAPPER . writerWithDefaultPrettyPrinter ( ) . writeValueAsString ( node ) ; LOG . debug ( "Post-processing of TemplateStep's message into JSON: {}" , newBody ) ; exchange . getIn ( ) . setBody ( newBody ) ; } catch ( Exception ex ) { throw new IllegalStateException ( "Failed to post-process the TemplateStep's message into JSON" , ex ) ; } }
public void test() { try { SearchTemplateResponse response = getClient ( ) . searchTemplate ( searchTemplateRequest , RequestOptions . DEFAULT ) ; SearchResponse searchResponse = response . getResponse ( ) ; List terms = ( ( ParsedTerms ) searchResponse . getAggregations ( ) . asMap ( ) . get ( "client_versions" ) ) . getBuckets ( ) ; List < String > results = ( List < String > ) terms . stream ( ) . map ( o -> ( ( ParsedTerms . ParsedBucket ) o ) . getKey ( ) ) . collect ( Collectors . toList ( ) ) ; handler . handle ( AsyncResultImpl . create ( results ) ) ; } catch ( IOException e ) { LOGGER . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { log . trace ( "Invoking synchronization.onAfterRoute: {} with {}" , synchronization , exchange ) ; ( ( SynchronizationRouteAware ) synchronization ) . onAfterRoute ( route , exchange ) ; } catch ( Throwable e ) { log . warn ( "Error during synchronization.onAfterRoute() processing" , e ) ; } }
public void test() { try { keyspace . prepareColumnMutation ( CassandraModel . CF_METRICS_METADATA , locator , metaKey ) . putValue ( metaValue , StringMetadataSerializer . get ( ) , null ) . execute ( ) ; } catch ( ConnectionException e ) { Instrumentation . markWriteError ( e ) ; log . error ( "Connection exception persisting metadata" , e ) ; throw e ; } finally { ctx . stop ( ) ; } }
public void test() { if ( logNum == 0 && t != null ) { LOG . warn ( t . getMessage ( ) ) ; } }
public void test() { if ( maxLogNum . intValue ( ) < 0 || currentLogNum . intValue ( ) < maxLogNum . intValue ( ) ) { logger . warn ( String . format ( "Requested log %s will not be recorded" , currentLogNum . intValue ( ) ) ) ; } }
public void test() { try { String request = params . getMandatoryString ( "request" ) ; dispatchWmsRequest ( request , params , httpServletRequest , httpServletResponse , catalogue ) ; } catch ( EdalException wmse ) { boolean v130 ; code_block = TryStatement ;  handleWmsException ( wmse , httpServletResponse , v130 ) ; } catch ( SocketException se ) { LOGGER . info ( "Got an IOException dispatching" , se ) ; } catch ( IOException ioe ) { code_block = IfStatement ; throw ioe ; } catch ( Exception e ) { e . printStackTrace ( ) ; throw new IOException ( e ) ; } }
@ Override public SchemaIdVersion addSchemaVersion ( SchemaMetadata schemaMetadata , SchemaVersion schemaVersion , boolean disableCanonicalCheck ) throws InvalidSchemaException , IncompatibleSchemaException , SchemaNotFoundException , SchemaBranchNotFoundException { LOGGER . info ( "addSchemaVersion {}" , schemaVersion ) ; return schemaVersionLifecycleManager . addSchemaVersion ( SchemaBranch . MASTER_BRANCH , schemaMetadata , schemaVersion , x -> registerSchemaMetadata ( x ) , disableCanonicalCheck ) ; }
public void test() { try { coordinator . declare ( txId , request ) ; } catch ( Exception e ) { log . error ( "Error declaring request {}" , request , e ) ; request . onFailure ( e ) ; } }
public void test() { for ( final EndpointDescription desc : availableEndpoints ) { LOG . info ( "Endpoint {} has endpoint {}" , desc . getEndpointId ( ) , desc . getEndpointId ( ) ) ; } }
public void test() { for ( final EndpointDescription desc : availableEndpoints ) { LOG . info ( "Endpoint {} has endpoint {}" , desc . getEndpointId ( ) , desc . getEndpointId ( ) ) ; } }
public void test() { if ( matcher . find ( ) ) { String csrfTokenString = matcher . group ( 1 ) ; LOGGER . debug ( "Found CSRF token: {}" , csrfTokenString ) ; return new DefaultCsrfToken ( CSRF_HEADER_NAME , CSRF_PARAM_NAME , csrfTokenString ) ; } else { throw new SessionAuthenticationException ( "Could not find CSRF_TOKEN variable on login page" ) ; } }
private void connectionUnreachable ( String webSocketServerUrl ) { logger . info ( "connectionUnreachable address: {}" , webSocketServerUrl ) ; retryConnection ( webSocketServerUrl ) ; }
public void test() { if ( activeThrottles . isEmpty ( ) ) { log . trace ( "No active throttling" ) ; } }
public void test() { if ( this . ldapContextFactory == null ) { logger . debug ( "Creating a new LDAP context factory" ) ; DefaultLdapContextFactory defaultFactory = new DefaultLdapContextFactory ( ) ; defaultFactory . setPrincipalSuffix ( this . principalSuffix ) ; defaultFactory . setSearchBase ( this . searchBase ) ; defaultFactory . setUrl ( this . url ) ; defaultFactory . setSystemUsername ( this . systemUsername ) ; defaultFactory . setSystemPassword ( getSystemPassword ( ) ) ; this . ldapContextFactory = defaultFactory ; } }
public void test() { try { Context ctx = cache . getJNDIContext ( ) ; ds = ( DataSource ) ctx . lookup ( "java:/XAPooledDataSource" ) ; } catch ( NamingException e ) { logger . debug ( "failed in naming lookup: " + e ) ; fail ( "failed in naming lookup: " , e ) ; return ; } catch ( Exception e ) { logger . debug ( "Exception caught during naming lookup: " + e ) ; fail ( "failed in naming lookup: " , e ) ; return ; } }
public void test() { try { Context ctx = cache . getJNDIContext ( ) ; ds = ( DataSource ) ctx . lookup ( "java:/XAPooledDataSource" ) ; } catch ( NamingException e ) { logger . debug ( "Naming Exception caught in lookup: " + e ) ; fail ( "failed in naming lookup: " , e ) ; return ; } catch ( Exception e ) { logger . debug ( "Exception caught in lookup: " + e ) ; fail ( "failed in naming lookup: " , e ) ; return ; } }
public void test() { try { code_block = ForStatement ; } catch ( SQLException e ) { logger . debug ( "runTest1 SQL Exception caught: " , e ) ; fail ( "runTest1 SQL Exception caught: " , e ) ; } catch ( Exception e ) { logger . debug ( "Exception caught in runTest1: " + e ) ; fail ( "Exception caught in runTest1: " , e ) ; e . printStackTrace ( ) ; } }
public void test() { try { code_block = ForStatement ; } catch ( SQLException e ) { logger . debug ( "Success SQLException caught in runTest1: " + e ) ; fail ( "runTest1 SQL Exception caught: " , e ) ; } catch ( Exception e ) { logger . debug ( "Exception caught in runTest1: " + e ) ; fail ( "Exception caught in runTest1: " , e ) ; e . printStackTrace ( ) ; } }
public void test() { if ( subscriber . isUnsubscribed ( ) ) { log . debug ( "unsubscribed" ) ; } else { log . debug ( "onCompleted" ) ; subscriber . onCompleted ( ) ; } }
public void test() { if ( subscriber . isUnsubscribed ( ) ) { log . debug ( "unsubscribed" ) ; } else { log . debug ( "subscribed" ) ; subscriber . onCompleted ( ) ; } }
@ Override public void onAuthenticationSuccess ( HttpServletRequest request , HttpServletResponse response , Authentication authentication ) throws IOException , ServletException { String userId = authentication . getName ( ) ; LOG . debug ( "Login successful for user {}" , userId ) ; Cookie newCookie = basicLoginService . createNewCookie ( userId ) ; response . addCookie ( newCookie ) ; response . setStatus ( HttpStatus . OK . value ( ) ) ; response . setContentType ( MediaType . APPLICATION_JSON_VALUE ) ; response . sendRedirect ( BasicLoginConstants . URI_MAIN ) ; return ; }
public void test() { try { lm . prepareDAG ( new FilterClassifierApp ( ) , conf ) ; LocalMode . Controller lc = lm . getController ( ) ; lc . run ( 20000 ) ; } catch ( Exception ex ) { logger . info ( ex . getMessage ( ) ) ; } }
public void test() { if ( primaryEmail == null ) { LOGGER . info ( "Cant send claim reminder email is null" ) ; return ; } }
public void test() { if ( ! notifications . isEmpty ( ) ) { EmailMessage digestMessage ; code_block = IfStatement ; digestMessage . setFrom ( DIGEST_FROM_ADDRESS ) ; digestMessage . setTo ( primaryEmail . getEmail ( ) ) ; boolean successfullySent = mailGunManager . sendEmail ( digestMessage . getFrom ( ) , digestMessage . getTo ( ) , digestMessage . getSubject ( ) , digestMessage . getBodyText ( ) , digestMessage . getBodyHtml ( ) ) ; LOG . info ( "Sending email: " + successfullySent ) ; code_block = IfStatement ; } }
public void test() { try { Float emailFrequencyDays = null ; Date recordActiveDate = null ; recordActiveDate = ( Date ) element [ 1 ] ; List < Notification > notifications = notificationManager . findNotificationsToSend ( orcid , emailFrequencyDays , recordActiveDate ) ; EmailEntity primaryEmail = emailDao . findPrimaryEmail ( orcid ) ; code_block = IfStatement ; code_block = IfStatement ; } catch ( RuntimeException e ) { LOGGER . error ( "Caught exception while processing user record" , e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( KBArticleServiceUtil . class , "fetchKBArticleByUrlTitle" , _fetchKBArticleByUrlTitleParameterTypes8 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , kbFolderId , urlTitle ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . knowledge . base . model . KBArticle ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { UserWorkspace userWorkspace = getUserWorkspace ( ) ; RulesProject selectedProject = userWorkspace . getProject ( repositoryId , currentProjectName , false ) ; AProjectResource projectResource ; code_block = IfStatement ; file = File . createTempFile ( "export-" , "-file" ) ; IOUtils . copyAndClose ( projectResource . getContent ( ) , new FileOutputStream ( file ) ) ; addCookie ( cookieName , "success" , - 1 ) ; final FacesContext facesContext = FacesContext . getCurrentInstance ( ) ; HttpServletResponse response = ( HttpServletResponse ) WebStudioUtils . getExternalContext ( ) . getResponse ( ) ; ExportFile . writeOutContent ( response , file , getFileName ( ) ) ; facesContext . responseComplete ( ) ; } catch ( Exception e ) { String msg = "Failed to export file version. " ; log . error ( msg , e ) ; addCookie ( cookieName , msg + e . getMessage ( ) , - 1 ) ; } finally { FileUtils . deleteQuietly ( file ) ; } }
public void test() { try { boolean result = keeperService . isKeeper ( hostPort ) ; return GenericRetMessage . createGenericRetMessage ( result ) ; } catch ( Exception e ) { logger . error ( "checkIsKeeper:{}" , e . getMessage ( ) ) ; return RetMessage . createFailMessage ( e . getMessage ( ) ) ; } }
@ Override public HttpService addingService ( ServiceReference < HttpService > serviceRef ) { LOG . info ( "creating new HttpServlet" ) ; httpService = super . addingService ( serviceRef ) ; HttpContext httpContext = new CustomHttpContext ( context . getBundle ( ) ) ; registerServlet ( httpContext ) ; registerResources ( httpContext ) ; return httpService ; }
public void test() { try { availableClientQueue . add ( client ) ; } catch ( IllegalStateException ise ) { LOG . warn ( "Client ({}) ise. Closing it now." , ise . getMessage ( ) ) ; client . close ( ) ; } }
public void test() { try { Gson gson = new Gson ( ) ; Set < String > setElementIds = gson . fromJson ( elementId , new TypeToken < Set < String > > ( ) code_block = "" ; . getType ( ) ) ; MediaPackage mediapackage = MediaPackageParser . getFromXml ( mediaPackageXml ) ; result = service . distributeSync ( channelId , mediapackage , setElementIds , checkAvailability ) ; } catch ( IllegalArgumentException e ) { logger . debug ( "Unable to distribute element {} to aws s3 channel" , mediaPackageXml , e ) ; return status ( Status . BAD_REQUEST ) . build ( ) ; } catch ( Exception e ) { logger . warn ( "Unable to distribute media package {}, element {} to aws s3 channel: {}" , mediaPackageXml , elementId , e ) ; return Response . serverError ( ) . status ( Status . INTERNAL_SERVER_ERROR ) . build ( ) ; } }
public void test() { try { Gson gson = new Gson ( ) ; Set < String > setElementIds = gson . fromJson ( elementId , new TypeToken < Set < String > > ( ) code_block = "" ; . getType ( ) ) ; MediaPackage mediapackage = MediaPackageParser . getFromXml ( mediaPackageXml ) ; result = service . distributeSync ( channelId , mediapackage , setElementIds , checkAvailability ) ; } catch ( IllegalArgumentException e ) { logger . debug ( "Unable to distribute element: {}" , e . getMessage ( ) ) ; return status ( Status . BAD_REQUEST ) . build ( ) ; } catch ( Exception e ) { logger . warn ( "Error distributing element" , e ) ; return Response . serverError ( ) . status ( Status . INTERNAL_SERVER_ERROR ) . build ( ) ; } }
public void test() { if ( ! result . isPresent ( ) && log . isDebugEnabled ( ) ) { log . debug ( "Could not find application id in namespace: " + namespace ) ; } }
public void test() { try { String sp = getSpName ( delegate ) ; code_block = IfStatement ; } catch ( MetadataProviderException e ) { log . error ( "Unable to get SpName" , e ) ; } }
@ Test public void parseBsonArrayWithValues ( ) throws IOException { BsonValue a = new BsonString ( "stest" ) ; BsonValue b = new BsonDouble ( 111 ) ; BsonValue c = new BsonBoolean ( true ) ; BsonDocument document = new BsonDocument ( ) . append ( "int32" , new BsonInt32 ( 12 ) ) . append ( "int64" , new BsonInt64 ( 77L ) ) . append ( "bo\"olean" , new BsonBoolean ( true ) ) . append ( "date" , new BsonDateTime ( new Date ( ) . getTime ( ) ) ) . append ( "double" , new BsonDouble ( 12.3 ) ) . append ( "string" , new BsonString ( "pinpoint" ) ) . append ( "objectId" , new BsonObjectId ( new ObjectId ( ) ) ) . append ( "code" , new BsonJavaScript ( "int i = 10;" ) ) . append ( "codeWithScope" , new BsonJavaScriptWithScope ( "int x = y" , new BsonDocument ( "y" , new BsonInt32 ( 1 ) ) ) ) . append ( "regex" , new BsonRegularExpression ( "^test.*regex.*xyz$" , "big" ) ) . append ( "symbol" , new BsonSymbol ( "wow" ) ) . append ( "timestamp" , new BsonTimestamp ( 0x12345678 , 5 ) ) . append ( "undefined" , new BsonUndefined ( ) ) . append ( "binary1" , new BsonBinary ( new byte [ ] code_block = "" ; ) ) . append ( "oldBinary" , new BsonBinary ( BsonBinarySubType . OLD_BINARY , new byte [ ] code_block = "" ; ) ) . append ( "arrayInt" , new BsonArray ( Arrays . asList ( a , b , c , new BsonInt32 ( 7 ) ) ) ) . append ( "document" , new BsonDocument ( "a" , new BsonInt32 ( 77 ) ) ) . append ( "dbPointer" , new BsonDbPointer ( "db.colloding" ,
@ Test public void parseBsonArrayWithValues ( ) throws IOException { BsonValue a = new BsonString ( "stest" ) ; BsonValue b = new BsonDouble ( 111 ) ; BsonValue c = new BsonBoolean ( true ) ; BsonDocument document = new BsonDocument ( ) . append ( "int32" , new BsonInt32 ( 12 ) ) . append ( "int64" , new BsonInt64 ( 77L ) ) . append ( "bo\"olean" , new BsonBoolean ( true ) ) . append ( "date" , new BsonDateTime ( new Date ( ) . getTime ( ) ) ) . append ( "double" , new BsonDouble ( 12.3 ) ) . append ( "string" , new BsonString ( "pinpoint" ) ) . append ( "objectId" , new BsonObjectId ( new ObjectId ( ) ) ) . append ( "code" , new BsonJavaScript ( "int i = 10;" ) ) . append ( "codeWithScope" , new BsonJavaScriptWithScope ( "int x = y" , new BsonDocument ( "y" , new BsonInt32 ( 1 ) ) ) ) . append ( "regex" , new BsonRegularExpression ( "^test.*regex.*xyz$" , "big" ) ) . append ( "symbol" , new BsonSymbol ( "wow" ) ) . append ( "timestamp" , new BsonTimestamp ( 0x12345678 , 5 ) ) . append ( "undefined" , new BsonUndefined ( ) ) . append ( "binary1" , new BsonBinary ( new byte [ ] code_block = "" ; ) ) . append ( "oldBinary" , new BsonBinary ( BsonBinarySubType . OLD_BINARY , new byte [ ] code_block = "" ; ) ) . append ( "arrayInt" , new BsonArray ( Arrays . asList ( a , b , c , new BsonInt32 ( 7 ) ) ) ) . append ( "document" , new BsonDocument ( "a" , new BsonInt32 ( 77 ) ) ) . append ( "dbPointer" , new BsonDbPointer ( "db.colloding" ,
public void test() { if ( LOGGER . isInfoEnabled ( ) ) { LOGGER . info ( "Success!" ) ; } }
public void test() { try { stop ( ) ; } catch ( DeploymentStopException e ) { LOGGER . error ( "Error stopping servlet container" , e ) ; } }
@ Override public Alert findAlertByPrimaryKey ( BigInteger id ) { requireNotDisposed ( ) ; requireArgument ( id != null && id . compareTo ( ZERO ) > 0 , "ID must be a positive non-zero value." ) ; EntityManager em = _emProvider . get ( ) ; em . getEntityManagerFactory ( ) . getCache ( ) . evictAll ( ) ; Alert result = Alert . findByPrimaryKey ( em , id , Alert . class ) ; _logger . debug ( "Query for alert with id : " + id ) ; return result ; }
public void test() { try { g . removeStatements ( xDMA , uri , null ) ; } catch ( NoSuchElementException x ) { LOGGER . trace ( "" , x ) ; } }
public void test() { try { XURI uri = getDataIdURI ( dataId ) ; code_block = TryStatement ;  g . addStatement ( xDMA , uri , Literal . create ( UNO . defaultContext , dataValue ) ) ; } catch ( Exception e ) { LOGGER . warn ( "Unable to add XURI {}" , dataId , e ) ; } }
public void test() { if ( properties . containsKey ( propertyKey ) ) { log . log ( Level . WARNING , "Duplicate definition for property '" + propertyKey + "' detected" ) ; } }
public void test() { try { List < String > allowedGroupCodes = this . getAllowedGroupCodes ( ) ; result = this . getPageManager ( ) . searchPages ( this . getPageCodeToken ( ) , allowedGroupCodes ) ; } catch ( Throwable t ) { _logger . error ( "Error on searching pages" , t ) ; throw new RuntimeException ( "Error on searching pages" , t ) ; } }
public void test() { try ( EntityManagerContainer emc = EntityManagerContainerFactory . instance ( ) . create ( ) ) { ids = attendanceDetailMobileService . listAllAnalyseWithStatus ( emc , 0 ) ; } catch ( Exception e ) { logger . error ( "ERROR" , e ) ; } }
public void test() { try { attendanceDetailMobileAnalyseServiceAdv . analyseAttendanceDetailMobile ( id , false ) ; } catch ( Exception e ) { logger . error ( "Error while analyseAttendanceDetailMobile: " + e . getMessage ( ) , e ) ; } }
public void test() { if ( lsn . v == - 1 || globalCommittedLsn . v == - 1 ) { logger . debug ( "Logged remote LSN {}" , lsn . v ) ; throw new GoneException ( RMResources . Gone ) ; } }
public void test() { if ( ReplicatedResourceClient . isGlobalStrongEnabled ( ) && this . isGlobalStrongRequest ( request , response ) ) { Utils . ValueHolder < Long > lsn = Utils . ValueHolder . initialize ( - 1L ) ; Utils . ValueHolder < Long > globalCommittedLsn = Utils . ValueHolder . initialize ( - 1L ) ; getLsnAndGlobalCommittedLsn ( response , lsn , globalCommittedLsn ) ; code_block = IfStatement ; request . requestContext . globalStrongWriteResponse = response ; request . requestContext . globalCommittedSelectedLSN = lsn . v ; request . requestContext . forceRefreshAddressCache = false ; code_block = IfStatement ; } else { LOGGER . debug ( "Only globalStrong is enabled, so not sending response" ) ; return Mono . just ( response ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceOrderServiceUtil . class , "updateCustomFields" , _updateCustomFieldsParameterTypes44 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , commerceOrderId , serviceContext ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . commerce . model . CommerceOrder ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
@ Override public void analyse ( String source , ArchiveRecordHeader header , InputStream tikainput , SolrRecord solr ) { final String url = Normalisation . sanitiseWARCHeaderValue ( header . getUrl ( ) ) ; LOG . debug ( "Skipping " + url ) ; final long start = System . nanoTime ( ) ; code_block = TryStatement ;  Instrument . timeRel ( "WARCPayloadAnalyzers.analyzetotal" , "WARCPayloadAnalyzers.analyzetikasolrextract" , start ) ; }
public void test() { try { code_block = IfStatement ; } catch ( Exception i ) { LOGGER . error ( "æ¶åçå¤±è´¥" , i ) ; } }
public void test() { try { UnitOfWorkHelper . doneSynchronizations ( correlatedExchange , correlatedExchange . adapt ( ExtendedExchange . class ) . handoverCompletions ( ) , LOG ) ; LOG . debug ( "Record with claim check number: {} committed." , claimCheck ) ; } catch ( Throwable t ) { LOG . error ( "Error while trying to complete record with claim." , t ) ; throw new RuntimeException ( t ) ; } finally { exchangesWaitingForAck [ claimCheck ] = null ; freeSlots . add ( claimCheck ) ; LOG . debug ( "Claim check number: {} freed." , claimCheck ) ; } }
public void test() { if ( cachedRowSet . size ( ) == 0 ) { return Optional . empty ( ) ; } else-if ( cachedRowSet . size ( ) > 1 ) { LOGGER . warn ( MessageFormat . format ( "Found multiple implementations for ScriptDesignTrace {0}. Returning first implementation" , scriptName ) ) ; } }
public void test() { if ( ! match . isBoolean ( ) ) { LOGGER . debug ( "Policy is not boolean, returning false" ) ; return policyRetrievalResult . withError ( ) ; } }
public void test() { try { druidDataSource . close ( ) ; } catch ( Exception e ) { LOG . error ( "Failed to close the DataSource" , e ) ; } }
public void test() { try { running = false ; logger . info ( " stop the canal client adapters" ) ; code_block = IfStatement ; code_block = ForStatement ; DatasourceConfig . DATA_SOURCES . clear ( ) ; } catch ( Throwable e ) { logger . error ( e . getMessage ( ) , e ) ; } finally { logger . info ( " canal client adapters are down." ) ; } }
public void test() { try { running = false ; logger . info ( " stop the canal client adapters" ) ; code_block = IfStatement ; code_block = ForStatement ; DatasourceConfig . DATA_SOURCES . clear ( ) ; } catch ( Throwable e ) { logger . warn ( " something goes wrong when stopping canal client adapters:" , e ) ; } finally { logger . info ( " the canal client adapters is finished" ) ; } }
private void createNetModeTransaction ( ) { LOG . debug ( "Create NetModeTransaction" ) ; final List < ByteString > argList = new ArrayList < > ( ) ; Lifecycle . InstallChaincodeArgs installChaincodeArgs = Lifecycle . InstallChaincodeArgs . newBuilder ( ) . setChaincodeInstallPackage ( ByteString . copyFrom ( chaincodeBytes ) ) . build ( ) ; argList . add ( ByteString . copyFromUtf8 ( action ) ) ; argList . add ( ByteString . copyFrom ( installChaincodeArgs . toByteArray ( ) ) ) ; args ( argList ) ; }
@ Pollable ( expectedSubTaskNumber = 3 , message = "Importing file: {fileName}" ) private void importFile ( DropImporter dropImporter , DropExporter dropExporter , @ MsgArg ( name = "fileName" , accessor = "getName" ) DropFile dropFile , TMTextUnitVariant . Status importStatus , @ ParentTask PollableTask parentTask , @ InjectCurrentTask PollableTask currentTask ) throws DropImporterException , DropExporterException , ImportDropException { log . info ( "Importing file " + currentTask . getName ( ) ) ; downloadDropFileContent ( dropImporter , dropFile , currentTask ) ; UpdateTMWithXLIFFResult updateReport = updateTMWithLocalizedXLIFF ( dropFile , importStatus , currentTask ) ; exportImportedFile ( dropExporter , dropFile , updateReport . getXliffContent ( ) , updateReport . getComment ( ) , currentTask ) ; }
@ Test public void testGetRepositoryPath ( ) { cmisObject . getProperties ( ) . getProperties ( ) . add ( new PropertyId ( PropertiesBase . OBJECTID , "repository" ) ) ; cmisObject . getProperties ( ) . getProperties ( ) . add ( new PropertyString ( PropertiesBase . OBJECTTYPEID , CmisObject . OBJECT_TYPE_FOLDER ) ) ; String path = navService . getRepositoryPath ( cmisObject ) ; assertTrue ( "repository" . equals ( path ) ) ; cmisObject . setProperties ( new CmisProperties ( ) ) ; cmisObject . getProperties ( ) . getProperties ( ) . add ( new PropertyId ( PropertiesBase . OBJECTID , "/admin/pat" ) ) ; cmisObject . getProperties ( ) . getProperties ( ) . add ( new PropertyString ( PropertiesBase . OBJECTTYPEID , null ) ) ; path = navService . getRepositoryPath ( cmisObject ) ; assertTrue ( "Repository path as expected" , "/admin" . equals ( path ) ) ; cmisObject . setProperties ( new CmisProperties ( ) ) ; cmisObject . getProperties ( ) . getProperties ( ) . add ( new PropertyId ( PropertiesBase . OBJECTID , "repository" ) ) ; cmisObject . getProperties ( ) . getProperties ( ) . add ( new PropertyString ( PropertiesBase . OBJECTTYPEID , null ) ) ; path = navService . getRepositoryPath ( cmisObject ) ; assertTrue ( "Repository path as expected" , "" . equals ( path ) ) ; log . info ( "Repository path: " + cmisObject ) ; }
static void abort ( AppContext context , RepairSegment segment , JmxProxy jmxConnection ) { postpone ( context , segment , context . storage . getRepairUnit ( segment . getRepairUnitId ( ) ) ) ; String metric = MetricRegistry . name ( SegmentRunner . class , "abort" , Optional . ofNullable ( segment . getCoordinatorHost ( ) ) . orElse ( "null" ) . replace ( '.' , '-' ) ) ; context . metricRegistry . counter ( metric ) . inc ( ) ; jmxConnection . cancelAllRepairs ( ) ; log . info ( "{} aborted" , jmxConnection . getName ( ) ) ; }
public void test() { try { String protocol = url . getProtocol ( ) ; String path = url . getPath ( ) ; code_block = IfStatement ; } catch ( Throwable t ) { LOGGER . error ( t . getMessage ( ) , t ) ; } }
public void test() { try { context . sendCeaMessage ( resultCode , message ( event ) , null ) ; switchToNextState ( OKAY ) ; } catch ( Exception e ) { logger . debug ( "Caught exception" , e ) ; doDisconnect ( ) ; doEndConnection ( ) ; } }
public void test() { switch ( type ( event ) ) { case DISCONNECT_EVENT : setTimer ( 0 ) ; doEndConnection ( ) ; break ; case TIMEOUT_EVENT : doDisconnect ( ) ; doEndConnection ( ) ; break ; case STOP_EVENT : setTimer ( 0 ) ; doDisconnect ( ) ; switchToNextState ( DOWN ) ; break ; case CEA_EVENT : setTimer ( 0 ) ; code_block = IfStatement ; break ; case CER_EVENT : int resultCode = context . processCerMessage ( key ( event ) , message ( event ) ) ; code_block = IfStatement ; break ; case SEND_MSG_EVENT : throw new IllegalStateException ( "Connection is down" ) ; default : log . debug ( "Unhandled event type: " + type ( event ) ) ; return false ; } }
public FilterSuchenFelderTxt findById ( sernet . gs . reveng . FilterSuchenFelderTxtId id ) { log . debug ( "getting FilterSuchenFelderTxt instance with id: " + id ) ; code_block = TryStatement ;  }
public void test() { if ( instance == null ) { log . debug ( "get successful, no instance found" ) ; } else { log . debug ( "get successful, instance found" ) ; } }
public void test() { if ( instance == null ) { log . debug ( "get successful, no instance found" ) ; } else { log . debug ( "get successful, instance found" ) ; } }
public void test() { try { FilterSuchenFelderTxt instance = ( FilterSuchenFelderTxt ) sessionFactory . getCurrentSession ( ) . get ( "sernet.gs.reveng.FilterSuchenFelderTxt" , id ) ; code_block = IfStatement ; return instance ; } catch ( RuntimeException re ) { log . error ( "get failed" , re ) ; throw re ; } }
public void test() { try { com . liferay . segments . model . SegmentsExperimentRel returnValue = SegmentsExperimentRelServiceUtil . updateSegmentsExperimentRel ( segmentsExperimentRelId , split ) ; return com . liferay . segments . model . SegmentsExperimentRelSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try { assertFalse ( running ) ; running = true ; code_block = TryStatement ;  } catch ( Throwable e ) { log . error ( "Test failed" , e ) ; addException ( e ) ; } }
public void test() { if ( actionMap . get ( "type" ) . equalsIgnoreCase ( FloodlightOFAction . TYPE_OUTPUT . toLowerCase ( ) ) ) { FloodlightOFAction action = new FloodlightOFAction ( ) ; action . setType ( FloodlightOFAction . TYPE_OUTPUT . toLowerCase ( ) ) ; action . setValue ( actionMap . get ( "port" ) ) ; actions . add ( action ) ; } else { logger . warn ( "action map " + actionMap . get ( "type" ) + " not found" ) ; } }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "JDBC::{}" , sql ) ; } }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "JDBC::{}" , sql ) ; } }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "JDBC::{}" , sql ) ; } }
public void test() { if ( Logger . logLevel <= ILogger . INFO ) { Logger . logger . info ( Logger . MISC_LOG + id + ": " + message , error ) ; } }
public void test() { if ( p instanceof SpecParameterIncludingDefinitionForInheritance < ? > ) p = ( ( SpecParameterIncludingDefinitionForInheritance < ? > ) p ) . resolveWithAncestor ( existingP ) ; } else { LOG . warn ( "Given parameter '" + getName ( ) + "' is not a SpecParameterIncludingDefinitionForInheritance" ) ; } }
@ Override public void draggedImage_movedTo ( NSImage image , NSPoint point ) { log . trace ( "draggedImage_movedTo" ) ; }
public void startTimedScheduler ( ) { logger . info ( "Starting scheduled scheduler" ) ; Date startTime = DateTimeUtil . now ( ) . plusMinutes ( START_DELAY_IN_MINUTES ) . toDate ( ) ; MotechEvent event = new MotechEvent ( SUBJECT ) ; RepeatingSchedulableJob job = new RepeatingSchedulableJob ( event , startTime , null , REPEAT_INTERVAL_IN_MINUTES * MILLIS_PER_MINUTE ) ; schedulerService . safeScheduleRepeatingJob ( job ) ; }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Refreshing." ) ; } }
public void test() { if ( tivoConfigData . isKeepConnActive ( ) ) { refreshJob = scheduler . schedule ( runnable , INIT_POLLING_DELAY_S , TimeUnit . SECONDS ) ; logger . debug ( "Status polling '{}' before reconnecting." , tivoConfigData . getPollInterval ( ) ) ; } else-if ( tivoConfigData . doPollChanges ( ) ) { refreshJob = scheduler . scheduleWithFixedDelay ( runnable , INIT_POLLING_DELAY_S , tivoConfigData . getPollInterval ( ) , TimeUnit . SECONDS ) ; logger . debug ( "Status polling '{}' will start in '{}' seconds." , getThing ( ) . getUID ( ) , INIT_POLLING_DELAY_S ) ; } else { tivoConnection . ifPresent ( connection code_block = LoopStatement ; ) ; } }
public void test() { if ( tivoConfigData . isKeepConnActive ( ) ) { refreshJob = scheduler . schedule ( runnable , INIT_POLLING_DELAY_S , TimeUnit . SECONDS ) ; logger . debug ( "Status collection '{}' will start in '{}' seconds." , getThing ( ) . getUID ( ) , INIT_POLLING_DELAY_S ) ; } else-if ( tivoConfigData . doPollChanges ( ) ) { refreshJob = scheduler . scheduleWithFixedDelay ( runnable , INIT_POLLING_DELAY_S , tivoConfigData . getPollInterval ( ) , TimeUnit . SECONDS ) ; logger . debug ( "Status collection '{}' will be polled successfully." , getThing ( ) . getUID ( ) ) ; } else { tivoConnection . ifPresent ( connection code_block = LoopStatement ; ) ; } }
public void test() { if ( systemPropertyName . isPresent ( ) && getProperty ( systemPropertyName . get ( ) ) != null ) { enabled = getBoolean ( systemPropertyName . get ( ) ) ; LOGGER . debug ( "Setting feature {} = {} for artifact [{}]" , feature , enabled , artifactName ) ; } else { enabled = featurePredicate . test ( featureContext ) ; LOGGER . debug ( "Setting feature {} = {} for artifact [{}]" , feature , enabled , artifactName ) ; } }
public void test() { if ( systemPropertyName . isPresent ( ) && getProperty ( systemPropertyName . get ( ) ) != null ) { enabled = getBoolean ( systemPropertyName . get ( ) ) ; LOGGER . debug ( "Setting feature {} = {} for artifact [{}] because of System Property '{}'" , feature , enabled , artifactName , systemPropertyName ) ; } else { LOGGER . debug ( "Setting feature {} = {} for artifact [{}] because it is not possible" , feature , enabled , artifactName ) ; enabled = featurePredicate . test ( featureContext ) ; } }
public void test() { try { xacmlPdp = new XacmlPdp ( dirPath , parser , environmentAttributes , securityLogger ) ; } catch ( PdpException e ) { logger . error ( "Error creating XacmlPdp" , e ) ; } }
@ Override public void warning ( final String msg ) { log . warning ( msg ) ; }
public void test() { try { fail ( vo , e . getErrorCode ( ) ) ; } catch ( Throwable t ) { logger . error ( t . getMessage ( ) , t ) ; } }
public void test() { try { flow . process ( ctx , this ) ; } catch ( WorkFlowException e ) { code_block = TryStatement ;  } catch ( Throwable t ) { LOGGER . error ( "Error executing flow" , t ) ; ErrorCode err = inerr ( t . getMessage ( ) ) ; code_block = TryStatement ;  } }
public void test() { try { fail ( vo , err ) ; } catch ( Throwable t1 ) { logger . error ( "" , t1 ) ; } }
@ Override public void onError ( final String reason ) { PrometheusExporter . instance ( ) . increaseTotalPushRequestsFail ( ) ; logger . warn ( reason ) ; pushMessageMetricsService . appendError ( pushMessageInformation , variant , reason ) ; }
public void test() { try { fileContainer . setBytes ( mpf . getBytes ( ) ) ; createFolderIfNotExists ( ) ; FileCopyUtils . copy ( mpf . getBytes ( ) , new BufferedOutputStream ( new FileOutputStream ( path + mpf . getOriginalFilename ( ) ) ) ) ; } catch ( IOException e ) { logger . error ( e . getMessage ( ) , e ) ; fileContainer . setError ( e . getMessage ( ) ) ; } }
public void test() { for ( ImageResizeMethod method : ImageResizeMethod . values ( ) ) { code_block = IfStatement ; Nd4j . getRandom ( ) . setSeed ( 12345 ) ; SameDiff sd = SameDiff . create ( ) ; boolean preserveAspectRatio = true ; boolean antialias = true ; SDVariable inputImage = sd . var ( Nd4j . rand ( DataType . FLOAT , 1 , 5 , 5 , 3 ) ) ; long [ ] expectedShape = new long [ ] code_block = "" ; ; SDVariable requestedSize = sd . constant ( Nd4j . createFromArray ( new long [ ] code_block = "" ; ) ) ; Function < INDArray , String > checkFunction = in code_block = LoopStatement ; ; SDVariable out = new ImageResize ( sd , inputImage , requestedSize , preserveAspectRatio , antialias , method ) . outputVariable ( ) . std ( true ) ; String err = OpValidation . validate ( new TestCase ( sd ) . gradientCheck ( false ) . expected ( "image_resize" , checkFunction ) ) ; log . info ( "Testing " + err ) ; assertNull ( err ) ; } }
@ Override public Number getAggregated ( ) { code_block = IfStatement ; Number result = currentValue - previousValue ; LOGGER . debug ( "Aggregated value: " + result ) ; return result ; }
public void test() { try { docModel . setProperty ( IRelationsManager . SERVICE_COMMONPART_NAME , targetField , newRefName ) ; repoSession . saveDocument ( docModel ) ; } catch ( ClientException e ) { logger . error ( "Cannot update " + targetField + " to " + newRefName , e ) ; } }
public void test() { try { boolean morePages = true ; code_block = WhileStatement ; } catch ( Exception e ) { logger . warn ( "Exception:" , e ) ; logger . debug ( Tools . errorToString ( e , true ) ) ; throw e ; } }
public void test() { try { repoSession . save ( ) ; } catch ( ClientException e ) { logger . error ( "Cannot save session" , e ) ; } }
public void test() { if ( ! connection . isConnected ( ) ) { connection . connect ( timeout ) ; connection . login ( username , password , resource , timeout ) ; connection . addStanzaListener ( new RayoMessageListener ( "offer" ) code_block = "" ; ) ; connection . addStanzaListener ( new RayoMessageListener ( "end" ) code_block = "" ; ) ; broadcastAvailability ( ) ; TimerTask pingTask = new TimerTask ( ) code_block = "" ; ; pingTimer = new Timer ( ) ; pingTimer . schedule ( pingTask , 5000 , 30000 ) ; connection . addStanzaListener ( new RayoMessageListener ( "ping" ) code_block = "" ; ) ; } else { logger . info ( "Connection is already connected" ) ; } }
public void test() { if ( dryrun ) { logger . info ( "running cycle " + cycle ) ; return ; } else { logger . info ( "running cycle " + cycle + " because dryrun is set to false" ) ; } }
public void test() { if ( dryrun ) { logger . info ( "skipping cycle " + cycle + " because dryrun is set to true" ) ; return ; } else { logger . info ( "skipping cycle " + cycle ) ; } }
private void downloadEnsemblData ( Path geneFolder ) throws IOException , InterruptedException { logger . info ( "Downloading gene '{}'" , geneFolder . toString ( ) ) ; List < String > downloadedUrls = new ArrayList < > ( 4 ) ; String ensemblHost = ensemblHostUrl + "/" + ensemblRelease ; code_block = IfStatement ; String bacteriaCollectionPath = "" ; code_block = IfStatement ; String version = ensemblRelease . split ( "-" ) [ 1 ] ; String url = ensemblHost + "/gtf/" + bacteriaCollectionPath + speciesShortName + "/*" + version + ".gtf.gz" ; String fileName = geneFolder . resolve ( speciesShortName + ".gtf.gz" ) . toString ( ) ; downloadFile ( url , fileName ) ; downloadedUrls . add ( url ) ; url = ensemblHost + "/fasta/" + bacteriaCollectionPath + speciesShortName + "/pep/*.pep.all.fa.gz" ; fileName = geneFolder . resolve ( speciesShortName + ".pep.all.fa.gz" ) . toString ( ) ; downloadFile ( url , fileName ) ; downloadedUrls . add ( url ) ; url = ensemblHost + "/fasta/" + bacteriaCollectionPath + speciesShortName + "/cdna/*.cdna.all.fa.gz" ; fileName = geneFolder . resolve ( speciesShortName + ".cdna.all.fa.gz" ) . toString ( ) ; downloadFile ( url , fileName ) ; downloadedUrls . add ( url ) ; saveVersionData ( EtlCommons . GENE_DATA , ENSEMBL_NAME , ensemblVersion , getTimeStamp ( ) , downloadedUrls , buildFolder . resolve ( "ensemblCoreVersion.json" ) ) ; }
public void test() { try { long resultCode = answer . getResultCodeAvp ( ) . getUnsigned32 ( ) ; code_block = IfStatement ; code_block = IfStatement ; deliverRoAnswer ( ( RoCreditControlRequest ) localEvent . getRequest ( ) , ( RoCreditControlAnswer ) localEvent . getAnswer ( ) ) ; } catch ( AvpDataException e ) { logger . warn ( "" , e ) ; setState ( ClientRoSessionState . IDLE , false ) ; } }
@ Override protected void shutDown ( ) throws Exception { shuttingDown = true ; offsetFlusherFuture . cancel ( false ) ; logRetentionFuture . cancel ( false ) ; checkpointFlusherFuture . cancel ( false ) ; dirtyLogFlushFuture . cancel ( false ) ; kafkaScheduler . shutdown ( ) ; logManager . shutdown ( ) ; offsetFlusher . run ( ) ; teardownLogMetrics ( ) ; LOGGER . info ( "Stopped" ) ; }
@ Test void testWithoutFields ( ) throws Exception { Logger logger = lc . getLogger ( getClass ( ) ) ; logger . info ( LOG_MESSAGE ) ; assertThat ( GelfTestSender . getMessages ( ) ) . hasSize ( 1 ) ; GelfMessage gelfMessage = GelfTestSender . getMessages ( ) . get ( 0 ) ; String myMdc = gelfMessage . getField ( MDC_MY_MDC ) ; assertThat ( myMdc ) . isNull ( ) ; }
public void test() { if ( eofAckCount >= EOF_ACK_LIMIT ) { log . info ( "CFDPDP sending EOF" ) ; eofSenderFuture . cancel ( false ) ; } else { log . info ( "CFDP sending EOF" ) ; EofPacket eof = new EofPacket ( ConditionCode . NO_ERROR , checksum , fileSize , null , directiveHeader ) ; simulator . transmitCfdp ( eof ) ; } }
public void test() { try { LocalDate lastUpdatedDate = LocalDate . parse ( lastUpdated , formatter ) . atStartOfDay ( ) . toLocalDate ( ) ; return today . isAfter ( lastUpdatedDate ) ; } catch ( DateTimeParseException e ) { log . error ( "Failed to parse last updated field '{}' and formatter '{}'" , lastUpdated , formatter . format ( lastUpdated ) ) ; return false ; } }
protected void printOperationInfo ( Logger logger ) { StringBuilder strb = new StringBuilder ( 35 ) ; strb . append ( "[" ) . append ( dateFormat . format ( new Date ( getPreExecutionTime ( ) ) ) ) . append ( " - " ) . append ( dateFormat . format ( new Date ( getPostExecutionTime ( ) ) ) ) . append ( " (" ) . append ( getPostExecutionTime ( ) - getPreExecutionTime ( ) ) . append ( "ms)]" ) ; strb . append ( " " ) . append ( getOperation ( ) . getClass ( ) . getSimpleName ( ) ) . append ( " " ) ; code_block = IfStatement ; logger . debug ( strb . toString ( ) ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( domainGroup == null ) { logger . warn ( "Can't find domain group {}" , domainGroupId ) ; return ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { switch ( level ) { case DEBUG : code_block = IfStatement ; break ; case ERROR : log . error ( measurement . toString ( ) ) ; break ; case FATAL : log . fatal ( measurement . toString ( ) ) ; break ; case INFO : code_block = IfStatement ; break ; case TRACE : code_block = IfStatement ; break ; case WARN : log . warn ( measurement . toString ( ) ) ; break ; } }
public void test() { switch ( level ) { case DEBUG : code_block = IfStatement ; break ; case ERROR : log . error ( measurement . toString ( ) ) ; break ; case FATAL : log . fatal ( measurement . toString ( ) ) ; break ; case INFO : code_block = IfStatement ; break ; case TRACE : code_block = IfStatement ; break ; case WARN : log . warn ( measurement . toString ( ) ) ; break ; } }
public void test() { if ( log . isInfoEnabled ( ) ) { log . info ( msg ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Refreshing." ) ; } }
public void test() { switch ( level ) { case DEBUG : code_block = IfStatement ; break ; case ERROR : log . error ( measurement . toString ( ) ) ; break ; case FATAL : log . fatal ( measurement . toString ( ) ) ; break ; case INFO : code_block = IfStatement ; break ; case TRACE : code_block = IfStatement ; break ; case WARN : log . warn ( measurement . toString ( ) ) ; break ; } }
public void test() { if ( logger . isWarnEnabled ( ) ) { logger . warn ( e . getMessage ( ) , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { String packageNames = StringUtils . collectionToCommaDelimitedString ( this . packages ) ; logger . debug ( "Found package names: {}" , packageNames ) ; } }
public void test() { try { defaultArticleURL = _portal . getControlPanelFullURL ( article . getGroupId ( ) , portletId , null ) ; } catch ( PortalException portalException ) { _log . error ( portalException , portalException ) ; } }
public void test() { try { AssetRendererFactory < JournalArticle > assetRendererFactory = AssetRendererFactoryRegistryUtil . getAssetRendererFactoryByClass ( JournalArticle . class ) ; AssetRenderer < JournalArticle > assetRenderer = assetRendererFactory . getAssetRenderer ( article , AssetRendererFactory . TYPE_LATEST_APPROVED ) ; return assetRenderer . getURLViewInContext ( liferayPortletRequest , null , defaultArticleURL ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception ex ) { _log . error ( "Could not enqueue scheduled job. " , ex ) ; _auditService . createAudit ( "Could not enqueue scheduled job. " + ex . getMessage ( ) , JPAEntity . class . cast ( job ) ) ; } }
public void test() { try { List < Organization > organizations = user . getOrganizations ( ) ; code_block = IfStatement ; } catch ( PortalException | SystemException e ) { logger . error ( "Could not retrieve organization" , e ) ; } }
public void test() { try { XarInstalledExtension xarInstalledExtension = addCacheXarExtension ( localExtension ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; continue ; } }
@ Override protected void connectionLog ( String message ) { logger . debug ( message ) ; }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { else-if ( Math . abs ( newStart - read . getAlignmentStart ( ) ) > MAX_POS_MOVE_ALLOWED ) { LOG . warn ( "Segment " + readPos + " ignored at position " + readPos ) ; return false ; } }
@ Override public synchronized V put ( K key , V value ) { clearGCCollected ( ) ; map . put ( key , new SoftValue < K , V > ( value , key , queue ) ) ; log . info ( "Added " + key + " to " + value ) ; return value ; }
public void test() { if ( deviceManagementProviderService == null ) { String msg = "Device Management service has not initialized." ; log . error ( msg ) ; throw new IllegalStateException ( msg ) ; } }
@ PatchMapping ( value = CommonConstants . PATH_ID ) @ ApiOperation ( value = "Update partially provider" ) @ ResponseStatus ( HttpStatus . OK ) public IdentityProviderDto patchProvider ( final @ RequestBody Map < String , Object > provider , final @ PathVariable String id ) { LOGGER . debug ( "Patch provider id={}" , id ) ; ParameterChecker . checkParameter ( "Parameters are mandatory : " , provider , id ) ; return service . patch ( buildUiHttpContext ( ) , provider , null , null , id , ProviderPatchType . JSON ) ; }
public void test() { if ( droppingSinks . contains ( entry . getKey ( ) ) ) { log . debug ( "Dropdropping Sink {}" , entry . getKey ( ) ) ; continue ; } }
public void test() { if ( dataSegment != null ) { dataSegments . add ( dataSegment ) ; } else { LOGGER . warn ( "Segment {} is null" , segmentId ) ; } }
public void test() { if ( m_nodeLogger . isDebugEnabled ( ) ) { FormattingTuple ft = MessageFormatter . arrayFormat ( format , arguments ) ; m_nodeLogger . debug ( ft . getMessage ( ) , ft . getThrowable ( ) ) ; } }
@ Transactional ( rollbackFor = ArrowheadException . class ) public void logIntraMeasurementDetailsToDB ( final QoSIntraPingMeasurementLog measurementLogSaved , final List < IcmpPingResponse > responseList , final ZonedDateTime aroundNow ) { logger . debug ( "logIntraMeasurementDetailsToDB started..." ) ; code_block = IfStatement ; final List < QoSIntraPingMeasurementLogDetails > measurementLogDetailsList = new ArrayList < > ( responseList . size ( ) ) ; int measurementSequenece = 0 ; code_block = ForStatement ; code_block = TryStatement ;  }
public void test() { try { qoSIntraPingMeasurementLogDetailsRepository . saveAll ( measurementLogDetailsList ) ; qoSIntraPingMeasurementLogDetailsRepository . flush ( ) ; } catch ( final Exception ex ) { logger . debug ( ex . getMessage ( ) , ex ) ; throw new ArrowheadException ( CoreCommonConstants . DATABASE_OPERATION_EXCEPTION_MSG ) ; } }
@ Test public void testObjectMethodNotifications ( ) throws Exception { String pid = apim . ingest ( TypeUtility . convertBytesToDataHandler ( demo998FOXMLObjectXML ) , FOXML1_1 . uri , "ingesting new foxml object" ) ; assertNotNull ( pid ) ; checkNotification ( pid , "ingest" ) ; LOGGER . info ( "Running TestManagementNotifications.testModifyObject..." ) ; String modifyResult = apim . modifyObject ( pid , "I" , "Updated Object Label" , null , "Changed state to inactive and updated label" ) ; assertNotNull ( modifyResult ) ; checkNotification ( pid , "modifyObject" ) ; LOGGER . info ( "Running TestManagementNotifications.testAddRelationship..." ) ; boolean addRelResult = apim . addRelationship ( pid , "rel:isRelatedTo" , "demo:5" , false , null ) ; assertTrue ( addRelResult ) ; checkNotification ( pid , "addRelationship" ) ; LOGGER . info ( "Running TestManagementNotifications.testAddRelationship..." ) ; addRelResult = apim . addRelationship ( PID . toURI ( pid ) , "rel:isRelatedTo" , "demo:6" , false , null ) ; assertTrue ( addRelResult ) ; checkNotification ( pid , "addRelationship" ) ; LOGGER . info ( "Running TestManagementNotifications.testAddRelationship..." ) ; addRelResult = apim . addRelationship ( PID . toURI ( pid ) + "/DS1" , "rel:isRelatedTo" , "demo:7" , false , null ) ; assertTrue ( addRelResult ) ; checkNotification ( pid , "addRelationship" ) ; LOGGER . info ( "Running TestManagementNotifications.testPurgeRelationship..." ) ; boolean purgeRelResult = apim . purgeRelationship ( pid , "rel:isRelatedTo" , "demo:5" , false , null ) ; assertTrue ( purgeRelResult ) ; checkNotification ( pid , "purgeRelationship" ) ; LOGGER . info ( "Running TestManagementNotifications.testPurgeRelationship..." ) ; purgeRelResult = apim . purgeRelationship ( PID . toURI ( pid ) , "purgeRelations
@ Test public void testObjectMethodNotifications ( ) throws Exception { LOGGER . info ( "Running TestManagementNotifications.testIngest..." ) ; String pid = apim . ingest ( TypeUtility . convertBytesToDataHandler ( demo998FOXMLObjectXML ) , FOXML1_1 . uri , "ingesting new foxml object" ) ; assertNotNull ( pid ) ; checkNotification ( pid , "ingest" ) ; LOGGER . info ( "Running TestManagementNotifications.testIngest..." ) ; String modifyResult = apim . modifyObject ( pid , "I" , "Updated Object Label" , null , "Changed state to inactive and updated label" ) ; assertNotNull ( modifyResult ) ; checkNotification ( pid , "modifyObject" ) ; LOGGER . info ( "Running TestManagementNotifications.testAddRelationship..." ) ; boolean addRelResult = apim . addRelationship ( pid , "rel:isRelatedTo" , "demo:5" , false , null ) ; assertTrue ( addRelResult ) ; checkNotification ( pid , "addRelationship" ) ; LOGGER . info ( "Running TestManagementNotifications.testAddRelationship..." ) ; addRelResult = apim . addRelationship ( PID . toURI ( pid ) , "rel:isRelatedTo" , "demo:6" , false , null ) ; assertTrue ( addRelResult ) ; checkNotification ( pid , "addRelationship" ) ; LOGGER . info ( "Running TestManagementNotifications.testAddRelationship..." ) ; addRelResult = apim . addRelationship ( PID . toURI ( pid ) + "/DS1" , "rel:isRelatedTo" , "demo:7" , false , null ) ; assertTrue ( addRelResult ) ; checkNotification ( pid , "addRelationship" ) ; LOGGER . info ( "Running TestManagementNotifications.testPurgeRelationship..." ) ; boolean purgeRelResult = apim . purgeRelationship ( pid , "rel:isRelatedTo" , "demo:5" , false , null ) ; assertTrue ( purgeRelResult ) ; checkNotification ( pid , "purgeRelationship" ) ; LOGGER . info ( "Running TestManagementNotifications.testPurgeRelationship..." ) ; purgeRelResult = apim
@ Test public void testObjectMethodNotifications ( ) throws Exception { LOGGER . info ( "Running TestManagementNotifications.testIngest..." ) ; String pid = apim . ingest ( TypeUtility . convertBytesToDataHandler ( demo998FOXMLObjectXML ) , FOXML1_1 . uri , "ingesting new foxml object" ) ; assertNotNull ( pid ) ; checkNotification ( pid , "ingest" ) ; LOGGER . info ( "Running TestManagementNotifications.testModifyObject..." ) ; String modifyResult = apim . modifyObject ( pid , "I" , "Updated Object Label" , null , "Changed state to inactive and updated label" ) ; assertNotNull ( modifyResult ) ; checkNotification ( pid , "modifyObject" ) ; LOGGER . info ( "Running TestManagementNotifications.testAddRelationship..." ) ; boolean addRelResult = apim . addRelationship ( pid , "rel:isRelatedTo" , "demo:5" , false , null ) ; assertTrue ( addRelResult ) ; checkNotification ( pid , "addRelationship" ) ; LOGGER . info ( "Running TestManagementNotifications.testAddRelationship..." ) ; addRelResult = apim . addRelationship ( PID . toURI ( pid ) , "rel:isRelatedTo" , "demo:6" , false , null ) ; assertTrue ( addRelResult ) ; checkNotification ( pid , "addRelationship" ) ; LOGGER . info ( "Running TestManagementNotifications.testAddRelationship..." ) ; addRelResult = apim . addRelationship ( PID . toURI ( pid ) + "/DS1" , "rel:isRelatedTo" , "demo:7" , false , null ) ; assertTrue ( addRelResult ) ; checkNotification ( pid , "addRelationship" ) ; LOGGER . info ( "Running TestManagementNotifications.testPurgeRelationship..." ) ; boolean purgeRelResult = apim . purgeRelationship ( pid , "rel:isRelatedTo" , "demo:5" , false , null ) ; assertTrue ( purgeRelResult ) ; checkNotification ( pid , "purgeRelationship" ) ; LOGGER . info ( "Running TestManagementNotifications.testPurgeRelationship..." ) ; purgeRelResult = ap
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { User user = Context . getAuthenticatedUser ( ) ; log . debug ( "User {}" , user ) ; code_block = IfStatement ; } }
public void test() { if ( user != null ) { logger . debug ( "has roles {}" , user . getAllRoles ( ) ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
protected void singleOrderBySameRangeScanGreater ( IoHelper io ) throws Exception { io . doSetup ( ) ; int size = 20 ; int queryLimit = 10 ; int startValue = 9 ; long start = System . currentTimeMillis ( ) ; List < String > expected = new ArrayList < String > ( size ) ; code_block = ForStatement ; app . waitForQueueDrainAndRefreshIndex ( ) ; Thread . sleep ( 500 ) ; long stop = System . currentTimeMillis ( ) ; logger . info ( "Writes took {} ms" , stop - start ) ; Query query = Query . fromQL ( "select * where index >= " + startValue + " order by index desc" ) ; query . setLimit ( queryLimit ) ; int count = 0 ; start = System . currentTimeMillis ( ) ; Results results ; do code_block = "" ; while ( results . hasCursor ( ) ) ; logger . info ( "Query took {} ms to return {} entities" , stop - start , count ) ; assertEquals ( expected . size ( ) - startValue , count ) ; stop = System . currentTimeMillis ( ) ; logger . info ( "Query took {} ms to return {} entities" , stop - start , count ) ; }
protected void singleOrderBySameRangeScanGreater ( IoHelper io ) throws Exception { io . doSetup ( ) ; int size = 20 ; int queryLimit = 10 ; int startValue = 9 ; long start = System . currentTimeMillis ( ) ; logger . info ( "Writing {} entities." , size ) ; List < String > expected = new ArrayList < String > ( size ) ; code_block = ForStatement ; app . waitForQueueDrainAndRefreshIndex ( ) ; Thread . sleep ( 500 ) ; long stop = System . currentTimeMillis ( ) ; logger . info ( "Executing query..." ) ; Query query = Query . fromQL ( "select * where index >= " + startValue + " order by index desc" ) ; query . setLimit ( queryLimit ) ; int count = 0 ; start = System . currentTimeMillis ( ) ; Results results ; do code_block = "" ; while ( results . hasCursor ( ) ) ; assertEquals ( expected . size ( ) - startValue , count ) ; stop = System . currentTimeMillis ( ) ; logger . info ( "Query took {} ms to return {} entities" , stop - start , count ) ; }
protected void singleOrderBySameRangeScanGreater ( IoHelper io ) throws Exception { io . doSetup ( ) ; int size = 20 ; int queryLimit = 10 ; int startValue = 9 ; long start = System . currentTimeMillis ( ) ; logger . info ( "Writing {} entities." , size ) ; List < String > expected = new ArrayList < String > ( size ) ; code_block = ForStatement ; app . waitForQueueDrainAndRefreshIndex ( ) ; Thread . sleep ( 500 ) ; long stop = System . currentTimeMillis ( ) ; logger . info ( "Writes took {} ms" , stop - start ) ; Query query = Query . fromQL ( "select * where index >= " + startValue + " order by index desc" ) ; query . setLimit ( queryLimit ) ; int count = 0 ; logger . info ( "Query took {} ms" , stop - start ) ; start = System . currentTimeMillis ( ) ; Results results ; do code_block = "" ; while ( results . hasCursor ( ) ) ; assertEquals ( expected . size ( ) - startValue , count ) ; stop = System . currentTimeMillis ( ) ; }
public void test() { try { conn = dataSource . getConnection ( ) ; stmt = conn . prepareStatement ( getUserSQL , ResultSet . TYPE_FORWARD_ONLY , ResultSet . CONCUR_READ_ONLY ) ; stmt . setFetchDirection ( ResultSet . FETCH_FORWARD ) ; stmt . setFetchSize ( getFetchSize ( ) ) ; setLongParameter ( stmt , 1 , userID ) ; log . debug ( "Executing SQL query: {}" , getUserSQL ) ; rs = stmt . executeQuery ( ) ; FastIDSet result = new FastIDSet ( ) ; code_block = WhileStatement ; code_block = IfStatement ; return result ; } catch ( SQLException sqle ) { log . warn ( "Exception while retrieving item s" , sqle ) ; throw new TasteException ( sqle ) ; } finally { IOUtils . quietClose ( rs , stmt , conn ) ; } }
public void test() { try { conn = dataSource . getConnection ( ) ; stmt = conn . prepareStatement ( getUserSQL , ResultSet . TYPE_FORWARD_ONLY , ResultSet . CONCUR_READ_ONLY ) ; stmt . setFetchDirection ( ResultSet . FETCH_FORWARD ) ; stmt . setFetchSize ( getFetchSize ( ) ) ; setLongParameter ( stmt , 1 , userID ) ; log . debug ( "Executing SQL query: {}" , getUserSQL ) ; rs = stmt . executeQuery ( ) ; FastIDSet result = new FastIDSet ( ) ; code_block = WhileStatement ; code_block = IfStatement ; return result ; } catch ( SQLException sqle ) { log . warn ( "Exception while exporting data" , sqle ) ; throw new TasteException ( sqle ) ; } finally { IOUtils . quietClose ( rs , stmt , conn ) ; } }
public void test() { try { Field [ ] fields = o . getClass ( ) . getDeclaredFields ( ) ; code_block = ForStatement ; } catch ( Exception e ) { LOG . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { code_block = WhileStatement ; } catch ( InterruptedException e ) { LOG . error ( "shutdown interrupted" ) ; } }
public void test() { try { logSafe ( "Stopping store {}-{}..." , topic , partitionGroup ) ; logSafe ( "Waiting for flush finished {}-{}..." , topic , partitionGroup ) ; code_block = TryStatement ;  stopWriteThread ( ) ; logSafe ( "Stopping flush thread {}-{}..." , topic , partitionGroup ) ; stopFlushThread ( ) ; flushCheckpoint ( ) ; code_block = IfStatement ; System . out . println ( "Store stopped. " + base . getAbsolutePath ( ) ) ; logSafe ( "Store stopped {}-{}." , topic , partitionGroup ) ; } catch ( Throwable t ) { log . warn ( "{}" , t , t ) ; } }
@ GET @ Produces ( MediaType . APPLICATION_XML ) public Response getProviders ( ) { logger . debug ( "StartOf getProviders" ) ; ProviderHelper providerRestService = getProviderHelper ( ) ; String serializedProviders = null ; code_block = TryStatement ;  logger . debug ( "EndOf getTemplates" ) ; return buildResponse ( 200 , serializedProviders ) ; }
public void test() { try { serializedProviders = providerRestService . getProviders ( ) ; } catch ( HelperException e ) { logger . info ( "getProviders exception:" + e . getMessage ( ) ) ; return buildResponse ( e ) ; } }
@ GET @ Produces ( MediaType . APPLICATION_XML ) public Response getProviders ( ) { logger . debug ( "StartOf getProviders - REQUEST for /providers" ) ; ProviderHelper providerRestService = getProviderHelper ( ) ; String serializedProviders = null ; code_block = TryStatement ;  logger . debug ( "EndOf getProviders" ) ; return buildResponse ( 200 , serializedProviders ) ; }
public String getApiKey ( ) { log . info ( "Query API Key" ) ; return readyElement ( apiKeyLabel ) . getAttribute ( "value" ) ; }
public void test() { if ( myPayload . getTimeBudget ( ) . isExpired ( ) ) { StringBuilder error = new StringBuilder ( 150 ) ; error . append ( "Timed out waiting to " ) ; int numServicesWaiting = 0 ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; error . append ( numServicesWaiting > 1 ? "responses" : "response" ) . append ( " from Server: " ) . append ( mySource . getName ( ) ) ; error . append ( ".  Consider increasing timeout settings in Edit->Settings->Servers." ) ; LOG . error ( error . toString ( ) ) ; sendResponseAndCleanup ( error . toString ( ) ) ; } else { myResponseTimer . setInitialDelay ( myPayload . getTimeBudget ( ) . getRemainingMilliseconds ( ) ) ; myResponseTimer . start ( ) ; } }
@ Override public void delete ( ConnectionParameterKey metadataKey ) { LOGGER . trace ( MessageFormat . format ( "Deleting ConnectionParameter {0}." , metadataKey . toString ( ) ) ) ; code_block = IfStatement ; String deleteStatement = deleteStatement ( metadataKey ) ; getMetadataRepository ( ) . executeUpdate ( deleteStatement ) ; }
public void test() { try ( Session session = modelDBHibernateUtil . getSessionFactory ( ) . openSession ( ) ) { Query < ? > query = session . createQuery ( GET_PROJECT_EXPERIMENTS_COUNT_HQL ) ; query . setParameterList ( ModelDBConstants . PROJECT_IDS , projectIds ) ; Long count = ( Long ) query . uniqueResult ( ) ; LOGGER . debug ( "getProjectIDs() : {}" , count ) ; return count ; } catch ( Exception ex ) { code_block = IfStatement ; } }
public void test() { try { records = this . getActionLogDAO ( ) . getActionRecords ( searchBean ) ; } catch ( Throwable t ) { _logger . error ( "Error loading actionlogger records" , t ) ; throw new ApsSystemException ( "Error loading actionlogger records" , t ) ; } }
@ Before public void setUp ( ) { LOG . info ( "Setting up" ) ; testPath = "file:///tmp/flume-test." + Calendar . getInstance ( ) . getTimeInMillis ( ) + "." + Thread . currentThread ( ) . getId ( ) ; sink = new HDFSEventSink ( ) ; sink . setName ( "HDFSEventSink-" + UUID . randomUUID ( ) . toString ( ) ) ; dirCleanup ( ) ; }
public void test() { if ( null != exchangeFound ) { LOG . info ( "saveExchange--updated-exchangeFound" ) ; exchangeFound . setName ( exchangeUpdate . getName ( ) ) ; exchangeFound . setDisabled ( exchangeUpdate . isDisabled ( ) ) ; exchangeFound . setKey ( exchangeUpdate . getKey ( ) ) ; exchangeFound . setTLSVersions ( exchangeUpdate . getTLSVersions ( ) ) ; exchangeFound . setType ( exchangeUpdate . getType ( ) ) ; exchangeFound . setUrl ( exchangeUpdate . getUrl ( ) ) ; exchangeFound . setSniName ( exchangeUpdate . getSniName ( ) ) ; exchangeFound . setCertificateAlias ( exchangeUpdate . getCertificateAlias ( ) ) ; } else { LOG . info ( "saveExchange--created-exchangeFound" ) ; exchanges . add ( exchangeUpdate ) ; } }
public void test() { try { code_block = IfStatement ; List < ExchangeType > exchanges = ExchangeManagerHelper . getAllExchanges ( exInfo , true ) ; String nameLookup = ExchangeManagerHelper . findExchangeTypeBy ( exchanges , nameLookup ) ; ExchangeType exchangeFound = ExchangeManagerHelper . findExchangeTypeBy ( exchanges , nameLookup ) ; code_block = IfStatement ; saveExchangeInfo ( ) ; bSave = true ; } catch ( ExchangeManagerException e ) { LOG . error ( "Unable to update exchange information: {}" , exInfo , e ) ; } }
@ ActivityStreamAuditable @ RestAccessControl ( permission = Permission . MANAGE_PAGES ) @ RequestMapping ( value = "/pages/{pageCode}" , method = RequestMethod . DELETE , produces = MediaType . APPLICATION_JSON_VALUE ) public ResponseEntity < SimpleRestResponse < ? > > deletePage ( @ ModelAttribute ( "user" ) UserDetails user , @ PathVariable String pageCode ) throws ApsSystemException { logger . debug ( "Deleting page {}" , pageCode ) ; code_block = IfStatement ; DataBinder binder = new DataBinder ( pageCode ) ; BindingResult bindingResult = binder . getBindingResult ( ) ; code_block = IfStatement ; getPageValidator ( ) . validateOnlinePage ( pageCode , bindingResult ) ; code_block = IfStatement ; getPageValidator ( ) . validateChildren ( pageCode , bindingResult ) ; code_block = IfStatement ; this . getPageService ( ) . removePage ( pageCode ) ; Map < String , String > payload = new HashMap < > ( ) ; payload . put ( "code" , pageCode ) ; return new ResponseEntity < > ( new SimpleRestResponse < > ( payload ) , HttpStatus . OK ) ; }
private void testBookieRecoveryToRandomBookies ( boolean async , int numBookiesToKill ) throws Exception { int numLedgers = 3 ; List < LedgerHandle > lhs = createLedgers ( numLedgers , 3 , 3 ) ; int numMsgs = 10 ; writeEntriestoLedgers ( numMsgs , 0 , lhs ) ; LOG . info ( "Created " + numMsgs ) ; Set < BookieSocketAddress > bookiesSrc = new HashSet < BookieSocketAddress > ( ) ; code_block = ForStatement ; code_block = ForStatement ; writeEntriestoLedgers ( numMsgs , 10 , lhs ) ; LOG . info ( "Now recover the data on the killed bookie (" + bookiesSrc + ") and replicate it to a random available one" ) ; code_block = IfStatement ; verifyLedgerMetadata ( lhs , bookiesSrc ) ; verifyRecoveredLedgers ( lhs , 2 * numMsgs - 1 ) ; }
private void testBookieRecoveryToRandomBookies ( boolean async , int numBookiesToKill ) throws Exception { int numLedgers = 3 ; List < LedgerHandle > lhs = createLedgers ( numLedgers , 3 , 3 ) ; int numMsgs = 10 ; writeEntriestoLedgers ( numMsgs , 0 , lhs ) ; LOG . info ( "Finished writing all ledger entries so shutdown {} bookies." , numBookiesToKill ) ; Set < BookieSocketAddress > bookiesSrc = new HashSet < BookieSocketAddress > ( ) ; code_block = ForStatement ; code_block = ForStatement ; writeEntriestoLedgers ( numMsgs , 10 , lhs ) ; LOG . info ( "Finished writing all ledger entries to the bookies." ) ; code_block = IfStatement ; verifyLedgerMetadata ( lhs , bookiesSrc ) ; verifyRecoveredLedgers ( lhs , 2 * numMsgs - 1 ) ; }
public void test() { try { renderEditViewForId ( request , response , id ) ; } catch ( TException e ) { log . error ( "Could not render edit view for id" , e ) ; } }
public void test() { if ( networkModel == null ) { logger . warn ( "Network model was null" ) ; } else { code_block = IfStatement ; } }
public void test() { if ( networkModel . getTopology ( ) == null ) { log . debug ( "" ) ; } else { code_block = ForStatement ; code_block = IfStatement ; } }
public void test() { try { SwitchPortStatistics switchPortStatistics = getSwitchPortStatisticsForNetworkElement ( ne , networkModel ) ; netStats . addPortSwitchStatistic ( ne . getId ( ) , switchPortStatistics ) ; } catch ( Exception e ) { logger . debug ( "Failed to retrieve the switch port statistics" , e ) ; } }
public void test() { if ( netStats . getSwitchStatistics ( ) . isEmpty ( ) ) { logger . debug ( "no switch statistics" ) ; } }
@ Override public void init ( ) { messageRouter . registerMessageProcessedListener ( this ) ; mqttClient . setMessageListener ( this ) ; mqttClient . start ( ) ; mqttClientFactory . createSender ( ownGbid ) . start ( ) ; subscribe ( ) ; logger . info ( "initialize {}" , ownGbid ) ; }
public void test() { try { LOGGER . debug ( "Creating JAXB context with context path: {}." , contextPath ) ; jaxbContext = JAXBContext . newInstance ( contextPath , CswJAXBElementProvider . class . getClassLoader ( ) ) ; } catch ( JAXBException e ) { LOGGER . debug ( "Unable to create JAXB context using contextPath: {}." , contextPath , e ) ; } }
public void test() { try { String agentDaemonQueueName = ApplicationConfigProvider . getInstance ( ) . getAgentDetails ( ) . getAgentPkgQueue ( ) ; code_block = IfStatement ; agentConfigDAL . updateAgentRunningStatus ( agentId , AGENTACTION . valueOf ( action ) ) ; } catch ( Exception e ) { log . error ( "Error updating agent running status" , e ) ; throw new InsightsCustomException ( e . toString ( ) ) ; } }
public void test() { try ( final Writer writer = new BufferedWriter ( new OutputStreamWriter ( out . stream ( ) , Constants . CHARSET_UTF_8 ) ) ) { writeCall . accept ( writer ) ; writer . flush ( ) ; } catch ( final Exception e ) { LOGGER . error ( "Exception writing call" , e ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { UDDIFindBusinessProxyObjectFactory uddiFactory = new UDDIFindBusinessProxyObjectFactory ( ) ; UDDIFindBusinessProxy uddiProxy = uddiFactory . getUDDIBusinessInfoProxy ( ) ; businessList = uddiProxy . findBusinessesFromUDDI ( exchange ) ; removeIgnoredBusinesses ( businessList ) ; } catch ( UDDIFindBusinessException e ) { String sErrorMessage = "Failed to call 'find_business' web service on the NHIN UDDI server.  Error: " + e . getMessage ( ) ; LOG . error ( sErrorMessage , e ) ; throw new UDDIAccessorException ( sErrorMessage , e ) ; } }
public void test() { try { WidgetValidatorCmsHelper . validateTitle ( widget , getLangManager ( ) , bindingResult ) ; WidgetValidatorCmsHelper . validateLink ( widget , getLangManager ( ) , getPageManager ( ) , bindingResult ) ; this . validateContentType ( widget , bindingResult ) ; this . validateFilters ( widget , bindingResult ) ; this . validateContentModel ( widget , bindingResult ) ; logger . debug ( "Widget {}: {}" , widget , widget ) ; } catch ( Throwable e ) { logger . error ( "error in validate wiget {} in page {}" , widget . getCode ( ) , page . getCode ( ) ) ; throw new RestServerError ( "error in widget config validation" , e ) ; } }
public void test() { try { logger . debug ( "validating widget {} for page {}" , widget . getCode ( ) , page . getCode ( ) ) ; WidgetValidatorCmsHelper . validateTitle ( widget , getLangManager ( ) , bindingResult ) ; WidgetValidatorCmsHelper . validateLink ( widget , getLangManager ( ) , getPageManager ( ) , bindingResult ) ; this . validateContentType ( widget , bindingResult ) ; this . validateFilters ( widget , bindingResult ) ; this . validateContentModel ( widget , bindingResult ) ; } catch ( Throwable e ) { logger . error ( "error in widget config validation" , e ) ; throw new RestServerError ( "error in widget config validation" , e ) ; } }
public void test() { if ( Message . RESERVED_FIELDS . contains ( csfr . key ( ) ) && ! Message . RESERVED_SETTABLE_FIELDS . contains ( csfr . key ( ) ) ) { final String message = "Cannot add static field. Field [" + csfr . key ( ) + "] is reserved." ; LOG . warn ( message ) ; throw new BadRequestException ( message ) ; } }
public void test() { { checkPermission ( RestPermissions . INPUTS_EDIT , inputId ) ; final MessageInput input = persistedInputs . get ( inputId ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; input . addStaticField ( csfr . key ( ) , csfr . value ( ) ) ; final Input mongoInput = inputService . find ( input . getPersistId ( ) ) ; inputService . addStaticField ( mongoInput , csfr . key ( ) , csfr . value ( ) ) ; final String msg = "Added static field [" + csfr . key ( ) + "] to input <" + inputId + ">." ; LOG . info ( msg ) ; activityWriter . write ( new Activity ( msg , StaticFieldsResource . class ) ) ; final URI inputUri = getUriBuilderToSelf ( ) . path ( InputsResource . class ) . path ( "{inputId}" ) . build ( mongoInput . getId ( ) ) ; return Response . created ( inputUri ) . build ( ) ; } }
public void test() { if ( provider instanceof ModifiableServerProvider ) { ( ( ModifiableServerProvider < StreamingServer > ) provider ) . addServer ( myServer ) ; } else { LOG . warn ( "Provider {} is not an ModifiableServerProvider." , provider . getClass ( ) . getName ( ) ) ; } }
public void test() { try { aquisitionDate = sdf . parse ( input . get ( "acquisitionDate" ) ) ; featureBuilder . add ( aquisitionDate ) ; } catch ( final ParseException e ) { LOGGER . warn ( "Unable to parse aquisition date: " + input . get ( "acquisitionDate" ) ) ; featureBuilder . add ( null ) ; } }
public void test() { if ( userToken . expiresWithin ( EXPIRY_THRESHOLD_MINUTES ) ) { logger . debug ( "Deleting expired token for user '" + user . getAccountName ( ) + "'" ) ; dao . delete ( userToken ) ; } else { logger . debug ( "Returning existing token for user '" + user . getAccountName ( ) + "'" ) ; return userToken ; } }
protected E getValidTokenForUser ( User user , Integer expirationTimeInMinutes ) throws NoSuchMethodException , SecurityException , InstantiationException , IllegalAccessException , IllegalArgumentException , InvocationTargetException { E userToken = findByUser ( user ) ; code_block = IfStatement ; userToken = buildConcreteInstance ( user , expirationTimeInMinutes ) ; dao . saveOrUpdate ( userToken ) ; final String tokenType = userToken . getClass ( ) . getSimpleName ( ) ; LOGGER . debug ( "Created user: " + tokenType ) ; return userToken ; }
private void scheduledLogStatus ( ) { logger . debug ( "Thing status update" ) ; scheduleRunAsync ( this :: scheduledLogStatus , STATUS_LOG_INTERVAL_MS , TimeUnit . MILLISECONDS ) ; }
public void test() { try { String port = "COM10" ; LoggingFactory . init ( Level . INFO ) ; MrlComm arduino = ( MrlComm ) Runtime . start ( "arduino" , "MrlComm" ) ; Servo servo01 = ( Servo ) Runtime . start ( "servo01" , "Servo" ) ; } catch ( Exception e ) { log . error ( "" , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { for ( Map < String , Object > location : locations ) { Entity entity = em . create ( "store" , location ) ; assertNotNull ( entity ) ; LOG . info ( "Created location: {}" , entity ) ; } }
public void test() { try { response = getIRODSProtocol ( ) . irodsFunction ( specificQueryInp ) ; } catch ( DataNotFoundException e ) { logger . info ( "" , e ) ; return new SpecificQueryResultSet ( specificQuery , specificQueryDefinition . getColumnNames ( ) ) ; } }
private SpecificQueryResultSet queryOnAliasGivenDefinition ( final SpecificQuery specificQuery , final int maxRows , final SpecificQueryDefinition specificQueryDefinition , final int userDefinedOffset ) throws JargonException { log . info ( "queryOnAliasGivenDefinition()" ) ; SpecificQueryInp specificQueryInp = SpecificQueryInp . instance ( specificQuery . getArguments ( ) , specificQuery . getQueryString ( ) , maxRows , specificQuery . getContinuationValue ( ) , specificQuery . getZoneHint ( ) ) ; Tag response = null ; code_block = TryStatement ;  int continuation = QueryResultProcessingUtils . getContinuationValue ( response ) ; boolean hasMoreRecords = false ; code_block = IfStatement ; List < IRODSQueryResultRow > resultRows = QueryResultProcessingUtils . translateResponseIntoResultSet ( response , specificQueryDefinition . getColumnNames ( ) , continuation , userDefinedOffset ) ; SpecificQueryResultSet results = new SpecificQueryResultSet ( specificQuery , resultRows , specificQueryDefinition . getColumnNames ( ) , hasMoreRecords , continuation ) ; closeResultSet ( results ) ; return results ; }
public void test() { for ( int a = 2 ; ; a ++ ) { BigInteger aPow = BigInteger . valueOf ( a ) . pow ( m ) ; long nthRoot = Roots . ithRoot ( aPow , n ) [ 0 ] . longValue ( ) ; long bMin = Math . max ( 0 , nthRoot - b_COUNT ) ; long bMax = nthRoot + b_COUNT ; code_block = ForStatement ; log . info ( "root = " + n ) ; code_block = ForStatement ; code_block = ForStatement ; } }
public void test() { try { code_block = ForStatement ; } catch ( Exception e ) { logger . error ( Messages . getInstance ( ) . getErrorString ( "Workspace.ERROR_0002_PROPS_EXCEPTION" ) , e ) ; } }
public void test() { try { DLAppServiceUtil . checkOutFileEntry ( fileEntryId , serviceContext ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { DeviceTypeDAO . rollbackTransaction ( ) ; } catch ( DeviceMgtPluginException iotDAOEx ) { String msg = "Error occurred while roll back the device dis enrol transaction :" + deviceId . toString ( ) ; log . warn ( msg , iotDAOEx ) ; } }
public void test() { try { code_block = IfStatement ; DeviceTypeDAO . beginTransaction ( ) ; status = deviceTypeDAO . getDeviceTypeDAO ( ) . deleteDevice ( deviceId . getId ( ) ) ; DeviceTypeDAO . commitTransaction ( ) ; } catch ( DeviceMgtPluginException e ) { code_block = TryStatement ;  String msg = "Error while removing the sampledevice device : " + deviceId . getId ( ) ; log . error ( msg , e ) ; throw new DeviceManagementException ( msg , e ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { invokable . triggerCheckpointAsync ( checkpointMetaData , checkpointOptions ) ; } catch ( RejectedExecutionException ex ) { LOG . error ( ex . getMessage ( ) ) ; } catch ( Throwable t ) { code_block = IfStatement ; } }
public void test() { if ( getExecutionState ( ) == ExecutionState . RUNNING ) { failExternally ( new Exception ( "Error while triggering checkpoint " + checkpointID + " to " + taskNameWithSubtask , t ) ) ; } else { log . warn ( "Error while triggering checkpoint " + checkpointID + " to " + taskNameWithSubtask , t ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( IngestException e ) { INGEST_LOGGER . error ( "Error deleting metacards for CatalogStore {}" , store . getId ( ) , e ) ; exceptions . add ( new ProcessingDetailsImpl ( store . getId ( ) , e ) ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Refreshing." ) ; } }
@ Override public Map < String , Object > getServiceBindingParameters ( UUID guid ) { logger . debug ( Messages . GETTING_SERVICE_INSTANCER_0_1 , guid ) ; return delegate . getServiceBindingParameters ( guid ) ; }
public void test() { if ( length != null ) { long blocks = length / _blockSize ; _log . info ( "Deleting [" + name + LENGTH + "]" ) ; _store . delete ( new BytesRef ( name + LENGTH ) ) ; _store . delete ( new BytesRef ( name + LASTMOD ) ) ; code_block = ForStatement ; writeFileNamesAndSync ( ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { try { java . util . List < com . liferay . blogs . model . BlogsEntry > returnValue = BlogsEntryServiceUtil . getGroupEntries ( groupId , status , max ) ; return com . liferay . blogs . model . BlogsEntrySoap . toSoapModels ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
void writeSolrConfiguration ( String core ) { File configDir = Paths . get ( this . dataDirectory . getAbsolutePath ( ) , core , "conf" ) . toFile ( ) ; boolean directoriesMade = configDir . mkdirs ( ) ; code_block = ForStatement ; logger . info ( "Done writing Solr configuration to " + core ) ; }
public void test() { try ( InputStream inputStream = ConfigurationFileProxy . class . getClassLoader ( ) . getResourceAsStream ( "solr/conf/" + filename ) ; FileOutputStream outputStream = new FileOutputStream ( currentFile ) ) { long byteCount = IOUtils . copyLarge ( inputStream , outputStream ) ; LOGGER . debug ( "Wrote out {} bytes for [{}]." , byteCount , currentFile ) ; } catch ( IOException e ) { LOGGER . warn ( "Unable to copy Solr configuration file: " + filename , e ) ; } }
public void test() { try ( InputStream inputStream = ConfigurationFileProxy . class . getClassLoader ( ) . getResourceAsStream ( "solr/conf/" + filename ) ; FileOutputStream outputStream = new FileOutputStream ( currentFile ) ) { long byteCount = IOUtils . copyLarge ( inputStream , outputStream ) ; LOGGER . debug ( "Wrote out {} bytes for [{}]." , byteCount , filename ) ; } catch ( IOException e ) { LOGGER . warn ( "Unable to copy Solr configuration files" , e ) ; } }
public void test() { if ( searchService == null ) { logger . warn ( "{} denies to handle request for {} since no search service is defined" , this , searchService ) ; return false ; } else-if ( feedURI == null ) { logger . warn ( "{} denies to handle request for {} since no uri is defined" , this , query ) ; return false ; } else-if ( query . length == 0 ) { logger . debug ( "{} denies to handle unknown request" , this ) ; return false ; } }
public void test() { if ( searchService == null ) { logger . warn ( "{} denies to handle request for {} due to missing search service" , this , query ) ; return false ; } else-if ( feedURI == null ) { logger . warn ( "{} denies to handle request for {} due to missing feed URI" , this , feedURI ) ; return false ; } else-if ( query . length == 0 ) { logger . debug ( "{} denies to handle unknown request" , this ) ; return false ; } }
public void test() { if ( searchService == null ) { logger . warn ( "{} denies to handle request for {} due to missing search service" , this , query ) ; return false ; } else-if ( feedURI == null ) { logger . warn ( "{} denies to handle request for {} since no uri is defined" , this , query ) ; return false ; } else-if ( query . length == 0 ) { logger . warn ( "{} denies to handle request for {} since query is empty" , this , query ) ; return false ; } }
public void test() { try { Connection conn = null ; code_block = TryStatement ;  code_block = IfStatement ; } catch ( SQLException e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( logger . isDebugable ( ) ) { logger . debug ( e . getMessage ( ) , e ) ; } }
public void test() { if ( batchTimeoutSecs <= 0 || batchTimeoutSecs > maxBatchTimeout ) { batchTimeoutSecs = maxBatchTimeout ; logger . warn ( "maxBatchTimeout {} is greater than 0 or greater than 0. batchTimeoutSecs is set to {}" , batchTimeoutSecs , maxBatchTimeout ) ; } }
@ Test ( timeOut = 10_000 ) public void testZkClientLosingSession ( ) throws Exception { long sessionId = zkClient . getZookeeperClient ( ) . getZooKeeper ( ) . getSessionId ( ) ; byte [ ] sessionPasswd = zkClient . getZookeeperClient ( ) . getZooKeeper ( ) . getSessionPasswd ( ) ; ZooKeeper zk = new ZooKeeper ( ZK_CLUSTER , 1000 , null , sessionId , sessionPasswd ) ; zk . close ( ) ; long previousMaxTimestamp = INITIAL_MAX_TS_VALUE ; code_block = ForStatement ; log . info ( "Closing ZooKeeper client." ) ; assertEquals ( storage . getMaxTimestamp ( ) , 1_000_000 * ITERATION_COUNT ) ; }
public void test() { try { byte [ ] utf8 = str . getBytes ( "UTF8" ) ; byte [ ] enc = ecipher . doFinal ( utf8 ) ; return Base64 . encodeBase64String ( enc ) ; } catch ( UnsupportedEncodingException e ) { LOGGER . trace ( "DesEncrypter encryption failed" , e ) ; throw new ApplicationException ( "DesEncrypter failed - UnsupportedEncodingException " , e ) ; } catch ( GeneralSecurityException e ) { LOGGER . trace ( "DesEncrypter encryption failed" , e ) ; throw new ApplicationException ( "DesEncrypter encryption failed - GeneralSecurityException" , e ) ; } }
public void test() { try { byte [ ] utf8 = str . getBytes ( "UTF8" ) ; byte [ ] enc = ecipher . doFinal ( utf8 ) ; return Base64 . encodeBase64String ( enc ) ; } catch ( UnsupportedEncodingException e ) { LOGGER . trace ( "DesEncrypter unsupported encoding exception" , e ) ; throw new ApplicationException ( "DesEncrypter failed - UnsupportedEncodingException " , e ) ; } catch ( GeneralSecurityException e ) { LOGGER . trace ( "DesEncrypter encryption exception" , e ) ; throw new ApplicationException ( "DesEncrypter encryption failed - GeneralSecurityException" , e ) ; } }
private BuildSetTask buildProjectsAndWaitForUpdates ( BuildConfigurationSet buildConfigurationSet , BuildCoordinator buildCoordinator , int nStatusUpdates , Consumer < BuildStatusChangedEvent > onStatusUpdate , List < BuildStatusChangedEvent > receivedStatuses , List < BuildSetStatusChangedEvent > receivedSetStatuses ) throws InterruptedException , CoreException { Consumer < BuildStatusChangedEvent > onStatusUpdateInternal = ( statusUpdate ) code_block = LoopStatement ; ; final Semaphore semaphore = registerReleaseListenersAndAcquireSemaphore ( onStatusUpdateInternal , nStatusUpdates ) ; final Semaphore buildSetSemaphore = registerBuildSetListeners ( receivedSetStatuses , BUILD_SET_STATUS_UPDATES ) ; BuildOptions buildOptions = new BuildOptions ( ) ; buildOptions . setRebuildMode ( RebuildMode . FORCE ) ; BuildSetTask buildSetTask = buildCoordinator . build ( buildConfigurationSet , MockUser . newTestUser ( 1 ) , buildOptions ) ; log . info ( "Waiting to receive {} build set status updates..." , buildConfigurationSet . toString ( ) ) ; assertBuildStartedSuccessfully ( buildSetTask ) ; waitForStatusUpdates ( nStatusUpdates , semaphore , "" ) ; log . debug ( "All status updates should be received. Semaphore has {} free entries." , semaphore . availablePermits ( ) ) ; log . info ( "Waiting to receive all {} build set status updates..." , BUILD_SET_STATUS_UPDATES ) ; waitForStatusUpdates ( BUILD_SET_STATUS_UPDATES , buildSetSemaphore , "build set task: " + buildSetTask ) ; log . debug ( "All status updates should be received. Semaphore has {} free entries." , semaphore . availablePermits ( ) ) ; return buildSetTask ; }
private BuildSetTask buildProjectsAndWaitForUpdates ( BuildConfigurationSet buildConfigurationSet , BuildCoordinator buildCoordinator , int nStatusUpdates , Consumer < BuildStatusChangedEvent > onStatusUpdate , List < BuildStatusChangedEvent > receivedStatuses , List < BuildSetStatusChangedEvent > receivedSetStatuses ) throws InterruptedException , CoreException { Consumer < BuildStatusChangedEvent > onStatusUpdateInternal = ( statusUpdate ) code_block = LoopStatement ; ; final Semaphore semaphore = registerReleaseListenersAndAcquireSemaphore ( onStatusUpdateInternal , nStatusUpdates ) ; final Semaphore buildSetSemaphore = registerBuildSetListeners ( receivedSetStatuses , BUILD_SET_STATUS_UPDATES ) ; BuildOptions buildOptions = new BuildOptions ( ) ; buildOptions . setRebuildMode ( RebuildMode . FORCE ) ; BuildSetTask buildSetTask = buildCoordinator . build ( buildConfigurationSet , MockUser . newTestUser ( 1 ) , buildOptions ) ; log . debug ( "Waiting to receive {} status updates..." , buildConfigurationSet ) ; assertBuildStartedSuccessfully ( buildSetTask ) ; log . info ( "Waiting to receive all {} status updates..." , nStatusUpdates ) ; waitForStatusUpdates ( nStatusUpdates , semaphore , "" ) ; log . debug ( "All status updates should be received. Semaphore has {} free entries." , semaphore . availablePermits ( ) ) ; waitForStatusUpdates ( BUILD_SET_STATUS_UPDATES , buildSetSemaphore , "build set task: " + buildSetTask ) ; log . debug ( "All status updates should be received. Semaphore has {} free entries." , semaphore . availablePermits ( ) ) ; return buildSetTask ; }
public void test() { if ( ! getOpts ( ) . isDryRun ( ) ) { strat . writeSrcFile ( doc ) ; log . info ( "Writing source file for document {}" , doc . getName ( ) ) ; } else { log . info ( "Writing source file for document {} (skipped due to dry run)" , doc . getName ( ) ) ; } }
public void test() { if ( ! getOpts ( ) . isDryRun ( ) ) { log . info ( "Writing source file for document {}" , doc . getName ( ) ) ; strat . writeSrcFile ( doc ) ; } else { log . debug ( "Skipping writing source file for document {}" , doc . getName ( ) ) ; } }
public void test() { try { signature = generateSignature ( msg , signHashAlgo ) ; } catch ( CryptoException E ) { LOGGER . warn ( "Could not generate signature" , E ) ; } }
public void test() { try ( Connection con = getDatabaseManager ( ) . getDataSource ( ) . getConnection ( ) ; PreparedStatement pstmt = con . prepareStatement ( "SELECT * FROM dex_offer AS offer where latest = true " + "AND offer.status = 0 AND offer.finish_time < ?" ) ) { int i = 0 ; pstmt . setLong ( ++ i , currentTime ) ; DbIterator < DexOrder > orders = getManyBy ( con , pstmt , true ) ; return CollectionUtil . toList ( orders ) ; } catch ( SQLException ex ) { logger . error ( ex . getMessage ( ) , ex ) ; } }
private RepairRun buildRepairRunFromRow ( Row repairRunResult , UUID id ) { Date startTime = repairRunResult . getTimestamp ( "start_time" ) ; Date pauseTime = repairRunResult . getTimestamp ( "pause_time" ) ; Date endTime = repairRunResult . getTimestamp ( "end_time" ) ; return RepairRun . builder ( repairRunResult . getString ( "cluster_name" ) , repairRunResult . getUUID ( "repair_unit_id" ) ) . creationTime ( new DateTime ( repairRunResult . getTimestamp ( "creation_time" ) ) ) . intensity ( repairRunResult . getDouble ( "intensity" ) ) . segmentCount ( repairRunResult . getInt ( "segment_count" ) ) . repairParallelism ( RepairParallelism . fromName ( repairRunResult . getString ( "repair_parallelism" ) ) ) . cause ( repairRunResult . getString ( "cause" ) ) . owner ( repairRunResult . getString ( "owner" ) ) . startTime ( null != startTime ? new DateTime ( startTime ) : null ) . pauseTime ( null != pauseTime ? new DateTime ( pauseTime ) : null ) . endTime ( null != endTime ? new DateTime ( endTime ) : null ) . lastEvent ( repairRunResult . getString ( "last_event" ) ) . runState ( RunState . valueOf ( repairRunResult . getString ( "state" ) ) ) . tables ( repairRunResult . getSet ( "tables" , String . class ) ) . build ( id ) ; LOG . info ( "repairRun " + id + ": " + repairRunResult ) ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { { validateBusinessObjectDefinitionTagKey ( businessObjectDefinitionTagKey ) ; BusinessObjectDefinitionTagEntity businessObjectDefinitionTagEntity = getBusinessObjectDefinitionTagEntity ( businessObjectDefinitionTagKey ) ; businessObjectDefinitionTagDao . delete ( businessObjectDefinitionTagEntity ) ; BusinessObjectDefinitionEntity businessObjectDefinitionEntity = businessObjectDefinitionDaoHelper . getBusinessObjectDefinitionEntity ( businessObjectDefinitionTagKey . getBusinessObjectDefinitionKey ( ) ) ; LOGGER . info ( "Modify the business object definition in the search index associated with the business object definition tag being deleted." + " businessObjectDefinitionId=\"{}\", searchIndexUpdateType=\"{}\"" , businessObjectDefinitionEntity . getId ( ) , SEARCH_INDEX_UPDATE_TYPE_UPDATE ) ; searchIndexUpdateHelper . modifyBusinessObjectDefinitionInSearchIndex ( businessObjectDefinitionEntity , SEARCH_INDEX_UPDATE_TYPE_UPDATE ) ; return createBusinessObjectDefinitionTagFromEntity ( businessObjectDefinitionTagEntity ) ; } }
public void test() { try { String url = conf . get ( DBConfiguration . URL_PROPERTY ) ; code_block = TryStatement ;  Properties properties = ConnectionConfig . getConnectionArguments ( conf . get ( DBUtils . CONNECTION_ARGUMENTS ) , conf . get ( DBConfiguration . USERNAME_PROPERTY ) , conf . get ( DBConfiguration . PASSWORD_PROPERTY ) ) ; connection = DriverManager . getConnection ( url , properties ) ; boolean autoCommitEnabled = conf . getBoolean ( AUTO_COMMIT_ENABLED , false ) ; code_block = IfStatement ; String level = conf . get ( TransactionIsolationLevel . CONF_KEY ) ; connection . setTransactionIsolation ( TransactionIsolationLevel . getLevel ( level ) ) ; } catch ( Exception e ) { logger . error ( "" , e ) ; throw Throwables . propagate ( e ) ; } }
public void test() { if ( _orphaned . size ( ) > 0 && logger . isInfoEnabled ( ) ) { logger . info ( "Morphaned [" + _orphaned . size ( ) + "]" ) ; } }
public void test() { if ( WARNED_READ_ONLY_ATTRIBUTES . add ( attribute . getName ( ) ) ) { LOG . warn ( message ) ; } else-if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( message ) ; } }
public void test() { if ( WARNED_READ_ONLY_ATTRIBUTES . add ( attribute . getName ( ) ) ) { LOG . warn ( message + " (future messages for this sensor logged at trace)" ) ; } else-if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( message + " (future messages for this sensor logged at trace)" ) ; } }
public void test() { try { MigrationEvent other = migrationEventDao . buildMigrationEvent ( migrationEvent . getMigrationEventId ( ) ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { out = new PrintWriter ( new FileWriter ( file , true ) ) ; } catch ( IOException e ) { log . error ( "Failed to open file: " + file , e ) ; out = new PrintWriter ( new NullWriter ( ) ) ; } }
public List < DataSourceColumnDto > findAllDataFileColumns ( final Long dataFileId ) { LOGGER . debug ( "findAllDataFileColumns() - dataFileId: {}" , dataFileId ) ; final List < DataSourceColumn > result = new ArrayList < > ( ) ; final DataFile dataFile = dataFileService . find ( dataFileId ) ; code_block = ForStatement ; return dataSourceColumnToDataSourceColumnDtoConverter . convertToList ( result ) ; }
public void test() { try { BigDecimal state = new BigDecimal ( parameters [ 1 ] ) ; updateState ( CHANNEL_SWITCH , state . compareTo ( BigDecimal . ZERO ) == 0 ? OnOffType . OFF : OnOffType . ON ) ; } catch ( NumberFormatException e ) { logger . debug ( "Could not parse number {}" , parameters [ 1 ] ) ; return ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
@ Override public void status ( WorkloadStatusSnapshot status , RecentThroughputAndDuration recentThroughputAndDuration , long completionTimeAsMilli ) { String statusString ; log . trace ( "Status for: {}" , status ) ; statusString = ( detailedStatus ) ? formatWithCt ( status . operationCount ( ) , status . runDurationAsMilli ( ) , status . durationSinceLastMeasurementAsMilli ( ) , status . throughput ( ) , recentThroughputAndDuration . throughput ( ) , recentThroughputAndDuration . duration ( ) , completionTimeAsMilli ) : formatWithoutCt ( status . operationCount ( ) , status . runDurationAsMilli ( ) , status . durationSinceLastMeasurementAsMilli ( ) , status . throughput ( ) , recentThroughputAndDuration . throughput ( ) , recentThroughputAndDuration . duration ( ) ) ; }
public void test() { if ( logger . isLoggable ( Level . INFO ) ) { logger . info ( e . getMessage ( ) , e ) ; } }
public void test() { if ( logger . isLoggable ( Level . INFO ) ) { logger . info ( e . getMessage ( ) , e ) ; } }
public void test() { try { DriverManager . deregisterDriver ( driver ) ; } catch ( SQLException e ) { logger . error ( "Error while deregistering driver " + driver , e ) ; } }
@ Path ( "/getAllShouldSucceed" ) @ POST public void getAllShouldSucceed ( ) { LOG . debug ( "Calling OpenstackContainerResource.getAllShouldSucceed()" ) ; String uri = String . format ( URI_FORMAT , OpenstackConstants . GET_ALL ) ; SwiftContainer [ ] containers = template . requestBody ( uri , null , SwiftContainer [ ] . class ) ; assertNotNull ( containers ) ; assertEquals ( 2 , containers . length ) ; assertEquals ( 100 , containers [ 0 ] . getTotalSize ( ) ) ; assertEquals ( "Test" , containers [ 0 ] . getName ( ) ) ; assertEquals ( "marktwain" , containers [ 1 ] . getName ( ) ) ; }
public void test() { try { String response = send ( params , CMBProperties . getInstance ( ) . getCQSServiceUrl ( ) ) ; return CqsStressTester . deserialize ( response , "QueueUrl" ) . trim ( ) ; } catch ( Exception e ) { logger . debug ( e . getMessage ( ) ) ; return null ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getCapabilities()" ) ; } }
private void getServerProfileTemplateById ( ) { ServerProfileTemplate template = this . serverProfileTemplateClient . getByName ( SERVER_PROFILE_TEMPLATE_NAME ) . get ( 0 ) ; template = serverProfileTemplateClient . getById ( template . getResourceId ( ) ) ; LOGGER . info ( "Server profile template object returned to client : " + template . toJsonString ( ) ) ; }
public void test() { if ( flowConfigurationFile == null ) { logger . error ( "Flow configuration file is null." ) ; return null ; } }
public void test() { if ( ! Files . exists ( flowPath ) || Files . size ( flowPath ) == 0 ) { logger . debug ( "Path {} does not exist." , flowPath ) ; return null ; } }
public void test() { try { code_block = IfStatement ; } catch ( IOException e ) { logger . error ( "Unable to create file " + path , e ) ; return null ; } }
protected void generatePojo ( TableDefinition table ) { JavaWriter out = newJavaWriter ( getFile ( table , Mode . POJO ) ) ; log . info ( "Generating POJO to " + table . getAbsolutePath ( ) ) ; generatePojo ( table , out ) ; closeJavaWriter ( out ) ; }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( req . toString ( ) ) ; log . trace ( Hexdump . toHexString ( this . sbuf , 4 , size ) ) ; } }
public void test() { try { CompletableFuture < List < IType > > future = subtypesCache . get ( searchParams , ( ) -> client . javaSubTypes ( searchParams ) . handle ( ( results , exception ) -> results . stream ( ) . map ( e -> detailed ? toType ( e . getRight ( ) ) : toTypeFromDescriptor ( e . getLeft ( ) ) ) . collect ( Collectors . toList ( ) ) ) ) ; return Mono . fromFuture ( future ) . flatMapMany ( results -> Flux . fromIterable ( results ) ) ; } catch ( ExecutionException e ) { logger . error ( e . getMessage ( ) , e ) ; return Flux . empty ( ) ; } }
@ Override public void execute ( ) throws Exception { logger . debug ( "Executing sub-command line" ) ; String subCommandString = getParsedSubCommand ( jobCommandOptions . jCommander ) ; configure ( ) ; code_block = SwitchStatement ; }
public void test() { switch ( subCommandString ) { case "secondary-index" : secondaryIndex ( ) ; break ; default : logger . warn ( "Subcommand not valid" ) ; break ; } }
public void test() { try { final Cipher cipher = cipherFactory . initCipher ( Cipher . ENCRYPT_MODE ) ; writeEncryptionParametersToStream ( os , cipher ) ; @ SuppressWarnings ( "resource" ) final CipherOutputStream cos = new CipherOutputStream ( os , cipher ) ; os = cos ; } catch ( final CipherException e ) { LOGGER . error ( "Unable to encrypt Encryption" , e ) ; return ; } }
public void test() { if ( success && ( aFile . exists ( ) && ! aFile . delete ( ) || ! temp . renameTo ( aFile ) ) ) { LOGGER . warn ( "Failed to rename file: " + aFile . getAbsolutePath ( ) + " to " + aFile . getAbsolutePath ( ) ) ; } else-if ( ! temp . delete ( ) && LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Failed to delete temp file: " + temp ) ; } }
public void test() { if ( success && ( aFile . exists ( ) && ! aFile . delete ( ) || ! temp . renameTo ( aFile ) ) ) { LOGGER . warn ( "Failed to rename preferences temp file [" + temp + "] to [" + aFile + "]: preferences were not saved correctly." ) ; } else-if ( ! temp . delete ( ) && LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Failed to delete preferences temp file [" + temp + "]" ) ; } }
public void test() { if ( ! singleSuggestionMode && bestEvaluatedDescriptions . getBestAccuracy ( ) > currentHighestAccuracy ) { currentHighestAccuracy = bestEvaluatedDescriptions . getBestAccuracy ( ) ; expressionTestCountLastImprovement = expressionTests ; timeLastImprovement = System . nanoTime ( ) ; long durationInMillis = getCurrentRuntimeInMilliSeconds ( ) ; String durationStr = getDurationAsString ( durationInMillis ) ; log . info ( "Duration: " + durationStr ) ; } }
@ Override public void onException ( ChunkedOutput < OutboundEvent > chunkedOutput , Exception exception ) { super . onException ( chunkedOutput , exception ) ; logger . debug ( "ChunkedOutputListener.onException()" , exception ) ; }
public void test() { try { getResponse ( ) . sendError ( 500 ) ; } catch ( Throwable t2 ) { LOGGER . warn ( "" , t2 ) ; } }
public void test() { try { keyInfo = SAML1ComponentBuilder . createKeyInfo ( keyInfoBean ) ; } catch ( SecurityException | WSSecurityException e ) { LOG . error ( "Could not create KeyInfo instance" , e ) ; throw new SAMLComponentBuilderException ( e . getLocalizedMessage ( ) , e ) ; } }
public void test() { try { fsm . fire ( event ) ; return event ; } catch ( Exception e ) { LOGGER . log ( Level . SEVERE , "Failed to fire event: " + event , e ) ; } }
public void test() { { LOG . error ( "schedule is not supported on Server.Please run your operation on Prism " ) ; throw FalconWebException . newAPIException ( "schedule is not supported on Server. Please run your operation " + "on Prism." ) ; } }
public void test() { if ( offset < 0 ) { log . error ( "Invalid offset: {}" , offset ) ; return ; } else-if ( offset > 0 ) { code_block = IfStatement ; code_block = TryStatement ;  } }
public void test() { try { code_block = IfStatement ; local . seek ( offset ) ; } catch ( Exception e ) { log . error ( IO_EXCEPTION_OCCURRED_DURING_PARALLEL_FILE_TRANSFER , e ) ; throw new JargonException ( IO_EXCEPTION_OCCURRED_DURING_PARALLEL_FILE_TRANSFER , e ) ; } }
private void processDisambiguatedOrg ( OrgDisambiguatedEntity entity ) { LOGGER . info ( "Processing {}" , entity . getId ( ) ) ; OrgDisambiguatedSolrDocument document = convertEntityToDocument ( entity ) ; code_block = IfStatement ; orgDisambiguatedDao . updateIndexingStatus ( entity . getId ( ) , IndexingStatus . DONE ) ; }
public void test() { if ( ! messaging . send ( document , updateSolrQueueName ) ) { orgDisambiguatedDao . updateIndexingStatus ( entity . getId ( ) , IndexingStatus . FAILED ) ; LOGGER . info ( "Entity indexing to " + entity . getId ( ) + " because it was not successful." ) ; return ; } }
public void test() { -> { LOGGER . info ( CALLED ) ; latch . countDown ( ) ; return rs ; } }
public void test() { try { final List < ScheduleMeasurements_args > list = new ArrayList < ScheduleMeasurements_args > ( ) ; argQueue . drainTo ( list ) ; code_block = IfStatement ; Thread . sleep ( 30000 ) ; } catch ( InterruptedException e ) { logger . error ( "Thread interrupted" , e ) ; } }
public void test() { try { code_block = WhileStatement ; } catch ( Throwable t ) { logger . error ( t . getMessage ( ) , t ) ; } }
public Boolean apply ( WebDriver driver ) { log . info ( "Activating." ) ; driver . manage ( ) . window ( ) . maximize ( ) ; return true ; }
public void test() { try { AddPDXTypeOp . execute ( ( ExecutablePool ) pool , id , type ) ; } catch ( ServerConnectivityException serverConnectivityException ) { LOGGER . debug ( "addPDXTypeOp failed" , serverConnectivityException ) ; throw serverConnectivityException ; } }
public void test() { if ( tryPort == 0 ) { LOG . warn ( String . format ( "Timeline server could not bind on port %d." , "Attempting port %d" , tryPort ) ) ; } else { LOG . warn ( String . format ( "Timeline server could not bind on port %d. " + "Attempting port %d + 1." , tryPort , tryPort ) ) ; } }
public void test() { if ( tryPort == 0 ) { LOG . warn ( "Timeline server could not bind on a random free port." ) ; } else { LOG . info ( "Timeline server started on port {}" , tryPort ) ; } }
public void test() { if ( e . getMessage ( ) != null && e . getMessage ( ) . contains ( "Failed to bind to" ) ) { code_block = IfStatement ; } else { log . error ( e . getMessage ( ) , e ) ; } }
@ Test public void testObjectToString ( ) { Foo arg = new Foo ( ) ; expect ( mockLog . isLevelEnabled ( Level . TRACE ) ) . andReturn ( true ) ; mockLog . log ( Level . TRACE , Foo . TO_STRING ) ; replay ( mockLog ) ; logger . trace ( mockLog . toString ( ) ) ; verify ( mockLog ) ; }
public void test() { try { Hits hits = _assetHelper . search ( searchContext , assetEntryQuery , assetEntryQuery . getStart ( ) , assetEntryQuery . getEnd ( ) ) ; return _assetHelper . getAssetEntries ( hits ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; } }
public void testOpenL ( ) throws SyntaxNodeException { boolean b ; long t = System . currentTimeMillis ( ) ; b = executeBooleanOpenLExprression ( data , OPENL_EXPR ) ; assertTrue ( b ) ; b = executeBooleanOpenLExprression ( data , NEG_OPENL_EXPR ) ; assertTrue ( b ) ; logger . debug ( "Opened at " + ( System . currentTimeMillis ( ) - t ) / 1000.0 ) ; }
public void test() { try { pin . open ( ) ; } catch ( KuraGPIODeviceException | KuraUnavailableDeviceException | IOException e ) { logger . error ( "Open Exception " , e ) ; } }
public void test() { if ( debug ) { logger . debug ( "Expected error: " + e . getMessage ( ) ) ; } }
public void test() { try { String filename = CommandHistory . getHistorySaver ( workspace . getId ( ) ) . getHistoryFilepath ( worksheetId ) ; JSONArray historyJson = CommandHistory . getHistorySaver ( workspace . getId ( ) ) . loadHistory ( filename ) ; filteredHistoryJson = HistoryJsonUtil . filterCommandsByTag ( tag , historyJson ) ; } catch ( JSONException e ) { logger . error ( "Error parsing command history!" , e ) ; } catch ( Exception e ) { logger . error ( "Error reading from history file!" , e ) ; } }
public void test() { try { String filename = CommandHistory . getHistorySaver ( workspace . getId ( ) ) . getHistoryFilepath ( worksheetId ) ; JSONArray historyJson = CommandHistory . getHistorySaver ( workspace . getId ( ) ) . loadHistory ( filename ) ; filteredHistoryJson = HistoryJsonUtil . filterCommandsByTag ( tag , historyJson ) ; } catch ( JSONException e ) { logger . error ( "Error occured while working with JSON!" , e ) ; } catch ( Exception e ) { logger . error ( "Error occured while loading history!" , e ) ; } }
public void test() { if ( userMessageLogger != null ) { userMessageLogger . log ( null , userMessageLogger ) ; } }
public void test() { try { directService . updateDomain ( updateDomain ) ; selectedDomain = null ; refreshDomains ( ) ; } catch ( DomainException domainException ) { logger . error ( "Error while updating domain" , domainException ) ; FacesContext . getCurrentInstance ( ) . validationFailed ( ) ; FacesContext . getCurrentInstance ( ) . addMessage ( "domainEditErrors" , new FacesMessage ( FacesMessage . SEVERITY_ERROR , "Cannot update domain: " + domainException . getLocalizedMessage ( ) , "" ) ) ; } }
public void test() { try { serviceRegistry . register ( ) ; } catch ( RuntimeException e ) { LOGGER . warn ( "Could not register ServiceRegistry" , e ) ; } }
public StgG20Anwb findById ( sernet . gs . reveng . StgG20AnwbId id ) { log . debug ( "getting StgG20Anwb instance with id: " + id ) ; code_block = TryStatement ;  }
public void test() { if ( instance == null ) { log . debug ( "get successful, no instance found" ) ; } else { log . debug ( "get successful, instance found" ) ; } }
public void test() { if ( instance == null ) { log . debug ( "get successful, no instance found" ) ; } else { log . debug ( "get successful, instance found" ) ; } }
public void test() { try { StgG20Anwb instance = ( StgG20Anwb ) sessionFactory . getCurrentSession ( ) . get ( "sernet.gs.reveng.StgG20Anwb" , id ) ; code_block = IfStatement ; return instance ; } catch ( RuntimeException re ) { log . error ( "get failed" , re ) ; throw re ; } }
public void test() { try { listener . onStartAsync ( event ) ; } catch ( Throwable e ) { log . error ( e ) ; } }
@ Override public void addUserGroupAsGroupAdmin ( final UserGroup userGroup ) throws DuplicateDataException , JargonException { log . info ( "addUserGroupAsGroupAdmin()" ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "user group:{}" , userGroup ) ; code_block = IfStatement ; code_block = TryStatement ;  }
@ Override public void addUserGroupAsGroupAdmin ( final UserGroup userGroup ) throws DuplicateDataException , JargonException { log . info ( "addUserGroupAsGroupAdmin()" ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "addUserGroupAsGroupAdmin()" ) ; code_block = TryStatement ;  }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Local versions:" ) ; code_block = ForStatement ; } }
public void test() { for ( Header h : method . getResponseHeaders ( ) ) { logger . debug ( "resp head[" + h . getName ( ) + "] => " + h . getValue ( ) ) ; } }
public void test() { try { int statusCode = httpClient . executeMethod ( method ) ; Assert . assertEquals ( method . getResponseBody ( ) , null , "expected null" ) ; code_block = IfStatement ; Assert . assertEquals ( statusCode , HttpStatus . SC_OK , "expected " + HttpStatus . SC_OK ) ; } catch ( HttpException e ) { logger . error ( "Fatal protocol violation:" , e ) ; } catch ( IOException e ) { logger . error ( "Fatal transport error" , e ) ; } catch ( Exception e ) { logger . error ( "unknown exception " , e ) ; } finally { method . releaseConnection ( ) ; } }
public void test() { try { int statusCode = httpClient . executeMethod ( method ) ; Assert . assertEquals ( method . getResponseBody ( ) , null , "expected null" ) ; code_block = IfStatement ; Assert . assertEquals ( statusCode , HttpStatus . SC_OK , "expected " + HttpStatus . SC_OK ) ; } catch ( HttpException e ) { logger . error ( "Fatal protocol violation: " , e ) ; } catch ( IOException e ) { logger . error ( "Fatal transport error" , e ) ; } catch ( Exception e ) { logger . error ( "unknown exception " , e ) ; } finally { method . releaseConnection ( ) ; } }
public void test() { try { int statusCode = httpClient . executeMethod ( method ) ; Assert . assertEquals ( method . getResponseBody ( ) , null , "expected null" ) ; code_block = IfStatement ; Assert . assertEquals ( statusCode , HttpStatus . SC_OK , "expected " + HttpStatus . SC_OK ) ; } catch ( HttpException e ) { logger . error ( "Fatal protocol violation: " , e ) ; } catch ( IOException e ) { logger . error ( "Fatal transport error" , e ) ; } catch ( Exception e ) { logger . error ( "unknown exception " , e ) ; } finally { method . releaseConnection ( ) ; } }
public void test() { try { listeners . put ( cls , ( ReaderListener ) cls . newInstance ( ) ) ; } catch ( Exception e ) { LOGGER . error ( "Failed to create ReaderListener" , e ) ; } }
public void test() { try { listener . beforeScan ( this , openAPI ) ; } catch ( Exception e ) { LOGGER . error ( "Unexpected error invoking beforeScan listener [" + listener . getClass ( ) . getName ( ) + "]" , e ) ; } }
public void test() { try { listener . afterScan ( this , openAPI ) ; } catch ( Exception e ) { LOGGER . error ( "Unexpected error invoking afterScan listener [" + listener . getClass ( ) . getName ( ) + "]" , e ) ; } }
public void test() { try { metadataIndexNode = MetadataIndexNode . deserializeFrom ( buffer ) ; } catch ( BufferOverflowException e ) { logger . error ( METADATA_INDEX_NODE_DESERIALIZE_ERROR , file ) ; throw e ; } }
public void test() { try { timeseriesMetadataList . add ( TimeseriesMetadata . deserializeFrom ( buffer , true ) ) ; } catch ( BufferOverflowException e ) { logger . error ( "Something error happened while deserializing TimeseriesMetadata of file {}" , file ) ; throw e ; } }
public void test() { try { Gson gson = new GsonBuilder ( ) . setPrettyPrinting ( ) . create ( ) ; String json = gson . toJson ( data ) ; Date timestamp = new Date ( System . currentTimeMillis ( ) ) ; Path logPath = Paths . get ( path , "timeMeasuring" ) ; Files . createDirectories ( logPath ) ; Files . write ( Paths . get ( logPath . toString ( ) , String . format ( "%s-%s.json" , name , dateFormat . format ( timestamp ) ) ) , json . getBytes ( Charset . forName ( "UTF-8" ) ) ) ; } catch ( Exception ex ) { log . error ( ex . getMessage ( ) , ex ) ; } }
@ Override public DocumentInfo findDocumentByDocumentId ( int documentId ) { DocumentInfo document = dao . findById ( DocumentInfo . class , documentId ) ; log . info ( "Found document with id: " + document . getId ( ) ) ; return document ; }
@ Test public void testTargetMappingUpdatesAfterRebind ( ) throws Exception { Iterable < StubAppServer > members = Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ; assertExpectedTargetsEventually ( members ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) ) ; rebind ( ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) , "location not managed after rebind" ) ; Iterable < StubAppServer > members2 = Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ; StubAppServer target1 = Iterables . get ( members2 , 0 ) ; StubAppServer target2 = Iterables . get ( members2 , 1 ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target1 , target2 ) ) ; log . info ( "resizing " + cluster + " - " + cluster . getChildren ( ) ) ; Integer result = cluster . resize ( 3 ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) ) ; log . info ( "resized " + cluster + " (" + result + ") - " + cluster . getChildren ( ) ) ; HashSet < StubAppServer > newEntities = Sets . newHashSet ( Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ) ; newEntities . remove ( target1 ) ; newEntities . remove ( target2 ) ; log . info ( "removing " + ImmutableSet . of ( target1 , target2 , target3 ) ) ; StubAppServer target3 = Iterables . getOnlyElement ( newEntities ) ; log . info ( "expecting " + ImmutableSet . of ( target1 , target2 , target3 ) ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target1 , target2 , target3 ) ) ; log . info ( "pretending one node down" ) ; target1 . sensors ( ) . set ( Stub
@ Test public void testTargetMappingUpdatesAfterRebind ( ) throws Exception { log . info ( "starting testTargetMappingUpdatesAfterRebind" ) ; Iterable < StubAppServer > members = Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ; assertExpectedTargetsEventually ( members ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) ) ; rebind ( ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) , "location not managed after rebind" ) ; Iterable < StubAppServer > members2 = Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ; StubAppServer target1 = Iterables . get ( members2 , 0 ) ; StubAppServer target2 = Iterables . get ( members2 , 1 ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target1 , target2 ) ) ; Integer result = cluster . resize ( 3 ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) ) ; log . info ( "resized " + cluster + " (" + result + ") - " + cluster . getChildren ( ) ) ; HashSet < StubAppServer > newEntities = Sets . newHashSet ( Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ) ; newEntities . remove ( target1 ) ; newEntities . remove ( target2 ) ; log . info ( "removing " + ImmutableSet . of ( target1 , target2 , target3 ) ) ; StubAppServer target3 = Iterables . getOnlyElement ( newEntities ) ; log . info ( "expecting " + ImmutableSet . of ( target1 , target2 , target3 ) ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target1 , target2 , target3 ) ) ; log . info ( "pretending one node down" ) ; target1 . sensors ( ) . set ( StubAppServer . SERVICE_
@ Test public void testTargetMappingUpdatesAfterRebind ( ) throws Exception { log . info ( "starting testTargetMappingUpdatesAfterRebind" ) ; Iterable < StubAppServer > members = Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ; assertExpectedTargetsEventually ( members ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) ) ; rebind ( ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) , "location not managed after rebind" ) ; Iterable < StubAppServer > members2 = Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ; StubAppServer target1 = Iterables . get ( members2 , 0 ) ; StubAppServer target2 = Iterables . get ( members2 , 1 ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target1 , target2 ) ) ; log . info ( "resizing " + cluster + " - " + cluster . getChildren ( ) ) ; Integer result = cluster . resize ( 3 ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) ) ; HashSet < StubAppServer > newEntities = Sets . newHashSet ( Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ) ; newEntities . remove ( target1 ) ; newEntities . remove ( target2 ) ; log . info ( "removing " + ImmutableSet . of ( target1 , target2 , target3 ) ) ; StubAppServer target3 = Iterables . getOnlyElement ( newEntities ) ; log . info ( "expecting " + ImmutableSet . of ( target1 , target2 , target3 ) ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target1 , target2 , target3 ) ) ; log . info ( "pretending one node down" ) ; target1 . sensors ( ) . set ( StubAppServer . SERVICE_UP , false ) ; assert
@ Test public void testTargetMappingUpdatesAfterRebind ( ) throws Exception { log . info ( "starting testTargetMappingUpdatesAfterRebind" ) ; Iterable < StubAppServer > members = Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ; assertExpectedTargetsEventually ( members ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) ) ; rebind ( ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) , "location not managed after rebind" ) ; Iterable < StubAppServer > members2 = Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ; StubAppServer target1 = Iterables . get ( members2 , 0 ) ; StubAppServer target2 = Iterables . get ( members2 , 1 ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target1 , target2 ) ) ; log . info ( "resizing " + cluster + " - " + cluster . getChildren ( ) ) ; Integer result = cluster . resize ( 3 ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) ) ; log . info ( "resized " + cluster + " (" + result + ") - " + cluster . getChildren ( ) ) ; HashSet < StubAppServer > newEntities = Sets . newHashSet ( Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ) ; newEntities . remove ( target1 ) ; newEntities . remove ( target2 ) ; log . info ( "restending one node down" ) ; StubAppServer target3 = Iterables . getOnlyElement ( newEntities ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target1 , target2 , target3 ) ) ; log . info ( "pretending one node down" ) ; target1 . sensors ( ) . set ( StubAppServer . SERVICE_UP , false ) ; assertExpectedTargetsEventually ( ImmutableSet
@ Test public void testTargetMappingUpdatesAfterRebind ( ) throws Exception { log . info ( "starting testTargetMappingUpdatesAfterRebind" ) ; Iterable < StubAppServer > members = Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ; assertExpectedTargetsEventually ( members ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) ) ; rebind ( ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) , "location not managed after rebind" ) ; Iterable < StubAppServer > members2 = Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ; StubAppServer target1 = Iterables . get ( members2 , 0 ) ; StubAppServer target2 = Iterables . get ( members2 , 1 ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target1 , target2 ) ) ; log . info ( "resizing " + cluster + " - " + cluster . getChildren ( ) ) ; Integer result = cluster . resize ( 3 ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) ) ; log . info ( "resized " + cluster + " (" + result + ") - " + cluster . getChildren ( ) ) ; HashSet < StubAppServer > newEntities = Sets . newHashSet ( Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ) ; newEntities . remove ( target1 ) ; newEntities . remove ( target2 ) ; StubAppServer target3 = Iterables . getOnlyElement ( newEntities ) ; log . info ( "expecting " + ImmutableSet . of ( target1 , target2 , target3 ) ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target1 , target2 , target3 ) ) ; target1 . sensors ( ) . set ( StubAppServer . SERVICE_UP , false ) ; log . info ( "expecting " + Immutable
@ Test public void testTargetMappingUpdatesAfterRebind ( ) throws Exception { log . info ( "starting testTargetMappingUpdatesAfterRebind" ) ; Iterable < StubAppServer > members = Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ; assertExpectedTargetsEventually ( members ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) ) ; rebind ( ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) , "location not managed after rebind" ) ; Iterable < StubAppServer > members2 = Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ; StubAppServer target1 = Iterables . get ( members2 , 0 ) ; StubAppServer target2 = Iterables . get ( members2 , 1 ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target1 , target2 ) ) ; log . info ( "resizing " + cluster + " - " + cluster . getChildren ( ) ) ; Integer result = cluster . resize ( 3 ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) ) ; log . info ( "resized " + cluster + " (" + result + ") - " + cluster . getChildren ( ) ) ; HashSet < StubAppServer > newEntities = Sets . newHashSet ( Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ) ; newEntities . remove ( target1 ) ; newEntities . remove ( target2 ) ; log . info ( "expecting " + ImmutableSet . of ( target1 , target2 ) ) ; StubAppServer target3 = Iterables . getOnlyElement ( newEntities ) ; log . info ( "expecting " + ImmutableSet . of ( target1 , target2 , target3 ) ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target1 , target2 , target3 ) ) ; log . info ( "pretending one
@ Test public void testTargetMappingUpdatesAfterRebind ( ) throws Exception { log . info ( "starting testTargetMappingUpdatesAfterRebind" ) ; Iterable < StubAppServer > members = Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ; assertExpectedTargetsEventually ( members ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) ) ; rebind ( ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) , "location not managed after rebind" ) ; Iterable < StubAppServer > members2 = Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ; StubAppServer target1 = Iterables . get ( members2 , 0 ) ; StubAppServer target2 = Iterables . get ( members2 , 1 ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target1 , target2 ) ) ; log . info ( "resizing " + cluster + " - " + cluster . getChildren ( ) ) ; Integer result = cluster . resize ( 3 ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) ) ; log . info ( "resized " + cluster + " (" + result + ") - " + cluster . getChildren ( ) ) ; HashSet < StubAppServer > newEntities = Sets . newHashSet ( Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ) ; newEntities . remove ( target1 ) ; newEntities . remove ( target2 ) ; log . info ( "expecting " + ImmutableSet . of ( target1 , target2 ) ) ; StubAppServer target3 = Iterables . getOnlyElement ( newEntities ) ; log . info ( "expecting " + ImmutableSet . of ( target1 , target2 , target3 ) ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target1 , target2 , target3 ) ) ; log . info ( "pretending one
public void test() { if ( session . getAllowedStateTransitions ( docRef ) . contains ( LifeCycleConstants . UNDELETE_TRANSITION ) ) { undeleteDocument ( session , doc ) ; undeleted . add ( docRef ) ; } else { LOG . warn ( "Ignoring undelete document {}" , docRef ) ; } }
public void test() { try { MapSqlParameterSource params = new MapSqlParameterSource ( ) ; params . addValue ( "processId" , processId ) ; List < JobProcessStatus > jobProcessStatusList = baseDao . geoApiNamedJbdcTemaplate . query ( JobProcessQuery . GET_JOB_PROCESS_STATUS . getSql ( baseDao . getJobSchema ( ) ) , params , statusHandler ) ; code_block = IfStatement ; } catch ( Exception ex ) { logger . error ( "Failed to get job status list." , ex ) ; } }
public void test() { if ( closingConnection . get ( ) || closed . get ( ) || failed . get ( ) ) { log . debug ( "Removing connection [id=" + id + "]" ) ; requests . remove ( id ) ; super . onFailure ( result ) ; } else { processAlternates ( provider . getAlternateURIs ( ) ) ; handleProviderFailure ( activeProvider , ProviderExceptionSupport . createOrPassthroughFatal ( result ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "to be updated object" ) ; logger . debug ( objectAsXmlString ( pottagCommon , PottagsCommon . class ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "to be created, pottag common" ) ; logger . debug ( objectAsXmlString ( pottag , PersonCommon . class ) ) ; } }
public void test() { try { return new CSVParser ( reader , csvFormat ) ; } catch ( final IOException e ) { final String message = "couldn't process lookup table to JSON, because couldn't read it with the given CSV format configuration" ; LOG . error ( message ) ; throw new DMPControllerException ( message , e ) ; } }
public void test() { try ( GenericXmlApplicationContext applicationContext = new GenericXmlApplicationContext ( new UrlResource ( resource ) ) ) { Map < String , AppEngine > beansOfType = applicationContext . getBeansOfType ( AppEngine . class ) ; code_block = IfStatement ; AppEngine appEngine = beansOfType . values ( ) . iterator ( ) . next ( ) ; logger . debug ( "Found app engine: {}" , appEngine ) ; return appEngine ; } }
public void test() { try { AccessControlPolicy [ ] applicable = editor . editAccessControlPolicies ( absPath ) ; return new AccessControlPolicyIteratorAdapter ( Arrays . asList ( applicable ) ) ; } catch ( AccessControlException e ) { log . debug ( e . getMessage ( ) ) ; } }
public void test() { try { com . liferay . portal . kernel . model . Region returnValue = RegionServiceUtil . addRegion ( countryId , active , name , position , regionCode , serviceContext ) ; return com . liferay . portal . kernel . model . RegionSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( mb == null ) { logger . warn ( "Can't find binary mb" ) ; return ; } }
@ Override public void onClose ( ) { logger . info ( "Connection to Client stopped" ) ; subscriber . onCompleted ( ) ; }
public void test() { if ( JBoss6VFS . valid != null && JBoss6VFS . valid ) { log . warn ( "JBoss 6 VFS API is not valid" ) ; JBoss6VFS . valid = false ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { try ( AccumuloClient client = Accumulo . newClient ( ) . to ( config . getInstanceName ( ) , config . getZookeepers ( ) ) . as ( config . getUsername ( ) , config . getPassword ( ) ) . build ( ) ) { Collection < Authorizations > authCollection = Collections . singleton ( new Authorizations ( config . getAuths ( ) . split ( "," ) ) ) ; code_block = IfStatement ; code_block = TryStatement ;  } catch ( Exception e ) { log . error ( "Error loading Accumulo client" , e ) ; } }
public void test() { if ( ! isSet ( key + IDP_ID ) ) { logger . error ( "key " + key + " already exists!" ) ; return false ; } }
public void test() { if ( getCertificateNames ( key ) . size ( ) == 0 ) { LOG . error ( "Certificate names are not valid" ) ; return false ; } }
public void test() { if ( ( translatioProfile == null || translatioProfile . isEmpty ( ) ) && ( embeddedTranslatioProfile == null || embeddedTranslatioProfile . isEmpty ( ) ) ) { LOGGER . info ( "Translator not configured" ) ; return false ; } }
public void test() { if ( ! allCollectionList . contains ( solr_collection_name ) ) { logger . info ( "Creating collection '" + solr_collection_name + "'" ) ; int shardsCalculation = solrCloudClient != null ? solrCloudClient . getClusterStateProvider ( ) . getLiveNodes ( ) . size ( ) : DEFAULT_VALUE ; int no_of_shards = EmbeddedServerUtil . getIntConfig ( SOLR_NO_SHARDS , shardsCalculation ) ; CollectionAdminRequest . Create createCollection = CollectionAdminRequest . createCollection ( solr_collection_name , solr_config_name , no_of_shards , no_of_replicas ) ; createCollection . setMaxShardsPerNode ( max_node_per_shards ) ; CollectionAdminResponse createResponse = createCollection . process ( solrClient ) ; code_block = IfStatement ; } else { logger . info ( "Collection already exists with name " + solr_collection_name ) ; return true ; } }
public void test() { if ( createResponse . getStatus ( ) != 0 ) { logger . severe ( "Error creating collection. collectionName=" + solr_collection_name + " , solr config name = " + solr_config_name + " , replicas = " + no_of_replicas + ", shards=" + no_of_shards + " , max node per shards = " + max_node_per_shards + ", response=" + createResponse ) ; return false ; } else { logger . info ( "Created collection. collectionName=" + solr_collection_name + " , solr config name = " + solr_config_name ) ; return true ; } }
public void test() { if ( ! allCollectionList . contains ( solr_collection_name ) ) { logger . info ( "Collection list found: " + solr_collection_name ) ; int shardsCalculation = solrCloudClient != null ? solrCloudClient . getClusterStateProvider ( ) . getLiveNodes ( ) . size ( ) : DEFAULT_VALUE ; int no_of_shards = EmbeddedServerUtil . getIntConfig ( SOLR_NO_SHARDS , shardsCalculation ) ; logger . info ( "No. of shards provided is : " + no_of_shards ) ; CollectionAdminRequest . Create createCollection = CollectionAdminRequest . createCollection ( solr_collection_name , solr_config_name , no_of_shards , no_of_replicas ) ; createCollection . setMaxShardsPerNode ( max_node_per_shards ) ; CollectionAdminResponse createResponse = createCollection . process ( solrClient ) ; code_block = IfStatement ; } else { return true ; } }
public void initTimer ( ) { final int delay = 30 ; final int interval = DEFAULT_INTERVAL ; log . debug ( "Initializing Timer" ) ; timerEvent . fire ( new TimerEvent ( new TimerSchedule ( delay , interval ) , new MetadataValidationEvent ( ) , Scheduled . Literal . INSTANCE ) ) ; }
public void test() { if ( ! raftInitialized ( ) ) { LOGGER . error ( "Raft incomplete initialization!" ) ; return false ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { try { OvmStoragePool . Details d = OvmStoragePool . getDetailsByUuid ( _conn , cmd . getStorageId ( ) ) ; return new GetStorageStatsAnswer ( cmd , d . totalSpace , d . usedSpace ) ; } catch ( Exception e ) { s_logger . debug ( "getStorageStatsAnswer(" + cmd . getStorageId ( ) + ") failed to " + cmd . getStorageId ( ) , e ) ; return new GetStorageStatsAnswer ( cmd , e . getMessage ( ) ) ; } }
public void test() { if ( clientResponse . getStatus ( ) != 200 ) { logger . warn ( "Couldn't contact AIDRTaggerAPI for sending error message" ) ; } }
public void test() { try { WebTarget webResource = client . target ( AnalyticsConfigurator . getInstance ( ) . getProperty ( AnalyticsConfigurationProperty . TAGGER_REST_URI ) + "/misc/sendErrorEmail" ) ; Form form = new Form ( ) ; form . param ( "module" , "AIDRAnalytics" ) ; form . param ( "code" , code ) ; form . param ( "description" , errorMsg ) ; clientResponse = webResource . request ( ) . post ( Entity . entity ( form , MediaType . APPLICATION_FORM_URLENCODED ) , Response . class ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Error during analytics: " + clientResponse ) ; } }
public void test() { if ( s_logger . isDebugEnabled ( ) ) { s_logger . debug ( log ( seq , "Unable to move " + req . toString ( ) ) ) ; } }
public void test() { try { sender . sendHeartbeat ( ) ; } catch ( Throwable e ) { LOG . error ( "Failed to send heartbeat to {}" , sender , e ) ; } }
public void test() { if ( f . exists ( ) ) { FileUtils . forceDelete ( f ) ; logger . debug ( "Removed file {}" , f . getAbsolutePath ( ) ) ; } }
public void test() { if ( logger . isTraceEnabled ( LogMarker . SERIALIZER_VERBOSE ) ) { logger . trace ( LogMarker . SERIALIZER_VERBOSE , "Serialize {}" , this ) ; } }
public void test() { try { activeMQComponent . start ( ) ; } catch ( Exception e ) { logger . error ( "Failed to start ActiveMQComponent" , e ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { if ( channelFuture . isSuccess ( ) ) { LOG . debug ( "Successfully connected to TSO [{}]" , tsoAddress ) ; } else { LOG . error ( "Failed connection attempt to TSO [{}] failed. Channel {}" , tsoAddress , channelFuture . channel ( ) ) ; fsm . sendEvent ( new ErrorEvent ( new ConnectionException ( ) ) ) ; } }
public void test() { if ( channelFuture . isSuccess ( ) ) { LOG . info ( "Connection to TSO [{}] established. Channel {}" , tsoAddress , channelFuture . channel ( ) ) ; } else { LOG . error ( "Connection to TSO [{}] failed" , tsoAddress ) ; fsm . sendEvent ( new ErrorEvent ( new ConnectionException ( ) ) ) ; } }
@ ShutdownHandler ( phase = Phase . INBOUND_EVENT_CONNECTORS ) @ Override public CompletableFuture < Void > shutdownAsync ( ) { logger . info ( "Shutting down coordinator" ) ; return coordinator . stop ( ) ; }
private void prepareKeyShareLength ( ) { entry . setPublicKeyLength ( entry . getPublicKey ( ) . getValue ( ) . length ) ; LOGGER . debug ( "KeyShareLength: " + entry . getPublicKey ( ) . getValue ( ) ) ; }
@ Override public void evaluate ( ) throws Throwable { URL composeYml = getClass ( ) . getResource ( REDIS_COMPOSE_YML ) ; assertThat ( composeYml ) . as ( "Cannot load resource " + REDIS_COMPOSE_YML ) . isNotNull ( ) ; redisCluster = new DockerComposeContainer < > ( "acceptance" , new File ( composeYml . getFile ( ) ) ) ; code_block = ForStatement ; redisCluster . withLocalCompose ( true ) ; redisCluster . waitingFor ( "redis-cluster-init_1" , Wait . forLogMessage ( ".*Ready to accept connections.*" , 1 ) ) ; redisCluster . start ( ) ; int port = redisCluster . getServicePort ( "redis-node-0" , REDIS_PORT ) ; Jedis jedis = new Jedis ( "localhost" , port ) ; List < ClusterNode > nodes = ClusterNodes . parseClusterNodes ( jedis . clusterNodes ( ) ) . getNodes ( ) ; nodes . forEach ( logger :: info ) ; logger . info ( "Connected to " + nodes ) ; assertThat ( nodes . stream ( ) . mapToInt ( x -> x . primary ? 1 : 0 ) . sum ( ) ) . as ( "Incorrect primary node count" ) . isEqualTo ( 3 ) ; Map < HostPort , HostPort > translationMappings = new HashMap < > ( ) ; List < RedisProxy > proxies = new ArrayList < > ( ) ; code_block = ForStatement ; proxies . forEach ( p -> p . configure ( translationMappings ) ) ; code_block = TryStatement ;  }
@ DisplayName ( "Gather all derived tables, export data up to height = 8000," + " delete rows up to height = 8000, import data back into db table" ) @ Test void testExportAndImportData ( ) { DirProvider dirProvider = mock ( DirProvider . class ) ; doReturn ( temporaryFolderExtension . newFolder ( "csvExport" ) . toPath ( ) ) . when ( dirProvider ) . getDataExportDir ( ) ; Set < String > excludeColumnNames = Set . of ( "DB_ID" , "LATEST" ) ; Collection < DerivedTableInterface > result = registry . getDerivedTables ( ) ; assertNotNull ( result ) ; int targetHeight = Integer . MAX_VALUE ; log . debug ( "Extracting table from {}" , targetHeight ) ; result . forEach ( item code_block = LoopStatement ; ) ; log . debug ( "Processed Tables = {}" , result ) ; }
public void test() { if ( minMaxValue . getCount ( ) > 0 ) { do code_block = "" ; while ( processedCount > 0 ) ; assertEquals ( minMaxValue . getCount ( ) , totalCount ) ; int deletedCount = dropDataByName ( minDbValue , maxDbValue , item . toString ( ) ) ; assertEquals ( minMaxValue . getCount ( ) , deletedCount ) ; log . debug ( "Table = {}, deleted = {}" , item . toString ( ) , deletedCount ) ; int imported = importCsv ( item . toString ( ) , batchLimit , dirProvider . getDataExportDir ( ) ) ; log . debug ( "Table = {}, imported rows = {}" , item . toString ( ) , imported ) ; assertEquals ( minMaxValue . getCount ( ) , imported , "incorrect value to '" + item . toString ( ) + "'" ) ; } }
public void test() { if ( minMaxValue . getCount ( ) > 0 ) { do code_block = "" ; while ( processedCount > 0 ) ; log . debug ( "Table = {}, exported rows = {}" , item . toString ( ) , totalCount ) ; assertEquals ( minMaxValue . getCount ( ) , totalCount ) ; int deletedCount = dropDataByName ( minDbValue , maxDbValue , item . toString ( ) ) ; assertEquals ( minMaxValue . getCount ( ) , deletedCount ) ; log . debug ( "Table = {}" , imported ) ; int imported = importCsv ( item . toString ( ) , batchLimit , dirProvider . getDataExportDir ( ) ) ; assertEquals ( minMaxValue . getCount ( ) , imported , "incorrect value to '" + item . toString ( ) + "'" ) ; } }
public void test() { try ( Connection con = extension . getDatabaseManager ( ) . getDataSource ( ) . getConnection ( ) ; PreparedStatement pstmt = con . prepareStatement ( "select * from " + item . toString ( ) + " where db_id BETWEEN ? and  ? limit ?" ) ; CsvWriter csvWriter = new CsvWriterImpl ( dirProvider . getDataExportDir ( ) , excludeColumnNames , translator ) ; ) { csvWriter . setOptions ( "fieldDelimiter=" ) ; MinMaxValue minMaxValue = item . getMinMaxValue ( targetHeight ) ; minDbValue = minMaxValue . getMin ( ) ; maxDbValue = minMaxValue . getMax ( ) ; log . debug ( "Table = {}" , minMaxValue ) ; assertTrue ( minMaxValue . getMax ( ) >= 0 ) ; log . debug ( "Table = {}, Min/Max = {} at height = {}" , item . toString ( ) , minMaxValue , targetHeight ) ; code_block = IfStatement ; } catch ( Exception e ) { Throwables . throwIfUnchecked ( e ) ; throw new RuntimeException ( e ) ; } }
@ DisplayName ( "Gather all derived tables, export data up to height = 8000," + " delete rows up to height = 8000, import data back into db table" ) @ Test void testExportAndImportData ( ) { DirProvider dirProvider = mock ( DirProvider . class ) ; doReturn ( temporaryFolderExtension . newFolder ( "csvExport" ) . toPath ( ) ) . when ( dirProvider ) . getDataExportDir ( ) ; Set < String > excludeColumnNames = Set . of ( "DB_ID" , "LATEST" ) ; Collection < DerivedTableInterface > result = registry . getDerivedTables ( ) ; assertNotNull ( result ) ; log . debug ( "Processing [{}] tables" , result . size ( ) ) ; int targetHeight = Integer . MAX_VALUE ; log . debug ( "Finished loading [{}] tables" , targetHeight ) ; result . forEach ( item code_block = LoopStatement ; ) ; }
public void test() { try { java . util . List < com . liferay . commerce . inventory . model . CommerceInventoryWarehouseItem > returnValue = CommerceInventoryWarehouseItemServiceUtil . getCommerceInventoryWarehouseItems ( companyId , sku , start , end ) ; return com . liferay . commerce . inventory . model . CommerceInventoryWarehouseItemSoap . toSoapModels ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( jmsTaskManagerState == STATE_PAUSED ) { LOG . debug ( "Already paused" ) ; return ; } }
public void test() { switch ( cacheLevel ) { case JMSConstants . CACHE_NONE : log . debug ( "No JMS Connection will be cached and shared between *all* " + "poller task invocations" ) ; break ; case JMSConstants . CACHE_CONNECTION : log . debug ( "Only the JMS Connection will be cached and shared between *all* " + "poller task invocations" ) ; break ; case JMSConstants . CACHE_SESSION : log . debug ( "The JMS Connection and Session will be cached and shared between " + "successive poller task invocations" ) ; break ; case JMSConstants . CACHE_CONSUMER : log . debug ( "The JMS Connection, Session and MessageConsumer will be cached and " + "shared between successive poller task invocations" ) ; break ; code_block = "" ; } }
public void test() { switch ( cacheLevel ) { case JMSConstants . CACHE_NONE : log . debug ( "No JMS resources will be cached/shared between poller " + "worker tasks of " + jmsConsumerName ) ; break ; case JMSConstants . CACHE_CONNECTION : log . debug ( "The JMS Connection will be cached/shared between poller " + "worker tasks of " + jmsConsumerName ) ; break ; case JMSConstants . CACHE_SESSION : log . debug ( "The JMS Connection and Session will be cached and shared between " + "successive poller task invocations" ) ; break ; case JMSConstants . CACHE_CONSUMER : log . debug ( "The JMS Connection, Session and MessageConsumer will be cached and " + "shared between successive poller task invocations" ) ; break ; code_block = "" ; } }
public void test() { switch ( cacheLevel ) { case JMSConstants . CACHE_NONE : log . debug ( "No JMS resources will be cached/shared between poller " + "worker tasks of " + jmsConsumerName ) ; break ; case JMSConstants . CACHE_CONNECTION : log . debug ( "Only the JMS Connection will be cached and shared between *all* " + "poller task invocations" ) ; break ; case JMSConstants . CACHE_SESSION : log . debug ( "The JMS Connection, Session and Session will be cached and " + "shared between successive pollers task invocations" ) ; break ; case JMSConstants . CACHE_CONSUMER : log . debug ( "The JMS Connection, Session and MessageConsumer will be cached and " + "shared between successive poller task invocations" ) ; break ; code_block = "" ; } }
public void test() { switch ( cacheLevel ) { case JMSConstants . CACHE_NONE : log . debug ( "No JMS resources will be cached/shared between poller " + "worker tasks of " + jmsConsumerName ) ; break ; case JMSConstants . CACHE_CONNECTION : log . debug ( "Only the JMS Connection will be cached and shared between *all* " + "poller task invocations" ) ; break ; case JMSConstants . CACHE_SESSION : log . debug ( "The JMS Connection and Session will be cached and shared between " + "successive poller task invocations" ) ; break ; case JMSConstants . CACHE_CONSUMER : log . debug ( "The JMS Connection and Session will be cached and shared between " + "successive poller task invocations" ) ; break ; code_block = "" ; } }
public synchronized void start ( ) { log . info ( "Starting JMS TaskManager" ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = SwitchStatement ; code_block = ForStatement ; jmsTaskManagerState = STATE_STARTED ; }
public void test() { try { addPortletTitleAddJournalArticleMenuItems ( menuItems , themeDisplay , portletRequest ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; } }
public void test() { try { return new SimpleAccount ( token . getPrincipal ( ) , token . getCredentials ( ) , "CloudSession" ) ; } catch ( Throwable t ) { logger . log ( t ) ; } }
public void test() { try { code_block = IfStatement ; throw new AuthenticationException ( "Unable to authenticate token" ) ; } catch ( UnknownUserException ex ) { LOG . warn ( "Authentication failed. Message: {}" , ex . getMessage ( ) ) ; throw new AuthenticationException ( ex . getMessage ( ) ) ; } catch ( UserBlockedException ex ) { LOG . warn ( "User blocked while blocked" , ex ) ; throw new AuthenticationException ( ex . getMessage ( ) ) ; } catch ( EmailNotConfirmedException ex ) { LOG . warn ( "Authentication failed. Message: {}" , ex . getMessage ( ) ) ; throw new AuthenticationException ( "EmailNotConfirmed" ) ; } catch ( InsufficientBucketTokensException ex ) { LOG . info ( "Insufficient bucket tokens: {}" , ex . getMessage ( ) ) ; throw new AuthenticationException ( ex . getMessage ( ) ) ; } catch ( NullPointerException npe ) { LOG . warn ( "NullPointer" , npe ) ; throw new AuthenticationException ( npe . getMessage ( ) ) ; } catch ( Throwable t ) { LOG . warn ( "Throwable" , t ) ; } }
public void test() { try { code_block = IfStatement ; throw new AuthenticationException ( "Unable to authenticate token" ) ; } catch ( UnknownUserException ex ) { LOG . warn ( "Authentication failed. Message: {}" , ex . getMessage ( ) ) ; throw new AuthenticationException ( ex . getMessage ( ) ) ; } catch ( UserBlockedException ex ) { LOG . warn ( "Blocked user {}" , ex ) ; throw new AuthenticationException ( ex . getMessage ( ) ) ; } catch ( EmailNotConfirmedException ex ) { LOG . warn ( "Authentication failed. Message: {}" , ex . getMessage ( ) ) ; throw new AuthenticationException ( "EmailNotConfirmed" ) ; } catch ( InsufficientBucketTokensException ex ) { LOG . warn ( "Authentication failed. Message: {}" , ex . getMessage ( ) ) ; throw new AuthenticationException ( ex . getMessage ( ) ) ; } catch ( NullPointerException npe ) { LOG . warn ( "NullPointer" , npe ) ; throw new AuthenticationException ( npe . getMessage ( ) ) ; } catch ( Throwable t ) { LOG . warn ( "Throwable" , t ) ; } }
public void test() { try { code_block = IfStatement ; throw new AuthenticationException ( "Unable to authenticate token" ) ; } catch ( UnknownUserException ex ) { LOG . warn ( "Authentication failed. Message: {}" , ex . getMessage ( ) ) ; throw new AuthenticationException ( ex . getMessage ( ) ) ; } catch ( UserBlockedException ex ) { LOG . warn ( "Blocked user {}" , ex ) ; throw new AuthenticationException ( ex . getMessage ( ) ) ; } catch ( EmailNotConfirmedException ex ) { LOG . warn ( "Authentication failed. Message: {}" , ex . getMessage ( ) ) ; throw new AuthenticationException ( "EmailNotConfirmed" ) ; } catch ( InsufficientBucketTokensException ex ) { LOG . info ( "Insufficient bucket tokens: {}" , ex . getMessage ( ) ) ; throw new AuthenticationException ( ex . getMessage ( ) ) ; } catch ( NullPointerException npe ) { LOG . warn ( "Null pointer exception" , npe ) ; throw new AuthenticationException ( npe . getMessage ( ) ) ; } catch ( Throwable t ) { LOG . warn ( "Throwable" , t ) ; } }
public void test() { try { code_block = IfStatement ; throw new AuthenticationException ( "Unable to authenticate token" ) ; } catch ( UnknownUserException ex ) { LOG . warn ( "Authentication failed. Message: {}" , ex . getMessage ( ) ) ; throw new AuthenticationException ( ex . getMessage ( ) ) ; } catch ( UserBlockedException ex ) { LOG . warn ( "Blocked user {}" , ex ) ; throw new AuthenticationException ( ex . getMessage ( ) ) ; } catch ( EmailNotConfirmedException ex ) { LOG . warn ( "Authentication failed. Message: {}" , ex . getMessage ( ) ) ; throw new AuthenticationException ( "EmailNotConfirmed" ) ; } catch ( InsufficientBucketTokensException ex ) { LOG . info ( "Insufficient bucket tokens: {}" , ex . getMessage ( ) ) ; throw new AuthenticationException ( ex . getMessage ( ) ) ; } catch ( NullPointerException npe ) { LOG . warn ( "NullPointer" , npe ) ; throw new AuthenticationException ( npe . getMessage ( ) ) ; } catch ( Throwable t ) { LOG . warn ( "Unknown error" , t ) ; } }
@ ExceptionHandler ( OAuthProblemException . class ) public ModelAndView handleOAuthProblemException ( OAuthProblemException ex ) { logger . debug ( ex . getMessage ( ) , ex ) ; ModelAndView modelAndView = new ModelAndView ( "oauth_error" ) ; modelAndView . addObject ( "exception" , ex ) ; return modelAndView ; }
public void test() { try { Constructor < ? extends Component > con = componentClass . getConstructor ( new Class [ ] code_block = "" ; ) ; Component comp = con . newInstance ( MockPageWithLinkAndComponent . COMPONENT_ID ) ; page . replace ( comp ) ; comp . setOutputMarkupId ( true ) ; target . add ( comp ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( LOGGER . isTraceEnabled ( ) ) { LOGGER . trace ( "Weak listener list status:{}" , System . lineSeparator ( ) ) ; } }
public void test() { { LOGGER . error ( throwable . getMessage ( ) ) ; latch . countDown ( ) ; } }
public void test() { if ( log ) { LOGGER . info ( "Exiting reader" ) ; } }
public void test() { if ( log ) { LOGGER . info ( "Exiting reader" ) ; } }
public void test() { if ( log ) { LOGGER . info ( "Exiting reader" ) ; } }
public void test() { try { ConnectionFactory factory = new ActiveMQConnectionFactory ( USER_NAME , PASSWORD , brokenUrl ) ; connection = factory . createConnection ( ) ; connection . start ( ) ; connection . getMetaData ( ) . getJMSVersion ( ) ; connection . close ( ) ; LOG . info ( "JMS Connection initialized" ) ; } catch ( Exception ex ) { code_block = TryStatement ;  return FAIL ; } }
public void test() { try { session . close ( ) ; connection . close ( ) ; } catch ( JMSException e ) { logger . error ( e . getMessage ( ) , e ) ; } }
@ Override public < T > Page < T > queryForPage ( ViewQuery query , PageRequest pr , Class < T > type ) { Assert . notNull ( query , "query may not be null" ) ; Assert . notNull ( pr , "PageRequest may not be null" ) ; Assert . notNull ( type , "type may not be null" ) ; LOG . debug ( "query: {}" , query ) ; query . dbPath ( dbURI . toString ( ) ) ; LOG . debug ( "startDocId: {}" , pr . getStartKeyDocId ( ) ) ; PageResponseHandler < T > ph = new PageResponseHandler < T > ( pr , type , objectMapper , query . isIgnoreNotFound ( ) ) ; query = PageRequest . applyPagingParameters ( query , pr ) ; return executeQuery ( query , ph ) ; }
@ Override public < T > Page < T > queryForPage ( ViewQuery query , PageRequest pr , Class < T > type ) { Assert . notNull ( query , "query may not be null" ) ; Assert . notNull ( pr , "PageRequest may not be null" ) ; Assert . notNull ( type , "type may not be null" ) ; LOG . debug ( "query: {}" , query ) ; query . dbPath ( dbURI . toString ( ) ) ; LOG . debug ( "startKey: {}" , pr . getStartKey ( ) ) ; PageResponseHandler < T > ph = new PageResponseHandler < T > ( pr , type , objectMapper , query . isIgnoreNotFound ( ) ) ; query = PageRequest . applyPagingParameters ( query , pr ) ; return executeQuery ( query , ph ) ; }
public void test() { if ( ! directory . exists ( ) ) { log . error ( "Cannot load stream definitions from " + directory . getAbsolutePath ( ) + " directory not found" ) ; return streamDefinitions ; } }
public void test() { if ( ! directory . isDirectory ( ) ) { log . error ( "Cannot load stream definitions from " + directory . getAbsolutePath ( ) + " not a directory" ) ; return streamDefinitions ; } }
public void test() { try { bufferedReader = new BufferedReader ( new FileReader ( fileEntry ) ) ; String line ; code_block = WhileStatement ; StreamDefinition streamDefinition = EventDefinitionConverterUtils . convertFromJson ( stringBuilder . toString ( ) . trim ( ) ) ; streamDefinitions . put ( streamDefinition . getStreamId ( ) , streamDefinition ) ; } catch ( FileNotFoundException e ) { log . error ( "Error in reading file " + fileEntry . getName ( ) , e ) ; } catch ( IOException e ) { log . error ( "Error in reading file " + fileEntry . getName ( ) , e ) ; } catch ( MalformedStreamDefinitionException e ) { log . error ( "Error in converting Stream definition " + e . getMessage ( ) , e ) ; } finally { code_block = TryStatement ;  } }
public void test() { try { code_block = IfStatement ; } catch ( IOException e ) { LOGGER . error ( "Unable to retrieve file" , e ) ; } }
private void assertEqualXMLConfigurations ( String expectedXMLConfiguration , String generatedXMLConfiguration ) throws IOException , SAXException , TransformerException , ParserConfigurationException { log . trace ( "Expected XML configuration:\n" + expectedXMLConfiguration ) ; log . trace ( "Generated configuration:\n" + generatedXMLConfiguration ) ; Assert . assertTrue ( "Generated configuration XML and expected configuration XML must be equal" , XmlHelper . compareXMLStrings ( generatedXMLConfiguration , expectedXMLConfiguration ) ) ; }
private void assertEqualXMLConfigurations ( String expectedXMLConfiguration , String generatedXMLConfiguration ) throws IOException , SAXException , TransformerException , ParserConfigurationException { log . trace ( "Produced XML configuration:\n" + generatedXMLConfiguration ) ; log . trace ( "Generated configuration XML and expected configuration XML must be equal" ) ; Assert . assertTrue ( "Generated configuration XML and expected configuration XML must be equal" , XmlHelper . compareXMLStrings ( generatedXMLConfiguration , expectedXMLConfiguration ) ) ; }
public void test() { if ( session == null ) { return ; } }
public void test() { try { CloseableHttpResponse response = httpClient . execute ( postRequest ) ; HttpStatus httpStatus = HttpStatus . valueOf ( response . getStatusLine ( ) . getStatusCode ( ) ) ; code_block = IfStatement ; } catch ( IOException e ) { LOGGER . error ( "Http post request failed" , e ) ; } finally { postRequest . releaseConnection ( ) ; } }
@ Override public void onTimeOut ( ) { LOG . error ( "Task timeout" ) ; _statusUpdateUtil . logError ( _originalMessage , SchedulerAsyncCallback . class , "Task timeout" , _manager ) ; addSummary ( _resultSummaryMap , _originalMessage , _manager , true ) ; }
public void createIdpEventInitCustomer ( final IdentityProvider sourceEvent ) { LOGGER . info ( "Create IdentityProvider" ) ; create ( getCurrentProofTenantIdentifier ( ) , sourceEvent . getIdentifier ( ) , MongoDbCollections . PROVIDERS , EventType . EXT_VITAMUI_CREATE_IDP , converters . getIdpConverter ( ) . convertToLogbook ( converters . getIdpConverter ( ) . convertEntityToDto ( sourceEvent ) ) ) ; }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { long percentage = Math . round ( bytes / ( ( double ) size ) * 100.0 ) ; LOGGER . debug ( String . format ( "%s: %d" , percentage , percent ) ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { time += System . currentTimeMillis ( ) ; double kBps = ( bytes / 1024.0 ) / ( time / 1000.0 ) ; LOGGER . debug ( "bps = {}, kBps = {}" , kBps , kBps ) ; } }
public void test() { if ( job . isSnapshot ( ) && CONFIG_COUNT_SNAPSHOT != null && CONFIG_COUNT_SNAPSHOT > 0 && job . getDomainConfigurationMap ( ) . size ( ) >= CONFIG_COUNT_SNAPSHOT ) { LOG . debug ( "Job " + job . getId ( ) + " has been snapshot. Not running." ) ; return false ; } }
public void test() { if ( NumberUtils . compareInf ( cfg . getMaxObjects ( ) , forceMaxObjectsPerDomain ) < 0 || ( job . isConfigurationSetsObjectLimit ( ) && NumberUtils . compareInf ( cfg . getMaxObjects ( ) , forceMaxObjectsPerDomain ) != 0 ) ) { LOG . info ( "The maximum number of " + forceMaxObjectsPerDomain + " is set to false because the maximum number of " + forceMaxObjectsPerDomain ) ; return false ; } }
public void test() { if ( NumberUtils . compareInf ( cfg . getMaxBytes ( ) , forceMaxBytesPerDomain ) < 0 || ( job . isConfigurationSetsByteLimit ( ) && NumberUtils . compareInf ( cfg . getMaxBytes ( ) , forceMaxBytesPerDomain ) != 0 ) ) { LOG . info ( "Not setting " + cfg . getMaxBytes ( ) + " to " + forceMaxBytesPerDomain ) ; return false ; } }
public void test() { if ( ( totalCountObjects > 0 ) && ( ( expectation + totalCountObjects ) > LIM_MAX_TOTAL_SIZE ) ) { log . warn ( "Hook " + ( ( expectation + totalCountObjects > 0 ) + " exceeded" ) ; return false ; } }
public void test() { if ( relDiff > LIM_MAX_REL_SIZE ) { logger . error ( "Reloading is too large: {}" , relDiff ) ; return false ; } }
public void test() { try { ResourceMonitor resourceMonitor = getOrCreateResourceMonitor ( resourceName ) ; code_block = IfStatement ; } catch ( Exception e ) { log . error ( "Failed to create resource monitor " , e ) ; } }
public void test() { if ( preCheck == ResponseCodeEnum . RECEIPT_NOT_FOUND ) { logger . error ( "getReceivedException:" + preCheck ) ; return null ; } else-if ( code == ResponseCodeEnum . UNKNOWN ) { Thread . sleep ( 200 ) ; } else-if ( code == ResponseCodeEnum . SUCCESS ) { return response . getTransactionGetReceipt ( ) ; } else { return response . getTransactionGetReceipt ( ) ; } }
public void test() { if ( maybeLoadBalancerPort . isPresent ( ) ) { String upstream = String . format ( "%s:%d" , task . getHostname ( ) , maybeLoadBalancerPort . get ( ) ) ; Optional < String > group = loadBalancerUpstreamGroup ; code_block = IfStatement ; upstreams . add ( new LoadBalancerUpstream ( upstream , group . orElse ( "default" ) , task . getRackId ( ) ) ) ; } else { LOG . warn ( "Unable to find upstream slave for task {}" , task ) ; } }
public void test() { for ( Pair < INDArray , String > p : NDArrayCreationUtil . getAllTestMatricesWithShape ( origShape [ 0 ] , origShape [ 1 ] , 12345 , DataType . DOUBLE ) ) { INDArray inArr = p . getFirst ( ) . muli ( 100 ) ; SameDiff sd = SameDiff . create ( ) ; SDVariable in = sd . var ( "in" , inArr ) ; SDVariable expand = sd . expandDims ( in , i ) ; SDVariable stdev = sd . standardDeviation ( "out" , expand , true ) ; log . info ( msg ) ; Map < String , INDArray > m = sd . outputAll ( null ) ; INDArray expOut = in . getArr ( ) . std ( true ) ; assertArrayEquals ( expExpandShape , m . get ( expand . name ( ) ) . shape ( ) ) ; INDArray expExpand = inArr . dup ( 'c' ) . reshape ( expExpandShape ) ; String msg = "expandDim=" + i + ", source=" + p . getSecond ( ) ; TestCase tc = new TestCase ( sd ) ; tc . testName ( msg ) . expectedOutput ( "out" , expOut ) . expectedOutput ( expand . name ( ) , expExpand ) ; String error = OpValidation . validate ( tc ) ; code_block = IfStatement ; } }
public void test() { if ( consumerLog == null ) { log . warn ( "log is null for topic: {}" , topic ) ; return ; } }
public void test() { switch ( consumeFromWhere ) { case UNKNOWN : break ; case EARLIEST : consumeQueueManager . update ( subject , group , bound . getMinOffset ( ) ) ; break ; case LATEST : consumeQueueManager . update ( subject , group , bound . getMaxOffset ( ) ) ; break ; } }
public void test() { if ( newFile . lastModified ( ) < entry . getTime ( ) ) { log . info ( "Ignoring file " + entry . getPath ( ) + " to " + entry . getTime ( ) ) ; overwrite = true ; } }
public void test() { if ( _cache == null ) { _cache = CacheBuilder . newBuilder ( ) . expireAfterWrite ( timeout , TimeUnit . SECONDS ) . build ( ) ; _log . info ( "done" ) ; _cache = CacheBuilder . newBuilder ( ) . expireAfterWrite ( timeout , TimeUnit . SECONDS ) . build ( ) ; _log . info ( "done" ) ; } }
public void test() { if ( appendEntryLimiter . tryAcquire ( positions . getRight ( ) ) ) { final var listener = new Listener ( this , positions . getRight ( ) , ActorClock . currentTimeMillis ( ) ) ; logStorage . append ( positions . getLeft ( ) , positions . getRight ( ) , copiedBuffer , listener ) ; blockPeek . markCompleted ( ) ; } else { log . warn ( "appendBackpressureMetrics. deferred" ) ; appendBackpressureMetrics . deferred ( ) ; } }
@ Override public Object invoke ( Object object , Method method , Object [ ] args ) throws Throwable { LOG . debug ( "Invoking " + object . getClass ( ) . getName ( ) + "" + method . getName ( ) + " for object " + object . getClass ( ) . getName ( ) ) ; LOG . debug ( object . getClass ( ) . getName ( ) + "" + method . getName ( ) + "()" ) ; return "RESULT" ; }
@ Override public Object invoke ( Object object , Method method , Object [ ] args ) throws Throwable { LOG . debug ( "invoke!!!" ) ; LOG . trace ( object . toString ( ) ) ; return "RESULT" ; }
public void test() { try { if ( logger . isTraceEnabled ( ) ) logger . trace ( "Responding with error: {}, v={} ON {}" , error . getMessage ( ) , request . connection ( ) . getVersion ( ) , Thread . currentThread ( ) . getName ( ) ) ; UnexpectedChannelExceptionHandler handler = new UnexpectedChannelExceptionHandler ( ctx . channel ( ) , true ) ; if ( error instanceof ExecutionException ) error = error . getCause ( ) ; if ( error instanceof CompletionException ) error = error . getCause ( ) ; flush ( new Message . Dispatcher . FlushItem ( ctx , ErrorMessage . fromException ( error , handler ) . setStreamId ( request . getStreamId ( ) ) , request . getSourceFrameBodySizeInBytes ( ) , this ) ) ; } catch ( Throwable t ) { logger . warn ( "Failed toResponding" , t ) ; } finally { ClientWarn . instance . resetWarnings ( ) ; } }
public void test() { try { configEntry = directoryService . getAdminSession ( ) . lookup ( defaultSearchDn ) ; } catch ( LdapException e ) { logger . debug ( "Could not lookup LDAP config entry" , e ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { Entry configEntry = null ; code_block = TryStatement ;  code_block = IfStatement ; code_block = IfStatement ; } catch ( ClassNotFoundException e ) { String msg = I18n . err ( I18n . ERR_293 , name ) ; LOG . error ( msg , e ) ; throw new ClassNotFoundException ( msg ) ; } catch ( Exception e ) { String msg = I18n . err ( I18n . ERR_70 , name ) ; LOG . error ( msg , e ) ; throw new ClassNotFoundException ( msg ) ; } }
public void test() { try { Entry configEntry = null ; code_block = TryStatement ;  code_block = IfStatement ; code_block = IfStatement ; } catch ( ClassNotFoundException e ) { String msg = I18n . err ( I18n . ERR_293 , name ) ; LOG . debug ( msg ) ; throw new ClassNotFoundException ( msg ) ; } catch ( Exception e ) { String msg = I18n . err ( I18n . ERR_70 , name ) ; LOG . error ( msg , e ) ; throw new ClassNotFoundException ( msg ) ; } }
public void test() { try { code_block = ForStatement ; } catch ( ConfigurationException e ) { log . error ( "Cannot read the configuration file" , e ) ; } }
@ Override public void stop ( ) { DestinationDispatcherThread < T > dispatcherThread = mDispatcherThread ; mDispatcherThread = null ; code_block = IfStatement ; closeFile ( ) ; mLogger . debug ( "<== LocalFileLogBuffer.stop()" ) ; mLogger . debug ( "== LocalFileLogBuffer.stop()" ) ; }
public void test() { try { dispatcherThread . join ( ) ; } catch ( InterruptedException e ) { logger . error ( "Interrupted while stopping the dispatcher thread" , e ) ; } }
@ Override public void stop ( ) { mLogger . debug ( "==> LocalFileLogBuffer.stop()" ) ; DestinationDispatcherThread < T > dispatcherThread = mDispatcherThread ; mDispatcherThread = null ; code_block = IfStatement ; closeFile ( ) ; mLogger . debug ( "<== LocalFileLogBuffer.stop()" ) ; }
@ Test public void testLogInitialize ( ) throws Exception { log = new JavaLogger ( ) ; ( ( JavaLogger ) log ) . setWorkDirectory ( U . defaultWorkDirectory ( ) ) ; ( ( LoggerNodeIdAware ) log ) . setNodeId ( UUID . fromString ( "00000000-1111-22-3333-444444444444" ) ) ; System . out . println ( log . toString ( ) ) ; assertTrue ( log . toString ( ) . contains ( "JavaLogger" ) ) ; assertTrue ( log . toString ( ) . contains ( JavaLogger . DFLT_CONFIG_PATH ) ) ; if ( log . isDebugEnabled ( ) ) log . debug ( "This is 'debug' message." ) ; assert log . isInfoEnabled ( ) ; log . info ( "This is 'info' message." ) ; log . warning ( "This is 'warning' message." ) ; log . warning ( "This is 'warning' message." , new Exception ( "It's a test warning exception" ) ) ; log . error ( "This is 'error' message." ) ; log . error ( "This is 'error' message." , new Exception ( "It's a test error exception" ) ) ; assert log . getLogger ( JavaLoggerTest . class . getName ( ) ) instanceof JavaLogger ; assert log . fileName ( ) != null ; assert ! log . fileName ( ) . contains ( "%" ) ; }
@ Test public void testLogInitialize ( ) throws Exception { log = new JavaLogger ( ) ; ( ( JavaLogger ) log ) . setWorkDirectory ( U . defaultWorkDirectory ( ) ) ; ( ( LoggerNodeIdAware ) log ) . setNodeId ( UUID . fromString ( "00000000-1111-22-3333-444444444444" ) ) ; System . out . println ( log . toString ( ) ) ; assertTrue ( log . toString ( ) . contains ( "JavaLogger" ) ) ; assertTrue ( log . toString ( ) . contains ( JavaLogger . DFLT_CONFIG_PATH ) ) ; if ( log . isDebugEnabled ( ) ) log . debug ( "This is 'debug' message." ) ; assert log . isInfoEnabled ( ) ; log . info ( "This is 'info' message." ) ; log . warning ( "This is 'warning' message." ) ; log . warning ( "This is 'warning' message." , new Exception ( "It's a test warning exception" ) ) ; log . error ( "This is 'error' message." ) ; log . error ( "This is 'error' message." , new Exception ( "It's a test error exception" ) ) ; assert log . getLogger ( JavaLoggerTest . class . getName ( ) ) instanceof JavaLogger ; assert log . fileName ( ) != null ; assert ! log . fileName ( ) . contains ( "%" ) ; }
@ Test public void testLogInitialize ( ) throws Exception { log = new JavaLogger ( ) ; ( ( JavaLogger ) log ) . setWorkDirectory ( U . defaultWorkDirectory ( ) ) ; ( ( LoggerNodeIdAware ) log ) . setNodeId ( UUID . fromString ( "00000000-1111-22-3333-444444444444" ) ) ; System . out . println ( log . toString ( ) ) ; assertTrue ( log . toString ( ) . contains ( "JavaLogger" ) ) ; assertTrue ( log . toString ( ) . contains ( JavaLogger . DFLT_CONFIG_PATH ) ) ; if ( log . isDebugEnabled ( ) ) log . debug ( "This is 'debug' message." ) ; assert log . isInfoEnabled ( ) ; log . info ( "This is 'info' message." ) ; log . warning ( "This is 'warning' message." ) ; log . warning ( "This is 'warning' message." , new Exception ( "It's a test warning exception" ) ) ; log . error ( "This is 'error' message." ) ; log . error ( "This is 'error' message." , new Exception ( "It's a test error exception" ) ) ; assert log . getLogger ( JavaLoggerTest . class . getName ( ) ) instanceof JavaLogger ; assert log . fileName ( ) != null ; assert ! log . fileName ( ) . contains ( "%" ) ; }
public void test() { try { IOUtils . copy ( stream . getInputStream ( ) , out ) ; } catch ( IOException ex ) { logger . error ( ex . getMessage ( ) ) ; throw ex ; } }
public void test() { try { long requestTimestamp = Instant . now ( ) . getMillis ( ) ; client . ingestHL7v2Message ( hl7v2Store . get ( ) , model ) ; messageIngestLatencyMs . update ( Instant . now ( ) . getMillis ( ) - requestTimestamp ) ; } catch ( Exception e ) { failedMessageWrites . inc ( ) ; HealthcareIOError < HL7v2Message > err = HealthcareIOError . of ( msg , e ) ; LOG . warn ( String . format ( "%s %s" , err . getErrorMessage ( ) , err . getStackTrace ( ) ) ) ; LOG . warn ( String . format ( "%s %s" , msg . getStackTrace ( ) ) ) ; context . output ( err ) ; } }
public void test() { try { long requestTimestamp = Instant . now ( ) . getMillis ( ) ; client . ingestHL7v2Message ( hl7v2Store . get ( ) , model ) ; messageIngestLatencyMs . update ( Instant . now ( ) . getMillis ( ) - requestTimestamp ) ; LOG . info ( String . format ( "Successfully ingested HL7v2 message %s" , msg ) ) ; } catch ( Exception e ) { failedMessageWrites . inc ( ) ; LOG . warn ( String . format ( "Failed to ingest message Error: %s Stacktrace: %s" , e . getMessage ( ) , Throwables . getStackTraceAsString ( e ) ) ) ; HealthcareIOError < HL7v2Message > err = HealthcareIOError . of ( msg , e ) ; context . output ( err ) ; } }
public void test() { if ( getInitParameters ( ) . isDebug ( ) ) { LOGGER . debug ( "Initializing" ) ; } }
@ Test public void testDenseSearch ( ) throws Exception { EntityManager em = app . getEntityManager ( ) ; assertNotNull ( em ) ; int numEntities = 25 ; float minLatitude = 48.32455f ; float maxLatitude = 48.46481f ; float minLongitude = 9.89561f ; float maxLongitude = 10.0471f ; float latitudeDelta = ( maxLatitude - minLatitude ) / numEntities ; float longitudeDelta = ( maxLongitude - minLongitude ) / numEntities ; code_block = ForStatement ; app . waitForQueueDrainAndRefreshIndex ( ) ; int limit = 8 ; long startTime = System . currentTimeMillis ( ) ; Query query = Query . fromQL ( "location within 1000 of 48.38626, 9.94175" ) ; query . setLimit ( limit ) ; Results results = em . searchCollection ( em . getApplicationRef ( ) , "stores" , query ) ; assertEquals ( 0 , results . size ( ) ) ; long endTime = System . currentTimeMillis ( ) ; logger . info ( "Time: " + endTime - startTime ) ; }
public void test() { if ( iteration % printIterations == 0 ) { double score = model . score ( ) ; logger . debug ( String . format ( "Score %d: %d" , iteration , score ) ) ; } }
public void test() { if ( isNotNullWithoutDefault ) { String sql = String . format ( "ALTER TABLE %s MODIFY %s VARCHAR(40) CHARACTER SET ascii COLLATE ascii_bin" , tableName , col . LAST_IP ) ; LOG . info ( sql ) ; st . execute ( sql ) ; } }
public void test() { if ( isErrorEnabled ( ) ) { FormattingTuple formattingTuple = MessageFormatter . format ( format , argument ) ; _log . error ( formattingTuple . getThrowable ( ) , formattingTuple . getThrowable ( ) ) ; } }
public void test() { if ( logger . isTraceEnabled ( LogMarker . DM_VERBOSE ) ) { logger . trace ( LogMarker . DM_VERBOSE , "Processing {}" , this ) ; } }
public void test() { if ( logger . isTraceEnabled ( LogMarker . DM_VERBOSE ) ) { logger . trace ( LogMarker . DM_VERBOSE , "Processing {}" , this ) ; } }
public void test() { if ( task == null ) { log . warn ( "Task '" + taskId + "' not found" ) ; throw new ObjectRetrievalFailureException ( TaskDefinition . class , taskId ) ; } }
public void test() { try { setterMethod . invoke ( step , value ) ; return true ; } catch ( Exception e ) { LOG . warn ( "failed to set parameter {}={}" , setterMethod . getName ( ) , value , e ) ; } }
public void test() { if ( LOG . isWarnEnabled ( ) ) { LOG . warn ( "Unhandled exception: " , e ) ; } }
public void test() { if ( StringUtils . isBlank ( tokenValue ) ) { logger . info ( "Token value is blank" ) ; unionFailResponse ( servletResponse ) ; return false ; } }
public void test() { try { subject . login ( token ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; unionFailResponse ( servletResponse ) ; return false ; } }
public void test() { try { InputStream fin = VocabDefinitions . class . getClassLoader ( ) . getResourceAsStream ( CF_PARAMETERS ) ; InputStreamReader freader = new InputStreamReader ( fin ) ; cfSet = new HashSet < String > ( ) ; StringBuilder builder = new StringBuilder ( ) ; char [ ] buffer = new char [ 1 ] ; code_block = WhileStatement ; } catch ( Exception ex ) { logger . error ( "Error loading vocabulary files" , ex ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
@ Override public synchronized void start ( ) { logger . info ( "Starting ..." ) ; executorService = Executors . newFixedThreadPool ( threadCount ) ; }
public void test() { try { countryCentrePoints = CountryCentrePoints . getInstance ( alaConfig . getLocationInfoConfig ( ) ) ; stateProvinceCentrePoints = StateProvinceCentrePoints . getInstance ( alaConfig . getLocationInfoConfig ( ) ) ; stateProvinceParser = StateProvinceParser . getInstance ( alaConfig . getLocationInfoConfig ( ) . getStateProvinceNamesFile ( ) ) ; } catch ( Exception e ) { logger . error ( "Error creating state cpus" , e ) ; throw new RuntimeException ( e . getMessage ( ) ) ; } }
public void test() { if ( ! ledgerRootExists ) { LOG . info ( "Ledger path {} doesn't exist" , ledgerRootPath ) ; return true ; } }
public void test() { -> { return false ; } }
public void test() { if ( roBookies != null && ! roBookies . isEmpty ( ) ) { LOG . debug ( "Skip deleting bookies {}" , regionPath ) ; return false ; } }
public void test() { try { Map < Locale , String > nameMap = LocalizationUtil . getLocalizationMap ( nameMapLanguageIds , nameMapValues ) ; Map < Locale , String > descriptionMap = LocalizationUtil . getLocalizationMap ( descriptionMapLanguageIds , descriptionMapValues ) ; com . liferay . dynamic . data . mapping . model . DDMTemplate returnValue = DDMTemplateServiceUtil . updateTemplate ( templateId , classPK , nameMap , descriptionMap , type , mode , language , script , cacheable , serviceContext ) ; return com . liferay . dynamic . data . mapping . model . DDMTemplateSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
@ RequestMapping ( value = "/api/samples/{sampleId}/metadata" , method = RequestMethod . GET ) public ModelMap getSampleMetadata ( @ PathVariable Long sampleId ) { ModelMap modelMap = new ModelMap ( ) ; logger . debug ( "Reading sample metadata for sample " + sampleId ) ; Sample s = sampleService . read ( sampleId ) ; Set < MetadataEntry > metadataForSample = sampleService . getMetadataForSample ( s ) ; SampleMetadataResponse response = buildSampleMetadataResponse ( s , metadataForSample ) ; modelMap . addAttribute ( RESTGenericController . RESOURCE_NAME , response ) ; return modelMap ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { graph . commit ( ) ; } catch ( Exception ex ) { log . error ( "Graph rollback failed." , ex ) ; graphRollback ( ) ; } }
public void test() { try { txId = ( TXId ) context . getArguments ( ) ; } catch ( ClassCastException e ) { LOG . error ( "ArgumentException ignored" , e ) ; throw e ; } }
public void test() { if ( isDebugEnabled ) { logger . debug ( "Closing client with key {}" , key ) ; } }
public void test() { if ( isDebugEnabled ) { logger . debug ( "Closing client with key {}" , key ) ; } }
public void test() { if ( isDebugEnabled ) { logger . debug ( "Closing client with key {}" , key ) ; } }
public void test() { if ( isDebugEnabled ) { logger . debug ( "Closing client with key {}" , key ) ; } }
public void test() { if ( exc instanceof Exception ) { context . runOnContext ( ( v ) code_block = LoopStatement ; ) ; } else { logger . error ( exc . getMessage ( ) , exc ) ; } }
public void test() { try { return Files . exists ( localPath ) && calculateLocalAttachmentHash ( localPath ) . equals ( attachment . getSha1 ( ) ) ? Optional . of ( localPath ) : Optional . empty ( ) ; } catch ( SW360ClientException e ) { log . error ( "Error fetching file: " + e . getMessage ( ) , e ) ; return Optional . empty ( ) ; } }
public void updateConfigGroup ( String clusterName , String groupId , ApiConfigGroup configGroup ) throws AmbariApiException { String confGroup = ApiUtils . objectToJson ( configGroup ) ; logger . debug ( "update config group: " + confGroup ) ; Response response = null ; code_block = TryStatement ;  handleAmbariResponse ( response ) ; }
public void test() { try { attemptSchedule ( taskNode ) ; schedulingAgent . schedule ( taskNode , new LifecycleState ( ) ) ; logger . info ( "Successfully scheduled {} to run every {}" , taskNode , taskNode . getSchedulingPeriod ( ) ) ; } catch ( final Exception e ) { logger . error ( "Failed to schedule {}" , taskNode , e ) ; componentLifeCycleThreadPool . schedule ( this , 30 , TimeUnit . SECONDS ) ; } }
public void test() { if ( sessionId . isEmpty ( ) ) { setUserState ( UserState . Disconnected ) ; logger . trace ( "USER - Disconnected: " + userName + " id: " + userId ) ; lostConnection ( ) ; } else-if ( userState == UserState . Created ) { setUserState ( UserState . Connected ) ; logger . trace ( "USER - created: " + userName + " id: " + userId ) ; } else { setUserState ( UserState . Connected ) ; reconnect ( ) ; logger . trace ( "USER - reconnected: " + userName + " id: " + userId ) ; } }
public void test() { if ( sessionId . isEmpty ( ) ) { setUserState ( UserState . Disconnected ) ; lostConnection ( ) ; logger . trace ( "USER - lost connection: " + userName + " id: " + userId ) ; } else-if ( userState == UserState . Created ) { setUserState ( UserState . Connected ) ; logger . trace ( "USER - connected: " + userName + " id: " + userId ) ; } else { setUserState ( UserState . Connected ) ; reconnect ( ) ; logger . trace ( "USER - reconnected: " + userName + " id: " + userId ) ; } }
public void test() { if ( sessionId . isEmpty ( ) ) { setUserState ( UserState . Disconnected ) ; lostConnection ( ) ; logger . trace ( "USER - lost connection: " + userName + " id: " + userId ) ; } else-if ( userState == UserState . Created ) { setUserState ( UserState . Connected ) ; logger . trace ( "USER - created: " + userName + " id: " + userId ) ; } else { setUserState ( UserState . Connected ) ; logger . trace ( "USER - reconnected: " + userName + " id: " + userId ) ; reconnect ( ) ; } }
public void test() { if ( ind . getMapProtocolVersion ( ) >= 3 ) { this . logger . debug ( "onSendAuthenticationInfoResp_V3" ) ; te = TestEvent . createReceivedEvent ( EventType . SendAuthenticationInfoResp_V3 , ind , sequence ++ ) ; } else { this . logger . debug ( "onSendAuthenticationInfoResp_V2" ) ; te = TestEvent . createReceivedEvent ( EventType . SendAuthenticationInfoResp_V2 , ind , sequence ++ ) ; } }
public void test() { if ( ind . getMapProtocolVersion ( ) >= 3 ) { this . logger . debug ( "onSendAuthenticationInfoResp_V3" ) ; te = TestEvent . createReceivedEvent ( EventType . SendAuthenticationInfoResp_V3 , ind , sequence ++ ) ; } else { this . logger . debug ( "onSendAuthenticationInfoResp_V2" ) ; te = TestEvent . createReceivedEvent ( EventType . SendAuthenticationInfoResp_V2 , ind , sequence ++ ) ; } }
public void onServiceStartedEvent ( ServiceStartedEvent event , ActivityContextInterface aci , EventContext eventContext ) { ServiceID serviceID = event . getService ( ) ; log . info ( "onServiceStartedEvent: event=" + event ) ; SbbStates . setSmscRxSmppServerServiceState ( true ) ; }
public void test() { try { MethodKey methodKey = new MethodKey ( DDMStructureVersionServiceUtil . class , "getStructureVersion" , _getStructureVersionParameterTypes1 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , structureVersionId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . dynamic . data . mapping . model . DDMStructureVersion ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { trackModels ( true ) ; } catch ( Throwable t ) { logger . error ( t . getMessage ( ) , t ) ; } }
public void test() { try { prepare ( 4 ) ; FakeAllocatableAction instance0 = new FakeAllocatableAction ( fao , 0 ) ; instance0 . assignResource ( rs ) ; FakeAllocatableAction instance1 = new FakeAllocatableAction ( fao , 1 ) ; instance1 . assignResource ( rs ) ; FakeAllocatableAction instance2 = new FakeAllocatableAction ( fao , 2 ) ; instance2 . assignResource ( rs ) ; FakeAllocatableAction instance3 = new FakeAllocatableAction ( fao , 3 ) ; instance3 . assignResource ( rs ) ; instance0 . tryToLaunch ( ) ; completed ( instance0 ) ; checkExecutions ( new int [ ] code_block = "" ; ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; fail ( e . getMessage ( ) ) ; } }
public void test() { try { IEntityManager entityManager = this . getEntityManager ( ) ; Map < String , AttributeInterface > attributeTypes = entityManager . getEntityAttributePrototypes ( ) ; Iterator < AttributeInterface > attributeIter = attributeTypes . values ( ) . iterator ( ) ; code_block = WhileStatement ; Collections . sort ( attributes , new BeanComparator ( "type" ) ) ; } catch ( Throwable t ) { _logger . error ( "Error extracting the allowed types of attribute elements" , t ) ; throw new RuntimeException ( "Error extracting the allowed types of attribute elements" , t ) ; } }
public static TokenizerEngine create ( ) { final TokenizerEngine engine = doCreate ( ) ; log . debug ( "Tokenizer {} created" , engine ) ; return engine ; }
public void test() { if ( ! config . isSet ( UnityServerConfiguration . SMS_CONF ) ) { LOGGER . debug ( "Skipping configuration update because the UnityServer configuration doesn't support {}" , UnityServerConfiguration . SMS_CONF ) ; return ; } }
public void test() { try { code_block = IfStatement ; File smsCfgFile = config . getFileValue ( UnityServerConfiguration . SMS_CONF , false ) ; String smsCfg = FileUtils . readFileToString ( smsCfgFile , Charset . defaultCharset ( ) ) ; NotificationChannel smsCh = new NotificationChannel ( UnityServerConfiguration . DEFAULT_SMS_CHANNEL , "Default SMS channel" , smsCfg , SMSFacility . NAME ) ; notManagement . addNotificationChannel ( smsCh ) ; } catch ( Exception e ) { logger . error ( "Can't load the SMS notification channel" , e ) ; throw new ConfigurationException ( "Can't load SMS notification channel configuration" , e ) ; } }
@ Test public void simplestUsage ( ) { Log log = LogFactory . getLog ( CommonsLoggingApiTest . class ) ; log . info ( "INFO" ) ; log . trace ( "TRACE" ) ; }
@ Test public void simplestUsage ( ) { Log log = LogFactory . getLog ( CommonsLoggingApiTest . class ) ; log . info ( "TRACE" ) ; log . info ( "INFO" , new Throwable ( ) ) ; }
public void test() { { logger . debug ( "Deleting query : {}" , queryId ) ; deleteNamedQuery ( PARAMETERIZED_QUERIES_REGION , queryId ) ; compiledQueries . remove ( queryId ) ; return new ResponseEntity < > ( HttpStatus . OK ) ; } }
public void test() { if ( client . isErrorRecoverable ( mktoResult . getErrors ( ) ) ) { LOG . debug ( "Recoverable error during operation : `{}`. Retrying..." , mktoResult . getErrorsString ( ) ) ; waitForRetryAttempInterval ( ) ; continue ; } else { LOG . error ( "Unrecoverable error : `{}`." , mktoResult . getErrorsString ( ) ) ; break ; } }
public void test() { if ( client . isErrorRecoverable ( mktoResult . getErrors ( ) ) ) { LOG . debug ( "Recoverable error during operation : `{}`. Retrying..." , mktoResult . getErrorsString ( ) ) ; waitForRetryAttempInterval ( ) ; continue ; } else { LOG . error ( "Unrecoverable error : `{}`." , mktoResult . getErrorsString ( ) ) ; break ; } }
@ Override public List < String > findUserGroupNames ( String userGroupName , boolean caseInsensitive ) throws JargonException { log . info ( "findUserGroupNames()" ) ; code_block = IfStatement ; log . info ( "caseInsensitive:{}" , caseInsensitive ) ; log . info ( "for user group name:{}" , userGroupName ) ; List < String > userGroups = new ArrayList < String > ( ) ; IRODSGenQueryBuilder builder = new IRODSGenQueryBuilder ( true , caseInsensitive , null ) ; code_block = TryStatement ;  IRODSGenQueryExecutor irodsGenQueryExecutor = getGenQueryExecutor ( ) ; StringBuilder sb = new StringBuilder ( ) ; sb . append ( userGroupName . trim ( ) ) ; sb . append ( '%' ) ; builder . addConditionAsGenQueryField ( RodsGenQueryEnum . COL_USER_GROUP_NAME , QueryConditionOperators . LIKE , sb . toString ( ) ) . addConditionAsGenQueryField ( RodsGenQueryEnum . COL_USER_TYPE , QueryConditionOperators . EQUAL , RODS_GROUP ) ; IRODSQueryResultSet resultSet = null ; code_block = TryStatement ;  code_block = ForStatement ; return userGroups ; }
@ Override public List < String > findUserGroupNames ( String userGroupName , boolean caseInsensitive ) throws JargonException { log . info ( "findUserGroups()" ) ; code_block = IfStatement ; log . info ( "for user group name:{}" , userGroupName ) ; log . info ( "user:{}" , userGroups ) ; List < String > userGroups = new ArrayList < String > ( ) ; IRODSGenQueryBuilder builder = new IRODSGenQueryBuilder ( true , caseInsensitive , null ) ; code_block = TryStatement ;  IRODSGenQueryExecutor irodsGenQueryExecutor = getGenQueryExecutor ( ) ; StringBuilder sb = new StringBuilder ( ) ; sb . append ( userGroupName . trim ( ) ) ; sb . append ( '%' ) ; builder . addConditionAsGenQueryField ( RodsGenQueryEnum . COL_USER_GROUP_NAME , QueryConditionOperators . LIKE , sb . toString ( ) ) . addConditionAsGenQueryField ( RodsGenQueryEnum . COL_USER_TYPE , QueryConditionOperators . EQUAL , RODS_GROUP ) ; IRODSQueryResultSet resultSet = null ; code_block = TryStatement ;  code_block = ForStatement ; return userGroups ; }
@ Override public List < String > findUserGroupNames ( String userGroupName , boolean caseInsensitive ) throws JargonException { log . info ( "findUserGroups()" ) ; code_block = IfStatement ; log . info ( "userGroupName:{}" , userGroupName ) ; log . info ( "caseInsensitive:{}" , caseInsensitive ) ; List < String > userGroups = new ArrayList < String > ( ) ; IRODSGenQueryBuilder builder = new IRODSGenQueryBuilder ( true , caseInsensitive , null ) ; code_block = TryStatement ;  IRODSGenQueryExecutor irodsGenQueryExecutor = getGenQueryExecutor ( ) ; StringBuilder sb = new StringBuilder ( ) ; sb . append ( userGroupName . trim ( ) ) ; sb . append ( '%' ) ; builder . addConditionAsGenQueryField ( RodsGenQueryEnum . COL_USER_GROUP_NAME , QueryConditionOperators . LIKE , sb . toString ( ) ) . addConditionAsGenQueryField ( RodsGenQueryEnum . COL_USER_TYPE , QueryConditionOperators . EQUAL , RODS_GROUP ) ; IRODSQueryResultSet resultSet = null ; code_block = TryStatement ;  code_block = ForStatement ; return userGroups ; }
public void test() { try { builder . addSelectAsGenQueryValue ( RodsGenQueryEnum . COL_USER_GROUP_NAME ) ; } catch ( GenQueryBuilderException e ) { log . error ( "query builder error" , e ) ; throw new JargonException ( "query builder error" , e ) ; } }
@ Override protected Object executeJob ( int gridSize , String type ) throws GridException { if ( type == null ) throw new IllegalArgumentException ( "Node type to start should be specified." ) ; GridConfiguration cfg = getConfig ( type ) ; String gridName = cfg . getGridName ( ) + " (" + UUID . randomUUID ( ) + ")" ; cfg . setGridName ( gridName ) ; log . info ( ">>> Starting Grid..." ) ; Grid g = G . start ( cfg ) ; log . info ( ">>> Grid started [nodeId=" + g . localNode ( ) . id ( ) + ", name='" + g . name ( ) + "']" ) ; return true ; }
@ Override protected Object executeJob ( int gridSize , String type ) throws GridException { log . info ( ">>> Starting grid node [currGridSize=" + gridSize + ", arg=" + type + "]" ) ; if ( type == null ) throw new IllegalArgumentException ( "Node type to start should be specified." ) ; GridConfiguration cfg = getConfig ( type ) ; String gridName = cfg . getGridName ( ) + " (" + UUID . randomUUID ( ) + ")" ; cfg . setGridName ( gridName ) ; Grid g = G . start ( cfg ) ; log . info ( ">>> Started new grid node [currGridSize=" + gridSize + ", arg=" + type + "]" ) ; return true ; }
public void test() { if ( trustCentre == null ) { logger . debug ( "TrustCentre is null" ) ; updateClientState ( SmartEnergyClientState . DISCOVER_TRUST_CENTRE ) ; return ; } }
public void test() { switch ( seState ) { case DISCOVER_KEY_ESTABLISHMENT_CLUSTER : code_block = IfStatement ; trustCentre = networkManager . getNode ( 0 ) ; code_block = IfStatement ; ZigBeeNode updatedTrustCentre = new ZigBeeNode ( networkManager , trustCentre . getIeeeAddress ( ) ) ; trustCenterKeyEstablishmentEndpoint = response . getMatchList ( ) . get ( 0 ) ; ZigBeeEndpoint keEndpoint = new ZigBeeEndpoint ( trustCentre , trustCenterKeyEstablishmentEndpoint ) ; keEndpoint . setProfileId ( ZigBeeProfileType . ZIGBEE_SMART_ENERGY . getKey ( ) ) ; updatedTrustCentre . addEndpoint ( keEndpoint ) ; ZclKeyEstablishmentCluster keCluster = new ZclKeyEstablishmentCluster ( keEndpoint ) ; keEndpoint . addInputCluster ( keCluster ) ; networkManager . updateNode ( updatedTrustCentre ) ; updateClientState ( SmartEnergyClientState . PERFORM_KEY_ESTABLISHMENT ) ; break ; case DISCOVER_METERING_SERVERS : ZigBeeNode node = networkManager . getNode ( response . getSourceAddress ( ) . getAddress ( ) ) ; code_block = IfStatement ; code_block = IfStatement ; ZigBeeNode updatedNode = new ZigBeeNode ( networkManager , node . getIeeeAddress ( ) , node . getNetworkAddress ( ) ) ; code_block = ForStatement ; setProfileSecurity ( updatedNode ) ; networkManager . updateNode ( updatedNode ) ; discoveryComplete ( ) ; logger . debug ( "SEP Extension: SEP discovery is using endpoint {}" , trustCenterKeepAliveEndpoint ) ; break ; case DISCOVER_KEEP_ALIVE : code_block = IfStatement ; trustCenterKeepAliveEndpoint = response . getMatchList ( ) . get ( 0 ) ; logger . debug ( "SEP Extension: SEP discovery is using endpoint {} for KeepAlive" , trustCenterKeepAliveEndpoint ) ; updateClientState ( SmartEnergyClientState . KEEP_ALIVE ) ; break ; default : break ; } }
public void test() { if ( ! cbkeProvider . isAuthorised ( node . getIeeeAddress ( ) ) ) { logger . debug ( "IeeeAddress {} not authorized" , node . getIeeeAddress ( ) ) ; return ; } }
public void test() { try { requestSimpleDescriptor ( endpoint ) ; } catch ( InterruptedException | ExecutionException e ) { LOG . error ( "Error trying to retrieve SimpleDescriptor for endpoint: {}" , endpoint . getDeployPath ( ) , e ) ; } }
public void test() { switch ( seState ) { case DISCOVER_KEY_ESTABLISHMENT_CLUSTER : code_block = IfStatement ; trustCentre = networkManager . getNode ( 0 ) ; code_block = IfStatement ; ZigBeeNode updatedTrustCentre = new ZigBeeNode ( networkManager , trustCentre . getIeeeAddress ( ) ) ; trustCenterKeyEstablishmentEndpoint = response . getMatchList ( ) . get ( 0 ) ; logger . debug ( "SEP Extension: SEP discovery is using endpoint {} for KeyEstablishment" , trustCenterKeyEstablishmentEndpoint ) ; ZigBeeEndpoint keEndpoint = new ZigBeeEndpoint ( trustCentre , trustCenterKeyEstablishmentEndpoint ) ; keEndpoint . setProfileId ( ZigBeeProfileType . ZIGBEE_SMART_ENERGY . getKey ( ) ) ; updatedTrustCentre . addEndpoint ( keEndpoint ) ; ZclKeyEstablishmentCluster keCluster = new ZclKeyEstablishmentCluster ( keEndpoint ) ; keEndpoint . addInputCluster ( keCluster ) ; networkManager . updateNode ( updatedTrustCentre ) ; updateClientState ( SmartEnergyClientState . PERFORM_KEY_ESTABLISHMENT ) ; break ; case DISCOVER_METERING_SERVERS : ZigBeeNode node = networkManager . getNode ( response . getSourceAddress ( ) . getAddress ( ) ) ; code_block = IfStatement ; code_block = IfStatement ; ZigBeeNode updatedNode = new ZigBeeNode ( networkManager , node . getIeeeAddress ( ) , node . getNetworkAddress ( ) ) ; code_block = ForStatement ; setProfileSecurity ( updatedNode ) ; networkManager . updateNode ( updatedNode ) ; discoveryComplete ( ) ; break ; case DISCOVER_KEEP_ALIVE : code_block = IfStatement ; trustCenterKeepAliveEndpoint = response . getMatchList ( ) . get ( 0 ) ; logger . debug ( "SEP Extension: SEP discovery is using endpoint {} for node {}" , trustCenterKeepAliveEndpoint , node . getMatch ( ) . get ( 0 ) ) ; updateClientState ( SmartEnergyClientState . KEEP_ALIVE ) ; break ; default : break ; } }
public ApplicationMap selectApplicationMapWithScatterData ( FilteredMapServiceOption option ) { StopWatch watch = new StopWatch ( ) ; watch . start ( ) ; final List < List < SpanBo > > filterList = selectFilteredSpan ( option . getTransactionIdList ( ) , option . getFilter ( ) , option . getColumnGetCount ( ) ) ; FilteredMapBuilder filteredMapBuilder = new FilteredMapBuilder ( applicationFactory , registry , option . getOriginalRange ( ) , option . getVersion ( ) ) ; filteredMapBuilder . serverMapDataFilter ( serverMapDataFilter ) ; filteredMapBuilder . addTransactions ( filterList ) ; FilteredMap filteredMap = filteredMapBuilder . build ( ) ; ApplicationMap map = createMap ( option , filteredMap ) ; Map < Application , ScatterData > applicationScatterData = filteredMap . getApplicationScatterData ( option . getOriginalRange ( ) . getFrom ( ) , option . getOriginalRange ( ) . getTo ( ) , option . getxGroupUnit ( ) , option . getyGroupUnit ( ) ) ; ApplicationMapWithScatterData applicationMapWithScatterData = new ApplicationMapWithScatterData ( map , applicationScatterData ) ; watch . stop ( ) ; logger . info ( "selectApplicationMapWithScatterData: {}" , watch . toString ( ) ) ; return applicationMapWithScatterData ; }
public void test() { if ( interpreterGroup == null ) { LOGGER . info ( "Skip interpreter group check." ) ; return ; } }
public void test() { if ( interpreterProcess == null ) { LOG . warn ( "Interpreter Execution process is null" ) ; return ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { if ( ! type . equals ( DataQuerySnapshot . TYPE ) ) { LOG . error ( "Unexpected snapshot type {}" , type ) ; return null ; } }
public void test() { if ( ! fieldsSet . add ( field ) ) { logger . debug ( "Field " + field + " already set" ) ; } }
@ Test ( expected = InvalidQueryException . class ) public void updateInvalidScript ( ) { Version fromVersion = Version . of ( 1 ) ; Version toVersion = Version . of ( 2 ) ; String schema = "schema" ; expect ( schemaProducer . schema ( fromVersion , toVersion ) ) . andReturn ( schema ) ; expect ( session . execute ( schema ) ) . andThrow ( new InvalidQueryException ( "expected message" ) ) ; logger . debug ( "CQL: {}" , schema ) ; expectLastCall ( ) ; logger . debug ( "CQL: {}" , schema ) ; expectLastCall ( ) ; mocks . replay ( ) ; code_block = TryStatement ;  }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; return false ; } }
public void test() { if ( location . getIndexDirectory ( ) . equals ( oldIndexDir ) ) { final IndexLocation updatedLocation = new IndexLocation ( newIndexDir , location . getIndexStartTimestamp ( ) , location . getPartitionName ( ) ) ; itr . set ( updatedLocation ) ; LOG . debug ( "Updating IndexLocation for {} to {}" , location , oldIndexDir ) ; replaced = true ; } }
public void test() { try { FileUtils . deleteFile ( oldIndexDir , true ) ; } catch ( IOException e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( ArrayUtil . contains ( listener , getListeners ( ) ) ) { log . warn ( "attempting to add a listener: " + listener ) ; } }
public void test() { try { final KVMStoragePool localStoragePool = _storagePoolMgr . createStoragePool ( _localStorageUUID , "localhost" , - 1 , _localStoragePath , "" , StoragePoolType . Filesystem ) ; final com . cloud . agent . api . StoragePoolInfo pi = new com . cloud . agent . api . StoragePoolInfo ( localStoragePool . getUuid ( ) , cmd . getPrivateIpAddress ( ) , _localStoragePath , _localStoragePath , StoragePoolType . Filesystem , localStoragePool . getCapacity ( ) , localStoragePool . getAvailable ( ) ) ; sscmd = new StartupStorageCommand ( ) ; sscmd . setPoolInfo ( pi ) ; sscmd . setGuid ( pi . getUuid ( ) ) ; sscmd . setDataCenter ( _dcId ) ; sscmd . setResourceType ( Storage . StorageResourceType . STORAGE_POOL ) ; } catch ( final CloudRuntimeException e ) { sscmdManager . LOG . warn ( "failed to initialize storage pool" , e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( SyncDeviceServiceUtil . class , "unregisterSyncDevice" , _unregisterSyncDeviceParameterTypes1 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , uuid ) ; code_block = TryStatement ;  } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { tx . rollback ( ) ; return Status . BACKOFF ; } catch ( Exception e2 ) { LOG . error ( "Error while rolling back transaction" , e2 ) ; } }
public void test() { try { tx . begin ( ) ; Event event = channel . take ( ) ; code_block = IfStatement ; String data = null ; code_block = IfStatement ; producer . send ( new KeyedMessage < String , String > ( topic , data ) ) ; tx . commit ( ) ; return Status . READY ; } catch ( Exception e ) { log . error ( "Recieved Message Exception." , e ) ; code_block = TryStatement ;  return Status . BACKOFF ; } finally { tx . close ( ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( model . hasPropertyAndNotNull ( ReservedIniKeys . OUTPUT_PREFIX . getKey ( ) ) ) { code_block = TryStatement ;  } else { LOGGER . debug ( "No need to import prefix" ) ; } }
public void test() { if ( model . hasPropertyAndNotNull ( ReservedIniKeys . OUTPUT_DIR . getKey ( ) ) ) { code_block = TryStatement ;  } else { Log . debug ( "No need to read output dir" ) ; } }
public void test() { if ( logger != null && logger . isDebugEnabled ( ) ) { logger . debug ( LogUtil . getMsg ( msg ) ) ; } }
public void test() { try { Thread . sleep ( monitorInterval ) ; } catch ( InterruptedException e ) { LOG . info ( e . toString ( ) ) ; break ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Failed to clean up." , e ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { data = readData ( path , null , true ) ; } catch ( ZkNoNodeException e ) { LOG . info ( "Node{} does not exist on Zookeeper" , path ) ; listener . getDataListener ( ) . handleDataDeleted ( path ) ; return ; } }
public void test() { if ( query instanceof GraphCentricQueryBuilder ) { LOG . debug ( "NativeJanusGraphQuery.vertices({}, {}): resultSize={}, {}" , offset , limit , getCountForDebugLog ( it ) , ( ( GraphCentricQueryBuilder ) query ) . constructQuery ( ElementCategory . VERTEX ) ) ; } else { LOG . debug ( "NativeJanusGraphQuery.vertices({}, {}): resultSize={}, {}" , offset , limit , getCountForDebugLog ( it ) , query ) ; } }
public void test() { if ( query instanceof GraphCentricQueryBuilder ) { LOG . debug ( "NativeJanusGraphQuery.vertices({}, {}): resultSize={}, {}" , offset , limit , getCountForDebugLog ( it ) , ( ( GraphCentricQueryBuilder ) query ) . constructQuery ( ElementCategory . VERTEX ) ) ; } else { LOG . debug ( "NativeJanusGraphQuery.vertices({}, {}): resultSize={}, {}" , offset , limit , getCountForDebugLog ( it ) , query ) ; } }
@ Override public void rollback ( ) throws TransactionException { logger . trace ( "rollback" ) ; }
public void test() { if ( log . isDebugEnabled ( ) || debug == 1 ) { log . debug ( "Arrays . deepToString ( msg ) ) ; } }
private int stageBundleForServing ( BundleDeployStatus status , String s3Path ) { int bundlesDownloaded = 0 ; List < String > bundles = listFiles ( s3Path , MAX_RESULTS ) ; code_block = IfStatement ; LOGGER . info ( "Got bundles: {}" , bundlesDownloaded ) ; code_block = ForStatement ; status . setStatus ( BundleDeployStatus . STATUS_COMPLETE ) ; return bundlesDownloaded ; }
public void test() { if ( bundles != null && ! bundles . isEmpty ( ) ) { LOGGER . info ( "Bundle StagingDirectory: " + bundles . size ( ) ) ; clearBundleStagingDirectory ( ) ; } else { return bundlesDownloaded ; } }
public void test() { try { String bundleFileLocation = _localBundleStagingPath + File . separator + bundleFilename ; _log . info ( "deleting bundle " + bundleFileLocation ) ; _fileUtil . unGzip ( new File ( bundleFileLocation ) , new File ( _localBundleStagingPath ) ) ; String tarFilename = parseTarName ( bundleFileLocation ) ; _log . info ( "unTar(" + tarFilename + ", " + _localBundleStagingPath + ")" ) ; _fileUtil . unTar ( new File ( tarFilename ) , new File ( _localBundleStagingPath ) ) ; _log . info ( "deleting bundle tar.gz=" + bundleFileLocation ) ; status . addBundleName ( bundleFilename ) ; new File ( tarFilename ) . delete ( ) ; new File ( bundleFileLocation ) . delete ( ) ; bundlesDownloaded ++ ; } catch ( Exception e ) { _log . error ( "exception exploding bundle=" + bundle , e ) ; } }
public void test() { try { String bundleFileLocation = _localBundleStagingPath + File . separator + bundleFilename ; _log . info ( "unGzip(" + bundleFileLocation + ", " + _localBundleStagingPath + ")" ) ; _fileUtil . unGzip ( new File ( bundleFileLocation ) , new File ( _localBundleStagingPath ) ) ; String tarFilename = parseTarName ( bundleFileLocation ) ; _log . info ( "deTar=" + tarFilename ) ; _fileUtil . unTar ( new File ( tarFilename ) , new File ( _localBundleStagingPath ) ) ; _log . info ( "deleting bundle tar.gz=" + bundleFileLocation ) ; status . addBundleName ( bundleFilename ) ; new File ( tarFilename ) . delete ( ) ; new File ( bundleFileLocation ) . delete ( ) ; bundlesDownloaded ++ ; } catch ( Exception e ) { _log . error ( "exception exploding bundle=" + bundle , e ) ; } }
public void test() { try { String bundleFileLocation = _localBundleStagingPath + File . separator + bundleFilename ; _log . info ( "unGzip(" + bundleFileLocation + ", " + _localBundleStagingPath + ")" ) ; _fileUtil . unGzip ( new File ( bundleFileLocation ) , new File ( _localBundleStagingPath ) ) ; String tarFilename = parseTarName ( bundleFileLocation ) ; _log . info ( "unTar(" + tarFilename + ", " + _localBundleStagingPath + ")" ) ; _fileUtil . unTar ( new File ( tarFilename ) , new File ( _localBundleStagingPath ) ) ; _log . info ( "addTar(" + tarFilename + ", " + _localBundleStagingPath + ")" ) ; status . addBundleName ( bundleFilename ) ; new File ( tarFilename ) . delete ( ) ; new File ( bundleFileLocation ) . delete ( ) ; bundlesDownloaded ++ ; } catch ( Exception e ) { _log . error ( "exception exploding bundle=" + bundle , e ) ; } }
public void test() { try { String bundleFileLocation = _localBundleStagingPath + File . separator + bundleFilename ; _log . info ( "unGzip(" + bundleFileLocation + ", " + _localBundleStagingPath + ")" ) ; _fileUtil . unGzip ( new File ( bundleFileLocation ) , new File ( _localBundleStagingPath ) ) ; String tarFilename = parseTarName ( bundleFileLocation ) ; _log . info ( "unTar(" + tarFilename + ", " + _localBundleStagingPath + ")" ) ; _fileUtil . unTar ( new File ( tarFilename ) , new File ( _localBundleStagingPath ) ) ; _log . info ( "deleting bundle tar.gz=" + bundleFileLocation ) ; status . addBundleName ( bundleFilename ) ; new File ( tarFilename ) . delete ( ) ; new File ( bundleFileLocation ) . delete ( ) ; bundlesDownloaded ++ ; } catch ( Exception e ) { _log . error ( "Unable to download bundles: " + e . getMessage ( ) ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { switch ( ( ( TellstickDevice ) device ) . getStatus ( ) ) { case JNA . CLibrary . TELLSTICK_TURNON : dimValue = new BigDecimal ( 100 ) ; break ; case JNA . CLibrary . TELLSTICK_TURNOFF : break ; case JNA . CLibrary . TELLSTICK_DIM : dimValue = new BigDecimal ( ( ( TellstickDevice ) device ) . getData ( ) ) ; dimValue = dimValue . multiply ( new BigDecimal ( 100 ) ) ; dimValue = dimValue . divide ( new BigDecimal ( 255 ) , 0 , RoundingMode . HALF_UP ) ; break ; default : logger . warn ( "Could not handle {} for {}" , ( ( ( TellstickDevice ) device ) . getStatus ( ) ) , device ) ; } }
public void test() { try { DatabaseDriverLoader . loadDriver ( new File ( d ) ) ; } catch ( IOException ioe ) { logger . error ( "Could not load database driver." , ioe ) ; } }
public void test() { if ( ! authority . isEmpty ( ) ) { List < Role > groupRoles ; if ( groupRoleProvider != null && addAsGroup ) groupRoles = groupRoleProvider . getRolesForGroup ( authority ) ; else groupRoles = Collections . emptyList ( ) ; String prefix = this . prefix ; code_block = IfStatement ; authority = ( prefix + authority ) . replaceAll ( ROLE_CLEAN_REGEXP , ROLE_CLEAN_REPLACEMENT ) ; logger . debug ( "Parsed LDAP role \"{}\" to role \"{}\"" , value , authority ) ; code_block = IfStatement ; authorities . add ( new SimpleGrantedAuthority ( authority ) ) ; } else { logger . warn ( "Parsed LDAP role \"{}\" cannot be parsed" , value ) ; } }
protected Job doLoad ( Configuration conf , TableDescriptor tableDescriptor ) throws Exception { Path outputDir = getTestDir ( TEST_NAME , "load-output" ) ; NMapInputFormat . setNumMapTasks ( conf , conf . getInt ( NUM_MAP_TASKS_KEY , NUM_MAP_TASKS_DEFAULT ) ) ; conf . set ( TABLE_NAME_KEY , tableDescriptor . getTableName ( ) . getNameAsString ( ) ) ; Job job = Job . getInstance ( conf ) ; job . setJobName ( TEST_NAME + " Load to " + tableDescriptor . getTableName ( ) ) ; job . setJarByClass ( this . getClass ( ) ) ; setMapperClass ( job ) ; job . setInputFormatClass ( NMapInputFormat . class ) ; job . setNumReduceTasks ( 0 ) ; setJobScannerConf ( job ) ; FileOutputFormat . setOutputPath ( job , outputDir ) ; TableMapReduceUtil . addDependencyJars ( job ) ; TableMapReduceUtil . addDependencyJarsForClasses ( job . getConfiguration ( ) , AbstractHBaseTool . class ) ; TableMapReduceUtil . initCredentials ( job ) ; LOG . info ( "Done." ) ; assertTrue ( job . waitForCompletion ( true ) ) ; return job ; }
public void test() { if ( blocksConnectedThisRound > 0 ) { log . info ( "Blocks connected: {}" , blocksConnectedThisRound ) ; } }
public void test() { if ( target == null ) { LOGGER . error ( "Cannot find '" + name + "' in classpath" ) ; } else { code_block = TryStatement ;  } }
public void test() { try { iEvent = target . getDeclaredConstructor ( event . getClass ( ) ) . newInstance ( event ) ; } catch ( NoSuchMethodException | SecurityException | InstantiationException | IllegalAccessException | IllegalArgumentException | InvocationTargetException e ) { logger . error ( "Exception during event dispatch" , e ) ; } }
public void test() { if ( autoDetectedFactory != null ) { logger . info ( String . format ( "Auto-detection selected discovery strategy: %s" , autoDetectedFactory . getClass ( ) ) ) ; discoveryStrategies . add ( autoDetectedFactory . newDiscoveryStrategy ( discoveryNode , logger , Collections . emptyMap ( ) ) ) ; } else { logger . warn ( String . format ( "Auto-detection strategy disabled: %s" , discoveryNode ) ) ; } }
public void test() { try { String sql = "SELECT id FROM dmhist_services WHERE system_name=? AND service_name=? LIMIT 1;" ; stmt = conn . prepareStatement ( sql ) ; stmt . setString ( 1 , systemName ) ; stmt . setString ( 2 , serviceName ) ; ResultSet rs = stmt . executeQuery ( ) ; rs . next ( ) ; id = rs . getInt ( "id" ) ; rs . close ( ) ; stmt . close ( ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) ) ; id = - 1 ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( BookmarksFolderServiceUtil . class , "getSubfolderIds" , _getSubfolderIdsParameterTypes16 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , folderIds , groupId , folderId , recurse ) ; code_block = TryStatement ;  } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { CollectRecord record = recordManager . load ( survey , s . getId ( ) , step , false ) ; modelWriter . printData ( record ) ; } catch ( Exception e ) { LOG . error ( "Error loading model: " + e . getMessage ( ) , e ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
@ Override @ Transactional public void disableScheduling ( ) { requireNotDisposed ( ) ; _log . info ( "Disabling scheduler service" ) ; _schedulingService . disableScheduling ( ) ; }
public GroupDto getGroupById ( final ExternalHttpContext context , final String id ) { LOGGER . debug ( "getGroupById started ..." ) ; final HttpEntity < Void > request = new HttpEntity < > ( buildHeaders ( context ) ) ; final URIBuilder builder = getUriBuilderFromPath ( "/groups/" + id + "/" ) ; final ResponseEntity < GroupDto > response = restTemplate . exchange ( buildUriBuilder ( builder ) , HttpMethod . GET , request , GroupDto . class ) ; checkResponse ( response ) ; return response . getBody ( ) ; }
public void test() { if ( LOG . isDebugEnabled ( ) && ! lockManager . isBtreeLockedForWrite ( getLockName ( ) ) ) { LOG . debug ( "The file doesn't own a write lock" ) ; } }
public void test() { try { pageHeader . setNextDataPage ( NO_PAGE ) ; pageHeader . setPrevDataPage ( NO_PAGE ) ; pageHeader . setDataLength ( 0 ) ; pageHeader . setNextTupleID ( ItemId . UNKNOWN_ID ) ; pageHeader . setRecordCount ( ( short ) 0 ) ; unlinkPages ( page . page ) ; page . setDirty ( true ) ; dataCache . remove ( page ) ; } catch ( final IOException ioe ) { LOG . error ( "Unable to flush page" , ioe ) ; } }
public void test() { try { processList = client . listAllProcesses ( ) . getProcessInfo ( ) ; code_block = IfStatement ; code_block = ForStatement ; } catch ( final RemoteException e ) { logger . error ( "RemoteException:" , e ) ; } }
public void test() { try { consentPurpose . setDisplayOrder ( Integer . parseInt ( member . getText ( ) ) ) ; } catch ( NumberFormatException e ) { LOGGER . warn ( "Unable to parse display order." , e ) ; consentPurpose . setDisplayOrder ( DEFAULT_DISPLAY_ORDER ) ; } }
@ Override public void registered ( ExecutorDriver executorDriver , Protos . ExecutorInfo executorInfo , Protos . FrameworkInfo frameworkInfo , Protos . SlaveInfo agentInfo ) { LOG . trace ( "Registered {} with Mesos agent {} for framework {}" , MesosUtils . formatForLogging ( executorInfo ) , MesosUtils . formatForLogging ( agentInfo ) , MesosUtils . formatForLogging ( frameworkInfo ) ) ; LOG . trace ( "Registered {} with Mesos agent {} for framework {}" , MesosUtils . formatForLogging ( executorInfo ) , MesosUtils . formatForLogging ( agentInfo ) , MesosUtils . formatForLogging ( frameworkInfo ) ) ; }
@ Override public void registered ( ExecutorDriver executorDriver , Protos . ExecutorInfo executorInfo , Protos . FrameworkInfo frameworkInfo , Protos . SlaveInfo agentInfo ) { LOG . debug ( "Registered {} with Mesos agent {} for framework {}" , executorInfo . getExecutorId ( ) . getValue ( ) , agentInfo . getId ( ) . getValue ( ) , frameworkInfo . getId ( ) . getValue ( ) ) ; LOG . debug ( "Registered {} with Mesos agent {} for framework {}" , executorInfo . getExecutorId ( ) . getValue ( ) , agentInfo . getId ( ) . getValue ( ) , frameworkInfo . getId ( ) . getValue ( ) ) ; }
private void reindexSpaces ( String index ) throws ImejiException { LOGGER . info ( "Starting reindex..." ) ; ElasticIndexer indexer = new ElasticIndexer ( index , ElasticTypes . spaces , ElasticService . ANALYSER ) ; SpaceController controller = new SpaceController ( ) ; List < Space > items = controller . retrieveAll ( ) ; LOGGER . info ( "+++ " + items . size ( ) + " items to index +++" ) ; indexer . indexBatch ( items ) ; indexer . commit ( ) ; LOGGER . info ( "Items reindexed!" ) ; }
private void reindexSpaces ( String index ) throws ImejiException { LOGGER . info ( "Indexing Spaces..." ) ; ElasticIndexer indexer = new ElasticIndexer ( index , ElasticTypes . spaces , ElasticService . ANALYSER ) ; SpaceController controller = new SpaceController ( ) ; List < Space > items = controller . retrieveAll ( ) ; LOGGER . info ( "Indexing spaces..." ) ; indexer . indexBatch ( items ) ; indexer . commit ( ) ; LOGGER . info ( "Items reindexed!" ) ; }
private void reindexSpaces ( String index ) throws ImejiException { LOGGER . info ( "Indexing Spaces..." ) ; ElasticIndexer indexer = new ElasticIndexer ( index , ElasticTypes . spaces , ElasticService . ANALYSER ) ; SpaceController controller = new SpaceController ( ) ; List < Space > items = controller . retrieveAll ( ) ; LOGGER . info ( "+++ " + items . size ( ) + " items to index +++" ) ; indexer . indexBatch ( items ) ; LOGGER . info ( "Done receiving spaces" ) ; indexer . commit ( ) ; }
@ Override public JSONObject getFirstPage ( ) throws IOException { URL apiURL = new URL ( baseURL + "&consumer_key=" + CONSUMER_KEY ) ; LOG . debug ( "apiURL: " + apiURL ) ; JSONObject json = Http . url ( apiURL ) . getJSON ( ) ; code_block = IfStatement ; return json ; }
public void test() { if ( applicationContext . getResource ( location ) . exists ( ) ) { loadedConfigurations . add ( location ) ; hasSource = true ; ResourcePropertySource source = new ResourcePropertySource ( applicationContext . getResource ( location ) ) ; sources . addFirst ( source ) ; propertyNames . addAll ( Arrays . asList ( source . getPropertyNames ( ) ) ) ; LOGGER . info ( "Log4j : {} added" , location ) ; } else { ignoredConfigurations . add ( location ) ; LOGGER . info ( "Log4j : {} not found" , location ) ; } }
public void test() { if ( applicationContext . getResource ( location ) . exists ( ) ) { loadedConfigurations . add ( location ) ; LOGGER . info ( "Log4j : {} added" , location ) ; hasSource = true ; ResourcePropertySource source = new ResourcePropertySource ( applicationContext . getResource ( location ) ) ; sources . addFirst ( source ) ; propertyNames . addAll ( Arrays . asList ( source . getPropertyNames ( ) ) ) ; } else { ignoredConfigurations . add ( location ) ; LOGGER . info ( "Log4j : {} not found" , location ) ; } }
public void test() { if ( resolver . containsProperty ( propertyName ) ) { LOGGER . debug ( "Log4j : property resolved {} -> {}" , propertyName , resolver . getProperty ( propertyName ) ) ; properties . put ( propertyName , resolver . getProperty ( propertyName ) ) ; } else { LOGGER . debug ( "Log4j : property resolved {} -> {}" , propertyName , resolver . getProperty ( propertyName ) ) ; } }
public void test() { if ( LOGGER . isWarnEnabled ( ) ) { LOGGER . warn ( "Unable to find parameter {}" , key ) ; } }
public void test() { if ( ! dir . isAbsolute ( ) ) { logger . info ( "DIR: " + dir . getAbsolutePath ( ) ) ; return false ; } }
public void test() { if ( ! dir . exists ( ) || ! dir . isDirectory ( ) || ! dir . canWrite ( ) ) { logger . error ( "Unable to create directory " + dir . getAbsolutePath ( ) ) ; return false ; } }
public void test() { if ( expectedContextEntries == null ) { getLogger ( ) . info ( "No context found to " + key ) ; } else { List < String > missingEntries = new ArrayList < > ( ) ; code_block = ForStatement ; code_block = IfStatement ; } }
public void test() { if ( missingEntries . size ( ) > 0 ) { log . warn ( String . format ( "Unable to retrieve records: %s" , missingEntries ) ) ; } }
public void test() { if ( ostream != null ) { LOG . debug ( "Acquired lock on file {}. LockFile= {}, Spout = {}" , fileToLock , lockFile , spoutId ) ; return new FileLock ( fs , lockFile , ostream , spoutId ) ; } else { LOG . debug ( "Unable to acquire lock on file {}. LockFile= {}, Spout = {}" , fileToLock , lockFile , spoutId ) ; return null ; } }
public void test() { try { FSDataOutputStream ostream = HdfsUtils . tryCreateFile ( fs , lockFile ) ; code_block = IfStatement ; } catch ( IOException e ) { LOG . error ( "Unable to create lock file " + lockFile , e ) ; throw e ; } }
private String createExistingServersDescription ( final String managementMachinePrefix , final MachineDetails [ ] existingManagementServers ) { final StringBuilder sb = new StringBuilder ( ) ; boolean first = true ; code_block = ForStatement ; final String serverDescriptions = sb . toString ( ) ; LOG . debug ( "Created existing existing management machines: {}" , serverDescriptions ) ; return serverDescriptions ; }
private void setXATerminator ( final XATerminator terminator ) { this . xaTerminator = terminator ; LOGGER . debug ( "setXATerminator={}" , terminator ) ; }
public void test() { try { typeOfChangeInProgress = ChangeType . REDOING ; List < OWLOntologyChange > redoChanges = redoStack . pop ( ) ; manager . applyChanges ( redoChanges ) ; } catch ( Exception e ) { logger . error ( "Error while executing redo stack" , e ) ; } finally { typeOfChangeInProgress = ChangeType . NORMAL ; } }
public void test() { try { return Integer . parseInt ( val ) ; } catch ( NumberFormatException e ) { LOG . warn ( "Can't parse variable '" + val + "'" , e ) ; return 0 ; } }
public void test() { try { Class < ? > clazz = Class . forName ( "org.apache.jasper.servlet.JspServletWrapper" ) ; Method method = ReflectionUtil . getDeclaredMethod ( clazz , "getDependants" ) ; _jspServletDependantsMap = Map . class . isAssignableFrom ( method . getReturnType ( ) ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; } }
public void test() { try ( CallableStatement query = connection . prepareCall ( "{call update_task_state(?, ?)}" ) ) { query . setObject ( 1 , taskId ) ; query . setString ( 2 , state . toString ( ) ) ; query . executeQuery ( ) ; } catch ( SQLException e ) { log . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { BroadcastSmResult broadcastSmResult = fireAcceptBroadcastSm ( broadcastSm ) ; code_block = IfStatement ; return broadcastSmResult ; } catch ( ProcessRequestException e ) { throw e ; } catch ( Exception e ) { String msg = "Invalid runtime exception thrown when processing broadcast_sm" ; logger . error ( msg , e ) ; throw new ProcessRequestException ( msg , SMPPConstant . STAT_ESME_RSYSERR ) ; } }
@ Test public void wildcardResourcesAreOrderedAlphabetically ( ) { final WroModel model = new WroModel ( ) ; final String uri = String . format ( ClasspathUriLocator . PREFIX + "%s/expander/order/**.js" , WroUtil . toPackageAsFolder ( getClass ( ) ) ) ; model . addGroup ( new Group ( "group" ) . addResource ( Resource . create ( uri , ResourceType . JS ) ) ) ; Mockito . when ( decoratedFactory . create ( ) ) . thenReturn ( model ) ; final WroModel changedModel = transformer . transform ( model ) ; LOG . info ( "model: {}" , uri ) ; final Group group = new WroModelInspector ( changedModel ) . getGroupByName ( "group" ) ; assertEquals ( 7 , group . getResources ( ) . size ( ) ) ; final List < Resource > resources = group . getResources ( ) ; assertEquals ( "01-xyc.js" , FilenameUtils . getName ( resources . get ( 0 ) . getUri ( ) ) ) ; assertEquals ( "02-xyc.js" , FilenameUtils . getName ( resources . get ( 1 ) . getUri ( ) ) ) ; assertEquals ( "03-jquery-ui.js" , FilenameUtils . getName ( resources . get ( 2 ) . getUri ( ) ) ) ; assertEquals ( "04-xyc.js" , FilenameUtils . getName ( resources . get ( 3 ) . getUri ( ) ) ) ; assertEquals ( "05-xyc.js" , FilenameUtils . getName ( resources . get ( 4 ) . getUri ( ) ) ) ; assertEquals ( "06-xyc.js" , FilenameUtils . getName ( resources . get ( 5 ) . getUri ( ) ) ) ; assertEquals ( "07-jquery-impromptu.js" , FilenameUtils . getName ( resources . get ( 6 ) . getUri ( ) ) ) ; }
public void test() { if ( session != null ) { session . setCerberus_selenium_wait_element ( session . getCerberus_selenium_wait_element_default ( ) ) ; session . setCerberus_appium_wait_element ( session . getCerberus_appium_wait_element_default ( ) ) ; session . setCerberus_sikuli_wait_element ( session . getCerberus_sikuli_wait_element_default ( ) ) ; LOG . debug ( "Setting Robot highlightElement back to default values : Selenium " + session . getCerberus_selenium_highlightElement_default ( ) + " Sikuli " + session . getCerberus_sikuli_highlightElement_default ( ) ) ; session . setCerberus_selenium_highlightElement ( session . getCerberus_selenium_highlightElement_default ( ) ) ; session . setCerberus_sikuli_highlightElement ( session . getCerberus_sikuli_highlightElement_default ( ) ) ; LOG . debug ( "Setting Robot minSimilarity back to default values : " + session . getCerberus_sikuli_minSimilarity_default ( ) ) ; session . setCerberus_sikuli_minSimilarity ( session . getCerberus_sikuli_minSimilarity_default ( ) ) ; LOG . debug ( "Setting Robot maxSimilarity back to default values : " + session . getCerberus_sikuli_default ( ) ) ; } }
public void test() { if ( session != null ) { LOG . debug ( "Setting Robot Timeout back to default values : Selenium " + session . getCerberus_selenium_wait_element_default ( ) + " Appium " + session . getCerberus_appium_wait_element_default ( ) + " Sikuli " + session . getCerberus_sikuli_wait_element_default ( ) ) ; session . setCerberus_selenium_wait_element ( session . getCerberus_selenium_wait_element_default ( ) ) ; session . setCerberus_appium_wait_element ( session . getCerberus_appium_wait_element_default ( ) ) ; session . setCerberus_sikuli_wait_element ( session . getCerberus_sikuli_wait_element_default ( ) ) ; session . setCerberus_selenium_highlightElement ( session . getCerberus_selenium_highlightElement_default ( ) ) ; session . setCerberus_sikuli_highlightElement ( session . getCerberus_sikuli_highlightElement_default ( ) ) ; LOG . debug ( "Setting Robot minSimilarity back to default values : " + session . getCerberus_sikuli_minSimilarity_default ( ) ) ; session . setCerberus_sikuli_minSimilarity ( session . getCerberus_sikuli_minSimilarity_default ( ) ) ; LOG . debug ( "Setting Robot maxSimilarity back to default values : " + session . getCerberus_sikuli_maxSimilarity_default ( ) ) ; } }
public void test() { if ( session != null ) { LOG . debug ( "Setting Robot Timeout back to default values : Selenium " + session . getCerberus_selenium_wait_element_default ( ) + " Appium " + session . getCerberus_appium_wait_element_default ( ) + " Sikuli " + session . getCerberus_sikuli_wait_element_default ( ) ) ; session . setCerberus_selenium_wait_element ( session . getCerberus_selenium_wait_element_default ( ) ) ; session . setCerberus_appium_wait_element ( session . getCerberus_appium_wait_element_default ( ) ) ; session . setCerberus_sikuli_wait_element ( session . getCerberus_sikuli_wait_element_default ( ) ) ; LOG . debug ( "Setting Robot highlightElement back to default values : Selenium " + session . getCerberus_selenium_highlightElement_default ( ) + " Sikuli " + session . getCerberus_sikuli_highlightElement_default ( ) ) ; session . setCerberus_selenium_highlightElement ( session . getCerberus_selenium_highlightElement_default ( ) ) ; session . setCerberus_sikuli_highlightElement ( session . getCerberus_sikuli_highlightElement_default ( ) ) ; session . setCerberus_sikuli_minSimilarity ( session . getCerberus_sikuli_minSimilarity_default ( ) ) ; LOG . debug ( "Setting Robot proxyElement back to default values" ) ; } }
@ Override public void onStopContainerError ( ContainerId containerId , Throwable t ) { LOG . error ( "Failed to stop Container " + containerId , t ) ; containers . remove ( containerId ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { IGefaehrdungsBaumElement elmt = ( IGefaehrdungsBaumElement ) parentElement ; return elmt . getGefaehrdungsBaumChildren ( ) . toArray ( ) ; } catch ( Exception e ) { LOG . error ( e . getMessage ( ) , e ) ; return null ; } }
public void test() { if ( values . contains ( StorageConstants . LATEST_PARTITION_VALUE ) ) { log . info ( "Dropping partition: " + values ) ; getClient ( ) . dropPartition ( storageTableName , values , false ) ; continue ; } }
public void test() { try { URL url = FileLocator . find ( Activator . getDefault ( ) . getBundle ( ) , new Path ( "/WebContent/WEB-INF/reportDeposit/" ) , null ) ; URL fileUrl = FileLocator . toFileURL ( url ) ; return FileUtils . toFile ( fileUrl ) . getAbsolutePath ( ) ; } catch ( IOException ex ) { log . error ( ex . getMessage ( ) , ex ) ; } }
public void test() { if ( getDefinedLocationByName ( "localhost" ) == null && ! BasicOsDetails . Factory . newLocalhostInstance ( ) . isWindows ( ) && LocationConfigUtils . isEnabled ( mgmt , "brooklyn.location.localhost" ) ) { ImmutableMap < String , LocationDefinition > oldDefined = ImmutableMap . copyOf ( definedLocations ) ; definedLocations . clear ( ) ; String id = Identifiers . makeRandomId ( 8 ) ; definedLocations . put ( id , localhost ( id ) ) ; definedLocations . putAll ( oldDefined ) ; LOG . info ( "Created shared location " + id ) ; } }
public void test() { try { connection . close ( ) ; } catch ( JMSException e ) { logger . error ( "Exception closing JMS connection" , e ) ; } }
public void test() { try { geo = gmlReader . read ( xml , null ) ; code_block = IfStatement ; } catch ( SAXException | IOException | ParserConfigurationException | GeoFormatException e ) { LOGGER . log ( Level . WARNING , e . getMessage ( ) , e ) ; geo = null ; } }
public void test() { try { ser = reader . getValue ( ) . getBytes ( UTF8_ENCODING ) ; } catch ( UnsupportedEncodingException e ) { LOG . warn ( "Error converting to UTF8 encoding: {}" , e . getLocalizedMessage ( ) ) ; } }
public void test() { if ( isDebug ) { logger . debug ( "Expected arguments, but found none." ) ; } }
@ Test public void testJacksonJSONParsing ( ) throws Exception { RepairScheduleStatus data = new RepairScheduleStatus ( ) ; data . setClusterName ( "testCluster" ) ; data . setColumnFamilies ( Lists . < String > newArrayList ( ) ) ; data . setCreationTime ( DateTime . now ( ) . withMillis ( 0 ) ) ; data . setDaysBetween ( 2 ) ; data . setId ( UUIDs . timeBased ( ) ) ; data . setIntensity ( 0.75 ) ; data . setIncrementalRepair ( false ) ; data . setKeyspaceName ( "testKeyspace" ) ; data . setOwner ( "testuser" ) ; data . setRepairParallelism ( RepairParallelism . PARALLEL ) ; data . setState ( RepairSchedule . State . ACTIVE ) ; ObjectMapper mapper = new ObjectMapper ( ) ; String dataAsJson = mapper . writeValueAsString ( data ) ; log . info ( dataAsJson ) ; RepairScheduleStatus dataAfter = SimpleReaperClient . parseRepairScheduleStatusJSON ( dataAsJson ) ; assertEquals ( data . getClusterName ( ) , dataAfter . getClusterName ( ) ) ; assertEquals ( data . getColumnFamilies ( ) , dataAfter . getColumnFamilies ( ) ) ; assertEquals ( data . getCreationTime ( ) , dataAfter . getCreationTime ( ) ) ; assertEquals ( data . getDaysBetween ( ) , dataAfter . getDaysBetween ( ) ) ; assertEquals ( data . getId ( ) , dataAfter . getId ( ) ) ; assertEquals ( data . getIntensity ( ) , dataAfter . getIntensity ( ) , 0.0 ) ; assertEquals ( data . getIncrementalRepair ( ) , dataAfter . getIncrementalRepair ( ) ) ; assertEquals ( data . getKeyspaceName ( ) , dataAfter . getKeyspaceName ( ) ) ; assertEquals ( data . getRepairParallelism ( ) , dataAfter . getRepairParallelism ( ) ) ; assertEquals ( data . getState ( ) , dataAfter . getState ( ) ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { long start = System . currentTimeMillis ( ) ; LOG . info ( "Testcase: " + this . getClass ( ) . getName ( ) + "testGetRecord, testDir=" + dataStoreDir ) ; doGetRecordTest ( ) ; LOG . info ( "Testcase: " + this . getClass ( ) . getName ( ) + "testGetRecord finished, time taken = [" + ( System . currentTimeMillis ( ) - start ) + "]ms" ) ; } catch ( Exception e ) { LOG . error ( "error:" , e ) ; } }
public void test() { try { long start = System . currentTimeMillis ( ) ; LOG . info ( "Testcase: " + this . getClass ( ) . getName ( ) + "testGetRecord, testDir=" + dataStoreDir ) ; doGetRecordTest ( ) ; LOG . info ( "Testcase: " + this . getClass ( ) . getName ( ) + "testGetRecord finished, time taken = [" + ( System . currentTimeMillis ( ) - start ) + "]ms" ) ; } catch ( Exception e ) { LOG . error ( "error:" , e ) ; } }
public void test() { try { long start = System . currentTimeMillis ( ) ; LOG . info ( "Testcase: " + this . getClass ( ) . getName ( ) + "testGetRecord, testDir=" + dataStoreDir ) ; doGetRecordTest ( ) ; LOG . info ( "Testcase: " + this . getClass ( ) . getName ( ) + "testGetRecord finished, time taken = [" + ( System . currentTimeMillis ( ) - start ) + "]ms" ) ; } catch ( Exception e ) { LOG . error ( "error:" , e ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { @ SuppressWarnings ( "unchecked" ) Class < RangerAdminClient > adminClass = ( Class < RangerAdminClient > ) Class . forName ( policySourceImpl ) ; ret = adminClass . newInstance ( ) ; } catch ( Exception excp ) { LOG . error ( "failed to instantiate admin class" , excp ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { RetryMessageModel retryMessageModel = DaoUtil . queryObject ( readDataSource , GET_SQL , new DaoUtil . QueryCallback < RetryMessageModel > ( ) code_block = "" ; ) ; return retryMessageModel ; } catch ( Exception e ) { logger . error ( String . format ( "%s topic:%s,app:%s,id:%d" , JoyQueueCode . CN_DB_ERROR . getMessage ( ) , topic , app , id ) , e ) ; throw new JoyQueueException ( String . format ( "%s topic:%s,app:%s,id:%d" , JoyQueueCode . CN_DB_ERROR . getMessage ( ) , topic , app , id ) , e , JoyQueueCode . CN_DB_ERROR . getCode ( ) ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
@ AcceptsPreStepAuth @ GET @ Path ( "/consents/{consentId}/status" ) public Response getConsentStatus ( @ NotEmpty @ NotBlank @ PathParam ( "consentId" ) String consentId ) throws BankRequestFailedException { XS2AFactoryInput xs2AFactoryInput = new XS2AFactoryInput ( ) ; xs2AFactoryInput . setConsentId ( consentId ) ; IOProcessor ioProcessor = new IOProcessor ( getXS2AStandard ( ) ) ; ioProcessor . modifyInput ( xs2AFactoryInput ) ; AISRequest request = new AISRequestFactory ( ) . create ( getXS2AStandard ( ) . getRequestClassProvider ( ) . consentStatus ( ) , xs2AFactoryInput ) ; request . getHeaders ( ) . putAll ( getAdditionalHeaders ( ) ) ; ioProcessor . modifyRequest ( request , xs2AFactoryInput ) ; ConsentStatus state = getXS2AStandard ( ) . getCs ( ) . getStatus ( request ) ; GetConsentStatusResponse response = new GetConsentStatusResponse ( state ) ; LOG . info ( "Successfully fetched status for consentId={}" , getXS2AStandard ( ) . getId ( ) ) ; return Response . status ( ResponseConstant . OK ) . entity ( response ) . build ( ) ; }
@ Override public void init ( FilterConfig filterConfig ) throws ServletException { super . init ( filterConfig ) ; forwardPath = filterConfig . getInitParameter ( "forwardPath" ) ; displayPath = filterConfig . getInitParameter ( "displayPath" ) ; logger . info ( "{}" , forwardPath ) ; }
public void test() { try { StorageResourceDescription storageResource = AgentUtils . getRegistryServiceClient ( ) . getStorageResource ( storageResourceId ) ; logger . info ( "Fetching credentials for cred store token " + token ) ; SSHCredential sshCredential = AgentUtils . getCredentialClient ( ) . getSSHCredential ( token , gatewayId ) ; code_block = IfStatement ; logger . info ( "Server user: " + loginUser ) ; logger . info ( "Description for token : " + token + " : " + sshCredential . getDescription ( ) ) ; SshAdaptorParams adaptorParams = new SshAdaptorParams ( ) ; adaptorParams . setHostName ( storageResource . getHostName ( ) ) ; adaptorParams . setUserName ( loginUser ) ; adaptorParams . setPassphrase ( sshCredential . getPassphrase ( ) ) ; adaptorParams . setPrivateKey ( sshCredential . getPrivateKey ( ) . getBytes ( ) ) ; adaptorParams . setPublicKey ( sshCredential . getPublicKey ( ) . getBytes ( ) ) ; adaptorParams . setStrictHostKeyChecking ( false ) ; init ( adaptorParams ) ; } catch ( Exception e ) { logger . error ( "Error while initializing ssh agent for storage resource " + storageResourceId + " to token " + token , e ) ; throw new AgentException ( "Error while initializing ssh agent for storage resource " + storageResourceId + " to token " + token , e ) ; } }
public void test() { try { logger . info ( "Initializing Storage Resource Adaptor for storage resource : " + storageResourceId + ", gateway : " + gatewayId + ", user " + loginUser + ", token : " + token ) ; StorageResourceDescription storageResource = AgentUtils . getRegistryServiceClient ( ) . getStorageResource ( storageResourceId ) ; SSHCredential sshCredential = AgentUtils . getCredentialClient ( ) . getSSHCredential ( token , gatewayId ) ; code_block = IfStatement ; logger . info ( "Description for token : " + token + " : " + sshCredential . getDescription ( ) ) ; SshAdaptorParams adaptorParams = new SshAdaptorParams ( ) ; adaptorParams . setHostName ( storageResource . getHostName ( ) ) ; adaptorParams . setUserName ( loginUser ) ; adaptorParams . setPassphrase ( sshCredential . getPassphrase ( ) ) ; adaptorParams . setPrivateKey ( sshCredential . getPrivateKey ( ) . getBytes ( ) ) ; adaptorParams . setPublicKey ( sshCredential . getPublicKey ( ) . getBytes ( ) ) ; adaptorParams . setStrictHostKeyChecking ( false ) ; logger . info ( "Storage Resource Adaptor: " + storageResource . getHostName ( ) ) ; init ( adaptorParams ) ; } catch ( Exception e ) { logger . error ( "Error while initializing ssh agent for storage resource " + storageResourceId + " to token " + token , e ) ; throw new AgentException ( "Error while initializing ssh agent for storage resource " + storageResourceId + " to token " + token , e ) ; } }
public void test() { try { logger . info ( "Initializing Storage Resource Adaptor for storage resource : " + storageResourceId + ", gateway : " + gatewayId + ", user " + loginUser + ", token : " + token ) ; StorageResourceDescription storageResource = AgentUtils . getRegistryServiceClient ( ) . getStorageResource ( storageResourceId ) ; logger . info ( "Fetching credentials for cred store token " + token ) ; SSHCredential sshCredential = AgentUtils . getCredentialClient ( ) . getSSHCredential ( token , gatewayId ) ; code_block = IfStatement ; SshAdaptorParams adaptorParams = new SshAdaptorParams ( ) ; adaptorParams . setHostName ( storageResource . getHostName ( ) ) ; adaptorParams . setUserName ( loginUser ) ; adaptorParams . setPassphrase ( sshCredential . getPassphrase ( ) ) ; adaptorParams . setPrivateKey ( sshCredential . getPrivateKey ( ) . getBytes ( ) ) ; adaptorParams . setPublicKey ( sshCredential . getPublicKey ( ) . getBytes ( ) ) ; adaptorParams . setStrictHostKeyChecking ( false ) ; init ( adaptorParams ) ; logger . info ( "Successfully initialized Storage Resource Adaptor for storage resource : " + storageResourceId + " to token " + token ) ; } catch ( Exception e ) { logger . error ( "Error while initializing ssh agent for storage resource " + storageResourceId + " to token " + token , e ) ; throw new AgentException ( "Error while initializing ssh agent for storage resource " + storageResourceId + " to token " + token , e ) ; } }
public void test() { try { logger . info ( "Initializing Storage Resource Adaptor for storage resource : " + storageResourceId + ", gateway : " + gatewayId + ", user " + loginUser + ", token : " + token ) ; StorageResourceDescription storageResource = AgentUtils . getRegistryServiceClient ( ) . getStorageResource ( storageResourceId ) ; logger . info ( "Fetching credentials for cred store token " + token ) ; SSHCredential sshCredential = AgentUtils . getCredentialClient ( ) . getSSHCredential ( token , gatewayId ) ; code_block = IfStatement ; logger . info ( "Description for token : " + token + " : " + sshCredential . getDescription ( ) ) ; SshAdaptorParams adaptorParams = new SshAdaptorParams ( ) ; adaptorParams . setHostName ( storageResource . getHostName ( ) ) ; adaptorParams . setUserName ( loginUser ) ; adaptorParams . setPassphrase ( sshCredential . getPassphrase ( ) ) ; adaptorParams . setPrivateKey ( sshCredential . getPrivateKey ( ) . getBytes ( ) ) ; adaptorParams . setPublicKey ( sshCredential . getPublicKey ( ) . getBytes ( ) ) ; adaptorParams . setStrictHostKeyChecking ( false ) ; init ( adaptorParams ) ; } catch ( Exception e ) { logger . error ( "Error while initializing ssh agent for storage resource " + storageResourceId + " to token " + token , e ) ; throw new AgentException ( "Error while initializing ssh agent for storage resource " + storageResourceId + " to token " + token , e ) ; } }
@ Override public boolean addTicketHostRestriction ( final String ticketId , final String host ) throws JargonException { Tag ticketOperationResponse = null ; boolean response = true ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "modifying ticket id:{}" , ticketId ) ; TicketAdminInp ticketPI = TicketAdminInp . instanceForModifyAddAccess ( ticketId , TicketModifyAddOrRemoveTypeEnum . TICKET_MODIFY_HOST , host ) ; log . info ( EXECUTING_TICKET_PI ) ; ProtocolExtensionPoint pep = irodsAccessObjectFactory . getProtocolExtensionPoint ( irodsAccount ) ; code_block = TryStatement ;  log . info ( "received response from ticket operation:{}" , ticketOperationResponse ) ; return response ; }
@ Override public boolean addTicketHostRestriction ( final String ticketId , final String host ) throws JargonException { Tag ticketOperationResponse = null ; boolean response = true ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "modifying ticket id/string:{}" , ticketId ) ; TicketAdminInp ticketPI = TicketAdminInp . instanceForModifyAddAccess ( ticketId , TicketModifyAddOrRemoveTypeEnum . TICKET_MODIFY_HOST , host ) ; log . info ( EXECUTING_TICKET_PI ) ; ProtocolExtensionPoint pep = irodsAccessObjectFactory . getProtocolExtensionPoint ( irodsAccount ) ; code_block = TryStatement ;  log . info ( "received response from ticket operation:{}" , ticketOperationResponse ) ; return response ; }
@ Override public boolean addTicketHostRestriction ( final String ticketId , final String host ) throws JargonException { Tag ticketOperationResponse = null ; boolean response = true ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "modifying ticket id/string:{}" , ticketId ) ; TicketAdminInp ticketPI = TicketAdminInp . instanceForModifyAddAccess ( ticketId , TicketModifyAddOrRemoveTypeEnum . TICKET_MODIFY_HOST , host ) ; log . info ( EXECUTING_TICKET_PI ) ; ProtocolExtensionPoint pep = irodsAccessObjectFactory . getProtocolExtensionPoint ( irodsAccount ) ; code_block = TryStatement ;  log . info ( "successfully added ticket operation:{}" , ticketOperationResponse ) ; return response ; }
public void test() { try { return new ScriptEngineManager ( ) ; } finally { long end = System . currentTimeMillis ( ) ; LOG . info ( "ScriptEngine manager started in " + ( end - start ) + " ms" ) ; } }
public void test() { if ( debug ) { logger . debug ( "Expected error: " + e . getMessage ( ) ) ; } }
public void test() { try { List < GenSolvablePolynomial < C > > G = e1 . twosidedGB ( modv , F ) ; code_block = IfStatement ; return G ; } catch ( PreemptingException e ) { throw new RuntimeException ( "SGBProxy e1 preempted " + e ) ; } catch ( Exception e ) { logger . info ( "SGBProxy e1 " + e ) ; logger . info ( "Exception SGBProxy F = " + F ) ; throw new RuntimeException ( "SGBProxy e1 " + e ) ; } }
public void test() { try { List < GenSolvablePolynomial < C > > G = e1 . twosidedGB ( modv , F ) ; code_block = IfStatement ; return G ; } catch ( PreemptingException e ) { throw new RuntimeException ( "SGBProxy e1 preempted " + e ) ; } catch ( Exception e ) { logger . info ( "SGBProxy e1 " + e ) ; logger . info ( "Exception SGBProxy F = " + F ) ; throw new RuntimeException ( "SGBProxy e1 " + e ) ; } }
@ Override public void audit ( String message , Supplier ... paramSuppliers ) { StringBuilder messageBuilder = new StringBuilder ( ) ; requestIpAndPortAndUserMessage ( PhaseInterceptorChain . getCurrentMessage ( ) , messageBuilder ) ; log . info ( messageBuilder . toString ( ) ) ; }
public void test() { try { Integer value = ( Integer ) myFormatter . stringToValue ( myCustomHeightField . getText ( ) ) ; return value . intValue ( ) ; } catch ( ParseException e ) { LOGGER . error ( "Failed to parse zoom level field" , e ) ; } }
public void test() { try { client . close ( ) ; } catch ( IOException ex ) { LOGGER . warn ( "Failed to close client: " + name , ex ) ; } }
private void createSchema ( Connection conn ) throws IOException { String createSchemaScriptPath = getCreateSchemaScriptPath ( ) ; logger . info ( "Database schema does not exists." ) ; ScriptRunner sr = new ScriptRunner ( conn ) ; sr . setDelimiter ( delimiter ) ; sr . setStopOnError ( true ) ; sr . setLogWriter ( null ) ; InputStream is = getClass ( ) . getClassLoader ( ) . getResourceAsStream ( createSchemaScriptPath ) ; String scriptContent = IOUtils . toString ( is ) ; Reader reader = new StringReader ( scriptContent . replaceAll ( CRAFTER_SCHEMA_NAME , studioConfiguration . getProperty ( DB_SCHEMA ) ) ) ; code_block = TryStatement ;  logger . info ( "Database schema created successfully." ) ; }
public void test() { try { sr . runScript ( reader ) ; } catch ( RuntimeSqlException e ) { LOG . warn ( "runScript failed: {}" , e . getMessage ( ) ) ; } }
public TransactionReceipt transfer ( Transaction transaction ) throws Throwable { TransactionBody txBody = com . hedera . services . legacy . proto . utils . CommonUtils . extractTransactionBody ( transaction ) ; Instant consensusTime = new Date ( ) . toInstant ( ) ; log . info ( "Transfer txBody: " + txBody ) ; TransactionRecord record = cryptoHandler . cryptoTransfer ( txBody , consensusTime ) ; log . info ( "Transfer record :: " + record ) ; Assert . assertNotNull ( record ) ; TransactionReceipt receipt = record . getReceipt ( ) ; return receipt ; }
public TransactionReceipt transfer ( Transaction transaction ) throws Throwable { log . info ( "\n-----------------------------------\ntransfer: request = " + com . hedera . services . legacy . proto . utils . CommonUtils . toReadableString ( transaction ) ) ; TransactionBody txBody = com . hedera . services . legacy . proto . utils . CommonUtils . extractTransactionBody ( transaction ) ; Instant consensusTime = new Date ( ) . toInstant ( ) ; TransactionRecord record = cryptoHandler . cryptoTransfer ( txBody , consensusTime ) ; Assert . assertNotNull ( record ) ; TransactionReceipt receipt = record . getReceipt ( ) ; log . info ( "Transaction received: " + record ) ; return receipt ; }
public void test() { try { MethodKey methodKey = new MethodKey ( MDRRuleServiceUtil . class , "addRule" , _addRuleParameterTypes0 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , ruleGroupId , nameMap , descriptionMap , type , typeSettings , serviceContext ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . mobile . device . rules . model . MDRRule ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public static IRealization selectRealization ( OLAPContext olapContext ) throws NoRealizationFoundException { ProjectManager prjMgr = ProjectManager . getInstance ( olapContext . olapSchema . getConfig ( ) ) ; String factTableName = olapContext . firstTableScan . getTableName ( ) ; String projectName = olapContext . olapSchema . getProjectName ( ) ; List < IRealization > realizations = Lists . newArrayList ( prjMgr . getRealizationsByTable ( projectName , factTableName ) ) ; RoutingRule . applyRules ( realizations , olapContext ) ; code_block = IfStatement ; logger . info ( "The realizations remaining: " ) ; logger . info ( RoutingRule . getPrintableText ( projectName , factTableName ) ) ; logger . info ( "The realization being chosen: " + realizations . get ( 0 ) . getName ( ) ) ; return realizations . get ( 0 ) ; }
public static IRealization selectRealization ( OLAPContext olapContext ) throws NoRealizationFoundException { ProjectManager prjMgr = ProjectManager . getInstance ( olapContext . olapSchema . getConfig ( ) ) ; String factTableName = olapContext . firstTableScan . getTableName ( ) ; String projectName = olapContext . olapSchema . getProjectName ( ) ; List < IRealization > realizations = Lists . newArrayList ( prjMgr . getRealizationsByTable ( projectName , factTableName ) ) ; logger . info ( "Find candidates by table " + factTableName + " and project=" + projectName + " : " + StringUtils . join ( realizations , "," ) ) ; RoutingRule . applyRules ( realizations , olapContext ) ; code_block = IfStatement ; logger . info ( RoutingRule . getPrintableText ( realizations ) ) ; logger . info ( "The realization being chosen: " + realizations . get ( 0 ) . getName ( ) ) ; logger . info ( "The realization being chosen: " + realizations . get ( 0 ) . getName ( ) ) ; return realizations . get ( 0 ) ; }
public static IRealization selectRealization ( OLAPContext olapContext ) throws NoRealizationFoundException { ProjectManager prjMgr = ProjectManager . getInstance ( olapContext . olapSchema . getConfig ( ) ) ; String factTableName = olapContext . firstTableScan . getTableName ( ) ; String projectName = olapContext . olapSchema . getProjectName ( ) ; List < IRealization > realizations = Lists . newArrayList ( prjMgr . getRealizationsByTable ( projectName , factTableName ) ) ; logger . info ( "Find candidates by table " + factTableName + " and project=" + projectName + " : " + StringUtils . join ( realizations , "," ) ) ; RoutingRule . applyRules ( realizations , olapContext ) ; code_block = IfStatement ; logger . info ( "The realizations remaining: " ) ; logger . info ( "The realization being chosen: " + realizations . get ( 0 ) . getName ( ) ) ; logger . info ( "The realization being chosen: " + realizations . get ( 0 ) . getName ( ) ) ; return realizations . get ( 0 ) ; }
public static IRealization selectRealization ( OLAPContext olapContext ) throws NoRealizationFoundException { ProjectManager prjMgr = ProjectManager . getInstance ( olapContext . olapSchema . getConfig ( ) ) ; String factTableName = olapContext . firstTableScan . getTableName ( ) ; String projectName = olapContext . olapSchema . getProjectName ( ) ; logger . info ( "Find candidates for project: " + factTableName ) ; List < IRealization > realizations = Lists . newArrayList ( prjMgr . getRealizationsByTable ( projectName , factTableName ) ) ; logger . info ( "Find candidates by table " + factTableName + " and project=" + projectName + " : " + StringUtils . join ( realizations , "," ) ) ; RoutingRule . applyRules ( realizations , olapContext ) ; code_block = IfStatement ; logger . info ( "The realizations remaining: " ) ; logger . info ( RoutingRule . getPrintableText ( realizations ) ) ; return realizations . get ( 0 ) ; }
@ Override protected void learnAxioms ( ) { logger . info ( "Iteration: " + pattern ) ; int modalDepth = MaximumModalDepthDetector . getMaxModalDepth ( pattern ) ; modalDepth ++ ; logger . info ( "Modal depth: " + modalDepth ) ; Model fragment = fragmentExtractor . extractFragment ( cls , modalDepth ) ; Set < OWLAxiom > instantiations = applyPattern ( pattern , dataFactory . getOWLClass ( IRI . create ( cls . toStringID ( ) ) ) , fragment ) ; code_block = ForStatement ; }
@ Override protected void learnAxioms ( ) { logger . info ( "Pattern: " + pattern ) ; int modalDepth = MaximumModalDepthDetector . getMaxModalDepth ( pattern ) ; modalDepth ++ ; Model fragment = fragmentExtractor . extractFragment ( cls , modalDepth ) ; Set < OWLAxiom > instantiations = applyPattern ( pattern , dataFactory . getOWLClass ( IRI . create ( cls . toStringID ( ) ) ) , fragment ) ; logger . info ( "Successfully instantiating " + instantiation . getClass ( ) . getSimpleName ( ) ) ; code_block = ForStatement ; }
public void test() { if ( rowIds . length == 0 ) { logger . error ( "parameter rowIds is empty, you should check it, just return a random one..." ) ; FutureResult < Vector [ ] > result = new FutureResult < > ( ) ; result . set ( new Vector [ 0 ] ) ; return result ; } }
public void test() { if ( ! notConverged . isEmpty ( ) ) { LOGGER . warn ( "Unable to convert to JSON key: " + notConverged . size ( ) ) ; } else { LOGGER . info ( "All {} key(s) on index {} have status {}" , converged . size ( ) , graphIndexName , status ) ; return new GraphIndexStatusReport ( true , graphIndexName , status , notConverged , converged , t . elapsed ( ) ) ; } }
public void test() { while ( iterator . hasNext ( ) ) { final MessageExt message = iterator . next ( ) ; String msgId = message . getMsgId ( ) ; LOG . info ( "Received message " + msgId ) ; iterator . remove ( ) ; } }
public void start ( ) throws Exception { Objects . nonNull ( config ) ; this . client = com . basho . riak . client . api . RiakClient . newClient ( config . getPort ( ) . intValue ( ) , config . getHosts ( ) ) ; Objects . nonNull ( client ) ; Objects . nonNull ( client . getRiakCluster ( ) ) ; assert ( client . getRiakCluster ( ) . getNodes ( ) . size ( ) > 0 ) ; LOG . info ( "Nodes : {}" , client . getRiakCluster ( ) . getNodes ( ) . size ( ) ) ; }
@ BeforeClass public static void start ( ) throws Exception { SubmarineConfiguration conf = SubmarineConfiguration . getInstance ( ) ; String serverHost = NetworkUtils . findAvailableHostAddress ( ) ; int serverPort = NetworkUtils . findRandomAvailablePortOnAllLocalInterfaces ( ) ; String clusterAdd = serverHost + ":" + serverPort ; conf . setClusterAddress ( clusterAdd ) ; startUp ( SubmarineServerClusterTest . class . getSimpleName ( ) ) ; Class clazz = ClusterClient . class ; Constructor constructor = null ; constructor = clazz . getDeclaredConstructor ( ) ; constructor . setAccessible ( true ) ; clusterClient = ( ClusterClient ) constructor . newInstance ( ) ; clusterClient . start ( SubmarineServerClusterTest . class . getSimpleName ( ) ) ; int wait = 0 ; code_block = WhileStatement ; LOG . info ( "Submarine server started" ) ; assertTrue ( "Can not start Submarine server!" , clusterClient . raftInitialized ( ) ) ; sleep ( 10000 ) ; }
public void test() { if ( maxLifetime > 0 ) { final long variance = maxLifetime > 10_000 ? ThreadLocalRandom . current ( ) . nextLong ( maxLifetime / 40 ) : 0 ; final long lifetime = maxLifetime - variance ; LOG . debug ( "Setting {} policies" , lifetime ) ; poolEntry . setFutureEol ( houseKeepingExecutorService . schedule ( new MaxLifetimeTask ( poolEntry ) , lifetime , MILLISECONDS ) ) ; } }
public void test() { if ( keepaliveTime > 0 ) { final long variance = ThreadLocalRandom . current ( ) . nextLong ( keepaliveTime / 10 ) ; final long heartbeatTime = keepaliveTime - variance ; poolEntry . setKeepalive ( houseKeepingExecutorService . scheduleWithFixedDelay ( new KeepaliveTask ( poolEntry ) , heartbeatTime , heartbeatTime , MILLISECONDS ) ) ; logger . info ( "Setting keepalive timer to " + poolEntry . getName ( ) ) ; } }
public void test() { if ( poolState == POOL_NORMAL ) { log . debug ( "pushing pool state" ) ; } }
@ Override public Double visit ( LessThanFilter lessThanFilter ) { int minBound = 9 - IntStream . rangeClosed ( 0 , 9 ) . filter ( i -> percentiles [ 9 - i ] < lessThanFilter . getValue ( ) . doubleValue ( ) ) . findFirst ( ) . orElse ( 0 ) ; final double result = ( ( double ) minBound + 1.0 ) / 10.0 ; log . debug ( "LessThanFilter: {}" , result ) ; return result ; }
public void test() { try { driver . stop ( ) ; } catch ( LensException e ) { log . warn ( "Error stopping driver" , e ) ; } }
public synchronized void stop ( ) { code_block = ForStatement ; logger . info ( "Stopping UDF service." ) ; drivers . clear ( ) ; udfStatusExpirySvc . shutdownNow ( ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "path=" + path ) ; logger . debug ( "extension=" + extension ) ; logger . debug ( "n=" + n ) ; logger . debug ( "to=" + to ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "path=" + path ) ; logger . debug ( "from=" + to ) ; logger . debug ( "to=" + to ) ; logger . debug ( "to=" + to ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "path=" + path ) ; logger . debug ( "extension=" + extension ) ; logger . debug ( "to=" + to ) ; logger . debug ( "from=" + to ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "path=" + path ) ; logger . debug ( "extension=" + extension ) ; logger . debug ( "n=" + n ) ; logger . debug ( "extension=" + extension ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { sessionMatchSpecs . set ( dao . getSessionMatchSpecs ( ) ) ; } catch ( RuntimeException e ) { log . warn ( "Could not load config from sessionMatchSpecs" , e ) ; } }
public void test() { if ( conf == null ) { LOG . error ( "No configuration file found!" ) ; return null ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( name != null ) { log . warn ( "Cannot close. Reason: " + name , e ) ; } else { log . warn ( "Cannot close. Reason: " + e . getMessage ( ) , e ) ; } }
public void test() { for ( ATTRIBUTE a : attribute2Transition2Double . keySet ( ) ) { logger . info ( "1<=>1 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_11 ) ) ; logger . info ( "2<=>2 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_22 ) ) ; logger . info ( "3<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_33 ) ) ; logger . info ( "1<=>2 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_12 ) ) ; logger . info ( "1<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_13 ) ) ; logger . info ( "2<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_23 ) ) ; logger . info ( "2<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_23 ) ) ; } }
public void test() { for ( ATTRIBUTE a : attribute2Transition2Double . keySet ( ) ) { logger . info ( "======={}=======" , a ) ; logger . info ( "1<=>2 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_22 ) ) ; logger . info ( "3<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_33 ) ) ; logger . info ( "1<=>2 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_12 ) ) ; logger . info ( "1<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_13 ) ) ; logger . info ( "2<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_23 ) ) ; logger . info ( "2<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_23 ) ) ; } }
public void test() { for ( ATTRIBUTE a : attribute2Transition2Double . keySet ( ) ) { logger . info ( "======={}=======" , a ) ; logger . info ( "1<=>1 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_11 ) ) ; logger . info ( "3<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_33 ) ) ; logger . info ( "1<=>2 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_12 ) ) ; logger . info ( "1<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_13 ) ) ; logger . info ( "2<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_23 ) ) ; logger . info ( "2<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_23 ) ) ; } }
public void test() { for ( ATTRIBUTE a : attribute2Transition2Double . keySet ( ) ) { logger . info ( "======={}=======" , a ) ; logger . info ( "1<=>1 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_11 ) ) ; logger . info ( "2<=>2 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_22 ) ) ; logger . info ( "1<=>2 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_12 ) ) ; logger . info ( "1<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_13 ) ) ; logger . info ( "2<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_23 ) ) ; logger . info ( "2<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_23 ) ) ; } }
public void test() { for ( ATTRIBUTE a : attribute2Transition2Double . keySet ( ) ) { logger . info ( "======={}=======" , a ) ; logger . info ( "1<=>1 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_11 ) ) ; logger . info ( "2<=>2 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_22 ) ) ; logger . info ( "3<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_33 ) ) ; logger . info ( "1<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_13 ) ) ; logger . info ( "2<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_23 ) ) ; logger . info ( "2<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_23 ) ) ; } }
public void test() { for ( ATTRIBUTE a : attribute2Transition2Double . keySet ( ) ) { logger . info ( "======={}=======" , a ) ; logger . info ( "1<=>1 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_11 ) ) ; logger . info ( "2<=>2 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_22 ) ) ; logger . info ( "3<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_33 ) ) ; logger . info ( "1<=>2 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_12 ) ) ; logger . info ( "2<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_23 ) ) ; logger . info ( "2<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_23 ) ) ; } }
public void test() { for ( ATTRIBUTE a : attribute2Transition2Double . keySet ( ) ) { logger . info ( "======={}=======" , a ) ; logger . info ( "1<=>1 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_11 ) ) ; logger . info ( "2<=>2 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_22 ) ) ; logger . info ( "3<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_33 ) ) ; logger . info ( "1<=>2 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_12 ) ) ; logger . info ( "1<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_13 ) ) ; logger . info ( "2<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_13 ) ) ; } }
public void test() { try { ByteArrayOutputStream output = new ByteArrayOutputStream ( ) ; writeXMLObject ( jaxbObject , output ) ; return readXMLObject ( new ByteArrayInputStream ( output . toByteArray ( ) ) , target ) ; } catch ( JAXBException e ) { LOGGER . error ( "Could not convert the xml object to JAXB object" , e ) ; } }
public void test() { try { com . liferay . commerce . inventory . model . CommerceInventoryWarehouse returnValue = CommerceInventoryWarehouseServiceUtil . addCommerceInventoryWarehouse ( externalReferenceCode , name , description , active , street1 , street2 , street3 , city , zip , commerceRegionCode , commerceCountryCode , latitude , longitude , serviceContext ) ; return com . liferay . commerce . inventory . model . CommerceInventoryWarehouseSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( span . getDuration ( ) < coreConfiguration . getSpanMinDuration ( ) . getMillis ( ) * 1000 ) { span . requestDiscarding ( ) ; logger . info ( "Discarding the span " + span + " because it exceeds the span threshold.  Terminating the span." ) ; } }
public void test() { try { return fs . listStatus ( new Path ( filePath ) ) ; } catch ( IOException e ) { logger . error ( "Get file list exception" , e ) ; throw new Exception ( "Get file list exception" , e ) ; } }
public void test() { for ( String cache : pools ) { code_block = IfStatement ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception exception ) { StringBundler sb = new StringBundler ( 17 ) ; sb . append ( "Something went wrong attempting to register service " ) ; sb . append ( "method {contextName=" ) ; sb . append ( contextName ) ; sb . append ( ",contextPath=" ) ; sb . append ( contextPath ) ; sb . append ( ",actionObject=" ) ; sb . append ( actionObject ) ; sb . append ( ",actionClass=" ) ; sb . append ( actionClass ) ; sb . append ( ",actionMethod=" ) ; sb . append ( actionMethod ) ; sb . append ( ",path=" ) ; sb . append ( path ) ; sb . append ( ",method=" ) ; sb . append ( method ) ; sb . append ( "} due to " ) ; sb . append ( exception . getMessage ( ) ) ; _log . error ( sb . toString ( ) , exception ) ; } }
@ Override public void execute ( ) { log . debug ( "starting" ) ; code_block = IfStatement ; log . debug ( "finished" ) ; }
@ Override public void execute ( ) { log . debug ( "starting" ) ; code_block = IfStatement ; log . debug ( "exiting" ) ; }
public void test() { try { DDMDataProviderOutputParametersSettings [ ] ddmDataProviderOutputParametersSettings = getDDMDataProviderOutputParametersSettings ( dataProviderInstanceId ) ; code_block = ForStatement ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; } }
public void test() { try { EncryptRequest req = new EncryptRequest ( ) . withKeyId ( kmsKeyArn ) . withEncryptionAlgorithm ( ASYMMETRIC_ALGORITHM ) . withPlaintext ( ByteBuffer . wrap ( tmp . getBytes ( ) ) ) ; ByteBuffer plainText = kmsClient . encrypt ( req ) . getCiphertextBlob ( ) ; return TO_DECRYPT_PREFIX + Base64 . getEncoder ( ) . encodeToString ( plainText . array ( ) ) + TO_DECRYPT_POSTFIX ; } catch ( RuntimeException e ) { log . error ( "Failed to encrypt KMS token" , e ) ; } }
public void test() { if ( logger . isTraceEnabled ( LogMarker . DM_VERBOSE ) ) { logger . trace ( LogMarker . DM_VERBOSE , "Processing {}" , this ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
@ Override protected void setUp ( ) throws Exception { code_block = IfStatement ; LOG . info ( "Setting up " + getClass ( ) . getSimpleName ( ) ) ; factory = createConnectionFactory ( bindAddress ) ; managementConnection = factory . createConnection ( ) ; managementSession = managementConnection . createSession ( false , Session . AUTO_ACKNOWLEDGE ) ; Destination startDestination = createDestination ( managementSession , getClass ( ) + ".start" ) ; Destination endDestination = createDestination ( managementSession , getClass ( ) + ".end" ) ; controller = new LoadController ( "Controller" , factory ) ; controller . setBatchSize ( batchSize ) ; controller . setNumberOfBatches ( numberOfBatches ) ; controller . setDeliveryMode ( deliveryMode ) ; controller . setConnectionPerMessage ( connectionPerMessage ) ; controller . setStartDestination ( startDestination ) ; controller . setNextDestination ( endDestination ) ; controller . setTimeout ( timeout ) ; clients = new LoadClient [ numberOfClients ] ; code_block = ForStatement ; super . setUp ( ) ; }
public void test() { try { jdbcTemplate . execute ( sql ) ; } catch ( BadSqlGrammarException ex ) { logger . error ( ex . getMessage ( ) ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { String userName = getUserName ( request . getAllHeaders ( ) ) ; reportIdDir = getDatasetBaseLocation ( ReportGenerationApp . REPORT_FILESET ) . append ( userName ) . append ( reportId ) ; code_block = IfStatement ; String shareId = encodeShareId ( new ReportIdentifier ( userName , reportId ) ) ; responder . sendJson ( 200 , new ShareId ( shareId ) , ShareId . class , GSON ) ; } catch ( IOException | GeneralSecurityException e ) { LOG . error ( "Failed to read report with id {}" , reportId , e ) ; responder . sendError ( 500 , String . format ( "Failed to read report with id %s because of error: %s" , reportId , e . getMessage ( ) ) ) ; return ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { log . debug ( "Starting Citrus before suite lifecycle" ) ; citrusInstance . get ( ) . beforeSuite ( configurationInstance . get ( ) . getSuiteName ( ) ) ; } catch ( Exception e ) { log . error ( CitrusRuntime . class . getSimpleName ( ) , e ) ; throw e ; } }
public void test() { try { tm ( 0 ) . commit ( ) ; assert false : "Exception expected" ; } catch ( Exception e ) { log . info ( e ) ; } }
public void test() { try { final String filename = args . getJsonDump ( ) ; final JsonDumper jsonDumper = new JsonDumper ( filename , args . getLanguages ( ) , args . getExtraTags ( ) ) ; NominatimConnector nominatimConnector = new NominatimConnector ( args . getHost ( ) , args . getPort ( ) , args . getDatabase ( ) , args . getUser ( ) , args . getPassword ( ) ) ; nominatimConnector . setImporter ( jsonDumper ) ; nominatimConnector . readEntireDatabase ( args . getCountryCodes ( ) . split ( "," ) ) ; log . info ( "nominalicated: {}" , filename ) ; } catch ( FileNotFoundException e ) { log . error ( "cannot create dump" , e ) ; } }
public void test() { try { final String filename = args . getJsonDump ( ) ; final JsonDumper jsonDumper = new JsonDumper ( filename , args . getLanguages ( ) , args . getExtraTags ( ) ) ; NominatimConnector nominatimConnector = new NominatimConnector ( args . getHost ( ) , args . getPort ( ) , args . getDatabase ( ) , args . getUser ( ) , args . getPassword ( ) ) ; nominatimConnector . setImporter ( jsonDumper ) ; nominatimConnector . readEntireDatabase ( args . getCountryCodes ( ) . split ( "," ) ) ; log . info ( "json dump was created: " + filename ) ; } catch ( FileNotFoundException e ) { log . warn ( "unable to find json dump" , e ) ; } }
public void test() { if ( lockImpl == null ) { log . error ( "Cannot acquire lock with name '{}'" , name ) ; future . complete ( Boolean . TRUE ) ; return future ; } }
public void test() { try { String body = issueComment . getBody ( ) ; code_block = IfStatement ; } catch ( IOException ex ) { LOG . error ( ex . getMessage ( ) , ex ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Refreshing." ) ; } }
public void test() { if ( trcResult . getChannel ( ) . waitForChannelToReachAgi ( 30 , TimeUnit . SECONDS ) ) { logger . info ( "Call never reached agi" ) ; } else { logger . error ( "Call never reached agi" ) ; } }
public void test() { if ( trcResult . isSuccess ( ) ) { code_block = IfStatement ; } else { _logger . info ( String . format ( "failed to delete card %s from container %s" , account , containerId ) ) ; } }
@ RequestMapping ( "/execute" ) public @ ResponseBody String execute ( @ RequestParam ( value = "experimentData" ) String experimentData ) { Object obj = JSONValue . parse ( experimentData ) ; JSONObject configuration = ( JSONObject ) obj ; String typeString = ( String ) configuration . get ( "type" ) ; ExperimentType type = null ; code_block = TryStatement ;  String matching = ( String ) configuration . get ( "matching" ) ; JSONArray jsonAnnotators = ( JSONArray ) configuration . get ( "annotator" ) ; String [ ] annotators = new String [ jsonAnnotators . size ( ) ] ; code_block = ForStatement ; JSONArray jsonDataset = ( JSONArray ) configuration . get ( "dataset" ) ; String [ ] datasets = new String [ jsonDataset . size ( ) ] ; code_block = ForStatement ; ExperimentTaskConfiguration [ ] configs = new ExperimentTaskConfiguration [ annotators . length * datasets . length ] ; int count = 0 ; code_block = ForStatement ; String experimentId = IDCreator . getInstance ( ) . createID ( ) ; LOGGER . info ( "Running ExperimentTask with {} documents" , count ) ; Experimenter exp = new Experimenter ( overseer , dao , globalRetriever , evFactory , configs , experimentId ) ; exp . setAnnotatorOutputWriter ( annotatorOutputWriter ) ; exp . run ( ) ; return experimentId ; }
public void test() { try { type = ExperimentType . valueOf ( typeString ) ; } catch ( IllegalArgumentException e ) { LOGGER . warn ( "Got a invalid ExperimentType value: {}" , typeString ) ; return null ; } }
public void test() { try { logger . info ( "parsing database" ) ; model = MolgenisModelParser . parseDbSchema ( options . model_database ) ; code_block = ForStatement ; logger . debug ( "read: " + model ) ; MolgenisModelValidator . validate ( model , options ) ; logger . info ( "parsing ui-schema" ) ; model = MolgenisModelParser . parseUiSchema ( options . path + options . model_userinterface , model ) ; MolgenisModelValidator . validateUI ( model , options ) ; logger . debug ( "validated: " + model ) ; } catch ( MolgenisModelException e ) { logger . error ( "Parsing failed: " + e . getMessage ( ) ) ; e . printStackTrace ( ) ; throw e ; } }
public void test() { try { logger . info ( "parsing db-schema from " + options . model_database ) ; model = MolgenisModelParser . parseDbSchema ( options . model_database ) ; code_block = ForStatement ; logger . debug ( "parsing model" ) ; MolgenisModelValidator . validate ( model , options ) ; logger . info ( "parsing ui-schema" ) ; model = MolgenisModelParser . parseUiSchema ( options . path + options . model_userinterface , model ) ; MolgenisModelValidator . validateUI ( model , options ) ; logger . debug ( "validated: " + model ) ; } catch ( MolgenisModelException e ) { logger . error ( "Parsing failed: " + e . getMessage ( ) ) ; e . printStackTrace ( ) ; throw e ; } }
public void test() { try { logger . info ( "parsing db-schema from " + options . model_database ) ; model = MolgenisModelParser . parseDbSchema ( options . model_database ) ; code_block = ForStatement ; logger . debug ( "read: " + model ) ; MolgenisModelValidator . validate ( model , options ) ; logger . info ( "parsing ui schema from " + options . path + options . model_userinterface ) ; model = MolgenisModelParser . parseUiSchema ( options . path + options . model_userinterface , model ) ; MolgenisModelValidator . validateUI ( model , options ) ; logger . debug ( "validated: " + model ) ; } catch ( MolgenisModelException e ) { logger . error ( "Parsing failed: " + e . getMessage ( ) ) ; e . printStackTrace ( ) ; throw e ; } }
public void test() { try { logger . info ( "parsing db-schema from " + options . model_database ) ; model = MolgenisModelParser . parseDbSchema ( options . model_database ) ; code_block = ForStatement ; logger . debug ( "read: " + model ) ; MolgenisModelValidator . validate ( model , options ) ; logger . info ( "parsing ui-schema" ) ; model = MolgenisModelParser . parseUiSchema ( options . path + options . model_userinterface , model ) ; logger . debug ( "validating UI" ) ; MolgenisModelValidator . validateUI ( model , options ) ; } catch ( MolgenisModelException e ) { logger . error ( "Parsing failed: " + e . getMessage ( ) ) ; e . printStackTrace ( ) ; throw e ; } }
public void test() { try { logger . info ( "parsing db-schema from " + options . model_database ) ; model = MolgenisModelParser . parseDbSchema ( options . model_database ) ; code_block = ForStatement ; logger . debug ( "read: " + model ) ; MolgenisModelValidator . validate ( model , options ) ; logger . info ( "parsing ui-schema" ) ; model = MolgenisModelParser . parseUiSchema ( options . path + options . model_userinterface , model ) ; logger . debug ( "validating UI" ) ; MolgenisModelValidator . validateUI ( model , options ) ; logger . debug ( "validated: " + model ) ; } catch ( MolgenisModelException e ) { e . printStackTrace ( ) ; throw e ; } }
public void test() { if ( customLog . isPresent ( ) ) { customLog . get ( ) . accept ( log , records ) ; } else { LOG . error ( "Custom log not found." ) ; } }
@ Test public void testReaderRead ( ) throws Exception { BeanConfig config = new BeanConfig ( ) ; config . setHost ( "localhost:8080" ) ; config . setSchemes ( new String [ ] code_block = "" ; ) ; config . setBasePath ( "/api" ) ; config . setTitle ( "Day" ) ; config . setLicense ( "Apache 2.0" ) ; config . setLicenseUrl ( "http://www.apache.org/licenses/LICENSE-2.0.html" ) ; config . setVersion ( "2.0" ) ; RestOpenApiReader reader = new RestOpenApiReader ( ) ; OasDocument openApi = reader . read ( context , context . getRestDefinitions ( ) , null , config , context . getName ( ) , new DefaultClassResolver ( ) ) ; assertNotNull ( openApi ) ; ObjectMapper mapper = new ObjectMapper ( ) ; mapper . enable ( SerializationFeature . INDENT_OUTPUT ) ; mapper . setSerializationInclusion ( JsonInclude . Include . NON_NULL ) ; Object dump = Library . writeNode ( openApi ) ; String json = mapper . writeValueAsString ( dump ) ; log . info ( json ) ; assertTrue ( json . contains ( "\"host\" : \"localhost:8080\"" ) ) ; assertTrue ( json . contains ( "\"default\" : \"friday\"" ) ) ; assertTrue ( json . contains ( "\"enum\" : [ \"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\" ]" ) ) ; assertTrue ( json . contains ( "\"$ref\" : \"/definitions/DayResponse\"" ) ) ; assertTrue ( json . contains ( "\"format\" : \"org.apache.camel.openapi.DayResponse\"" ) ) ; assertTrue ( json . contains ( "\"X-Rate-Limit-Limit\" : {" ) ) ; assertTrue ( json . contains ( "\"description\" : \"The number of allowed requests in the current period\"" ) ) ; context . stop ( ) ; }
public void test() { if ( resp == null ) { logger . error ( "getKVPessimisticRollbackMethod failed without a cause" ) ; regionManager . onRequestFail ( region ) ; bo . doBackOff ( BoRegionMiss , new TiClientInternalException ( "getKVPessimisticRollbackMethod failed without a cause" ) ) ; continue ; } }
public void test() { if ( reportId < 0 ) { _log . error ( reportId ) ; return null ; } }
public void test() { try { ResponseEntity < Resource > responseEntity = restTemplate . exchange ( baseUrl + "report/" + reportId , HttpMethod . GET , new HttpEntity < > ( headers ) , Resource . class ) ; return responseEntity . getBody ( ) . getInputStream ( ) ; } catch ( RestClientException | IOException e ) { LOGGER . error ( "Could not retrieve report" , e ) ; return null ; } }
public void test() { try { jdbcTemplate . update ( "DELETE FROM auth WHERE auth LIKE 'SSO %' AND object_type=9" ) ; jobsAdded . incrementAndGet ( ) ; } catch ( RuntimeException e ) { LOGGER . error ( "Failed to delete SSO auth lines from auth index table" , e ) ; } }
public void test() { try { listener . gotUserLists ( lists ) ; } catch ( Exception e ) { logger . warn ( "Exception at getUserLists" , e ) ; } }
public void doGet ( HttpServletRequest request , HttpServletResponse response ) throws ServletException , IOException { logger . debug ( "Request Path Info: " + request . getPathInfo ( ) ) ; logger . debug ( "Request Param: " + request . getQueryString ( ) ) ; String jsonOutput ; String inGeomWKT = request . getParameter ( "geometry" ) ; String fromSRID = request . getParameter ( "srid" ) ; logger . debug ( "GeomID: " + fromSRID ) ; JSONObject obj = new JSONObject ( ) ; JSONArray arr = new JSONArray ( ) ; code_block = TryStatement ;  jsonOutput = arr . toString ( ) ; PrintWriter pw = response . getWriter ( ) ; response . setContentType ( MimeType . APPLICATION_JSON ) ; pw . write ( jsonOutput ) ; return ; }
public void doGet ( HttpServletRequest request , HttpServletResponse response ) throws ServletException , IOException { logger . debug ( "Request URL: " + request . getRequestURI ( ) ) ; logger . debug ( "Request Param: " + request . getQueryString ( ) ) ; String jsonOutput ; String inGeomWKT = request . getParameter ( "geometry" ) ; String fromSRID = request . getParameter ( "srid" ) ; logger . debug ( "GeomID: " + fromSRID ) ; JSONObject obj = new JSONObject ( ) ; JSONArray arr = new JSONArray ( ) ; code_block = TryStatement ;  jsonOutput = arr . toString ( ) ; PrintWriter pw = response . getWriter ( ) ; response . setContentType ( MimeType . APPLICATION_JSON ) ; pw . write ( jsonOutput ) ; return ; }
public void doGet ( HttpServletRequest request , HttpServletResponse response ) throws ServletException , IOException { logger . debug ( "Request URL: " + request . getRequestURI ( ) ) ; logger . debug ( "Request Path Info: " + request . getPathInfo ( ) ) ; String jsonOutput ; String inGeomWKT = request . getParameter ( "geometry" ) ; String fromSRID = request . getParameter ( "srid" ) ; logger . debug ( "GeomID: " + fromSRID ) ; JSONObject obj = new JSONObject ( ) ; JSONArray arr = new JSONArray ( ) ; code_block = TryStatement ;  jsonOutput = arr . toString ( ) ; PrintWriter pw = response . getWriter ( ) ; response . setContentType ( MimeType . APPLICATION_JSON ) ; pw . write ( jsonOutput ) ; return ; }
public void test() { try { code_block = IfStatement ; } catch ( JSONException e ) { LOGGER . error ( "Caught exception while processing JSON response" , e ) ; } }
@ Override public void save ( Note note , AuthenticationInfo subject ) throws IOException { code_block = IfStatement ; String jsonNote = GSON . toJson ( note ) ; String token = getUserToken ( subject . getUser ( ) ) ; LOG . info ( "ZeppelinHub REST API saving note {}" , note . getId ( ) ) ; restApiClient . put ( token , jsonNote ) ; }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { URL log4jurl = getURL ( Log4jURL ) ; DOMConfigurator . configure ( log4jurl ) ; } catch ( Exception e ) { LOG . error ( "Cannot read Log4j: " + Log4jURL ) ; return false ; } }
public void test() { try { com . liferay . portal . kernel . model . Layout returnValue = LayoutServiceUtil . fetchLayout ( groupId , privateLayout , layoutId ) ; return com . liferay . portal . kernel . model . LayoutSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { for ( SQLException ex = e ; ex != null ; ex = ex . getNextException ( ) ) { WGLOG . error ( ex , "E04004" , profile . getResourceName ( ) , script . getName ( ) , script . getColumnNames ( ) ) ; } }
public void test() { try { customizer . customize ( cdkAtom , cmlAtom ) ; } catch ( Exception exception ) { logger . error ( "Could not customize Customizer " + customizer . getClass ( ) . getName ( ) ) ; logger . debug ( exception ) ; } }
public void test() { try { customizer . customize ( cdkAtom , cmlAtom ) ; } catch ( Exception exception ) { logger . error ( "Error while customizing CML output with customizer: " , exception ) ; logger . debug ( exception ) ; } }
public void test() { try { MantaHttpHeaders headers = new MantaHttpHeaders ( currentResponse . getAllHeaders ( ) ) ; e . setResponseHeaders ( headers ) ; } catch ( RuntimeException re ) { LOG . error ( re . getMessage ( ) , re ) ; } }
@ Override public void removeProcedureForOffering ( String offering , String procedure ) { CacheValidation . notNullOrEmpty ( OFFERING , offering ) ; CacheValidation . notNullOrEmpty ( PROCEDURE , procedure ) ; LOG . trace ( "Removing procedure {} from offering {}" , procedure , offering ) ; this . proceduresForOfferings . getOrDefault ( offering , Collections . emptySet ( ) ) . remove ( procedure ) ; }
public void test() { { log . info ( Color . GREEN + "Transfer_4_1 : missing column transfer_name" + Color . NORMAL ) ; Context context = new Context ( ) ; CheckPointReport result = verifyValidation ( log , context , "transfer_4_1" , GTFS_1_GTFS_Common_15 , SEVERITY . ERROR , RESULT . NOK , true ) ; Assert . assertEquals ( result . getCheckPointErrorCount ( ) , 5 , "detail count" ) ; int i = 2 ; code_block = ForStatement ; } }
public void test() { { logger . debug ( "New pingMeasurement entry received" ) ; final QoSIntraPingMeasurementResponseDTO response = pingService . getMedianIntraPingMeasurement ( Utilities . convertStringToQoSMeasurementAttribute ( attribute ) ) ; logger . debug ( "PingMeasurement entry successfully retrieved" ) ; return response ; } }
public void test() { { logger . debug ( "New getIntraPingMedianMeasurement get request recieved with attribute: {}" , attribute ) ; final QoSIntraPingMeasurementResponseDTO response = pingService . getMedianIntraPingMeasurement ( Utilities . convertStringToQoSMeasurementAttribute ( attribute ) ) ; logger . debug ( "Got response: {}" , response ) ; return response ; } }
public void test() { if ( INFO . getInt ( 0 ) < 0 ) { throw new Error ( "Parameter " + INFO . getInt ( 0 ) + " to gesvd() was not valid" ) ; } else-if ( INFO . getInt ( 0 ) > 0 ) { logger . info ( "Parameter " + INFO . getInt ( 0 ) + " to gesvd() was done." ) ; } }
@ Override public void initialize ( ) { code_block = IfStatement ; code_block = IfStatement ; port = ( String ) getConfig ( ) . get ( PORT ) ; sleep = 250 ; interval = 5000 ; super . initialize ( ) ; log . info ( "Port: " + port ) ; }
public void test() { for ( String cache : pools ) { code_block = IfStatement ; } }
public void test() { for ( String cache : pools ) { code_block = IfStatement ; } }
public void test() { if ( throwable instanceof FileNotFoundException ) { code_block = IfStatement ; } else { logger . warn ( message , throwable ) ; } }
@ Test public void testDeserialize_out ( ) throws Exception { Optional < ? extends RpcDefinition > loadRpc = ConverterUtils . loadRpc ( this . effectiveModelContext , SIMPLE_IO_RPC_QNAME ) ; String loadIoRpcOut = loadResourceAsString ( "input-output-rpc-out.json" ) ; NormalizedNode < ? , ? > deserializeRpc = bindingSerializer . deserialize ( loadRpc . get ( ) , new StringReader ( loadIoRpcOut ) ) ; Assert . assertNotNull ( deserializeRpc ) ; LOG . info ( deserializeRpc . toString ( ) ) ; }
public void test() { try { MetricSlice currentSlice = MetricSlice . from ( me . getId ( ) , startTime , endTime , me . getFilters ( ) ) ; DataFrame df = this . aggregationLoader . loadAggregate ( currentSlice , Collections . < String > emptyList ( ) , - 1 ) ; anomaly . setAvgCurrentVal ( df . getDouble ( DataFrame . COL_VALUE , 0 ) ) ; } catch ( Exception e ) { LOG . error ( "Failed to load metric values" , e ) ; anomaly . setAvgCurrentVal ( Double . NaN ) ; } }
public void test() { if ( k instanceof ChukwaArchiveKey && v instanceof ChunkImpl ) { ChunkImpl value = ( ChunkImpl ) v ; Report xtrReport = Report . createFromString ( new String ( value . getData ( ) ) ) ; code_block = TryStatement ;  t = new Text ( value . getData ( ) ) ; } else-if ( k instanceof ChukwaRecordKey && v instanceof ChukwaRecord ) { ChukwaRecord value = ( ChukwaRecord ) v ; Report xtrReport = Report . createFromString ( value . getValue ( Record . bodyField ) ) ; bw = new BytesWritable ( xtrReport . getMetadata ( ) . getTaskId ( ) . get ( ) ) ; t = new Text ( value . getValue ( Record . bodyField ) ) ; } else { LOG . error ( "Unsupported report {}" , k ) ; return ; } }
public void test() { try { User user = userDAO . findByLoginName ( loginName ) ; code_block = ForStatement ; } catch ( InstanceNotFoundException e ) { LOG . info ( "User " + loginName + " not found in database." ) ; } }
public void test() { try { packetInfoList = PcapParser . parse ( input . getBinary ( 0 ) ) ; code_block = IfStatement ; } catch ( Exception e ) { LOG . error ( "Error parsing packet!" , e ) ; collector . fail ( input ) ; e . printStackTrace ( ) ; JSONObject error = ErrorGenerator . generateErrorMessage ( "Alerts problem: " + input . getBinary ( 0 ) , e ) ; collector . emit ( "error" , new Values ( error ) ) ; return ; } }
public void test() { try { hikari . setMetricsTrackerFactory ( new MicrometerMetricsTrackerFactory ( this . registry ) ) ; } catch ( Exception ex ) { LOGGER . error ( ex . getMessage ( ) , ex ) ; } }
public void test() { if ( StringUtils . hasText ( this . prefix ) && ! path . startsWith ( this . prefix ) ) { logger . warn ( "Invalid path: {}" , path ) ; return Mono . empty ( ) ; } }
public void test() { if ( this . logger . isDebugEnabled ( ) ) { this . logger . debug ( "Received request from [" + destination + "]" ) ; } }
public void test() { try { BayesParameters params = new BayesParameters ( job . get ( "bayes.parameters" , "" ) ) ; log . info ( "{}" , params . print ( ) ) ; Algorithm algorithm ; Datastore datastore ; code_block = IfStatement ; classifier = new ClassifierContext ( algorithm , datastore ) ; classifier . initialize ( ) ; defaultCategory = params . get ( "defaultCat" ) ; gramSize = params . getGramSize ( ) ; log . info ( "Bayes Name: " + defaultCategory ) ; } catch ( IOException ex ) { log . warn ( ex . toString ( ) , ex ) ; } catch ( InvalidDatastoreException e ) { log . error ( e . toString ( ) , e ) ; } }
public void test() { try { BayesParameters params = new BayesParameters ( job . get ( "bayes.parameters" , "" ) ) ; log . info ( "Bayes Parameter {}" , params . print ( ) ) ; Algorithm algorithm ; Datastore datastore ; code_block = IfStatement ; classifier = new ClassifierContext ( algorithm , datastore ) ; classifier . initialize ( ) ; defaultCategory = params . get ( "defaultCat" ) ; gramSize = params . getGramSize ( ) ; log . info ( "Bayes Parameter {}" , params . print ( ) ) ; } catch ( IOException ex ) { log . warn ( ex . toString ( ) , ex ) ; } catch ( InvalidDatastoreException e ) { log . error ( e . toString ( ) , e ) ; } }
public void test() { if ( "bayes" . equalsIgnoreCase ( params . get ( "classifierType" ) ) ) { log . info ( "Testing Bayes Classifier" ) ; algorithm = new BayesAlgorithm ( ) ; datastore = new InMemoryBayesDatastore ( params ) ; } else-if ( "cbayes" . equalsIgnoreCase ( params . get ( "classifierType" ) ) ) { log . info ( "Testing Complementary Bayes Classifier" ) ; algorithm = new CBayesAlgorithm ( ) ; datastore = new InMemoryBayesDatastore ( params ) ; } else { throw new IllegalArgumentException ( "Unrecognized classifier type: " + params . get ( "classifierType" ) ) ; } }
public void test() { if ( "bayes" . equalsIgnoreCase ( params . get ( "classifierType" ) ) ) { log . info ( "Testing Bayes Classifier" ) ; algorithm = new BayesAlgorithm ( ) ; datastore = new InMemoryBayesDatastore ( params ) ; } else-if ( "cbayes" . equalsIgnoreCase ( params . get ( "classifierType" ) ) ) { log . info ( "Testing CBayes Classifier" ) ; algorithm = new CBayesAlgorithm ( ) ; datastore = new InMemoryBayesDatastore ( params ) ; } else { throw new IllegalArgumentException ( "Unrecognized classifier type: " + params . get ( "classifierType" ) ) ; } }
public void test() { try { BayesParameters params = new BayesParameters ( job . get ( "bayes.parameters" , "" ) ) ; log . info ( "Bayes Parameter {}" , params . print ( ) ) ; log . info ( "{}" , params . print ( ) ) ; Algorithm algorithm ; Datastore datastore ; code_block = IfStatement ; classifier = new ClassifierContext ( algorithm , datastore ) ; classifier . initialize ( ) ; defaultCategory = params . get ( "defaultCat" ) ; gramSize = params . getGramSize ( ) ; } catch ( IOException ex ) { log . warn ( ex . toString ( ) , ex ) ; } catch ( InvalidDatastoreException e ) { log . error ( e . toString ( ) , e ) ; } }
public void test() { try { BayesParameters params = new BayesParameters ( job . get ( "bayes.parameters" , "" ) ) ; log . info ( "Bayes Parameter {}" , params . print ( ) ) ; log . info ( "{}" , params . print ( ) ) ; Algorithm algorithm ; Datastore datastore ; code_block = IfStatement ; classifier = new ClassifierContext ( algorithm , datastore ) ; classifier . initialize ( ) ; defaultCategory = params . get ( "defaultCat" ) ; gramSize = params . getGramSize ( ) ; } catch ( IOException ex ) { log . warn ( ex . toString ( ) , ex ) ; } catch ( InvalidDatastoreException e ) { log . error ( e . toString ( ) , e ) ; } }
public void test() { try { changeZkState ( States . CLOSED ) ; } catch ( IOException e ) { LOG . error ( "Error while closing ZooKeeper client" , e ) ; } }
protected void initialize ( ) { atomManager = createAtomManager ( db ) ; log . debug ( "Atom manager initialization complete." ) ; reasoner = createReasoner ( ) ; termStore = createTermStore ( ) ; groundRuleStore = createGroundRuleStore ( ) ; termGenerator = createTermGenerator ( ) ; termStore . ensureVariableCapacity ( atomManager . getCachedRVACount ( ) ) ; log . info ( "Initialized" ) ; completeInitialize ( ) ; }
protected void initialize ( ) { log . debug ( "Creating persisted atom mannager." ) ; atomManager = createAtomManager ( db ) ; reasoner = createReasoner ( ) ; termStore = createTermStore ( ) ; groundRuleStore = createGroundRuleStore ( ) ; termGenerator = createTermGenerator ( ) ; termStore . ensureVariableCapacity ( atomManager . getCachedRVACount ( ) ) ; completeInitialize ( ) ; log . debug ( "Completed." ) ; }
public void test() { try { exporter . writeCsar ( entryServiceTemplate . get ( ) , out , exportConfiguration ) ; } catch ( RepositoryCorruptException | InterruptedException | AccountabilityException | ExecutionException e ) { LOG . error ( "Failed to export CSAR: {}" , e . getMessage ( ) ) ; throw new IOException ( "Failed to export CSAR" , e ) ; } }
public void stopForcefully ( ) { stopRequested . set ( true ) ; log . info ( "Stopped" ) ; runThread . interrupt ( ) ; }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
@ Test public void testBlackListMultiValueIncluded ( ) throws Exception { log . info ( "------  testBlackListMultiValueIncluded  ------" ) ; String cont = "'europe'" ; String state = "'mississippi'" ; code_block = ForStatement ; }
public void test() { if ( LOG . isWarnEnabled ( ) ) { LOG . warn ( "Unhandled exception: " , e ) ; } }
public void test() { if ( s_logger . isDebugEnabled ( ) ) { s_logger . debug ( log ( seq , "Unable to move " + req . toString ( ) ) ) ; } }
public void test() { try { String result = connectController . getConfigManagementService ( ) . putConnectorConfig ( connectorName , configs ) ; code_block = IfStatement ; } catch ( Exception e ) { log . error ( "Failed to put connector configuration" , e ) ; context . result ( "failed" ) ; } }
public void test() { if ( ! exec . isTerminated ( ) ) { LOG . warn ( "Executor is still running. Shutting down." ) ; } }
public void test() { for ( PropertySource < ? > propertySource : propertySources ) { logger . info ( "Environment order " + propertySource . getName ( ) ) ; } }
public void attachDirty ( ModZobjBstMassMitarb instance ) { log . debug ( "attaching dirty ModZobjBstMassMitarb instance" ) ; code_block = TryStatement ;  }
public void test() { try { getSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { getSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { result = metaServerConsoleServiceManagerWrapper . get ( dc ) . changePrimaryDcCheck ( cluster , shard , newPrimaryDc ) ; future ( ) . setSuccess ( result ) ; } catch ( Exception e ) { getLogger ( ) . error ( "[MigrateDcCheck][Failed]{}-{}-{}-{}" , cluster , shard , dc , newPrimaryDc ) ; future ( ) . setFailure ( e ) ; } }
public void test() { if ( refreshSchema ) { logger . error ( "Error during schema refresh ({}). The schema from Cluster.getMetadata() might appear stale. Asynchronously submitting job to fix." , e . getMessage ( ) ) ; submitSchemaRefresh ( targetType , targetKeyspace , targetName ) ; } else { logger . warn ( "Error during schema refresh." ) ; } }
public void test() { try { client . startElection ( request , handler ) ; } catch ( Exception e ) { log . error ( "Could not start election" , e ) ; } }
public void test() { if ( line . contains ( ".." ) ) { log . info ( "{} has some.." , LOG_PREFIX ) ; } else { String fullPath = TARGET_DIR + File . separator + line ; File file = new File ( fullPath ) ; log . info ( "{} deleting file  {}" , LOG_PREFIX , fullPath ) ; code_block = IfStatement ; } }
public void test() { if ( file . delete ( ) ) { log . debug ( "{} deleted file {}" , LOG_PREFIX , fullPath ) ; } else { log . error ( "{} could not delete file {}" , LOG_PREFIX , fullPath ) ; } }
public void test() { if ( file . delete ( ) ) { log . info ( "{} successfully deleted file {}" , LOG_PREFIX , fullPath ) ; } else { log . warn ( "{} failed to delete file {}" , LOG_PREFIX , fullPath ) ; } }
public void test() { try ( BufferedReader br = new BufferedReader ( new InputStreamReader ( new FileInputStream ( deleteFileListName ) , StandardCharsets . UTF_8 ) ) ; ) { String line ; code_block = WhileStatement ; } catch ( SecurityException | IOException e ) { LOG . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( ! validate ( key , authData ) ) { keyStr = new String ( key , UTF_8 ) ; LOG . error ( "Invalid auth data: {}" , key ) ; return KeeperException . Code . AUTHFAILED ; } }
@ Override public KeeperException . Code handleAuthentication ( ServerObjs serverObjs , byte [ ] authData ) { LOG . debug ( "handleAuthentication" ) ; byte [ ] key = getKey ( serverObjs . getZks ( ) ) ; String authStr = new String ( authData , UTF_8 ) ; String keyStr = "" ; code_block = IfStatement ; serverObjs . getCnxn ( ) . addAuthInfo ( new Id ( getScheme ( ) , keyStr ) ) ; return KeeperException . Code . OK ; }
public boolean checkPermission ( String permission ) { logger . info ( permission ) ; return true ; }
@ Override public Point doGetLocation ( ) { Point point = element . getLocation ( ) ; LOGGER . debug ( MessageFormat . format ( Messages . getInstance ( ) . getString ( "GenerateLocation" ) , element . getLocation ( ) . toString ( ) ) ) ; return point ; }
@ GET @ Produces ( MediaType . APPLICATION_JSON + ";charset=UTF-8" ) @ Path ( "{personID}/{resourceID}" ) public Response < Resource > getResourceFromPersonById ( @ PathParam ( "said" ) String said , @ PathParam ( "personID" ) String personID , @ PathParam ( "resourceID" ) String resourceID ) { logger . info ( "called API method: GET /dime/rest/" + said + "/resource/" + personID + "/" + resourceID ) ; Data < Resource > data ; code_block = TryStatement ;  return Response . ok ( data ) ; }
public void test() { try { SchemaRegistry < Schema > registry = ( SchemaRegistry < Schema > ) Class . forName ( props . getProperty ( KafkaAvroMessageEncoder . KAFKA_MESSAGE_CODER_SCHEMA_REGISTRY_CLASS ) ) . newInstance ( ) ; log . info ( "Underlying schema registry for topic: " + topicName + " is: " + registry ) ; registry . init ( props ) ; this . registry = new CachedSchemaRegistry < Schema > ( registry , props ) ; this . latestSchema = registry . getLatestSchemaByTopic ( topicName ) . getSchema ( ) ; } catch ( Exception e ) { log . error ( "Unable to get the latest schema for topic: " + topicName , e ) ; throw new MessageDecoderException ( e ) ; } }
public void test() { try { SchemaRegistry < Schema > registry = ( SchemaRegistry < Schema > ) Class . forName ( props . getProperty ( KafkaAvroMessageEncoder . KAFKA_MESSAGE_CODER_SCHEMA_REGISTRY_CLASS ) ) . newInstance ( ) ; log . info ( "Prop " + KafkaAvroMessageEncoder . KAFKA_MESSAGE_CODER_SCHEMA_REGISTRY_CLASS + " is: " + props . getProperty ( KafkaAvroMessageEncoder . KAFKA_MESSAGE_CODER_SCHEMA_REGISTRY_CLASS ) ) ; registry . init ( props ) ; this . registry = new CachedSchemaRegistry < Schema > ( registry , props ) ; this . latestSchema = registry . getLatestSchemaByTopic ( topicName ) . getSchema ( ) ; } catch ( Exception e ) { log . error ( "Error getting the latest Schema" , e ) ; throw new MessageDecoderException ( e ) ; } }
private void onThingWithSerialNumber ( final ThingTypeUID deviceType , final String deviceTypeName , final DeviceDTO device , final String name ) { logger . debug ( "onThingWithSerialNumber: {}" , deviceTypeName ) ; final String serialNumber = device . getSerialNumber ( ) ; ThingUID localBridgeUID = this . bridgeUID ; code_block = IfStatement ; }
@ BeforeClass public static void createFsCrawlerJobDir ( ) throws IOException { metadataDir = rootTmpDir . resolve ( ".fscrawler" ) ; code_block = IfStatement ; copyDefaultResources ( metadataDir ) ; log . info ( "Metadata directory created." ) ; }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { if ( event . getMinResponseTime ( ) != 0 ) { logger . info ( "{} dbMaintainer start" , traceItem ) ; dbMaintainer . run ( traceItem ) ; logger . info ( "{} dbMaintainer end" , traceItem ) ; return new HealthStatus ( ) . withStatus ( "OK" ) ; } }
public void test() { try { configurationUrl = Config . class . getClassLoader ( ) . getResource ( defaultConfigFile ) ; code_block = IfStatement ; in = Config . class . getClassLoader ( ) . getResourceAsStream ( defaultConfigFile ) ; code_block = IfStatement ; } catch ( RuntimeException e ) { log . error ( "Unable to find default configuration file" , e ) ; throw new HazelcastException ( e ) ; } }
public void test() { try { IEditorInput editorInput = editorReference . getEditorInput ( ) ; code_block = IfStatement ; } catch ( Exception e ) { Log . error ( e ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { try { executor . submit ( ( ) -> executeThread ( task , description ) ) . get ( timeout . toMillis ( ) , TimeUnit . MILLISECONDS ) ; } catch ( InterruptedException e ) { log . error ( "{} interrupted in task: {}" , name , description ) ; } catch ( TimeoutException e ) { log . error ( "{} timed out running task: {}" , name , description ) ; } catch ( Throwable e ) { log . error ( "{} caught exception in task: {}" , name , description , e ) ; } }
public void test() { try { executor . submit ( ( ) -> executeThread ( task , description ) ) . get ( timeout . toMillis ( ) , TimeUnit . MILLISECONDS ) ; } catch ( InterruptedException e ) { log . warn ( "{} was interrupted running task: {}" , name , description ) ; } catch ( TimeoutException e ) { log . warn ( "{} timeout running task: {}" , name , description ) ; } catch ( Throwable e ) { log . error ( "{} caught exception in task: {}" , name , description , e ) ; } }
public void test() { try { executor . submit ( ( ) -> executeThread ( task , description ) ) . get ( timeout . toMillis ( ) , TimeUnit . MILLISECONDS ) ; } catch ( InterruptedException e ) { log . warn ( "{} was interrupted running task: {}" , name , description ) ; } catch ( TimeoutException e ) { log . error ( "{} timed out running task: {}" , name , description ) ; } catch ( Throwable e ) { log . error ( "{} failed running task: {}" , name , description , e ) ; } }
public void test() { if ( thing == null ) { logger . warn ( "Trying to add a null thing with id {}" , label ) ; } else { thing . setLabel ( label ) ; return thing ; } }
public void test() { if ( KeystoreType . PKCS12 . toString ( ) . equalsIgnoreCase ( keyStoreType ) ) { logger . warn ( "Command line argument --" + KEY_STORE_TYPE_WARNING ) ; } }
@ Override public void bindServiceInstance ( String applicationName , String serviceInstanceName ) { logger . debug ( Messages . BINDING_SERVICE_INSTANCE_1 , applicationName , serviceInstanceName ) ; delegate . bindServiceInstance ( applicationName , serviceInstanceName ) ; }
public void test() { try { registryMutex . acquire ( ) ; } catch ( InterruptedException e ) { LOGGER . warn ( "Interrupted while waiting to get the registry" , e ) ; return null ; } }
public void test() { if ( ! events . isEmpty ( ) ) { StringBuilder sb = new StringBuilder ( ) ; sb . append ( "[" ) ; events . stream ( ) . sorted ( Comparator . comparing ( ConfigEvent :: getName ) ) . forEach ( event code_block = LoopStatement ; ) ; sb . append ( "\n" ) . append ( "]" ) ; log . debug ( sb . toString ( ) ) ; } }
@ PreDestroy public void stop ( ) throws IOException { super . stop ( ) ; if ( _ftpClient != null ) _ftpClient . disconnect ( ) ; LOG . info ( "Disconnected" ) ; }
public void test() { if ( IS_TIMER_COMPSS_ENABLED ) { final long timeBindOriginalFilesEnd = System . nanoTime ( ) ; final float timeBindOriginalFilesElapsed = ( timeBindOriginalFilesEnd - timeBindOriginalFilesStart ) / ( float ) NANO_TO_MS ; LOG . info ( "Request timeBind original files({})" , timeBindOriginalFilesElapsed ) ; } }
private void assertSendReceiveLargeMessage ( JmsProvider jmsProvider , MessageProducer sender , MessageConsumer receiver , double sizeInMB , int mode , int count , List < javax . jms . Message > messages ) { List < javax . jms . Message > recvd ; LOGGER . info ( "Send receive messages" ) ; jmsProvider . sendMessages ( sender , messages , mode , javax . jms . Message . DEFAULT_PRIORITY , javax . jms . Message . DEFAULT_TIME_TO_LIVE ) ; recvd = jmsProvider . receiveMessages ( receiver , count , 2000 ) ; assertThat ( "Wrong count of received messages" , recvd . size ( ) , Matchers . is ( count ) ) ; LOGGER . info ( "{}MB {} message received" , sizeInMB , mode == DeliveryMode . PERSISTENT ? "durable" : "non-durable" ) ; }
private void assertSendReceiveLargeMessage ( JmsProvider jmsProvider , MessageProducer sender , MessageConsumer receiver , double sizeInMB , int mode , int count , List < javax . jms . Message > messages ) { List < javax . jms . Message > recvd ; LOGGER . info ( "Send receive messages" ) ; jmsProvider . sendMessages ( sender , messages , mode , javax . jms . Message . DEFAULT_PRIORITY , javax . jms . Message . DEFAULT_TIME_TO_LIVE ) ; LOGGER . info ( "{}MB {} message sent" , sizeInMB , mode == DeliveryMode . PERSISTENT ? "durable" : "non-durable" ) ; recvd = jmsProvider . receiveMessages ( receiver , count , 2000 ) ; assertThat ( "Wrong count of received messages" , recvd . size ( ) , Matchers . is ( count ) ) ; }
public void test() { try ( Session session = modelDBHibernateUtil . getSessionFactory ( ) . openSession ( ) ) { ExperimentRunEntity experimentRunObj = session . get ( ExperimentRunEntity . class , experimentRunId ) ; LOGGER . debug ( "Got ExperimentRun Tag" ) ; return experimentRunObj . getProtoObject ( ) . getTagsList ( ) ; } catch ( Exception ex ) { code_block = IfStatement ; } }
public void test() { try { Thread . sleep ( 1000 ) ; } catch ( InterruptedException e ) { logger . error ( "" , e ) ; } }
public void test() { try { Map < String , Object > config = note . getConfig ( ) ; code_block = IfStatement ; } catch ( ClassCastException e ) { LOGGER . error ( "Unexpected exception" , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
@ Override public void close ( ) throws Exception { log . debug ( "[{}] Closing slot: {}" , this , slotManagerMetricGroup ) ; suspend ( ) ; slotManagerMetricGroup . close ( ) ; }
public void test() { try { metadata = OIDCProviderMetadata . parse ( resourceRetriever . retrieveResource ( new URL ( discoveryUrl ) ) . getContent ( ) ) ; } catch ( IOException | ParseException e ) { LOGGER . debug ( "Could not fetch OIDC metadata from OIDC Provider." , e ) ; return null ; } }
protected FilterHolder getCorsFilter ( ) { CrossOriginFilter filter = new CrossOriginFilter ( ) ; FilterHolder filterHolder = new FilterHolder ( filter ) ; List < String > allowedOrigins = genericEndpointProperties . getListOfValues ( RESTEndpointProperties . ENABLED_CORS_ORIGINS ) ; StringJoiner originsJoiner = new StringJoiner ( "," ) ; allowedOrigins . forEach ( origin -> originsJoiner . add ( origin ) ) ; List < String > allowedHeaders = genericEndpointProperties . getListOfValues ( RESTEndpointProperties . ENABLED_CORS_HEADERS ) ; StringJoiner headersJoiner = new StringJoiner ( "," ) ; allowedHeaders . forEach ( origin -> headersJoiner . add ( origin ) ) ; String allowedHeadersSpec = allowedHeaders . isEmpty ( ) ? "*" : headersJoiner . toString ( ) ; LOG . debug ( "Creating CORS filter for {}" , allowedHeadersSpec ) ; filterHolder . setInitParameter ( CrossOriginFilter . ALLOWED_HEADERS_PARAM , allowedHeadersSpec ) ; filterHolder . setInitParameter ( CrossOriginFilter . ALLOWED_ORIGINS_PARAM , originsJoiner . toString ( ) ) ; filterHolder . setInitParameter ( CrossOriginFilter . ALLOWED_METHODS_PARAM , "GET,POST,HEAD,DELETE,PUT,OPTIONS" ) ; return filterHolder ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
@ Test public void ff_addProjectNegativeTags ( ) { LOGGER . info ( "Add Project Negative test start................................" ) ; List < String > tagsList = new ArrayList < > ( ) ; tagsList . add ( "Add Test Tag " + Calendar . getInstance ( ) . getTimeInMillis ( ) ) ; tagsList . add ( "Add Test Tag 2 " + Calendar . getInstance ( ) . getTimeInMillis ( ) ) ; AddProjectTags addProjectTagsRequest = AddProjectTags . newBuilder ( ) . addAllTags ( tagsList ) . build ( ) ; code_block = TryStatement ;  addProjectTagsRequest = AddProjectTags . newBuilder ( ) . setId ( "sdasd" ) . addAllTags ( project . getTagsList ( ) ) . build ( ) ; code_block = TryStatement ;  LOGGER . info ( "Add Project tags Negative test stop................................" ) ; }
public void test() { try { projectServiceStub . addProjectTags ( addProjectTagsRequest ) ; fail ( ) ; } catch ( StatusRuntimeException ex ) { Status status = Status . fromThrowable ( ex ) ; LOGGER . warn ( "Error Code : " + status . getCode ( ) + " Description : " + status . getDescription ( ) ) ; assertEquals ( Status . INVALID_ARGUMENT . getCode ( ) , status . getCode ( ) ) ; } }
@ Test public void ff_addProjectNegativeTags ( ) { LOGGER . info ( "Add Project Tags Negative test start................................" ) ; List < String > tagsList = new ArrayList < > ( ) ; tagsList . add ( "Add Test Tag " + Calendar . getInstance ( ) . getTimeInMillis ( ) ) ; tagsList . add ( "Add Test Tag 2 " + Calendar . getInstance ( ) . getTimeInMillis ( ) ) ; AddProjectTags addProjectTagsRequest = AddProjectTags . newBuilder ( ) . addAllTags ( tagsList ) . build ( ) ; code_block = TryStatement ;  addProjectTagsRequest = AddProjectTags . newBuilder ( ) . setId ( "sdasd" ) . addAllTags ( project . getTagsList ( ) ) . build ( ) ; code_block = TryStatement ;  LOGGER . info ( "Add Project tags Negative test stop................................" ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { LOG . debug ( "Checking whether {} is listening for RPCs" , mNodeAddress ) ; NetworkAddressUtils . pingService ( mNodeAddress , mServiceType , mConf , mUserState ) ; LOG . debug ( "Successfully connected to {}" , mNodeAddress ) ; return true ; } catch ( UnavailableException e ) { LOG . warn ( "Failed to discover service: {}" , mNodeAddress , e ) ; } catch ( AlluxioStatusException e ) { throw new RuntimeException ( e ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { if ( e instanceof ThriftSecurityException ) { result . sec = ( ThriftSecurityException ) e ; result . setSecIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof ThriftSecurityException ) { result . sec = ( ThriftSecurityException ) e ; result . setSecIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof ThriftSecurityException ) { result . sec = ( ThriftSecurityException ) e ; result . setSecIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { try { fcall . sendResponse ( fb , msg , msgType , seqid ) ; } catch ( java . lang . Exception ex ) { _LOGGER . error ( "Exception writing to internal frame buffer" , ex ) ; fb . close ( ) ; } }
public void test() { try { kuraPayload = new CloudPayloadProtoBufDecoderImpl ( payload ) . buildFromByteArray ( ) ; } catch ( Exception e ) { logger . debug ( "Cannot decode Kura Payload: {}" , e . getMessage ( ) ) ; kuraPayload = new KuraPayload ( ) ; kuraPayload . setBody ( payload ) ; } }
@ Test public void scenario18Test2 ( ) throws IOException { final String testObj = ingestObj ( "/rest/read_append_resource" ) ; final String id = "/rest/read_append_resource/" + getRandomUniqueId ( ) ; ingestObj ( id ) ; logger . debug ( "user18 can't read (no ACL): {}" , id ) ; final HttpGet requestGet = getObjMethod ( id ) ; setAuth ( requestGet , "user18" ) ; assertEquals ( HttpStatus . SC_OK , getStatus ( requestGet ) ) ; logger . debug ( "user18 can't append (no ACL): {}" , id ) ; final HttpPatch requestPatch = patchObjMethod ( id ) ; setAuth ( requestPatch , "user18" ) ; requestPatch . setHeader ( "Content-type" , "application/sparql-update" ) ; requestPatch . setEntity ( new StringEntity ( "INSERT code_block = "" ; WHERE {}" ) ) ; assertEquals ( HttpStatus . SC_FORBIDDEN , getStatus ( requestPatch ) ) ; ingestAcl ( "fedoraAdmin" , "/acls/18/read-append-acl.ttl" , testObj + "/fcr:acl" ) ; logger . debug ( "user18 can't delete (no ACL): {}" , id ) ; final HttpDelete requestDelete = deleteObjMethod ( id ) ; setAuth ( requestDelete , "user18" ) ; assertEquals ( HttpStatus . SC_FORBIDDEN , getStatus ( requestDelete ) ) ; logger . debug ( "user18 can read (ACL read, append): {}" , id ) ; assertEquals ( HttpStatus . SC_OK , getStatus ( requestGet ) ) ; logger . debug ( "user18 can append (ACL read, append): {}" , id ) ; assertEquals ( HttpStatus . SC_NO_CONTENT , getStatus ( requestPatch ) ) ; logger . debug ( "user18 still can't delete (ACL read, append): {}" , id ) ; assertEquals ( HttpStatus . SC_FORBIDDEN , getStatus ( requestDelete ) ) ; }
@ Test public void scenario18Test2 ( ) throws IOException { final String testObj = ingestObj ( "/rest/read_append_resource" ) ; final String id = "/rest/read_append_resource/" + getRandomUniqueId ( ) ; ingestObj ( id ) ; logger . debug ( "user18 can read (has ACL:READ): {}" , id ) ; final HttpGet requestGet = getObjMethod ( id ) ; setAuth ( requestGet , "user18" ) ; assertEquals ( HttpStatus . SC_OK , getStatus ( requestGet ) ) ; logger . debug ( "user18 can't delete (no ACL): {}" , id ) ; final HttpPatch requestPatch = patchObjMethod ( id ) ; setAuth ( requestPatch , "user18" ) ; requestPatch . setHeader ( "Content-type" , "application/sparql-update" ) ; requestPatch . setEntity ( new StringEntity ( "INSERT code_block = "" ; WHERE {}" ) ) ; assertEquals ( HttpStatus . SC_FORBIDDEN , getStatus ( requestPatch ) ) ; ingestAcl ( "fedoraAdmin" , "/acls/18/read-append-acl.ttl" , testObj + "/fcr:acl" ) ; logger . debug ( "user18 can't delete (no ACL): {}" , id ) ; final HttpDelete requestDelete = deleteObjMethod ( id ) ; setAuth ( requestDelete , "user18" ) ; assertEquals ( HttpStatus . SC_FORBIDDEN , getStatus ( requestDelete ) ) ; logger . debug ( "user18 can read (ACL read, append): {}" , id ) ; assertEquals ( HttpStatus . SC_OK , getStatus ( requestGet ) ) ; logger . debug ( "user18 can append (ACL read, append): {}" , id ) ; assertEquals ( HttpStatus . SC_NO_CONTENT , getStatus ( requestPatch ) ) ; logger . debug ( "user18 still can't delete (ACL read, append): {}" , id ) ; assertEquals ( HttpStatus . SC_FORBIDDEN , getStatus ( requestDelete ) ) ; }
@ Test public void scenario18Test2 ( ) throws IOException { final String testObj = ingestObj ( "/rest/read_append_resource" ) ; final String id = "/rest/read_append_resource/" + getRandomUniqueId ( ) ; ingestObj ( id ) ; logger . debug ( "user18 can read (has ACL:READ): {}" , id ) ; final HttpGet requestGet = getObjMethod ( id ) ; setAuth ( requestGet , "user18" ) ; assertEquals ( HttpStatus . SC_OK , getStatus ( requestGet ) ) ; logger . debug ( "user18 can't append (no ACL): {}" , id ) ; final HttpPatch requestPatch = patchObjMethod ( id ) ; setAuth ( requestPatch , "user18" ) ; requestPatch . setHeader ( "Content-type" , "application/sparql-update" ) ; requestPatch . setEntity ( new StringEntity ( "INSERT code_block = "" ; WHERE {}" ) ) ; assertEquals ( HttpStatus . SC_FORBIDDEN , getStatus ( requestPatch ) ) ; ingestAcl ( "fedoraAdmin" , "/acls/18/read-append-acl.ttl" , testObj + "/fcr:acl" ) ; logger . debug ( "user18 still can't delete (no ACL): {}" , id ) ; final HttpDelete requestDelete = deleteObjMethod ( id ) ; setAuth ( requestDelete , "user18" ) ; assertEquals ( HttpStatus . SC_FORBIDDEN , getStatus ( requestDelete ) ) ; logger . debug ( "user18 can read (ACL read, append): {}" , id ) ; assertEquals ( HttpStatus . SC_OK , getStatus ( requestGet ) ) ; logger . debug ( "user18 can append (ACL read, append): {}" , id ) ; assertEquals ( HttpStatus . SC_NO_CONTENT , getStatus ( requestPatch ) ) ; logger . debug ( "user18 still can't delete (ACL read, append): {}" , id ) ; assertEquals ( HttpStatus . SC_FORBIDDEN , getStatus ( requestDelete ) ) ; }
@ Test public void scenario18Test2 ( ) throws IOException { final String testObj = ingestObj ( "/rest/read_append_resource" ) ; final String id = "/rest/read_append_resource/" + getRandomUniqueId ( ) ; ingestObj ( id ) ; logger . debug ( "user18 can read (has ACL:READ): {}" , id ) ; final HttpGet requestGet = getObjMethod ( id ) ; setAuth ( requestGet , "user18" ) ; assertEquals ( HttpStatus . SC_OK , getStatus ( requestGet ) ) ; logger . debug ( "user18 can't append (no ACL): {}" , id ) ; final HttpPatch requestPatch = patchObjMethod ( id ) ; setAuth ( requestPatch , "user18" ) ; requestPatch . setHeader ( "Content-type" , "application/sparql-update" ) ; requestPatch . setEntity ( new StringEntity ( "INSERT code_block = "" ; WHERE {}" ) ) ; assertEquals ( HttpStatus . SC_FORBIDDEN , getStatus ( requestPatch ) ) ; ingestAcl ( "fedoraAdmin" , "/acls/18/read-append-acl.ttl" , testObj + "/fcr:acl" ) ; logger . debug ( "user18 can't delete (no ACL): {}" , id ) ; final HttpDelete requestDelete = deleteObjMethod ( id ) ; setAuth ( requestDelete , "user18" ) ; assertEquals ( HttpStatus . SC_FORBIDDEN , getStatus ( requestDelete ) ) ; assertEquals ( HttpStatus . SC_OK , getStatus ( requestGet ) ) ; logger . debug ( "user18 can append (ACL read, append): {}" , id ) ; assertEquals ( HttpStatus . SC_NO_CONTENT , getStatus ( requestPatch ) ) ; logger . debug ( "user18 still can't delete (ACL read, append): {}" , id ) ; assertEquals ( HttpStatus . SC_FORBIDDEN , getStatus ( requestDelete ) ) ; logger . debug ( "user18 still can't delete (ACL read, append): {}" , id ) ; }
@ Test public void scenario18Test2 ( ) throws IOException { final String testObj = ingestObj ( "/rest/read_append_resource" ) ; final String id = "/rest/read_append_resource/" + getRandomUniqueId ( ) ; ingestObj ( id ) ; logger . debug ( "user18 can read (has ACL:READ): {}" , id ) ; final HttpGet requestGet = getObjMethod ( id ) ; setAuth ( requestGet , "user18" ) ; assertEquals ( HttpStatus . SC_OK , getStatus ( requestGet ) ) ; logger . debug ( "user18 can't append (no ACL): {}" , id ) ; final HttpPatch requestPatch = patchObjMethod ( id ) ; setAuth ( requestPatch , "user18" ) ; requestPatch . setHeader ( "Content-type" , "application/sparql-update" ) ; requestPatch . setEntity ( new StringEntity ( "INSERT code_block = "" ; WHERE {}" ) ) ; assertEquals ( HttpStatus . SC_FORBIDDEN , getStatus ( requestPatch ) ) ; ingestAcl ( "fedoraAdmin" , "/acls/18/read-append-acl.ttl" , testObj + "/fcr:acl" ) ; logger . debug ( "user18 can't delete (no ACL): {}" , id ) ; final HttpDelete requestDelete = deleteObjMethod ( id ) ; setAuth ( requestDelete , "user18" ) ; assertEquals ( HttpStatus . SC_FORBIDDEN , getStatus ( requestDelete ) ) ; logger . debug ( "user18 can read (ACL read, append): {}" , id ) ; assertEquals ( HttpStatus . SC_OK , getStatus ( requestGet ) ) ; assertEquals ( HttpStatus . SC_NO_CONTENT , getStatus ( requestPatch ) ) ; logger . debug ( "user18 still can't delete (ACL read, append): {}" , id ) ; assertEquals ( HttpStatus . SC_FORBIDDEN , getStatus ( requestDelete ) ) ; logger . debug ( "user18 still can't delete (ACL read, append): {}" , id ) ; }
@ Test public void scenario18Test2 ( ) throws IOException { final String testObj = ingestObj ( "/rest/read_append_resource" ) ; final String id = "/rest/read_append_resource/" + getRandomUniqueId ( ) ; ingestObj ( id ) ; logger . debug ( "user18 can read (has ACL:READ): {}" , id ) ; final HttpGet requestGet = getObjMethod ( id ) ; setAuth ( requestGet , "user18" ) ; assertEquals ( HttpStatus . SC_OK , getStatus ( requestGet ) ) ; logger . debug ( "user18 can't append (no ACL): {}" , id ) ; final HttpPatch requestPatch = patchObjMethod ( id ) ; setAuth ( requestPatch , "user18" ) ; requestPatch . setHeader ( "Content-type" , "application/sparql-update" ) ; requestPatch . setEntity ( new StringEntity ( "INSERT code_block = "" ; WHERE {}" ) ) ; assertEquals ( HttpStatus . SC_FORBIDDEN , getStatus ( requestPatch ) ) ; ingestAcl ( "fedoraAdmin" , "/acls/18/read-append-acl.ttl" , testObj + "/fcr:acl" ) ; logger . debug ( "user18 can't delete (no ACL): {}" , id ) ; final HttpDelete requestDelete = deleteObjMethod ( id ) ; setAuth ( requestDelete , "user18" ) ; assertEquals ( HttpStatus . SC_FORBIDDEN , getStatus ( requestDelete ) ) ; logger . debug ( "user18 can read (ACL read, append): {}" , id ) ; assertEquals ( HttpStatus . SC_OK , getStatus ( requestGet ) ) ; logger . debug ( "user18 can append (ACL read, append): {}" , id ) ; assertEquals ( HttpStatus . SC_NO_CONTENT , getStatus ( requestPatch ) ) ; logger . debug ( "user18 can write (ACL read, append): {}" , id ) ; assertEquals ( HttpStatus . SC_FORBIDDEN , getStatus ( requestDelete ) ) ; }
public void test() { try { fileName = key . substring ( key . lastIndexOf ( '/' ) + 1 ) ; log . info ( "Copying " + fileName + " to " + s3Bucket . getAbsolutePath ( ) ) ; s3client . copyObject ( s3Bucket , key , s3Bucket , to + "/" + fileName ) ; } catch ( Exception e ) { log . info ( "    Copy " + fileName + "failed" , e ) ; ErrorManageUtil . uploadError ( "all" , "all" , "all" , e . getMessage ( ) ) ; } }
public void test() { try { fileName = key . substring ( key . lastIndexOf ( '/' ) + 1 ) ; s3client . copyObject ( s3Bucket , key , s3Bucket , to + "/" + fileName ) ; log . debug ( "   Copy " + fileName + " to backup folder" ) ; } catch ( Exception e ) { log . error ( e . getMessage ( ) ) ; ErrorManageUtil . uploadError ( "all" , "all" , "all" , e . getMessage ( ) ) ; } }
private StringBuilder buildExportTableArg ( StringBuilder builder , CatalogTable catalog ) throws FalconException { builder . append ( "--skip-dist-cache" ) . append ( ImportExportCommon . ARG_SEPARATOR ) ; Iterator < String > itr = Splitter . on ( "" ) . split ( catalog . getUri ( ) ) . iterator ( ) ; String dbTable = itr . next ( ) ; String partitions = itr . next ( ) ; LOG . debug ( "Target partitions {}" , partitions ) ; Iterator < String > itrDbTable = Splitter . on ( ":" ) . split ( dbTable ) . iterator ( ) ; itrDbTable . next ( ) ; String db = itrDbTable . next ( ) ; String table = itrDbTable . next ( ) ; LOG . debug ( "Target database {}, table {}" , db , table ) ; builder . append ( "--hcatalog-database" ) . append ( ImportExportCommon . ARG_SEPARATOR ) . append ( String . format ( "${coord:databaseIn('%s')}" , FeedExportCoordinatorBuilder . EXPORT_DATAIN_NAME ) ) . append ( ImportExportCommon . ARG_SEPARATOR ) ; builder . append ( "--hcatalog-table" ) . append ( ImportExportCommon . ARG_SEPARATOR ) . append ( String . format ( "${coord:tableIn('%s')}" , FeedExportCoordinatorBuilder . EXPORT_DATAIN_NAME ) ) . append ( ImportExportCommon . ARG_SEPARATOR ) ; Map < String , String > partitionsMap = ImportExportCommon . getPartitionKeyValues ( partitions ) ; code_block = IfStatement ; return builder ; }
private StringBuilder buildExportTableArg ( StringBuilder builder , CatalogTable catalog ) throws FalconException { LOG . info ( "Catalog URI {}" , catalog . getUri ( ) ) ; builder . append ( "--skip-dist-cache" ) . append ( ImportExportCommon . ARG_SEPARATOR ) ; Iterator < String > itr = Splitter . on ( "" ) . split ( catalog . getUri ( ) ) . iterator ( ) ; String dbTable = itr . next ( ) ; String partitions = itr . next ( ) ; Iterator < String > itrDbTable = Splitter . on ( ":" ) . split ( dbTable ) . iterator ( ) ; itrDbTable . next ( ) ; String db = itrDbTable . next ( ) ; String table = itrDbTable . next ( ) ; builder . append ( "--hcatalog-database" ) . append ( ImportExportCommon . ARG_SEPARATOR ) . append ( String . format ( "${coord:databaseIn('%s')}" , FeedExportCoordinatorBuilder . EXPORT_DATAIN_NAME ) ) . append ( ImportExportCommon . ARG_SEPARATOR ) ; builder . append ( "--hcatalog-table" ) . append ( ImportExportCommon . ARG_SEPARATOR ) . append ( String . format ( "${coord:tableIn('%s')}" , FeedExportCoordinatorBuilder . EXPORT_DATAIN_NAME ) ) . append ( ImportExportCommon . ARG_SEPARATOR ) ; Map < String , String > partitionsMap = ImportExportCommon . getPartitionKeyValues ( partitions ) ; code_block = IfStatement ; LOG . info ( "Built partition URI {}" , builder ) ; return builder ; }
public void test() { if ( counterMap . containsKey ( key ) ) { int tally = counterMap . get ( key ) . decrementAndGet ( ) ; logger . debug ( "DecrementAndGet key: {}, count: {}" , key , countMap . get ( key ) ) ; return tally ; } else { counterMap . put ( key , new AtomicInteger ( - 1 ) ) ; logger . debug ( "DecrementAndGet key: {}, count: {}" , key , - 1 ) ; return - 1 ; } }
public void test() { if ( counterMap . containsKey ( key ) ) { int tally = counterMap . get ( key ) . decrementAndGet ( ) ; logger . debug ( "DecrementAndGet key: {}, count: {}" , key , tally ) ; return tally ; } else { counterMap . put ( key , new AtomicInteger ( - 1 ) ) ; logger . debug ( "Unknown key: {}, count: {}" , key , countMap . get ( key ) ) ; return - 1 ; } }
public void test() { if ( m_trackedObjectHash . remove ( obj ) == null ) { LOGGER . debug ( "Ignoring " + name + " (" + m_trackedObjectHash . size ( ) + ")" ) ; } else { LOGGER . debug ( "Removing " + name + " (" + m_trackedObjectHash . size ( ) + " remaining)" ) ; } }
public void test() { if ( m_trackedObjectHash . remove ( obj ) == null ) { LOGGER . debug ( "Attempted to remove " + name + ", which was not tracked" ) ; } else { LOGGER . debug ( "Attempted to remove " + name + ", which was not tracked" ) ; } }
@ Test public void callSetAndGetGetAttributeArrayTypeDef ( ) { logger . info ( name . getMethodName ( ) ) ; String [ ] stringArray = code_block = "" ; ; ArrayTypeDefStruct arrayTypeDefArg = new ArrayTypeDefStruct ( stringArray ) ; genericGetterSetterTestMethod ( arrayTypeDefArg , "AttributeArrayTypeDef" ) ; logger . info ( name . getMethodName ( ) + " - OK" ) ; }
public void test() { for ( String entityName : names ) { r = helper . getProcessInstanceStatus ( entityName , null ) ; LOGGER . info ( "Entity: " + entityName ) ; InstancesResult . Instance [ ] instancesFromStatus = r . getInstances ( ) ; LOGGER . info ( "Instances from -getStatus API: " + Arrays . toString ( instancesFromStatus ) ) ; EntitySummaryResult . EntitySummary [ ] summaries = helper . getEntitySummary ( clusterName , null ) . getEntitySummaryResult ( ) . getEntitySummaries ( ) ; EntitySummaryResult . EntitySummary summaryItem = null ; code_block = ForStatement ; Assert . assertNotNull ( summaryItem , "Appropriate summary not found for : " + entityName ) ; EntitySummaryResult . Instance [ ] instancesFromSummary = summaryItem . getInstances ( ) ; LOGGER . info ( "Instances from SummaryResult: " + Arrays . toString ( instancesFromSummary ) ) ; softAssert . assertEquals ( instancesFromSummary . length , 7 , "7 instances should be present in " + "summary." ) ; code_block = ForStatement ; } }
public void test() { for ( String entityName : names ) { LOGGER . info ( "Working with " + entityType + " : " + entityName ) ; r = helper . getProcessInstanceStatus ( entityName , null ) ; InstancesResult . Instance [ ] instancesFromStatus = r . getInstances ( ) ; LOGGER . info ( "Entity: " + entityName ) ; EntitySummaryResult . EntitySummary [ ] summaries = helper . getEntitySummary ( clusterName , null ) . getEntitySummaryResult ( ) . getEntitySummaries ( ) ; EntitySummaryResult . EntitySummary summaryItem = null ; code_block = ForStatement ; Assert . assertNotNull ( summaryItem , "Appropriate summary not found for : " + entityName ) ; EntitySummaryResult . Instance [ ] instancesFromSummary = summaryItem . getInstances ( ) ; LOGGER . info ( "Instances from SummaryResult: " + Arrays . toString ( instancesFromSummary ) ) ; softAssert . assertEquals ( instancesFromSummary . length , 7 , "7 instances should be present in " + "summary." ) ; code_block = ForStatement ; } }
public void test() { for ( String entityName : names ) { LOGGER . info ( "Working with " + entityType + " : " + entityName ) ; r = helper . getProcessInstanceStatus ( entityName , null ) ; LOGGER . info ( "Entity: " + entityName ) ; InstancesResult . Instance [ ] instancesFromStatus = r . getInstances ( ) ; LOGGER . info ( "Instances from -getStatus API: " + Arrays . toString ( instancesFromStatus ) ) ; EntitySummaryResult . EntitySummary [ ] summaries = helper . getEntitySummary ( clusterName , null ) . getEntitySummaryResult ( ) . getEntitySummaries ( ) ; EntitySummaryResult . EntitySummary summaryItem = null ; code_block = ForStatement ; Assert . assertNotNull ( summaryItem , "Appropriate summary not found for : " + entityName ) ; EntitySummaryResult . Instance [ ] instancesFromSummary = summaryItem . getInstances ( ) ; softAssert . assertEquals ( instancesFromSummary . length , 7 , "7 instances should be present in " + "summary." ) ; code_block = ForStatement ; } }
public void test() { try { code_block = IfStatement ; } catch ( Throwable t ) { logger . error ( t . getMessage ( ) , t ) ; } }
public void test() { try { Runtime . main ( new String [ ] code_block = "" ; ) ; WebGui webgui = ( WebGui ) Runtime . create ( "webgui" , "WebGui" ) ; webgui . autoStartBrowser ( false ) ; webgui . setPort ( 8888 ) ; webgui . startService ( ) ; Runtime . start ( "python" , "Python" ) ; boolean done = true ; code_block = IfStatement ; MqttBroker broker = ( MqttBroker ) Runtime . start ( "broker" , "MqttBroker" ) ; broker . listen ( ) ; Mqtt mqtt01 = ( Mqtt ) Runtime . start ( "mqtt01" , "Mqtt" ) ; mqtt01 . connect ( "mqtt://localhost:1883" ) ; Runtime . start ( "neo" , "NeoPixel" ) ; Arduino arduino = ( Arduino ) Runtime . start ( "arduino" , "Arduino" ) ; arduino . connect ( "/dev/ttyACM0" ) ; code_block = ForStatement ; Platform . setVirtual ( true ) ; Servo pan = ( Servo ) Runtime . start ( "pan" , "Servo" ) ; Servo tilt = ( Servo ) Runtime . start ( "tilt" , "Servo" ) ; pan . setPin ( 3 ) ; pan . setMinMax ( 30.0 , 70.0 ) ; tilt . setPin ( "D4" ) ; arduino . attach ( pan ) ; arduino . attach ( tilt ) ; log . info ( "Python initialized" ) ; } catch ( Exception e ) { log . error ( "main threw" , e ) ; } }
public void test() { try { Runtime . main ( new String [ ] code_block = "" ; ) ; WebGui webgui = ( WebGui ) Runtime . create ( "webgui" , "WebGui" ) ; webgui . autoStartBrowser ( false ) ; webgui . setPort ( 8888 ) ; webgui . startService ( ) ; Runtime . start ( "python" , "Python" ) ; boolean done = true ; code_block = IfStatement ; MqttBroker broker = ( MqttBroker ) Runtime . start ( "broker" , "MqttBroker" ) ; broker . listen ( ) ; Mqtt mqtt01 = ( Mqtt ) Runtime . start ( "mqtt01" , "Mqtt" ) ; mqtt01 . connect ( "mqtt://localhost:1883" ) ; Runtime . start ( "neo" , "NeoPixel" ) ; Arduino arduino = ( Arduino ) Runtime . start ( "arduino" , "Arduino" ) ; arduino . connect ( "/dev/ttyACM0" ) ; code_block = ForStatement ; Platform . setVirtual ( true ) ; Servo pan = ( Servo ) Runtime . start ( "pan" , "Servo" ) ; Servo tilt = ( Servo ) Runtime . start ( "tilt" , "Servo" ) ; pan . setPin ( 3 ) ; pan . setMinMax ( 30.0 , 70.0 ) ; tilt . setPin ( "D4" ) ; arduino . attach ( pan ) ; arduino . attach ( tilt ) ; log . info ( "leaving main" ) ; } catch ( Exception e ) { log . error ( "main threw" , e ) ; } }
public void test() { try { Message [ ] messages = monitoringStrategy . monitor ( folder ) ; code_block = ForStatement ; } catch ( FolderClosedException ex ) { logger . debug ( "Folder closed, reopening" ) ; code_block = IfStatement ; } catch ( MessagingException ex ) { logger . warn ( "Unable to monitor messages" , ex ) ; } }
public void test() { try { openFolder ( ) ; code_block = WhileStatement ; } catch ( InterruptedException ex ) { Thread . currentThread ( ) . interrupt ( ) ; } catch ( MessagingException ex ) { LOGGER . error ( ex . getMessage ( ) , ex ) ; } }
public void onBecomeOfflineFromSlave ( Message message , NotificationContext context ) { DummyProcess . sleep ( _transDelay ) ; logger . info ( "DummyStateModel.onBecomeOfflineFromSlave()" ) ; }
@ Override public synchronized void mark ( final int readlimit ) { byteBuffer . mark ( ) ; LOG . debug ( "Marked read limit {}" , readlimit ) ; }
public void test() { try { SecretKey rootKey = KeyProviderFactory . requiresRootKey ( repositoryEncryptionConfiguration . getKeyProviderImplementation ( ) ) ? CryptoUtils . getRootKey ( ) : null ; return buildKeyProviderFromConfig ( rootKey , repositoryEncryptionConfiguration ) ; } catch ( KeyManagementException e ) { String msg = "Encountered an error building the key provider" ; log . error ( msg ) ; throw new IOException ( msg , e ) ; } }
public static Long addMedicalData ( String providerNo , MyOscarLoggedInInfo myOscarLoggedInInfo , MedicalDataTransfer4 medicalDataTransfer , String oscarDataType , Object localOscarObjectId , boolean completed , boolean active ) throws NotAuthorisedException_Exception , UnsupportedEncodingException_Exception , InvalidRequestException_Exception { logger . debug ( "Adding patient data" ) ; Long resultId = MedicalDataManager . addMedicalData ( myOscarLoggedInInfo , medicalDataTransfer , completed , active ) ; addSendRemoteDataLog ( providerNo , oscarDataType , localOscarObjectId , medicalDataTransfer . getData ( ) ) ; return ( resultId ) ; }
@ Override public boolean isSatisified ( ) throws Exception { LOG . info ( "Current Backup Count = " + failoverTransport . getCurrentBackups ( ) ) ; return failoverTransport . getCurrentBackups ( ) == 1 ; }
public void test() { try { super . start ( ) ; super . destroy ( ) ; this . with ( LAST_UPDATED_ORDERBY . clone ( ) ) . direction ( DIRECTION . DESC ) . with ( POSTED_TIME_ORDERBY . clone ( ) ) . direction ( DIRECTION . DESC ) ; } catch ( Exception ex ) { LOG . warn ( ex ) ; } }
@ Override public void run ( ) { WeldRequestScopeAdapter . getInstance ( ) . activateContext ( ) ; WeldSessionScopeAdapter . getInstance ( ) . activateContext ( httpSession ) ; WeldConversationScopeAdapter . getInstance ( ) . activateContext ( conversationState ) ; LOGGER . info ( "Conversation started" ) ; Assert . assertFalse ( conversation . isTransient ( ) ) ; Assert . assertTrue ( conversationState . isLongRunning ( ) ) ; final ConversationScopeBean conversationScopeBean = getInstance ( ConversationScopeBean . class ) ; Assert . assertEquals ( "wrong state" , conversationState . getConversationId ( ) , conversationScopeBean . getId ( ) ) ; WeldConversationScopeAdapter . getInstance ( ) . deactivateContext ( ) ; WeldSessionScopeAdapter . getInstance ( ) . deactivateContext ( ) ; WeldRequestScopeAdapter . getInstance ( ) . deactivateContext ( ) ; final int runner = doneRunnerCount . incrementAndGet ( ) ; }
public void test() { try { mongoPort = NetworkUtil . getAvailableLocalPort ( ) ; RuntimeConfig runtimeConfig = Defaults . runtimeConfigFor ( Command . MongoD , LOGGER ) . processOutput ( ProcessOutput . getDefaultInstanceSilent ( ) ) . build ( ) ; MongodConfig mongodConfig = MongodConfig . builder ( ) . version ( Version . V4_0_12 ) . net ( new Net ( DEFAULT_MONGO_HOST , mongoPort , Network . localhostIsIPv6 ( ) ) ) . build ( ) ; MongodStarter runtime = MongodStarter . getInstance ( runtimeConfig ) ; mongodExecutable = runtime . prepare ( mongodConfig ) ; mongodExecutable . start ( ) ; } catch ( IOException e ) { LOGGER . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { exec . setProgress ( "Starting command" ) ; Runtime rt = Runtime . getRuntime ( ) ; exec . setProgress ( "External command is running..." ) ; final Process proc ; code_block = IfStatement ; final MutableBoolean procDone = new MutableBoolean ( false ) ; ThreadUtils . threadWithContext ( new CheckCanceledRunnable ( proc , procDone , exec ) ) . start ( ) ; Thread stdErrThread = ThreadUtils . threadWithContext ( new StdErrCatchRunnable ( proc , exec , m_extErrout ) ) ; stdErrThread . setName ( "ExtTool StdErr collector" ) ; stdErrThread . start ( ) ; Thread stdOutThread = ThreadUtils . threadWithContext ( new StdOutCatchRunnable ( proc , exec , m_extOutput ) ) ; stdOutThread . setName ( "ExtTool StdOut collector" ) ; stdOutThread . start ( ) ; exitVal = proc . waitFor ( ) ; synchronized ( procDone ) code_block = "" ; exec . checkCanceled ( ) ; exec . setProgress ( "External command done." ) ; String message = "External commands terminated with exit code: " + exitVal ; LOGGER . info ( message ) ; code_block = IfStatement ; } catch ( InterruptedException ie ) { throw ie ; } catch ( Exception e ) { LOGGER . error ( "Execution failed (with exception): " + e . getMessage ( ) , e ) ; throw e ; } catch ( Throwable t ) { LOGGER . fatal ( "Execution failed (with error): " + t . getMessage ( ) , t ) ; throw new Exception ( t ) ; } }
public void test() { if ( exitVal == 0 ) { LOGGER . info ( message ) ; } else { LOGGER . info ( message ) ; } }
public void test() { if ( exitVal == 0 ) { LOGGER . debug ( message ) ; } else { LOGGER . warn ( message ) ; } }
public void test() { try { exec . setProgress ( "Starting command" ) ; Runtime rt = Runtime . getRuntime ( ) ; LOGGER . debug ( "Launching command: '" + getCmdString ( ) + "'" ) ; exec . setProgress ( "External command is running..." ) ; final Process proc ; code_block = IfStatement ; final MutableBoolean procDone = new MutableBoolean ( false ) ; ThreadUtils . threadWithContext ( new CheckCanceledRunnable ( proc , procDone , exec ) ) . start ( ) ; Thread stdErrThread = ThreadUtils . threadWithContext ( new StdErrCatchRunnable ( proc , exec , m_extErrout ) ) ; stdErrThread . setName ( "ExtTool StdErr collector" ) ; stdErrThread . start ( ) ; Thread stdOutThread = ThreadUtils . threadWithContext ( new StdOutCatchRunnable ( proc , exec , m_extOutput ) ) ; stdOutThread . setName ( "ExtTool StdOut collector" ) ; stdOutThread . start ( ) ; exitVal = proc . waitFor ( ) ; synchronized ( procDone ) code_block = "" ; exec . checkCanceled ( ) ; exec . setProgress ( "External command done." ) ; String message = "External commands terminated with exit code: " + exitVal ; LOGGER . info ( message ) ; code_block = IfStatement ; } catch ( InterruptedException ie ) { throw ie ; } catch ( Exception e ) { throw e ; } catch ( Throwable t ) { LOGGER . fatal ( "Execution failed (with error): " + t . getMessage ( ) , t ) ; throw new Exception ( t ) ; } }
public void test() { try { exec . setProgress ( "Starting command" ) ; Runtime rt = Runtime . getRuntime ( ) ; LOGGER . debug ( "Launching command: '" + getCmdString ( ) + "'" ) ; exec . setProgress ( "External command is running..." ) ; final Process proc ; code_block = IfStatement ; final MutableBoolean procDone = new MutableBoolean ( false ) ; ThreadUtils . threadWithContext ( new CheckCanceledRunnable ( proc , procDone , exec ) ) . start ( ) ; Thread stdErrThread = ThreadUtils . threadWithContext ( new StdErrCatchRunnable ( proc , exec , m_extErrout ) ) ; stdErrThread . setName ( "ExtTool StdErr collector" ) ; stdErrThread . start ( ) ; Thread stdOutThread = ThreadUtils . threadWithContext ( new StdOutCatchRunnable ( proc , exec , m_extOutput ) ) ; stdOutThread . setName ( "ExtTool StdOut collector" ) ; stdOutThread . start ( ) ; exitVal = proc . waitFor ( ) ; synchronized ( procDone ) code_block = "" ; exec . checkCanceled ( ) ; exec . setProgress ( "External command done." ) ; String message = "External commands terminated with exit code: " + exitVal ; LOGGER . info ( message ) ; code_block = IfStatement ; } catch ( InterruptedException ie ) { throw ie ; } catch ( Exception e ) { LOGGER . error ( "Execution failed (with exception): " + e . getMessage ( ) , e ) ; throw e ; } catch ( Throwable t ) { throw new Exception ( t ) ; } }
public void test() { -> { log . debug ( "updateDevice started..." ) ; final Optional < String > resourceVersion = Optional . ofNullable ( ctx . get ( KEY_RESOURCE_VERSION ) ) ; return getService ( ) . updateDevice ( tenantId . result ( ) , deviceId . result ( ) , device . result ( ) , resourceVersion , span ) ; } }
public void test() { if ( JBoss6VFS . valid == Boolean . TRUE ) { log . debug ( "JBoss 6 VFS API is not available in this environment" ) ; JBoss6VFS . valid = Boolean . FALSE ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { onException . accept ( e ) ; } catch ( Exception ex ) { LOGGER . warn ( "Exception onException" , ex ) ; } }
private void throwValidationError ( ErrorMessage error ) { log . warn ( error . getMessage ( ) ) ; throw new IllegalQueryException ( error ) ; }
public void test() { if ( allowed ) { httpExchange . putAttachment ( PRINCIPAL_NAME_KEY , ( ( JwtAuthenticationToken ) a ) . getName ( ) ) ; httpExchange . setStatusCode ( StatusCodes . OK ) ; return ; } else { LOG . warn ( "Invalid access token: {}" , a ) ; } }
public void test() { if ( a instanceof JwtAuthenticationToken ) { LOG . debug ( "Authentication token is present." ) ; boolean allowed = false ; Collection < GrantedAuthority > grantedAuthorities = ( ( JwtAuthenticationToken ) a ) . getAuthorities ( ) ; code_block = ForStatement ; code_block = IfStatement ; } else { LOG . warn ( "No authentication token present." ) ; } }
public void test() { try { checksumFile = new File ( new URI ( checksumStr ) ) ; code_block = IfStatement ; } catch ( URISyntaxException e ) { log . error ( "Unable to parse checksum: {}" , checksumStr , e ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Refreshing." ) ; } }
public void test() { if ( error != null ) { log . error ( "periodic commit failed" , error ) ; } else { log . trace ( "periodic commit succeeded" ) ; setCommittedOffsets ( committedOffsets ) ; } }
public void test() { if ( error != null ) { log . info ( "periodic commit failed: {}" , error . toString ( ) ) ; } else { setCommittedOffsets ( committedOffsets ) ; log . info ( "periodic commit completed" ) ; } }
public void test() { if ( ! offsets . isEmpty ( ) ) { code_block = IfStatement ; final Consumer < String , Buffer > wrappedConsumer = getKafkaConsumer ( ) . asStream ( ) . unwrap ( ) ; wrappedConsumer . commitAsync ( offsets , ( committedOffsets , error ) code_block = LoopStatement ; ) ; } else { log . info ( "Nothing to commit" ) ; } }
public void test() { -> { log . debug ( "Configuring FileBasedAccessControl for {}" , config ) ; return new FileBasedAccessControl ( config ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( AppConfig . get ( ) . isMaintenanceMode ( ) ) { LOG . debug ( "This service is maintenance mode now." ) ; return ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( "Closing connection {}" , connection ) ; } }
public void test() { try { JobResult result = job . execute ( ) ; code_block = IfStatement ; code_block = IfStatement ; } catch ( Exception e ) { LOG . error ( "Failed to execute redelivery task." , e ) ; } }
public void test() { try { code_block = IfStatement ; new Thread ( ( ) code_block = LoopStatement ; ) . start ( ) ; } catch ( InterruptedException e ) { logger . error ( "Thread interrupted" , e ) ; Thread . currentThread ( ) . interrupt ( ) ; } }
public void test() { try { FedizResponse federationResponse = processSigninRequest ( responseToken , request , response ) ; code_block = IfStatement ; LOG . debug ( "RSTR validated successfully" ) ; return createPrincipal ( request , response , federationResponse ) ; } catch ( ProcessingException e ) { LOG . debug ( "Failed to validate RSTR" , e ) ; } }
@ Bean ( name = "wsDistributionAutomationInboundDomainResponsesMessageListenerContainer" ) public DefaultMessageListenerContainer messageListenerContainer ( @ Qualifier ( "wsDistributionAutomationInboundDomainResponsesMessageListener" ) final MessageListener messageListener ) { LOGGER . info ( "Initializing wsDistributionAutomationInboundDomainResponsesMessageListenerContainer bean." ) ; return this . jmsConfigurationFactory . initMessageListenerContainer ( messageListener ) ; }
@ Override public Object visit ( PropertyIsEqualTo filter , Object data ) { ExpressionValueVisitor expressionVisitor = new ExpressionValueVisitor ( ) ; String propertyName = ( String ) filter . getExpression1 ( ) . accept ( expressionVisitor , data ) ; Object literalValue = filter . getExpression2 ( ) . accept ( expressionVisitor , data ) ; String mappedPropertyName = getMappedPropertyName ( propertyName ) ; LOGGER . debug ( "Property: {}" , mappedPropertyName ) ; return new SolrQuery ( mappedPropertyName + ":" + QUOTE + escapeSpecialCharacters ( literalValue . toString ( ) ) + QUOTE ) ; }
public void test() { try { stat = conn . prepareStatement ( DELETE_WORK_GUI ) ; stat . setString ( 1 , typeCode ) ; stat . setString ( 2 , stepCode ) ; stat . executeUpdate ( ) ; } catch ( Throwable t ) { _logger . error ( "Error deleting work gui - typeCode {} - stepCode " , typeCode , stepCode , t ) ; throw new RuntimeException ( "Error deleting work gui - typeCode " + typeCode + " - stepCode " + stepCode , t ) ; } finally { closeDaoResources ( null , stat ) ; } }
@ GetMapping ( CommonConstants . PATH_ID ) public LogbookOperationDto getAllPaginated ( @ PathVariable ( "id" ) String id ) { LOGGER . debug ( "getPaginated {}" , id ) ; ParameterChecker . checkParameter ( "The Identifier is a mandatory parameter: " , id ) ; final VitamContext vitamContext = securityService . buildVitamContext ( securityService . getTenantIdentifier ( ) ) ; return ingestInternalService . getOne ( vitamContext , id ) ; }
public void test() { try { RandomAccessFile raf = myInsertToRAFMap . get ( Integer . valueOf ( dcr . getInsertNum ( ) ) ) ; code_block = IfStatement ; code_block = IfStatement ; } catch ( IOException | BadPaddingException | IllegalBlockSizeException | ShortBufferException e ) { LOGGER . error ( e ) ; } finally { myRetrieveLock . unlock ( ) ; } }
public static int checkIfFeedCoordExist ( AbstractEntityHelper helper , String feedName , String coordType ) throws OozieClientException { LOGGER . info ( "Looking for feed: " + feedName ) ; int numberOfCoord = 0 ; final OozieClient oozieClient = helper . getOozieClient ( ) ; code_block = IfStatement ; List < String > bundleIds = OozieUtil . getBundles ( oozieClient , feedName , EntityType . FEED ) ; LOGGER . info ( "bundleIds: " + bundleIds ) ; code_block = ForStatement ; return numberOfCoord ; }
public static int checkIfFeedCoordExist ( AbstractEntityHelper helper , String feedName , String coordType ) throws OozieClientException { LOGGER . info ( "feedName: " + feedName ) ; LOGGER . info ( "versionType: " + numberOfCoord ) ; int numberOfCoord = 0 ; final OozieClient oozieClient = helper . getOozieClient ( ) ; code_block = IfStatement ; List < String > bundleIds = OozieUtil . getBundles ( oozieClient , feedName , EntityType . FEED ) ; code_block = ForStatement ; return numberOfCoord ; }
public void test() { for ( String bundleId : bundleIds ) { LOGGER . info ( "BundleId: " + bundleId ) ; OozieUtil . waitForCoordinatorJobCreation ( oozieClient , bundleId ) ; List < CoordinatorJob > coords = OozieUtil . getBundleCoordinators ( oozieClient , bundleId ) ; LOGGER . info ( "coords: " + coords ) ; code_block = ForStatement ; } }
public void test() { for ( String bundleId : bundleIds ) { LOGGER . info ( "bundleId: " + bundleId ) ; OozieUtil . waitForCoordinatorJobCreation ( oozieClient , bundleId ) ; List < CoordinatorJob > coords = OozieUtil . getBundleCoordinators ( oozieClient , bundleId ) ; LOGGER . info ( "coords: " + coords ) ; code_block = ForStatement ; } }
public void test() { try { zk = ZooKeeperStorage . zkOpen ( appConf ) ; nodes = getChildList ( zk ) ; code_block = ForStatement ; zk . close ( ) ; } catch ( Exception e ) { LOG . error ( "Unable to open bookie" , e ) ; } finally { if ( zk != null ) zk . close ( ) ; } }
public void test() { try { code_block = TryStatement ;  long sleepMillis = ( long ) ( Math . random ( ) * interval ) ; LOG . info ( "Next execution: " + new Date ( new Date ( ) . getTime ( ) + sleepMillis ) ) ; Thread . sleep ( sleepMillis ) ; } catch ( Exception e ) { isRunning = false ; LOG . error ( "Cleanup failed: " + e . getMessage ( ) , e ) ; } }
public void test() { try { code_block = TryStatement ;  long sleepMillis = ( long ) ( Math . random ( ) * interval ) ; LOG . info ( "Next execution: " + new Date ( new Date ( ) . getTime ( ) + sleepMillis ) ) ; Thread . sleep ( sleepMillis ) ; } catch ( Exception e ) { isRunning = false ; LOG . error ( "Cleanup failed: " + e . getMessage ( ) , e ) ; } }
@ Override public void onSubscribe ( Subscription s ) { logger . info ( "[subscribe] {}" , s ) ; subscription = s ; subscriber . onSubscribe ( this ) ; }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "JDBC::{}" , sql ) ; } }
@ Override public void nodeFinished ( CnATreeElement node , int depth ) { LOG . debug ( "Node finished: {}" , node ) ; }
@ Override public void exists ( ) { status = Status . EXIST ; signalChildren ( ) ; logger . trace ( "{} exists" , this ) ; }
@ Override public COREEnvelopeRealTimeResponse realTimeTransaction ( COREEnvelopeRealTimeRequest msg , AssertionType assertion ) { LOG . trace ( "Using realTimeTransaction: {}" , assertion ) ; return new COREEnvelopeRealTimeResponse ( ) ; }
public void test() { try { readOperation = getReadOperation ( ) ; } catch ( Exception exn ) { LOG . error ( exn . getMessage ( ) , exn ) ; readOperation = null ; return new NullProgressTracker ( ) ; } }
public void test() { try { grpcWriteOperation = Iterables . getOnlyElement ( Iterables . filter ( operations , RemoteGrpcPortWriteOperation . class ) ) ; bundleProcessOperation = Iterables . getOnlyElement ( Iterables . filter ( operations , RegisterAndProcessBundleOperation . class ) ) ; } catch ( IllegalArgumentException | NoSuchElementException exn ) { LOG . debug ( exn . getMessage ( ) , exn ) ; grpcWriteOperation = null ; bundleProcessOperation = null ; } }
public void test() { try { cQuery . registerCq ( clientProxyId , ccn , cqState ) ; code_block = IfStatement ; } catch ( CqException cqe ) { logger . warn ( "Unable to register Cq: {}" , cqe . getMessage ( ) ) ; throw cqe ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try ( FileInputStream fis = new FileInputStream ( file ) ) { return parseSuppressionRules ( fis ) ; } catch ( SAXException | IOException ex ) { LOGGER . debug ( ex . getMessage ( ) , ex ) ; throw new SuppressionParseException ( ex ) ; } }
public void test() { if ( _log . isInfoEnabled ( ) ) { _log . info ( StringBundler . concat ( "Removing " , companyId , " from company " , companyId , " to " , company . getCompanyId ( ) , " using " , company . getCompanyId ( ) ) ) ; } }
public void test() { if ( _log . isInfoEnabled ( ) ) { _log . info ( StringBundler . concat ( "Removing " , companyId , " from company " , companyId , " to " , company . getCompanyId ( ) , " using " , company . getCompanyId ( ) ) ) ; } }
public void test() { if ( _log . isInfoEnabled ( ) ) { _log . info ( StringBundler . concat ( "Removing " , companyId , " from company " , companyId , " to " , company . getCompanyId ( ) , " using " , company . getCompanyId ( ) ) ) ; } }
public void test() { if ( _log . isInfoEnabled ( ) ) { _log . info ( StringBundler . concat ( "Removing " , companyId , " from company " , companyId , " to " , company . getCompanyId ( ) , " using " , company . getCompanyId ( ) ) ) ; } }
public void test() { try { ri . activate ( ) ; return ri . getComponent ( ) ; } catch ( Exception e ) { logger . error ( "Could not activate component" , e ) ; } }
public void test() { if ( ! serviceClass . getSimpleName ( ) . equals ( "TypeProvider" ) ) { LOG . log ( Level . INFO , "TypeProvider is not supported for type {0}" , serviceClass ) ; } }
public void test() { if ( log . isInfoEnabled ( ) ) { log . info ( msg ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( IOException | RemoteControllerException e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void testExampleFunctionsYamlMatch ( ) throws IOException { Reader input = Streams . reader ( new ResourceUtils ( this ) . getResourceFromUrl ( "example-with-function.yaml" ) ) ; DeploymentPlan plan = platform . pdp ( ) . parseDeploymentPlan ( input ) ; Map < ? , ? > cfg1 = ( Map < ? , ? > ) plan . getServices ( ) . get ( 0 ) . getCustomAttributes ( ) . get ( BrooklynCampReservedKeys . BROOKLYN_CONFIG ) ; Map < ? , ? > cfg = MutableMap . copyOf ( cfg1 ) ; Assert . assertEquals ( cfg . remove ( "literalValue1" ) , "$brooklyn: is a fun place" ) ; Assert . assertEquals ( cfg . remove ( "literalValue2" ) , "$brooklyn: is a fun place" ) ; Assert . assertEquals ( cfg . remove ( "literalValue3" ) , "$brooklyn: is a fun place" ) ; Assert . assertEquals ( cfg . remove ( "literalValue4" ) , "$brooklyn: is a fun place" ) ; Assert . assertEquals ( cfg . remove ( "$brooklyn:1" ) , "key to the city" ) ; Assert . assertTrue ( cfg . isEmpty ( ) , "" + cfg ) ; Assert . assertEquals ( plan . getName ( ) , "example-with-function" ) ; Assert . assertEquals ( plan . getCustomAttributes ( ) . get ( "location" ) , "localhost" ) ; AssemblyTemplate at = platform . pdp ( ) . registerDeploymentPlan ( plan ) ; Assert . assertEquals ( at . getName ( ) , "example-with-function" ) ; Assert . assertEquals ( at . getCustomAttributes ( ) . get ( "location" ) , "localhost" ) ; PlatformComponentTemplate pct = at . getPlatformComponentTemplates ( ) . links ( ) . iterator ( ) . next ( ) . resolve ( ) ; Object cfg2 = pct . getCustomAttributes ( ) . get ( BrooklynCampReservedKeys . BROOKLYN_CONFIG ) ; Assert . assertEquals ( cfg2 == pct
public void test() { try { JSONObject form = json . getJSONObject ( "form" ) ; putPropertyArrayToObject ( form ) ; Object fields = form . get ( "field" ) ; code_block = IfStatement ; } catch ( JSONException e ) { LOG . error ( e ) ; } }
public void test() { try { ProcessorFactory . getProcessor ( thing ) . processInfoUpdate ( this , station , name , location ) ; } catch ( ProcessorNotFoundException e ) { logger . error ( "Unable to find station " + name ) ; } }
public void test() { if ( _log . isInfoEnabled ( ) ) { _log . info ( StringBundler . concat ( "Removing " , companyId , " from company " , companyId , " to " , company . getCompanyId ( ) , " using " , company . getCompanyId ( ) ) ) ; } }
public void test() { try { return new AdapterRestClient ( this . props , this . getUri ( "/" + aaiVnfId + VF_MODULES + aaiVfModuleId + "/rollback" ) . build ( ) ) . delete ( req , RollbackVfModuleResponse . class ) ; } catch ( InternalServerErrorException e ) { logger . error ( "InternalServerErrorException in rollbackVNFV" , e ) ; throw new VnfAdapterClientException ( e . getMessage ( ) ) ; } }
public void test() { if ( snapshotObj == null ) { s_logger . info ( "Snapshot " + snapshotId + " not found" ) ; snapshotDao . remove ( snapshotId ) ; return true ; } }
public void test() { try { snapshotObj . processEvent ( Snapshot . Event . OperationFailed ) ; } catch ( NoTransitionException e1 ) { s_logger . debug ( "Failed to change snapshot state: " + e1 . toString ( ) ) ; } }
public void test() { try { snapshotObj . processEvent ( Snapshot . Event . DestroyRequested ) ; List < VolumeDetailVO > volumesFromSnapshot = volumeDetailsDaoImpl . findDetails ( "SNAPSHOT_ID" , String . valueOf ( snapshotId ) , null ) ; code_block = IfStatement ; } catch ( NoTransitionException e ) { s_logger . debug ( "Failed to destroy snapshot" , e ) ; return false ; } }
public void test() { try { snapshotSvr . deleteSnapshot ( snapshotObj ) ; snapshotObj . processEvent ( Snapshot . Event . OperationSucceeded ) ; UsageEventUtils . publishUsageEvent ( EventTypes . EVENT_SNAPSHOT_OFF_PRIMARY , snapshotObj . getAccountId ( ) , snapshotObj . getDataCenterId ( ) , snapshotId , snapshotObj . getName ( ) , null , null , 0L , snapshotObj . getClass ( ) . getName ( ) , snapshotObj . getUuid ( ) ) ; } catch ( Exception e ) { logger . error ( "Failed to delete snapshot: " + snapshotObj , e ) ; code_block = TryStatement ;  return false ; } }
public void test() { try { snapshotObj . processEvent ( Snapshot . Event . OperationFailed ) ; } catch ( NoTransitionException e1 ) { s_logger . debug ( "Failed to change snapshot state: " + e1 . toString ( ) ) ; } }
public void test() { try { initConcreteDataGrid ( ( JvstmDataGridConfig ) jvstmConfig ) ; } catch ( Exception e ) { LOGGER . error ( e ) ; throw new RuntimeException ( e ) ; } }
private void initFile ( File writeOnlyFile ) throws IOException { code_block = IfStatement ; code_block = IfStatement ; writeOnlyFile . createNewFile ( ) ; writeOnlyFile . deleteOnExit ( ) ; addShutDownHook ( ) ; LOG . info ( "Created file {}" , writeOnlyFile . getAbsolutePath ( ) ) ; }
public synchronized void removePendingHpcJob ( final HpcJobInfo hpcJobInfo ) { HpcAccount hpcAccount = hpcJobInfo . getHpcAccount ( ) ; LOGGER . debug ( "removedPendingHpcJob: name: " + hpcJobInfo . getHpcAccountName ( ) ) ; LOGGER . debug ( "removedPendingHpcJob: algorithm: " + hpcJobInfo . getAlgoId ( ) ) ; LOGGER . debug ( "removedPendingHpcJob: status: " + hpcJobInfo . getStatus ( ) ) ; LOGGER . debug ( "removedPendingHpcJob: pid: " + hpcJobInfo . getPid ( ) ) ; Set < HpcJobInfo > hpcJobInfos = pendingHpcJobInfoMap . get ( hpcAccount ) ; code_block = IfStatement ; }
public synchronized void removePendingHpcJob ( final HpcJobInfo hpcJobInfo ) { HpcAccount hpcAccount = hpcJobInfo . getHpcAccount ( ) ; LOGGER . debug ( "removedPendingHpcJob: connection: " + hpcAccount . getConnectionName ( ) ) ; LOGGER . debug ( "removedPendingHpcJob: status: " + hpcJobInfo . getStatus ( ) ) ; LOGGER . debug ( "removedPendingHpcJob: pid: " + hpcJobInfo . getPid ( ) ) ; LOGGER . debug ( "removedPendingHpcJob: " + hpcJobInfo . getPid ( ) ) ; Set < HpcJobInfo > hpcJobInfos = pendingHpcJobInfoMap . get ( hpcAccount ) ; code_block = IfStatement ; }
public synchronized void removePendingHpcJob ( final HpcJobInfo hpcJobInfo ) { HpcAccount hpcAccount = hpcJobInfo . getHpcAccount ( ) ; LOGGER . debug ( "removedPendingHpcJob: connection: " + hpcAccount . getConnectionName ( ) ) ; LOGGER . debug ( "removedPendingHpcJob: algorithm: " + hpcJobInfo . getAlgoId ( ) ) ; LOGGER . debug ( "removedPendingHpcJob: pid: " + hpcJobInfo . getPid ( ) ) ; LOGGER . debug ( "removedPendingHpcJob: " + hpcJobInfo . getPid ( ) ) ; Set < HpcJobInfo > hpcJobInfos = pendingHpcJobInfoMap . get ( hpcAccount ) ; code_block = IfStatement ; }
public synchronized void removePendingHpcJob ( final HpcJobInfo hpcJobInfo ) { HpcAccount hpcAccount = hpcJobInfo . getHpcAccount ( ) ; LOGGER . debug ( "removedPendingHpcJob: connection: " + hpcAccount . getConnectionName ( ) ) ; LOGGER . debug ( "removedPendingHpcJob: algorithm: " + hpcJobInfo . getAlgoId ( ) ) ; LOGGER . debug ( "removedPendingHpcJob: status: " + hpcJobInfo . getStatus ( ) ) ; LOGGER . debug ( "removedPendingHpcJob: " + hpcJobInfos . toString ( ) ) ; Set < HpcJobInfo > hpcJobInfos = pendingHpcJobInfoMap . get ( hpcAccount ) ; code_block = IfStatement ; }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Refreshing." ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Refreshing." ) ; } }
public void test() { try { executor . submit ( new RunCommand ( successCommand , exchange ) ) . get ( ) ; } catch ( Exception e ) { LOG . warn ( "Error during run command execution." , e ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getCapabilities()" ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getCapabilities()" ) ; } }
public void test() { try { do code_block = "" ; while ( in . available ( ) > 0 ) ; } catch ( IOException e1 ) { logger . error ( e1 . getMessage ( ) , e1 ) ; } }
public void test() { try { ParameterizedType pt = ( ParameterizedType ) f . getGenericType ( ) ; return ( Type [ ] ) pt . getActualTypeArguments ( ) ; } catch ( Exception e ) { logger . log ( Level . WARNING , "Could not find method to " + f . getName ( ) , e ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { String orcid = emailManagerReadOnly . findOrcidIdByEmail ( username ) ; code_block = IfStatement ; } catch ( javax . persistence . NoResultException nre ) { LOGGER . debug ( "No user found" , nre ) ; } catch ( Exception e ) { LOGGER . error ( "Error finding user " + username , e ) ; } }
public void test() { try { String orcid = emailManagerReadOnly . findOrcidIdByEmail ( username ) ; code_block = IfStatement ; } catch ( javax . persistence . NoResultException nre ) { LOGGER . error ( "User " + username + " was not found" ) ; } catch ( Exception e ) { LOGGER . error ( "Error while retrieving user " + username , e ) ; } }
public void test() { try { ApplicationContext appContext = WebApplicationContextUtils . getWebApplicationContext ( this . getServletContext ( ) ) ; testcaseService = appContext . getBean ( ITestCaseService . class ) ; applicationService = appContext . getBean ( IApplicationService . class ) ; PolicyFactory policy = Sanitizers . FORMATTING . and ( Sanitizers . LINKS ) ; String test = policy . sanitize ( httpServletRequest . getParameter ( "test" ) ) ; String testcase = policy . sanitize ( httpServletRequest . getParameter ( "testcase" ) ) ; JSONObject export = new JSONObject ( ) ; export . put ( "version" , Infos . getInstance ( ) . getProjectVersion ( ) ) ; export . put ( "user" , httpServletRequest . getUserPrincipal ( ) ) ; SimpleDateFormat formatter = new SimpleDateFormat ( "yyyy-MM-dd'T'HH:mm:ss.SSSXXX" ) ; export . put ( "date" , formatter . format ( new Date ( ) ) ) ; TestCase tcInfo = testcaseService . findTestCaseByKeyWithDependency ( test , testcase ) ; ObjectMapper mapper = new ObjectMapper ( ) ; JSONObject tcInfoJSON = new JSONObject ( mapper . writeValueAsString ( tcInfo ) ) ; tcInfoJSON . remove ( "bugs" ) ; tcInfoJSON . put ( "bugs" , tcInfo . getBugs ( ) ) ; tcInfoJSON . remove ( "conditionOptions" ) ; tcInfoJSON . put ( "conditionOptions" , tcInfo . getConditionOptions ( ) ) ; JSONArray tcJA = new JSONArray ( ) ; tcJA . add ( tcInfoJSON ) ; export . put ( "testcases" , tcJA ) ; Application appInfo = applicationService . convert ( applicationService . readByKey ( tcInfo . getApplication ( ) ) ) ; JSONObject app = new JSONObject ( mapper . writeValueAsString ( appInfo ) ) ; export . put ( "application" , app ) ; export . put ( "invariants" , new JSONArray ( ) ) ; export . put ( "applicationsObjects" , new JSONArray ( ) ) ; export . put ( "datalibs" ,
public void electLeader ( ) { log . debug ( "Starting the leader election" ) ; jobNodeStorage . executeInLeader ( LeaderNode . LATCH , new LeaderElectionExecutionCallback ( ) ) ; log . debug ( "Leader election completed." ) ; }
@ Override public List < IBaseResource > process ( List < ResourcePersistentId > theResourcePersistentId ) { String collect = theResourcePersistentId . stream ( ) . map ( pid -> pid . getId ( ) . toString ( ) ) . collect ( Collectors . joining ( "," ) ) ; ourLog . trace ( "Processing search: {}" , collect ) ; IFhirResourceDao < ? > dao = myDaoRegistry . getResourceDao ( myResourceType ) ; Class < ? extends IBaseResource > resourceTypeClass = myContext . getResourceDefinition ( myResourceType ) . getImplementingClass ( ) ; ISearchBuilder sb = mySearchBuilderFactory . newSearchBuilder ( dao , myResourceType , resourceTypeClass ) ; List < IBaseResource > outgoing = new ArrayList < > ( ) ; sb . loadResourcesByPid ( theResourcePersistentId , Collections . emptyList ( ) , outgoing , false , null ) ; ourLog . trace ( "Loaded resources: {}" , outgoing . stream ( ) . map ( t -> t . getIdElement ( ) . getValue ( ) ) . collect ( Collectors . joining ( ", " ) ) ) ; return outgoing ; }
public void test() { if ( expiredObject != null ) { LOG . debug ( "Found expiredObject: " + key ) ; return expiredObject ; } else { LOG . error ( "ExpiredObject not found: " + key ) ; return expiredObject ; } }
public void test() { if ( expiredObject != null ) { LOG . debug ( "Found ExpiredObject: " + expiredObject . getKey ( ) ) ; return expiredObject ; } else { LOG . debug ( "No expiredObject found!" ) ; return expiredObject ; } }
public void test() { try { conn = provider . getConnection ( ) ; conn . setAutoCommit ( false ) ; PreparedStatement query = conn . prepareStatement ( "select " + this . expiredObjectColumnName + ", value, type, iat, exp from expired_objects where " + this . expiredObjectColumnName + " = ?" ) ; query . setString ( 1 , key . trim ( ) ) ; ResultSet rs = query . executeQuery ( ) ; ExpiredObject expiredObject = null ; rs . next ( ) ; code_block = IfStatement ; query . close ( ) ; conn . commit ( ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Error while retrieving object from expired object." , e ) ; rollbackSilently ( conn ) ; return null ; } finally { IOUtils . closeSilently ( conn ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( null == entry . elementText ( "updated" ) ) { logger . error ( "Entry not found" ) ; return ; } }
public void test() { try { channelManager . deleteNodeItemById ( node , entry . elementText ( "id" ) ) ; } catch ( NodeStoreException e ) { logger . error ( e ) ; } }
public void test() { try { code_block = IfStatement ; String inReplyTo = null ; Element reply ; code_block = IfStatement ; Date updatedDate = Conf . parseDate ( entry . elementText ( "updated" ) ) ; NodeItemImpl nodeItem = new NodeItemImpl ( node , GlobalItemIDImpl . toLocalId ( entry . elementText ( "id" ) ) , updatedDate , entry . asXML ( ) , inReplyTo ) ; code_block = TryStatement ;  channelManager . addNodeItem ( nodeItem ) ; } catch ( IllegalArgumentException e ) { log . error ( e . getMessage ( ) ) ; e . printStackTrace ( ) ; return ; } }
public void test() { try { code_block = IfStatement ; } catch ( JsonSyntaxException e ) { logger . debug ( "JsonSyntaxException occurred during execution: {}" , e . getMessage ( ) , e ) ; } }
public void test() { try { registryRoot = registryHelper . ensureDirectory ( PIPELINE_EXECUTOR_REGISTRY_PATH ) ; } catch ( Exception e ) { LOG . error ( "Could not create registry root directory" , e ) ; } }
public void test() { if ( ! Arrays . equals ( mLatestLayout , layoutUpdate ) ) { LOG . debug ( "Updating layout update {}" , layoutUpdate ) ; mLatestLayout = layoutUpdate ; mHandler . update ( layoutUpdate ) ; } }
public void test() { try { final byte [ ] layoutUpdate = mZKClient . getData ( mTableLayoutFile , mWatcher , mLayoutStat ) ; code_block = IfStatement ; } catch ( KeeperException . NoNodeException nne ) { LOG . debug ( "No node found during layout update" ) ; } catch ( KeeperException ke ) { LOG . error ( "Unrecoverable ZooKeeper error: {}" , ke . getMessage ( ) ) ; } }
public void test() { try { final byte [ ] layoutUpdate = mZKClient . getData ( mTableLayoutFile , mWatcher , mLayoutStat ) ; code_block = IfStatement ; } catch ( KeeperException . NoNodeException nne ) { LOG . info ( "Tracked table layout node for table {} has been removed. Tracking will cease." , mTableURI ) ; } catch ( KeeperException ke ) { LOG . error ( "Couldn't update layout for table {}" , mTableURI , ke ) ; } }
public void test() { try { LOG . debug ( "Starting SNMP Trap producer on {}" , this . endpoint . getAddress ( ) ) ; code_block = IfStatement ; snmp = new Snmp ( transport ) ; LOG . debug ( "SnmpTrap: getting pdu from body" ) ; PDU trap = exchange . getIn ( ) . getBody ( PDU . class ) ; trap . setErrorIndex ( 0 ) ; trap . setErrorStatus ( 0 ) ; trap . setMaxRepetitions ( 0 ) ; LOG . debug ( "SnmpTrap: sent" ) ; code_block = IfStatement ; snmp . send ( trap , this . target ) ; LOG . debug ( "SnmpTrap: sent" ) ; } finally { code_block = TryStatement ;  code_block = TryStatement ;  } }
public void test() { try { LOG . debug ( "Starting SNMP Trap producer on {}" , this . endpoint . getAddress ( ) ) ; code_block = IfStatement ; snmp = new Snmp ( transport ) ; LOG . debug ( "SnmpTrap: getting pdu from body" ) ; PDU trap = exchange . getIn ( ) . getBody ( PDU . class ) ; trap . setErrorIndex ( 0 ) ; trap . setErrorStatus ( 0 ) ; trap . setMaxRepetitions ( 0 ) ; LOG . debug ( "SnmpTrap: sending back" ) ; code_block = IfStatement ; LOG . debug ( "SnmpTrap: sending" ) ; snmp . send ( trap , this . target ) ; } finally { code_block = TryStatement ;  code_block = TryStatement ;  } }
public void test() { try { JcrWorkspaceFilter . saveFilter ( filter , defNode , autoSave ) ; } catch ( RepositoryException e ) { log . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { FileVersion fileVersion = fileEntry . getFileVersion ( ) ; code_block = TryStatement ;  } catch ( Exception exception ) { _log . error ( exception , exception ) ; return null ; } }
@ Test public void callMethodWithExistingErrorEnum ( ) { logger . info ( name . getMethodName ( ) + "" ) ; code_block = TryStatement ;  code_block = TryStatement ;  code_block = TryStatement ;  logger . info ( name . getMethodName ( ) + " - OK" ) ; }
@ Test public void callMethodWithExistingErrorEnum ( ) { logger . info ( name . getMethodName ( ) + "" ) ; code_block = TryStatement ;  code_block = TryStatement ;  code_block = TryStatement ;  logger . info ( name . getMethodName ( ) + " OK" ) ; }
public void test() { if ( totalQueryTime > _defaultLargeQueryLatencyMs ) { _log . info ( "Large query time reported {} ms." , totalQueryTime ) ; } }
@ Test public void testResponseUp ( ) { log . info ( "testResponseUp" ) ; WebTarget target = target ( ) . path ( "logs" ) ; String s = target . request ( ) . get ( String . class ) ; assertEquals ( "Logs resource is up!" , s ) ; }
public void test() { if ( cancelParams instanceof CancelParams ) { String id = ( ( CancelParams ) cancelParams ) . getId ( ) ; LOG . debug ( "Client cancels: " + id ) ; CompletableFuture < ? > future ; synchronized ( receivedRequestMap ) code_block = "" ; if ( future != null ) future . cancel ( true ) ; else LOG . debug ( "Unmatched cancel notification for request id " + id ) ; return true ; } else { LOG . warn ( "Unknown cancel request for request id " + id ) ; } }
public void test() { if ( cancelParams != null ) { code_block = IfStatement ; } else { log . warn ( "No cancel request." ) ; } }
public void test() { try { code_block = IfStatement ; UserDetails user = this . extractUser ( authentication . getPrincipal ( ) . toString ( ) , authentication . getCredentials ( ) . toString ( ) , false ) ; code_block = IfStatement ; } catch ( AuthenticationException e ) { throw e ; } catch ( Exception e ) { logger . error ( "Error detected during user authentication" , e ) ; throw new AuthenticationServiceException ( "Error detected during user authentication" , e ) ; } }
public void test() { if ( ! isWSDLAvailable ( serviceUrl ) ) { isServiceUnDeployed = true ; log . info ( serviceUrl + " is not deployed on " + serviceUrl ) ; break ; } }
public void test() { try { siegfriedPlugin . init ( ) ; } catch ( PluginException e ) { LOGGER . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { IdentifierNode [ ] variableAndType = Tokenizer . tokenize ( typeIdentifierNode . getModule ( ) , SpreadsheetSymbols . TYPE_DELIMITER . toString ( ) ) ; code_block = IfStatement ; } catch ( OpenLCompilationException e ) { LOG . debug ( "Could not parse header: {}" , typeIdentifierNode ) ; SyntaxNodeException error = SyntaxNodeExceptionUtils . createError ( "Cannot parse header." , typeIdentifierNode ) ; getBindingContext ( ) . addError ( error ) ; } }
public void test() { if ( log . isFatalEnabled ( ) ) { log . fatal ( msg ) ; } }
public void test() { switch ( mAppearance ) { case FLASHRED : case RED : lowTurnout . getBean ( ) . setCommandedState ( Turnout . THROWN ) ; break ; case FLASHYELLOW : case YELLOW : highTurnout . getBean ( ) . setCommandedState ( Turnout . THROWN ) ; break ; case FLASHGREEN : case GREEN : lowTurnout . getBean ( ) . setCommandedState ( Turnout . CLOSED ) ; break ; case DARK : highTurnout . getBean ( ) . setCommandedState ( Turnout . CLOSED ) ; break ; default : log . warn ( "Unknown appearance: " + mAppearance ) ; } }
@ Override public void ping ( ) { LOG . info ( "ping {}" , getClass ( ) . getSimpleName ( ) ) ; invocationCount ++ ; }
private void assignToOwner ( Entity entity ) { LOG . info ( "Assigning entity {} to owner {}..." , entityIdentity , ownerName ) ; EntityIdentity entityIdentity = new EntityIdentity ( entity ) ; String ownerName = entity . getString ( ownerAttributeName ) ; Sid ownerSid = createUserSid ( ownerName ) ; MutableAcl acl = ( MutableAcl ) mutableAclService . readAclById ( entityIdentity ) ; acl . setOwner ( ownerSid ) ; removeAllEntries ( acl ) ; acl . insertAce ( 0 , WRITE , ownerSid , true ) ; mutableAclService . updateAcl ( acl ) ; LOG . info ( "Assigned entity {} to owner {}." , entityIdentity , ownerName ) ; }
private void assignToOwner ( Entity entity ) { EntityIdentity entityIdentity = new EntityIdentity ( entity ) ; String ownerName = entity . getString ( ownerAttributeName ) ; LOG . debug ( "Assigning entity {} to owner {}..." , entityIdentity , ownerName ) ; Sid ownerSid = createUserSid ( ownerName ) ; MutableAcl acl = ( MutableAcl ) mutableAclService . readAclById ( entityIdentity ) ; acl . setOwner ( ownerSid ) ; removeAllEntries ( acl ) ; acl . insertAce ( 0 , WRITE , ownerSid , true ) ; mutableAclService . updateAcl ( acl ) ; LOG . debug ( "Assigned entity {} to owner {}..." , entityIdentity , ownerName ) ; }
public void test() { try { code_block = IfStatement ; } catch ( HiveException e ) { String msg = "Error in role operation " + operation . getOperationName ( ) + " on role name " + name + ", error message " + e . getMessage ( ) ; LOG . error ( msg , e ) ; console . printError ( msg ) ; return RETURN_CODE_FAILURE ; } catch ( IOException e ) { String msg = "IO Error in role operation " + e . getMessage ( ) ; LOG . info ( msg , e ) ; console . printError ( msg ) ; return RETURN_CODE_FAILURE ; } finally { closeQuiet ( outStream ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( HiveException e ) { String msg = "Error in role operation " + operation . getOperationName ( ) + " on role name " + name + ", error message " + e . getMessage ( ) ; LOG . warn ( msg , e ) ; console . printError ( msg ) ; return RETURN_CODE_FAILURE ; } catch ( IOException e ) { String msg = "IO Error in role operation " + e . getMessage ( ) ; LOG . error ( msg , e ) ; console . printError ( msg ) ; return RETURN_CODE_FAILURE ; } finally { closeQuiet ( outStream ) ; } }
public void test() { try { return Context . getCurrentContext ( ) . evaluateString ( scope , IOUtils . toString ( is ) , filename , 0 , null ) ; } catch ( IOException e ) { LOGGER . error ( "Unable to evaluate {}" , filename , e ) ; } }
@ Override public void initialize ( ) { logger . debug ( "Initializing thing {}" , getThing ( ) . getUID ( ) ) ; commManager = null ; String threadName = "OH-binding-" + getThing ( ) . getUID ( ) . getAsString ( ) ; String errorMsg = null ; code_block = IfStatement ; code_block = IfStatement ; }
public void test() { try { logger . trace ( "Powermax job..." ) ; updateMotionSensorState ( ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Powermax job error: " , e ) ; } }
public void test() { if ( graphResponse . get ( "response" ) . getAsJsonObject ( ) . get ( "results" ) . getAsJsonArray ( ) . get ( 0 ) . getAsJsonObject ( ) . get ( "data" ) . getAsJsonArray ( ) . size ( ) == 0 ) { healthQuery = "" ; healthQuery = healthQuery + CREATE + nodeLabels + " {props})" ; JsonObject graphResponse1 = dbHandler . executeQueryWithData ( healthQuery , dataList ) ; log . debug ( "GraphQLResponse1 {}" , graphResponse1 ) ; parseGraphResponseForError ( graphResponse1 , routingKey ) ; } }
public void test() { if ( customArgument instanceof AdditionalInfo ) { final AdditionalInfo info = ( AdditionalInfo ) customArgument ; final String authId = info . get ( EXT_INFO_KEY_HONO_AUTH_ID , String . class ) ; final Device device = info . get ( EXT_INFO_KEY_HONO_DEVICE , Device . class ) ; LOG . trace ( "Extracting additionalInfo {} from device {}" , authId , device ) ; return info ; } }
public void recordImportFailure ( Owner owner , Throwable error , String filename ) { ImportRecord record = new ImportRecord ( owner ) ; code_block = IfStatement ; record . setUpstreamConsumer ( createImportUpstreamConsumer ( owner , null ) ) ; record . setFileName ( filename ) ; record . recordStatus ( ImportRecord . Status . FAILURE , error . getMessage ( ) ) ; this . importRecordCurator . create ( record ) ; log . debug ( "Failed to record import failure {}: {}" , record , error ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( context == null ) { transaction = createAndActivateTransaction ( clazz , signature ) ; } else-if ( active == null ) { logger . debug ( "Creating transaction for method {}" , signature ) ; transaction = createAndActivateTransaction ( clazz , context . getJobDetail ( ) . getKey ( ) . toString ( ) ) ; } else { logger . debug ( "Not creating transaction for method {} because there is already a transaction running ({})" , signature , active ) ; } }
public void test() { if ( context == null ) { logger . warn ( "Cannot correctly name transaction for method {} because JobExecutionContext is null" , signature ) ; transaction = createAndActivateTransaction ( clazz , signature ) ; } else-if ( active == null ) { transaction = createAndActivateTransaction ( clazz , context . getJobDetail ( ) . getKey ( ) . toString ( ) ) ; } else { logger . warn ( "Cannot correctly name transaction for method {}" , signature ) ; } }
public void test() { if ( file . exists ( ) ) { logger . warning ( "File already exists: " + file ) ; } }
@ MCRCommand ( syntax = "export all permissions to file {0}" , help = "Export all permissions from the Access Control System to the file {0}." , order = 50 ) public static void exportAllPermissionsToFile ( String filename ) throws Exception { MCRAccessInterface accessImpl = MCRAccessManager . getAccessImpl ( ) ; Element mcrpermissions = new Element ( "mcrpermissions" ) ; mcrpermissions . addNamespaceDeclaration ( XSI_NAMESPACE ) ; mcrpermissions . addNamespaceDeclaration ( XLINK_NAMESPACE ) ; mcrpermissions . setAttribute ( "noNamespaceSchemaLocation" , "MCRPermissions.xsd" , XSI_NAMESPACE ) ; Document doc = new Document ( mcrpermissions ) ; Collection < String > permissions = accessImpl . getPermissions ( ) ; code_block = ForStatement ; File file = new File ( filename ) ; code_block = IfStatement ; FileOutputStream fos = new FileOutputStream ( file ) ; String mcrEncoding = MCRConfiguration2 . getString ( "MCR.Metadata.DefaultEncoding" ) . orElse ( DEFAULT_ENCODING ) ; XMLOutputter out = new XMLOutputter ( Format . getPrettyFormat ( ) . setEncoding ( mcrEncoding ) ) ; out . output ( doc , fos ) ; LOGGER . info ( "Export all permissions to file {}" , filename ) ; }
@ Test public void test_apply_variant_01 ( ) { Log . debug ( "Test" ) ; Variant variant = new Variant ( transcript . getParent ( ) , 290 , "TTT" , "AAA" ) ; checkApplyMnp ( variant , transcript . cds ( ) , transcript . protein ( ) , 1 , 300 , 399 ) ; }
public boolean clientExistsThenEvict ( Integer demographicNo ) { logger . debug ( "ClientExistsThenEvict {}" , demographicNo ) ; boolean exists = false ; Demographic existingDemo = this . getClientByDemographicNo ( demographicNo ) ; exists = ( existingDemo != null ) ; if ( exists ) this . getHibernateTemplate ( ) . evict ( existingDemo ) ; return exists ; }
public void test() { try { java . util . List < com . liferay . dynamic . data . lists . model . DDLRecordVersion > returnValue = DDLRecordVersionServiceUtil . getRecordVersions ( recordId ) ; return com . liferay . dynamic . data . lists . model . DDLRecordVersionSoap . toSoapModels ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try { byte [ ] arr = U . marshal ( ctx , th ) ; errorsBytes . add ( arr ) ; } catch ( IgniteCheckedException e ) { log . error ( "Failed to marshal data: " , e ) ; } }
public void test() { try { MessageDigest . getInstance ( "MD4" ) ; } catch ( NoSuchAlgorithmException ex ) { logger . error ( ex ) ; return ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( "Closing connection {}" , connection ) ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( "Closing connection {}" , connection ) ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( "Closing connection {}" , connection ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void testBSPTaskSelfDestroy ( ) { CompletionService < Integer > completionService = new ExecutorCompletionService < Integer > ( this . testBSPTaskService ) ; TestBSPProcessRunner runner = new TestBSPProcessRunner ( 0 , workerServer . getListenerAddress ( ) . getPort ( ) ) ; Future < Integer > future = completionService . submit ( runner ) ; code_block = TryStatement ;  workerServer . stop ( ) ; umbilical = null ; workerServer = null ; Integer exitValue = - 1 ; code_block = TryStatement ;  assertEquals ( 69 , exitValue . intValue ( ) ) ; logger . info ( "testBSPTaskSelfDestroy started" ) ; runner . destroyProcess ( ) ; }
public void test() { try { code_block = WhileStatement ; } catch ( Exception e ) { LOG . error ( "Caught exception while running assignments" , e ) ; } }
public void test() { try { exitValue = future . get ( 20000 , TimeUnit . MILLISECONDS ) ; } catch ( InterruptedException e1 ) { LOG . error ( "Interrupted Exception." , e1 ) ; } catch ( ExecutionException e1 ) { LOG . error ( "ExecutionException Exception." , e1 ) ; } catch ( TimeoutException e ) { LOG . error ( "TimeoutException Exception." , e ) ; } }
public void test() { try { exitValue = future . get ( 20000 , TimeUnit . MILLISECONDS ) ; } catch ( InterruptedException e1 ) { LOG . error ( "Interrupted Exception." , e1 ) ; } catch ( ExecutionException e1 ) { LOG . error ( "ExecutionException Exception." , e1 ) ; } catch ( TimeoutException e ) { LOG . error ( "TimeoutException Exception." , e ) ; } }
public void test() { try { exitValue = future . get ( 20000 , TimeUnit . MILLISECONDS ) ; } catch ( InterruptedException e1 ) { LOG . error ( "Interrupted Exception." , e1 ) ; } catch ( ExecutionException e1 ) { LOG . error ( "ExecutionException Exception." , e1 ) ; } catch ( TimeoutException e ) { LOG . error ( "TimeoutException Exception." , e ) ; } }
@ Test public void testLedgerDeleteWithExistingEntryLogs ( ) throws Exception { LedgerHandle [ ] lhs = writeLedgerEntries ( 3 , 1024 , 1024 ) ; restartBookies ( ) ; code_block = ForStatement ; LOG . info ( "Waiting for GC thread to finish" ) ; Thread . sleep ( 2 * baseConf . getGcWaitTime ( ) ) ; code_block = ForStatement ; }
public void test() { { Map < String , String > env = setupEnvironment ( peer . getConfiguration ( ) ) ; List < String > cmd = setupCommand ( peer . getConfiguration ( ) ) ; TaskAttemptID taskid = peer . getTaskId ( ) ; File stdout = TaskLog . getTaskLogFile ( taskid , TaskLog . LogName . STDOUT ) ; File stderr = TaskLog . getTaskLogFile ( taskid , TaskLog . LogName . STDERR ) ; long logLength = TaskLog . getTaskLogLength ( peer . getConfiguration ( ) ) ; code_block = IfStatement ; checkParentFile ( stdout ) ; checkParentFile ( stderr ) ; LOG . debug ( "STDERR: " + stderr . getAbsolutePath ( ) ) ; LOG . debug ( "DEBUG: cmd: " + cmd ) ; LOG . debug ( "DEBUG: cmd: " + cmd ) ; process = runClient ( cmd , env ) ; code_block = TryStatement ;  } }
public void test() { { Map < String , String > env = setupEnvironment ( peer . getConfiguration ( ) ) ; List < String > cmd = setupCommand ( peer . getConfiguration ( ) ) ; TaskAttemptID taskid = peer . getTaskId ( ) ; File stdout = TaskLog . getTaskLogFile ( taskid , TaskLog . LogName . STDOUT ) ; File stderr = TaskLog . getTaskLogFile ( taskid , TaskLog . LogName . STDERR ) ; long logLength = TaskLog . getTaskLogLength ( peer . getConfiguration ( ) ) ; LOG . debug ( "DEBUG: " + logLength ) ; code_block = IfStatement ; checkParentFile ( stdout ) ; LOG . debug ( "STDOUT: " + stdout . getAbsolutePath ( ) ) ; checkParentFile ( stderr ) ; LOG . debug ( "DEBUG: cmd: " + cmd ) ; process = runClient ( cmd , env ) ; code_block = TryStatement ;  } }
public void test() { { Map < String , String > env = setupEnvironment ( peer . getConfiguration ( ) ) ; List < String > cmd = setupCommand ( peer . getConfiguration ( ) ) ; TaskAttemptID taskid = peer . getTaskId ( ) ; File stdout = TaskLog . getTaskLogFile ( taskid , TaskLog . LogName . STDOUT ) ; File stderr = TaskLog . getTaskLogFile ( taskid , TaskLog . LogName . STDERR ) ; long logLength = TaskLog . getTaskLogLength ( peer . getConfiguration ( ) ) ; LOG . info ( "LogLength: " + logLength ) ; code_block = IfStatement ; checkParentFile ( stdout ) ; LOG . debug ( "STDOUT: " + stdout . getAbsolutePath ( ) ) ; checkParentFile ( stderr ) ; LOG . debug ( "STDERR: " + stderr . getAbsolutePath ( ) ) ; process = runClient ( cmd , env ) ; code_block = TryStatement ;  } }
public void test() { if ( streamingEnabled ) { LOG . debug ( "DEBUG: Client connected! - start BinaryProtocol!" ) ; downlink = new StreamingProtocol ( peer , process . getOutputStream ( ) , process . getInputStream ( ) ) ; } else { serverSocket . setSoTimeout ( SERVER_SOCKET_TIMEOUT ) ; clientSocket = serverSocket . accept ( ) ; LOG . debug ( "DEBUG: Client connected! - start BinaryProtocol!" ) ; downlink = new BinaryProtocol < K1 , V1 , K2 , V2 , M > ( peer , clientSocket . getOutputStream ( ) , clientSocket . getInputStream ( ) ) ; } }
public void test() { if ( streamingEnabled ) { LOG . debug ( "DEBUG: waiting for StreamingProtocol to " + serverSocket . getLocalSocketAddress ( ) ) ; downlink = new StreamingProtocol ( peer , process . getOutputStream ( ) , process . getInputStream ( ) ) ; } else { LOG . debug ( "DEBUG: waiting for Client at " + serverSocket . getLocalSocketAddress ( ) ) ; serverSocket . setSoTimeout ( SERVER_SOCKET_TIMEOUT ) ; clientSocket = serverSocket . accept ( ) ; downlink = new BinaryProtocol < K1 , V1 , K2 , V2 , M > ( peer , clientSocket . getOutputStream ( ) , clientSocket . getInputStream ( ) ) ; } }
public void test() { while ( ( inputLine = br . readLine ( ) ) != null ) { logger . info ( inputLine ) ; } }
public void test() { if ( table . getSelectedRow ( ) < 0 ) { MessageDialog . error ( null , "angal.common.pleaseselectarow.msg" ) ; } else { ExamRowBrowsingManager manager = Context . getApplicationContext ( ) . getBean ( ExamRowBrowsingManager . class ) ; ExamRow row = ( ExamRow ) ( ( ( ExamRowBrowsingModel ) model ) . getValueAt ( table . getSelectedRow ( ) , - 1 ) ) ; int n = JOptionPane . showConfirmDialog ( null , MessageBundle . getMessage ( "angal.exa.deleteexamresult" ) + " \"" + row . getDescription ( ) + "\" ?" , MessageBundle . getMessage ( "angal.hospital" ) , JOptionPane . YES_NO_OPTION ) ; code_block = IfStatement ; } }
@ Override public void instantiateServiceInstance ( ) { LOG . info ( "Instantiating {}" , this . getClass ( ) . getSimpleName ( ) ) ; endpointRpcRegistry = new EndpointRpcRegistry ( dataBroker ) ; serviceRpcRegistration = rpcProviderRegistry . addRpcImplementation ( EndpointService . class , this ) ; }
public void test() { try { dispatcher . execute ( ( ) -> deliverNextPending ( ) ) ; } catch ( RejectedExecutionException rje ) { LOG . warn ( "Evicted early!" , rje ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { -> { logger . warn ( "An error was an error." , ex ) ; nc . addNotification ( "Sorry, there was an error." ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( constraint != null ) { log . warn ( "Could not execute constraint " + constraint ) ; } else { exec . forkScenario ( c . getSimpleName ( ) ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( WikiPageServiceUtil . class , "getRecentChanges" , _getRecentChangesParameterTypes35 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , nodeId , start , end ) ; Object returnObj = null ; code_block = TryStatement ;  return ( java . util . List < com . liferay . wiki . model . WikiPage > ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { ResultSet resultSetComment = statement . executeQuery ( sqlQueryComment ) ; code_block = WhileStatement ; JdbcUtils . close ( resultSetComment ) ; } catch ( SQLException e1 ) { logger . error ( "Unable to execute queryComment" , e1 ) ; } }
public void test() { try { int columnCount = metaData . getColumnCount ( ) ; code_block = ForStatement ; Statement statement = connection . createStatement ( ) ; code_block = IfStatement ; JdbcUtils . close ( statement ) ; } catch ( SQLException e ) { log . error ( e . getMessage ( ) , e ) ; } }
public void updateVfModule ( String cloudSiteId , String cloudOwner , String tenantId , String vnfType , String vnfVersion , String vnfName , String requestType , String volumeGroupHeatStackId , String baseVfHeatStackId , String vfModuleStackId , String modelCustomizationUuid , Map < String , Object > inputs , MsoRequest msoRequest , Holder < Map < String , String > > outputs , Holder < VnfRollback > rollback ) throws VnfException { logger . error ( "UpdateVfModule: Unsupported command" ) ; throw new VnfException ( "UpdateVfModule:  Unsupported command" , MsoExceptionCategory . USERDATA ) ; }
private void regexReplace ( String predicate , String regexMatch , String newValue ) throws IOException { String query = "" + "SELECT ?s ?o \n" + "WHERE {\n" + "  ?s <" + predicate + "> ?o .\n" + "  FILTER (regex(str(?o), \"" + regexMatch + "\", \"s\")) .\n" + "}" ; StringBuilder insertQ = new StringBuilder ( "INSERT DATA {\n" ) ; StringBuilder deleteQ = new StringBuilder ( "DELETE DATA {\n" ) ; int modifyCounter = 0 ; code_block = ForStatement ; log . debug ( "Modifying " + Integer . toString ( modifyCounter ) + " Records." ) ; insertQ . append ( "} \n" ) ; deleteQ . append ( "} \n" ) ; log . debug ( "Removing old data:\n" + deleteQ ) ; this . model . executeUpdateQuery ( deleteQ . toString ( ) ) ; log . debug ( "Inserting updated data:\n" + insertQ ) ; this . model . executeUpdateQuery ( insertQ . toString ( ) ) ; log . debug ( "Inserting updated data:\n" + insertQ ) ; }
public void test() { for ( QuerySolution s : IterableAdaptor . adapt ( this . model . executeSelectQuery ( query ) ) ) { modifyCounter ++ ; Literal obj = s . getLiteral ( "o" ) ; RDFDatatype datatype = obj . getDatatype ( ) ; String lang = obj . getLanguage ( ) ; String objStr = obj . getValue ( ) . toString ( ) ; String oldStr = encodeString ( objStr , datatype , lang ) ; log . debug ( "oldValue: " + oldStr ) ; String newStr = encodeString ( objStr . replaceAll ( regexMatch , newValue ) , datatype , lang ) ; log . debug ( "Modified: " + sUri ) ; String sUri = s . getResource ( "s" ) . getURI ( ) ; log . debug ( "newValue: " + newStr ) ; deleteQ . append ( "  <" + sUri + "> <" + predicate + "> " + oldStr + " . \n" ) ; insertQ . append ( "  <" + sUri + "> <" + predicate + "> " + newStr + " . \n" ) ; } }
public void test() { for ( QuerySolution s : IterableAdaptor . adapt ( this . model . executeSelectQuery ( query ) ) ) { modifyCounter ++ ; Literal obj = s . getLiteral ( "o" ) ; RDFDatatype datatype = obj . getDatatype ( ) ; String lang = obj . getLanguage ( ) ; String objStr = obj . getValue ( ) . toString ( ) ; String oldStr = encodeString ( objStr , datatype , lang ) ; log . trace ( "Replacing record" ) ; String newStr = encodeString ( objStr . replaceAll ( regexMatch , newValue ) , datatype , lang ) ; String sUri = s . getResource ( "s" ) . getURI ( ) ; log . debug ( "oldValue: " + sUri ) ; log . debug ( "newValue: " + newStr ) ; deleteQ . append ( "  <" + sUri + "> <" + predicate + "> " + oldStr + " . \n" ) ; insertQ . append ( "  <" + sUri + "> <" + predicate + "> " + newStr + " . \n" ) ; } }
public void test() { for ( QuerySolution s : IterableAdaptor . adapt ( this . model . executeSelectQuery ( query ) ) ) { modifyCounter ++ ; Literal obj = s . getLiteral ( "o" ) ; RDFDatatype datatype = obj . getDatatype ( ) ; String lang = obj . getLanguage ( ) ; String objStr = obj . getValue ( ) . toString ( ) ; String oldStr = encodeString ( objStr , datatype , lang ) ; log . trace ( "Replacing record" ) ; log . debug ( "oldValue: " + oldStr ) ; String newStr = encodeString ( objStr . replaceAll ( regexMatch , newValue ) , datatype , lang ) ; log . trace ( "NewValue: " + newStr ) ; String sUri = s . getResource ( "s" ) . getURI ( ) ; deleteQ . append ( "  <" + sUri + "> <" + predicate + "> " + oldStr + " . \n" ) ; insertQ . append ( "  <" + sUri + "> <" + predicate + "> " + newStr + " . \n" ) ; } }
private void regexReplace ( String predicate , String regexMatch , String newValue ) throws IOException { String query = "" + "SELECT ?s ?o \n" + "WHERE {\n" + "  ?s <" + predicate + "> ?o .\n" + "  FILTER (regex(str(?o), \"" + regexMatch + "\", \"s\")) .\n" + "}" ; log . debug ( query ) ; StringBuilder insertQ = new StringBuilder ( "INSERT DATA {\n" ) ; StringBuilder deleteQ = new StringBuilder ( "DELETE DATA {\n" ) ; int modifyCounter = 0 ; code_block = ForStatement ; insertQ . append ( "} \n" ) ; deleteQ . append ( "} \n" ) ; log . debug ( "Removing old data:\n" + deleteQ ) ; log . debug ( "Inserting updated data:\n" + insertQ ) ; this . model . executeUpdateQuery ( deleteQ . toString ( ) ) ; log . debug ( "Inserting updated data:\n" + insertQ ) ; this . model . executeUpdateQuery ( insertQ . toString ( ) ) ; }
private void regexReplace ( String predicate , String regexMatch , String newValue ) throws IOException { String query = "" + "SELECT ?s ?o \n" + "WHERE {\n" + "  ?s <" + predicate + "> ?o .\n" + "  FILTER (regex(str(?o), \"" + regexMatch + "\", \"s\")) .\n" + "}" ; log . debug ( query ) ; StringBuilder insertQ = new StringBuilder ( "INSERT DATA {\n" ) ; StringBuilder deleteQ = new StringBuilder ( "DELETE DATA {\n" ) ; int modifyCounter = 0 ; code_block = ForStatement ; log . debug ( "Modifying " + Integer . toString ( modifyCounter ) + " Records." ) ; insertQ . append ( "} \n" ) ; deleteQ . append ( "} \n" ) ; log . debug ( "Deleting " + deleteQ . toString ( ) ) ; this . model . executeUpdateQuery ( deleteQ . toString ( ) ) ; log . debug ( "Inserting updated data:\n" + insertQ ) ; this . model . executeUpdateQuery ( insertQ . toString ( ) ) ; }
private void regexReplace ( String predicate , String regexMatch , String newValue ) throws IOException { String query = "" + "SELECT ?s ?o \n" + "WHERE {\n" + "  ?s <" + predicate + "> ?o .\n" + "  FILTER (regex(str(?o), \"" + regexMatch + "\", \"s\")) .\n" + "}" ; log . debug ( query ) ; StringBuilder insertQ = new StringBuilder ( "INSERT DATA {\n" ) ; StringBuilder deleteQ = new StringBuilder ( "DELETE DATA {\n" ) ; int modifyCounter = 0 ; code_block = ForStatement ; log . debug ( "Modifying " + Integer . toString ( modifyCounter ) + " Records." ) ; insertQ . append ( "} \n" ) ; deleteQ . append ( "} \n" ) ; log . debug ( "Adding old data:\n" + deleteQ ) ; log . debug ( "Removing old data:\n" + deleteQ ) ; this . model . executeUpdateQuery ( deleteQ . toString ( ) ) ; this . model . executeUpdateQuery ( insertQ . toString ( ) ) ; }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
private void shrinkIfBelowCapacity ( ) { int currentLoad = maxTotalInFlight . getAndSet ( totalInFlight . get ( ) ) ; int maxRequestsPerConnection = options ( ) . getMaxRequestsPerConnection ( hostDistance ) ; int needed = currentLoad / maxRequestsPerConnection + 1 ; if ( currentLoad % maxRequestsPerConnection > options ( ) . getNewConnectionThreshold ( hostDistance ) ) needed += 1 ; needed = Math . max ( needed , options ( ) . getCoreConnectionsPerHost ( hostDistance ) ) ; int actual = open . get ( ) ; int toTrash = Math . max ( 0 , actual - needed ) ; logger . debug ( "Trash: " + toTrash ) ; if ( toTrash <= 0 ) return ; for ( Connection connection : connections ) code_block = IfStatement ; }
public void test() { try { Files . delete ( file . toPath ( ) ) ; } catch ( IOException e ) { log . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { session = hibernateTemplate . getSessionFactory ( ) . openSession ( ) ; query = session . createQuery ( "From ComprehensionTestQuestionBo CTQBO where CTQBO.studyId=:studyId" + " and CTQBO.active=1 order by CTQBO.sequenceNo asc" ) ; query . setInteger ( "studyId" , studyId ) ; comprehensionTestQuestionList = query . list ( ) ; } catch ( Exception e ) { logger . error ( "StudyDAOImpl - getStudyQuestionList() - ERROR " , e ) ; } finally { code_block = IfStatement ; } }
@ Override public void start ( Map < String , String > configProps ) { this . configProps = configProps ; logger . info ( "Starting..." ) ; }
public static void main ( String [ ] args ) throws Exception { StorageCleanupJob cli = new StorageCleanupJob ( ) ; cli . execute ( args ) ; LOGGER . info ( "Job finished." ) ; }
public void test() { try { ObjectMetadata objectMetadata = ( ObjectMetadata ) callCOSClientWithRetry ( getObjectMetadataRequest ) ; return objectMetadata . getContentLength ( ) ; } catch ( Exception e ) { String errMsg = String . format ( "Getting file length occurs an exception." + "COS key: %s, exception: %s" , key , e . toString ( ) ) ; LOGGER . error ( errMsg , e ) ; handleException ( new Exception ( errMsg ) , key ) ; return 0 ; } }
private void reAttemptDelivery ( Mail mail , int retries ) throws MailQueue . MailQueueException { DeliveryRetriesHelper . incrementRetries ( mail ) ; mail . setLastUpdated ( dateSupplier . get ( ) ) ; Duration delay = getNextDelay ( DeliveryRetriesHelper . retrieveRetries ( mail ) ) ; code_block = IfStatement ; queue . enQueue ( mail , delay ) ; LOG . debug ( "Delivering message {} to {} after {} attempts" , mail . getSubject ( ) , retries , delay ) ; }
protected void onInPacketAdded ( final String networkId , final InPacketAdded msg ) { log . debug ( "" ) ; code_block = IfStatement ; }
public void test() { try { DataSources . destroy ( sourceConfig . getDataSource ( ) ) ; } catch ( SQLException e ) { LOGGER . error ( "Error destroying data source " + sourceConfig . getDataSource ( ) , e ) ; } }
public void test() { try { txn . start ( ) ; String sql = INSERT_ACCOUNT ; PreparedStatement pstmt = null ; pstmt = txn . prepareAutoCloseStatement ( sql ) ; code_block = ForStatement ; pstmt . executeBatch ( ) ; txn . commit ( ) ; } catch ( Exception ex ) { txn . rollback ( ) ; s_logger . error ( "Error inserting account instances" , ex ) ; throw new CloudRuntimeException ( ex . getMessage ( ) ) ; } }
public void test() { if ( topic . isPresent ( ) ) { Future future = kafkaProducer . send ( new ProducerRecord < String , String > ( topic . get ( ) , jsonMessage ) ) ; results . add ( new AbstractMap . SimpleEntry < > ( messageId , future ) ) ; } else { LOG . warn ( "Topic {} not found." , messageId ) ; } }
public void test() { try { SearchControls ctls = new SearchControls ( ) ; ctls . setReturningObjFlag ( true ) ; ctls . setSearchScope ( SearchControls . OBJECT_SCOPE ) ; ctls . setReturningAttributes ( new String [ ] code_block = "" ; ) ; NamingEnumeration < SearchResult > objResults = ctx . search ( "" , "objectclass=*" , ctls ) ; code_block = WhileStatement ; if ( dNs . isEmpty ( ) ) LOGGER . warn ( "No base DNs could be located for LDAP context" ) ; } catch ( Exception e ) { LOGGER . error ( "" , e ) ; } }
@ PayloadRoot ( localPart = "SendNotificationRequest" , namespace = MICROGRIDS_NOTIFICATION_NAMESPACE ) @ ResponsePayload public SendNotificationResponse sendNotification ( @ OrganisationIdentification final String organisationIdentification , @ RequestPayload final SendNotificationRequest request ) throws WebServiceException { LOGGER . info ( "Incoming SendNotificationRequest for organisation: {} device: {}." , organisationIdentification , request . getNotification ( ) . getDeviceIdentification ( ) ) ; this . notificationService . handleNotification ( request . getNotification ( ) , organisationIdentification ) ; return new SendNotificationResponse ( ) ; }
public void test() { try { date = sdf . parse ( strDate ) ; cal . setTime ( date ) ; month = cal . get ( Calendar . MONTH ) ; year = cal . get ( Calendar . YEAR ) ; } catch ( ParseException ex ) { log . error ( "Error parsing date format: " + strDate ) ; } }
public void test() { try { patternKey = Pattern . compile ( regExpKey ) ; patterns . put ( regExpKey , patternKey ) ; } catch ( PatternSyntaxException ex ) { logger . warn ( "Invalid regular expression: " + regExpKey ) ; } }
public void test() { try { patternValue = Pattern . compile ( regExpValue ) ; patterns . put ( regExpValue , patternValue ) ; } catch ( PatternSyntaxException ex ) { logger . error ( "Invalid regular expression: " + regExpValue , ex ) ; } }
public void test() { try { double value = Double . parseDouble ( sValue ) ; code_block = IfStatement ; } catch ( NumberFormatException ex ) { LOGGER . error ( "Can not parse the double" , ex ) ; } }
public void test() { if ( ! found ) { s_logger . debug ( "Unable to find property " + propName ) ; } }
public void test() { if ( confirmationServletFactory == null ) { LOG . info ( "No confirmation servletFactory set. Cannot create confirmation servlet." ) ; return ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { { logger . debug ( "Removing group '{}'" , groupName ) ; assertObjectExists ( groupManagerService . get ( groupName ) , "group" , groupName ) ; UberfireRestResponse response = resourceHelper . removeGroup ( groupName ) ; return createResponse ( response ) ; } }
public void test() { try { realConsumer . accept ( objectMapper . readValue ( value , clazz ) ) ; } catch ( JsonProcessingException e ) { log . warn ( "Object {} is malformed" , value , e ) ; } }
public void test() { try { String [ ] modes = StringUtils . split ( mode , ',' ) ; world = AccessType . valueOf ( modes [ 0 ] . trim ( ) ) ; trusted = AccessType . valueOf ( modes [ 1 ] . trim ( ) ) ; } catch ( RuntimeException e ) { LOG . error ( "Invalid access type." , e ) ; } }
public void test() { if ( item != null && item instanceof KeyValue ) { final KeyValue keyValue = ( KeyValue ) item ; result . put ( keyValue . getKey ( ) , keyValue . getValue ( ) ) ; } else { Log . warn ( "Failed to load command '{}' of type '{}'" , item . getClass ( ) . getName ( ) , item . getClass ( ) . getName ( ) ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { if ( errorOk ) { return null ; } else { String errorMessage = String . format ( "Error performing %s on %s: %d, %s" , method , path , response . code ( ) , responseString ) ; LOGGER . info ( errorMessage ) ; throw new RuntimeException ( errorMessage ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { if ( e instanceof ThriftSecurityException ) { result . sec = ( ThriftSecurityException ) e ; result . setSecIsSet ( true ) ; msg = result ; } else-if ( e instanceof ThriftTableOperationException ) { result . tope = ( ThriftTableOperationException ) e ; result . setTopeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof ThriftSecurityException ) { result . sec = ( ThriftSecurityException ) e ; result . setSecIsSet ( true ) ; msg = result ; } else-if ( e instanceof ThriftTableOperationException ) { result . tope = ( ThriftTableOperationException ) e ; result . setTopeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof ThriftSecurityException ) { result . sec = ( ThriftSecurityException ) e ; result . setSecIsSet ( true ) ; msg = result ; } else-if ( e instanceof ThriftTableOperationException ) { result . tope = ( ThriftTableOperationException ) e ; result . setTopeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { try { fcall . sendResponse ( fb , msg , msgType , seqid ) ; } catch ( java . lang . Exception ex ) { _LOGGER . error ( "Exception writing to internal frame buffer" , ex ) ; fb . close ( ) ; } }
private void takeSnapshot ( OutputStream out ) throws IOException { log . info ( "Taking snapshot for region {}" , getClass ( ) . getName ( ) ) ; ObjectOutputStream stream = new ObjectOutputStream ( out ) ; stream . writeUTF ( getClass ( ) . getName ( ) ) ; stream . writeInt ( LOG_VERSION ) ; stream . writeObject ( myServiceID ) ; stream . writeLong ( eventID ) ; stream . writeInt ( unicastPort ) ; stream . writeObject ( memberGroups ) ; stream . writeObject ( lookupGroups ) ; stream . writeLong ( announcementSeqNo ) ; marshalAttributes ( lookupAttrs , stream ) ; marshalLocators ( lookupLocators , stream ) ; code_block = ForStatement ; stream . writeObject ( null ) ; code_block = ForStatement ; stream . writeObject ( null ) ; stream . flush ( ) ; }
public void test() { if ( ploc . frameIndexList . size ( ) >= MAX_FRAME_DEPTH ) { LOGGER . error ( "FRAME_DEPTH_DEPTH {}" , ploc . toString ( ) ) ; return null ; } }
@ Override public void connectionLost ( Throwable throwable ) { String message = getLogPrefix ( ) + "connection lost" ; log . error ( message , throwable ) ; receiver . getAdapter ( ) . getMessageKeeper ( ) . add ( message ) ; ibisExceptionListener . exceptionThrown ( this , throwable ) ; }
public void test() { try ( Writer writer = new FileWriterWithEncoding ( cacheFile , StandardCharsets . UTF_8 ) ) { writer . write ( token ) ; writer . flush ( ) ; cacheFile . setReadable ( false , false ) ; cacheFile . setReadable ( true , true ) ; code_block = IfStatement ; } catch ( IOException ioe ) { code_block = IfStatement ; } catch ( KrbException e ) { LOG . error ( "Fail to write token!" , e ) ; } }
public void test() { if ( iteration > 1 && timeSpent > 86400 && ! actType . equals ( "home" ) ) { log . info ( "ignoring " + actType + " to " + actType ) ; } }
public void test() { if ( usingTeradataJdbcDriver ( connection ( ) ) ) { String tableNameInDatabase = mutableTablesState ( ) . get ( TABLE_NAME_MUTABLE ) . getNameInDatabase ( ) ; String insertSqlWithTable = String . format ( INSERT_SQL , tableNameInDatabase ) ; String selectSqlWithTable = String . format ( SELECT_STAR_SQL , tableNameInDatabase ) ; defaultQueryExecutor ( ) . executeQuery ( insertSqlWithTable , param ( TINYINT , null ) , param ( SMALLINT , null ) , param ( INTEGER , null ) , param ( BIGINT , null ) , param ( FLOAT , null ) , param ( DOUBLE , null ) , param ( DECIMAL , null ) , param ( DECIMAL , null ) , param ( TIMESTAMP , null ) , param ( DATE , null ) , param ( VARCHAR , null ) , param ( VARCHAR , null ) , param ( CHAR , null ) , param ( BOOLEAN , null ) , param ( VARBINARY , new byte [ ] code_block = "" ; ) ) ; QueryResult result = defaultQueryExecutor ( ) . executeQuery ( selectSqlWithTable ) ; assertColumnTypes ( result ) ; assertThat ( result ) . containsOnly ( row ( null , null , null , null , null , null , null , null , null , null , null , null , null , null , new byte [ ] code_block = "" ; ) ) ; } else { LOGGER . warn ( "attempting to verify existence of JDBC driver" ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( JournalFolderServiceUtil . class , "getDDMStructures" , _getDDMStructuresParameterTypes5 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupIds , folderId , restrictionType , orderByComparator ) ; Object returnObj = null ; code_block = TryStatement ;  return ( java . util . List < com . liferay . dynamic . data . mapping . model . DDMStructure > ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { if ( missEqualsFail ) { log . warn ( "[{}] Not found (LL {},{}; {})" , kvStoreType , hex , lat , lng , x , y ) ; } else { log . warn ( "[{}] For colour {} (LL {},{}; pixel {},{}) the webservice gave zero locations." , kvStoreType , hex , hex , lat , lng , x , y ) ; } }
public void test() { if ( missEqualsFail ) { log . error ( "[{}] For colour {} (LL {},{}; pixel {},{}) the webservice gave zero locations." , kvStoreType , hex , lat , lng , x , y ) ; } else { log . error ( "[{}] For colour {} (LL {},{}; pixel {},{}) the webservice gave zero locations." , kvStoreType , hex , lat , lng , x , y ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( null != kickUser ) { banIp = kickUser . getHostname ( ) ; logger . info ( String . format ( "KICKED message: %s" , kickUser . toString ( ) ) ) ; final Map < ReturnableData , Object > kickData = new HashMap < ReturnableData , Object > ( ) ; kickData . put ( LongPollResponse . EVENT , LongPollEvent . BANNED . toString ( ) ) ; final QueuedMessage qm = new QueuedMessage ( MessageType . KICKED , kickData ) ; kickUser . enqueueMessage ( qm ) ; connectedUsers . removeUser ( kickUser , DisconnectReason . BANNED ) ; } else { banIp = request . getParameter ( AjaxRequest . NICKNAME ) ; logger . info ( String . format ( "Banning %s by request of %s" , banIp , user . getNickname ( ) ) ) ; } }
public void test() { if ( receivingEndpoint == null ) { logger . debug ( "received: {}" , message ) ; return null ; } }
public void test() { if ( sendingEndpoints . get ( sender . getName ( ) ) != null ) { LOG . warn ( "Cannot send message to remote sender: {}" , sender . getName ( ) ) ; return null ; } }
public void test() { if ( oldSendingEndpoint != null ) { LOG . debug ( "Removed old sender for {}" , oldSendingEndpoint ) ; return null ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( handler == null ) { logger . debug ( HANDLER_IS_NULL ) ; return ; } }
public void test() { try { found = isFullTrigger ? definitionsService . getFullTrigger ( tenantId , triggerId ) : definitionsService . getTrigger ( tenantId , triggerId ) ; } catch ( NotFoundException e ) { } catch ( IllegalArgumentException e ) { throw new BadRequestException ( "Bad arguments: " + e . getMessage ( ) ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; throw new InternalServerException ( e . toString ( ) ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( GroupServiceUtil . class , "updateGroup" , _updateGroupParameterTypes38 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , typeSettings ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . portal . kernel . model . Group ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( l != null && l . length > 0 ) { logger . info ( "\tfound '" + l + "' on " + this ) ; } }
public void test() { { log . info ( Color . GREEN + "Calendar_3_8 : empty column trip_id" + Color . NORMAL ) ; Context context = new Context ( ) ; CheckPointReport result = verifyValidation ( log , context , "calendar_3_8" , GTFS_1_GTFS_Common_12 , SEVERITY . ERROR , RESULT . NOK , true ) ; Assert . assertEquals ( result . getCheckPointErrorCount ( ) , 1 , "detail count" ) ; code_block = ForStatement ; } }
public void test() { if ( table . getSelectedRow ( ) < 0 ) { MessageDialog . error ( MedicalBrowser . this , "angal.common.pleaseselectarow.msg" ) ; } else { selectedrow = table . convertRowIndexToModel ( table . getSelectedRow ( ) ) ; Medical med = ( Medical ) ( ( ( MedicalBrowsingModel ) model ) . getValueAt ( selectedrow , - 1 ) ) ; StringBuilder deleteMessage = new StringBuilder ( ) . append ( MessageBundle . getMessage ( "angal.medicals.deletemedical" ) ) . append ( " \"" ) . append ( med . getDescription ( ) ) . append ( "\" ?" ) ; int n = JOptionPane . showConfirmDialog ( MedicalBrowser . this , deleteMessage . toString ( ) , MessageBundle . getMessage ( "angal.hospital" ) , JOptionPane . YES_NO_OPTION ) ; boolean deleted ; code_block = TryStatement ;  code_block = IfStatement ; } }
@ PayloadRoot ( localPart = "ClearAlarmRegisterAsyncRequest" , namespace = SMARTMETER_MONITORING_NAMESPACE ) @ ResponsePayload public ClearAlarmRegisterResponse getClearAlarmRegisterResponse ( @ OrganisationIdentification final String organisationIdentification , @ RequestPayload final ClearAlarmRegisterAsyncRequest request ) throws OsgpException { log . info ( "GetClear alarm register response received from organisation: {}." , organisationIdentification ) ; ClearAlarmRegisterResponse response = null ; code_block = TryStatement ;  return response ; }
public void MethodWithClassDbAnnotated ( ) { LOG . info ( "MethodWithClassDbAnnotated called!" ) ; _dummy . sayHello ( ) ; }
public void test() { try { MimeType mimeType = getMimeType ( contentTypeList ) ; CreateResponse createResponse ; code_block = IfStatement ; String id = createResponse . getCreatedMetacards ( ) . get ( 0 ) . getId ( ) ; LOGGER . debug ( "Create Response id [{}]" , id ) ; LOGGER . debug ( "Entry successfully saved, id: {}" , id ) ; code_block = IfStatement ; LOGGER . debug ( "Entry successfully saved, id: {}" , id ) ; return id ; } catch ( SourceUnavailableException e ) { String exceptionMessage = "Cannot create catalog entry because source is unavailable: " ; LOGGER . info ( exceptionMessage , e ) ; throw new InternalServerErrorException ( exceptionMessage ) ; } catch ( InternalIngestException e ) { String exceptionMessage = "Error while storing entry in catalog: " ; LOGGER . info ( exceptionMessage , e ) ; throw new InternalServerErrorException ( exceptionMessage ) ; } catch ( MetacardCreationException | IngestException e ) { String errorMessage = "Error while storing entry in catalog: " ; LOGGER . info ( errorMessage , e ) ; throw new CatalogServiceException ( errorMessage ) ; } finally { IOUtils . closeQuietly ( message ) ; } }
public void test() { try { LOGGER . debug ( "POST" ) ; MimeType mimeType = getMimeType ( contentTypeList ) ; CreateResponse createResponse ; code_block = IfStatement ; LOGGER . debug ( "POST" ) ; String id = createResponse . getCreatedMetacards ( ) . get ( 0 ) . getId ( ) ; LOGGER . debug ( "Entry successfully saved, id: {}" , id ) ; code_block = IfStatement ; return id ; } catch ( SourceUnavailableException e ) { String exceptionMessage = "Cannot create catalog entry because source is unavailable: " ; LOGGER . info ( exceptionMessage , e ) ; throw new InternalServerErrorException ( exceptionMessage ) ; } catch ( InternalIngestException e ) { String exceptionMessage = "Error while storing entry in catalog: " ; LOGGER . info ( exceptionMessage , e ) ; throw new InternalServerErrorException ( exceptionMessage ) ; } catch ( MetacardCreationException | IngestException e ) { String errorMessage = "Error while storing entry in catalog: " ; LOGGER . info ( errorMessage , e ) ; throw new CatalogServiceException ( errorMessage ) ; } finally { IOUtils . closeQuietly ( message ) ; } }
public void test() { try { LOGGER . debug ( "POST" ) ; MimeType mimeType = getMimeType ( contentTypeList ) ; CreateResponse createResponse ; code_block = IfStatement ; LOGGER . debug ( "POST" ) ; String id = createResponse . getCreatedMetacards ( ) . get ( 0 ) . getId ( ) ; LOGGER . debug ( "Create Response id [{}]" , id ) ; code_block = IfStatement ; return id ; } catch ( SourceUnavailableException e ) { String exceptionMessage = "Cannot create catalog entry because source is unavailable: " ; LOGGER . info ( exceptionMessage , e ) ; throw new InternalServerErrorException ( exceptionMessage ) ; } catch ( InternalIngestException e ) { String exceptionMessage = "Error while storing entry in catalog: " ; LOGGER . info ( exceptionMessage , e ) ; throw new InternalServerErrorException ( exceptionMessage ) ; } catch ( MetacardCreationException | IngestException e ) { String errorMessage = "Error while storing entry in catalog: " ; LOGGER . info ( errorMessage , e ) ; throw new CatalogServiceException ( errorMessage ) ; } finally { IOUtils . closeQuietly ( message ) ; } }
public void test() { if ( INGEST_LOGGER . isInfoEnabled ( ) ) { INGEST_LOGGER . info ( "{} metacards were successfully ingested. {}." , operation . getName ( ) , buildIngestLog ( request ) ) ; } }
public void test() { try { LOGGER . debug ( "POST" ) ; MimeType mimeType = getMimeType ( contentTypeList ) ; CreateResponse createResponse ; code_block = IfStatement ; String id = createResponse . getCreatedMetacards ( ) . get ( 0 ) . getId ( ) ; LOGGER . debug ( "Create Response id [{}]" , id ) ; LOGGER . debug ( "Entry successfully saved, id: {}" , id ) ; code_block = IfStatement ; return id ; } catch ( SourceUnavailableException e ) { String exceptionMessage = "Cannot create catalog entry because source is unavailable: " ; LOGGER . info ( exceptionMessage , e ) ; throw new InternalServerErrorException ( exceptionMessage ) ; } catch ( InternalIngestException e ) { String exceptionMessage = "Error while storing entry in catalog: " ; LOGGER . info ( exceptionMessage , e ) ; throw new InternalServerErrorException ( exceptionMessage ) ; } catch ( MetacardCreationException | IngestException e ) { String errorMessage = "Error while storing entry in catalog: " ; LOGGER . error ( errorMessage , e ) ; throw new CatalogServiceException ( errorMessage ) ; } finally { IOUtils . closeQuietly ( message ) ; } }
public void test() { try { disconnect ( sessionId , "Processing has stopped." ) ; } catch ( IOException e ) { logger . error ( "Disconnecting threw" , e ) ; } }
@ Override public void saveTestSuiteResult ( ) { LOGGER . info ( "save the test suite" ) ; testSuite . refreshState ( ) ; MapSqlParameterSource tcParameters = getCompleteParameters ( ) ; LOGGER . debug ( "write the following values to 'sakuli_suites': " + tcParameters . getValues ( ) + " ==>  now execute ...." ) ; String sqlStatement = "UPDATE sakuli_suites " + createSqlSetStringForNamedParameter ( tcParameters . getValues ( ) ) + " where id=:id" ; LOGGER . debug ( "SQL-Statement for update 'sakuli_suites': " + sqlStatement ) ; int affectedRows = getNamedParameterJdbcTemplate ( ) . update ( sqlStatement , tcParameters ) ; LOGGER . info ( "update 'sakuli_suites' affected " + affectedRows + " rows" ) ; }
@ Override public void saveTestSuiteResult ( ) { LOGGER . debug ( "Saving test suite" ) ; testSuite . refreshState ( ) ; LOGGER . info ( "save the results of the test suite to the table 'sakuli_suites'" ) ; MapSqlParameterSource tcParameters = getCompleteParameters ( ) ; String sqlStatement = "UPDATE sakuli_suites " + createSqlSetStringForNamedParameter ( tcParameters . getValues ( ) ) + " where id=:id" ; LOGGER . debug ( "SQL-Statement for update 'sakuli_suites': " + sqlStatement ) ; int affectedRows = getNamedParameterJdbcTemplate ( ) . update ( sqlStatement , tcParameters ) ; LOGGER . info ( "update 'sakuli_suites' affected " + affectedRows + " rows" ) ; }
@ Override public void saveTestSuiteResult ( ) { testSuite . refreshState ( ) ; LOGGER . info ( "save the results of the test suite to the table 'sakuli_suites'" ) ; MapSqlParameterSource tcParameters = getCompleteParameters ( ) ; LOGGER . debug ( "write the following values to 'sakuli_suites': " + tcParameters . getValues ( ) + " ==>  now execute ...." ) ; String sqlStatement = "UPDATE sakuli_suites " + createSqlSetStringForNamedParameter ( tcParameters . getValues ( ) ) + " where id=:id" ; LOGGER . debug ( sqlStatement ) ; int affectedRows = getNamedParameterJdbcTemplate ( ) . update ( sqlStatement , tcParameters ) ; LOGGER . info ( "update 'sakuli_suites' affected " + affectedRows + " rows" ) ; }
@ Override public void saveTestSuiteResult ( ) { testSuite . refreshState ( ) ; LOGGER . info ( "save the results of the test suite to the table 'sakuli_suites'" ) ; MapSqlParameterSource tcParameters = getCompleteParameters ( ) ; LOGGER . debug ( "write the following values to 'sakuli_suites': " + tcParameters . getValues ( ) + " ==>  now execute ...." ) ; String sqlStatement = "UPDATE sakuli_suites " + createSqlSetStringForNamedParameter ( tcParameters . getValues ( ) ) + " where id=:id" ; LOGGER . debug ( "SQL-Statement for update 'sakuli_suites': " + sqlStatement ) ; int affectedRows = getNamedParameterJdbcTemplate ( ) . update ( sqlStatement , tcParameters ) ; LOGGER . info ( "Updated " + affectedRows + " rows." ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( candidateEventType == null ) { LOG . debug ( "EventType is null" ) ; return null ; } }
public void test() { if ( ! ( candidateEventType instanceof BaseXMLEventType ) ) { LOGGER . debug ( "Invalid event type {}" , candidateEventType ) ; return null ; } }
public void testSoftRefCache ( ) throws Exception { ICache < byte [ ] > c = CacheManager . inst . getCache ( "bytesCache" , byte [ ] . class , CacheType . SOFT_REFERENCE ) ; code_block = ForStatement ; log . debug ( "size: " + c . size ( ) ) ; log . debug ( "size: " + c . size ( ) ) ; code_block = ForStatement ; log . debug ( "size: " + c . size ( ) ) ; log . debug ( CacheManager . inst ) ; }
public void testSoftRefCache ( ) throws Exception { ICache < byte [ ] > c = CacheManager . inst . getCache ( "bytesCache" , byte [ ] . class , CacheType . SOFT_REFERENCE ) ; code_block = ForStatement ; log . debug ( "size: " + c . size ( ) ) ; log . debug ( "size: " + c . size ( ) ) ; code_block = ForStatement ; log . debug ( "size: " + c . size ( ) ) ; log . debug ( c ) ; }
public void test() { if ( correctlyInspected ) { logger . debug ( "Node {} removed from network" , node ) ; network . notifyNodeDiscovered ( node ) ; return ; } else { logger . warn ( "Node {} removed from network because no endpoints have been discovered" , node ) ; network . removeNode ( node ) ; } }
public void test() { if ( isNew ) { logger . trace ( "Inspecting node {}" , node ) ; ZDO_NODE_DESC_REQ nodeDescriptorReq = new ZDO_NODE_DESC_REQ ( nwkAddress . get16BitValue ( ) ) ; final ZDO_NODE_DESC_RSP nodeResult = driver . sendZDONodeDescriptionRequest ( nodeDescriptorReq ) ; code_block = IfStatement ; ZDO_POWER_DESC_REQ powerDescriptorReq = new ZDO_POWER_DESC_REQ ( nwkAddress . get16BitValue ( ) ) ; final ZDO_POWER_DESC_RSP powerResult = driver . sendZDOPowerDescriptionRequest ( powerDescriptorReq ) ; code_block = IfStatement ; correctlyInspected = inspectEndpointOfNode ( nwk , node ) ; code_block = IfStatement ; } else { logger . trace ( "Inspecting existing node {}" , node ) ; code_block = IfStatement ; } }
public void test() { if ( nodeResult == null ) { logger . debug ( "Node {} is null" , nodeId ) ; } else { node . setNodeDescriptor ( new ZigBeeNodeDescriptor ( nodeResult ) ) ; code_block = IfStatement ; code_block = IfStatement ; } }
public void test() { if ( powerResult == null ) { logger . debug ( "Node {} does not exist" , node . getName ( ) ) ; } else { node . setPowerDescriptor ( new ZigBeeNodePowerDescriptor ( powerResult ) ) ; } }
public void test() { if ( isNew ) { logger . debug ( "Inspecting new node {}" , node ) ; ZDO_NODE_DESC_REQ nodeDescriptorReq = new ZDO_NODE_DESC_REQ ( nwkAddress . get16BitValue ( ) ) ; final ZDO_NODE_DESC_RSP nodeResult = driver . sendZDONodeDescriptionRequest ( nodeDescriptorReq ) ; code_block = IfStatement ; ZDO_POWER_DESC_REQ powerDescriptorReq = new ZDO_POWER_DESC_REQ ( nwkAddress . get16BitValue ( ) ) ; final ZDO_POWER_DESC_RSP powerResult = driver . sendZDOPowerDescriptionRequest ( powerDescriptorReq ) ; code_block = IfStatement ; correctlyInspected = inspectEndpointOfNode ( nwk , node ) ; code_block = IfStatement ; } else { logger . debug ( "Inspecting new node {}" , node ) ; code_block = IfStatement ; } }
public void test() { try { KeyValue keyValue = parser . parse ( ) ; Assert . fail ( ) ; } catch ( OracleConnectionStringException e ) { LOG . warn ( e . getMessage ( ) ) ; } }
public static void main ( String [ ] args ) throws Exception { Preconditions . checkArgument ( args . length == 5 , "Illegal number of arguments. Expected: 5, Actual: " + args . length ) ; String stormId = args [ 0 ] ; String assignmentId = args [ 1 ] ; String supervisorPort = args [ 2 ] ; String portStr = args [ 3 ] ; String workerId = args [ 4 ] ; Map < String , Object > conf = ConfigUtils . readStormConfig ( ) ; Utils . setupWorkerUncaughtExceptionHandler ( ) ; StormCommon . validateDistributedMode ( conf ) ; LOG . info ( "Distributed Mode is starting." ) ; int supervisorPortInt = Integer . parseInt ( supervisorPort ) ; Worker worker = new Worker ( conf , null , stormId , assignmentId , supervisorPortInt , Integer . parseInt ( portStr ) , workerId ) ; int workerShutdownSleepSecs = ObjectReader . getInt ( conf . get ( Config . SUPERVISOR_WORKER_SHUTDOWN_SLEEP_SECS ) ) ; Utils . addShutdownHookWithDelayedForceKill ( worker :: shutdown , workerShutdownSleepSecs ) ; worker . start ( ) ; }
@ Override public boolean waitForChannelToReachAgi ( long timeout , TimeUnit timeunit ) throws InterruptedException { logger . info ( "Wait for channel to reach agi " + this + " " + timeout ) ; boolean tmp = hasReachedAgi . await ( timeout , timeunit ) ; logger . info ( "Result of waiting for channel to reach agi " + this + " " + tmp ) ; return tmp ; }
@ Override public boolean waitForChannelToReachAgi ( long timeout , TimeUnit timeunit ) throws InterruptedException { logger . info ( "Waiting for channel to reach agi " + this ) ; boolean tmp = hasReachedAgi . await ( timeout , timeunit ) ; logger . info ( "Channel to reach agi " + this ) ; return tmp ; }
public void test() { try { service = applicationContext . getBean ( defName ) ; } catch ( Throwable t ) { logger . error ( "error in hasReferencingObjects" , t ) ; service = null ; } }
public void test() { try { String [ ] defNames = applicationContext . getBeanNamesForType ( PageUtilizer . class ) ; code_block = ForStatement ; } catch ( ApsSystemException ex ) { logger . error ( "error loading references for page {}" , page . getCode ( ) , ex ) ; throw new RestServerError ( "error in getReferencingObjects " , ex ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public static synchronized Cipher getCipher ( String transformation ) throws NoSuchAlgorithmException , NoSuchPaddingException , NoSuchProviderException { Cipher result = Cipher . getInstance ( transformation ) ; LOGGER . debug ( "Cipher " + transformation ) ; return result ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { -> { logger . warn ( "Got unexpected exception" , e ) ; testThread . interrupt ( ) ; } }
public void test() { try { CountDownLatch latch = new CountDownLatch ( iterations ) ; long factor = ( logger . isDebugEnabled ( ) ? 25 : 1 ) * 100 ; Thread testThread = Thread . currentThread ( ) ; Scheduler . Task task = client . getScheduler ( ) . schedule ( ( ) code_block = LoopStatement ; , iterations * factor , TimeUnit . MILLISECONDS ) ; long successes = 0 ; long begin = System . nanoTime ( ) ; code_block = ForStatement ; assertTrue ( latch . await ( iterations , TimeUnit . SECONDS ) ) ; long end = System . nanoTime ( ) ; assertThat ( successes , Matchers . greaterThan ( 0L ) ) ; task . cancel ( ) ; long elapsed = TimeUnit . NANOSECONDS . toMillis ( end - begin ) ; logger . info ( "elapsed={}ms" , elapsed ) ; return true ; } catch ( Exception x ) { x . printStackTrace ( ) ; return false ; } }
public void test() { try { this . indexedFields = this . helper . getIndexedFields ( config . getDatatypeFilter ( ) ) ; } catch ( Exception ex ) { LOGGER . error ( "got exception when using MetadataHelper to get indexed fields " , ex ) ; throw new RuntimeException ( "got exception when using MetadataHelper to get indexed fields " , ex ) ; } }
public static void main ( String [ ] args ) throws Exception { Logging . initialize ( ) ; DistributedQueryRunner queryRunner = createRedisQueryRunner ( new RedisServer ( ) , ImmutableMap . of ( "http-server.http.port" , "8080" ) , "string" , TpchTable . getTables ( ) ) ; Logger log = Logger . get ( RedisQueryRunner . class ) ; log . info ( "======== SERVER STARTED ========" ) ; log . info ( "\n====\n%s\n====" , queryRunner . getCoordinator ( ) . getBaseUrl ( ) ) ; }
public static void main ( String [ ] args ) throws Exception { Logging . initialize ( ) ; DistributedQueryRunner queryRunner = createRedisQueryRunner ( new RedisServer ( ) , ImmutableMap . of ( "http-server.http.port" , "8080" ) , "string" , TpchTable . getTables ( ) ) ; Logger log = Logger . get ( RedisQueryRunner . class ) ; log . info ( "======== SERVER STARTED ========" ) ; log . info ( "\n====\n%s\n====" , queryRunner . getCoordinator ( ) . getBaseUrl ( ) ) ; }
@ Override public void clearCompositePhenomenonsForObservableProperty ( String observableProperty ) { LOG . trace ( "Clearing composite phenotypes for observableProperty {}" , observableProperty ) ; this . compositePhenomenonsForObservableProperty . remove ( observableProperty ) ; }
public void test() { try { com . liferay . portal . kernel . model . Group returnValue = GroupServiceUtil . getUserGroup ( companyId , userId ) ; return com . liferay . portal . kernel . model . GroupSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { -> { log . debug ( "Removing module {}" , module ) ; view . removeView ( openModuleViews . get ( module ) ) ; openModuleViews . remove ( module ) ; } }
public void attachDirty ( StgMbGefaehrskatTxt instance ) { log . debug ( "attaching dirty StgMbGefaehrskatTxt instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { ConfigInfo oldConfigInfo = findConfigInfo ( configInfo . getDataId ( ) , configInfo . getGroup ( ) , configInfo . getTenant ( ) ) ; String appNameTmp = oldConfigInfo . getAppName ( ) ; code_block = IfStatement ; int rows = updateConfigInfoAtomicCas ( configInfo , srcIp , srcUser , time , configAdvanceInfo ) ; code_block = IfStatement ; String configTags = configAdvanceInfo == null ? null : ( String ) configAdvanceInfo . get ( "config_tags" ) ; code_block = IfStatement ; insertConfigHistoryAtomic ( oldConfigInfo . getId ( ) , oldConfigInfo , srcIp , srcUser , time , "U" ) ; } catch ( CannotGetJdbcConnectionException e ) { LogUtil . FATAL_LOG . error ( "[db-error] " + e . toString ( ) , e ) ; throw e ; } }
public void test() { try ( FileOutputStream stream = new FileOutputStream ( taskFile ) ) { stream . write ( errorMessage . getBytes ( ) ) ; code_block = IfStatement ; stream . close ( ) ; } catch ( IOException ioe ) { log . warn ( "Failed to write task file: " + ioe . getMessage ( ) , ioe ) ; } }
public void test() { if ( ! overwrite && getBackendConnector ( ) . isBugExisting ( vulnId ) ) { log . info ( "{} is overwritten." , vulnId ) ; return ; } }
public void test() { for ( ConstructChange chg : changes ) { LOG . debug ( chg ) ; } }
private void shutdownDroplet ( Exchange exchange ) throws Exception { Action action = getEndpoint ( ) . getDigitalOceanClient ( ) . shutdownDroplet ( dropletId ) ; LOG . trace ( "Shutdown {}" , action ) ; exchange . getMessage ( ) . setBody ( action ) ; }
private void buildCluster ( ) { EmbeddedConnectCluster . Builder builder = new EmbeddedConnectCluster . Builder ( ) ; Properties brokerProps = new Properties ( ) ; brokerProps . put ( "auto.create.topics.enable" , String . valueOf ( true ) ) ; Map < String , String > workerProps = new HashMap < > ( ) ; workerProps . put ( WorkerConfig . OFFSET_COMMIT_INTERVAL_MS_CONFIG , String . valueOf ( OFFSET_COMMIT_INTERVAL_MS ) ) ; LOG . info ( "Using the following address: {}" , address ) ; String address = "http://localhost:" + NetworkUtils . getFreePort ( ) ; LOG . info ( "Using the following address for  the listener configuration: {}" , address ) ; workerProps . put ( WorkerConfig . LISTENERS_CONFIG , address ) ; String pluginPaths = PluginPathHelper . getInstance ( ) . pluginPaths ( ) ; LOG . info ( "Adding the returned directories to the plugin path. This may take A Very long time to complete" ) ; workerProps . put ( WorkerConfig . PLUGIN_PATH_CONFIG , pluginPaths ) ; LOG . info ( "Building the embedded Kafka connect instance" ) ; this . cluster = builder . name ( "connect-cluster" ) . numWorkers ( 1 ) . numBrokers ( 1 ) . brokerProps ( brokerProps ) . workerProps ( workerProps ) . maskExitProcedures ( true ) . build ( ) ; LOG . info ( "Built the embedded Kafka connect instance" ) ; }
private void buildCluster ( ) { LOG . info ( "Creating the embedded Kafka connect instance" ) ; EmbeddedConnectCluster . Builder builder = new EmbeddedConnectCluster . Builder ( ) ; Properties brokerProps = new Properties ( ) ; brokerProps . put ( "auto.create.topics.enable" , String . valueOf ( true ) ) ; Map < String , String > workerProps = new HashMap < > ( ) ; workerProps . put ( WorkerConfig . OFFSET_COMMIT_INTERVAL_MS_CONFIG , String . valueOf ( OFFSET_COMMIT_INTERVAL_MS ) ) ; String address = "http://localhost:" + NetworkUtils . getFreePort ( ) ; workerProps . put ( WorkerConfig . LISTENERS_CONFIG , address ) ; String pluginPaths = PluginPathHelper . getInstance ( ) . pluginPaths ( ) ; LOG . info ( "Adding the returned directories to the plugin path. This may take Aiking long time to complete" ) ; workerProps . put ( WorkerConfig . PLUGIN_PATH_CONFIG , pluginPaths ) ; LOG . info ( "Building the embedded Kafka connect instance" ) ; this . cluster = builder . name ( "connect-cluster" ) . numWorkers ( 1 ) . numBrokers ( 1 ) . brokerProps ( brokerProps ) . workerProps ( workerProps ) . maskExitProcedures ( true ) . build ( ) ; LOG . info ( "Built the embedded Kafka connect instance" ) ; }
private void buildCluster ( ) { LOG . info ( "Creating the embedded Kafka connect instance" ) ; EmbeddedConnectCluster . Builder builder = new EmbeddedConnectCluster . Builder ( ) ; Properties brokerProps = new Properties ( ) ; brokerProps . put ( "auto.create.topics.enable" , String . valueOf ( true ) ) ; Map < String , String > workerProps = new HashMap < > ( ) ; workerProps . put ( WorkerConfig . OFFSET_COMMIT_INTERVAL_MS_CONFIG , String . valueOf ( OFFSET_COMMIT_INTERVAL_MS ) ) ; String address = "http://localhost:" + NetworkUtils . getFreePort ( ) ; LOG . info ( "Using the following address for  the listener configuration: {}" , address ) ; workerProps . put ( WorkerConfig . LISTENERS_CONFIG , address ) ; String pluginPaths = PluginPathHelper . getInstance ( ) . pluginPaths ( ) ; workerProps . put ( WorkerConfig . PLUGIN_PATH_CONFIG , pluginPaths ) ; LOG . info ( "Loading the embedded Kafka connect instance" ) ; this . cluster = builder . name ( "connect-cluster" ) . numWorkers ( 1 ) . numBrokers ( 1 ) . brokerProps ( brokerProps ) . workerProps ( workerProps ) . maskExitProcedures ( true ) . build ( ) ; LOG . info ( "Built the embedded Kafka connect instance" ) ; }
private void buildCluster ( ) { LOG . info ( "Creating the embedded Kafka connect instance" ) ; EmbeddedConnectCluster . Builder builder = new EmbeddedConnectCluster . Builder ( ) ; Properties brokerProps = new Properties ( ) ; brokerProps . put ( "auto.create.topics.enable" , String . valueOf ( true ) ) ; Map < String , String > workerProps = new HashMap < > ( ) ; workerProps . put ( WorkerConfig . OFFSET_COMMIT_INTERVAL_MS_CONFIG , String . valueOf ( OFFSET_COMMIT_INTERVAL_MS ) ) ; String address = "http://localhost:" + NetworkUtils . getFreePort ( ) ; LOG . info ( "Using the following address for  the listener configuration: {}" , address ) ; workerProps . put ( WorkerConfig . LISTENERS_CONFIG , address ) ; String pluginPaths = PluginPathHelper . getInstance ( ) . pluginPaths ( ) ; LOG . info ( "Adding the returned directories to the plugin path. This may take A performance long time to complete" ) ; workerProps . put ( WorkerConfig . PLUGIN_PATH_CONFIG , pluginPaths ) ; LOG . info ( "Added the returned directory to the broker" ) ; this . cluster = builder . name ( "connect-cluster" ) . numWorkers ( 1 ) . numBrokers ( 1 ) . brokerProps ( brokerProps ) . workerProps ( workerProps ) . maskExitProcedures ( true ) . build ( ) ; LOG . info ( "Built the embedded Kafka connect instance" ) ; }
private void buildCluster ( ) { LOG . info ( "Creating the embedded Kafka connect instance" ) ; EmbeddedConnectCluster . Builder builder = new EmbeddedConnectCluster . Builder ( ) ; Properties brokerProps = new Properties ( ) ; brokerProps . put ( "auto.create.topics.enable" , String . valueOf ( true ) ) ; Map < String , String > workerProps = new HashMap < > ( ) ; workerProps . put ( WorkerConfig . OFFSET_COMMIT_INTERVAL_MS_CONFIG , String . valueOf ( OFFSET_COMMIT_INTERVAL_MS ) ) ; String address = "http://localhost:" + NetworkUtils . getFreePort ( ) ; LOG . info ( "Using the following address for  the listener configuration: {}" , address ) ; workerProps . put ( WorkerConfig . LISTENERS_CONFIG , address ) ; String pluginPaths = PluginPathHelper . getInstance ( ) . pluginPaths ( ) ; LOG . info ( "Plugin paths: {}" , pluginPaths ) ; LOG . info ( "Adding the returned directories to the plugin path. This may take Aiking long time to complete" ) ; workerProps . put ( WorkerConfig . PLUGIN_PATH_CONFIG , pluginPaths ) ; LOG . info ( "Building the embedded Kafka connect instance" ) ; this . cluster = builder . name ( "connect-cluster" ) . numWorkers ( 1 ) . numBrokers ( 1 ) . brokerProps ( brokerProps ) . workerProps ( workerProps ) . maskExitProcedures ( true ) . build ( ) ; }
private void prepareGetResourceConfigurations ( ) throws Exception { final String complexResourceJSONString = DMPPersistenceUtil . getResourceAsString ( "complex_resource.json" ) ; final Resource expectedComplexResource = objectMapper . readValue ( complexResourceJSONString , Resource . class ) ; Assert . assertNotNull ( "the complex resource shouldn't be null" , expectedComplexResource ) ; Assert . assertNotNull ( "the name of the complex resource shouldn't be null" , expectedComplexResource . getName ( ) ) ; Assert . assertNotNull ( "the description of the complex resource shouldn't be null" , expectedComplexResource . getDescription ( ) ) ; Assert . assertNotNull ( "the type of the complex resource shouldn't be null" , expectedComplexResource . getType ( ) ) ; Assert . assertNotNull ( "the attributes of the complex resource shouldn't be null" , expectedComplexResource . getAttributes ( ) ) ; Assert . assertNotNull ( "the configurations of the complex resource shouldn't be null" , expectedComplexResource . getConfigurations ( ) ) ; Assert . assertFalse ( "the configurations of the complex resource shouldn't be empty" , expectedComplexResource . getConfigurations ( ) . isEmpty ( ) ) ; final ResourceServiceTestUtils resourceServiceTestUtils = resourcesResourceTestUtils . getPersistenceServiceTestUtils ( ) ; Resource complexResource = resourceServiceTestUtils . getJpaService ( ) . createObjectTransactional ( ) . getObject ( ) ; final Set < Configuration > createdConfigurations = Sets . newLinkedHashSet ( ) ; final ConfigurationServiceTestUtils configurationServiceTestUtils = resourceServiceTestUtils . getConfigurationsServiceTestUtils ( ) ; code_block = ForStatement ; complexResource . setName ( expectedComplexResource . getName ( ) ) ; complexResource . setDescription ( expectedComplexResource . getDescription ( ) ) ; complexResource . setType ( expectedComplexResource . getType ( ) ) ; complexResource . setAttributes ( expectedComplexResource . getAttributes ( ) ) ; final Resource updatedComplexResource = resourceServiceTestUtils . updateAndCompareObject ( complexResource , complexResource ) ; Assert . assertNotNull ( "updated resource shouldn't be null" , updatedComplexResource ) ; Assert . assert
@ Test public void happyPath ( ) throws IOException { logStart ( ) ; String updateAAIGenericVnfRequest = FileUtil . readResourceFile ( "__files/VfModularity/UpdateAAIGenericVnfRequest.xml" ) ; MockGetGenericVnfByIdWithDepth ( wireMockServer , "skask" , 1 , "VfModularity/GenericVnf.xml" ) ; MockPutGenericVnf ( wireMockServer , "/skask" , 200 ) ; MockPatchGenericVnf ( wireMockServer , "skask" ) ; String businessKey = UUID . randomUUID ( ) . toString ( ) ; Map < String , Object > variables = new HashMap < > ( ) ; variables . put ( "mso-request-id" , UUID . randomUUID ( ) . toString ( ) ) ; variables . put ( "isDebugLogEnabled" , "true" ) ; variables . put ( "UpdateAAIGenericVnfRequest" , updateAAIGenericVnfRequest ) ; invokeSubProcess ( "UpdateAAIGenericVnf" , businessKey , variables ) ; Assert . assertTrue ( isProcessEnded ( businessKey ) ) ; String response = ( String ) getVariableFromHistory ( businessKey , "UAAIGenVnf_updateGenericVnfResponse" ) ; Integer responseCode = ( Integer ) getVariableFromHistory ( businessKey , "UAAIGenVnf_updateGenericVnfResponseCode" ) ; logger . debug ( "Subflow response: {}" , response ) ; logger . debug ( "Subflow response: {}" , response ) ; Assert . assertEquals ( 200 , responseCode . intValue ( ) ) ; logEnd ( ) ; }
@ Test public void happyPath ( ) throws IOException { logStart ( ) ; String updateAAIGenericVnfRequest = FileUtil . readResourceFile ( "__files/VfModularity/UpdateAAIGenericVnfRequest.xml" ) ; MockGetGenericVnfByIdWithDepth ( wireMockServer , "skask" , 1 , "VfModularity/GenericVnf.xml" ) ; MockPutGenericVnf ( wireMockServer , "/skask" , 200 ) ; MockPatchGenericVnf ( wireMockServer , "skask" ) ; String businessKey = UUID . randomUUID ( ) . toString ( ) ; Map < String , Object > variables = new HashMap < > ( ) ; variables . put ( "mso-request-id" , UUID . randomUUID ( ) . toString ( ) ) ; variables . put ( "isDebugLogEnabled" , "true" ) ; variables . put ( "UpdateAAIGenericVnfRequest" , updateAAIGenericVnfRequest ) ; invokeSubProcess ( "UpdateAAIGenericVnf" , businessKey , variables ) ; Assert . assertTrue ( isProcessEnded ( businessKey ) ) ; String response = ( String ) getVariableFromHistory ( businessKey , "UAAIGenVnf_updateGenericVnfResponse" ) ; Integer responseCode = ( Integer ) getVariableFromHistory ( businessKey , "UAAIGenVnf_updateGenericVnfResponseCode" ) ; logger . debug ( "Subflow response: {}" , response ) ; logger . debug ( "Subflow response: {}" , response ) ; Assert . assertEquals ( 200 , responseCode . intValue ( ) ) ; logEnd ( ) ; }
public void test() { try { return new BufferedInputStream ( fs . open ( new Path ( filePath ) ) ) ; } catch ( IOException e ) { log . error ( "Unable to open file " + filePath , e ) ; return null ; } }
synchronized void reset ( ) throws Exception { instanceIndex . incrementAndGet ( ) ; isConnected . set ( false ) ; connectionStartMs = System . currentTimeMillis ( ) ; handleHolder . closeAndReset ( ) ; handleHolder . getZooKeeper ( ) ; log . info ( "Reset client connection started." ) ; }
public void test() { try { MDC . clear ( ) ; loop ( ) ; } catch ( InterruptedException e ) { log . debug ( "MDC has been interrupted" ) ; Thread . currentThread ( ) . interrupt ( ) ; return ; } catch ( Throwable e ) { log . error ( "Error" , e ) ; throw e ; } }
public void test() { try { MDC . clear ( ) ; loop ( ) ; } catch ( InterruptedException e ) { log . info ( "Interrupted. Stopping loop." ) ; Thread . currentThread ( ) . interrupt ( ) ; return ; } catch ( Throwable e ) { log . error ( "Error while executing Loop" , e ) ; throw e ; } }
@ Bean public IamExternalRestClientFactory iamRestClientFactory ( ) { LOGGER . info ( "Initializing IamServerRestClientFactory bean." ) ; return new IamExternalRestClientFactory ( iamClientProperties , restTemplateBuilder ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "to be created, conservation common" ) ; logger . debug ( objectAsXmlString ( conservationCommon , ConservationCommon . class ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { case tests the Group Management APIs" ) public void GroupManagementTest ( ) throws AutomationFrameworkException { URL url = Thread . currentThread ( ) . getContextClassLoader ( ) . getResource ( "jmeter-scripts" + File . separator + "GroupManagementAPI.jmx" ) ; JMeterTest script = new JMeterTest ( new File ( url . getPath ( ) ) ) ; JMeterTestManager manager = new JMeterTestManager ( ) ; log . info ( "Running group management api test cases using jmeter scripts" ) ; manager . runTest ( script ) ; } }
public void test() { if ( isValidLeader ( expectedLeaderId ) ) { code_block = IfStatement ; } else { LOG . info ( "{} has no leader" , name ) ; } }
public void test() { try { disruptorExec . awaitTermination ( 3 , SECONDS ) ; LOG . info ( "\tReply Processor Disruptor executor shutdown" ) ; } catch ( InterruptedException e ) { LOG . error ( "\tReply Processor Disruptor executor executor shutdown interrupted" , e ) ; Thread . currentThread ( ) . interrupt ( ) ; } }
public void attachDirty ( DtsPackageTxt instance ) { log . debug ( "attaching dirty DtsPackageTxt instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getCapabilities()" ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getCapabilities()" ) ; } }
public void test() { try { MigrationReportInstance reportInstance = processAdminServiceBase . migrateProcessInstance ( containerId , processInstanceId , targetContainerId , targetProcessId , payload , type ) ; return createCorrectVariant ( reportInstance , headers , Response . Status . CREATED , conversationIdHeader ) ; } catch ( ProcessInstanceNotFoundException e ) { return notFound ( MessageFormat . format ( PROCESS_INSTANCE_NOT_FOUND , processInstanceId ) , v , conversationIdHeader ) ; } catch ( DeploymentNotFoundException e ) { return notFound ( MessageFormat . format ( CONTAINER_NOT_FOUND , containerId ) , v , conversationIdHeader ) ; } catch ( Exception e ) { logger . error ( "Unexpected error during processing {}" , e . getMessage ( ) , e ) ; return internalServerError ( errorMessage ( e ) , v , conversationIdHeader ) ; } }
public void test() { try { InetSocketAddress address = NetUtils . createSocketAddr ( queryMasterHostAndPort ) ; ExecutionBlockContextRequest . Builder request = ExecutionBlockContextRequest . newBuilder ( ) ; request . setExecutionBlockId ( executionBlockId . getProto ( ) ) . setWorker ( getWorkerContext ( ) . getConnectionInfo ( ) . getProto ( ) ) ; client = RpcClientManager . getInstance ( ) . newClient ( address , QueryMasterProtocol . class , true , rpcParams ) ; QueryMasterProtocol . QueryMasterProtocolService . Interface stub = client . getStub ( ) ; CallFuture < ExecutionBlockContextResponse > callback = new CallFuture < > ( ) ; stub . getExecutionBlockContext ( callback . getController ( ) , request . build ( ) , callback ) ; ExecutionBlockContextResponse contextProto = callback . get ( ) ; ExecutionBlockContext context = new ExecutionBlockContext ( getWorkerContext ( ) , contextProto , client , pullServerService ) ; context . init ( ) ; return context ; } catch ( Throwable e ) { LOG . error ( "Unexpected error." , e ) ; RpcClientManager . cleanup ( client ) ; throw new RuntimeException ( e ) ; } }
public void test() { try { inProgressKBCommentsCount = _kbSuggestionListDisplayContext . getInProgressKBCommentsCount ( ) ; } catch ( PortalException portalException ) { _log . error ( portalException , portalException ) ; } }
@ Test public void testQuery ( ) throws Exception { CreationTools . createJobDef ( null , true , "pyl.KillMe" , null , "jqm-tests/jqm-test-pyl/target/test.jar" , TestHelpers . qNormal , 42 , "jqm-test-kill" , null , "Franquin" , "ModuleMachin" , "other" , "other" , false , cnx ) ; cnx . commit ( ) ; JqmClientFactory . getClient ( ) . enqueue ( "jqm-test-kill" , "test" ) ; JqmClientFactory . getClient ( ) . enqueue ( "jqm-test-kill" , "test" ) ; JqmClientFactory . getClient ( ) . enqueue ( "jqm-test-kill" , "test" ) ; JqmClientFactory . getClient ( ) . enqueue ( "jqm-test-kill" , "test" ) ; JqmClientFactory . getClient ( ) . enqueue ( "jqm-test-kill" , "test" ) ; jqmlogger . debug ( "COUNT ALL     " + cnx . runSelectSingle ( "ji_select_count_all" , Integer . class ) ) ; jqmlogger . debug ( "COUNT ALL     " + cnx . runSelectSingle ( "ji_select_count_ALL" , Integer . class ) ) ; Assert . assertEquals ( 0 , Query . create ( ) . setQueryLiveInstances ( true ) . setQueryHistoryInstances ( false ) . addStatusFilter ( com . enioka . jqm . api . State . ENDED ) . run ( ) . size ( ) ) ; Assert . assertEquals ( 5 , Query . create ( ) . setQueryLiveInstances ( true ) . setQueryHistoryInstances ( false ) . addStatusFilter ( com . enioka . jqm . api . State . SUBMITTED ) . run ( ) . size ( ) ) ; }
@ Test public void testQuery ( ) throws Exception { CreationTools . createJobDef ( null , true , "pyl.KillMe" , null , "jqm-tests/jqm-test-pyl/target/test.jar" , TestHelpers . qNormal , 42 , "jqm-test-kill" , null , "Franquin" , "ModuleMachin" , "other" , "other" , false , cnx ) ; cnx . commit ( ) ; JqmClientFactory . getClient ( ) . enqueue ( "jqm-test-kill" , "test" ) ; JqmClientFactory . getClient ( ) . enqueue ( "jqm-test-kill" , "test" ) ; JqmClientFactory . getClient ( ) . enqueue ( "jqm-test-kill" , "test" ) ; JqmClientFactory . getClient ( ) . enqueue ( "jqm-test-kill" , "test" ) ; JqmClientFactory . getClient ( ) . enqueue ( "jqm-test-kill" , "test" ) ; jqmlogger . debug ( "COUNT RUNNING " + cnx . runSelectSingle ( "ji_select_count_running" , Integer . class ) ) ; Assert . assertEquals ( 0 , Query . create ( ) . setQueryLiveInstances ( true ) . setQueryHistoryInstances ( false ) . addStatusFilter ( com . enioka . jqm . api . State . RUNNING ) . addStatusFilter ( com . enioka . jqm . api . State . ENDED ) . run ( ) . size ( ) ) ; Assert . assertEquals ( 5 , Query . create ( ) . setQueryLiveInstances ( true ) . setQueryHistoryInstances ( false ) . addStatusFilter ( com . enioka . jqm . api . State . SUBMITTED ) . run ( ) . size ( ) ) ; jqmlogger . debug ( "COUNT END " + cnx . runSelectSingle ( "ji_select_count_ SUBMITTED" , Integer . class ) ) ; }
private static void pre ( ) { PropertyUtils . init ( ) ; LOGGER . info ( "Properties: {}" , PropertyUtils . name ( ) ) ; }
public void test() { try { writeFileHandle . close ( ) ; } catch ( IOException e ) { log . warn ( "Exception closing " + writeFileHandle , e ) ; } }
public void test() { try { User user = SecurityPasswordCreationContentPanel . this . getModelObject ( ) ; securityManagementService . updatePassword ( user , passwordModel . getObject ( ) ) ; Session . get ( ) . success ( getString ( "security.password.creation.validate.success" ) ) ; throw LoginSuccessPage . linkDescriptor ( ) . newRestartResponseException ( ) ; } catch ( RestartResponseException e ) { throw e ; } catch ( Exception e ) { LOGGER . error ( "Error occurred while updating password" , e ) ; Session . get ( ) . error ( getString ( "common.error.unexpected" ) ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
@ OnMessage public void echo ( Session session , String message ) throws Exception { LOGGER . debug ( "echo {}" , message ) ; session . getBasicRemote ( ) . sendObject ( message ) ; verifyRunningThread ( session , LOGGER ) ; }
private static void logIndentOptions ( @ Nonnull PsiFile file , @ Nonnull FileIndentOptionsProvider provider , @ Nonnull IndentOptions options ) { log . debug ( file . getAbsolutePath ( ) + ": " + file . getAbsolutePath ( ) ) ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { if ( codeSource == null ) { LOG . info ( "No codeSource named {}" , name ) ; return ; } }
public void test() { if ( ! Files . exists ( serviceDescriptorFilePath ) ) { LOGGER . error ( "Unable to find service descriptor, please check the configuration." ) ; return ; } }
public void test() { try { com . liferay . segments . model . SegmentsEntry returnValue = SegmentsEntryServiceUtil . getSegmentsEntry ( segmentsEntryId ) ; return com . liferay . segments . model . SegmentsEntrySoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
@ Override public Model getConciseBoundedDescription ( String resource , int depth , boolean withTypesForLeafs ) { logger . debug ( "getConciseBoundedDescription: {}" , resource ) ; Model cbd = ModelFactory . createDefaultModel ( ) ; cbd . add ( getIncomingModel ( resource , depth ) ) ; cbd . add ( getOutgoingModel ( resource , depth ) ) ; logger . debug ( "CBD size: {}" , cbd . size ( ) ) ; return cbd ; }
@ Override public Model getConciseBoundedDescription ( String resource , int depth , boolean withTypesForLeafs ) { logger . debug ( "computing CBD of depth {} for {} ..." , resource , depth ) ; Model cbd = ModelFactory . createDefaultModel ( ) ; cbd . add ( getIncomingModel ( resource , depth ) ) ; cbd . add ( getOutgoingModel ( resource , depth ) ) ; logger . debug ( "corresponding Model: {}" , cbd ) ; return cbd ; }
public void test() { if ( s_logger . isTraceEnabled ( ) ) { s_logger . trace ( "Closing " + _ex . toString ( ) ) ; } }
public void test() { if ( s_logger . isDebugEnabled ( ) ) { s_logger . debug ( log ( seq , "Unable to move " + req . toString ( ) ) ) ; } }
public void test() { try { code_block = IfStatement ; clearLockTimes ( ) ; closeConnection ( ) ; } catch ( final SQLException e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { { final List < AddressDO > list = getList ( ) ; final byte [ ] xls = addressCampaignValueExport . export ( list , personalAddressMap , addressCampaignValueMap , form . getSearchFilter ( ) . getAddressCampaign ( ) != null ? form . getSearchFilter ( ) . getAddressCampaign ( ) . getTitle ( ) : "" ) ; code_block = IfStatement ; final String filename = "ProjectForge-AddressCampaignValueExport_" + DateHelper . getDateAsFilenameSuffix ( new Date ( ) ) + ".xlsx" ; Logger . info ( this , "Export [%s]" , filename ) ; DownloadUtils . setDownloadTarget ( xls , filename ) ; } }
public void test() { try { @ SuppressWarnings ( "unchecked" ) List < SyndEntry > entries = ( List < SyndEntry > ) feed . getEntries ( ) ; code_block = ForStatement ; } catch ( IllegalArgumentException e ) { log . debug ( "Ignoring invalid feed: {}" , e . getMessage ( ) ) ; } }
@ Override public Integer getSerial ( ) { logger . debug ( "Getting serial: " , super . getSerial ( ) ) ; return super . getSerial ( ) ; }
private List < BigQuerySplit > readFromBigQuery ( TableId tableId , Optional < List < ColumnHandle > > projectedColumns , int actualParallelism , Optional < String > filter ) { List < ColumnHandle > columns = projectedColumns . orElse ( ImmutableList . of ( ) ) ; List < String > projectedColumnsNames = columns . stream ( ) . map ( column -> ( ( BigQueryColumnHandle ) column ) . getName ( ) ) . collect ( toImmutableList ( ) ) ; ReadSession readSession = new ReadSessionCreator ( readSessionCreatorConfig , bigQueryClient , bigQueryStorageClientFactory ) . create ( tableId , projectedColumnsNames , filter , actualParallelism ) ; LOG . debug ( "Read session for tableId: {}" , tableId ) ; return readSession . getStreamsList ( ) . stream ( ) . map ( stream -> BigQuerySplit . forStream ( stream . getName ( ) , readSession . getAvroSchema ( ) . getSchema ( ) , columns ) ) . collect ( toImmutableList ( ) ) ; }
public void test() { try { removeListener ( listener ) ; } catch ( Exception e ) { log . warn ( "Unable to remove listener " + listener , e ) ; } }
public void test() { try { from ( "direct:start" ) . setHeader ( "QUERY" , constant ( "arl*" ) ) . process ( new LuceneQueryProcessor ( "target/simpleindexDir" , analyzer , null , 20 , 20 ) ) . to ( "direct:next" ) ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; } }
public void test() { for ( int i = 0 ; i < hits . getNumberOfHits ( ) ; i ++ ) { LOG . debug ( "Hit " + i + " Score:" + hits . getHit ( ) . get ( i ) . getScore ( ) ) ; LOG . debug ( "Hit " + i + " Data:" + hits . getHit ( ) . get ( i ) . getData ( ) ) ; LOG . debug ( "" ) ; } }
public void test() { for ( int i = 0 ; i < hits . getNumberOfHits ( ) ; i ++ ) { LOG . debug ( "Hit " + i + " Index Location:" + hits . getHit ( ) . get ( i ) . getHitLocation ( ) ) ; LOG . debug ( "Hit " + i + " Data:" + hits . getHit ( ) . get ( i ) . getData ( ) ) ; LOG . debug ( "Hit " + i + " Data:" + hits . getHit ( ) . get ( i ) . getData ( ) ) ; } }
public void test() { for ( int i = 0 ; i < hits . getNumberOfHits ( ) ; i ++ ) { LOG . debug ( "Hit " + i + " Index Location:" + hits . getHit ( ) . get ( i ) . getHitLocation ( ) ) ; LOG . debug ( "Hit " + i + " Score:" + hits . getHit ( ) . get ( i ) . getScore ( ) ) ; LOG . debug ( "Hit " + i + " score:" + hits . getHit ( ) . get ( i ) . getScore ( ) ) ; } }
public void test() { try { logFile . close ( ) ; } catch ( IOException e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { bundle . uninstall ( ) ; } catch ( BundleException bundleException ) { _log . error ( bundle , bundleException ) ; } }
public void test() { if ( bugCheck != 1 ) { logger . log ( Level . WARNING , "Detected new bug [" + bugName + "]: " + bugCheck ) ; } }
public void test() { if ( userAgentMatch ( userAgent ) ) { LOGGER . debug ( "the user-agent matches successful filter." ) ; getAuthenticationEntryPoint ( ) . commence ( request , response , _exception ) ; } else { LOGGER . debug ( "the user-agent does not match, skipping filter." ) ; chain . doFilter ( request , response ) ; } }
public void test() { if ( userAgentMatch ( userAgent ) ) { LOGGER . debug ( "the user-agent matched and no Authorization header was sent, returning a 401." ) ; getAuthenticationEntryPoint ( ) . commence ( request , response , _exception ) ; } else { LOGGER . debug ( "the user-agent matched and no Authorization header was found" ) ; chain . doFilter ( request , response ) ; } }
@ Override public byte [ ] serializeHandshakeMessageContent ( ) { LOGGER . debug ( "Serializing CertificateMessage" ) ; code_block = IfStatement ; return getAlreadySerialized ( ) ; }
@ Test public void testGetJavaScriptOption ( ) { AnimateDuration animateDuration = new AnimateDuration ( 500 ) ; String expectedJavascript = "500" ; log . info ( expectedJavascript ) ; String generatedJavascript = animateDuration . getJavascriptOption ( ) . toString ( ) ; log . info ( generatedJavascript ) ; assertEquals ( generatedJavascript , expectedJavascript ) ; }
@ Test public void testGetJavaScriptOption ( ) { AnimateDuration animateDuration = new AnimateDuration ( 500 ) ; String expectedJavascript = "500" ; String generatedJavascript = animateDuration . getJavascriptOption ( ) . toString ( ) ; log . info ( expectedJavascript ) ; log . info ( generatedJavascript ) ; assertEquals ( generatedJavascript , expectedJavascript ) ; }
public void test() { try { FileUtils . moveDirectory ( dataTargetFolder ( ) , dataSourceFolder ( ) ) ; logger . info ( "Successfully moved data folder {" + dataTargetFolder ( ) . getAbsolutePath ( ) + "}" ) ; } catch ( Exception e ) { throw new RuntimeException ( "Could not move data folder to backup location {" + dataTargetFolder ( ) . getAbsolutePath ( ) + "}. Maybe the permissions not allowing this?" ) ; } }
public void test() { if ( cacheEventLogger != null ) { final String cacheEventLoggerAttributePrefix = auxPrefix + CACHE_EVENT_LOGGER_PREFIX + ATTRIBUTE_PREFIX ; log . debug ( "Setting cache event [{0}]" , auxPrefix ) ; PropertySetter . setProperties ( cacheEventLogger , props , cacheEventLoggerAttributePrefix + "." ) ; } else { log . info ( "No cache event logger defined for auxiliary [{0}]" , auxPrefix ) ; } }
public void test() { if ( cacheEventLogger != null ) { final String cacheEventLoggerAttributePrefix = auxPrefix + CACHE_EVENT_LOGGER_PREFIX + ATTRIBUTE_PREFIX ; PropertySetter . setProperties ( cacheEventLogger , props , cacheEventLoggerAttributePrefix + "." ) ; log . info ( "Using custom cache event logger [{0}] for auxiliary [{1}]" , cacheEventLogger , auxPrefix ) ; } else { log . info ( "No cache event logger defined for auxPrefix ) ; } }
public void test() { try { super . start ( ) ; super . destroy ( ) ; this . with ( LAST_UPDATED_ORDERBY . clone ( ) ) . direction ( DIRECTION . DESC ) . with ( POSTED_TIME_ORDERBY . clone ( ) ) . direction ( DIRECTION . DESC ) ; } catch ( Exception ex ) { LOG . warn ( ex ) ; } }
@ Override public void timeout ( long timeoutMs ) { this . timeoutMs = timeoutMs ; LOG . info ( "timeout {} ms" , timeoutMs ) ; }
public void test() { if ( StringUtils . isBlank ( value ) ) { logger . error ( "Missing value to '" + key + "'" ) ; } else { code_block = TryStatement ;  } }
public void test() { try { attr . add ( value , anyUtils ) ; } catch ( InvalidPlainAttrValueException e ) { String valueToPrint = value . length ( ) > 40 ? value . substring ( 0 , 20 ) + "..." : value ; log . warn ( e . getMessage ( ) , e ) ; invalidValues . getElements ( ) . add ( schema . getKey ( ) + ": " + valueToPrint + " - " + e . getMessage ( ) ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( indexed > 0 && rows != indexed ) { log . info ( "Moved " + rows + " rows" ) ; return rows ; } }
public void test() { try { BaseAppliance appliance = ondusService . getAppliance ( getRoom ( ) , config . applianceId ) . orElse ( null ) ; code_block = IfStatement ; return ( T ) appliance ; } catch ( IOException e ) { logger . debug ( "Failed to get appliance" , e ) ; updateStatus ( ThingStatus . OFFLINE , ThingStatusDetail . COMMUNICATION_ERROR , e . getMessage ( ) ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { if ( e instanceof InterpreterRPCException ) { result . ex = ( InterpreterRPCException ) e ; result . setExIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof InterpreterRPCException ) { result . ex = ( InterpreterRPCException ) e ; result . setExIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof InterpreterRPCException ) { result . ex = ( InterpreterRPCException ) e ; result . setExIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { try { fcall . sendResponse ( fb , msg , msgType , seqid ) ; } catch ( java . lang . Exception ex ) { _LOGGER . error ( "Exception writing to internal frame buffer" , ex ) ; fb . close ( ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isInfoEnabled ( ) ) { final Instant now = Instant . now ( ) ; final String entityName = entity . getClass ( ) . getSimpleName ( ) ; logger . info ( "Found entity {} in {}ms" , entityName , now ) ; } }
public void test() { if ( currentRetries >= MAX_RETRIES ) { log . error ( "Giving up after too many retries" , currentRetries ) ; giveUp . run ( ) ; } else { code_block = TryStatement ;  } }
public void test() { try { daoManager . startTransaction ( em ) ; UserDelegateEntity item = em . find ( UserDelegateEntity . class , id ) ; em . remove ( item ) ; daoManager . commitTransaction ( em ) ; } catch ( Exception e ) { daoManager . rollBackTransaction ( em ) ; logger . error ( "**** Error in UserDelegateEntityDAO:" , e ) ; } finally { daoManager . closeEntityManager ( em ) ; } }
@ Override @ BeforeEach public void setUp ( ) throws Exception { super . setUp ( ) ; LOG . debug ( "Writing file" ) ; testDirectory ( "in" , true ) ; Files . write ( testFile ( "in/file.dat" ) , "Line" . getBytes ( ) ) ; LOG . debug ( "Writing file DONE..." ) ; }
@ Override @ BeforeEach public void setUp ( ) throws Exception { super . setUp ( ) ; LOG . debug ( "Running test directory..." ) ; testDirectory ( "in" , true ) ; LOG . debug ( "Writing file..." ) ; Files . write ( testFile ( "in/file.dat" ) , "Line" . getBytes ( ) ) ; }
public void test() { try { String currentCounter = this . getCurrentIndex ( ) ; this . pageContext . getOut ( ) . print ( currentCounter ) ; _logger . info ( "loaded counter {}" , currentCounter ) ; } catch ( Throwable t ) { throw new JspException ( "error creating (or modifying) counter" , t ) ; } }
public void test() { try { ld = getAdminConnection ( ) ; ld . setTimeOut ( 0 ) ; RbacCreateSessionRequest rbacCreateSessionRequest = new RbacCreateSessionRequestImpl ( ) ; rbacCreateSessionRequest . setTenantId ( user . getContextId ( ) ) ; rbacCreateSessionRequest . setUserIdentity ( user . getUserId ( ) ) ; rbacCreateSessionRequest . setPassword ( new String ( user . getPassword ( ) ) ) ; code_block = IfStatement ; RbacCreateSessionResponse rbacCreateSessionResponse = ( RbacCreateSessionResponse ) ld . extended ( rbacCreateSessionRequest ) ; session = new Session ( user , rbacCreateSessionResponse . getSessionId ( ) ) ; code_block = IfStatement ; } catch ( LdapException e ) { String error = "createSession userId [" + user . getUserId ( ) + "] caught LDAPException=" + " msg=" + e . getMessage ( ) ; LOG . error ( error ) ; throw new SecurityException ( GlobalErrIds . ACEL_CREATE_SESSION_ERR , error , e ) ; } finally { closeAdminConnection ( ld ) ; } }
public void test() { try { serviceCenterTaskMonitor . beginCycle ( interval ) ; microserviceServiceCenterTask . run ( ) ; serviceCenterTaskMonitor . endCycle ( ) ; } catch ( Throwable e ) { LOGGER . error ( e . getMessage ( ) , e ) ; } }
@ Override public void appendToComment ( String st ) { logger . debug ( "not implemented" ) ; }
public void test() { if ( s_logger . isDebugEnabled ( ) ) { s_logger . debug ( log ( seq , "Unable to move " + req . toString ( ) ) ) ; } }
public void test() { if ( s_logger . isDebugEnabled ( ) ) { s_logger . debug ( log ( seq , "Unable to move " + req . toString ( ) ) ) ; } }
public void test() { if ( s_logger . isDebugEnabled ( ) ) { s_logger . debug ( log ( seq , "Unable to move " + req . toString ( ) ) ) ; } }
public void test() { if ( s_logger . isDebugEnabled ( ) ) { s_logger . debug ( log ( seq , "Unable to move " + req . toString ( ) ) ) ; } }
public void test() { try { boolean isStoragePoolStoragepolicyComplaince = storageMgr . isStoragePoolComplaintWithStoragePolicy ( requestVolumes , pool ) ; code_block = IfStatement ; } catch ( StorageUnavailableException e ) { LOGGER . error ( "Unable to check if storage is available for volume: " + pool , e ) ; return false ; } }
public void test() { try ( DataOutputStream outputStream = new DataOutputStream ( new BufferedOutputStream ( new FileOutputStream ( tempFile ) ) ) ) { synchronized ( partitionTable ) code_block = "" ; } catch ( IOException e ) { logger . error ( "Error creating temp file." , e ) ; } }
public void test() { try { Files . delete ( Paths . get ( oldFile . getAbsolutePath ( ) ) ) ; } catch ( IOException e ) { logger . error ( "Unable to delete file {}" , oldFile . getAbsolutePath ( ) ) ; } }
public void test() { if ( ! tempFile . renameTo ( oldFile ) ) { logger . warn ( "rename file={} failed to recover" , tempFile . getAbsolutePath ( ) ) ; } }
public void test() { if ( canRefresh ( ) ) { removeAll ( ) ; populateCache ( ) ; } else { logger . debug ( "Can't refresh" ) ; } }
public void test() { try { this . getRoleDAO ( ) . addRole ( role ) ; this . getRoleCacheWrapper ( ) . addRole ( role ) ; } catch ( Throwable t ) { logger . error ( "Error while adding a role" , t ) ; throw new ApsSystemException ( "Error while adding a role" , t ) ; } }
public void test() { try { ret = parseEnum ( enumeration , name ) ; } catch ( IllegalArgumentException e ) { LOGGER . warn ( "Error parsing enumeration: {}, name: {}" , enumeration . getName ( ) , name , e ) ; } catch ( NullPointerException e ) { LOGGER . warn ( "Error parsing enumeration: {}, name: {}" , enumeration . getName ( ) , name ) ; } }
public void test() { try { ret = parseEnum ( enumeration , name ) ; } catch ( IllegalArgumentException e ) { LOGGER . warn ( "Invalid name for enumeration: {}, name: {}" , enumeration . getName ( ) , name ) ; } catch ( NullPointerException e ) { LOGGER . warn ( "Null value for enumeration: {}, name: {}" , enumeration . getName ( ) , name ) ; } }
public void test() { if ( object instanceof String ) { String name = ( String ) object ; code_block = TryStatement ;  } else { logger . warn ( "Skipping input: {}" , object . getClass ( ) ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Refreshing." ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Refreshing." ) ; } }
public void test() { try { this . socket . close ( ) ; } catch ( IOException ioe ) { logger . error ( "IOException closing socket" , ioe ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
@ Test public void onlyLastCoalescingWorkShouldBeExecuted ( ) throws InterruptedException { log . debug ( "StreamWorkManagerTest.onlyLastCoalescingWorkShouldBeExecuted() beginning" ) ; int longDurationMs = 20_000 ; int shortDurationMs = 1_000 ; SleepWork longWork = createCoalescing ( longDurationMs ) ; SleepWork shortWork = createCoalescing ( shortDurationMs ) ; service . schedule ( shortWork ) ; service . schedule ( longWork ) ; service . schedule ( shortWork ) ; long start = System . currentTimeMillis ( ) ; assertTrue ( service . awaitCompletion ( 60 , TimeUnit . SECONDS ) ) ; long elapsed = System . currentTimeMillis ( ) - start ; log . debug ( "Execution time: {} ms" , elapsed ) ; tracker . assertDiff ( 0 , 0 , 4 , 0 ) ; assertTrue ( String . valueOf ( elapsed ) , elapsed < longDurationMs ) ; }
public void test() { try { insertPatternIdPs . setString ( 1 , axiomString ) ; insertPatternIdPs . setString ( 2 , axiomRenderer . render ( axiom ) ) ; insertPatternIdPs . setString ( 3 , getAxiomType ( axiom ) ) ; insertPatternIdPs . execute ( ) ; } catch ( SQLException e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void await ( long timeoutMillis ) throws InterruptedException { code_block = IfStatement ; long c = latch . getCount ( ) ; Preconditions . checkState ( 0 < c ) ; String msg = "Did not read expected number of messages before timeout was reached (latch count is " + c + ")" ; LOG . error ( msg ) ; throw new AssertionError ( msg ) ; }
public void test() { try { cursor = search ( "" , LdapConstants . OBJECT_CLASS_STAR , SearchScope . OBJECT , attributes ) ; code_block = IfStatement ; } catch ( Exception e ) { String msg = I18n . err ( I18n . ERR_04156_FAILED_FETCHING_ROOT_DSE ) ; LOG . error ( msg ) ; throw new LdapException ( msg , e ) ; } finally { code_block = IfStatement ; } }
public void test() { try { cursor . close ( ) ; } catch ( Exception e ) { logger . warn ( "Fail to close cursor" , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( resourceIdentifier == null ) { LOGGER . debug ( "ResourceIdentifier is null." ) ; return null ; } }
private List < String > installLinux ( List < String > urls , String saveAs ) { String apt = chainGroup ( installPackage ( MutableMap . of ( "apt" , "python-httplib2 libssl0.9.8" ) , null ) , sudo ( format ( "dpkg -i %s" , saveAs ) ) ) ; String yum = chainGroup ( "which yum" , ok ( sudo ( "sed -i.bk s/^enabled=1$/enabled=0/ /etc/yum/pluginconf.d/subscription-manager.conf" ) ) , ok ( sudo ( "yum check-update" ) ) , sudo ( "yum install -y pkgconfig" ) , sudo ( "[ -f /etc/redhat-release ] && (grep -i \"red hat\" /etc/redhat-release && sudo yum install -y openssl098e) || :" ) , sudo ( format ( " rpm --install %s" , saveAs ) ) ) ; String link = new DownloadProducerFromUrlAttribute ( ) . apply ( new BasicDownloadRequirement ( this ) ) . getPrimaryLocations ( ) . iterator ( ) . next ( ) ; LOGGER . info ( "Attempting to install {}" , saveAs ) ; return ImmutableList . < String > builder ( ) . add ( INSTALL_CURL ) . addAll ( Arrays . asList ( INSTALL_CURL , BashCommands . require ( BashCommands . alternatives ( BashCommands . simpleDownloadUrlAs ( urls , saveAs ) , "curl -f -L -k " + BashStringEscapes . wrapBash ( link ) + " -H 'Referer: http://www.couchbase.com/downloads'" + " -o " + saveAs ) , "Could not retrieve " + saveAs + " (from " + urls . size ( ) + " sites)" , 9 ) ) ) . add ( alternatives ( apt , yum ) ) . build ( ) ; }
SwaggerResource swaggerResource ( String name , String location ) { LOGGER . debug ( "Creating swagger resource" ) ; SwaggerResource swaggerResource = new SwaggerResource ( ) ; swaggerResource . setName ( name ) ; swaggerResource . setLocation ( location ) ; swaggerResource . setSwaggerVersion ( "3.0" ) ; return swaggerResource ; }
@ Override @ RequestMapping public @ ResponseBody void query ( @ RequestParam ( value = "query" , required = false ) String query , @ RequestParam ( value = "format" , required = false ) String format , @ RequestParam ( value = "offset" , required = false ) Integer offset , @ RequestParam ( value = "limit" , required = false ) Integer limit , @ RequestParam ( value = "inference" , required = false ) boolean inference , HttpServletRequest request , HttpServletResponse response ) throws QueryParseException , LodeException , IOException { final long startTime = System . currentTimeMillis ( ) ; String v = request . getHeader ( "Referer" ) ; LoggingContext . put ( "webui" , v != null && v . contains ( "/mesh/query" ) ) ; if ( query != null ) LoggingContext . put ( "query" , query ) ; if ( format != null ) LoggingContext . put ( "format" , format ) ; if ( limit != null ) LoggingContext . put ( "limit" , limit ) ; if ( offset != null ) LoggingContext . put ( "offset" , offset ) ; LoggingContext . put ( "inference" , inference ) ; super . query ( query , format , offset , limit , inference , request , response ) ; final long responseTime = System . currentTimeMillis ( ) - startTime ; LoggingContext . put ( "responsetime" , responseTime ) ; LoggingContext . clear ( ) ; log . info ( "Response: {}" , responseTime ) ; }
public void test() { if ( e instanceof RuntimeException || e instanceof Error ) { LOG . warn ( logMsg , e ) ; } else-if ( exceptionsHandler . isTerse ( e . getClass ( ) ) ) { LOG . info ( logMsg ) ; } else { LOG . info ( logMsg , e ) ; } }
public void test() { if ( e instanceof RuntimeException || e instanceof Error ) { LOG . warn ( logMsg , e ) ; } else-if ( exceptionsHandler . isTerse ( e . getClass ( ) ) ) { LOG . info ( logMsg ) ; } else { LOG . info ( logMsg , e ) ; } }
public void test() { if ( e instanceof RuntimeException || e instanceof Error ) { LOG . warn ( logMsg , e ) ; } else-if ( exceptionsHandler . isTerse ( e . getClass ( ) ) ) { LOG . info ( logMsg ) ; } else { LOG . info ( logMsg , e ) ; } }
public void test() { if ( buf . size ( ) > maxRespSize ) { LOG . warn ( "Large response size " + buf . size ( ) + " for call " + call . toString ( ) ) ; buf = new ByteArrayOutputStream ( INITIAL_RESP_BUF_SIZE ) ; } }
public void test() { try { setupResponse ( buf , call , ( error == null ) ? Status . SUCCESS : Status . ERROR , value , errorClass , error ) ; code_block = IfStatement ; channelWrite ( ctx , call . response ) ; } catch ( Exception e ) { LOGGER . error ( "Error writing response" , e ) ; error = null ; } finally { IOUtils . closeStream ( buf ) ; } }
public void test() { if ( Boolean . TRUE . equals ( p . isArray ) ) { p . vendorExtensions . put ( "x-protobuf-type" , "repeated" ) ; } else-if ( Boolean . TRUE . equals ( p . isMap ) ) { logger . warn ( "parameter is a 'x-protobuf-type' option, but will use 'x-type' instead." ) ; } }
public void test() { try { long t = System . currentTimeMillis ( ) ; int numOfRows = transactionTemplate . execute ( new TransactionCallback < Integer > ( ) code_block = "" ; ) ; t = System . currentTimeMillis ( ) - t ; if ( logger . isDebugEnabled ( ) ) logger . debug ( getClass ( ) . getSimpleName ( ) + ": " + numOfRows + " rows where processed in " + t + " ms" ) ; else-if ( t > TimeUnit . MINUTES . toMillis ( 1 ) ) logger . warn ( "Rolling between table " + previousTable + " to table " + activeTable + ", took :" + t + " ms" ) ; } catch ( DataAccessException ex ) { logger . error ( "Error during processing" , ex ) ; } }
public void test() { { log . info ( Color . GREEN + "Calendar_2_6 : empty column trip_id" + Color . NORMAL ) ; Context context = new Context ( ) ; CheckPointReport result = verifyValidation ( log , context , "calendar_2_6" , GTFS_1_GTFS_Common_9 , SEVERITY . ERROR , RESULT . NOK , true ) ; Assert . assertEquals ( result . getCheckPointErrorCount ( ) , 1 , "detail count" ) ; code_block = ForStatement ; } }
public void test() { try { long entryScopeGroupId = GetterUtil . getLong ( element . elementText ( OpenSearchUtil . getQName ( "scopeGroupId" , OpenSearchUtil . LIFERAY_NAMESPACE ) ) ) ; code_block = IfStatement ; resultRows . add ( element ) ; } catch ( Exception exception ) { logger . error ( exception . getMessage ( ) , exception ) ; totalRows -- ; } }
public void test() { try { xml = XMLUtil . stripInvalidChars ( xml ) ; Document document = SAXReaderUtil . read ( xml ) ; Element rootElement = document . getRootElement ( ) ; List < Element > elements = rootElement . elements ( "entry" ) ; totalRows = GetterUtil . getInteger ( rootElement . elementText ( OpenSearchUtil . getQName ( "totalResults" , OpenSearchUtil . OS_NAMESPACE ) ) ) ; code_block = ForStatement ; } catch ( Exception exception ) { _log . error ( exception . getMessage ( ) ) ; } }
public void test() { try { String [ ] peerInfo = channel . remoteAddress ( ) . toString ( ) . replace ( "/" , "" ) . split ( ":" ) ; return new Pair < > ( peerInfo [ 0 ] , Integer . parseInt ( peerInfo [ 1 ] ) ) ; } catch ( Exception e ) { logger . debug ( "Failed to parse peer info." , e ) ; } }
@ Test public void testNumericAndRange ( ) throws Exception { log . info ( "------  testNumericAndRange  ------" ) ; String query = "(" + CityField . NUM . name ( ) + GTE_OP + "30)" + AND_OP + "(" + CityField . NUM . name ( ) + LTE_OP + "105)" ; runTest ( "((_Bounded_ = true) && (" + query + "))" , query ) ; }
public void test() { if ( idsSet . size ( ) == upperBoundPerPartition ) { logger . warn ( " idsSet.size() == idsSet.size()" ) ; } }
public void test() { try { return Seq . seq ( podExecutor . exec ( pod , StackgresClusterContainers . PATRONI , "sh" , "-c" , Seq . seq ( PatroniStatsScripts . getScripts ( ) ) . map ( tt -> "echo \"" + tt . v1 . getName ( ) + ":$( (" + tt . v2 + ") 2>&1 | tr -d '\\n')\"\n" ) . toString ( ) ) ) . peek ( line code_block = LoopStatement ; ) . filter ( line -> ! line . endsWith ( "failed" ) ) . map ( line -> Tuple . tuple ( line , line . indexOf ( ":" ) ) ) . map ( tt -> Tuple . tuple ( PatroniStatsScripts . fromName ( tt . v1 . substring ( 0 , tt . v2 ) ) , tt . v1 . substring ( tt . v2 + 1 ) ) ) . collect ( ImmutableMap . toImmutableMap ( Tuple2 :: v1 , Tuple2 :: v2 ) ) ; } catch ( Exception ex ) { LOG . error ( "Error while running periodic stats" , ex ) ; return ImmutableMap . < PassengeriStatsScripts , String > of ( ) ; } }
@ Override public void run ( ) { logger . info ( "run called" ) ; RadioStation oldStation = currentStation ; currentStationIndex ++ ; currentStationIndex = currentStationIndex % stationsList . size ( ) ; currentStation = stationsList . get ( currentStationIndex ) ; currentStationChanged ( currentStation ) ; deferred . resolve ( ) ; }
@ Test public void dropImport ( ) throws Exception { Repository repository = createTestRepoUsingRepoService ( ) ; getL10nJCommander ( ) . run ( "push" , "-r" , repository . getName ( ) , "-s" , getInputResourcesTestDir ( "source" ) . getAbsolutePath ( ) ) ; Asset asset = assetClient . getAssetByPathAndRepositoryId ( "source-xliff.xliff" , repository . getId ( ) ) ; importTranslations ( asset . getId ( ) , "source-xliff_" , "fr-FR" ) ; importTranslations ( asset . getId ( ) , "source-xliff_" , "ja-JP" ) ; Asset asset2 = assetClient . getAssetByPathAndRepositoryId ( "source2-xliff.xliff" , repository . getId ( ) ) ; importTranslations ( asset2 . getId ( ) , "source2-xliff_" , "fr-FR" ) ; importTranslations ( asset2 . getId ( ) , "source2-xliff_" , "ja-JP" ) ; waitForRepositoryToHaveStringsForTranslations ( repository . getId ( ) ) ; getL10nJCommander ( ) . run ( "drop-export" , "-r" , repository . getName ( ) ) ; final Long dropId = getLastDropIdFromOutput ( outputCapture ) ; Console mockConsole = mock ( Console . class ) ; when ( mockConsole . readLine ( Long . class ) ) . thenAnswer ( new Answer < Long > ( ) code_block = "" ; ) ; L10nJCommander l10nJCommander = getL10nJCommander ( ) ; DropImportCommand dropImportCommand = l10nJCommander . getCommand ( DropImportCommand . class ) ; dropImportCommand . console = mockConsole ; int numberOfFrenchTranslationsBefore = getNumberOfFrenchTranslations ( repository ) ; localizeDropFiles ( dropRepository . findById ( dropId ) . orElse ( null ) ) ; l10nJCommander . run ( new String [ ] code_block = "" ; ) ; int numberOfFrenchTranslationsAfter = getNumberOfFrenchTranslations ( repository ) ; assertEquals ( "
public void test() { try { PrintWriter printwriter = new PrintWriter ( stringwriter ) ; printwriter . printf ( id , pattern ) ; formattedID = stringwriter . toString ( ) ; } catch ( IllegalFormatException e ) { log . error ( "Could not format ID %s" , id ) ; } }
@ RestAccessControl ( permission = Permission . SUPERUSER ) @ RequestMapping ( value = "/report/{reportCode}" , method = RequestMethod . GET , produces = MediaType . APPLICATION_JSON_VALUE ) public ResponseEntity < SimpleRestResponse < DumpReportDto > > getDumpReport ( @ PathVariable String reportCode ) { logger . debug ( "Extracting dump report -> {}" , reportCode ) ; DumpReportDto result = this . getDatabaseService ( ) . getDumpReportDto ( reportCode ) ; logger . debug ( "Extracted dump report -> {}" , result ) ; return new ResponseEntity < > ( new SimpleRestResponse < > ( result ) , HttpStatus . OK ) ; }
@ RestAccessControl ( permission = Permission . SUPERUSER ) @ RequestMapping ( value = "/report/{reportCode}" , method = RequestMethod . GET , produces = MediaType . APPLICATION_JSON_VALUE ) public ResponseEntity < SimpleRestResponse < DumpReportDto > > getDumpReport ( @ PathVariable String reportCode ) { logger . debug ( "Required dump report -> code {}" , reportCode ) ; DumpReportDto result = this . getDatabaseService ( ) . getDumpReportDto ( reportCode ) ; logger . debug ( "Result of dump report -> {}" , result ) ; return new ResponseEntity < > ( new SimpleRestResponse < > ( result ) , HttpStatus . OK ) ; }
public void test() { if ( debugEnabled ) { log . debug ( "destroy({}) Destroy ({})" , this , channel ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { try { Class < ? > clazz = Class . forName ( verifyProcessClassName ) ; VerifyProcess verifyProcess = ( VerifyProcess ) clazz . newInstance ( ) ; code_block = IfStatement ; verifyProcess . verify ( ) ; code_block = IfStatement ; return true ; } catch ( ClassNotFoundException classNotFoundException ) { _log . error ( verifyProcessClassName + " cannot be found" , classNotFoundException ) ; } catch ( IllegalAccessException illegalAccessException ) { _log . error ( verifyProcessClassName + " cannot be accessed" , illegalAccessException ) ; } catch ( InstantiationException instantiationException ) { _log . error ( verifyProcessClassName + " cannot be initiated" , instantiationException ) ; } }
public void test() { try { Class < ? > clazz = Class . forName ( verifyProcessClassName ) ; VerifyProcess verifyProcess = ( VerifyProcess ) clazz . newInstance ( ) ; code_block = IfStatement ; verifyProcess . verify ( ) ; code_block = IfStatement ; return true ; } catch ( ClassNotFoundException classNotFoundException ) { _log . error ( verifyProcessClassName + " cannot be found" , classNotFoundException ) ; } catch ( IllegalAccessException illegalAccessException ) { _log . error ( verifyProcessClassName + " cannot be accessed" , illegalAccessException ) ; } catch ( InstantiationException instantiationException ) { _log . error ( verifyProcessClassName + " cannot be initiated" , instantiationException ) ; } }
public void test() { try { Class < ? > clazz = Class . forName ( verifyProcessClassName ) ; VerifyProcess verifyProcess = ( VerifyProcess ) clazz . newInstance ( ) ; code_block = IfStatement ; verifyProcess . verify ( ) ; code_block = IfStatement ; return true ; } catch ( ClassNotFoundException classNotFoundException ) { _log . error ( verifyProcessClassName + " cannot be found" , classNotFoundException ) ; } catch ( IllegalAccessException illegalAccessException ) { _log . error ( verifyProcessClassName + " cannot be accessed" , illegalAccessException ) ; } catch ( InstantiationException instantiationException ) { _log . error ( verifyProcessClassName + " cannot be created" , instantiationException ) ; } }
public void debug ( Object msg , Throwable thr ) { logger . debug ( msg , thr ) ; }
public void test() { try { DigirMetadata metadata = getDigirMetadata ( endpoint ) ; String code = MachineTagUtils . firstTag ( dataset , TagName . DIGIR_CODE ) . getValue ( ) ; int numberOfRecords = metadata . getResources ( ) . stream ( ) . filter ( ( r ) -> code . equals ( r . getCode ( ) ) ) . findFirst ( ) . map ( DigirResource :: getNumberOfRecords ) . orElse ( 0 ) ; code_block = IfStatement ; return null ; } catch ( Exception e ) { LOGGER . error ( "Unable to retrieve count of DiGIR dataset [" + dataset . getKey ( ) + "]" , e ) ; throw new MetadataException ( "Unable to retrieve count of DiGIR dataset [" + dataset . getKey ( ) + "]" , e , ErrorCode . OTHER_ERROR ) ; } }
public void test() { if ( customScriptConfiguration == null ) { logger . debug ( "No custom script configuration found" ) ; } else { this . authAcr = customScriptConfiguration . getCustomScript ( ) . getName ( ) ; boolean result = externalAuthenticationService . executeExternalAuthenticate ( customScriptConfiguration , null , 1 ) ; logger . info ( "Authentication result for user '{}', result: '{}'" , credentials . getUsername ( ) , result ) ; code_block = IfStatement ; } }
public void test() { try { ecdsaPkGroupsStatic = new LinkedList < > ( ) ; ecdsaPkGroupsEphemeral = new LinkedList < > ( ) ; ecdsaPkGroupsTls13 = new LinkedList < > ( ) ; ecdsaCertSigGroupsStatic = new LinkedList < > ( ) ; ecdsaCertSigGroupsEphemeral = new LinkedList < > ( ) ; ecdsaCertSigGroupsTls13 = new LinkedList < > ( ) ; Set < CertificateChain > certificates = new HashSet < > ( ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; } catch ( Exception E ) { LOGGER . error ( "Could not execute KdsScript" , E ) ; return getCouldNotExecuteResult ( ) ; } }
public void test() { try { r . run ( ) ; } catch ( final Exception e ) { LOGGER . warn ( "Failed to process shutdown task." , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( testName + ": status = " + statusCode ) ; logger . debug ( testName + ": " + body ) ; logger . debug ( testName + ": " + res ) ; } }
private void killOozieJob ( String jobId ) { String out = sshOozieClient . killJob ( jobId ) ; LOG . info ( "Killing job " + jobId + " from " + output ) ; }
public void test() { for ( ObjectName on : s ) { log . info ( on . toString ( ) ) ; } }
public void test() { if ( ! connection . isValid ( 10 ) ) { LOG . debug ( "Establishing connection" ) ; establishConnection ( ) ; jdbcWriter . prepareStatement ( connection ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( SQLException e ) { LOG . error ( "jdbc exception when reconnect db.." , e ) ; } catch ( ClassNotFoundException e ) { LOG . error ( "load jdbc class error when reconnect db.." , e ) ; } catch ( IOException e ) { LOG . error ( "jdbc io exception.." , e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( SQLException e ) { LOG . error ( "check connection open failed.." , e ) ; } catch ( ClassNotFoundException e ) { LOG . error ( "jdbc io exception.." , e ) ; } catch ( IOException e ) { LOG . error ( "jdbc io exception.." , e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( SQLException e ) { LOG . error ( "check connection open failed.." , e ) ; } catch ( ClassNotFoundException e ) { LOG . error ( "load jdbc class error when reconnect db.." , e ) ; } catch ( IOException e ) { LOG . error ( "I/O error when reconnect db.." , e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CommercePriceEntryServiceUtil . class , "upsertCommercePriceEntry" , _upsertCommercePriceEntryParameterTypes27 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , commercePriceEntryId , cProductId , cpInstanceUuid , commercePriceListId , externalReferenceCode , price , discountDiscovery , discountLevel1 , discountLevel2 , discountLevel3 , discountLevel4 , displayDateMonth , displayDateDay , displayDateYear , displayDateHour , displayDateMinute , expirationDateMonth , expirationDateDay , expirationDateYear , expirationDateHour , expirationDateMinute , neverExpire , skuExternalReferenceCode , serviceContext ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . commerce . price . list . model . CommercePriceEntry ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
private void getAllLogicalSwitches ( ) { ResourceCollection < LogicalSwitch > logicalSwitches = client . getAll ( ) ; LOGGER . info ( "Logical switches returned to client: {}" , logicalSwitches . toJsonString ( ) ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try ( CloseableHttpClient closeableHttpClient = httpClientBuilder . useSystemProperties ( ) . build ( ) ) { HttpPost httpPost = new HttpPost ( callbackURL ) ; httpPost . setEntity ( new StringEntity ( _objectMapper . writeValueAsString ( Collections . singletonMap ( id , executeStatus ) ) , ContentType . APPLICATION_JSON ) ) ; closeableHttpClient . execute ( httpPost ) ; } catch ( Exception exception ) { _log . error ( "Unable to send callback url" , exception ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( tokenResult . succeeded ( ) ) { AccessToken token = tokenResult . result ( ) ; headerMap . set ( "Authorization" , "Bearer " + token . principal ( ) . getString ( "access_token" ) ) ; log . info ( "Login successful." ) ; resultHandler . handle ( Future . succeededFuture ( ) ) ; } else { log . error ( "Access Token Error: {0}." , tokenResult . cause ( ) . getMessage ( ) ) ; resultHandler . handle ( Future . failedFuture ( tokenResult . cause ( ) ) ) ; } }
public void test() { if ( tokenResult . succeeded ( ) ) { log . debug ( "OAuth2 Keycloak exchange succeeded." ) ; AccessToken token = tokenResult . result ( ) ; headerMap . set ( "Authorization" , "Bearer " + token . principal ( ) . getString ( "access_token" ) ) ; resultHandler . handle ( Future . succeededFuture ( ) ) ; } else { log . error ( "OAuth2 Keycloak exchange failed: " + tokenResult . cause ( ) . getMessage ( ) , tokenResult . cause ( ) ) ; resultHandler . handle ( Future . failedFuture ( tokenResult . cause ( ) ) ) ; } }
public void test() { try { org . structr . web . entity . dom . DocumentFragment fragment = app . create ( org . structr . web . entity . dom . DocumentFragment . class ) ; fragment . setOwnerDocument ( thisPage ) ; return fragment ; } catch ( FrameworkException fex ) { final Logger logger = LoggerFactory . getLogger ( Page . class ) ; logger . warn ( "Unable to create fragment: {}" , fex . getMessage ( ) ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { final InetAddress address = SocketUtils . addressByName ( host ) ; socket = new ServerSocket ( 0 , 3 , address ) ; return ( InetSocketAddress ) socket . getLocalSocketAddress ( ) ; } catch ( final Exception e ) { LOG . error ( e . getMessage ( ) , e ) ; return null ; } finally { code_block = IfStatement ; } }
public void test() { try { socket . close ( ) ; } catch ( final Exception e ) { logger . error ( "Failed to close multicast socket due to {}" , e . getMessage ( ) ) ; } }
public void saveModel ( CustomModel custModel , String filename ) throws IOException { File dir = new File ( modelDir ) ; code_block = IfStatement ; log . info ( "Saving model to: {}" , dir . getAbsolutePath ( ) ) ; File f = new File ( filename ) ; ModelSerializer . writeModel ( custModel . getModel ( ) , filename , true ) ; String labelFilename = filename + ".labels" ; FileWriter fw = new FileWriter ( new File ( labelFilename ) ) ; fw . write ( StringUtils . join ( custModel . getLabels ( ) , "|" ) ) ; fw . flush ( ) ; fw . close ( ) ; log . info ( "Model saved: {}" , f . getAbsolutePath ( ) ) ; }
public void saveModel ( CustomModel custModel , String filename ) throws IOException { File dir = new File ( modelDir ) ; code_block = IfStatement ; File f = new File ( filename ) ; log . info ( "Saving DL4J computation graph model to {}" , f . getAbsolutePath ( ) ) ; ModelSerializer . writeModel ( custModel . getModel ( ) , filename , true ) ; String labelFilename = filename + ".labels" ; FileWriter fw = new FileWriter ( new File ( labelFilename ) ) ; fw . write ( StringUtils . join ( custModel . getLabels ( ) , "|" ) ) ; fw . flush ( ) ; fw . close ( ) ; log . info ( "Model saved to {}" , custModel ) ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
private GrpcTransportConfig loadGrpcTransportConfig ( ) { GrpcTransportConfig grpcTransportConfig = new GrpcTransportConfig ( ) ; grpcTransportConfig . read ( profilerConfig . getProperties ( ) ) ; LOGGER . info ( "Load GrpcTransportConfig from {}" , profilerConfig ) ; return grpcTransportConfig ; }
public void test() { if ( buttonList != null && component <= buttonList . size ( ) ) { return buttonList . get ( component - 1 ) ; } else { logger . debug ( " Button not found" ) ; return 0 ; } }
public void test() { if ( deviceButtonMap != null ) { List < Integer > buttonList = deviceButtonMap . get ( integrationID ) ; code_block = IfStatement ; } else { logger . debug ( "No device button found for integrationID: {}" , integrationID ) ; return 0 ; } }
@ Override public void versionCommand ( final org . locationtech . geowave . service . grpc . protobuf . VersionCommandParametersProtos request , final io . grpc . stub . StreamObserver < org . locationtech . geowave . service . grpc . protobuf . GeoWaveReturnTypesProtos . StringResponseProtos > responseObserver ) { final VersionCommand cmd = new VersionCommand ( ) ; final Map < FieldDescriptor , Object > m = request . getAllFields ( ) ; GeoWaveGrpcServiceCommandUtil . setGrpcToCommandFields ( m , cmd ) ; final File configFile = GeoWaveGrpcServiceOptions . geowaveConfigFile ; final OperationParams params = new ManualOperationParams ( ) ; params . getContext ( ) . put ( ConfigOptions . PROPERTIES_FILE_CONTEXT , configFile ) ; cmd . prepare ( params ) ; LOGGER . info ( "Executing Version Command..." ) ; code_block = TryStatement ;  }
public void test() { try { cmd . computeResults ( params ) ; final StringResponseProtos resp = StringResponseProtos . newBuilder ( ) . build ( ) ; responseObserver . onNext ( resp ) ; responseObserver . onCompleted ( ) ; } catch ( final Exception e ) { LOGGER . error ( "Exception encountered executing command" , e ) ; responseObserver . onError ( e ) ; } }
public void startPeriodicFetching ( ) { code_block = IfStatement ; _log . info ( "Starting splitFetcher every {} seconds" , _refreshEveryNSeconds . get ( ) ) ; _scheduledFuture = _scheduledExecutorService . scheduleWithFixedDelay ( _splitFetcher . get ( ) , 0L , _refreshEveryNSeconds . get ( ) , TimeUnit . SECONDS ) ; }
public void test() { if ( query . getFields ( ) == null ) { client . getSession ( ) . execute ( CassandraQueryFactory . getTruncateTableQuery ( mapping ) ) ; } else { logger . debug ( "Truncate fields: " + query . getTruncateTableQuery ( ) ) ; } }
public void test() { try { List < Object > objectArrayList = new ArrayList < > ( ) ; code_block = IfStatement ; return 0 ; } catch ( Exception e ) { LOG . error ( "" , e ) ; throw new GoraException ( e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( UnsupportedEncodingException e ) { logger . error ( e . getMessage ( ) , e ) ; return null ; } }
@ Override public OutputStream createOutputStream ( long arg0 ) throws IOException { LOG . debug ( "createOutputStream(" + arg0 + ")" ) ; file = new MyOutputStream ( ) ; return file ; }
public void test() { if ( log . isInfoEnabled ( ) ) { log . info ( msg ) ; } }
public void test() { if ( bridge == null ) { logger . warn ( "Received a null bridge." ) ; return ; } }
public void test() { if ( x10Function > 0 ) { X10Interface x10Interface = cm11aHandler . getX10Interface ( ) ; x10Interface . scheduleHWUpdate ( this ) ; } else { logger . debug ( "Unknown X10 interface." ) ; } }
public void test() { if ( cm11aHandler != null && cm11aHandler . getThing ( ) . getStatus ( ) . equals ( ThingStatus . ONLINE ) ) { code_block = IfStatement ; code_block = IfStatement ; } else { logger . debug ( "Cannot connect to CM11a handler." ) ; } }
public void test() { try { file . delete ( ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { switch ( strategy ) { case IGNORE : LOG . debug ( message ) ; break ; case LOGGING : LOG . warn ( message ) ; break ; case FAIL : throw new IllegalArgumentException ( message ) ; default : throw new AssertionError ( strategy ) ; } }
public void test() { switch ( strategy ) { case IGNORE : LOG . debug ( message ) ; break ; case LOGGING : LOG . info ( message ) ; break ; case FAIL : throw new IllegalArgumentException ( message ) ; default : throw new AssertionError ( strategy ) ; } }
public void test() { while ( jobs . hasNext ( ) ) { long oldID = jobs . next ( ) ; long newID = jobDaoProvider . get ( ) . rescheduleJob ( oldID ) ; log . info ( "Rescheduling job [job=" + oldID + "]" ) ; ++ resubmitcount ; } }
public void test() { try { AIP aip = model . retrieveAIP ( representation . getAipId ( ) ) ; List < String > ancestors = SolrUtils . getAncestors ( aip . getParentId ( ) , model ) ; indexRepresentation ( aip , representation , ancestors ) . addTo ( ret ) ; code_block = IfStatement ; } catch ( RequestNotValidException | NotFoundException | GenericException | AuthorizationDeniedException e ) { LOGGER . error ( "Error while indexing AIPs" , e ) ; ret . add ( e ) ; } }
protected void runTestQueryWithUniqueness ( Set < Set < String > > expected , String querystr , Date startDate , Date endDate , Map < String , String > extraParms , AccumuloClient client ) throws Exception { log . debug ( "Starting" ) ; QueryImpl settings = new QueryImpl ( ) ; settings . setBeginDate ( startDate ) ; settings . setEndDate ( endDate ) ; settings . setPagesize ( Integer . MAX_VALUE ) ; settings . setQueryAuthorizations ( auths . serialize ( ) ) ; settings . setQuery ( querystr ) ; settings . setParameters ( extraParms ) ; settings . setId ( UUID . randomUUID ( ) ) ; log . debug ( "query: " + settings . getQuery ( ) ) ; log . debug ( "logic: " + settings . getQueryLogicName ( ) ) ; GenericQueryConfiguration config = logic . initialize ( client , settings , authSet ) ; logic . setupQuery ( config ) ; DocumentTransformer transformer = ( DocumentTransformer ) ( logic . getTransformer ( settings ) ) ; TransformIterator iter = new DatawaveTransformIterator ( logic . iterator ( ) , transformer ) ; List < Object > eventList = new ArrayList < > ( ) ; code_block = WhileStatement ; BaseQueryResponse response = transformer . createResponse ( eventList ) ; Assert . assertTrue ( response instanceof DefaultEventQueryResponse ) ; DefaultEventQueryResponse eventQueryResponse = ( DefaultEventQueryResponse ) response ; code_block = ForStatement ; Assert . assertTrue ( expected . isEmpty ( ) ) ; }
protected void runTestQueryWithUniqueness ( Set < Set < String > > expected , String querystr , Date startDate , Date endDate , Map < String , String > extraParms , AccumuloClient client ) throws Exception { log . debug ( "runTestQueryWithUniqueness" ) ; QueryImpl settings = new QueryImpl ( ) ; settings . setBeginDate ( startDate ) ; settings . setEndDate ( endDate ) ; settings . setPagesize ( Integer . MAX_VALUE ) ; settings . setQueryAuthorizations ( auths . serialize ( ) ) ; settings . setQuery ( querystr ) ; settings . setParameters ( extraParms ) ; settings . setId ( UUID . randomUUID ( ) ) ; log . debug ( "query: " + settings . getQueryString ( ) ) ; log . debug ( "logic: " + settings . getQueryLogicName ( ) ) ; GenericQueryConfiguration config = logic . initialize ( client , settings , authSet ) ; logic . setupQuery ( config ) ; DocumentTransformer transformer = ( DocumentTransformer ) ( logic . getTransformer ( settings ) ) ; TransformIterator iter = new DatawaveTransformIterator ( logic . iterator ( ) , transformer ) ; List < Object > eventList = new ArrayList < > ( ) ; code_block = WhileStatement ; BaseQueryResponse response = transformer . createResponse ( eventList ) ; Assert . assertTrue ( response instanceof DefaultEventQueryResponse ) ; DefaultEventQueryResponse eventQueryResponse = ( DefaultEventQueryResponse ) response ; code_block = ForStatement ; Assert . assertTrue ( expected . isEmpty ( ) ) ; }
protected void runTestQueryWithUniqueness ( Set < Set < String > > expected , String querystr , Date startDate , Date endDate , Map < String , String > extraParms , AccumuloClient client ) throws Exception { log . debug ( "runTestQueryWithUniqueness" ) ; QueryImpl settings = new QueryImpl ( ) ; settings . setBeginDate ( startDate ) ; settings . setEndDate ( endDate ) ; settings . setPagesize ( Integer . MAX_VALUE ) ; settings . setQueryAuthorizations ( auths . serialize ( ) ) ; settings . setQuery ( querystr ) ; settings . setParameters ( extraParms ) ; settings . setId ( UUID . randomUUID ( ) ) ; log . debug ( "query: " + settings . getQuery ( ) ) ; GenericQueryConfiguration config = logic . initialize ( client , settings , authSet ) ; logic . setupQuery ( config ) ; DocumentTransformer transformer = ( DocumentTransformer ) ( logic . getTransformer ( settings ) ) ; TransformIterator iter = new DatawaveTransformIterator ( logic . iterator ( ) , transformer ) ; List < Object > eventList = new ArrayList < > ( ) ; code_block = WhileStatement ; BaseQueryResponse response = transformer . createResponse ( eventList ) ; log . debug ( "response: " + response ) ; Assert . assertTrue ( response instanceof DefaultEventQueryResponse ) ; DefaultEventQueryResponse eventQueryResponse = ( DefaultEventQueryResponse ) response ; code_block = ForStatement ; Assert . assertTrue ( expected . isEmpty ( ) ) ; }
public void test() { try { count ++ ; TextMessage tm = ( TextMessage ) m ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; } catch ( Exception e ) { failed = true ; latch . countDown ( ) ; LOG . debug ( "Received exception " , e ) ; } }
public void test() { try { longVal = ( Long ) value ; } catch ( ClassCastException cce ) { logger . warn ( "Unable to cast long value to long: " + value ) ; return "" ; } }
public void test() { try { spi ( grid0 ) . waitForBlocked ( ) ; } catch ( InterruptedException e ) { log . error ( "Interrupted while sleeping" , e ) ; } }
public void test() { if ( QUERY_LOGGER . isDebugEnabled ( ) ) { QUERY_LOGGER . debug ( "query {}" , query ) ; } }
public void test() { if ( eventTableOrNull == null ) { QUERY_PLAN_LOG . info ( prefix + indexText + eventTableOrNull . toQueryPlan ( ) ) ; } else { QUERY_PLAN_LOG . info ( prefix + indexText + eventTableOrNull . toQueryPlan ( ) ) ; } }
public void test() { if ( eventTableOrNull == null ) { QUERY_PLAN_LOG . info ( prefix + indexText ) ; } else { QUERY_PLAN_LOG . info ( prefix + indexText ) ; } }
public void test() { try { code_block = IfStatement ; if ( null != result ) return result ; StringApiResponse response = new StringApiResponse ( ) ; response . setResult ( SUCCESS , null ) ; result = response ; } catch ( NoSuchMethodException e ) { _logger . error ( "No such method '{}' of class '{}'" , apiMethod . getSpringBeanMethod ( ) , bean . getClass ( ) , e ) ; throw new ApiException ( IApiErrorCodes . API_METHOD_ERROR , "Method not supported - " + this . buildApiSignature ( apiMethod ) , Response . Status . INTERNAL_SERVER_ERROR ) ; } catch ( InvocationTargetException e ) { code_block = IfStatement ; } catch ( Throwable t ) { _logger . error ( "Error executing method '{}' of class '{}'" , apiMethod . getSpringBeanMethod ( ) , bean . getClass ( ) , t ) ; throw t ; } }
private void successMessage ( String msg ) { LOG . info ( msg ) ; messages . add ( msg ) ; }
public void test() { try { int elementIndex = this . getElementIndex ( ) ; ListAttributeInterface currentAttribute = ( ListAttributeInterface ) entity . getAttribute ( this . getAttributeName ( ) ) ; code_block = IfStatement ; _logger . debug ( "Element oy type {} removed fomr the list {}" , currentAttribute . getNestedAttributeTypeCode ( ) , currentAttribute . getName ( ) ) ; } catch ( Throwable t ) { _logger . error ( "error in removeElement" , t ) ; return FAILURE ; } }
public void test() { try { RpObject rpObj = new RpObject ( getDnForRp ( rp . getOxdId ( ) ) , rp . getOxdId ( ) , Jackson2 . serializeWithoutNulls ( rp ) ) ; this . persistenceEntryManager . merge ( rpObj ) ; LOG . debug ( "Updated RP: {} " , rp ) ; return true ; } catch ( Exception e ) { LOG . error ( "Failed to update RP: {} " , rp , e ) ; } }
public void test() { try { RpObject rpObj = new RpObject ( getDnForRp ( rp . getOxdId ( ) ) , rp . getOxdId ( ) , Jackson2 . serializeWithoutNulls ( rp ) ) ; this . persistenceEntryManager . merge ( rpObj ) ; LOG . debug ( "RP updated successfully. RP : {} " , rpObj ) ; return true ; } catch ( Exception e ) { LOG . error ( "Failed to update RP: {} " , e . getMessage ( ) , e ) ; } }
public void test() { try { Properties props = new Properties ( ) ; props . load ( new FileReader ( desc ) ) ; return props ; } catch ( IOException e ) { logger . error ( "Failed to load properties from file: " + desc , e ) ; } }
public void test() { if ( ! this . worldDir . delete ( ) ) { log . warn ( "Unable to delete directory " + this . worldDir ) ; } }
private void verifyContextualFilter ( Filter filter , String expectedPropertyName , String expectedSearchTerm ) { LikeFilterImpl likeFilter = ( LikeFilterImpl ) filter ; AttributeExpressionImpl expression = ( AttributeExpressionImpl ) likeFilter . getExpression ( ) ; LOGGER . debug ( "found property = [{}]" , expectedPropertyName ) ; assertEquals ( expectedPropertyName , expression . getPropertyName ( ) ) ; String extractedSearchTerm = likeFilter . getLiteral ( ) ; LOGGER . debug ( "extractedSearchTerm = [{}]" , extractedSearchTerm ) ; assertEquals ( expectedSearchTerm , extractedSearchTerm ) ; }
private void verifyContextualFilter ( Filter filter , String expectedPropertyName , String expectedSearchTerm ) { LikeFilterImpl likeFilter = ( LikeFilterImpl ) filter ; AttributeExpressionImpl expression = ( AttributeExpressionImpl ) likeFilter . getExpression ( ) ; LOGGER . debug ( "propertyName = {}" , expression . getPropertyName ( ) ) ; LOGGER . debug ( "propertyName = {}" , expression . getPropertyName ( ) ) ; assertEquals ( expectedPropertyName , expression . getPropertyName ( ) ) ; String extractedSearchTerm = likeFilter . getLiteral ( ) ; assertEquals ( expectedSearchTerm , extractedSearchTerm ) ; }
public void test() { try { theClient . update ( ) . resource ( next ) . execute ( ) ; } catch ( BaseServerResponseException e ) { ourLog . error ( "Failed to update server" , e ) ; } }
public void test() { if ( LOGGER . isInfoEnabled ( ) ) { LOGGER . info ( "Success!" ) ; } }
public void test() { try { Map < Integer , String > oldWorkerGroupMap = workerGroupDao . queryAllOldWorkerGroup ( dataSource . getConnection ( ) ) ; Map < Integer , String > processDefinitionJsonMap = processDefinitionDao . queryAllProcessDefinition ( dataSource . getConnection ( ) ) ; code_block = ForStatement ; code_block = IfStatement ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { Ignite ignite = grid == null ? startGrid ( idx ) : grid ; IgniteCache < Object , Object > cache = getCache ( ignite ) ; cache . put ( ignite . cluster ( ) . localNode ( ) . id ( ) , UUID . randomUUID ( ) ) ; code_block = WhileStatement ; } catch ( Exception e ) { log . error ( "Expected error: " + e , e ) ; failed . set ( true ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( signalDocument == null ) { logger . warn ( "Target document doesn't exist or is not a signal" ) ; return ; } }
public void test() { try { Pair < String , CT > pair = ServerClient . getConnection ( context , factory , true ) ; server = pair . getFirst ( ) ; client = pair . getSecond ( ) ; return exec . execute ( client ) ; } catch ( TApplicationException tae ) { log . error ( "Error connecting to server" , tae ) ; throw new AccumuloServerException ( server , tae ) ; } catch ( TTransportException tte ) { sleepUninterruptibly ( 100 , TimeUnit . MILLISECONDS ) ; } finally { if ( client != null ) ServerClient . close ( client ) ; } }
public void test() { try { admin . removeCustomizedStateConfig ( clusterId ) ; } catch ( Exception ex ) { logger . error ( "Failed to remove customizedStateConfig for cluster " + clusterId , ex ) ; return serverError ( ex ) ; } }
public void test() { try { log . debug ( "Validating usecase: {}" , usecase ) ; JsonObject response = new JsonObject ( ) ; response . addProperty ( "UniqueUsecase" , autoMLConfigDAL . isUsecaseExisting ( usecase ) ) ; response . addProperty ( "Usecase" , usecase ) ; return response ; } catch ( Exception e ) { log . error ( "Error occured while validating usecase" , e ) ; throw new InsightsCustomException ( e . getMessage ( ) ) ; } }
public void test() { if ( command . intValue ( ) > 6 ) { addContentItemToPresetContainer ( command . intValue ( ) , currentContentItem ) ; } else { logger . debug ( "Ignoring invalid command: {}" , command . intValue ( ) ) ; } }
private CdiContainer doCreateContainer ( Bundle bundle ) { Set < Bundle > extensions = new HashSet < > ( ) ; findExtensions ( bundle , extensions ) ; LOG . debug ( "Extension {}" , bundle ) ; return factory . createContainer ( bundle , extensions ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Refreshing." ) ; } }
public void test() { try { watcher . registerTriState ( triStateName , new SharedTriStateListener ( ) code_block = "" ; ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "exception" , e ) ; } }
public void test() { if ( commandQueue . size ( ) >= WEB_REQUEST_QUEUE_MAX_SIZE ) { logger . warn ( "Request queue size is greater than the WEB_REQUEST_QUEUE_MAX_SIZE" ) ; } else { logger . warn ( "Could not add command to queue - IllegalStateException" ) ; } }
public void test() { if ( commandQueue . size ( ) >= WEB_REQUEST_QUEUE_MAX_SIZE ) { logger . debug ( "Could not add command to command queue because queue is already full. Maybe SolarEdge is down?" ) ; } else { logger . trace ( "Could not add command to command queue because queue is full." ) ; } }
public void test() { try { log . debug ( "Starting WorkerTaskManager ..." ) ; cleanupAndMakeTmpTaskDir ( ) ; registerLocationListener ( ) ; restoreRestorableTasks ( ) ; initAssignedTasks ( ) ; initCompletedTasks ( ) ; scheduleCompletedTasksCleanup ( ) ; lifecycleLock . started ( ) ; log . debug ( "Started." ) ; } catch ( Exception e ) { log . makeAlert ( e , "Exception starting WorkerTaskManager." ) . emit ( ) ; throw e ; } finally { lifecycleLock . exitStart ( ) ; } }
public void test() { try { log . debug ( "Starting..." ) ; cleanupAndMakeTmpTaskDir ( ) ; registerLocationListener ( ) ; restoreRestorableTasks ( ) ; initAssignedTasks ( ) ; initCompletedTasks ( ) ; scheduleCompletedTasksCleanup ( ) ; lifecycleLock . started ( ) ; log . debug ( "Worker started." ) ; } catch ( Exception e ) { log . makeAlert ( e , "Exception starting WorkerTaskManager." ) . emit ( ) ; throw e ; } finally { lifecycleLock . exitStart ( ) ; } }
private void activateResource ( String resourceId ) throws ResourceException , CorruptStateException { Resource resource = ( Resource ) getResource ( resourceId ) ; checkResourceCanBeStarted ( resource ) ; code_block = IfStatement ; logger . debug ( "  Obtaining capabilities..." ) ; List < ? extends ICapability > oldCapabilities = resource . getCapabilities ( ) ; List < ICapability > capabilities = createCapabilities ( resource ) ; logger . debug ( "  Capabilities obtained. Loading capabilities..." ) ; resource . setCapabilities ( capabilities ) ; logger . debug ( "  Capabilities obtained. Loading bootstrapper..." ) ; IResourceBootstrapper oldBootstrapper = resource . getBootstrapper ( ) ; code_block = IfStatement ; logger . debug ( "  Bootstrapper loaded" ) ; IProfile oldProfile = resource . getProfile ( ) ; code_block = IfStatement ; code_block = TryStatement ;  }
public void test() { try { destroyProtocolSessions ( resourceId ) ; } catch ( Exception e1 ) { logger . error ( "Error destorying protocolSessions" , e1 ) ; } }
private void activateResource ( String resourceId ) throws ResourceException , CorruptStateException { logger . debug ( "Activating resource " + resourceId + " ..." ) ; Resource resource = ( Resource ) getResource ( resourceId ) ; checkResourceCanBeStarted ( resource ) ; code_block = IfStatement ; List < ? extends ICapability > oldCapabilities = resource . getCapabilities ( ) ; logger . debug ( "  Capabilities obtained. Loading capabilities..." ) ; List < ICapability > capabilities = createCapabilities ( resource ) ; resource . setCapabilities ( capabilities ) ; logger . debug ( "  Capabilities obtained. Loading bootstrapper..." ) ; IResourceBootstrapper oldBootstrapper = resource . getBootstrapper ( ) ; code_block = IfStatement ; logger . debug ( "  Bootstrapper loaded" ) ; IProfile oldProfile = resource . getProfile ( ) ; code_block = IfStatement ; code_block = TryStatement ;  }
private void activateResource ( String resourceId ) throws ResourceException , CorruptStateException { logger . debug ( "Activating resource " + resourceId + " ..." ) ; Resource resource = ( Resource ) getResource ( resourceId ) ; checkResourceCanBeStarted ( resource ) ; code_block = IfStatement ; logger . debug ( "  Obtaining capabilities..." ) ; List < ? extends ICapability > oldCapabilities = resource . getCapabilities ( ) ; List < ICapability > capabilities = createCapabilities ( resource ) ; resource . setCapabilities ( capabilities ) ; logger . debug ( "  Capabilities loaded" ) ; IResourceBootstrapper oldBootstrapper = resource . getBootstrapper ( ) ; code_block = IfStatement ; logger . debug ( "  Bootstrapper loaded" ) ; IProfile oldProfile = resource . getProfile ( ) ; code_block = IfStatement ; code_block = TryStatement ;  }
private void activateResource ( String resourceId ) throws ResourceException , CorruptStateException { logger . debug ( "Activating resource " + resourceId + " ..." ) ; Resource resource = ( Resource ) getResource ( resourceId ) ; checkResourceCanBeStarted ( resource ) ; code_block = IfStatement ; logger . debug ( "  Obtaining capabilities..." ) ; List < ? extends ICapability > oldCapabilities = resource . getCapabilities ( ) ; logger . debug ( "  Current capabilities obtained. Loading capabilities..." ) ; List < ICapability > capabilities = createCapabilities ( resource ) ; resource . setCapabilities ( capabilities ) ; logger . debug ( "  Capabilities obtained. Loading bootstrapper..." ) ; IResourceBootstrapper oldBootstrapper = resource . getBootstrapper ( ) ; code_block = IfStatement ; IProfile oldProfile = resource . getProfile ( ) ; code_block = IfStatement ; code_block = TryStatement ;  }
public void test() { try { logger . debug ( "Rolling back" ) ; code_block = TryStatement ;  } catch ( ResourceException re ) { resource . setCapabilities ( oldCapabilities ) ; resource . setBootstrapper ( oldBootstrapper ) ; resource . setProfile ( oldProfile ) ; code_block = IfStatement ; logger . debug ( "Rolling back done" ) ; throw re ; } }
public void test() { try { destroyProtocolSessions ( resourceId ) ; } catch ( Exception e ) { logger . warn ( "Error destorying protocolSessions" , e ) ; } }
public void test() { try { code_block = TryStatement ;  } catch ( ResourceException re ) { logger . debug ( "Rolling back activation..." ) ; resource . setCapabilities ( oldCapabilities ) ; resource . setBootstrapper ( oldBootstrapper ) ; resource . setProfile ( oldProfile ) ; code_block = IfStatement ; logger . error ( "Rolled back activation failed" , re ) ; throw re ; } }
@ Test public void fileImageInputStreamExtImpl ( ) { LOGGER . info ( "Testing capabilities of URLImageInputStreamSpi: SUCCESS!!!" ) ; code_block = TryStatement ;  LOGGER . info ( "Testing capabilities of URLImageInputStreamSpi: SUCCESS!!!" ) ; }
public void test() { if ( LOG . isInfoEnabled ( ) ) { LOG . info ( String . format ( "Call to '%s' on file '%s'" , uri . toString ( ) , file ) ) ; } }
public void test() { try { List < ExecuteBuildingBlock > flowsToExecute = ( List < ExecuteBuildingBlock > ) execution . getVariable ( "flowsToExecute" ) ; String handlingCode = ( String ) execution . getVariable ( HANDLINGCODE ) ; final boolean aLaCarte = ( boolean ) execution . getVariable ( BBConstants . G_ALACARTE ) ; int currentSequence = ( int ) execution . getVariable ( BBConstants . G_CURRENT_SEQUENCE ) ; logger . debug ( "Current Sequence: {}" , currentSequence ) ; ExecuteBuildingBlock ebb = flowsToExecute . get ( currentSequence - 1 ) ; String bbFlowName = ebb . getBuildingBlock ( ) . getBpmnFlowName ( ) ; code_block = IfStatement ; flowManipulatorListenerRunner . postModifyFlows ( flowsToExecute , new DelegateExecutionImpl ( execution ) ) ; } catch ( Exception ex ) { logger . error ( "Failed to post process Execute BB" , ex ) ; workflowAction . buildAndThrowException ( execution , "Failed to post process Execute BB" ) ; } }
@ AfterAll static void cdiContainerDown ( ) { KafkaConnector factory = getInstance ( KafkaConnector . class , KAFKA_CONNECTOR_LITERAL ) . get ( ) ; Collection < KafkaPublisher < ? , ? > > resources = factory . resources ( ) ; assertFalse ( resources . isEmpty ( ) ) ; cdiContainer . close ( ) ; assertTrue ( resources . isEmpty ( ) ) ; LOG . info ( "Kdi container stopped" ) ; }
public void test() { if ( value instanceof String ) { log . debug ( "Found plugins {}" , value . toString ( ) ) ; extensionInfo . plugins . add ( ( String ) value ) ; } else-if ( value instanceof String [ ] ) { log . debug ( "Found plugins {}" , Arrays . toString ( ( String [ ] ) value ) ) ; extensionInfo . plugins . addAll ( Arrays . asList ( ( String [ ] ) value ) ) ; } else { log . debug ( "Found plugin {}" , value . toString ( ) ) ; extensionInfo . plugins . add ( value . toString ( ) ) ; } }
public void test() { if ( value instanceof String ) { log . debug ( "Found plugin {}" , value ) ; extensionInfo . plugins . add ( ( String ) value ) ; } else-if ( value instanceof String [ ] ) { log . debug ( "Found extension {}" , value ) ; extensionInfo . plugins . addAll ( Arrays . asList ( ( String [ ] ) value ) ) ; } else { log . debug ( "Found plugin {}" , value . toString ( ) ) ; extensionInfo . plugins . add ( value . toString ( ) ) ; } }
public void test() { if ( value instanceof String ) { log . debug ( "Found plugin {}" , value ) ; extensionInfo . plugins . add ( ( String ) value ) ; } else-if ( value instanceof String [ ] ) { log . debug ( "Found plugins {}" , Arrays . toString ( ( String [ ] ) value ) ) ; extensionInfo . plugins . addAll ( Arrays . asList ( ( String [ ] ) value ) ) ; } else { log . debug ( "Ignoring plugin {}" , value . getClass ( ) ) ; extensionInfo . plugins . add ( value . toString ( ) ) ; } }
public void test() { try { TreeMapState ps = new TreeMapState ( this , tree , path , bundle ) ; ps . process ( ) ; processNodes . addAndGet ( ps . touched ( ) ) ; } catch ( RuntimeException ex ) { throw ex ; } catch ( Exception ex ) { logger . error ( "" , ex ) ; } }
public void test() { if ( future . isSuccess ( ) ) { LOG . debug ( "Request {} successful after {} ms" , request , durationMillis ) ; stats . reportSuccessfulRequest ( durationMillis ) ; } else { LOG . debug ( "Request {} failed after {} ms" , request , durationMillis , future . cause ( ) ) ; stats . reportFailedRequest ( ) ; } }
public void test() { if ( future . isSuccess ( ) ) { LOG . debug ( "Request {} was successfully answered after {} ms." , request , durationMillis ) ; stats . reportSuccessfulRequest ( durationMillis ) ; } else { LOG . warn ( "Request {} did not respond after {} ms." , request , durationMillis ) ; stats . reportFailedRequest ( ) ; } }
public void test() { if ( ! groupName . equals ( namespaceMeta . getConfig ( ) . getGroupName ( ) ) ) { log . warn ( "Group name '" + name + "' not equal to '" + groupName + "'" ) ; } }
public void test() { if ( ! "rwx" . equals ( permissions ) ) { log . warn ( "Wrong permissions to " + permissions ) ; } }
public void test() { for ( String cache : pools ) { code_block = IfStatement ; } }
public void test() { try { return this . componentManager . getInstance ( DashboardRenderer . class , layout ) ; } catch ( ComponentLookupException e ) { this . logger . error ( "Failed to lookup dashboard renderer" , e ) ; return null ; } }
public void test() { for ( String cache : pools ) { code_block = IfStatement ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { if ( providerEventStates != null ) { recordEventState ( provider , providerEventStates ) ; } else { logger . debug ( "No state found for provider: {}" , provider ) ; } }
public List findByExample ( StgMbMasGef instance ) { log . debug ( "finding StgMbMasGef instance by example" ) ; code_block = TryStatement ;  }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.StgMbMasGef" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.StgMbMasGef" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
@ Test public void testHandleGetRequest ( ) { String jsonResponse = JsonLoader . loadJson ( DomainHelper . getRestUrlV2 ( ) + "/content/emojis" ) ; logger . info ( "jsonResponse: " + jsonResponse ) ; JSONArray emojisJSONArray = new JSONArray ( jsonResponse ) ; logger . info ( "emojisJSONArray.length(): " + emojisJSONArray . length ( ) ) ; assertThat ( emojisJSONArray . length ( ) > 0 , is ( true ) ) ; JSONObject emojiJsonObject = emojisJSONArray . getJSONObject ( 0 ) ; assertThat ( emojiJsonObject . getString ( "glyph" ) , not ( nullValue ( ) ) ) ; }
@ Test public void testHandleGetRequest ( ) { String jsonResponse = JsonLoader . loadJson ( DomainHelper . getRestUrlV2 ( ) + "/content/emojis" ) ; logger . info ( "jsonResponse: " + jsonResponse ) ; JSONArray emojisJSONArray = new JSONArray ( jsonResponse ) ; logger . info ( "jsonResponse: " + emojisJSONArray . length ( ) ) ; assertThat ( emojisJSONArray . length ( ) > 0 , is ( true ) ) ; JSONObject emojiJsonObject = emojisJSONArray . getJSONObject ( 0 ) ; assertThat ( emojiJsonObject . getString ( "glyph" ) , not ( nullValue ( ) ) ) ; }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( ! Objects . equals ( rawPluginPath , transformedPluginPath ) ) { LOGGER . warn ( "The plugin [" + rawPluginPath + "] raw plugin path is deprecated." ) ; } }
public void test() { try { code_block = IfStatement ; if ( fieldNames . isEmpty ( ) ) return ; code_block = IfStatement ; } catch ( IllegalArgumentException e ) { logger . log ( Level . WARNING , e . getMessage ( ) , e ) ; } }
public void test() { try { closeProducer ( producer ) ; closeSession ( session ) ; connection . close ( ) ; } catch ( JMSException e ) { logger . info ( e . getMessage ( ) ) ; } }
public void failedToCapture ( SnapshotStateId componentId ) { updateCapture ( componentId , SnapshotComponentCounter . ComponentState . FAILED ) ; log . debug ( "failed to capture from snapshot %s" , componentId ) ; }
public void test() { if ( mStream == null ) { return ; } }
@ Override public void start ( ) throws Exception { mRunning = true ; mJournalSystem . start ( ) ; startMasters ( false ) ; code_block = IfStatement ; code_block = TryStatement ;  code_block = WhileStatement ; LOG . info ( "Journal system stopped." ) ; }
public void test() { try { mLeaderSelector . start ( getRpcAddress ( ) ) ; } catch ( IOException e ) { LOG . error ( "Failed to start leader Selector." , e ) ; throw new RuntimeException ( e ) ; } }
public void test() { try { File file = P2Cache . getCacheFile ( indexUrl ) ; code_block = TryStatement ;  } catch ( Exception e ) { logger . error ( "" , e ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { try { return new NaturalDateParser ( ) . parse ( string ) . asMap ( ) ; } catch ( NaturalDateParser . DateNotParsableException e ) { logger . error ( e . getMessage ( ) , e ) ; throw new WebApplicationException ( e , 422 ) ; } }
public void test() { if ( user != null ) { sortedUsers . add ( user ) ; } else { log . warn ( "Group with id '" + id + "' not found in UserGroupCache. groupIds string was: " + userIds ) ; } }
@ Override public void onLinkRemoteOpen ( Event event ) { logger . debug ( "onLinkRemoteOpen called" ) ; this . linkStateCallback . onReceiverLinkRemoteOpen ( ) ; }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( sn != UNSET_SUBNETWORK && sn != UNCLEAR_SUBNETWORK ) { LOGGER . error ( "subnetworkId for node " + nodeId + " (" + createPoint ( graph , nodeId ) + ") already set (" + sn + "). " + "Cannot change to " + subnetworkId ) ; failed . set ( true ) ; return false ; } }
public void test() { try { int retVal = remoteTaskAssignmentEJB . insertOneTaskAssignment ( id , userId ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Error processing task" , e ) ; } }
private void handleFlowFileTooBig ( final ProcessSession session , final FlowFile flowFileCandidate , final String message ) { final FlowFile tooBig = session . putAttribute ( flowFileCandidate , message , "record too big " + flowFileCandidate . getSize ( ) + " max allowed " + MAX_MESSAGE_SIZE ) ; getLogger ( ) . error ( "Failed to transfer FlowFile {}" , flowFileCandidate . getName ( ) ) ; session . transfer ( tooBig , REL_FAILURE ) ; }
@ Override byte [ ] execute ( ) throws KeeperException , InterruptedException { LOG . debug ( "ZK Call - getData [{0}] [{1}]" , path , watch ) ; return ZooKeeperClient . super . getData ( path , watch , stat ) ; }
public void test() { if ( leak ) { long leaks = ( created + acquired ) - ( released + discarded ) ; log . warn ( "{}{} ({}) usage (leaks detected: {}) [pooled: {}, created: {}, acquired: {} released: {}, discarded: {}]" , name , id , uri , pooled , created , acquired , released , discarded ) ; } else { log . info ( "{}{} ({}) usage [pooled: {}, created: {}, acquired: {} released: {}, discarded: {}]" , name , id , uri , pooled , created , acquired , released , discarded ) ; } }
public void test() { if ( leak ) { long leaks = ( created + acquired ) - ( released + discarded ) ; log . warn ( "{}{} ({}) usage (leaks detected: {}) [pooled: {}, created: {}, acquired: {} released: {}, discarded: {}]" , name , id , uri , leaks , pooled , created , acquired , released , discarded ) ; } else { log . info ( "{}{} ({}) usage [pooled: {}, created: {}, acquired: {} released: {}, discarded: {}]" , name , id , uri , pooled , created , acquired , released , discarded ) ; } }
public void test() { try { String cusId = FdahpStudyDesignerUtil . isEmpty ( request . getParameter ( "cusId" ) ) ? "" : request . getParameter ( "cusId" ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "StudyController - deleteStudy - ERROR" , e ) ; } }
public void test() { if ( s_logger . isDebugEnabled ( ) ) { s_logger . debug ( log ( seq , "Unable to move " + req . toString ( ) ) ) ; } }
public void test() { if ( profiler . getDurationInMillis ( ) > 1000 ) { code_block = IfStatement ; } else { LOGGER . info ( "Profiler already suspended" ) ; } }
public void test() { if ( s_logger . isDebugEnabled ( ) ) { s_logger . debug ( log ( seq , "Unable to move " + req . toString ( ) ) ) ; } }
public void test() { if ( profiler . getDurationInMillis ( ) > 1000 ) { code_block = IfStatement ; } else { LOGGER . info ( "Profiler already suspended" ) ; } }
public void test() { try { code_block = SwitchStatement ; } catch ( final Throwable e ) { logger . warn ( "Unexpected exception when processing event: {}" , event , e ) ; } }
private Future < Void > invokePermissionsForModule ( Tenant tenant , ModuleDescriptor mdFrom , ModuleDescriptor mdTo , ModuleDescriptor permsModule , ProxyContext pc ) { PermissionList pl ; InterfaceDescriptor permInt = permsModule . getSystemInterface ( "_tenantPermissions" ) ; String permIntVer = permInt . getVersion ( ) ; logger . info ( "invokePermissionsForModule: {}" , permIntVer ) ; code_block = SwitchStatement ; String pljson = Json . encodePrettily ( pl ) ; logger . info ( "tenantPerms Req: {}" , pljson ) ; String permPath = "" ; List < RoutingEntry > routingEntries = permInt . getAllRoutingEntries ( ) ; ModuleInstance permInst = null ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; return proxyService . callSystemInterface ( tenant . getId ( ) , permInst , pljson , pc ) . compose ( cres code_block = LoopStatement ; ) ; }
private Properties readPropertyFile ( String file ) throws IOException { String fileName = file . startsWith ( "/" ) ? file : "/" + file ; LOGGER . info ( "Reading properties from: " + fileName ) ; return Util . readProperties ( fileName ) ; }
public void test() { try { riskIncidence . setId ( IdUtils . createUUID ( ) ) ; riskIncidence . setDetectedOn ( new Date ( ) ) ; String riskIncidenceAsJson = JsonUtils . getJsonFromObject ( riskIncidence ) ; StoragePath riskIncidencePath = ModelUtils . getRiskIncidenceStoragePath ( riskIncidence . getId ( ) ) ; storage . createBinary ( riskIncidencePath , new StringContentPayload ( riskIncidenceAsJson ) , false ) ; } catch ( GenericException | RequestNotValidException | AuthorizationDeniedException | NotFoundException | AlreadyExistsException e ) { LOGGER . error ( "Error creating risk in storage" , e ) ; } }
public void test() { if ( records == null || records . isEmpty ( ) ) { log . warn ( "Got no records from given xml, but no record was found - returning first record." ) ; } else-if ( records . size ( ) == 1 ) { result = records . get ( 0 ) ; } else-if ( unitQualifier == null ) { log . warn ( "Got multiple records from given xml, but no unitQualifier set - returning first record as a guess." ) ; result = records . get ( 0 ) ; } else { code_block = ForStatement ; code_block = IfStatement ; } }
public void test() { if ( result == null ) { LOGGER . error ( "Unable to retrieve metadata from " + source ) ; } }
public void test() { try { String inputPath = "../../data/census/census_148d_test.dummy" ; String loadPath = LOCAL_FS + TMP_PATH + "/model/DeepFM" ; String predictPath = LOCAL_FS + TMP_PATH + "/predict" ; conf . set ( AngelConf . ANGEL_PREDICT_DATA_PATH , inputPath ) ; conf . set ( AngelConf . ANGEL_LOAD_MODEL_PATH , loadPath ) ; conf . set ( AngelConf . ANGEL_PREDICT_PATH , predictPath ) ; conf . set ( AngelConf . ANGEL_ACTION_TYPE , MLConf . ANGEL_ML_PREDICT ( ) ) ; GraphRunner runner = new GraphRunner ( ) ; runner . predict ( conf ) ; } catch ( Exception x ) { LOG . error ( "run predictTest failed " , x ) ; throw x ; } }
public void test() { try { redirect ( getUserPageUrl ( ) ) ; } catch ( IOException e ) { logger . error ( "Exception during to redirect." , e ) ; } }
public void test() { if ( e < EPS_MIN ) { String msg = String . format ( "epsilon:%.2g less than esp_min=%.2g" , eps , EPS_MIN ) ; logger . warn ( msg ) ; e = max ( eps , EPS_MIN ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public String pom ( ) { String requestUrl = this . base + "/pom.xml" ; log . info ( "Requesting {}" , requestUrl ) ; return this . restTemplate . exchange ( RequestEntity . get ( URI . create ( requestUrl ) ) . accept ( mediaTypes ( ) ) . build ( ) , String . class ) . getBody ( ) ; }
public DeviceConnection connect ( final DeviceConnectionParameters deviceConnectionParameters , final String organisationIdentification , final boolean cacheConnection ) throws ConnectionFailureException { final String deviceIdentification = deviceConnectionParameters . getDeviceIdentification ( ) ; final String serverName = deviceConnectionParameters . getServerName ( ) ; final IED ied = deviceConnectionParameters . getIed ( ) ; LOGGER . info ( "Connecting to device: {}" , deviceIdentification ) ; code_block = TryStatement ;  final InetAddress inetAddress = this . convertIpAddress ( deviceConnectionParameters . getIpAddress ( ) ) ; final DateTime startTime = DateTime . now ( ) ; Iec61850ClientBaseEventListener eventListener = null ; code_block = TryStatement ;  final Iec61850Device iec61850Device = this . iec61850DeviceRepository . findByDeviceIdentification ( deviceIdentification ) ; final int port = this . determinePortForIec61850Device ( ied , iec61850Device ) ; final Iec61850ClientAssociation iec61850ClientAssociation = this . iec61850Client . connect ( deviceIdentification , inetAddress , eventListener , port ) ; final ClientAssociation clientAssociation = iec61850ClientAssociation . getClientAssociation ( ) ; clientAssociation . setResponseTimeout ( this . responseTimeout ) ; ServerModel serverModel ; code_block = TryStatement ;  final Iec61850Connection iec61850Connection = new Iec61850Connection ( iec61850ClientAssociation , serverModel , startTime , ied ) ; code_block = IfStatement ; final DeviceConnection connection = new DeviceConnection ( iec61850Connection , deviceIdentification , organisationIdentification , serverName ) ; final DateTime endTime = DateTime . now ( ) ; LOGGER . info ( "Connected to device: {}, fetched server model. Start time: {}, end time: {}, total time in milliseconds: {}" , deviceIdentification , startTime , endTime , endTime . minus ( startTime . getMillis ( ) ) . getMillis ( ) ) ; this . iec61850RtuDeviceReportingService . enableReportingForDevice ( connection , deviceIdentification , serverName ) ; return connection ; }
public DeviceConnection connect ( final DeviceConnectionParameters deviceConnectionParameters , final String organisationIdentification , final boolean cacheConnection ) throws ConnectionFailureException { final String deviceIdentification = deviceConnectionParameters . getDeviceIdentification ( ) ; final String serverName = deviceConnectionParameters . getServerName ( ) ; final IED ied = deviceConnectionParameters . getIed ( ) ; code_block = TryStatement ;  final InetAddress inetAddress = this . convertIpAddress ( deviceConnectionParameters . getIpAddress ( ) ) ; LOGGER . info ( "Trying to connect to deviceIdentification: {} at IP address {} using response time-out: {}" , deviceIdentification , deviceConnectionParameters . getIpAddress ( ) , this . responseTimeout ) ; final DateTime startTime = DateTime . now ( ) ; Iec61850ClientBaseEventListener eventListener = null ; code_block = TryStatement ;  final Iec61850Device iec61850Device = this . iec61850DeviceRepository . findByDeviceIdentification ( deviceIdentification ) ; final int port = this . determinePortForIec61850Device ( ied , iec61850Device ) ; final Iec61850ClientAssociation iec61850ClientAssociation = this . iec61850Client . connect ( deviceIdentification , inetAddress , eventListener , port ) ; final ClientAssociation clientAssociation = iec61850ClientAssociation . getClientAssociation ( ) ; clientAssociation . setResponseTimeout ( this . responseTimeout ) ; ServerModel serverModel ; code_block = TryStatement ;  final Iec61850Connection iec61850Connection = new Iec61850Connection ( iec61850ClientAssociation , serverModel , startTime , ied ) ; code_block = IfStatement ; final DeviceConnection connection = new DeviceConnection ( iec61850Connection , deviceIdentification , organisationIdentification , serverName ) ; final DateTime endTime = DateTime . now ( ) ; LOGGER . info ( "Connecting to device: {}" , deviceIdentification ) ; this . iec61850RtuDeviceReportingService . enableReportingForDevice ( connection , deviceIdentification , serverName ) ; return connection ; }
public void test() { try { scanner . stop ( ) ; code_block = IfStatement ; } catch ( Exception e ) { LOG . warn ( "Error restarting webapp" , e ) ; } }
public void test() { try { final Patient patient = this . repository . get ( patientId ) ; code_block = IfStatement ; final JSONObject genePanel = this . genePanelFactory . withMatchCount ( withMatchCount ) . build ( patient , excludeRejectedGenes ) . toJSON ( ) ; return Response . ok ( genePanel , MediaType . APPLICATION_JSON_TYPE ) . build ( ) ; } catch ( final SecurityException ex ) { this . logger . debug ( "Could not retrieve gene parameters" , ex ) ; return Response . status ( Response . Status . UNAUTHORIZED ) . build ( ) ; } }
public void test() { if ( ! eachRowArray . get ( 0 ) . isJsonNull ( ) ) { String contentText = eachRowArray . get ( 0 ) . getAsString ( ) ; String contentId = eachRowArray . get ( 1 ) . getAsString ( ) ; code_block = IfStatement ; observationList . add ( contentText ) ; } else { LOGGER . error ( "contentId is null" ) ; } }
@ Override public ListenableFuture < ? extends DOMRpcResult > create ( final LogicalDatastoreType store , final YangInstanceIdentifier path , final NormalizedNode < ? , ? > data , final Optional < ModifyAction > defaultOperation ) { LOG . debug ( "{}: Create Config {} {}" , id , store , path ) ; masterActor . tell ( new CreateEditConfigRequest ( store , new NormalizedNodeMessage ( path , data ) , defaultOperation . orElse ( null ) ) , ActorRef . noSender ( ) ) ; return createResult ( ) ; }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "JDBC::{}" , sql ) ; } }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "JDBC::{}" , sql ) ; } }
public void test() { try { code_block = TryStatement ;  code_block = WhileStatement ; } catch ( IOException e ) { logger . error ( e . getMessage ( ) , e ) ; } finally { } }
public void test() { try { obj . put ( JsonKeys . updateType . name ( ) , "PropertyList" ) ; code_block = ForStatement ; obj . put ( JsonKeys . properties . name ( ) , resultArray ) ; pw . println ( obj . toString ( ) ) ; } catch ( Exception e ) { e . printStackTrace ( ) ; logger . error ( "Error in generating JSON List" ) ; } }
public void test() { try { properties . load ( inStream ) ; return properties ; } catch ( IOException e ) { log . error ( "test-options.properties could not be found" , e ) ; throw new IllegalAccessError ( "test-options.properties could not be found" ) ; } }
@ Test public void test_04_Intron ( ) { Log . debug ( "Test" ) ; if ( verbose ) Log . debug ( transcript ) ; Variant variant = new Variant ( chromosome , 920 , "" , "C" , "" ) ; if ( verbose ) Log . debug ( "Variant: " + variant ) ; if ( verbose ) Log . debug ( "Variant (before): " + variant ) ; Variant variantShifted = variant . realignLeft ( ) ; if ( verbose ) Log . debug ( "Variant (after): " + variantShifted ) ; Assert . assertFalse ( variant == variantShifted ) ; Assert . assertEquals ( 925 , variantShifted . getStart ( ) ) ; }
public void test() { try { mailService . sendApiFeatureStateEmail ( apiFeature , stateValue , email , createStateMailMessage ( ( TenantApiUsageState ) state , apiFeature , stateValue ) ) ; } catch ( ThingsboardException e ) { log . error ( "Failed to send api feature state: {}" , e . getMessage ( ) ) ; } }
public void test() { if ( StringUtils . isNotEmpty ( email ) ) { result . forEach ( ( apiFeature , stateValue ) code_block = LoopStatement ; ) ; } else { logger . debug ( "No email provided" ) ; } }
@ Override protected void doSync ( ) throws IOException { LOG . debug ( "writing hsync" ) ; this . writer . hsync ( ) ; }
public void test() { if ( count % 100 == 0 ) { log . info ( "Processed {} lines" , count ) ; } }
@ Override @ SuppressWarnings ( "unchecked" ) public void put ( Object tuple ) { LOG . trace ( "put into tuple {}" , tuple ) ; inputPort . process ( ( T ) tuple ) ; }
public void test() { try { commitBlock ( sessionId , blockId , false ) ; } catch ( BlockDoesNotExistException e ) { LOG . debug ( "Block does not exist in block {}" , blockId ) ; } catch ( InvalidWorkerStateException e ) { LOG . debug ( "Invalid worker state while committing block." , e ) ; } }
public void test() { try { commitBlock ( sessionId , blockId , false ) ; } catch ( BlockDoesNotExistException e ) { LOG . warn ( "Block {} does not exist while being committed." , blockId ) ; } catch ( InvalidWorkerStateException e ) { LOG . warn ( "Invalid worker state: {}" , workerId ) ; } }
public void test() { try { Compute computeService = createComputeService ( ) ; Compute . Instances . List request = computeService . instances ( ) . list ( project , zone ) ; List < String > instanceIds = new ArrayList < > ( ) ; InstanceList response ; do code_block = "" ; while ( response . getNextPageToken ( ) != null ) ; log . debug ( "Converted to [%s]" , String . join ( "," , instanceIds ) ) ; return instanceIds ; } catch ( Exception e ) { log . warn ( e . getMessage ( ) , e ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { try { doMergeFragments ( fragmentsToMerge ) ; } catch ( Exception e ) { log . warn ( e , "Unable to merge fragments" ) ; } }
public void test() { if ( waitForOperation ( context . getGceApi ( ) . operations ( ) , operation ) == 1 ) { logger . info ( "Operation {}  completed on {}" , operation . operationType ( ) , operation . targetLink ( ) ) ; } else { logger . info ( String . format ( "Operation %s  was successfully done on %s\n." , operation . operationType ( ) , operation . targetLink ( ) ) ) ; } }
public void test() { if ( waitForOperation ( context . getGceApi ( ) . operations ( ) , operation ) == 1 ) { logger . warn ( String . format ( "%s operation has timedout: %s\n" , operation . operationType ( ) , operation . httpErrorMessage ( ) ) ) ; } else { logger . error ( String . format ( "%s operation failed" , operation . operationType ( ) ) ) ; } }
public void test() { try { return property . get ( this ) ; } catch ( IllegalAccessException e ) { log . debug ( "Cannot access property " + property . getName ( ) , e ) ; } }
@ Test public void importFetched ( ) throws Exception { Repository repository = createTestRepoUsingRepoService ( ) ; getL10nJCommander ( ) . run ( "push" , "-r" , repository . getName ( ) , "-s" , getInputResourcesTestDir ( "source" ) . getAbsolutePath ( ) ) ; Asset asset = assetClient . getAssetByPathAndRepositoryId ( "source-xliff.xliff" , repository . getId ( ) ) ; importTranslations ( asset . getId ( ) , "source-xliff_" , "fr-FR" ) ; importTranslations ( asset . getId ( ) , "source-xliff_" , "ja-JP" ) ; Asset asset2 = assetClient . getAssetByPathAndRepositoryId ( "source2-xliff.xliff" , repository . getId ( ) ) ; importTranslations ( asset2 . getId ( ) , "source2-xliff_" , "fr-FR" ) ; importTranslations ( asset2 . getId ( ) , "source2-xliff_" , "ja-JP" ) ; waitForRepositoryToHaveStringsForTranslations ( repository . getId ( ) ) ; getL10nJCommander ( ) . run ( "drop-export" , "-r" , repository . getName ( ) ) ; final Long dropId = getLastDropIdFromOutput ( outputCapture ) ; Console mockConsole = mock ( Console . class ) ; verify ( mockConsole , never ( ) ) . readLine ( Long . class ) ; L10nJCommander l10nJCommander = getL10nJCommander ( ) ; DropImportCommand dropImportCommand = l10nJCommander . getCommand ( DropImportCommand . class ) ; dropImportCommand . console = mockConsole ; int numberOfFrenchTranslationsBefore = getNumberOfFrenchTranslations ( repository ) ; localizeDropFiles ( dropRepository . findById ( dropId ) . orElse ( null ) ) ; l10nJCommander . run ( new String [ ] code_block = "" ; ) ; int numberOfFrenchTranslationsAfter = getNumberOfFrenchTranslations ( repository ) ; logger . info ( "importFetched" ) ; assertEquals ( "
public void test() { if ( offset > validBytes ) { logger . debug ( "Packet packet isCorrupt" ) ; packet . setIsCorrupt ( true ) ; } }
public void test() { try { complete ( buildExecutionSession , runningEnvironment , onComplete ) ; } catch ( InterruptedException e ) { LOG . error ( "Build execution interrupted!" , e ) ; } }
public void test() { if ( LOGGER . isInfoEnabled ( ) ) { LOGGER . info ( "Success!" ) ; } }
public void test() { try { ExternalResponse result = remoteLogics . exec ( AuthenticationToken . ANONYMOUS , MainController . getSessionInfo ( ) , "Authentication.syncUsers[ISTRING[100], JSONFILE]" , new ExternalRequest ( new Object [ ] code_block = "" ; ) ) ; JSONArray unlockedUsers = new JSONArray ( new String ( ( FileData ) result . results [ 0 ] ) . getRawFile ( ) . getBytes ( ) , StandardCharsets . UTF_8 ) ) ; List < Object > currentUsers = unlockedUsers . toList ( ) ; List < UserInfo > newUserInfos = new ArrayList < > ( ) ; code_block = ForStatement ; code_block = ForStatement ; userInfos . set ( newUserInfos ) ; } catch ( RemoteException e ) { Log . fatal ( "RemoteException" , e ) ; } }
public String enqueueGetConfigurationRequest ( @ Identification final String organisationIdentification , @ Identification final String deviceIdentification , final int messagePriority ) throws FunctionalException { final Organisation organisation = this . domainHelperService . findOrganisation ( organisationIdentification ) ; final Device device = this . domainHelperService . findActiveDevice ( deviceIdentification ) ; this . domainHelperService . isAllowed ( organisation , device , DeviceFunction . GET_CONFIGURATION ) ; this . domainHelperService . isInMaintenance ( device ) ; LOGGER . debug ( "enqueueGetConfigurationRequest called with organisation {} and device {}" , organisationIdentification , deviceIdentification ) ; final String correlationUid = this . correlationIdProviderService . getCorrelationId ( organisationIdentification , deviceIdentification ) ; final DeviceMessageMetadata deviceMessageMetadata = new DeviceMessageMetadata ( deviceIdentification , organisationIdentification , correlationUid , MessageType . GET_CONFIGURATION . name ( ) , messagePriority ) ; final CommonRequestMessage message = new CommonRequestMessage . Builder ( ) . deviceMessageMetadata ( deviceMessageMetadata ) . build ( ) ; this . commonRequestMessageSender . send ( message ) ; return correlationUid ; }
public void test() { try { MethodKey methodKey = new MethodKey ( JournalArticleServiceUtil . class , "addArticleDefaultValues" , _addArticleDefaultValuesParameterTypes4 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , classNameId , classPK , titleMap , descriptionMap , content , ddmStructureKey , ddmTemplateKey , layoutUuid , indexable , smallImage , smallImageURL , smallImageFile , serviceContext ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . journal . model . JournalArticle ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( ! channelFuture . isSuccess ( ) ) { logger . debug ( "Failed to send a 413 Request Entity Too Large." , channelFuture . cause ( ) ) ; return false ; } }
@ Override public boolean validateObject ( ChannelFuture channelFuture ) { code_block = IfStatement ; code_block = IfStatement ; Channel channel = channelFuture . channel ( ) ; boolean answer = channel . isActive ( ) ; logger . trace ( "validateObject: {} -> {}" , channel , answer ) ; return answer ; }
public void test() { try { clientProxy . subscribe ( serviceInfo . getName ( ) , serviceInfo . getGroupName ( ) , serviceInfo . getClusters ( ) ) ; } catch ( NacosException e ) { LogUtil . NAMING_LOGGER . error ( e . toString ( ) , e ) ; } }
@ Override @ SuppressWarnings ( "unchecked" ) public List < ProjectSampleAnalysisOutputInfo > getAllAnalysisOutputInfoSharedWithProject ( Long projectId , Set < UUID > workflowIds ) { final String query = "SELECT\n" + "  s.id AS sampleId,\n" + "  s.sampleName AS sampleName,\n" + "  a.id AS analysisId,\n" + "  aofmap.analysis_output_file_key AS analysisOutputFileKey,\n" + "  aof.file_path AS filePath,\n" + "  aof.id AS analysisOutputFileId,\n" + "  a.analysis_type AS analysisType,\n" + "  asub.workflow_id AS workflowId,\n" + "  aof.created_date AS createdDate,\n" + "  asub.name AS analysisSubmissionName,\n" + "  asub.id AS analysisSubmissionId,\n" + "  u.id AS userId,\n" + "  u.firstName AS userFirstName,\n" + "  u.lastName AS userLastName\n" + "FROM analysis_output_file aof\n" + "  INNER JOIN analysis_output_file_map aofmap ON aof.id = aofmap.analysisOutputFilesMap_id\n" + "  INNER JOIN analysis a ON aofmap.analysis_id = a.id\n" + "  INNER JOIN analysis_submission asub ON a.id = asub.analysis_id\n" + "  INNER JOIN analysis_submission_sequencing_object o ON asub.id = o.analysis_submission_id\n" + "  INNER JOIN sample_sequencingobject sso ON sso.sequencingobject_id = o.sequencing_object_id\n" + "  INNER JOIN sample s ON sso.sample_id = s.id\n" + "  INNER JOIN project_sample psample ON s.id = psample.sample_id\n" + "  INNER JOIN user u ON asub.submitter" +
public void test() { if ( bSession == null ) { log . error ( "Base Session is null for sessionId: {}" , sessionId ) ; return ; } else { code_block = TryStatement ;  } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Failed to clean up." , e ) ; } }
public void test() { try { DiameterTimerTaskData data = ( DiameterTimerTaskData ) getData ( ) ; BaseSession bSession = sessionDataSource . getSession ( data . getSessionId ( ) ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Failed to retrieve timer task." , e ) ; } }
public void test() { if ( ! schema . exists ( ) ) { code_block = IfStatement ; schema . create ( ) ; createdSchemas . add ( schema ) ; LOG . debug ( "Created schema: " + schema ) ; } else { LOG . debug ( "Skipping creation of existing schema: " + schema ) ; } }
public void test() { if ( ! schema . exists ( ) ) { code_block = IfStatement ; LOG . debug ( "Creating schema: " + schema ) ; schema . create ( ) ; createdSchemas . add ( schema ) ; } else { LOG . debug ( "Schema already exists: " + schema ) ; } }
public void test() { try { Thread . sleep ( 1000 ) ; } catch ( InterruptedException e1 ) { logger . error ( "Interrupted while looping." , e1 ) ; } }
public void test() { if ( commitNeeded ) { log . debug ( "Flushing {} task" , this ) ; stateMgr . flushCache ( ) ; recordCollector . flush ( ) ; hasPendingTxCommit = eosEnabled ; return committableOffsetsAndMetadata ( ) ; } else { log . debug ( "Skipped preparing {} task for commit since there is nothing to commit" , state ( ) ) ; return Collections . emptyMap ( ) ; } }
public void test() { if ( commitNeeded ) { stateMgr . flushCache ( ) ; recordCollector . flush ( ) ; hasPendingTxCommit = eosEnabled ; log . debug ( "Prepared {} task for committing" , state ( ) ) ; return committableOffsetsAndMetadata ( ) ; } else { log . debug ( "No commit needed to prepare, skipping commit" ) ; return Collections . emptyMap ( ) ; } }
private void getExtraUnmanagedStorageVolumes ( ) { ResourceCollection < ExtraStorageVolume > extraStorageVolumes = this . storageVolumeAttachmentClient . getExtraUnmanagedStorageVolumes ( ) ; LOGGER . info ( "Extra StorageVolumes returned to client : " + extraStorageVolumes . toJsonString ( ) ) ; }
public void test() { try { ResourceInterface resource = loadResource ( resourceId ) ; BaseResourceDataBean resourceFile = new BaseResourceDataBean ( ) ; resourceFile . setResourceType ( resource . getType ( ) ) ; resourceFile . setResourceId ( resourceId ) ; resourceFile . setMetadata ( resource . getMetadata ( ) ) ; resourceFile . setOwner ( resource . getOwner ( ) ) ; resourceFile . setFolderPath ( resource . getFolderPath ( ) ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; resourceFile . setMainGroup ( resource . getMainGroup ( ) ) ; resourceFile . setCategories ( convertCategories ( categories ) ) ; resourceManager . updateResource ( resourceFile ) ; return convertResourceToDto ( resourceManager . loadResource ( resourceId ) ) ; } catch ( ApsSystemException e ) { logger . error ( "Error loading resources" , e ) ; throw new RestServerError ( "plugins.jacms.resources.resourceManager.error.persistence" , e ) ; } catch ( IOException e ) { throw new RestServerError ( "plugins.jacms.resources.image.errorReadingStream" , e ) ; } }
public void test() { try { tunnelClient . connect ( true ) ; } catch ( Throwable e ) { LOG . error ( "Trying to connect to tunnel server" , e ) ; } }
public void test() { try ( AutoLock l = lock . lockForWrite ( ) ) { AclRecord record = ( ( MutableAclRecord ) mutableAcl ) . getAclRecord ( ) ; crud . save ( record ) ; } catch ( IOException e ) { logger . error ( e . getLocalizedMessage ( ) , e ) ; throw new InternalErrorException ( e ) ; } }
@ Override void refreshConfiguration ( ResourceManager configuration ) { super . refreshConfiguration ( configuration ) ; int newMaxUnits = getIntegerParameter ( PRM_QUANTITY ) ; int delta = newMaxUnits - previousMaxUnits ; previousMaxUnits = newMaxUnits ; logger . debug ( "Updating {} to {}" , delta , newMaxUnits ) ; this . availableUnits . addAndGet ( delta ) ; this . defaultConsumption = getIntegerParameter ( PRM_CONSUMPTION ) ; }
@ Override protected ModelAndView onSubmit ( HttpServletRequest request , HttpServletResponse response , Object command , BindException errors ) throws Exception { AddFileMetadataCommand addFileMetadataCommand = ( AddFileMetadataCommand ) command ; log . debug ( "FileId = " + fileId ) ; int fileId = Integer . parseInt ( request . getParameter ( "fileId" ) ) ; ModelAndView mav = new ModelAndView ( "redirect:/experiments/data/detail.html?fileId=" + fileId ) ; code_block = IfStatement ; log . debug ( "Creating new FileMetadata object" ) ; FileMetadataParamVal metadata = new FileMetadataParamVal ( ) ; metadata . setId ( new FileMetadataParamValId ( addFileMetadataCommand . getParamId ( ) , fileId ) ) ; log . debug ( "Setting the metadata value = " + addFileMetadataCommand . getParamValue ( ) ) ; metadata . setMetadataValue ( addFileMetadataCommand . getParamValue ( ) ) ; log . debug ( "Saving new file metadata entry" ) ; fileMetadataParamValDao . create ( metadata ) ; log . debug ( "Returning MAV" ) ; return mav ; }
public void test() { if ( ! auth . userIsOwnerOrCoexpOfCorrespExperiment ( fileId ) ) { logger . debug ( "Return mav [" + fileId + "]" ) ; return mav ; } }
@ Override protected ModelAndView onSubmit ( HttpServletRequest request , HttpServletResponse response , Object command , BindException errors ) throws Exception { AddFileMetadataCommand addFileMetadataCommand = ( AddFileMetadataCommand ) command ; int fileId = Integer . parseInt ( request . getParameter ( "fileId" ) ) ; ModelAndView mav = new ModelAndView ( "redirect:/experiments/data/detail.html?fileId=" + fileId ) ; log . debug ( "Checking the permission." ) ; code_block = IfStatement ; FileMetadataParamVal metadata = new FileMetadataParamVal ( ) ; log . debug ( "Setting the metadata id = " + metadata . getId ( ) ) ; metadata . setId ( new FileMetadataParamValId ( addFileMetadataCommand . getParamId ( ) , fileId ) ) ; log . debug ( "Setting the metadata value = " + addFileMetadataCommand . getParamValue ( ) ) ; metadata . setMetadataValue ( addFileMetadataCommand . getParamValue ( ) ) ; log . debug ( "Saving new file metadata entry" ) ; fileMetadataParamValDao . create ( metadata ) ; log . debug ( "Returning MAV" ) ; return mav ; }
@ Override protected ModelAndView onSubmit ( HttpServletRequest request , HttpServletResponse response , Object command , BindException errors ) throws Exception { AddFileMetadataCommand addFileMetadataCommand = ( AddFileMetadataCommand ) command ; log . debug ( "Processing file id" ) ; int fileId = Integer . parseInt ( request . getParameter ( "fileId" ) ) ; ModelAndView mav = new ModelAndView ( "redirect:/experiments/data/detail.html?fileId=" + fileId ) ; log . debug ( "Checking the permission." ) ; code_block = IfStatement ; log . debug ( "Creating new FileMetadata object" ) ; FileMetadataParamVal metadata = new FileMetadataParamVal ( ) ; metadata . setId ( new FileMetadataParamValId ( addFileMetadataCommand . getParamId ( ) , fileId ) ) ; metadata . setMetadataValue ( addFileMetadataCommand . getParamValue ( ) ) ; log . debug ( "Saving new file metadata entry" ) ; fileMetadataParamValDao . create ( metadata ) ; log . debug ( "Returning MAV" ) ; return mav ; }
@ Override protected ModelAndView onSubmit ( HttpServletRequest request , HttpServletResponse response , Object command , BindException errors ) throws Exception { AddFileMetadataCommand addFileMetadataCommand = ( AddFileMetadataCommand ) command ; log . debug ( "FileId = " + fileId ) ; int fileId = Integer . parseInt ( request . getParameter ( "fileId" ) ) ; ModelAndView mav = new ModelAndView ( "redirect:/experiments/data/detail.html?fileId=" + fileId ) ; log . debug ( "Checking the permission." ) ; code_block = IfStatement ; log . debug ( "Creating new FileMetadata object" ) ; FileMetadataParamVal metadata = new FileMetadataParamVal ( ) ; metadata . setId ( new FileMetadataParamValId ( addFileMetadataCommand . getParamId ( ) , fileId ) ) ; log . debug ( "Setting the metadata value = " + addFileMetadataCommand . getParamValue ( ) ) ; metadata . setMetadataValue ( addFileMetadataCommand . getParamValue ( ) ) ; fileMetadataParamValDao . create ( metadata ) ; log . debug ( "Returning MAV" ) ; return mav ; }
@ Override protected ModelAndView onSubmit ( HttpServletRequest request , HttpServletResponse response , Object command , BindException errors ) throws Exception { AddFileMetadataCommand addFileMetadataCommand = ( AddFileMetadataCommand ) command ; int fileId = Integer . parseInt ( request . getParameter ( "fileId" ) ) ; ModelAndView mav = new ModelAndView ( "redirect:/experiments/data/detail.html?fileId=" + fileId ) ; log . debug ( "Checking the permission." ) ; code_block = IfStatement ; log . debug ( "Creating new FileMetadata object" ) ; FileMetadataParamVal metadata = new FileMetadataParamVal ( ) ; metadata . setId ( new FileMetadataParamValId ( addFileMetadataCommand . getParamId ( ) , fileId ) ) ; log . debug ( "Setting the metadata value = " + addFileMetadataCommand . getParamValue ( ) ) ; metadata . setMetadataValue ( addFileMetadataCommand . getParamValue ( ) ) ; log . debug ( "Saving new file metadata entry" ) ; fileMetadataParamValDao . create ( metadata ) ; log . debug ( "Returning new file metadata entry" ) ; return mav ; }
public void test() { if ( s_logger . isDebugEnabled ( ) ) { s_logger . debug ( log ( seq , "Unable to move " + req . toString ( ) ) ) ; } }
@ Secured ( ServicesData . ROLE_CREATE_CONTEXTS ) @ ResponseStatus ( HttpStatus . CREATED ) @ PostMapping public ContextDto create ( final @ Valid @ RequestBody ContextDto contextDto ) { LOGGER . debug ( "Create {}" , contextDto ) ; return contextExternalService . create ( contextDto ) ; }
private void commitCheckpointMark ( KafkaCheckpointMark checkpointMark ) { LOG . debug ( "Committing checkpoint marker from {} to {}" , checkpointMark . getPartitions ( ) . stream ( ) . filter ( p -> p . getNextOffset ( ) != UNINITIALIZED_OFFSET ) . collect ( Collectors . toMap ( p -> new TopicPartition ( p . getTopic ( ) , p . getPartition ( ) ) , p -> new OffsetAndMetadata ( p . getNextOffset ( ) ) ) ) ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( ! warningsLogged [ 0 ] ) { warningsLogged [ 0 ] = true ; getLogger ( ) . warning ( message ) ; } }
public void test() { if ( ! warningsLogged [ 1 ] ) { warningsLogged [ 1 ] = true ; getLogger ( ) . warn ( message ) ; } }
public void test() { if ( ! warningsLogged [ 2 ] ) { warningsLogged [ 2 ] = true ; getLogger ( ) . warn ( message ) ; } }
@ Test public void testPOSTObjectsWithLegacyEntitiesAndInbuiltSchemaAsOutputSchema ( ) throws Exception { objectJSONString = DMPPersistenceUtil . getResourceAsString ( "old.fotothek.project.json" ) ; final Response response = target ( "/robust" ) . request ( MediaType . APPLICATION_JSON_TYPE ) . accept ( MediaType . APPLICATION_JSON_TYPE ) . post ( Entity . json ( objectJSONString ) ) ; Assert . assertEquals ( "201 Created was expected" , 201 , response . getStatus ( ) ) ; final String responseString = response . readEntity ( String . class ) ; Assert . assertNotNull ( "the response JSON shouldn't be null" , responseString ) ; final Project actualObject = objectMapper . readValue ( responseString , pojoClass ) ; Assert . assertNotNull ( "the response project shouldn't be null" , actualObject ) ; LegacyProjectResourceTest . LOG . debug ( "end POST {}s with legacy entities test" , pojoClassName ) ; }
@ Test public void testPOSTObjectsWithLegacyEntitiesAndInbuiltSchemaAsOutputSchema ( ) throws Exception { LegacyProjectResourceTest . LOG . debug ( "start POST {}s with legacy entities test" , pojoClassName ) ; objectJSONString = DMPPersistenceUtil . getResourceAsString ( "old.fotothek.project.json" ) ; final Response response = target ( "/robust" ) . request ( MediaType . APPLICATION_JSON_TYPE ) . accept ( MediaType . APPLICATION_JSON_TYPE ) . post ( Entity . json ( objectJSONString ) ) ; Assert . assertEquals ( "201 Created was expected" , 201 , response . getStatus ( ) ) ; final String responseString = response . readEntity ( String . class ) ; Assert . assertNotNull ( "the response JSON shouldn't be null" , responseString ) ; final Project actualObject = objectMapper . readValue ( responseString , pojoClass ) ; Assert . assertNotNull ( "the response project shouldn't be null" , actualObject ) ; LegacyProjectResourceTest . LOG . debug ( "end POST {}s with legacy entities test" , pojoClassName ) ; }
public void test() { try { code_block = IfStatement ; response . setContentType ( "application/json" ) ; response . getWriter ( ) . print ( jsonResponse . toString ( ) ) ; } catch ( JSONException e ) { LOG . warn ( "JSON exception when getting Country List." , e ) ; response . setContentType ( "text/html" ) ; response . getWriter ( ) . print ( e . getMessage ( ) ) ; } catch ( CerberusException ex ) { LOG . warn ( "JSON exception when getting Country List." , ex ) ; } }
public void test() { try { code_block = IfStatement ; response . setContentType ( "application/json" ) ; response . getWriter ( ) . print ( jsonResponse . toString ( ) ) ; } catch ( JSONException e ) { LOG . warn ( e ) ; response . setContentType ( "text/html" ) ; response . getWriter ( ) . print ( e . getMessage ( ) ) ; } catch ( CerberusException ex ) { LOG . error ( ex ) ; } }
public void test() { if ( ( task . getState ( ) == JobTaskState . ALLOCATED ) || task . getState ( ) . isQueuedState ( ) ) { LOG . info ( "Got task {}" , task . getId ( ) ) ; } }
public void test() { if ( task != null ) { task . setPreFailErrorCode ( 0 ) ; HostState host = hostManager . getHostState ( task . getHostUUID ( ) ) ; code_block = IfStatement ; log . warn ( "[task.revert] sending revert message to host: {}/{}" , host . getHost ( ) , host . getHostUuid ( ) ) ; spawnMQ . sendControlMessage ( new CommandTaskRevert ( host . getHostUuid ( ) , jobUUID , task . getTaskID ( ) , backupType , rev , time , getTaskReplicaTargets ( task . getAllReplicas ( ) ) , false ) ) ; } else { log . warn ( "[task.revert] task not found to host: {}" , jobUUID ) ; } }
public void start ( ModuleDefineHolder moduleDefineHolder , int ttl ) { log . info ( "Starting cache update..." ) ; final long timeInterval = 10 ; Executors . newSingleThreadScheduledExecutor ( ) . scheduleAtFixedRate ( new RunnableWithExceptionProtection ( ( ) -> update ( moduleDefineHolder ) , t -> log . error ( "Cache update failure." , t ) ) , 1 , timeInterval , TimeUnit . SECONDS ) ; this . ttl = ttl ; }
void logTextUnitSearcherError ( TextUnitSearcherError textUnitSearcherError ) throws TextUnitSearcherError { this . logger . warn ( "An error occured: {}" , textUnitSearcherError . getMessage ( ) ) ; }
public void test() { try { GroupRestClient . provisionMembers ( model . getObject ( ) . getKey ( ) , ProvisionAction . DEPROVISION ) ; SyncopeConsoleSession . get ( ) . success ( getString ( Constants . OPERATION_SUCCEEDED ) ) ; target . add ( container ) ; } catch ( SyncopeClientException e ) { LOG . error ( "While provisioning group {}" , model . getObject ( ) . getKey ( ) , e ) ; SyncopeConsoleSession . get ( ) . onException ( e ) ; } }
public void test() { try { LOGGER . debug ( "Refreshing repo" ) ; final PullResult result = getGit ( ) . pull ( ) . setProgressMonitor ( PROGRESS_MONITOR ) . setRebase ( true ) . setCredentialsProvider ( user ) . call ( ) ; code_block = IfStatement ; LOGGER . debug ( "Finished refresh" ) ; } catch ( final Exception e ) { LOGGER . error ( "Error when refreshing git directory " + workspaceProvider . getRootDirectory ( ) , e ) ; } }
public void test() { try { LOGGER . debug ( "Started refresh with git pull" ) ; final PullResult result = getGit ( ) . pull ( ) . setProgressMonitor ( PROGRESS_MONITOR ) . setRebase ( true ) . setCredentialsProvider ( user ) . call ( ) ; code_block = IfStatement ; LOGGER . debug ( "Finished refresh" ) ; } catch ( final Exception e ) { LOGGER . warn ( "Failed to refresh git pull" , e ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
private void loadConfig ( ComponentContext cc ) throws ConfigurationException { String orgStr = OsgiUtil . getComponentContextProperty ( cc , ORGANIZATION_KEY , DEFAULT_ORGANIZATION_VALUE ) ; code_block = TryStatement ;  url = OsgiUtil . getComponentContextProperty ( cc , CANVAS_URL_KEY ) ; code_block = IfStatement ; token = OsgiUtil . getComponentContextProperty ( cc , CANVAS_USER_TOKEN_KEY ) ; String cacheSizeStr = OsgiUtil . getComponentContextProperty ( cc , CACHE_SIZE_KEY , DEFAULT_CACHE_SIZE_VALUE . toString ( ) ) ; cacheSize = NumberUtils . toInt ( cacheSizeStr ) ; String cacheExpireStr = OsgiUtil . getComponentContextProperty ( cc , CACHE_EXPIRATION_KEY , DEFAULT_CACHE_EXPIRATION_VALUE . toString ( ) ) ; cacheExpiration = NumberUtils . toInt ( cacheExpireStr ) ; logger . debug ( "Canvas authentication: {}" , rolesStr ) ; String rolesStr = OsgiUtil . getComponentContextProperty ( cc , CANVAS_INSTRUCTOR_ROLES_KEY , DEFAULT_CANVAS_INSTRUCTOR_ROLES ) ; instructorRoles = parsePropertyLineAsSet ( rolesStr ) ; logger . debug ( "Canvas instructor roles: {}" , instructorRoles ) ; String ignoredUsersStr = OsgiUtil . getComponentContextProperty ( cc , IGNORED_USERNAMES_KEY , DEFAULT_INGROED_USERNAMES ) ; ignoredUsernames = parsePropertyLineAsSet ( ignoredUsersStr ) ; logger . debug ( "Ignored users: {}" , ignoredUsernames ) ; }
private void loadConfig ( ComponentContext cc ) throws ConfigurationException { String orgStr = OsgiUtil . getComponentContextProperty ( cc , ORGANIZATION_KEY , DEFAULT_ORGANIZATION_VALUE ) ; code_block = TryStatement ;  url = OsgiUtil . getComponentContextProperty ( cc , CANVAS_URL_KEY ) ; code_block = IfStatement ; logger . debug ( "Canvas URL: {}" , url ) ; token = OsgiUtil . getComponentContextProperty ( cc , CANVAS_USER_TOKEN_KEY ) ; String cacheSizeStr = OsgiUtil . getComponentContextProperty ( cc , CACHE_SIZE_KEY , DEFAULT_CACHE_SIZE_VALUE . toString ( ) ) ; cacheSize = NumberUtils . toInt ( cacheSizeStr ) ; logger . debug ( "Canvas roles: {}" , roles ) ; String cacheExpireStr = OsgiUtil . getComponentContextProperty ( cc , CACHE_EXPIRATION_KEY , DEFAULT_CACHE_EXPIRATION_VALUE . toString ( ) ) ; cacheExpiration = NumberUtils . toInt ( cacheExpireStr ) ; String rolesStr = OsgiUtil . getComponentContextProperty ( cc , CANVAS_INSTRUCTOR_ROLES_KEY , DEFAULT_CANVAS_INSTRUCTOR_ROLES ) ; instructorRoles = parsePropertyLineAsSet ( rolesStr ) ; logger . debug ( "Ignored users: {}" , ignoredUsernames ) ; String ignoredUsersStr = OsgiUtil . getComponentContextProperty ( cc , IGNORED_USERNAMES_KEY , DEFAULT_INGROED_USERNAMES ) ; ignoredUsernames = parsePropertyLineAsSet ( ignoredUsersStr ) ; logger . debug ( "Ignored users: {}" , ignoredUsernames ) ; }
private void loadConfig ( ComponentContext cc ) throws ConfigurationException { String orgStr = OsgiUtil . getComponentContextProperty ( cc , ORGANIZATION_KEY , DEFAULT_ORGANIZATION_VALUE ) ; code_block = TryStatement ;  url = OsgiUtil . getComponentContextProperty ( cc , CANVAS_URL_KEY ) ; code_block = IfStatement ; logger . debug ( "Canvas URL: {}" , url ) ; token = OsgiUtil . getComponentContextProperty ( cc , CANVAS_USER_TOKEN_KEY ) ; String cacheSizeStr = OsgiUtil . getComponentContextProperty ( cc , CACHE_SIZE_KEY , DEFAULT_CACHE_SIZE_VALUE . toString ( ) ) ; cacheSize = NumberUtils . toInt ( cacheSizeStr ) ; String cacheExpireStr = OsgiUtil . getComponentContextProperty ( cc , CACHE_EXPIRATION_KEY , DEFAULT_CACHE_EXPIRATION_VALUE . toString ( ) ) ; cacheExpiration = NumberUtils . toInt ( cacheExpireStr ) ; logger . debug ( "Canvas roles: {}" , rolesStr ) ; String rolesStr = OsgiUtil . getComponentContextProperty ( cc , CANVAS_INSTRUCTOR_ROLES_KEY , DEFAULT_CANVAS_INSTRUCTOR_ROLES ) ; instructorRoles = parsePropertyLineAsSet ( rolesStr ) ; logger . debug ( "Canvas instructor roles: {}" , instructorRoles ) ; String ignoredUsersStr = OsgiUtil . getComponentContextProperty ( cc , IGNORED_USERNAMES_KEY , DEFAULT_INGROED_USERNAMES ) ; ignoredUsernames = parsePropertyLineAsSet ( ignoredUsersStr ) ; }
public void test() { if ( ! optTmpNewRegistry . isPresent ( ) ) { log . warn ( "Could not find temporary RESTXQ registry on disk" ) ; } else { final Path tmpNewRegistry = optTmpNewRegistry . get ( ) ; log . info ( "Preparing new RESTXQ registry on disk: {}" , tmpNewRegistry . toAbsolutePath ( ) . toString ( ) ) ; code_block = TryStatement ;  } }
public void test() { if ( optRegistry . isPresent ( ) ) { final Path registry = optRegistry . get ( ) ; final Path localTmpNewRegistry = Files . copy ( tmpNewRegistry , registry . getParent ( ) . resolve ( tmpNewRegistry . getFileName ( ) ) ) ; Files . move ( localTmpNewRegistry , registry , StandardCopyOption . REPLACE_EXISTING , StandardCopyOption . ATOMIC_MOVE ) ; LOG . debug ( "Updated RESTXQ registry to {}" , registry ) ; } else { throw new IOException ( "Unable to retrieve existing RESTXQ registry" ) ; } }
public void test() { try { code_block = TryStatement ;  final Optional < Path > optRegistry = getRegistryFile ( false ) ; code_block = IfStatement ; } catch ( final IOException ioe ) { LOGGER . warn ( "Unable to load registry file: " + ioe . getMessage ( ) , ioe ) ; } finally { TemporaryFileManager . getInstance ( ) . returnTemporaryFile ( tmpNewRegistry ) ; } }
public void test() { if ( getLabHandler ( ) . getResponseCode ( ) == HL7LabHandler . OK ) { log . debug ( "Labels collection success." ) ; } }
public void test() { try { List < String > versionList = new ArrayList < > ( ) ; String executorsPath = "/" + namespace + ExecutorNodePath . getExecutorNodePath ( ) ; code_block = IfStatement ; List < String > executors = curatorFrameworkOp . getChildren ( executorsPath ) ; code_block = IfStatement ; code_block = ForStatement ; return getVersionStrFromList ( versionList ) ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; return "" ; } }
public void test() { { logger . debug ( "Received Event: {}" , response ) ; XdsSchedulerManager . getInstance ( ) . stopSubscriptionDiscoveryScheduling ( ) ; latestReceived = response ; code_block = TryStatement ;  } }
public void test() { { logger . error ( "Error occurred during subscription discovery" , throwable ) ; XdsSchedulerManager . getInstance ( ) . startSubscriptionDiscoveryScheduling ( ) ; nack ( throwable ) ; } }
public void test() { try { DiscoveryRequest req = DiscoveryRequest . newBuilder ( ) . setNode ( Node . newBuilder ( ) . setId ( nodeId ) . build ( ) ) . setVersionInfo ( latestACKed . getVersionInfo ( ) ) . setTypeUrl ( Constants . SUBSCRIPTION_LIST_TYPE_URL ) . build ( ) ; reqObserver . onNext ( req ) ; logger . debug ( "Subscription Type url: {}" , Constants . SUBSCRIPTION_LIST_TYPE_URL ) ; } catch ( Exception e ) { logger . error ( "Unexpected error occurred in API discovery service" , e ) ; reqObserver . onError ( e ) ; } }
public void test() { try { DiscoveryRequest req = DiscoveryRequest . newBuilder ( ) . setNode ( Node . newBuilder ( ) . setId ( nodeId ) . build ( ) ) . setVersionInfo ( latestACKed . getVersionInfo ( ) ) . setTypeUrl ( Constants . SUBSCRIPTION_LIST_TYPE_URL ) . build ( ) ; reqObserver . onNext ( req ) ; logger . debug ( "Sent Discovery request for type url: " + Constants . SUBSCRIPTION_LIST_TYPE_URL ) ; } catch ( Exception e ) { logger . error ( "Unexpected error." , e ) ; reqObserver . onError ( e ) ; } }
public void onSessionInit ( @ Observes @ Initialized ( SessionScoped . class ) Object payload ) { log . info ( "Session initialized" ) ; }
@ Test public void callMethodWithMultipleByteBufferParametersAsync ( ) { logger . info ( name . getMethodName ( ) + "" ) ; final Semaphore resultAvailable = new Semaphore ( 0 ) ; code_block = TryStatement ;  logger . info ( name . getMethodName ( ) + " - OK" ) ; }
public void test() { if ( error instanceof JoynrRuntimeException ) { logger . info ( name . getMethodName ( ) + " - callback - caught exception" , error ) ; } else { logger . info ( name . getMethodName ( ) + " - callback - caught exception" ) ; } }
public void test() { if ( error instanceof JoynrRuntimeException ) { logger . info ( name . getMethodName ( ) + " - callback - caught exception " + ( ( JoynrRuntimeException ) error ) . getMessage ( ) ) ; } else { logger . info ( name . getMethodName ( ) + " - callback - caught exception " + error ) ; } }
public void test() { try { logger . info ( name . getMethodName ( ) + " - callback not received" ) ; Assert . assertTrue ( name . getMethodName ( ) + " - FAILED - callback not received in time" , resultAvailable . tryAcquire ( 10 , TimeUnit . SECONDS ) ) ; logger . info ( name . getMethodName ( ) + " - wait for callback is over" ) ; Assert . assertTrue ( name . getMethodName ( ) + " - FAILED - callback reported error" , methodWithMultipleByteBufferParametersAsyncCallbackResult ) ; } catch ( InterruptedException | JoynrRuntimeException e ) { fail ( name . getMethodName ( ) + " - FAILED - caught unexpected exception: " + e . getMessage ( ) ) ; } }
public void test() { try { logger . info ( name . getMethodName ( ) + " - about to wait for callback" ) ; Assert . assertTrue ( name . getMethodName ( ) + " - FAILED - callback not received in time" , resultAvailable . tryAcquire ( 10 , TimeUnit . SECONDS ) ) ; logger . info ( name . getMethodName ( ) + " - FAILED - callback reported error" ) ; Assert . assertTrue ( name . getMethodName ( ) + " - FAILED - callback reported error" , methodWithMultipleByteBufferParametersAsyncCallbackResult ) ; } catch ( InterruptedException | JoynrRuntimeException e ) { fail ( name . getMethodName ( ) + " - FAILED - caught unexpected exception: " + e . getMessage ( ) ) ; } }
public void test() { try { CommerceAccountGroupRelServiceUtil . deleteCommerceAccountGroupRels ( className , classPK ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try { Thread . sleep ( 10000 ) ; } catch ( InterruptedException e ) { LOG . warn ( "Interrupted while sleeping" , e ) ; running = false ; } }
public void test() { try { code_block = IfStatement ; } catch ( Throwable e ) { logger . error ( "Failed to BEFORE process. {}" , e . getMessage ( ) , e ) ; throw new RuntimeException ( e ) ; } }
@ ParallelNamespaceTest @ Tag ( INTERNAL_CLIENTS_USED ) void testSendMessagesPlainAnonymous ( ExtensionContext extensionContext ) { final String namespaceName = extensionContext . getStore ( ExtensionContext . Namespace . GLOBAL ) . get ( Constants . NAMESPACE_KEY ) . toString ( ) ; final String clusterName = mapWithClusterNames . get ( extensionContext . getDisplayName ( ) ) ; final String topicName = mapWithTestTopics . get ( extensionContext . getDisplayName ( ) ) ; final String clientsName = mapWithKafkaClientNames . get ( extensionContext . getDisplayName ( ) ) ; resourceManager . createResource ( extensionContext , KafkaTemplates . kafkaEphemeral ( clusterName , 3 ) . build ( ) ) ; resourceManager . createResource ( extensionContext , KafkaTopicTemplates . topic ( clusterName , topicName ) . build ( ) ) ; resourceManager . createResource ( extensionContext , KafkaClientsTemplates . kafkaClients ( false , clusterName + "-" + Constants . KAFKA_CLIENTS ) . build ( ) ) ; final String defaultKafkaClientsPodName = kubeClient ( namespaceName ) . listPodsByPrefixInName ( namespaceName , clusterName + "-" + Constants . KAFKA_CLIENTS ) . get ( 0 ) . getMetadata ( ) . getName ( ) ; InternalKafkaClient internalKafkaClient = new InternalKafkaClient . Builder ( ) . withUsingPodName ( defaultKafkaClientsPodName ) . withTopicName ( topicName ) . withNamespaceName ( namespaceName ) . withClusterName ( clusterName ) . withMessageCount ( MESSAGE_COUNT ) . withListenerName ( Constants . PLAIN_LISTENER_DEFAULT_NAME ) . build ( ) ; internalKafkaClient . checkProducedAndConsumedMessages ( internalKafkaClient . sendMessagesPlain ( ) , internalKafkaClient . receiveMessagesPlain ( ) ) ; Service kafkaService = kubeClient ( namespaceName ) . getService ( namespaceName , KafkaResources . bootstrapServiceName ( clusterName ) ) ; String kafkaServiceDiscoveryAnnotation = kafkaService . getMetadata (
public void test() { try { return slaveSyncClient . syncCheckpoint ( ) ; } catch ( Exception e ) { LOG . error ( "Failed to sync checkpoint." , e ) ; code_block = TryStatement ;  } }
public void test() { if ( recognizer == null ) { log . warn ( "Cannot find any recognizer for [{}]" , getName ( ) ) ; return ; } }
public void refreshCurrentMasterHealthStatus ( ) { long currentTime = System . currentTimeMillis ( ) ; Set < String > toDeleteTargetDcIds = new HashSet < > ( ) ; Set < String > unhealthyTargetDcIds = new HashSet < > ( ) ; code_block = ForStatement ; toDeleteTargetDcIds . forEach ( remoteMasterLastHealthDelayTimes :: remove ) ; toDeleteTargetDcIds . forEach ( remoteMasterInstances :: remove ) ; updateCurrentMasterHealthStatus ( clusterId , shardId , unhealthyTargetDcIds ) ; log . info ( "[TaskTracker-{}] refreshing current master health status: {}" , clusterId , shardId ) ; }
public void test() { if ( major > CURRENT_MAJOR_VERSION || ( major == CURRENT_MAJOR_VERSION && minor > CURRENT_MINOR_VERSION ) ) { log . warn ( "ORC file %s is written by a newer Hive version %s. Current Hive version is %s." , orcDataSource , Joiner . on ( '.' ) . join ( version ) , CURRENT_MAJOR_VERSION , CURRENT_MINOR_VERSION ) ; } }
public void test() { try ( Connection conn = buildConnection ( serverUrl ) ; Statement stmt = conn . createStatement ( ) ) { StringBuilder sb = new StringBuilder ( ) ; code_block = IfStatement ; ResultSet rs = stmt . executeQuery ( sb . toString ( ) ) ; code_block = WhileStatement ; } catch ( SQLException e ) { LOGGER . error ( "{}" , e . getMessage ( ) , e ) ; } }
public void test() { { int trans = ++ transRover ; boolean relevantTrans = true ; ClientType clientType = relevantTrans ? ClientType . randomClientType ( ) : null ; int count = 1000 ; LOG . info ( "Publishing Trans[id=" + trans + ", count=" + count + ", clientType=" + clientType + "]" ) ; Connection con = cf . createConnection ( ) ; Session sess = con . createSession ( true , Session . SESSION_TRANSACTED ) ; MessageProducer prod = sess . createProducer ( null ) ; code_block = ForStatement ; Message message = sess . createMessage ( ) ; message . setIntProperty ( "ID" , ++ messageRover ) ; message . setIntProperty ( "TRANS" , trans ) ; message . setBooleanProperty ( "COMMIT" , true ) ; message . setBooleanProperty ( "RELEVANT" , relevantTrans ) ; prod . send ( topic , message ) ; clientManager . onServerMessage ( message ) ; committingTransaction = trans ; sess . commit ( ) ; committingTransaction = - 1 ; LOG . info ( "Committed Trans[id=" + trans + ", count=" + count + ", clientType=" + clientType + "], ID=" + messageRover ) ; sess . close ( ) ; con . close ( ) ; } }
@ Test public void testEmptyStructuredConfig ( ) throws Exception { Entity app = createAndStartApplication ( loadYaml ( "test-entity-basic-template.yaml" , "  brooklyn.config:" , "    test.confName: \"\"" , "    test.confListThing: !!seq []" , "    test.confSetThing: !!set {}" , "    test.confMapThing: !!map {}" ) ) ; waitForApplicationTasks ( app ) ; Assert . assertEquals ( app . getDisplayName ( ) , "test-entity-basic-template" ) ; log . info ( "App started:" ) ; Dumper . dumpInfo ( app ) ; Entity entity = app . getChildren ( ) . iterator ( ) . next ( ) ; Assert . assertNotNull ( entity , "Expected app to have child entity" ) ; Assert . assertTrue ( entity instanceof TestEntity , "Expected TestEntity, found " + entity . getClass ( ) ) ; TestEntity testEntity = ( TestEntity ) entity ; List < String > thingList = ( List < String > ) testEntity . getConfig ( TestEntity . CONF_LIST_THING ) ; Set < String > thingSet = ( Set < String > ) testEntity . getConfig ( TestEntity . CONF_SET_THING ) ; Map < String , String > thingMap = testEntity . getConfig ( TestEntity . CONF_MAP_THING ) ; Assert . assertEquals ( thingList , Lists . newArrayList ( ) ) ; Assert . assertEquals ( thingSet , ImmutableSet . of ( ) ) ; Assert . assertEquals ( thingMap , ImmutableMap . of ( ) ) ; }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { if ( ! success ) { logger . error ( String . format ( "Failed to delete temporary file %s" , file ) ) ; } }
public void test() { -> { LOG . trace ( "Shutting down JVM" ) ; code_block = IfStatement ; LOG . trace ( "OnShutdown complete" ) ; } }
public void test() { if ( waits > 0 && waits % 5 == 0 ) { LOG . info ( msg ) ; } else { LOG . trace ( msg ) ; } }
public void test() { if ( waits > 0 && waits % 5 == 0 ) { LOG . info ( msg ) ; } else { LOG . error ( msg ) ; } }
public void test() { try { InputStream source = ResourceUtils . create ( ) . getResourceFromUrl ( url ) ; CatalogDto result = ( CatalogDto ) new CatalogXmlSerializer ( ) . deserialize ( new InputStreamReader ( source ) ) ; if ( log . isDebugEnabled ( ) ) log . debug ( "Retrieved catalog from: {}" , url ) ; return result ; } catch ( Throwable t ) { log . error ( "Error loading catalog from: " + url , t ) ; throw Exceptions . propagate ( t ) ; } }
public void test() { try { Ignite node = ignite ( idx . getAndIncrement ( ) % nodes ) ; log . info ( "Start thread [node=" + node . name ( ) + ']' ) ; IgniteCache cache = node . cache ( cacheName ) ; Map < Integer , Integer > map = new LinkedHashMap < > ( ) ; code_block = IfStatement ; while ( ! stop . get ( ) ) cache . putAll ( map ) ; } catch ( Exception e ) { log . error ( "Unexpected error: " + e , e ) ; err . set ( true ) ; stop . set ( true ) ; } }
protected void handleError ( final Exception e , final String correlationUid , final String organisationIdentification , final String deviceIdentification , final String messageType , final int messagePriority ) { LOGGER . info ( "handling error: {} for message type: {}" , e . getMessage ( ) , messageType ) ; OsgpException osgpException = null ; code_block = IfStatement ; final ResponseMessage responseMessage = ResponseMessage . newResponseMessageBuilder ( ) . withCorrelationUid ( correlationUid ) . withOrganisationIdentification ( organisationIdentification ) . withDeviceIdentification ( deviceIdentification ) . withResult ( ResponseMessageResultType . NOT_OK ) . withOsgpException ( osgpException ) . withMessagePriority ( messagePriority ) . build ( ) ; this . responseMessageSender . send ( responseMessage , messageType ) ; }
public void test() { if ( LOG . isWarnEnabled ( ) ) { LOG . warn ( "Unhandled exception: " , e ) ; } }
public void test() { try { JtsSpatialContextFactory contextFactory = new JtsSpatialContextFactory ( ) ; contextFactory . allowMultiOverlap = true ; SpatialContext spatialContext = contextFactory . newSpatialContext ( ) ; Shape shape = ( Shape ) spatialContext . readShapeFromWkt ( wkt ) ; Point center = shape . getCenter ( ) ; return center ; } catch ( java . text . ParseException parseException ) { logger . error ( "Could not parse center geometry" , parseException ) ; } }
public void test() { if ( request . getAuditMessage ( ) == null ) { LOG . info ( "No Security message provided" ) ; return result ; } }
public void test() { try { String url = oProxyHelper . getUrlLocalHomeCommunity ( NhincConstants . AUDIT_REPO_SERVICE_NAME ) ; code_block = IfStatement ; } catch ( Exception e ) { LOG . error ( "error getting AUDIT_REPO_SERVICE" , e ) ; } }
public void test() { try { readRecordsFromStorage ( records ) ; } catch ( Exception e ) { LOGGER . debug ( "Failed to read records from storage" , e ) ; store . convertListToCurrentRecordSize ( MeasurementSchedule . PROP_MSCHED ) ; readRecordsFromStorage ( records ) ; } }
public void test() { if ( _specificVerboseLogger . isTraceEnabled ( ) ) { String packetsBatchText = packets . isEmpty ( ) ? "IDLE-STATE" : "firstPacketKey=" + packets . get ( 0 ) . getKey ( ) + ", lastPacketKey=" + packets . get ( packets . size ( ) - 1 ) . getEndKey ( ) ; _specificVerboseLogger . trace ( StringBatchText ) ; } }
public void test() { try { listener . checkedUserListMembership ( user ) ; } catch ( Exception e ) { logger . warn ( "Exception at showUserListMembership" , e ) ; } }
public void test() { if ( name . equals ( OrganizationIdentityProvider . NAME ) ) { setType ( Type . USER ) ; } else-if ( name . equals ( SpaceIdentityProvider . NAME ) ) { setType ( Type . SPACE ) ; } else { log . info ( "Unhandled type: {}" , name ) ; } }
public void test() { try { Map < String , Object > result = new HashMap < > ( ) ; result . put ( "status" , - 1 ) ; result . put ( "error" , message . getMessageId ( ) ) ; ServletResponse response = asyncContext . getResponse ( ) ; response . setContentType ( "application/json" ) ; MAPPER . writeValue ( response . getWriter ( ) , result ) ; } catch ( Exception e ) { LOG . error ( e . getMessage ( ) , e ) ; } finally { asyncContext . complete ( ) ; } }
public void test() { if ( key != null ) { code_block = IfStatement ; code_block = IfStatement ; } else { logger . debug ( "Key not found in memory: {}" , toString ( ) ) ; } }
public void test() { if ( hasChanges ) { log . info ( "has changes" ) ; } }
public void test() { if ( new File ( file ) . exists ( ) ) { entityManagerFactoryCallable . getUnitInfo ( ) . addMappingFileName ( file ) ; } else { LOG . warn ( "Ignoring mapping file: " + file ) ; } }
@ Override protected PaymentTransactionInfoPlugin doCallSpecificOperationCallback ( ) throws PaymentPluginApiException { LOG . debug ( "Received call specific operation callback for paymentId: {}" , paymentStateContext . getPaymentId ( ) ) ; return new DefaultNoOpPaymentInfoPlugin ( paymentStateContext . getPaymentId ( ) , paymentStateContext . getTransactionId ( ) , TransactionType . CHARGEBACK , paymentStateContext . getAmount ( ) , paymentStateContext . getCurrency ( ) , null , null , PaymentPluginStatus . PROCESSED , null , null ) ; }
public void test() { try { JSONObject jsonObject = jsonFactory . createJSONObject ( propertyValue ) ; String languageId = themeDisplay . getLanguageId ( ) ; code_block = IfStatement ; return jsonObject . getString ( getDefaultLanguageId ( ) ) ; } catch ( JSONException jsonException ) { _log . error ( jsonException , jsonException ) ; } }
@ Override public void afterBulk ( long executionId , BulkRequest request , BulkResponse response ) { long msec = response . getTook ( ) . getMillis ( ) ; eventCounter . scope ( "bulks_received" ) . incrBy ( 1 ) ; eventCounter . scope ( "bulk_msec" ) . incrBy ( msec ) ; Iterator < BulkItemResponse > bulkitemiterator = response . iterator ( ) ; int itemcount = 0 ; int acked = 0 ; int failurecount = 0 ; synchronized ( waitAck ) code_block = "" ; LOGGER . debug ( "Items/{} ({})" , itemcount , acked ) ; }
public void test() { if ( f . getStatus ( ) . equals ( RestStatus . CONFLICT ) ) { eventCounter . scope ( "doc_conflicts" ) . incrBy ( 1 ) ; LOG . debug ( "Doc conflict ID {}" , id ) ; } else { LOG . error ( "Document conflict for {}: {}" , id , f . getStatus ( ) ) ; failed = true ; } }
public void test() { if ( xx != null ) { LOG . debug ( "Acked {} tuple(s) for ID {}" , xx . size ( ) , id ) ; code_block = ForStatement ; waitAck . invalidate ( id ) ; } else { LOG . warn ( "Could not find unacked tuple for {}" , id ) ; } }
public void test() { for ( String kinaw : waitAck . asMap ( ) . keySet ( ) ) { logger . debug ( kinaw ) ; } }
@ Test public void testMultipleErrors ( log , executor ) { Logger log = Mockito . mock ( Logger . class ) ; ScheduledExecutorService executor = Mockito . mock ( ScheduledExecutorService . class ) ; Duration seconds = Duration . seconds ( 30 ) ; RateLimitedLog rateLimitedLog = new DefaultRateLimitedLogFactory ( executor , seconds ) . from ( log ) ; Mockito . verify ( executor ) . scheduleWithFixedDelay ( any ( ) , eq ( 1L ) , eq ( 1L ) , eq ( 1L ) , eq ( TimeUnit . MINUTES ) ) ; Throwable t1 = new Throwable ( ) ; Throwable t2 = new Throwable ( ) ; Throwable t3 = new Throwable ( ) ; rateLimitedLog . error ( t2 , "Test error: {}" , "second!" ) ; rateLimitedLog . error ( t3 , "Test error: {}" , "third!" ) ; Mockito . verify ( log ) . error ( "Test error: first!" , t1 ) ; ArgumentCaptor < Runnable > captor1 = ArgumentCaptor . forClass ( Runnable . class ) ; Mockito . verify ( executor ) . schedule ( captor1 . capture ( ) , eq ( 30L ) , eq ( TimeUnit . SECONDS ) ) ; Mockito . verifyNoMoreInteractions ( log , executor ) ; Mockito . reset ( executor ) ; captor1 . getValue ( ) . run ( ) ; Mockito . verify ( log ) . error ( "Encountered {} {} within the last {}: {}" , 2L , "errors" , Duration . seconds ( 30 ) , "Test error: third!" , t3 ) ; ArgumentCaptor < Runnable > captor2 = ArgumentCaptor . forClass ( Runnable . class ) ; Mockito . verify ( executor ) . schedule ( captor2 . capture ( ) , eq ( 30L ) , eq ( TimeUnit . SECONDS ) ) ; Mockito . verifyNoMoreInteractions ( log , executor ) ; captor2 . getValue ( ) . run ( ) ; Mockito . verifyNoMoreInteractions ( log , executor ) ; }
@ Test public void testMultipleErrors ( log , executor ) { Logger log = Mockito . mock ( Logger . class ) ; ScheduledExecutorService executor = Mockito . mock ( ScheduledExecutorService . class ) ; Duration seconds = Duration . seconds ( 30 ) ; RateLimitedLog rateLimitedLog = new DefaultRateLimitedLogFactory ( executor , seconds ) . from ( log ) ; Mockito . verify ( executor ) . scheduleWithFixedDelay ( any ( ) , eq ( 1L ) , eq ( 1L ) , eq ( 1L ) , eq ( TimeUnit . MINUTES ) ) ; Throwable t1 = new Throwable ( ) ; Throwable t2 = new Throwable ( ) ; Throwable t3 = new Throwable ( ) ; rateLimitedLog . error ( t1 , "Test error: {}" , "first!" ) ; rateLimitedLog . error ( t3 , "Test error: {}" , "third!" ) ; Mockito . verify ( log ) . error ( "Test error: first!" , t1 ) ; ArgumentCaptor < Runnable > captor1 = ArgumentCaptor . forClass ( Runnable . class ) ; Mockito . verify ( executor ) . schedule ( captor1 . capture ( ) , eq ( 30L ) , eq ( TimeUnit . SECONDS ) ) ; Mockito . verifyNoMoreInteractions ( log , executor ) ; Mockito . reset ( executor ) ; captor1 . getValue ( ) . run ( ) ; Mockito . verify ( log ) . error ( "Encountered {} {} within the last {}: {}" , 2L , "errors" , Duration . seconds ( 30 ) , "Test error: third!" , t3 ) ; ArgumentCaptor < Runnable > captor2 = ArgumentCaptor . forClass ( Runnable . class ) ; Mockito . verify ( executor ) . schedule ( captor2 . capture ( ) , eq ( 30L ) , eq ( TimeUnit . SECONDS ) ) ; Mockito . verifyNoMoreInteractions ( log , executor ) ; captor2 . getValue ( ) . run ( ) ; Mockito . verifyNoMoreInteractions ( log , executor ) ; }
@ Test public void testMultipleErrors ( log , executor ) { Logger log = Mockito . mock ( Logger . class ) ; ScheduledExecutorService executor = Mockito . mock ( ScheduledExecutorService . class ) ; Duration seconds = Duration . seconds ( 30 ) ; RateLimitedLog rateLimitedLog = new DefaultRateLimitedLogFactory ( executor , seconds ) . from ( log ) ; Mockito . verify ( executor ) . scheduleWithFixedDelay ( any ( ) , eq ( 1L ) , eq ( 1L ) , eq ( 1L ) , eq ( TimeUnit . MINUTES ) ) ; Throwable t1 = new Throwable ( ) ; Throwable t2 = new Throwable ( ) ; Throwable t3 = new Throwable ( ) ; rateLimitedLog . error ( t1 , "Test error: {}" , "first!" ) ; rateLimitedLog . error ( t2 , "Test error: {}" , "second!" ) ; Mockito . verify ( log ) . error ( "Test error: first!" , t1 ) ; ArgumentCaptor < Runnable > captor1 = ArgumentCaptor . forClass ( Runnable . class ) ; Mockito . verify ( executor ) . schedule ( captor1 . capture ( ) , eq ( 30L ) , eq ( TimeUnit . SECONDS ) ) ; Mockito . verifyNoMoreInteractions ( log , executor ) ; Mockito . reset ( executor ) ; captor1 . getValue ( ) . run ( ) ; Mockito . verify ( log ) . error ( "Encountered {} {} within the last {}: {}" , 2L , "errors" , Duration . seconds ( 30 ) , "Test error: third!" , t3 ) ; ArgumentCaptor < Runnable > captor2 = ArgumentCaptor . forClass ( Runnable . class ) ; Mockito . verify ( executor ) . schedule ( captor2 . capture ( ) , eq ( 30L ) , eq ( TimeUnit . SECONDS ) ) ; Mockito . verifyNoMoreInteractions ( log , executor ) ; captor2 . getValue ( ) . run ( ) ; Mockito . verifyNoMoreInteractions ( log , executor ) ; }
public void requestReadAlarmRegister ( final DeviceMessageMetadata deviceMessageMetadata , final ReadAlarmRegisterRequest readAlarmRegisterRequestValueObject ) throws FunctionalException { LOGGER . info ( "requestReadAlarmRegister for organisationIdentification: {} for deviceIdentification: {}" , deviceMessageMetadata . getOrganisationIdentification ( ) , deviceMessageMetadata . getDeviceIdentification ( ) ) ; final SmartMeter smartMeteringDevice = this . domainHelperService . findSmartMeter ( deviceMessageMetadata . getDeviceIdentification ( ) ) ; final ReadAlarmRegisterRequestDto readAlarmRegisterRequestDto = this . monitoringMapper . map ( readAlarmRegisterRequestValueObject , ReadAlarmRegisterRequestDto . class ) ; this . osgpCoreRequestMessageSender . send ( new RequestMessage ( deviceMessageMetadata . getCorrelationUid ( ) , deviceMessageMetadata . getOrganisationIdentification ( ) , deviceMessageMetadata . getDeviceIdentification ( ) , smartMeteringDevice . getIpAddress ( ) , readAlarmRegisterRequestDto ) , deviceMessageMetadata . getMessageType ( ) , deviceMessageMetadata . getMessagePriority ( ) , deviceMessageMetadata . getScheduleTime ( ) , deviceMessageMetadata . bypassRetry ( ) ) ; }
@ Override public void initialize ( ) { logger . debug ( "Initializing Station configuration" ) ; StationConfiguration config = getConfigAs ( StationConfiguration . class ) ; logger . debug ( "config refresh = {} min" , config . refresh ) ; updateStatus ( ThingStatus . UNKNOWN ) ; code_block = IfStatement ; getReferences ( ) ; refreshJob = scheduler . scheduleWithFixedDelay ( this :: updateAndPublish , 0 , config . refresh , TimeUnit . MINUTES ) ; }
@ Override public void initialize ( ) { logger . debug ( "Initializing VigiCrues handler." ) ; StationConfiguration config = getConfigAs ( StationConfiguration . class ) ; updateStatus ( ThingStatus . UNKNOWN ) ; code_block = IfStatement ; getReferences ( ) ; refreshJob = scheduler . scheduleWithFixedDelay ( this :: updateAndPublish , 0 , config . refresh , TimeUnit . MINUTES ) ; logger . debug ( "VigiCrues handler started." ) ; }
@ Override public Map < String , IMonomer > getMonomers ( ) { logger . debug ( "Getting Monomers" ) ; return super . getMonomers ( ) ; }
public List findByExample ( StgMsZykTxt instance ) { log . debug ( "finding StgMsZykTxt instance by example" ) ; code_block = TryStatement ;  }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.StgMsZykTxt" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.StgMsZykTxt" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
@ Override public void validationError ( Throwable throwable ) { wbNotification . fire ( new NotificationEvent ( i18n . validationError ( ) , NotificationEvent . NotificationType . ERROR ) ) ; LOG . error ( throwable . getMessage ( ) , throwable ) ; }
public void test() { if ( regexp == null ) { return Optional . empty ( ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( PatternSyntaxException e ) { LOGGER . warn ( "Error parsing regular expression: {}" , e . getMessage ( ) ) ; return Optional . empty ( ) ; } }
public void test() { if ( event . cSID != null ) { log . debug ( "Destroying cSID : " + event . cSID . getLongId ( ) ) ; int count = event . session . destroySession ( event . cSID ) ; event . status . getOperatorMeasurement ( ) . changeActiveValue ( - count ) ; } else { log . debug ( "Destroying plan session : " + event . sID . getLongId ( ) ) ; int count = event . session . destroySession ( ) ; event . status . getOperatorMeasurement ( ) . changeActiveValue ( - count ) ; } }
public void test() { if ( event . cSID != null ) { log . debug ( "Destroying container session : " + event . sID . getLongId ( ) ) ; int count = event . session . destroySession ( event . cSID ) ; event . status . getOperatorMeasurement ( ) . changeActiveValue ( - count ) ; } else { log . debug ( "Destroying session session : " + event . sID . getLongId ( ) ) ; int count = event . session . destroySession ( ) ; event . status . getOperatorMeasurement ( ) . changeActiveValue ( - count ) ; } }
public void test() { if ( started ) { started = false ; Enumeration < String > keys = allocatedLimiters . keys ( ) ; Collections . asList ( keys ) . forEach ( name -> Tasks . shutdownExecutor ( name , allocatedLimiters . get ( name ) . executor , 5 ) ) ; } else { LOG . debug ( "Ignoring exception, it already stopped" ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( ClassCastException E ) { LOGGER . warn ( E ) ; return null ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void warn ( CharSequence charSequence ) { seenWarning ( charSequence ) ; logger . warn ( charSequence ) ; }
@ ExceptionHandler ( UaaException . class ) public ResponseEntity < UaaException > handleException ( UaaException e ) { logger . error ( "Unexpected exception" , e ) ; code_block = IfStatement ; return new ResponseEntity < > ( e , HttpStatus . valueOf ( e . getHttpStatus ( ) ) ) ; }
@ Override public void getCurrentConfigFailed ( VehicleState . GetCurrentConfigErrorEnum error , ReplyContext replyContext ) { logger . warn ( "getCurrentConfig failed:" , replyContext ) ; }
public void test() { try { Entry entry = readEntry ( ldif ) ; Dn newDn = getDnFactory ( ) . create ( dn ) ; entry . setDn ( newDn ) ; return new DefaultEntry ( schemaManager , entry ) ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; return null ; } }
public void test() { if ( mBus . getShuntSusceptance ( ) != 0 ) { String busId = getId ( BUS_PREFIX , mBus . getNumber ( ) ) ; String shuntId = getId ( SHUNT_PREFIX , mBus . getNumber ( ) ) ; double zb = voltageLevel . getNominalV ( ) * voltageLevel . getNominalV ( ) / perUnitContext . getBaseMva ( ) ; ShuntCompensatorAdder adder = voltageLevel . newShuntCompensator ( ) . setId ( shuntId ) . setConnectableBus ( busId ) . setBus ( busId ) . setSectionCount ( 1 ) ; adder . newLinearModel ( ) . setBPerSection ( mBus . getShuntSusceptance ( ) / perUnitContext . getBaseMva ( ) / zb ) . setMaximumSectionCount ( 1 ) . add ( ) ; ShuntCompensator newShunt = adder . add ( ) ; LOG . info ( "Add a bus {}" , mBus . getShuntSusceptance ( ) ) ; } }
@ Override public void sendCommfaultTag ( long tagID , String tagName , boolean value , String pDescription ) { long timestamp = System . currentTimeMillis ( ) ; SourceDataTagValue commfaultTagValue = SourceDataTagValue . builder ( ) . id ( tagID ) . name ( tagName ) . controlTag ( true ) . value ( value ) . quality ( new SourceDataTagQuality ( ) ) . timestamp ( new Timestamp ( timestamp ) ) . daqTimestamp ( new Timestamp ( timestamp ) ) . priority ( JmsMessagePriority . PRIORITY_HIGHEST . getPriority ( ) ) . timeToLive ( DataTagConstants . TTL_FOREVER ) . valueDescription ( pDescription ) . build ( ) ; distributeValue ( commfaultTagValue ) ; LOGGER . info ( "sendCommfaultTag({}) sent a tag {} - {}" , tagID , tagName , value ) ; }
public void test() { default : { logger . error ( "Unknown error type" ) ; return SenseiProtos . ErrorType . UnknownError ; } }
public void test() { try { if ( msg . getSrc ( ) == null ) msg . setSrc ( local_addr ) ; ByteArrayDataOutputStream out = new ByteArrayDataOutputStream ( msg . size ( ) ) ; msg . writeTo ( out ) ; code_block = ForStatement ; } catch ( Exception ex ) { logger . warn ( ex . toString ( ) ) ; } }
@ Override public String getName ( ) { logger . debug ( "Getting name: " , super . getName ( ) ) ; return super . getName ( ) ; }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Refreshing." ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Refreshing." ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { for ( int i = 0 ; i < size ; i ++ ) { logger . info ( MSG ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { zone = Integer . parseInt ( m . group ( 1 ) ) ; } catch ( NumberFormatException e ) { logger . warn ( "Invalid zone: " + m . group ( 1 ) ) ; return ; } }
public void test() { try { dci . executeTemplateCommands ( ) ; dci . scanGlobalDocumentCommands ( ) ; dci . scanInsertFormValueCommands ( ) ; } catch ( WMCommandsFailedException e ) { LOGGER . error ( "WARNING: " + e . getMessage ( ) ) ; } }
public void test() { try { elasticsearchConnection . getClient ( ) . prepareIndex ( ) . setIndex ( FQL_STORE_INDEX ) . setType ( DOCUMENT_TYPE_NAME ) . setId ( fqlStore . getId ( ) ) . setSource ( objectMapper . writeValueAsBytes ( fqlStore ) , XContentType . JSON ) . execute ( ) . get ( ) ; } catch ( Exception e ) { logger . error ( "Couldn't save FQL query: " + fqlStore . getQuery ( ) ) ; throw new FqlPersistenceException ( "Couldn't save FQL query: " + fqlStore . getQuery ( ) + " Error Message: " + e . getMessage ( ) , e ) ; } }
public void test() { try { com . liferay . dynamic . data . mapping . model . DDMTemplate returnValue = DDMTemplateServiceUtil . getTemplate ( groupId , classNameId , templateKey , includeAncestorTemplates ) ; return com . liferay . dynamic . data . mapping . model . DDMTemplateSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try { TextMessage tm = ( TextMessage ) m ; LOG . info ( "consumer received message :" + tm . getText ( ) ) ; receivedText = tm . getText ( ) ; latch . countDown ( ) ; LOG . info ( "consumer received message :" + receivedText ) ; consumerSession . commit ( ) ; LOG . info ( "committed transaction" ) ; } catch ( JMSException e ) { code_block = TryStatement ;  e . printStackTrace ( ) ; } }
public void test() { try { tm = producerSession . createTextMessage ( ) ; tm . setText ( "Hello, " + new Date ( ) ) ; producer . send ( tm ) ; LOG . info ( "sent: " + tm . getText ( ) ) ; } catch ( JMSException e ) { e . printStackTrace ( ) ; } }
public void testTransaction ( ) throws Exception { ActiveMQConnectionFactory factory = new ActiveMQConnectionFactory ( "vm://localhost?broker.persistent=false" ) ; connection = factory . createConnection ( ) ; queue = new ActiveMQQueue ( getClass ( ) . getName ( ) + "." + getName ( ) ) ; producerSession = connection . createSession ( false , Session . AUTO_ACKNOWLEDGE ) ; consumerSession = connection . createSession ( true , 0 ) ; producer = producerSession . createProducer ( queue ) ; consumer = consumerSession . createConsumer ( queue ) ; consumer . setMessageListener ( new MessageListener ( ) code_block = "" ; ) ; connection . start ( ) ; TextMessage tm = null ; LOG . info ( "waiting..." ) ; code_block = TryStatement ;  latch . await ( 2 , TimeUnit . SECONDS ) ; assertNotNull ( receivedText ) ; LOG . info ( "test completed, destination=" + receivedText ) ; }
public void testTransaction ( ) throws Exception { ActiveMQConnectionFactory factory = new ActiveMQConnectionFactory ( "vm://localhost?broker.persistent=false" ) ; connection = factory . createConnection ( ) ; queue = new ActiveMQQueue ( getClass ( ) . getName ( ) + "." + getName ( ) ) ; producerSession = connection . createSession ( false , Session . AUTO_ACKNOWLEDGE ) ; consumerSession = connection . createSession ( true , 0 ) ; producer = producerSession . createProducer ( queue ) ; consumer = consumerSession . createConsumer ( queue ) ; consumer . setMessageListener ( new MessageListener ( ) code_block = "" ; ) ; connection . start ( ) ; TextMessage tm = null ; LOG . info ( "Started" ) ; code_block = TryStatement ;  LOG . info ( "Waiting for latch" ) ; latch . await ( 2 , TimeUnit . SECONDS ) ; assertNotNull ( receivedText ) ; }
public void test() { try { listener . onSuccess ( ) ; } catch ( final IllegalArgumentException e ) { LOG . warn ( "Could not register request RTT" , e ) ; listener . onIgnore ( ) ; } }
public void test() { if ( listener != null ) { code_block = TryStatement ;  metrics . decInflight ( ) ; } else { Loggers . TRANSPORT_LOGGER . warn ( "[{}-{}-{}] Could not find peer connection for {}" , logPrefix , endpoint , uri ) ; } }
@ GetMapping ( RestApi . ARCHIVE_UNIT_INFO + CommonConstants . PATH_ID ) @ Secured ( ServicesData . ROLE_GET_ARCHIVE ) public ResponseEntity < ResultsDto > findUnitById ( final @ PathVariable ( "id" ) String id ) { LOGGER . debug ( "getArchiveUnitById {}" , id ) ; ParameterChecker . checkParameter ( "The Identifier is a mandatory parameter: " , id ) ; return archivesSearchExternalService . findUnitById ( id ) ; }
public void test() { try { tmSocket . close ( ) ; } catch ( IOException e ) { logger . debug ( "could not close TM socket" , e ) ; } }
public void test() { try { conn = this . getConnection ( ) ; conn . setAutoCommit ( false ) ; stat = conn . prepareStatement ( DELETE_USED_TOKEN ) ; stat . setString ( 1 , token ) ; stat . executeUpdate ( ) ; conn . commit ( ) ; } catch ( Throwable t ) { this . executeRollback ( conn ) ; _logger . error ( "Error removing consumed Token" , t ) ; throw new RuntimeException ( "Error removing consumed Token" , t ) ; } finally { closeDaoResources ( null , stat , conn ) ; } }
public void test() { if ( resource != null ) { LOG . info ( "Found Beans in '{}'." , pkg ) ; } else { LOG . info ( "No Beans in '{}' found. Requests {} will fail." , pkg , paths ) ; } }
public void test() { if ( resource != null ) { LOG . info ( "rest({}).packages({})" , paths , pkg ) ; } else { LOG . info ( "rest({}).packages({})" , paths , pkg ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { ErrorLogAnalyzer analyzer = new ErrorLogAnalyzer ( moduleManager , errorLogListenerManager , config ) ; analyzer . doAnalysis ( browserErrorLog ) ; } catch ( Throwable e ) { logErrorCounter . inc ( ) ; log . error ( e . getMessage ( ) , e ) ; } finally { timer . finish ( ) ; } }
public void test() { try { CPDefinitionSpecificationOptionValueServiceUtil . deleteCPDefinitionSpecificationOptionValues ( cpDefinitionId ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
protected InputStream getInputStream ( String profileId ) throws IOException { final String profileUrl = registryBaseURl + profileId + "/xml" ; LOG . debug ( "Getting InputStream from registry: {}" , profileUrl ) ; return new URL ( profileUrl ) . openStream ( ) ; }
public void test() { if ( result . isSuccessful ( ) ) { LOG . debug ( "{} succeeded: {} -> {}" , prefix , nodeId . getValue ( ) ) ; } else { final Collection < RpcError > errors = requireNonNullElse ( result . getErrors ( ) , ImmutableList . of ( ) ) ; LOG . debug ( "{} failed: {} -> {}" , prefix , nodeId . getValue ( ) , errors ) ; } }
public void test() { if ( result . isSuccessful ( ) ) { LOG . debug ( "{} finished successfully: {}" , prefix , nodeId . getValue ( ) ) ; } else { final Collection < RpcError > errors = requireNonNullElse ( result . getErrors ( ) , ImmutableList . of ( ) ) ; LOG . warn ( "{} failed to finish with errors: {}" , prefix , errors . get ( ) ) ; } }
public void test() { if ( result != null ) { code_block = IfStatement ; } else { LOGGER . warn ( "Unable to delete temporary file: " + file . getAbsolutePath ( ) ) ; } }
private void updateRouterNetworkRef ( Connection conn ) { s_logger . debug ( "Updating router/network references..." ) ; code_block = TryStatement ;  s_logger . debug ( "Done updating router/network references" ) ; }
public void test() { try { updatePriority ( id , taskParam . getPriority ( ) . ordinal ( ) ) ; } catch ( Exception e ) { log . error ( "Error updating priority" , e ) ; } }
@ OnWebSocketConnect public void onConnect ( Session session ) { LOG . info ( "Successfully connected" ) ; this . session = session ; }
public void test() { try { final Connection testConnection = DriverManager . getConnection ( configuration . getConnectionUrl ( ) , configuration . getUsername ( ) , configuration . getPassword ( ) ) ; testConnection . close ( ) ; available = true ; } catch ( SQLException ex ) { LOGGER . error ( "unable to connect to pool" , ex ) ; available = false ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { code_block = IfStatement ; Thread . sleep ( 2000 ) ; } catch ( InterruptedException e ) { logger . error ( "" , e ) ; } }
public void test() { try { com . liferay . portal . kernel . model . User returnValue = UserServiceUtil . updateUser ( userId , oldPassword , newPassword1 , newPassword2 , passwordReset , reminderQueryQuestion , reminderQueryAnswer , screenName , emailAddress , facebookId , openId , hasPortrait , portraitBytes , languageId , timeZoneId , greeting , comments , firstName , middleName , lastName , prefixId , suffixId , male , birthdayMonth , birthdayDay , birthdayYear , smsSn , facebookSn , jabberSn , skypeSn , twitterSn , jobTitle , groupIds , organizationIds , roleIds , com . liferay . portal . model . impl . UserGroupRoleModelImpl . toModels ( userGroupRoles ) , userGroupIds , com . liferay . portal . model . impl . AddressModelImpl . toModels ( addresses ) , com . liferay . portal . model . impl . EmailAddressModelImpl . toModels ( addresses ) , com . liferay . portal . model . impl . PhoneModelImpl . toModels ( phones ) , com . liferay . portal . model . impl . WebsiteModelImpl . toModels ( websites ) , com . liferay . portlet . announcements . model . impl . AnnouncementsDeliveryModelImpl . toModels ( announcementsDelivers ) , serviceContext ) ; return com . liferay . portal . kernel . model . UserSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( matcher . find ( ) ) { handler . updateChannel ( PLAY_MODE , new StringType ( KaleidescapeStatusCodes . PLAY_MODE . get ( matcher . group ( 1 ) ) ) ) ; handler . updateChannel ( PLAY_SPEED , new StringType ( matcher . group ( 2 ) ) ) ; handler . updateChannel ( TITLE_NUM , new DecimalType ( Integer . parseInt ( matcher . group ( 3 ) ) ) ) ; handler . updateChannel ( TITLE_LENGTH , new QuantityType < Time > ( Integer . parseInt ( matcher . group ( 4 ) ) , handler . apiSecondUnit ) ) ; handler . updateChannel ( TITLE_LOC , new QuantityType < Time > ( Integer . parseInt ( matcher . group ( 5 ) ) , handler . apiSecondUnit ) ) ; handler . updateChannel ( CHAPTER_NUM , new DecimalType ( Integer . parseInt ( matcher . group ( 6 ) ) ) ) ; handler . updateChannel ( CHAPTER_LENGTH , new QuantityType < Time > ( Integer . parseInt ( matcher . group ( 7 ) ) , handler . apiSecondUnit ) ) ; handler . updateChannel ( CHAPTER_LOC , new QuantityType < Time > ( Integer . parseInt ( matcher . group ( 8 ) ) , handler . apiSecondUnit ) ) ; } else { logger . debug ( "No matches pattern '{}'" , matcher . group ( 1 ) ) ; } }
private void persistCoviCodes ( LocalDate localDate ) throws Exception { logger . info ( "Persisting CoviCodes" ) ; List < CoviCode > coviCodes = this . coviCodeGenerator . generateCoviCodes ( localDate ) ; coviCodes . forEach ( code -> coviCodeRepository . saveDoNothingOnConflict ( code . getCode ( ) , code . getStartInterval ( ) , code . getEndInterval ( ) ) ) ; logger . info ( "Done persisting" ) ; }
private void persistCoviCodes ( LocalDate localDate ) throws Exception { List < CoviCode > coviCodes = this . coviCodeGenerator . generateCoviCodes ( localDate ) ; logger . info ( "Persisting " + coviCodes . size ( ) + " cints for start " + coviCodes . get ( 0 ) . getStartInterval ( ) + " to end " + coviCodes . get ( coviCodes . size ( ) - 1 ) . getEndInterval ( ) ) ; coviCodes . forEach ( code -> coviCodeRepository . saveDoNothingOnConflict ( code . getCode ( ) , code . getStartInterval ( ) , code . getEndInterval ( ) ) ) ; logger . info ( "Done saving COVCodes" ) ; }
public void test() { try { InputStream schRules = FileUtils . openInputStream ( new File ( path ) ) ; rules = TemplatesFactory . newInstance ( ) . getTemplates ( schRules , transformerFactory ) ; } catch ( FileNotFoundException e ) { LOG . error ( "Failed to load configuration file {}" , path , e ) ; throw classPathException ; } }
public void test() { try { InetSocketAddress inetSocketAddress = ( InetSocketAddress ) socketAddress ; String hostname = inetSocketAddress . getHostName ( ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . warn ( "Failed to resolve hostname" , e ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { try { WebHookConfig webHookConfig = populateWebHookConfiguration ( registerWebhookjson ) ; status = webhookConfigurationDAL . updateWebHookConfiguration ( webHookConfig ) ; } catch ( Exception e ) { log . error ( "Error updating webHookConfiguration object" , e ) ; throw new InsightsCustomException ( e . toString ( ) ) ; } }
public void test() { if ( ! lastRefreshTokenTried . compareAndSet ( null , currentRefreshToken ) ) { LOGGER . debug ( "Trying to refresh a refresh token but no refresh token will be sent" ) ; return ; } }
public void test() { if ( statusLine . getStatusCode ( ) != HttpStatus . SC_OK ) { final HttpEntity entity = response . getEntity ( ) ; LOG . error ( entity . toString ( ) ) ; return ; } }
public void test() { try ( CloseableHttpClient client = createHttpClient ( ) ) { final HttpUriRequest request = createHttpRequest ( ) ; code_block = TryStatement ;  } catch ( final IOException e ) { LOGGER . error ( "Failed to send HTTP GET request" , e ) ; } }
public void test() { if ( routingMap != null ) { return routingMap . getRangeByPartitionKeyRangeId ( partitionKeyRangeId ) ; } else { LOGGER . info ( "Could not find routing map for routing map [{}]" , partitionKeyRangeId ) ; return null ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { if ( debug ) { logger . debug ( "Expected error: " + e . getMessage ( ) ) ; } }
public void test() { try { appendMessage ( baos , MAGIC_NUMBER ) ; appendMessage ( baos , 1 ) ; appendMessage ( baos , SOFT_RESET ) ; byte [ ] message = sendMessage ( baos ) ; code_block = IfStatement ; code_block = IfStatement ; return message ; } catch ( Exception e ) { log . error ( "reset threw" , e ) ; return null ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { XNamed textSection = UNO . XNamed ( textSections . get ( sectionNameComplete ) ) ; String nameBase = generateCompleteName ( ) ; String name = nameBase ; int count = 1 ; code_block = WhileStatement ; textSection . setName ( name ) ; sectionNameComplete = name ; return true ; } catch ( Exception x ) { LOGGER . error ( "" , x ) ; return false ; } }
@ Override public void run ( ) { log . debug ( " EngineCorrelatorModule started ====" ) ; code_block = TryStatement ;  log . debug ( " EngineCorrelatorModule Completed ====" ) ; }
public void test() { try { ApplicationConfigInterface . loadConfiguration ( ) ; code_block = IfStatement ; EngineStatusLogger . getInstance ( ) . createEngineStatusNode ( "Correlation Execution Completed" , PlatformServiceConstants . SUCCESS ) ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; EngineStatusLogger . getInstance ( ) . createEngineStatusNode ( "Correlation Execution has some issue  " , PlatformServiceConstants . FAILURE ) ; } }
@ Override public void run ( ) { log . debug ( " EngineCorrelatorModule start  ====" ) ; code_block = TryStatement ;  log . debug ( " EngineCorrelatorModule end  ====" ) ; }
public void test() { if ( logger . isErrorEnabled ( ) ) { logger . error ( "Application startup failed" , failure ) ; registerLoggedException ( failure ) ; } }
public void test() { try { return this . eventData . getClass ( ) . getMethod ( method . getName ( ) ) ; } catch ( NoSuchMethodException exception ) { LOG . error ( exception . getMessage ( ) ) ; throw new RuntimeException ( exception ) ; } }
public void test() { if ( isEmpty ( ) ) { return false ; } }
public void test() { if ( isDebug ) { logger . debug ( "Expected arguments, but found none." ) ; } }
public void test() { try { System . setProperty ( "org.apache.catalina.startup.EXIT_ON_INIT_FAILURE" , "true" ) ; _tomcat1 = getTestUtils ( ) . tomcatBuilder ( ) . port ( _portTomcat1 ) . memcachedNodes ( "http://localhost:" + couchbasePort + "/pools" ) . sticky ( true ) . memcachedProtocol ( "binary" ) . username ( "default" ) . buildAndStart ( ) ; } catch ( final Throwable e ) { LOG . error ( "Cannot start Couchbase: " , e ) ; throw e ; } }
public void test() { if ( isDebug ) { logger . debug ( "Expected arguments, but found none." ) ; } }
public void test() { try { executor . execute ( new Runnable ( ) code_block = "" ; ) ; } catch ( RejectedExecutionException ree ) { log . error ( ree . getMessage ( ) , ree ) ; } }
public void test() { try { return context . getClient ( AmazonEC2Client . class ) . describeInstances ( new DescribeInstancesRequest ( ) . withInstanceIds ( instanceId ) ) . getReservations ( ) . stream ( ) . map ( Reservation :: getInstances ) . flatMap ( Collection :: stream ) . filter ( i -> i . getInstanceId ( ) . equals ( instanceId ) ) . map ( Instance :: getImageId ) . findFirst ( ) ; } catch ( final AmazonClientException e ) { LOGGER . warn ( "Could not get instance information on instance " + instanceId , e ) ; return empty ( ) ; } }
public void test() { if ( size > warnLimit ) { String type = sessionId == null ? "stateless" : "stateful" ; log . warn ( type + " for session " + sessionId + " uses " + type + " instead." ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { if ( fileLocation == null ) { LOGGER . info ( "Property not set, using default" ) ; return props ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { if ( ! serviceTemplate . hasBuildPlan ( ) ) { BPELSituationAwareBuildProcessBuilder . LOG . debug ( "ServiceTemplate {} has no BuildPlan, generating BuildPlan" , serviceTemplate . getQName ( ) . toString ( ) ) ; final BPELPlan newBuildPlan = buildPlan ( csarName , definitions , serviceTemplate ) ; code_block = IfStatement ; } else { BPELituationAwareBuildProcessBuilder . LOG . debug ( "ServiceTemplate {} has no BuildPlan" , serviceTemplate . getQName ( ) . toString ( ) ) ; } }
public void test() { if ( ! plans . isEmpty ( ) ) { log . info ( "Created newBuildPlan {}" , plans . size ( ) ) ; } }
@ Override public void register ( EhcacheXAResource resource , boolean forRecovery ) { log . warn ( "Unable to register [" + resource . toString ( ) + "]" ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { code_block = ForStatement ; } catch ( Throwable e ) { logger . error ( e . getMessage ( ) , e ) ; return new MProcResult ( ) ; } }
public void test() { try { _log . error ( throwable ) ; } catch ( Exception exception ) { printMsg ( throwable . getMessage ( ) ) ; } }
public void test() { try { idName = featureModel . getEntityMetadata ( ) . getIdentifierPropertyName ( ) ; } catch ( LayerException e ) { LOGGER . log ( Level . WARNING , e . getMessage ( ) , e ) ; idName = HIBERNATE_ID ; } }
public void test() { if ( semsResponse . isOk ( ) ) { StatusResponse statusResponse = gson . fromJson ( response , StatusResponse . class ) ; code_block = IfStatement ; currentStatus = statusResponse . getStatus ( ) ; updateStatus ( ThingStatus . ONLINE ) ; return currentStatus ; } else-if ( semsResponse . isSessionInvalid ( ) ) { logger . error ( "Unexpected status code received from SEMS portal. Please check your station ID." ) ; login ( ) ; return getStationStatus ( stationUUID ) ; } else-if ( semsResponse . isError ( ) ) { throw new ConfigurationException ( "ERROR status code received from SEMS portal. Please check your station ID" ) ; } else { throw new CommunicationException ( String . format ( "Unknown status code received from SEMS portal: %s - %s" , semsResponse . getCode ( ) , semsResponse . getMsg ( ) ) ) ; } }
public String decodeSAMLPostResponse ( String encodedReponse ) { String trimmed = encodedReponse . replaceAll ( "\r\n" , "" ) ; String base64DecodedResponse = new String ( Base64 . decodeBase64 ( trimmed ) ) ; logger . debug ( "Decoded SAML Response from '" + base64DecodedResponse + "'" ) ; return base64DecodedResponse ; }
void initValue ( ) { String effectiveKey = doUpdateFinalValue ( ) ; LOGGER . debug ( effectiveKey + "=" + effectiveKey ) ; }
public void load ( ) throws RuntimeException { LOG . debug ( "load()" ) ; List < Organisation > tempOrganisations ; tempOrganisations = registryManager . getOrganisations ( ) ; Organisation o = new Organisation ( ) ; o . setName ( "" ) ; organisations . add ( o ) ; organisations . addAll ( tempOrganisations ) ; LOG . debug ( "organisations returned: " + organisations . size ( ) ) ; }
public void load ( ) throws RuntimeException { LOG . debug ( "getting list of organisations from registry" ) ; List < Organisation > tempOrganisations ; tempOrganisations = registryManager . getOrganisations ( ) ; Organisation o = new Organisation ( ) ; o . setName ( "" ) ; organisations . add ( o ) ; organisations . addAll ( tempOrganisations ) ; LOG . debug ( "loaded list of organisations" ) ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
@ Override public void error ( String msg , Object [ ] os , Throwable t ) { logger . error ( msg , os ) ; logger . error ( "" , t ) ; }
@ Override public void error ( String msg , Object [ ] os , Throwable t ) { logger . error ( msg , os ) ; logger . error ( t . getMessage ( ) , t ) ; }
@ Test public void loadCacheErrorDirectTemp ( ) throws Exception { LOG . info ( "Started loadCacheErrorDirectTemp" ) ; loadDirectBackendTemp ( 64 * 1024 * 1024 ) ; LOG . info ( "Finished loadCacheErrorDirectTemp" ) ; }
@ Test public void loadCacheErrorDirectTemp ( ) throws Exception { LOG . info ( "Started loadCacheErrorDirectTemp" ) ; loadDirectBackendTemp ( 64 * 1024 * 1024 ) ; LOG . info ( "Finished loadCacheErrorDirectTemp" ) ; }
@ Override public Operator run ( ) { long start = System . currentTimeMillis ( ) ; UResultOperator uResultOperator = new UResultOperator ( _planNode . run ( ) ) ; long end = System . currentTimeMillis ( ) ; LOG . info ( "Run " + ( end - start ) + " ms" ) ; return uResultOperator ; }
public void persist ( StgSysNetzMapping transientInstance ) { log . debug ( "persisting StgSysNetzMapping instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . persist ( transientInstance ) ; log . debug ( "persist successful" ) ; } catch ( RuntimeException re ) { log . error ( "persist failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . persist ( transientInstance ) ; log . debug ( "persist successful" ) ; } catch ( RuntimeException re ) { log . error ( "persist failed" , re ) ; throw re ; } }
@ Override public IRingSet getConnectedRings ( IRing ring ) { logger . debug ( "Getting connected ring: " , ring ) ; return super . getConnectedRings ( ring ) ; }
public void test() { try { listener . onInterceptorConfig ( interceptors ) ; } catch ( Throwable ex ) { LOGGER . error ( ex . toString ( ) , ex ) ; } }
@ RestAccessControl ( permission = Permission . SUPERUSER ) @ RequestMapping ( value = "/profileTypeAttributes" , method = RequestMethod . GET , produces = MediaType . APPLICATION_JSON_VALUE ) public ResponseEntity < PagedRestResponse < String > > getUserProfileAttributeTypes ( RestListRequest requestList ) throws JsonProcessingException { logger . debug ( "getAttributeTypes" ) ; this . getProfileTypeValidator ( ) . validateRestListRequest ( requestList , AttributeTypeDto . class ) ; PagedMetadata < String > result = this . getUserProfileTypeService ( ) . getAttributeTypes ( requestList ) ; this . getProfileTypeValidator ( ) . validateRestListResult ( requestList , result ) ; return new ResponseEntity < > ( new PagedRestResponse < > ( result ) , HttpStatus . OK ) ; }
public void test() { if ( result instanceof RegistrationResponse . Success ) { log . debug ( "Registration with {} at {} added." , targetName , targetAddress ) ; S success = ( S ) result ; completionFuture . complete ( RetryingRegistrationResult . success ( gateway , success ) ) ; } else-if ( result instanceof RegistrationResponse . Rejection ) { log . debug ( "Registration with {} at {} was rejected." , targetName , targetAddress ) ; R rejection = ( R ) result ; completionFuture . complete ( RetryingRegistrationResult . rejection ( rejection ) ) ; } else { code_block = IfStatement ; log . info ( "Pausing and re-attempting registration in {} ms" , retryingRegistrationConfiguration . getRefusedDelayMillis ( ) ) ; registerLater ( gateway , 1 , retryingRegistrationConfiguration . getInitialRegistrationTimeoutMillis ( ) , retryingRegistrationConfiguration . getRefusedDelayMillis ( ) ) ; } }
public void test() { if ( result instanceof RegistrationResponse . Success ) { log . debug ( "Registration with {} at {} was successful." , targetName , targetAddress ) ; S success = ( S ) result ; completionFuture . complete ( RetryingRegistrationResult . success ( gateway , success ) ) ; } else-if ( result instanceof RegistrationResponse . Rejection ) { R rejection = ( R ) result ; completionFuture . complete ( RetryingRegistrationResult . rejection ( rejection ) ) ; log . debug ( "Registration with {} at {} was rejected." , targetName , targetAddress ) ; } else { code_block = IfStatement ; log . info ( "Pausing and re-attempting registration in {} ms" , retryingRegistrationConfiguration . getRefusedDelayMillis ( ) ) ; registerLater ( gateway , 1 , retryingRegistrationConfiguration . getInitialRegistrationTimeoutMillis ( ) , retryingRegistrationConfiguration . getRefusedDelayMillis ( ) ) ; } }
public void test() { if ( result instanceof RegistrationResponse . Failure ) { RegistrationResponse . Failure failure = ( RegistrationResponse . Failure ) result ; log . warn ( "Received registration failure to registration node: {}" , failure . getMessage ( ) ) ; } else { log . error ( "Received unknown response to registration attempt: {}" , result ) ; } }
public void test() { if ( result instanceof RegistrationResponse . Failure ) { RegistrationResponse . Failure failure = ( RegistrationResponse . Failure ) result ; log . info ( "Registration failure at {} occurred." , targetName , failure . getReason ( ) ) ; } else { log . info ( "Registration failure at {} occurred." , targetName ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( ExceptionUtils . stripCompletionException ( failure ) instanceof TimeoutException ) { code_block = IfStatement ; long newTimeoutMillis = Math . min ( 2 * timeoutMillis , retryingRegistrationConfiguration . getMaxRegistrationTimeoutMillis ( ) ) ; log . info ( "Aborting registration on {} ms, retrying registration..." , attempt + 1 ) ; register ( gateway , attempt + 1 , newTimeoutMillis ) ; } else { log . info ( "Pausing and re-attempting registration in {} ms" , retryingRegistrationConfiguration . getErrorDelayMillis ( ) ) ; registerLater ( gateway , 1 , retryingRegistrationConfiguration . getInitialRegistrationTimeoutMillis ( ) , retryingRegistrationConfiguration . getErrorDelayMillis ( ) ) ; } }
public void test() { if ( ExceptionUtils . stripCompletionException ( failure ) instanceof TimeoutException ) { code_block = IfStatement ; long newTimeoutMillis = Math . min ( 2 * timeoutMillis , retryingRegistrationConfiguration . getMaxRegistrationTimeoutMillis ( ) ) ; log . debug ( "Registration at {} is now at {}" , targetName , attempt + 1 ) ; register ( gateway , attempt + 1 , newTimeoutMillis ) ; } else { log . error ( "Registration at {} failed due to an error" , targetName , failure ) ; registerLater ( gateway , 1 , retryingRegistrationConfiguration . getInitialRegistrationTimeoutMillis ( ) , retryingRegistrationConfiguration . getErrorDelayMillis ( ) ) ; } }
public void test() { if ( entry == null ) { String msg = "Cannot add an empty entry" ; log . error ( msg ) ; throw new IllegalArgumentException ( msg ) ; } }
public void test() { { String startTime = TimeUtil . getTimeWrtSystemTime ( - 15 ) ; String endTime = TimeUtil . getTimeWrtSystemTime ( 60 ) ; LOGGER . info ( "Start time : " + startTime ) ; processBundle . setProcessValidity ( startTime , endTime ) ; processBundle . submitFeedsScheduleProcess ( prism ) ; TimeUtil . sleepSeconds ( 10 ) ; InstanceUtil . waitTillInstanceReachState ( serverOC . get ( 0 ) , Util . readEntityName ( processBundle . getProcessData ( ) ) , 0 , CoordinatorAction . Status . WAITING , EntityType . PROCESS ) ; String oldProcess = processBundle . getProcessData ( ) ; String oldBundleID = OozieUtil . getLatestBundleID ( cluster1OC , Util . readEntityName ( oldProcess ) , EntityType . PROCESS ) ; List < String > oldNominalTimes = OozieUtil . getActionsNominalTime ( cluster1OC , oldBundleID , EntityType . PROCESS ) ; processBundle . setProcessProperty ( "someProp" , "someVal" ) ; String updateTime = TimeUtil . addMinsToTime ( endTime , 60 ) ; LOGGER . info ( "Original Feed : " + Util . prettyPrintXml ( oldProcess ) ) ; LOGGER . info ( "Updated Feed :" + Util . prettyPrintXml ( processBundle . getProcessData ( ) ) ) ; LOGGER . info ( "Update Time : " + updateTime ) ; ServiceResponse r = prism . getProcessHelper ( ) . update ( oldProcess , processBundle . getProcessData ( ) ) ; AssertUtil . assertSucceeded ( r ) ; OozieUtil . verifyNewBundleCreation ( cluster1OC , oldBundleID , oldNominalTimes , oldProcess , true , false ) ; InstanceUtil . waitTillInstancesAreCreated ( cluster1OC , processBundle . getProcessData ( ) , 1 ) ; OozieUtil . verifyNewBundleCreation ( cluster1OC , oldBundleID , oldNominalTimes , oldProcess , true , true ) ; } }
public void test() { { LOGGER . info ( "Running test updateTimeAfterEndTimeProcess" ) ; String startTime = TimeUtil . getTimeWrtSystemTime ( - 15 ) ; String endTime = TimeUtil . getTimeWrtSystemTime ( 60 ) ; processBundle . setProcessValidity ( startTime , endTime ) ; processBundle . submitFeedsScheduleProcess ( prism ) ; TimeUtil . sleepSeconds ( 10 ) ; InstanceUtil . waitTillInstanceReachState ( serverOC . get ( 0 ) , Util . readEntityName ( processBundle . getProcessData ( ) ) , 0 , CoordinatorAction . Status . WAITING , EntityType . PROCESS ) ; String oldProcess = processBundle . getProcessData ( ) ; String oldBundleID = OozieUtil . getLatestBundleID ( cluster1OC , Util . readEntityName ( oldProcess ) , EntityType . PROCESS ) ; List < String > oldNominalTimes = OozieUtil . getActionsNominalTime ( cluster1OC , oldBundleID , EntityType . PROCESS ) ; processBundle . setProcessProperty ( "someProp" , "someVal" ) ; String updateTime = TimeUtil . addMinsToTime ( endTime , 60 ) ; LOGGER . info ( "Updated Feed :" + Util . prettyPrintXml ( processBundle . getProcessData ( ) ) ) ; LOGGER . info ( "Update Time : " + updateTime ) ; ServiceResponse r = prism . getProcessHelper ( ) . update ( oldProcess , processBundle . getProcessData ( ) ) ; AssertUtil . assertSucceeded ( r ) ; OozieUtil . verifyNewBundleCreation ( cluster1OC , oldBundleID , oldNominalTimes , oldProcess , true , false ) ; InstanceUtil . waitTillInstancesAreCreated ( cluster1OC , processBundle . getProcessData ( ) , 1 ) ; OozieUtil . verifyNewBundleCreation ( cluster1OC , oldBundleID , oldNominalTimes , oldProcess , true , true ) ; LOGGER . info ( "Done test updateTimeAfterEndTimeProcess" ) ; } }
public void test() { { LOGGER . info ( "Running test updateTimeAfterEndTimeProcess" ) ; String startTime = TimeUtil . getTimeWrtSystemTime ( - 15 ) ; String endTime = TimeUtil . getTimeWrtSystemTime ( 60 ) ; processBundle . setProcessValidity ( startTime , endTime ) ; processBundle . submitFeedsScheduleProcess ( prism ) ; TimeUtil . sleepSeconds ( 10 ) ; InstanceUtil . waitTillInstanceReachState ( serverOC . get ( 0 ) , Util . readEntityName ( processBundle . getProcessData ( ) ) , 0 , CoordinatorAction . Status . WAITING , EntityType . PROCESS ) ; String oldProcess = processBundle . getProcessData ( ) ; String oldBundleID = OozieUtil . getLatestBundleID ( cluster1OC , Util . readEntityName ( oldProcess ) , EntityType . PROCESS ) ; List < String > oldNominalTimes = OozieUtil . getActionsNominalTime ( cluster1OC , oldBundleID , EntityType . PROCESS ) ; processBundle . setProcessProperty ( "someProp" , "someVal" ) ; String updateTime = TimeUtil . addMinsToTime ( endTime , 60 ) ; LOGGER . info ( "Original EndTime:" + endTime ) ; LOGGER . info ( "Original Feed : " + Util . prettyPrintXml ( oldProcess ) ) ; LOGGER . info ( "Update Time : " + updateTime ) ; ServiceResponse r = prism . getProcessHelper ( ) . update ( oldProcess , processBundle . getProcessData ( ) ) ; AssertUtil . assertSucceeded ( r ) ; OozieUtil . verifyNewBundleCreation ( cluster1OC , oldBundleID , oldNominalTimes , oldProcess , true , false ) ; InstanceUtil . waitTillInstancesAreCreated ( cluster1OC , processBundle . getProcessData ( ) , 1 ) ; OozieUtil . verifyNewBundleCreation ( cluster1OC , oldBundleID , oldNominalTimes , oldProcess , true , true ) ; } }
public void test() { { LOGGER . info ( "Running test updateTimeAfterEndTimeProcess" ) ; String startTime = TimeUtil . getTimeWrtSystemTime ( - 15 ) ; String endTime = TimeUtil . getTimeWrtSystemTime ( 60 ) ; processBundle . setProcessValidity ( startTime , endTime ) ; processBundle . submitFeedsScheduleProcess ( prism ) ; TimeUtil . sleepSeconds ( 10 ) ; InstanceUtil . waitTillInstanceReachState ( serverOC . get ( 0 ) , Util . readEntityName ( processBundle . getProcessData ( ) ) , 0 , CoordinatorAction . Status . WAITING , EntityType . PROCESS ) ; String oldProcess = processBundle . getProcessData ( ) ; String oldBundleID = OozieUtil . getLatestBundleID ( cluster1OC , Util . readEntityName ( oldProcess ) , EntityType . PROCESS ) ; List < String > oldNominalTimes = OozieUtil . getActionsNominalTime ( cluster1OC , oldBundleID , EntityType . PROCESS ) ; processBundle . setProcessProperty ( "someProp" , "someVal" ) ; String updateTime = TimeUtil . addMinsToTime ( endTime , 60 ) ; LOGGER . info ( "Time: " + updateTime + "ms" ) ; LOGGER . info ( "Original Feed : " + Util . prettyPrintXml ( oldProcess ) ) ; LOGGER . info ( "Updated Feed :" + Util . prettyPrintXml ( processBundle . getProcessData ( ) ) ) ; ServiceResponse r = prism . getProcessHelper ( ) . update ( oldProcess , processBundle . getProcessData ( ) ) ; AssertUtil . assertSucceeded ( r ) ; OozieUtil . verifyNewBundleCreation ( cluster1OC , oldBundleID , oldNominalTimes , oldProcess , true , false ) ; InstanceUtil . waitTillInstancesAreCreated ( cluster1OC , processBundle . getProcessData ( ) , 1 ) ; OozieUtil . verifyNewBundleCreation ( cluster1OC , oldBundleID , oldNominalTimes , oldProcess , true , true ) ; } }
public void test() { try { SpringApplication . run ( PowerJobServerApplication . class , args ) ; } catch ( Throwable t ) { log . error ( "Error starting PowerJobServer" , t ) ; throw t ; } }
public void test() { if ( LOG . isInfoEnabled ( ) ) { LOG . info ( String . format ( "Call to '%s' on file '%s'" , uri . toString ( ) , file ) ) ; } }
public void setLevel ( @ Nonnull Level level ) { this . level = level ; LOGGER . debug ( "{}: Set level to {}" , level , level ) ; parent . setLevel ( level ) ; }
public void test() { if ( e . getCode ( ) == HttpURLConnection . HTTP_CONFLICT ) { log . debug ( "received {} when updating lease lock" , e . getCode ( ) , e ) ; } else { log . error ( "received {} when updating lease lock" , e . getCode ( ) , e ) ; } }
public void test() { if ( e . getCode ( ) == HttpURLConnection . HTTP_CONFLICT ) { log . debug ( "received {} when updating lease lock" , e . getCode ( ) , e ) ; } else { log . error ( "received {} when updating lease lock" , e . getCode ( ) , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( invalid ) { String msg = msgBldr . toString ( ) ; logger . debug ( msg ) ; throw new InvalidDocumentException ( msg ) ; } }
public void test() { if ( peekNullable ( ) == null && waitTime > 0 ) { boolean await = dataAvailable . await ( Math . min ( waitTime , MAX_AWAIT_AVAILABLE_DATA ) , TimeUnit . MILLISECONDS ) ; LOG . info ( "Wait for data available" ) ; } }
public void test() { try { code_block = WhileStatement ; return peekNullable ( ) != null ; } catch ( InterruptedException e ) { Thread . currentThread ( ) . interrupt ( ) ; logger . debug ( e . getMessage ( ) , e ) ; return false ; } }
public void test() { if ( limit != Integer . MAX_VALUE ) { log . warning ( String . format ( "Set to %d characters for %d" , limit , l ) ) ; } else { limit = l ; } }
public void test() { if ( ! readMarker . hasIdentifier ( ) ) { this . messageTimeStart = readMarker . getStartTime ( times ) ; log . info ( "Loaded unidentified ReadMarker start time {} into {}" , messageTimeStart , this ) ; } else { long savedTimestamp = readSetting ( readMarker . getIdentifier ( ) , getMarkerColumn ( partitionId , bucketId ) , times . getTime ( readMarker . getStartTime ( times ) ) ) ; this . messageTimeStart = times . getTime ( savedTimestamp ) ; log . info ( "Loaded identified ReadMarker start time {} into {}" , messageTimeStart , this ) ; } }
@ Override public Document getNextPage ( Document doc ) throws IOException { String nextUrl = "" ; Element elem = doc . select ( "li.next > a" ) . first ( ) ; nextUrl = elem . attr ( "href" ) ; code_block = IfStatement ; LOGGER . info ( "Response: " + nextUrl ) ; return Http . url ( "https://eroshare.com" + nextUrl ) . get ( ) ; }
public void test() { try { logger . info ( "trying to create template in Tagger " ) ; logger . info ( "url: " + taggerMainUrl + "/customuitemplate/crisisID/" + crisisID ) ; WebTarget webResource = client . target ( taggerMainUrl + "/customuitemplate/crisisID/" + crisisID ) ; ObjectMapper objectMapper = JacksonWrapper . getObjectMapper ( ) ; Response clientResponse = webResource . request ( MediaType . APPLICATION_JSON ) . get ( ) ; String jsonResponse = clientResponse . readEntity ( String . class ) ; logger . info ( "Received jsonResponse: " + jsonResponse ) ; code_block = IfStatement ; return null ; } catch ( Exception e ) { logger . error ( "Error while creating new template in Tagger" , e ) ; throw new AidrException ( "Error while creating new template in Tagger" , e ) ; } }
public void test() { try { logger . info ( "getPublicLandingPageTemplate: " + crisisID ) ; WebTarget webResource = client . target ( taggerMainUrl + "/customuitemplate/crisisID/" + crisisID ) ; ObjectMapper objectMapper = JacksonWrapper . getObjectMapper ( ) ; logger . info ( "ObjectMapper objectMapper = " + objectMapper . getObjectMapper ( ) ) ; Response clientResponse = webResource . request ( MediaType . APPLICATION_JSON ) . get ( ) ; String jsonResponse = clientResponse . readEntity ( String . class ) ; logger . info ( "Received jsonResponse: " + jsonResponse ) ; code_block = IfStatement ; return null ; } catch ( Exception e ) { logger . error ( "Error while creating new template in Tagger" , e ) ; throw new AidrException ( "Error while creating new template in Tagger" , e ) ; } }
public void test() { try { logger . info ( "getPublicLandingPageTemplate: " + crisisID ) ; logger . info ( "url: " + taggerMainUrl + "/customuitemplate/crisisID/" + crisisID ) ; WebTarget webResource = client . target ( taggerMainUrl + "/customuitemplate/crisisID/" + crisisID ) ; ObjectMapper objectMapper = JacksonWrapper . getObjectMapper ( ) ; Response clientResponse = webResource . request ( MediaType . APPLICATION_JSON ) . get ( ) ; String jsonResponse = clientResponse . readEntity ( String . class ) ; logger . info ( "Response code: " + jsonResponse ) ; code_block = IfStatement ; return null ; } catch ( Exception e ) { logger . error ( "Error while creating new template in Tagger" , e ) ; throw new AidrException ( "Error while creating new template in Tagger" , e ) ; } }
public void test() { try { logger . info ( "getPublicLandingPageTemplate: " + crisisID ) ; logger . info ( "url: " + taggerMainUrl + "/customuitemplate/crisisID/" + crisisID ) ; WebTarget webResource = client . target ( taggerMainUrl + "/customuitemplate/crisisID/" + crisisID ) ; ObjectMapper objectMapper = JacksonWrapper . getObjectMapper ( ) ; Response clientResponse = webResource . request ( MediaType . APPLICATION_JSON ) . get ( ) ; String jsonResponse = clientResponse . readEntity ( String . class ) ; logger . info ( "Received jsonResponse: " + jsonResponse ) ; code_block = IfStatement ; return null ; } catch ( Exception e ) { logger . error ( "Error while creating new template in Tagger" , e ) ; throw new AidrException ( "Error while creating new template in Tagger" , e ) ; } }
public void test() { if ( blob == null ) { logger . debug ( "No blob found with key: {}" , key ) ; } }
public void test() { if ( ! ( any instanceof Element ) ) { LOGGER . error ( "Invalid Object: {}" , any ) ; return null ; } }
public void attachClean ( SysHilfe instance ) { log . debug ( "attaching clean SysHilfe instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . lock ( instance , LockMode . NONE ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { db = DB . newEmbeddedDB ( DBConfigurationBuilder . newBuilder ( ) . build ( ) ) ; db . start ( ) ; dbUrl = db . getConfiguration ( ) . getURL ( DEFAULT_DB_NAME ) ; connection = DriverManager . getConnection ( dbUrl ) ; code_block = TryStatement ;  } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; stopDb ( ) ; } }
public void test() { try { restoreFromDb ( sessionHandle ) ; } catch ( LensException le ) { log . error ( "Error restoring session from db" , le ) ; } }
@ Test public void test_systemSchemaAccess_stateOnlyNoLdapGroupUser ( ) throws Exception { HttpUtil . makeRequest ( stateOnlyUserClient , HttpMethod . GET , config . getBrokerUrl ( ) + "/status" , null ) ; LOG . info ( "Checking sys.servers query as stateOnlyNoLdapGroupUser..." ) ; verifySystemSchemaQuery ( stateOnlyNoLdapGroupUserClient , SYS_SCHEMA_SEGMENTS_QUERY , Collections . emptyList ( ) ) ; LOG . info ( "Checking sys.servers query as stateOnlyNoLdapGroupUser..." ) ; verifySystemSchemaServerQuery ( stateOnlyNoLdapGroupUserClient , SYS_SCHEMA_SERVERS_QUERY , adminServers ) ; LOG . info ( "Checking sys.server_segments query as stateOnlyNoLdapGroupUser..." ) ; verifySystemSchemaQuery ( stateOnlyNoLdapGroupUserClient , SYS_SCHEMA_SERVER_SEGMENTS_QUERY , Collections . emptyList ( ) ) ; LOG . info ( "Checking sys.tasks query as stateOnlyNoLdapGroupUser..." ) ; verifySystemSchemaQuery ( stateOnlyNoLdapGroupUserClient , SYS_SCHEMA_TASKS_QUERY , Collections . emptyList ( ) ) ; }
@ Test public void test_systemSchemaAccess_stateOnlyNoLdapGroupUser ( ) throws Exception { HttpUtil . makeRequest ( stateOnlyUserClient , HttpMethod . GET , config . getBrokerUrl ( ) + "/status" , null ) ; LOG . info ( "Checking sys.segments query as stateOnlyNoLdapGroupUser..." ) ; verifySystemSchemaQuery ( stateOnlyNoLdapGroupUserClient , SYS_SCHEMA_SEGMENTS_QUERY , Collections . emptyList ( ) ) ; LOG . info ( "Checking sys.server_segments query as stateOnlyNoLdapGroupUser..." ) ; verifySystemSchemaServerQuery ( stateOnlyNoLdapGroupUserClient , SYS_SCHEMA_SERVERS_QUERY , adminServers ) ; LOG . info ( "Checking sys.server_segments query as stateOnlyNoLdapGroupUser..." ) ; verifySystemSchemaQuery ( stateOnlyNoLdapGroupUserClient , SYS_SCHEMA_SERVER_SEGMENTS_QUERY , Collections . emptyList ( ) ) ; LOG . info ( "Checking sys.tasks query as stateOnlyNoLdapGroupUser..." ) ; verifySystemSchemaQuery ( stateOnlyNoLdapGroupUserClient , SYS_SCHEMA_TASKS_QUERY , Collections . emptyList ( ) ) ; }
@ Test public void test_systemSchemaAccess_stateOnlyNoLdapGroupUser ( ) throws Exception { HttpUtil . makeRequest ( stateOnlyUserClient , HttpMethod . GET , config . getBrokerUrl ( ) + "/status" , null ) ; LOG . info ( "Checking sys.segments query as stateOnlyNoLdapGroupUser..." ) ; verifySystemSchemaQuery ( stateOnlyNoLdapGroupUserClient , SYS_SCHEMA_SEGMENTS_QUERY , Collections . emptyList ( ) ) ; LOG . info ( "Checking sys.servers query as stateOnlyNoLdapGroupUser..." ) ; verifySystemSchemaServerQuery ( stateOnlyNoLdapGroupUserClient , SYS_SCHEMA_SERVERS_QUERY , adminServers ) ; LOG . info ( "Checking sys.service query as stateOnlyNoLdapGroupUser..." ) ; verifySystemSchemaQuery ( stateOnlyNoLdapGroupUserClient , SYS_SCHEMA_SERVER_SEGMENTS_QUERY , Collections . emptyList ( ) ) ; LOG . info ( "Checking sys.tasks query as stateOnlyNoLdapGroupUser..." ) ; verifySystemSchemaQuery ( stateOnlyNoLdapGroupUserClient , SYS_SCHEMA_TASKS_QUERY , Collections . emptyList ( ) ) ; }
@ Test public void test_systemSchemaAccess_stateOnlyNoLdapGroupUser ( ) throws Exception { HttpUtil . makeRequest ( stateOnlyUserClient , HttpMethod . GET , config . getBrokerUrl ( ) + "/status" , null ) ; LOG . info ( "Checking sys.segments query as stateOnlyNoLdapGroupUser..." ) ; verifySystemSchemaQuery ( stateOnlyNoLdapGroupUserClient , SYS_SCHEMA_SEGMENTS_QUERY , Collections . emptyList ( ) ) ; LOG . info ( "Checking sys.servers query as stateOnlyNoLdapGroupUser..." ) ; verifySystemSchemaServerQuery ( stateOnlyNoLdapGroupUserClient , SYS_SCHEMA_SERVERS_QUERY , adminServers ) ; LOG . info ( "Checking sys.server_segments query as stateOnlyNoLdapGroupUser..." ) ; verifySystemSchemaQuery ( stateOnlyNoLdapGroupUserClient , SYS_SCHEMA_SERVER_SEGMENTS_QUERY , Collections . emptyList ( ) ) ; LOG . info ( "Checking sys.tasks query as stateOnlyNoLdapGroupUser..." ) ; verifySystemSchemaQuery ( stateOnlyNoLdapGroupUserClient , SYS_SCHEMA_TASKS_QUERY , Collections . emptyList ( ) ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( state . error ( ) . isEmpty ( ) ) { log . debug ( "{}: Worker {} finished" , nodeName , task . id ) ; } else { log . warn ( "{}: Worker {} finished with error '{}' and status '{}'" , nodeName , task . id , state . error ( ) , JsonUtil . toJsonString ( state . status ( ) ) ) ; task . maybeSetError ( state . error ( ) ) ; } }
public void test() { if ( state . error ( ) . isEmpty ( ) ) { log . info ( "{}: Worker {} finished with status '{}'" , nodeName , task . id , JsonUtil . toJsonString ( state . status ( ) ) ) ; } else { log . error ( "{}: Worker {} failed with status '{}': {}" , nodeName , task . id , JsonUtil . toJsonString ( state . status ( ) ) , state . error ( ) ) ; task . maybeSetError ( state . error ( ) ) ; } }
public void test() { if ( activeWorkerIds . isEmpty ( ) ) { log . info ( "{}: task {} started" , nodeName , task . id ) ; task . doneMs = time . milliseconds ( ) ; task . state = TaskStateType . DONE ; } else-if ( ( task . state == TaskStateType . RUNNING ) && ( ! task . error . isEmpty ( ) ) ) { log . info ( "{}: task {} stopped with error {}.  Stopping worker(s): {}" , nodeName , task . id , task . error , Utils . mkString ( activeWorkerIds , "{" , "}" , ": " , ", " ) ) ; task . state = TaskStateType . STOPPING ; code_block = ForStatement ; } }
public void test() { if ( activeWorkerIds . isEmpty ( ) ) { task . doneMs = time . milliseconds ( ) ; task . state = TaskStateType . DONE ; log . info ( "{}: Task {} is now complete on {} with error: {}" , nodeName , task . id , Utils . join ( task . workerIds . keySet ( ) , ", " ) , task . error . isEmpty ( ) ? "(none)" : task . error ) ; } else-if ( ( task . state == TaskStateType . RUNNING ) && ( ! task . error . isEmpty ( ) ) ) { task . state = TaskStateType . STOPPING ; log . info ( "{}: Task {} is running on {}" , nodeName , task . id , Utils . join ( task . id , ", " ) ) ; code_block = ForStatement ; } }
void receiveRecords ( TlsContext receiveFromCtx ) { LOGGER . info ( "Received: " + receiveFromCtx ) ; receivedRecords = receiveMessageHelper . receiveRecords ( receiveFromCtx ) ; LOGGER . info ( "Records received (" + receiveFromAlias + "): " + receivedRecords . size ( ) ) ; executedAsPlanned = true ; }
void receiveRecords ( TlsContext receiveFromCtx ) { LOGGER . debug ( "Receiving records..." ) ; receivedRecords = receiveMessageHelper . receiveRecords ( receiveFromCtx ) ; LOGGER . debug ( "Received records: " + receivedRecords ) ; executedAsPlanned = true ; }
public void test() { try { ownedBundle . handleUnloadRequest ( pulsar , 5 , TimeUnit . MINUTES ) ; } catch ( IllegalStateException ex ) { } catch ( Exception ex ) { logger . error ( "Failed to handle unload request" , ex ) ; pulsar . getShutdownService ( ) . shutdown ( - 1 ) ; } }
public void test() { if ( record . getId ( ) == null ) { LOG . error ( "Missing ID from record" ) ; continue ; } }
public void test() { try { Constructor < T > constructor = clazz . getConstructor ( new Class [ ] code_block = "" ; ) ; T instance = constructor . newInstance ( record ) ; map . put ( record . getId ( ) , instance ) ; } catch ( Exception e ) { logger . warn ( "Cannot instantiate " + record , e ) ; } }
public final void start ( BundleContext context ) throws Exception { serviceRegistration = context . registerService ( ProxyTargetLocatorFactory . class , new SpringDMProxyTargetLocatorFactory ( ) , null ) ; LOGGER . info ( "Proxy TargetLocatorFactory started." ) ; }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Failed to clean up." , e ) ; } }
public void test() { try { Log . debug ( "Annotating Lane " + laneSWID ) ; Lane lane = ll . findLane ( "/" + laneSWID ) ; lane . getLaneAttributes ( ) . clear ( ) ; code_block = ForStatement ; ll . updateLane ( "/" + laneSWID , lane ) ; } catch ( IOException ex ) { Log . error ( "IOException while updating lane " + laneSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } catch ( JAXBException ex ) { Log . error ( "JAXBException while updating lane " + laneSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } catch ( ResourceException ex ) { Log . error ( "ResourceException while updating lane " + laneSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } }
public void test() { try { Log . debug ( "Annotating Lane " + laneSWID ) ; Lane lane = ll . findLane ( "/" + laneSWID ) ; lane . getLaneAttributes ( ) . clear ( ) ; code_block = ForStatement ; ll . updateLane ( "/" + laneSWID , lane ) ; } catch ( IOException ex ) { Log . error ( "IOException while updating lane " + laneSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } catch ( JAXBException ex ) { Log . error ( "JAXBException while updating lane " + laneSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } catch ( ResourceException ex ) { Log . error ( "ResourceException while updating lane " + laneSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } }
public void test() { try { Log . debug ( "Annotating Lane " + laneSWID ) ; Lane lane = ll . findLane ( "/" + laneSWID ) ; lane . getLaneAttributes ( ) . clear ( ) ; code_block = ForStatement ; ll . updateLane ( "/" + laneSWID , lane ) ; } catch ( IOException ex ) { Log . error ( "IOException while updating lane " + laneSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } catch ( JAXBException ex ) { Log . error ( "JAXBException while updating lane " + laneSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } catch ( ResourceException ex ) { Log . error ( "ResourceException while updating lane " + laneSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } }
public void test() { try { Log . debug ( "Annotating Lane " + laneSWID ) ; Lane lane = ll . findLane ( "/" + laneSWID ) ; lane . getLaneAttributes ( ) . clear ( ) ; code_block = ForStatement ; ll . updateLane ( "/" + laneSWID , lane ) ; } catch ( IOException ex ) { Log . error ( "IOException while updating lane " + laneSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } catch ( JAXBException ex ) { Log . error ( "JAXBException while updating lane " + laneSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } catch ( ResourceException ex ) { Log . error ( "ResourceException while updating lane " + laneSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } }
private List < ModuleType > getAllModuleTypesFallback ( Throwable e ) { e . printStackTrace ( ) ; logger . error ( "Failed to get all modules: " + e . getMessage ( ) ) ; return null ; }
public void test() { try { MBEAN_SERVER . registerMBean ( object , objectName ) ; } catch ( LinkageError | Exception e ) { LOGGER . log ( Level . FINE , "Failed to register MBean" , e ) ; } }
public void test() { if ( getCause ( e ) instanceof ConnectException ) { logger . error ( String . format ( "%s, cannot setup beans" , m ) , e ) ; } else-if ( e instanceof InterruptedException ) { logger . error ( String . format ( "%s, interrupted" , m ) ) ; } else { logger . error ( String . format ( "%s, cannot setup beans" , m ) , e ) ; } }
public void test() { if ( getCause ( e ) instanceof ConnectException ) { logger . error ( String . format ( "%s, cannot connect to %s" , m , jmx ) ) ; } else-if ( e instanceof InterruptedException ) { logger . error ( String . format ( "%s, cannot setup beans" , m ) , jmx ) ; } else { logger . error ( String . format ( "%s, cannot setup beans" , m ) , e ) ; } }
public void test() { if ( getCause ( e ) instanceof ConnectException ) { logger . error ( String . format ( "%s, cannot connect to %s" , m , jmx ) ) ; } else-if ( e instanceof InterruptedException ) { logger . error ( String . format ( "%s, interrupted" , m ) ) ; } else { logger . error ( String . format ( "%s, exception" , m , e ) ) ; } }
public void test() { if ( LOG . isWarnEnabled ( ) ) { LOG . warn ( "Unhandled exception: " , e ) ; } }
protected void loadConf ( Configuration conf ) { this . slop = normalizeSlop ( conf . getFloat ( "hbase.regions.slop" , getDefaultSlop ( ) ) ) ; this . rackManager = new RackManager ( conf ) ; useRegionFinder = conf . getBoolean ( "hbase.master.balancer.uselocality" , true ) ; code_block = IfStatement ; this . isByTable = conf . getBoolean ( HConstants . HBASE_MASTER_LOADBALANCE_BYTABLE , isByTable ) ; LOG . info ( "Using MASTER_MASTER_LOADBALANCE_BYTABLE: {}" , isByTable ) ; }
public void test() { if ( _logger . isDebugEnabled ( ) ) { _logger . debug ( "[" + _handler . getClass ( ) . getName ( ) + "] DirectPersistencyListHandler[" + toString ( ) + "]" ) ; } }
public void test() { if ( _logger . isDebugEnabled ( ) ) { _logger . debug ( "[" + _handler . getClass ( ) . getName ( ) + "] DirectPersistencyListHandler[" + toString ( ) + "]" ) ; } }
public void test() { if ( authenticator != null ) { authenticator . decorateProxyRequest ( clientRequest , proxyResponse , proxyRequest ) ; } else { LOGGER . error ( "No authenticator found for request {}" , proxyRequest . getRequestURL ( ) ) ; } }
public void test() { if ( applicationInstanceId == null ) { applicationInstanceId = this . applicationDescriptor . toString ( ) ; set ( AstrixSettings . APPLICATION_INSTANCE_ID , this . applicationDescriptor . toString ( ) ) ; Objects . requireNonNull ( AstrixSettings . APPLICATION_INSTANCE_ID . getFrom ( config ) . get ( ) ) ; log . info ( "ApplicationInstanceId={}" , applicationInstanceId ) ; } else { log . info ( "Current applicationInstanceId={}" , applicationInstanceId ) ; } }
public void test() { if ( applicationInstanceId == null ) { applicationInstanceId = this . applicationDescriptor . toString ( ) ; set ( AstrixSettings . APPLICATION_INSTANCE_ID , this . applicationDescriptor . toString ( ) ) ; log . info ( "No applicationInstanceId set, using name of ApplicationDescriptor as applicationInstanceId: {}" , applicationInstanceId ) ; Objects . requireNonNull ( AstrixSettings . APPLICATION_INSTANCE_ID . getFrom ( config ) . get ( ) ) ; } else { log . info ( "ApplicationInstanceId set to {}" , this . applicationDescriptor . toString ( ) ) ; } }
@ ExceptionHandler ( HttpMessageNotReadableException . class ) @ ResponseStatus ( HttpStatus . BAD_REQUEST ) @ ResponseBody public ValidationErrorResponse handleHttpMessageNotReadableException ( HttpMessageNotReadableException e ) { logger . error ( "request body is not readable" ) ; ValidationErrorResponse error = new ValidationErrorResponse ( ) ; error . getViolations ( ) . add ( new Violation ( "" , "request body is required" ) ) ; return error ; }
public void test() { try { HttpSession session = request . getSession ( ) ; SessionObject userSession = ( SessionObject ) session . getAttribute ( FdahpStudyDesignerConstants . SESSION_OBJECT ) ; userBO . setModifiedBy ( userSession . getUserId ( ) ) ; userBO . setModifiedOn ( FdahpStudyDesignerUtil . getCurrentDateTime ( ) ) ; userId = userSession . getUserId ( ) ; message = dashBoardAndProfileService . updateProfileDetails ( userBO , userId , userSession ) ; code_block = IfStatement ; code_block = IfStatement ; mav = new ModelAndView ( "redirect:/adminDashboard/viewUserDetails.do" ) ; } catch ( Exception e ) { logger . error ( "DashBoardAndProfileController - updateProfileDetails() - ERROR" , e ) ; } }
public void test() { if ( ex . getMessage ( ) . contains ( "Does Not Exist" ) ) { } else { LOGGER . info ( "Tried to delete a non-existing organization with id: {}" , id ) ; } }
public void test() { try { dev . getAdapter ( ) . removeDevice ( dev . getRawDevice ( ) ) ; } catch ( DBusException ex ) { code_block = IfStatement ; } catch ( RuntimeException ex ) { s_logger . warn ( "Exception removing from device" , ex ) ; } }
public void test() { if ( ! oldOrg . isEndorsementApproved ( ) && newOrg . isEndorsementApproved ( ) ) { LOG . info ( "Starting ingestion of all datasets in [{}]" , newOrg . getKey ( ) ) ; DatasetVisitor visitor = dataset code_block = LoopStatement ; ; visitOwnedDatasets ( newOrg . getKey ( ) , visitor ) ; } else-if ( occurrenceMutator . requiresUpdate ( oldOrg , newOrg ) && newOrg . getNumPublishedDatasets ( ) > 0 ) { LOG . info ( "Starting ingestion of all datasets of org [{}] because it has changed country from [{}] to [{}]" , newOrg . getKey ( ) , oldOrg . getCountry ( ) , newOrg . getCountry ( ) ) ; DatasetVisitor visitor = dataset code_block = LoopStatement ; ; visitOwnedDatasets ( newOrg . getKey ( ) , visitor ) ; } }
public void test() { try { messagePublisher . send ( new StartCrawlMessage ( dataset . getKey ( ) ) ) ; } catch ( IOException e ) { LOG . warn ( "Unable to send start crawl message {}" , dataset . getKey ( ) , e ) ; } }
public void test() { if ( ! oldOrg . isEndorsementApproved ( ) && newOrg . isEndorsementApproved ( ) ) { LOG . info ( "Starting crawl of all datasets for newly promoted org [{}]" , newOrg . getKey ( ) ) ; DatasetVisitor visitor = dataset code_block = LoopStatement ; ; visitOwnedDatasets ( newOrg . getKey ( ) , visitor ) ; } else-if ( occurrenceMutator . requiresUpdate ( oldOrg , newOrg ) && newOrg . getNumPublishedDatasets ( ) > 0 ) { LOG . info ( "Starting crawl of all datasets for new organisation [{}]" , newOrg . getKey ( ) ) ; DatasetVisitor visitor = dataset code_block = LoopStatement ; ; visitOwnedDatasets ( newOrg . getKey ( ) , visitor ) ; } }
public void test() { try { Class < CustomDatatype < ? > > datatypeClazz = ( Class < CustomDatatype < ? > > ) Class . forName ( datatypeClassname ) . asSubclass ( CustomDatatype . class ) ; globalProperty = new GlobalProperty ( property , defaultValue , description , datatypeClazz , datatypeConfig ) ; } catch ( ClassCastException ex ) { log . error ( "The property [" + property + "] is not a subclass of." , ex ) ; } catch ( ClassNotFoundException ex ) { log . error ( "The class specified by 'datatypeClassname'' (" + datatypeClassname + ") could not be found." , ex ) ; } }
public void test() { try { Class < CustomDatatype < ? > > datatypeClazz = ( Class < CustomDatatype < ? > > ) Class . forName ( datatypeClassname ) . asSubclass ( CustomDatatype . class ) ; globalProperty = new GlobalProperty ( property , defaultValue , description , datatypeClazz , datatypeConfig ) ; } catch ( ClassCastException ex ) { log . error ( "The class specified by 'datatypeClassname''' (" + datatypeClassname + ") must be a subtype of 'org.openmrs.customdatatype.CustomDatatype<?>'." , ex ) ; } catch ( ClassNotFoundException ex ) { log . error ( "The class specified by 'org.openmrs.customdatatype.CustomDatatype<?>'." , ex ) ; } }
@ Test public void pausedDoesNotAppend ( ) { alertAppender . pause ( ) ; logger . info ( alertAppender . getLogEvents ( ) ) ; assertThat ( alertAppender . getLogEvents ( ) ) . isEmpty ( ) ; }
public void test() { try { Timeout t = new Timeout ( Duration . create ( 10 , TimeUnit . SECONDS ) ) ; obj = Await . result ( Patterns . ask ( actorRef , request , t ) , t . duration ( ) ) ; } catch ( ProjectCommonException pce ) { throw pce ; } catch ( Exception e ) { logger . error ( context , "Unable to communicate with actor: Exception occurred with error message = " + e . getMessage ( ) , e ) ; ProjectCommonException . throwServerErrorException ( ResponseCode . unableToCommunicateWithActor , ResponseCode . unableToCommunicateWithActor . getErrorMessage ( ) ) ; } }
public void test() { if ( notifyAllNodes ) { logger . debug ( "Notifying all nodes from {} to {}" , currentStatus , status ) ; } else { logger . debug ( "Notifying cluster coordinator that node status changed from {} to {}" , currentStatus , status ) ; } }
public void test() { if ( notifyAllNodes ) { logger . debug ( "Notifying all nodes that status changed from {} to {}" , currentStatus , status ) ; } else { logger . debug ( "Notifying all nodes that status changed from {} to {}" , currentStatus , status ) ; } }
public void test() { if ( currentState == null || currentState != status . getState ( ) ) { final boolean notifyAllNodes = isActiveClusterCoordinator ( ) ; code_block = IfStatement ; notifyOthersOfNodeStatusChange ( status , notifyAllNodes , waitForCoordinator ) ; } else { LOG . debug ( "Current state is {} and current state is {}" , status . getState ( ) , currentState ) ; } }
public void test() { if ( resp . isFailed ( ) && resp . getFailure ( ) != null ) { final DocWriteRequest < ? > req = requests . get ( i ) ; final Failure failure = resp . getFailure ( ) ; getLogger ( ) . warn ( "Failed to write {}: {}" , req . toString ( ) , failure . getMessage ( ) ) ; } }
public void test() { try { ss = new ServerSocket ( port ) ; } catch ( IOException e ) { log . error ( "Failed to open server socket." , e ) ; System . exit ( - 1 ) ; } }
public void test() { try { String check = "<p><a href=\"smb://hoge/data\" title=\"UNCPathLink\" rel=\"nofollow\">UNCPathLink</a></p>" ; org . junit . Assert . assertEquals ( check , result ) ; } catch ( AssertionError e ) { LOG . info ( "Failure : " + e . getMessage ( ) ) ; LOG . info ( "[Base]   : " + base ) ; LOG . info ( "[Result] : " + result ) ; throw e ; } }
public void test() { try { String check = "<p><a href=\"smb://hoge/data\" title=\"UNCPathLink\" rel=\"nofollow\">UNCPathLink</a></p>" ; org . junit . Assert . assertEquals ( check , result ) ; } catch ( AssertionError e ) { LOG . info ( "Sanitize" ) ; LOG . info ( "[Result] : " + check ) ; LOG . info ( "[Result] : " + result ) ; throw e ; } }
public void test() { try { String check = "<p><a href=\"smb://hoge/data\" title=\"UNCPathLink\" rel=\"nofollow\">UNCPathLink</a></p>" ; org . junit . Assert . assertEquals ( check , result ) ; } catch ( AssertionError e ) { LOG . info ( "Sanitize" ) ; LOG . info ( "[Base]   : " + base ) ; LOG . info ( "[Base]   : " + base ) ; throw e ; } }
public void createAccountsAndTransfer ( CryptoServiceGrpc . CryptoServiceBlockingStub stub ) throws Exception { Path startUpAccountPathJson = getStartupPath ( ) ; String path = startUpAccountPathJson . toString ( ) ; hederaStartupAccount = getStartupAccountMap ( path ) ; AccountKeyListObj payerAccountDetails = getPayerAccount ( hederaStartupAccount ) ; AccountID nodeAccount = AccountID . newBuilder ( ) . setAccountNum ( 3 ) . setRealmNum ( 0 ) . setShardNum ( 0 ) . build ( ) ; KeyPair accountKeyPair = new KeyPairGenerator ( ) . generateKeyPair ( ) ; KeyPairObj genKeyPairObj = payerAccountDetails . getKeyPairList ( ) . get ( 0 ) ; PrivateKey genesisPrivateKey = genKeyPairObj . getPrivateKey ( ) ; KeyPair genKeyPair = new KeyPair ( genKeyPairObj . getPublicKey ( ) , genesisPrivateKey ) ; TestHelper . initializeFeeClient ( channel , payerAccountDetails . getAccountId ( ) , genKeyPair , nodeAccount ) ; Transaction transaction = TestHelper . createAccountWithFee ( payerAccountDetails . getAccountId ( ) , nodeAccount , accountKeyPair , 10000l , Collections . singletonList ( genesisPrivateKey ) ) ; TransactionResponse response = stub . createAccount ( transaction ) ; Assert . assertNotNull ( response ) ; Assert . assertEquals ( ResponseCodeEnum . OK , response . getNodeTransactionPrecheckCode ( ) ) ; log . info ( "Transaction created successfully." ) ; TransactionBody body = TransactionBody . parseFrom ( transaction . getBodyBytes ( ) ) ; AccountID newlyCreateAccountId1 = TestHelper . getTxReceipt ( body . getTransactionID ( ) , stub ) . getAccountID ( ) ; Assert . assertNotNull ( newlyCreateAccountId1 ) ; log . info ( "Account ID " + newlyCreateAccountId1 . getAccountNum ( ) + " created successfully." ) ; log . info ( "--------------------------------------" ) ; Transaction transfer = TestHelper . createTransferSigMap ( payerAccountDetails . getAccountId ( ) , genKeyPair , newlyCreateAccountId1 , payerAccountDetails .
public void createAccountsAndTransfer ( CryptoServiceGrpc . CryptoServiceBlockingStub stub ) throws Exception { Path startUpAccountPathJson = getStartupPath ( ) ; String path = startUpAccountPathJson . toString ( ) ; hederaStartupAccount = getStartupAccountMap ( path ) ; AccountKeyListObj payerAccountDetails = getPayerAccount ( hederaStartupAccount ) ; AccountID nodeAccount = AccountID . newBuilder ( ) . setAccountNum ( 3 ) . setRealmNum ( 0 ) . setShardNum ( 0 ) . build ( ) ; KeyPair accountKeyPair = new KeyPairGenerator ( ) . generateKeyPair ( ) ; KeyPairObj genKeyPairObj = payerAccountDetails . getKeyPairList ( ) . get ( 0 ) ; PrivateKey genesisPrivateKey = genKeyPairObj . getPrivateKey ( ) ; KeyPair genKeyPair = new KeyPair ( genKeyPairObj . getPublicKey ( ) , genesisPrivateKey ) ; TestHelper . initializeFeeClient ( channel , payerAccountDetails . getAccountId ( ) , genKeyPair , nodeAccount ) ; Transaction transaction = TestHelper . createAccountWithFee ( payerAccountDetails . getAccountId ( ) , nodeAccount , accountKeyPair , 10000l , Collections . singletonList ( genesisPrivateKey ) ) ; TransactionResponse response = stub . createAccount ( transaction ) ; Assert . assertNotNull ( response ) ; Assert . assertEquals ( ResponseCodeEnum . OK , response . getNodeTransactionPrecheckCode ( ) ) ; log . info ( "Pre Check Response of Create first account :: " + response . getNodeTransactionPrecheckCode ( ) . name ( ) ) ; TransactionBody body = TransactionBody . parseFrom ( transaction . getBodyBytes ( ) ) ; AccountID newlyCreateAccountId1 = TestHelper . getTxReceipt ( body . getTransactionID ( ) , stub ) . getAccountID ( ) ; Assert . assertNotNull ( newlyCreateAccountId1 ) ; log . info ( "--------------------------------------" ) ; Transaction transfer = TestHelper . createTransferSigMap ( payerAccountDetails . getAccountId ( ) , genKeyPair , newlyCreateAccountId1 , payerAccountDetails . getAccountId ( ) ) ;
public void createAccountsAndTransfer ( CryptoServiceGrpc . CryptoServiceBlockingStub stub ) throws Exception { Path startUpAccountPathJson = getStartupPath ( ) ; String path = startUpAccountPathJson . toString ( ) ; hederaStartupAccount = getStartupAccountMap ( path ) ; AccountKeyListObj payerAccountDetails = getPayerAccount ( hederaStartupAccount ) ; AccountID nodeAccount = AccountID . newBuilder ( ) . setAccountNum ( 3 ) . setRealmNum ( 0 ) . setShardNum ( 0 ) . build ( ) ; KeyPair accountKeyPair = new KeyPairGenerator ( ) . generateKeyPair ( ) ; KeyPairObj genKeyPairObj = payerAccountDetails . getKeyPairList ( ) . get ( 0 ) ; PrivateKey genesisPrivateKey = genKeyPairObj . getPrivateKey ( ) ; KeyPair genKeyPair = new KeyPair ( genKeyPairObj . getPublicKey ( ) , genesisPrivateKey ) ; TestHelper . initializeFeeClient ( channel , payerAccountDetails . getAccountId ( ) , genKeyPair , nodeAccount ) ; Transaction transaction = TestHelper . createAccountWithFee ( payerAccountDetails . getAccountId ( ) , nodeAccount , accountKeyPair , 10000l , Collections . singletonList ( genesisPrivateKey ) ) ; TransactionResponse response = stub . createAccount ( transaction ) ; Assert . assertNotNull ( response ) ; Assert . assertEquals ( ResponseCodeEnum . OK , response . getNodeTransactionPrecheckCode ( ) ) ; log . info ( "Pre Check Response of Create first account :: " + response . getNodeTransactionPrecheckCode ( ) . name ( ) ) ; TransactionBody body = TransactionBody . parseFrom ( transaction . getBodyBytes ( ) ) ; AccountID newlyCreateAccountId1 = TestHelper . getTxReceipt ( body . getTransactionID ( ) , stub ) . getAccountID ( ) ; Assert . assertNotNull ( newlyCreateAccountId1 ) ; log . info ( "Account ID " + newlyCreateAccountId1 . getAccountNum ( ) + " created successfully." ) ; Transaction transfer = TestHelper . createTransferSigMap ( payerAccountDetails . getAccountId ( ) , genKeyPair , null
public void createAccountsAndTransfer ( CryptoServiceGrpc . CryptoServiceBlockingStub stub ) throws Exception { Path startUpAccountPathJson = getStartupPath ( ) ; String path = startUpAccountPathJson . toString ( ) ; hederaStartupAccount = getStartupAccountMap ( path ) ; AccountKeyListObj payerAccountDetails = getPayerAccount ( hederaStartupAccount ) ; AccountID nodeAccount = AccountID . newBuilder ( ) . setAccountNum ( 3 ) . setRealmNum ( 0 ) . setShardNum ( 0 ) . build ( ) ; KeyPair accountKeyPair = new KeyPairGenerator ( ) . generateKeyPair ( ) ; KeyPairObj genKeyPairObj = payerAccountDetails . getKeyPairList ( ) . get ( 0 ) ; PrivateKey genesisPrivateKey = genKeyPairObj . getPrivateKey ( ) ; KeyPair genKeyPair = new KeyPair ( genKeyPairObj . getPublicKey ( ) , genesisPrivateKey ) ; TestHelper . initializeFeeClient ( channel , payerAccountDetails . getAccountId ( ) , genKeyPair , nodeAccount ) ; Transaction transaction = TestHelper . createAccountWithFee ( payerAccountDetails . getAccountId ( ) , nodeAccount , accountKeyPair , 10000l , Collections . singletonList ( genesisPrivateKey ) ) ; TransactionResponse response = stub . createAccount ( transaction ) ; Assert . assertNotNull ( response ) ; Assert . assertEquals ( ResponseCodeEnum . OK , response . getNodeTransactionPrecheckCode ( ) ) ; log . info ( "Pre Check Response of Create first account :: " + response . getNodeTransactionPrecheckCode ( ) . name ( ) ) ; TransactionBody body = TransactionBody . parseFrom ( transaction . getBodyBytes ( ) ) ; AccountID newlyCreateAccountId1 = TestHelper . getTxReceipt ( body . getTransactionID ( ) , stub ) . getAccountID ( ) ; Assert . assertNotNull ( newlyCreateAccountId1 ) ; log . info ( "Account ID " + newlyCreateAccountId1 . getAccountNum ( ) + " created successfully." ) ; log . info ( "--------------------------------------" ) ; Transaction transfer = TestHelper . createTransferSigMap ( payerAccountDetails . getAccount
public void createAccountsAndTransfer ( CryptoServiceGrpc . CryptoServiceBlockingStub stub ) throws Exception { Path startUpAccountPathJson = getStartupPath ( ) ; String path = startUpAccountPathJson . toString ( ) ; hederaStartupAccount = getStartupAccountMap ( path ) ; AccountKeyListObj payerAccountDetails = getPayerAccount ( hederaStartupAccount ) ; AccountID nodeAccount = AccountID . newBuilder ( ) . setAccountNum ( 3 ) . setRealmNum ( 0 ) . setShardNum ( 0 ) . build ( ) ; KeyPair accountKeyPair = new KeyPairGenerator ( ) . generateKeyPair ( ) ; KeyPairObj genKeyPairObj = payerAccountDetails . getKeyPairList ( ) . get ( 0 ) ; PrivateKey genesisPrivateKey = genKeyPairObj . getPrivateKey ( ) ; KeyPair genKeyPair = new KeyPair ( genKeyPairObj . getPublicKey ( ) , genesisPrivateKey ) ; TestHelper . initializeFeeClient ( channel , payerAccountDetails . getAccountId ( ) , genKeyPair , nodeAccount ) ; Transaction transaction = TestHelper . createAccountWithFee ( payerAccountDetails . getAccountId ( ) , nodeAccount , accountKeyPair , 10000l , Collections . singletonList ( genesisPrivateKey ) ) ; TransactionResponse response = stub . createAccount ( transaction ) ; Assert . assertNotNull ( response ) ; Assert . assertEquals ( ResponseCodeEnum . OK , response . getNodeTransactionPrecheckCode ( ) ) ; log . info ( "Pre Check Response of Create first account :: " + response . getNodeTransactionPrecheckCode ( ) . name ( ) ) ; TransactionBody body = TransactionBody . parseFrom ( transaction . getBodyBytes ( ) ) ; AccountID newlyCreateAccountId1 = TestHelper . getTxReceipt ( body . getTransactionID ( ) , stub ) . getAccountID ( ) ; Assert . assertNotNull ( newlyCreateAccountId1 ) ; log . info ( "Account ID " + newlyCreateAccountId1 . getAccountNum ( ) + " created successfully." ) ; log . info ( "--------------------------------------" ) ; Transaction transfer = TestHelper . createTransferSigMap ( payerAccountDetails . getAccount
public void createAccountsAndTransfer ( CryptoServiceGrpc . CryptoServiceBlockingStub stub ) throws Exception { Path startUpAccountPathJson = getStartupPath ( ) ; String path = startUpAccountPathJson . toString ( ) ; hederaStartupAccount = getStartupAccountMap ( path ) ; AccountKeyListObj payerAccountDetails = getPayerAccount ( hederaStartupAccount ) ; AccountID nodeAccount = AccountID . newBuilder ( ) . setAccountNum ( 3 ) . setRealmNum ( 0 ) . setShardNum ( 0 ) . build ( ) ; KeyPair accountKeyPair = new KeyPairGenerator ( ) . generateKeyPair ( ) ; KeyPairObj genKeyPairObj = payerAccountDetails . getKeyPairList ( ) . get ( 0 ) ; PrivateKey genesisPrivateKey = genKeyPairObj . getPrivateKey ( ) ; KeyPair genKeyPair = new KeyPair ( genKeyPairObj . getPublicKey ( ) , genesisPrivateKey ) ; TestHelper . initializeFeeClient ( channel , payerAccountDetails . getAccountId ( ) , genKeyPair , nodeAccount ) ; Transaction transaction = TestHelper . createAccountWithFee ( payerAccountDetails . getAccountId ( ) , nodeAccount , accountKeyPair , 10000l , Collections . singletonList ( genesisPrivateKey ) ) ; TransactionResponse response = stub . createAccount ( transaction ) ; Assert . assertNotNull ( response ) ; Assert . assertEquals ( ResponseCodeEnum . OK , response . getNodeTransactionPrecheckCode ( ) ) ; log . info ( "Pre Check Response of Create first account :: " + response . getNodeTransactionPrecheckCode ( ) . name ( ) ) ; TransactionBody body = TransactionBody . parseFrom ( transaction . getBodyBytes ( ) ) ; AccountID newlyCreateAccountId1 = TestHelper . getTxReceipt ( body . getTransactionID ( ) , stub ) . getAccountID ( ) ; Assert . assertNotNull ( newlyCreateAccountId1 ) ; log . info ( "Account ID " + newlyCreateAccountId1 . getAccountNum ( ) + " created successfully." ) ; log . info ( "--------------------------------------" ) ; Transaction transfer = TestHelper . createTransferSigMap ( payerAccountDetails . getAccount
public void createAccountsAndTransfer ( CryptoServiceGrpc . CryptoServiceBlockingStub stub ) throws Exception { Path startUpAccountPathJson = getStartupPath ( ) ; String path = startUpAccountPathJson . toString ( ) ; hederaStartupAccount = getStartupAccountMap ( path ) ; AccountKeyListObj payerAccountDetails = getPayerAccount ( hederaStartupAccount ) ; AccountID nodeAccount = AccountID . newBuilder ( ) . setAccountNum ( 3 ) . setRealmNum ( 0 ) . setShardNum ( 0 ) . build ( ) ; KeyPair accountKeyPair = new KeyPairGenerator ( ) . generateKeyPair ( ) ; KeyPairObj genKeyPairObj = payerAccountDetails . getKeyPairList ( ) . get ( 0 ) ; PrivateKey genesisPrivateKey = genKeyPairObj . getPrivateKey ( ) ; KeyPair genKeyPair = new KeyPair ( genKeyPairObj . getPublicKey ( ) , genesisPrivateKey ) ; TestHelper . initializeFeeClient ( channel , payerAccountDetails . getAccountId ( ) , genKeyPair , nodeAccount ) ; Transaction transaction = TestHelper . createAccountWithFee ( payerAccountDetails . getAccountId ( ) , nodeAccount , accountKeyPair , 10000l , Collections . singletonList ( genesisPrivateKey ) ) ; TransactionResponse response = stub . createAccount ( transaction ) ; Assert . assertNotNull ( response ) ; Assert . assertEquals ( ResponseCodeEnum . OK , response . getNodeTransactionPrecheckCode ( ) ) ; log . info ( "Pre Check Response of Create first account :: " + response . getNodeTransactionPrecheckCode ( ) . name ( ) ) ; TransactionBody body = TransactionBody . parseFrom ( transaction . getBodyBytes ( ) ) ; AccountID newlyCreateAccountId1 = TestHelper . getTxReceipt ( body . getTransactionID ( ) , stub ) . getAccountID ( ) ; Assert . assertNotNull ( newlyCreateAccountId1 ) ; log . info ( "Account ID " + newlyCreateAccountId1 . getAccountNum ( ) + " created successfully." ) ; log . info ( "--------------------------------------" ) ; Transaction transfer = TestHelper . createTransferSigMap ( payerAccountDetails . getAccount
public void test() { try { code_block = IfStatement ; } }
public void test() { try { conn = this . getConnection ( ) ; stat = conn . prepareStatement ( LOAD_BPMWIDGETINFOS_ID ) ; res = stat . executeQuery ( ) ; code_block = WhileStatement ; } catch ( Throwable t ) { _logger . error ( "Error loading BpmWidgetInfo list" , t ) ; throw new RuntimeException ( "Error loading BpmWidgetInfo list" , t ) ; } finally { closeDaoResources ( res , stat , conn ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( AclServiceException e ) { logger . error ( e . getMessage ( ) , e ) ; throw new WebApplicationException ( Response . Status . INTERNAL_SERVER_ERROR ) ; } }
@ Override public synchronized void start ( ) { code_block = ForStatement ; _isStarted = true ; logger . info ( "Started!" ) ; }
public void test() { if ( task == null ) { LOG . warn ( "Task is null" ) ; return - 1L ; } }
public void test() { try { DocumentDTO doc = ( DocumentDTO ) task ; CollectionDTO crisisDTO = remoteCrisisEJB . findCrisisByID ( crisisID ) ; doc . setCrisisDTO ( crisisDTO ) ; doc . setHasHumanLabels ( false ) ; DocumentDTO savedDoc = remoteDocumentEJB . addDocument ( doc ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Warning: " + e . getMessage ( ) ) ; } }
public int send ( @ NotNull RawMercuryRequest request , @ NotNull Callback callback ) throws IOException { LOG . trace ( "send {}" , request ) ; ByteArrayOutputStream bytesOut = new ByteArrayOutputStream ( ) ; DataOutputStream out = new DataOutputStream ( bytesOut ) ; int seq ; synchronized ( seqHolder ) code_block = "" ; out . writeShort ( ( short ) 4 ) ; out . writeInt ( seq ) ; out . writeByte ( 1 ) ; out . writeShort ( 1 + request . payload . length ) ; byte [ ] headerBytes = request . header . toByteArray ( ) ; out . writeShort ( headerBytes . length ) ; out . write ( headerBytes ) ; code_block = ForStatement ; Packet . Type cmd = Packet . Type . forMethod ( request . header . getMethod ( ) ) ; session . send ( cmd , bytesOut . toByteArray ( ) ) ; callbacks . put ( ( long ) seq , callback ) ; return seq ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
@ Override public void createOrReplaceFunction ( String schemaName , String functionName , Supplier < String > supplier ) { final String objectName = DataDefinitionUtil . getQualifiedName ( schemaName , functionName ) ; final StringBuilder ddl = new StringBuilder ( ) . append ( "CREATE OR REPLACE FUNCTION " ) . append ( objectName ) . append ( System . lineSeparator ( ) ) . append ( supplier . get ( ) ) ; LOGGER . info ( ddl ) ; final String ddlString = ddl . toString ( ) ; code_block = IfStatement ; runStatement ( ddlString ) ; }
@ Test public void applicationLoggerAboveLevelUnaffectedByLoweringLogLevelChanges ( ) { geodeConsoleAppender . clearLogEvents ( ) ; when ( config . getLogLevel ( ) ) . thenReturn ( FINE . intLevel ( ) ) ; configuration . configChanged ( ) ; applicationLogger . info ( logMessage ) ; assertThat ( geodeConsoleAppender . getLogEvents ( ) ) . hasSize ( 1 ) ; }
public void test() { try { MethodKey methodKey = new MethodKey ( BookmarksEntryServiceUtil . class , "addEntry" , _addEntryParameterTypes0 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , folderId , name , url , description , serviceContext ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . bookmarks . model . BookmarksEntry ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { syncUser = project . getRemoteStatus ( ) . getReadBy ( ) ; } catch ( NullPointerException e ) { logger . error ( e . getMessage ( ) ) ; } }
public void test() { try { final MimeMessage mimeMessage = this . javaMailSender . createMimeMessage ( ) ; final MimeMessageHelper message = new MimeMessageHelper ( mimeMessage , "UTF-8" ) ; message . setSubject ( messageSource . getMessage ( "email.syncexpired.subject" , new Object [ ] code_block = "" ; , locale ) ) ; message . setFrom ( serverEmail ) ; message . setTo ( syncUser . getEmail ( ) ) ; final String htmlContent = templateEngine . process ( SYNC_EXPIRED_TEMPLATE , ctx ) ; message . setText ( htmlContent , true ) ; javaMailSender . send ( mimeMessage ) ; message . setText ( htmlContent , true ) ; javaMailSender . send ( mimeMessage ) ; } catch ( Exception e ) { logger . error ( "Failed to send e-mail for sync failure." , e ) ; throw new MailSendException ( "Failed to send e-mail for sync failure." , e ) ; } }
public String createMessage ( Command c ) { String message = null ; String page = null ; String behavior = null ; String relay = null ; code_block = IfStatement ; code_block = IfStatement ; message = "GET /" + page + " HTTP 1.1\r\n\r\n" ; LOG . info ( message ) ; return ( message ) ; }
public CloudWithRelaysResponseDTO getCloudsWithExclusiveGatewayAndPublicRelays ( final String operator , final String name ) { logger . debug ( "getCloudsWithExclusiveGatewayAndPublicRelays started..." ) ; Assert . isTrue ( ! Utilities . isEmpty ( operator ) , "operator is null or empty" ) ; Assert . isTrue ( ! Utilities . isEmpty ( name ) , "name is null or empty" ) ; final UriComponents uri = getGatekeeperGetCloudUri ( operator , name ) ; final ResponseEntity < CloudWithRelaysResponseDTO > response = httpService . sendRequest ( uri , HttpMethod . GET , CloudWithRelaysResponseDTO . class ) ; return response . getBody ( ) ; }
public void test() { try { temp = fileIOOperations . fileRead ( fd , b , off , len ) ; } catch ( JargonException e ) { log . error ( "jargon exception reading file" , e ) ; throw new IOException ( e ) ; } }
protected void addContribution ( RepositoryDescriptor cdesc ) { log . info ( "addContribution" ) ; RepositoryDescriptor descriptor = getRepositoryDescriptor ( cdesc ) ; SQLRepositoryService sqlRepositoryService = Framework . getLocalService ( SQLRepositoryService . class ) ; sqlRepositoryService . addContribution ( descriptor ) ; }
public void test() { if ( dagNode == null ) { logger . debug ( "No dag node found in {}" , path ) ; return ; } }
public void test() { try { boolean isUpdated = updateJobState ( ) ; code_block = IfStatement ; } catch ( IOException e ) { LOG . error ( "Failed to update job state" , e ) ; } }
public void test() { if ( ! knownAliases . containsKey ( alias ) ) { LOG . warn ( "Ignoring duplicate alias: {}" , alias ) ; return ; } }
public void test() { if ( registeredCommands . containsKey ( alias ) ) { logger . info ( "alias {} already registered" , alias ) ; return ; } }
public void test() { try { code_block = IfStatement ; activityFieldStore . flush ( oldBatch . getUpdates ( ) ) ; } catch ( Exception ex ) { LOG . error ( ex . getMessage ( ) , ex ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { pd = getProjectDescriptor ( project ) ; } catch ( Exception e ) { logger . log ( Level . WARNING , "Failed to get project descriptor" , e ) ; } }
private void loadGraph ( String name , String path ) { this Graph graph = GraphFactory . open ( path ) ; this . graphs . put ( name , graph ) ; code_block = IfStatement ; LOG . info ( "Loaded graph: " + graph ) ; }
public void test() { if ( this . requireAuthentication ( ) && ! ( graph instanceof HugeGraphAuthProxy ) ) { LOG . warn ( "Graph " + graph . getId ( ) + " is required by HugeGraphAuthProxy" ) ; } }
public void test() { if ( checkResultIsEmpty ( result ) ) { initApp ( id , appName ) ; } else { logger . info ( appName + " app already exists" ) ; } }
public void test() { try { logger . info ( toString ( ) ) ; } catch ( NullPointerException ex ) { code_block = ForStatement ; throw ex ; } }
public void test() { if ( ! _suppressTruncateTable ) { truncateTableIfPossible ( conn , tableMetaList ) ; } else { logger . debug ( "Truncate table {}" , tableName ) ; } }
public void test() { if ( ! _suppressDropForeignKey ) { dropForeignKey ( conn , tableMetaList ) ; } else { _log . debug ( "Dropped foreign key" ) ; } }
public void test() { if ( ! _suppressDropTable ) { dropTable ( conn , tableMetaList ) ; } else { logger . debug ( "Dropped table: {}" , tableName ) ; } }
public void test() { if ( ! _suppressDropSequence ) { dropSequence ( conn , tableMetaList ) ; } else { LOG . debug ( "Dropping non-sequence: {}" , tableName ) ; } }
@ Override @ NonNullByDefault ( ) { continue ; }
protected final Connection allocateConnection ( ConnectionRequestInfo connRequestInfo ) throws ResourceException { Connection connection = ( Connection ) this . connectionManager . allocateConnection ( this . managedConnectionFactory , connRequestInfo ) ; LOG . debug ( "Created a new connection: {}" , connection ) ; return connection ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { if ( _log . isWarnEnabled ( ) ) { StringBundler sb = new StringBundler ( 14 ) ; sb . append ( "Unable to register service method {actionClass=" ) ; sb . append ( actionClass ) ; sb . append ( ", actionMethod=" ) ; sb . append ( actionMethod ) ; sb . append ( ", contextName=" ) ; sb . append ( contextName ) ; sb . append ( ", contextPath=" ) ; sb . append ( contextPath ) ; sb . append ( ", method=" ) ; sb . append ( method ) ; sb . append ( ", path=" ) ; sb . append ( path ) ; sb . append ( "} due to " ) ; sb . append ( exception . getMessage ( ) ) ; _log . warn ( sb . toString ( ) , exception ) ; } }
@ Override public XDRAcknowledgementType provideAndRegisterDocumentSetBAsyncResponse ( RegistryResponseType request , AssertionType assertion , NhinTargetCommunitiesType targets ) { LOG . trace ( "Entering XDRAcknowledgementTypeProvider. provideAndRegisterDocumentSetBAsyncResponse" ) ; XDRAcknowledgementType ack = new XDRAcknowledgementType ( ) ; RegistryResponseType regResp = new RegistryResponseType ( ) ; regResp . setStatus ( NhincConstants . XDR_RESP_ACK_STATUS_MSG ) ; ack . setMessage ( regResp ) ; return ack ; }
@ Override public void onSuccess ( TbQueueMsgMetadata metadata ) { log . trace ( "[{}] Request sent: {}" , logId , metadata ) ; }
public void test() { if ( resource == null ) { logger . warn ( "resource not found: " + resource ) ; return null ; } }
public void test() { if ( isReservedWord ( name ) ) { LOGGER . warn ( name + " (reserved word) cannot be used as model name. Renamed to " + ( "model_" + name ) ) ; name = "model_" + name ; } }
public void test() { if ( name . matches ( "^\\d.*" ) ) { LOGGER . warn ( name + " (model name starts with number) cannot be used as model name. Renamed to " + ( "model_" + name ) ) ; name = "model_" + name ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { if ( senseHatInterfaceRefCnt . getAndIncrement ( ) == 0 ) { logger . info ( "Opening Sense Hat..." ) ; senseHatInterface = new SenseHatInterface ( senseHat ) ; logger . info ( "Opening Sense Hat...done" ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { ScmConfig scmConfig = conf . getObject ( ScmConfig . class ) ; LOG . debug ( "Setting scmConfig " + scmConfig ) ; } }
public void test() { try { Collection < SupervisionEvent > allCurrentEvents = clientRequestHandler . getCurrentSupervisionStatus ( ) ; code_block = ForStatement ; c2monConnectionEstablished = true ; log . info ( "refreshSupervisionStatus() - supervision event cache was successfully updated with " + allCurrentEvents . size ( ) + " events." ) ; } catch ( Exception e ) { log . error ( "refreshSupervisionStatus() - ERROR" , e ) ; c2monConnectionEstablished = false ; } }
public void test() { try { writeJSON ( request , response , jsonObject ) ; } catch ( IOException e ) { log . error ( "Problem generating RequestStatus" , e ) ; } }
private void layoutAcyclicParts ( ) throws CDKException { logger . debug ( "Start of handleAliphatics" ) ; int safetyCounter = 0 ; IAtomContainer unplacedAtoms = null ; IAtomContainer placedAtoms = null ; IAtomContainer longestUnplacedChain = null ; IAtom atom = null ; Vector2d direction = null ; Vector2d startVector = null ; boolean done ; do code_block = "" ; while ( ! done && safetyCounter <= molecule . getAtomCount ( ) ) ; logger . debug ( "End of handleAliphatics" ) ; }
public void test() { if ( atom != null ) { unplacedAtoms = getUnplacedAtoms ( atom ) ; placedAtoms = getPlacedAtoms ( atom ) ; longestUnplacedChain = atomPlacer . getLongestUnplacedChain ( molecule , atom ) ; logger . debug ( "-----------" ) ; code_block = TryStatement ;  logger . debug ( "---end of longest unplaced chain---" ) ; code_block = IfStatement ; } else { done = true ; } }
public void test() { try { logger . debug ( AtomPlacer . listNumbers ( molecule , longestUnplacedChain ) ) ; } catch ( Exception exc ) { logger . error ( "An error occurred while trying to list the molecule." ) ; logger . debug ( exc ) ; } }
public void test() { try { logger . debug ( "Start at atom no. " + ( molecule . indexOf ( atom ) + 1 ) ) ; } catch ( Exception exc ) { logger . error ( "Problem in iteration" ) ; logger . debug ( exc ) ; } }
public void test() { try { logger . debug ( "Start at atom no. " + ( molecule . indexOf ( atom ) + 1 ) ) ; logger . debug ( AtomPlacer . listNumbers ( molecule , longestUnplacedChain ) ) ; } catch ( Exception exc ) { logger . error ( "Exception" , exc ) ; } }
public void test() { if ( atom != null ) { unplacedAtoms = getUnplacedAtoms ( atom ) ; placedAtoms = getPlacedAtoms ( atom ) ; longestUnplacedChain = atomPlacer . getLongestUnplacedChain ( molecule , atom ) ; logger . debug ( "---start of longest unplaced chain---" ) ; code_block = TryStatement ;  code_block = IfStatement ; } else { done = true ; logger . debug ( "---" ) ; } }
public void test() { if ( placedAtoms . getAtomCount ( ) > 1 ) { logger . debug ( "trying to place neighbors of atom " + ( molecule . indexOf ( atom ) + 1 ) ) ; atomPlacer . distributePartners ( atom , placedAtoms , GeometryUtil . get2DCenter ( placedAtoms ) , unplacedAtoms , bondLength ) ; direction = new Vector2d ( longestUnplacedChain . getAtom ( 1 ) . getPoint2d ( ) ) ; startVector = new Vector2d ( atom . getPoint2d ( ) ) ; direction . sub ( startVector ) ; logger . debug ( "Done placing neighbors of atom " + ( molecule . indexOf ( atom ) + 1 ) ) ; } else { logger . debug ( "Less than or equal one atoms placed already" ) ; logger . debug ( "Trying to get next bond vector." ) ; direction = atomPlacer . getNextBondVector ( atom , placedAtoms . getAtom ( 0 ) , GeometryUtil . get2DCenter ( molecule ) , true ) ; logger . debug ( "After or equal one atoms placed already" ) ; } }
public void test() { if ( placedAtoms . getAtomCount ( ) > 1 ) { logger . debug ( "More than one atoms placed already" ) ; atomPlacer . distributePartners ( atom , placedAtoms , GeometryUtil . get2DCenter ( placedAtoms ) , unplacedAtoms , bondLength ) ; direction = new Vector2d ( longestUnplacedChain . getAtom ( 1 ) . getPoint2d ( ) ) ; startVector = new Vector2d ( atom . getPoint2d ( ) ) ; direction . sub ( startVector ) ; logger . debug ( "Done placing neighbors of atom " + ( molecule . indexOf ( atom ) + 1 ) ) ; } else { logger . debug ( "Less than or equal one atoms placed already" ) ; logger . debug ( "Trying to get next bond vector." ) ; direction = atomPlacer . getNextBondVector ( atom , placedAtoms . getAtom ( 0 ) , GeometryUtil . get2DCenter ( molecule ) , true ) ; logger . debug ( "Done placement of atom " + direction ) ; } }
public void test() { if ( placedAtoms . getAtomCount ( ) > 1 ) { logger . debug ( "More than one atoms placed already" ) ; logger . debug ( "trying to place neighbors of atom " + ( molecule . indexOf ( atom ) + 1 ) ) ; atomPlacer . distributePartners ( atom , placedAtoms , GeometryUtil . get2DCenter ( placedAtoms ) , unplacedAtoms , bondLength ) ; direction = new Vector2d ( longestUnplacedChain . getAtom ( 1 ) . getPoint2d ( ) ) ; startVector = new Vector2d ( atom . getPoint2d ( ) ) ; direction . sub ( startVector ) ; logger . debug ( "completed" ) ; } else { logger . debug ( "Less than or equal one atoms placed already" ) ; logger . debug ( "Trying to get next bond vector." ) ; direction = atomPlacer . getNextBondVector ( atom , placedAtoms . getAtom ( 0 ) , GeometryUtil . get2DCenter ( molecule ) , true ) ; } }
public void test() { if ( placedAtoms . getAtomCount ( ) > 1 ) { logger . debug ( "More than one atoms placed already" ) ; logger . debug ( "trying to place neighbors of atom " + ( molecule . indexOf ( atom ) + 1 ) ) ; atomPlacer . distributePartners ( atom , placedAtoms , GeometryUtil . get2DCenter ( placedAtoms ) , unplacedAtoms , bondLength ) ; direction = new Vector2d ( longestUnplacedChain . getAtom ( 1 ) . getPoint2d ( ) ) ; startVector = new Vector2d ( atom . getPoint2d ( ) ) ; direction . sub ( startVector ) ; logger . debug ( "Done placing neighbors of atom " + ( molecule . indexOf ( atom ) + 1 ) ) ; } else { logger . debug ( "Less than or equal one atoms placed already" ) ; direction = atomPlacer . getNextBondVector ( atom , placedAtoms . getAtom ( 0 ) , GeometryUtil . get2DCenter ( molecule ) , true ) ; logger . debug ( "Done placement of atom " + direction ) ; } }
private void layoutAcyclicParts ( ) throws CDKException { logger . debug ( "Start of handleAliphatics" ) ; int safetyCounter = 0 ; IAtomContainer unplacedAtoms = null ; IAtomContainer placedAtoms = null ; IAtomContainer longestUnplacedChain = null ; IAtom atom = null ; Vector2d direction = null ; Vector2d startVector = null ; boolean done ; do code_block = "" ; while ( ! done && safetyCounter <= molecule . getAtomCount ( ) ) ; logger . debug ( "End of handleAliphatics" ) ; }
public void test() { try { AccountManager user = new AccountManager ( connection ) ; user . createAccount ( userName , password ) ; LOGGER . debug ( "XMPP user created" ) ; connection . login ( userName , password ) ; } catch ( XMPPException e ) { LOGGER . debug ( "XMPP user existing" ) ; connection . login ( userName , password ) ; } }
public void test() { try { AccountManager user = new AccountManager ( connection ) ; user . createAccount ( userName , password ) ; LOGGER . debug ( "XMPP user created" ) ; connection . login ( userName , password ) ; } catch ( XMPPException e ) { LOGGER . warn ( "XMPP user creation failed" , e ) ; connection . login ( userName , password ) ; } }
public void test() { if ( sessionIdObject == null ) { final String reason = "sid parameter in request is not valid. Logout is rejected. sid parameter in request can be skipped or otherwise valid value must be provided." ; log . error ( reason ) ; throw new WebApplicationException ( createErrorResponse ( postLogoutRedirectUri , EndSessionErrorResponseType . INVALID_GRANT_AND_SESSION , reason ) ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isErrorEnabled ( ) ) { log . error ( throwable . getMessage ( ) ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { final Set < String > names = properties . keySet ( ) ; LOG . debug ( "Set properties: {}" , names ) ; } }
public TMTextUnitVariant addTextUnitVariant ( long assetId , long localeId , String name , String content , String comment ) throws VirtualAssetRequiredException , VirutalAssetMissingTextUnitException { logger . debug ( "addTextUnitVariant" ) ; TextUnitSearcherParameters textUnitSearcherParameters = new TextUnitSearcherParameters ( ) ; textUnitSearcherParameters . setAssetId ( assetId ) ; textUnitSearcherParameters . setName ( name ) ; textUnitSearcherParameters . setSearchType ( SearchType . EXACT ) ; textUnitSearcherParameters . setUsedFilter ( UsedFilter . USED ) ; textUnitSearcherParameters . setLimit ( 1 ) ; List < TextUnitDTO > textUnitDTOs = textUnitSearcher . search ( textUnitSearcherParameters ) ; code_block = IfStatement ; code_block = IfStatement ; return tmService . addCurrentTMTextUnitVariant ( textUnitDTOs . get ( 0 ) . getTmTextUnitId ( ) , localeId , content ) ; }
public void test() { if ( textUnitDTOs . isEmpty ( ) ) { String msg = MessageFormat . format ( "Missing TmTextUnit for assetId: {0} and name: {1}" , assetId , name ) ; logger . debug ( msg ) ; throw new VirutalAssetMissingTextUnitException ( msg ) ; } }
@ GET @ Timed @ Path ( "{id}" ) @ Produces ( APPLICATION_JSON_WITH_CHARSET ) public String get ( @ Context GraphManager manager , @ PathParam ( "graph" ) String graph , @ PathParam ( "id" ) String id ) { LOG . debug ( "Graph [{}] get user: {}" , graph , id ) ; HugeGraph g = graph ( manager , graph ) ; HugeUser user = manager . authManager ( ) . getUser ( IdGenerator . of ( id ) ) ; return manager . serializer ( g ) . writeAuthElement ( user ) ; }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Refreshing." ) ; } }
public void testRelevanceHashMapInt2StringArrayWay ( ) throws Exception { logger . info ( "executing test case testRelevanceHashMapInt2StringArrayWay" ) ; String req = "{\"sort\":[\"_score\"],\"query\":{\"query_string\":{\"query\":\"\",\"relevance\":{\"model\":{\"function_params\":[\"_INNER_SCORE\",\"thisYear\",\"year\",\"goodYear\",\"mileageWeight\",\"mileage\",\"color\",\"yearcolor\",\"colorweight\",\"category\",\"categorycolor\"],\"facets\":{\"int\":[\"year\",\"mileage\"],\"string\":[\"color\",\"category\"],\"long\":[\"groupid\"]},\"function\":\"if(yearcolor.containsKey(year) && yearcolor.get(year).equals(color)) return 100000f; if(goodYear.contains(year)) return (float)Math.exp(2d);   if(year==thisYear) return 87f   ; return  _INNER_SCORE;\",\"variables\":{\"map_int_float\":[\"mileageWeight\"],\"map_int_string\":[\"yearcolor\"],\"set_int\":[\"goodYear\"],\"int\":[\"thisYear\"],\"map_string_float\":[\"colorweight\"],\"map_string_string\":[\"categorycolor\"]}},\"values\":{\"thisYear\":2001,\"yearcolor\":{\"value\":[\"red\"],\"key\":[1998]},\"mileageWeight\":{\"value\":[777.9,10.2],\"key\":[11400,11000]},\"colorweight\":{\"value\":[335.5],\"key\":[\"red\"]},\"goodYear\":[1996,1997],\"categorycolor\":{\"value\":[\"red\"],\"key\":[\"compact\"]}}}}},\"fetchStored\":false,\"from\":0,\"explain\":false,\"size\":6}" ; JSONObject res = search ( new JSONObject ( req ) ) ; assertEquals ( "numhits is wrong" , 15000 , res . getInt ( "numhits" ) ) ;
public void test() { if ( configs == null ) { LOG . warn ( "No knox configured for knox.url" ) ; } else { String knoxUrl = configs . get ( "knox.url" ) ; String knoxAdminUser = configs . get ( "username" ) ; String knoxAdminPassword = configs . get ( "password" ) ; knoxClient = new KnoxClient ( knoxUrl , knoxAdminUser , knoxAdminPassword ) ; } }
public void stop ( ) { logger . info ( "SmscStatProviderJmx Stopping ..." ) ; statsReporter . stop ( ) ; logger . info ( "SmscStatProviderJmx Stopped ..." ) ; }
public void stop ( ) { logger . info ( "SmscStatProviderJmx Stopping ..." ) ; statsReporter . stop ( ) ; logger . info ( "SmscStatProviderJmx Stopped." ) ; }
public void test() { try { lh = bkc . createLedger ( digestType , ledgerPassword ) ; ledgerId = lh . getId ( ) ; LedgerHandle lhOpen = bkc . openLedgerNoRecovery ( ledgerId , digestType , ledgerPassword ) ; LOG . debug ( "Checking that it is empty" ) ; long readLastConfirmed = lhOpen . readLastConfirmed ( ) ; assertTrue ( "Last confirmed has the wrong value" , readLastConfirmed == LedgerHandle . INVALID_ENTRY_ID ) ; LOG . debug ( "Going to write one entry" ) ; ByteBuffer entry = ByteBuffer . allocate ( 4 ) ; entry . putInt ( rng . nextInt ( maxInt ) ) ; entry . position ( 0 ) ; entries . add ( entry . array ( ) ) ; entriesSize . add ( entry . array ( ) . length ) ; lh . addEntry ( entry . array ( ) ) ; LOG . debug ( "Checking that it is still empty even after writing one entry" ) ; readLastConfirmed = lhOpen . readLastConfirmed ( ) ; assertTrue ( readLastConfirmed == LedgerHandle . INVALID_ENTRY_ID ) ; LOG . debug ( "Going to write one entry" ) ; entry = ByteBuffer . allocate ( 4 ) ; entry . putInt ( rng . nextInt ( maxInt ) ) ; entry . position ( 0 ) ; entries . add ( entry . array ( ) ) ; entriesSize . add ( entry . array ( ) . length ) ; lh . addEntry ( entry . array ( ) ) ; LOG . info ( "Checking that it has an entry" ) ; readLastConfirmed = lhOpen . readLastConfirmed ( ) ; assertTrue ( readLastConfirmed == 0L ) ; lh . close ( ) ; lhOpen . close ( ) ; } catch ( BKException e ) { LOG . error ( "Test failed" , e ) ; fail ( "Test failed due to BookKeeper exception" ) ; } catch ( InterruptedException e ) { LOG . error ( "Test failed" , e ) ; fail ( "Test failed due to interruption" ) ; } }
public void test() { try { lh = bkc . createLedger ( digestType , ledgerPassword ) ; ledgerId = lh . getId ( ) ; LOG . info ( "Ledger ID: " + lh . getId ( ) ) ; LedgerHandle lhOpen = bkc . openLedgerNoRecovery ( ledgerId , digestType , ledgerPassword ) ; long readLastConfirmed = lhOpen . readLastConfirmed ( ) ; assertTrue ( "Last confirmed has the wrong value" , readLastConfirmed == LedgerHandle . INVALID_ENTRY_ID ) ; LOG . debug ( "Going to write one entry" ) ; ByteBuffer entry = ByteBuffer . allocate ( 4 ) ; entry . putInt ( rng . nextInt ( maxInt ) ) ; entry . position ( 0 ) ; entries . add ( entry . array ( ) ) ; entriesSize . add ( entry . array ( ) . length ) ; lh . addEntry ( entry . array ( ) ) ; LOG . debug ( "Checking that it is still empty even after writing one entry" ) ; readLastConfirmed = lhOpen . readLastConfirmed ( ) ; assertTrue ( readLastConfirmed == LedgerHandle . INVALID_ENTRY_ID ) ; LOG . debug ( "Going to find one entry" ) ; entry = ByteBuffer . allocate ( 4 ) ; entry . putInt ( rng . nextInt ( maxInt ) ) ; entry . position ( 0 ) ; entries . add ( entry . array ( ) ) ; entriesSize . add ( entry . array ( ) . length ) ; lh . addEntry ( entry . array ( ) ) ; LOG . info ( "Checking that it has an entry" ) ; readLastConfirmed = lhOpen . readLastConfirmed ( ) ; assertTrue ( readLastConfirmed == 0L ) ; lh . close ( ) ; lhOpen . close ( ) ; } catch ( BKException e ) { LOG . error ( "Test failed" , e ) ; fail ( "Test failed due to BookKeeper exception" ) ; } catch ( InterruptedException e ) { LOG . error ( "Test failed" , e ) ; fail ( "Test failed due to interruption" ) ; } }
public void test() { try { lh = bkc . createLedger ( digestType , ledgerPassword ) ; ledgerId = lh . getId ( ) ; LOG . info ( "Ledger ID: " + lh . getId ( ) ) ; LedgerHandle lhOpen = bkc . openLedgerNoRecovery ( ledgerId , digestType , ledgerPassword ) ; LOG . debug ( "Checking that it is empty" ) ; long readLastConfirmed = lhOpen . readLastConfirmed ( ) ; assertTrue ( "Last confirmed has the wrong value" , readLastConfirmed == LedgerHandle . INVALID_ENTRY_ID ) ; ByteBuffer entry = ByteBuffer . allocate ( 4 ) ; entry . putInt ( rng . nextInt ( maxInt ) ) ; entry . position ( 0 ) ; entries . add ( entry . array ( ) ) ; entriesSize . add ( entry . array ( ) . length ) ; lh . addEntry ( entry . array ( ) ) ; LOG . debug ( "Checking that it is still empty even after writing one entry" ) ; readLastConfirmed = lhOpen . readLastConfirmed ( ) ; assertTrue ( readLastConfirmed == LedgerHandle . INVALID_ENTRY_ID ) ; entry = ByteBuffer . allocate ( 4 ) ; entry . putInt ( rng . nextInt ( maxInt ) ) ; entry . position ( 0 ) ; entries . add ( entry . array ( ) ) ; entriesSize . add ( entry . array ( ) . length ) ; lh . addEntry ( entry . array ( ) ) ; LOG . info ( "Checking that it has an entry" ) ; readLastConfirmed = lhOpen . readLastConfirmed ( ) ; assertTrue ( readLastConfirmed == 0L ) ; LOG . debug ( "Finished reading entry" ) ; lh . close ( ) ; lhOpen . close ( ) ; } catch ( BKException e ) { LOG . error ( "Test failed" , e ) ; fail ( "Test failed due to BookKeeper exception" ) ; } catch ( InterruptedException e ) { LOG . error ( "Test failed" , e ) ; fail ( "Test failed due to interruption" ) ; } }
public void test() { try { lh = bkc . createLedger ( digestType , ledgerPassword ) ; ledgerId = lh . getId ( ) ; LOG . info ( "Ledger ID: " + lh . getId ( ) ) ; LedgerHandle lhOpen = bkc . openLedgerNoRecovery ( ledgerId , digestType , ledgerPassword ) ; LOG . debug ( "Checking that it is empty" ) ; long readLastConfirmed = lhOpen . readLastConfirmed ( ) ; assertTrue ( "Last confirmed has the wrong value" , readLastConfirmed == LedgerHandle . INVALID_ENTRY_ID ) ; LOG . debug ( "Going to write one entry" ) ; ByteBuffer entry = ByteBuffer . allocate ( 4 ) ; entry . putInt ( rng . nextInt ( maxInt ) ) ; entry . position ( 0 ) ; entries . add ( entry . array ( ) ) ; entriesSize . add ( entry . array ( ) . length ) ; lh . addEntry ( entry . array ( ) ) ; readLastConfirmed = lhOpen . readLastConfirmed ( ) ; assertTrue ( readLastConfirmed == LedgerHandle . INVALID_ENTRY_ID ) ; LOG . debug ( "Going to write one entry" ) ; entry = ByteBuffer . allocate ( 4 ) ; entry . putInt ( rng . nextInt ( maxInt ) ) ; entry . position ( 0 ) ; entries . add ( entry . array ( ) ) ; entriesSize . add ( entry . array ( ) . length ) ; lh . addEntry ( entry . array ( ) ) ; LOG . info ( "Checking that it has an entry" ) ; readLastConfirmed = lhOpen . readLastConfirmed ( ) ; assertTrue ( readLastConfirmed == 0L ) ; lh . close ( ) ; lhOpen . close ( ) ; } catch ( BKException e ) { LOG . error ( "Test failed" , e ) ; fail ( "Test failed due to BookKeeper exception" ) ; } catch ( InterruptedException e ) { LOG . error ( "Test failed" , e ) ; fail ( "Test failed due to interruption" ) ; } }
public void test() { try { lh = bkc . createLedger ( digestType , ledgerPassword ) ; ledgerId = lh . getId ( ) ; LOG . info ( "Ledger ID: " + lh . getId ( ) ) ; LedgerHandle lhOpen = bkc . openLedgerNoRecovery ( ledgerId , digestType , ledgerPassword ) ; LOG . debug ( "Checking that it is empty" ) ; long readLastConfirmed = lhOpen . readLastConfirmed ( ) ; assertTrue ( "Last confirmed has the wrong value" , readLastConfirmed == LedgerHandle . INVALID_ENTRY_ID ) ; LOG . debug ( "Going to write one entry" ) ; ByteBuffer entry = ByteBuffer . allocate ( 4 ) ; entry . putInt ( rng . nextInt ( maxInt ) ) ; entry . position ( 0 ) ; entries . add ( entry . array ( ) ) ; entriesSize . add ( entry . array ( ) . length ) ; lh . addEntry ( entry . array ( ) ) ; LOG . debug ( "Checking that it is still empty even after writing one entry" ) ; readLastConfirmed = lhOpen . readLastConfirmed ( ) ; assertTrue ( readLastConfirmed == LedgerHandle . INVALID_ENTRY_ID ) ; LOG . debug ( "Going to write one entry" ) ; entry = ByteBuffer . allocate ( 4 ) ; entry . putInt ( rng . nextInt ( maxInt ) ) ; entry . position ( 0 ) ; entries . add ( entry . array ( ) ) ; entriesSize . add ( entry . array ( ) . length ) ; lh . addEntry ( entry . array ( ) ) ; readLastConfirmed = lhOpen . readLastConfirmed ( ) ; assertTrue ( readLastConfirmed == 0L ) ; lh . close ( ) ; lhOpen . close ( ) ; } catch ( BKException e ) { LOG . error ( "Test failed" , e ) ; fail ( "Test failed due to BookKeeper exception" ) ; } catch ( InterruptedException e ) { LOG . error ( "Test failed" , e ) ; fail ( "Test failed due to interruption" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( ! transactionCoordinatorBuilder . isJta ( ) ) { logger . info ( "Jta transaction coordinator is not Jta, not JTA" ) ; return false ; } }
public void test() { if ( transactionManager == null ) { log . error ( "The transaction manager is null [{}]" , transactionManager ) ; return false ; } }
@ Override public List < LwM2mObject > findLwM2mObjectPage ( TenantId tenantId , String sortProperty , String sortOrder , PageLink pageLink ) { logger . debug ( "findLwM2mObjectPage() - sortProperty: {} sortOrder: {}" , sortProperty , sortOrder ) ; validateId ( tenantId , INCORRECT_TENANT_ID + tenantId ) ; PageData < TbResource > resourcePageData = resourceService . findTenantResourcesByResourceTypeAndPageLink ( tenantId , ResourceType . LWM2M_MODEL , pageLink ) ; return resourcePageData . getData ( ) . stream ( ) . flatMap ( s -> Stream . ofNullable ( toLwM2mObject ( s , false ) ) ) . sorted ( getComparator ( sortProperty , sortOrder ) ) . collect ( Collectors . toList ( ) ) ; }
public void test() { try { result = eval . evaluate ( eventsPerStream , true , null ) ; } catch ( Throwable t ) { log . error ( "Error during test execution" , t ) ; fail ( ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( NumberFormatException nfe ) { log . error ( " Caused by: ." , nfe ) ; throw new SchemaGenerationException ( nfe ) ; } }
private Response getSamlpPostLogoutRequest ( String relayState , LogoutWrapper < LogoutRequest > logoutRequest ) throws SignatureException , WSSecurityException { LOG . debug ( "Invoking getSamLPPostLogoutRequest" ) ; String encodedSamlRequest = encodeSaml ( logoutRequest ) ; String singleLogoutLocation = idpMetadata . getSingleLogoutLocation ( ) ; String submitFormUpdated = String . format ( submitForm , singleLogoutLocation , SAML_REQUEST , encodedSamlRequest , relayState ) ; Response . ResponseBuilder ok = Response . ok ( submitFormUpdated ) ; return ok . build ( ) ; }
public void test() { try { client . createTable ( table . getTTable ( ) ) ; } catch ( NoSuchObjectException e ) { throw new HiveMetaStoreException ( "Hive table not found: " + table . getDbName ( ) + "." + table . getTableName ( ) ) ; } catch ( AlreadyExistsException e ) { LOG . info ( "Table already exists" , e ) ; } catch ( InvalidObjectException e ) { throw new HiveMetaStoreException ( "Invalid table" , e ) ; } }
public static void deleteMessage ( String relativeQueueUrl , String receiptHandle ) throws Exception { logger . debug ( "Deleting receipt handle " + receiptHandle ) ; long ts1 = System . currentTimeMillis ( ) ; code_block = IfStatement ; long ts2 = System . currentTimeMillis ( ) ; CMBControllerServlet . valueAccumulator . addToCounter ( AccumulatorName . CNSCQSTime , ts2 - ts1 ) ; }
public void test() { try { UserCache userCache = ( UserCache ) ctx . getBean ( "userCache" ) ; code_block = IfStatement ; } catch ( NoSuchBeanDefinitionException exc ) { log . error ( "Unable to find UserCache bean in bean cache" , exc ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Throwable e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { getModel ( ) . removeNode ( element ) ; } catch ( NullPointerException e ) { LOGGER . debug ( e ) ; } }
public void test() { if ( ! this . disposed ) { logger . warn ( "Already disposed" ) ; } }
public void test() { -> { LOGGER . info ( "Loaded {} results" , result . size ( ) ) ; } }
public void test() { try { Set < String > beanProps = new HashSet < String > ( ) ; PropertyDescriptor [ ] propDescriptors = getBeanInfo ( clazz ) . getPropertyDescriptors ( ) ; code_block = ForStatement ; return beanProps ; } catch ( IntrospectionException e ) { log . warn ( e . getMessage ( ) , e ) ; } }
public void test() { for ( String cache : pools ) { code_block = IfStatement ; } }
@ PUT @ Path ( "/changePassword" ) @ SubmarineApi public Response changePassword ( SysUser sysUser ) { LOG . info ( "changePassword invoked!" ) ; code_block = TryStatement ;  return new JsonResponse . Builder < > ( Response . Status . OK ) . success ( true ) . message ( "delete  user successfully!" ) . build ( ) ; }
public void test() { try { userService . changePassword ( sysUser ) ; } catch ( Exception e ) { LOG . error ( "delete user failed!" , e ) ; return new JsonResponse . Builder < > ( Response . Status . OK ) . success ( false ) . message ( "delete user failed!" ) . build ( ) ; } }
@ AfterClass public static void destroy ( ) throws IOException { LOG . info ( "Destroying embedded node" ) ; embeddedNode . stop ( ) ; FileUtils . deleteDirectory ( new File ( ".\\data" ) ) ; }
public void test() { while ( it . hasNext ( ) ) { Map . Entry entry = ( Map . Entry ) it . next ( ) ; Index index = ( Index ) entry . getValue ( ) ; logger . info ( "The partitioned index created on this region " + " " + index ) ; logger . info ( "Current number of buckets indexed : " + "" + ( ( PartitionedIndex ) index ) . getNumberOfIndexedBuckets ( ) ) ; } }
@ Override public Response toResponse ( JobRunningException jobRunningException ) { LOG . error ( "Job running" , jobRunningException ) ; return Response . status ( Status . INTERNAL_SERVER_ERROR ) . entity ( new JobRunningExceptionInfo ( jobRunningException ) ) . build ( ) ; }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "JDBC::{}" , sql ) ; } }
public String setNamespace ( String futureNamespace ) { String previousNamespace = namespace ; namespace = futureNamespace ; log . trace ( "Using namespace {}" , futureNamespace ) ; return previousNamespace ; }
public void test() { try { SocketChannel socketChannel = SocketChannel . open ( ) ; Socket socket = socketChannel . socket ( ) ; LOGGER . debug ( "Connected to address {}." , socket . getRemoteSocketAddress ( ) ) ; socket . connect ( address , CONNECT_TIMEOUT ) ; code_block = IfStatement ; LOGGER . debug ( "Connected to address {}." , socket . getRemoteSocketAddress ( ) ) ; ByteBuffer tokenBuffer = ByteBuffer . wrap ( token ) ; do code_block = "" ; while ( tokenBuffer . remaining ( ) > 0 ) ; LOGGER . debug ( "Exchanged token successfully" ) ; return new DaemonConnection ( socketChannel ) ; } catch ( DaemonException . ConnectException e ) { throw e ; } catch ( Exception e ) { throw new DaemonException . ConnectException ( String . format ( "Could not connect to server %s." , address ) , e ) ; } }
public void test() { try { LOGGER . debug ( "Trying to connect to address {}." , address ) ; SocketChannel socketChannel = SocketChannel . open ( ) ; Socket socket = socketChannel . socket ( ) ; LOGGER . debug ( "Connected to {}." , token ) ; socket . connect ( address , CONNECT_TIMEOUT ) ; code_block = IfStatement ; ByteBuffer tokenBuffer = ByteBuffer . wrap ( token ) ; do code_block = "" ; while ( tokenBuffer . remaining ( ) > 0 ) ; LOGGER . debug ( "Exchanged token successfully" ) ; return new DaemonConnection ( socketChannel ) ; } catch ( DaemonException . ConnectException e ) { throw e ; } catch ( Exception e ) { throw new DaemonException . ConnectException ( String . format ( "Could not connect to server %s." , address ) , e ) ; } }
public void test() { try { File file = new File ( ConfigCore . getKitodoConfigDirectory ( ) + "kitodo_exportXml.xml" ) ; code_block = IfStatement ; } catch ( ConfigurationException | RuntimeException e ) { log . error ( e . getMessage ( ) , e ) ; nss = new HashMap < > ( ) ; } }
public void test() { try { req . setCharacterEncoding ( "UTF-8" ) ; } catch ( UnsupportedEncodingException e ) { log . error ( "Encoding not supported" , e ) ; } }
public void test() { if ( debug ) { logger . debug ( "Expected error: " + e . getMessage ( ) ) ; } }
public void test() { if ( debug ) { logger . debug ( "Expected error: " + e . getMessage ( ) ) ; } }
public void test() { if ( debug ) { logger . debug ( "Expected error: " + e . getMessage ( ) ) ; } }
public void test() { if ( debug ) { logger . debug ( "Expected error: " + e . getMessage ( ) ) ; } }
public void test() { if ( router != null ) { Iterator < Map < QualifiedName , String > > matcher = router . matcher ( portalPath , req . getParameterMap ( ) ) ; boolean started = false ; boolean processed = false ; code_block = TryStatement ;  code_block = IfStatement ; } else { log . error ( "Registry is null" ) ; res . sendError ( HttpServletResponse . SC_INTERNAL_SERVER_ERROR ) ; } }
public void onDeviceConnected ( final NodeId nodeId ) { final DeviceMastership mastership = new DeviceMastership ( nodeId , reconciliationRegistry , clusterSingletonService ) ; deviceMasterships . put ( nodeId , mastership ) ; log . debug ( "Device connected for node {}" , nodeId ) ; }
public void test() { { LOGGER . error ( throwable . getMessage ( ) ) ; latch . countDown ( ) ; } }
public void exceptionCaught ( ChannelHandlerContext ctx , ExceptionEvent e ) throws Exception { LOGGER . error ( "exceptionCaught" , e ) ; }
public void test() { try ( InputStream inputStream = jarFile . getInputStream ( jarEntry ) ) { pomInformation . parsePom ( inputStream ) ; } catch ( IOException e ) { LOGGER . debug ( "Unable to parse file '" + jarFile . getName ( ) + "'" , e ) ; } }
public void test() { try { fieldAces . set ( acl , aces ) ; } catch ( Exception e ) { LOG . debug ( e . getMessage ( ) , e ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getCapabilities()" ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( DepotEntryGroupRelServiceUtil . class , "updateDDMStructuresAvailable" , _updateDDMStructuresAvailableParameterTypes5 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , depotEntryGroupRelId , ddmStructuresAvailable ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . depot . model . DepotEntryGroupRel ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( null == availableConnection ) { LOG . debug ( "No available connection available for table {}" , mTableName ) ; code_block = IfStatement ; availableConnection = new PooledKijiTable ( mTableFactory . openTable ( mTableName ) , this ) ; mPoolSize ++ ; code_block = IfStatement ; } else { LOG . debug ( "Cache hit for table {}" , mTableName ) ; } }
public void test() { if ( null == availableConnection ) { code_block = IfStatement ; LOG . debug ( "Cache miss for table {}" , mTableName ) ; availableConnection = new PooledKijiTable ( mTableFactory . openTable ( mTableName ) , this ) ; mPoolSize ++ ; code_block = IfStatement ; } else { LOG . debug ( "Using available connection {}" , availableConnection ) ; } }
@ Override @ AdapterDelegationEvent ( beforeBuilder = PRPAIN201305UV02EventDescriptionBuilder . class , afterReturningBuilder = PRPAIN201305UV02AdapterEventDescBuilder . class , serviceType = "Patient Discovery MPI" , version = "1.0" ) public PRPAIN201306UV02 findCandidates ( PRPAIN201305UV02 findCandidatesRequest , AssertionType assertion ) { LOG . trace ( "Using Mpi" ) ; return findCandidatesMpi ( findCandidatesRequest , assertion ) ; }
public void test() { for ( String cache : pools ) { code_block = IfStatement ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CPDefinitionServiceUtil . class , "updateSubscriptionInfo" , _updateSubscriptionInfoParameterTypes25 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , cpDefinitionId , subscriptionEnabled , subscriptionLength , subscriptionType , subscriptionTypeSettingsUnicodeProperties , maxSubscriptionCycles , serviceContext ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . commerce . product . model . CPDefinition ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public static void main ( String [ ] args ) { String path = Paths . get ( "." ) . toAbsolutePath ( ) . normalize ( ) . toString ( ) ; startupLogger . info ( path ) ; }
public void test() { try { MethodKey methodKey = new MethodKey ( BookmarksEntryServiceUtil . class , "getGroupEntriesCount" , _getGroupEntriesCountParameterTypes11 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( ( Integer ) returnObj ) . intValue ( ) ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { LOGGER . debug ( "Received message from OSGP-Core" ) ; final ObjectMessage objectMessage = ( ObjectMessage ) message ; final MessageProcessor processor = this . messageProcessorMap . getMessageProcessor ( objectMessage ) ; processor . processMessage ( objectMessage ) ; } catch ( final Exception e ) { LOGGER . error ( "Exception while handling a request from OSGP-Core: " , e ) ; } }
public void test() { try { LOGGER . info ( "Received message of type: {}" , message . getJMSType ( ) ) ; final ObjectMessage objectMessage = ( ObjectMessage ) message ; final MessageProcessor processor = this . messageProcessorMap . getMessageProcessor ( objectMessage ) ; processor . processMessage ( objectMessage ) ; } catch ( final Exception e ) { LOGGER . error ( "Exception: " , e ) ; } }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Caught exception while executing callback" , e ) ; } }
public void test() { try { LOGGER . info ( f1 . getAbsolutePath ( ) ) ; LOGGER . info ( f2 . getAbsolutePath ( ) ) ; final Diff diff = new AstComparator ( ) . compare ( f1 , f2 ) ; code_block = IfStatement ; } catch ( Exception e ) { e . printStackTrace ( ) ; LOGGER . error ( "Error when trying to compare " + f1 + " and " + f2 ) ; } }
public void test() { try { LOGGER . info ( f1 . getAbsolutePath ( ) ) ; LOGGER . info ( f2 . getAbsolutePath ( ) ) ; final Diff diff = new AstComparator ( ) . compare ( f1 , f2 ) ; code_block = IfStatement ; } catch ( Exception e ) { e . printStackTrace ( ) ; LOGGER . error ( "Error when trying to compare " + f1 + " and " + f2 ) ; } }
public void test() { try { LOGGER . info ( f1 . getAbsolutePath ( ) ) ; LOGGER . info ( f2 . getAbsolutePath ( ) ) ; LOGGER . info ( f2 . getAbsolutePath ( ) ) ; final Diff diff = new AstComparator ( ) . compare ( f1 , f2 ) ; code_block = IfStatement ; } catch ( Exception e ) { e . printStackTrace ( ) ; } }
public void test() { try { pair = pairRemoteService . mirrorSequencingObject ( pair ) ; syncSequencingObject ( pair , sample , fileStatus ) ; } catch ( Exception e ) { logger . error ( "Could not synchronize pair {}" , pair . getRemoteStatus ( ) . getURL ( ) , e ) ; throw new ProjectSynchronizationException ( "Could not synchronize pair " + pair . getRemoteStatus ( ) . getURL ( ) , e ) ; } }
public void test() { if ( consumerKafkaGroup != null ) { LOG . info ( "Shutting down consumerKafkaGroup" ) ; consumerKafkaGroup . setKafkaSourceState ( null ) ; consumerKafkaGroup . shutdown ( ) ; } }
public void test() { try { session . deleteData ( paths , time ) ; putBack ( session ) ; return ; } catch ( IoTDBConnectionException e ) { logger . warn ( "DeleteData failed" , e ) ; cleanSessionAndMayThrowConnectionException ( session , i , e ) ; } catch ( StatementExecutionException | RuntimeException e ) { putBack ( session ) ; throw e ; } }
public void test() { try { if ( previousConsentToView == null ) return ( getAllRemoteFacilities ( ) ) ; else return ( getPreviousConsentFacilities ( loggedInInfo ) ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; integratorServerError = true ; return ( null ) ; } }
public void test() { try { metrics . unregister ( ) ; LOGGER . info ( "SCMSecurityProtocolServer stopped." ) ; getRpcServer ( ) . stop ( ) ; } catch ( Exception ex ) { LOGGER . error ( "SCMSecurityProtocolServer stop failed." , ex ) ; } }
public void test() { try { LOGGER . info ( "Stopping the SCMSecurityProtocolServer." ) ; metrics . unregister ( ) ; getRpcServer ( ) . stop ( ) ; } catch ( Exception ex ) { LOGGER . error ( "Failed to stop SCMSecurityProtocolServer." , ex ) ; } }
public void test() { try { ctx . rollbackTransaction ( ) ; state . counter . rolled ++ ; logger . fatal ( name + ": Exception in txn " + state . counter , txnException ) ; } catch ( Exception rollException ) { state . counter . failedRollbacks ++ ; logger . fatal ( name + ": Exception in txn " + state . counter , rollException ) ; } }
public void test() { try { TransactionContext ctx = targetInstance . newTransactionContext ( ) ; code_block = TryStatement ;  } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; } finally { firstLock . unlock ( ) ; } }
public void test() { try { logger . fatal ( e ) ; } catch ( Throwable ignored ) { } }
public void test() { try { BatchStatement batch = new BatchStatement ( BatchStatement . Type . UNLOGGED ) ; code_block = ForStatement ; Session session = DatastaxIO . getSession ( ) ; session . execute ( batch ) ; } catch ( Exception ex ) { Instrumentation . markWriteError ( ) ; log . error ( "Exception during batch execution: " , ex ) ; } finally { ctx . stop ( ) ; } }
public void test() { try { Thread . sleep ( 2000 ) ; } catch ( InterruptedException e ) { logger . error ( "" , e ) ; } }
public void test() { try { groomServer . run ( ) ; driver . sendStatusUpdate ( TaskStatus . newBuilder ( ) . setTaskId ( task . getTaskId ( ) ) . setState ( TaskState . TASK_FINISHED ) . build ( ) ) ; code_block = TryStatement ;  driver . stop ( ) ; } catch ( Throwable t ) { log . error ( "Cannot run task." , t ) ; driver . stop ( ) ; System . exit ( 1 ) ; } }
public void test() { try { deleteWriteFile ( ) ; } catch ( IOException e ) { logger . warn ( "Failed to delete write file: {}" , e . getMessage ( ) ) ; } }
public void test() { try { persistDefaultAuthResources ( ) ; } catch ( AuthServerException e ) { logger . error ( "Unable to persist default auth resources" , e ) ; } }
public void test() { if ( this . gtidPositioning ) { LOGGER . info ( "replicator started at position: {}" , client . getBinlogFilename ( ) + ":" + client . getBinlogPosition ( ) ) ; client . setBinlogFilename ( "" ) ; client . setBinlogPosition ( 4L ) ; client . connect ( 5000 ) ; throw new ClientReconnectedException ( ) ; } else { LOGGER . warn ( "replicator stopped at position: {} -- restarting" , client . getBinlogFilename ( ) + ":" + client . getBinlogPosition ( ) ) ; client . connect ( 5000 ) ; } }
public void test() { if ( this . gtidPositioning ) { LOGGER . warn ( "replicator stopped at position: {} -- restarting" , client . getGtidSet ( ) ) ; client . setBinlogFilename ( "" ) ; client . setBinlogPosition ( 4L ) ; client . connect ( 5000 ) ; throw new ClientReconnectedException ( ) ; } else { LOGGER . info ( "replicator stopped at position: {}" , client . getGtidSet ( ) ) ; client . connect ( 5000 ) ; } }
@ Override public void initialize ( ) { HwSerialBridgeConfig configuration = getConfigAs ( HwSerialBridgeConfig . class ) ; serialPortName = configuration . getSerialPort ( ) ; updateTime = configuration . getUpdateTime ( ) ; code_block = IfStatement ; code_block = IfStatement ; logger . debug ( "Lutron HomeWorks RS232 Bridge Handler Initializing." ) ; logger . debug ( "   Serial Port: {}," , serialPortName ) ; logger . debug ( "   Baud:       {}," , baudRate ) ; logger . debug ( "   Baud:      {}," , updateTime ) ; scheduler . execute ( ( ) -> openConnection ( ) ) ; }
@ Override public void initialize ( ) { logger . debug ( "Initializing the Lutron HomeWorks RS232 bridge handler" ) ; HwSerialBridgeConfig configuration = getConfigAs ( HwSerialBridgeConfig . class ) ; serialPortName = configuration . getSerialPort ( ) ; updateTime = configuration . getUpdateTime ( ) ; code_block = IfStatement ; code_block = IfStatement ; logger . debug ( "   Serial Port: {}," , serialPortName ) ; logger . debug ( "   Baud:       {}," , baudRate ) ; logger . debug ( "   Baud:       {}," , updateTime ) ; scheduler . execute ( ( ) -> openConnection ( ) ) ; }
@ Override public void initialize ( ) { logger . debug ( "Initializing the Lutron HomeWorks RS232 bridge handler" ) ; HwSerialBridgeConfig configuration = getConfigAs ( HwSerialBridgeConfig . class ) ; serialPortName = configuration . getSerialPort ( ) ; updateTime = configuration . getUpdateTime ( ) ; code_block = IfStatement ; code_block = IfStatement ; logger . debug ( "Lutron HomeWorks RS232 Bridge Handler Initializing." ) ; logger . debug ( "   Baud:       {}," , baudRate ) ; logger . debug ( "   Baud: {}" , serialPortName ) ; scheduler . execute ( ( ) -> openConnection ( ) ) ; }
public void test() { if ( filter == null || filter . accept ( obj ) ) { handler . handle ( obj ) ; } else { LOG . debug ( "Skipping unknown filter {}" , filter ) ; } }
public void test() { if ( tokenCredential != null ) { LOGGER . debug ( "Connecting to " + endpoint + " using token credential." ) ; builder . credential ( tokenCredential ) ; } else-if ( ( connection . getClientId ( ) != null && StringUtils . isNotEmpty ( connection . getClientId ( ) ) ) && connection . getEndpoint ( ) != null ) { LOGGER . debug ( "Connecting to " + endpoint + " using Client ID from configuration file." ) ; ManagedIdentityCredentialBuilder micBuilder = new ManagedIdentityCredentialBuilder ( ) . clientId ( connection . getClientId ( ) ) ; builder . credential ( micBuilder . build ( ) ) ; } else-if ( StringUtils . isNotEmpty ( connection . getConnectionString ( ) ) ) { LOGGER . debug ( "Connecting to " + endpoint + " using Connecting String." ) ; builder . connectionString ( connection . getConnectionString ( ) ) ; } else-if ( connection . getEndpoint ( ) != null ) { LOGGER . debug ( "Connecting to " + endpoint + " using Azure System Assigned Identity or Azure User Assigned Identity." ) ; ManagedIdentityCredentialBuilder micBuilder = new ManagedIdentityCredentialBuilder ( ) ; builder . credential ( micBuilder . build ( ) ) ; } else { throw new IllegalArgumentException ( "No Configuration method was set for connecting to App Configuration" ) ; } }
public void test() { if ( tokenCredential != null ) { LOGGER . debug ( "Connecting to " + endpoint + " using AppConfigurationCredentialProvider." ) ; builder . credential ( tokenCredential ) ; } else-if ( ( connection . getClientId ( ) != null && StringUtils . isNotEmpty ( connection . getClientId ( ) ) ) && connection . getEndpoint ( ) != null ) { LOGGER . debug ( "Connecting to " + endpoint + " using Client ID." ) ; ManagedIdentityCredentialBuilder micBuilder = new ManagedIdentityCredentialBuilder ( ) . clientId ( connection . getClientId ( ) ) ; builder . credential ( micBuilder . build ( ) ) ; } else-if ( StringUtils . isNotEmpty ( connection . getConnectionString ( ) ) ) { LOGGER . debug ( "Connecting to " + endpoint + " using Connecting String." ) ; builder . connectionString ( connection . getConnectionString ( ) ) ; } else-if ( connection . getEndpoint ( ) != null ) { LOGGER . debug ( "Connecting to " + endpoint + " using Azure System Assigned Identity or Azure User Assigned Identity." ) ; ManagedIdentityCredentialBuilder micBuilder = new ManagedIdentityCredentialBuilder ( ) ; builder . credential ( micBuilder . build ( ) ) ; } else { throw new IllegalArgumentException ( "No Configuration method was set for connecting to App Configuration" ) ; } }
public void test() { if ( tokenCredential != null ) { LOGGER . debug ( "Connecting to " + endpoint + " using AppConfigurationCredentialProvider." ) ; builder . credential ( tokenCredential ) ; } else-if ( ( connection . getClientId ( ) != null && StringUtils . isNotEmpty ( connection . getClientId ( ) ) ) && connection . getEndpoint ( ) != null ) { LOGGER . debug ( "Connecting to " + endpoint + " using Client ID from configuration file." ) ; ManagedIdentityCredentialBuilder micBuilder = new ManagedIdentityCredentialBuilder ( ) . clientId ( connection . getClientId ( ) ) ; builder . credential ( micBuilder . build ( ) ) ; } else-if ( StringUtils . isNotEmpty ( connection . getConnectionString ( ) ) ) { LOGGER . debug ( "Connecting to " + endpoint + " using ConnectionString." ) ; builder . connectionString ( connection . getConnectionString ( ) ) ; } else-if ( connection . getEndpoint ( ) != null ) { LOGGER . debug ( "Connecting to " + endpoint + " using Azure System Assigned Identity or Azure User Assigned Identity." ) ; ManagedIdentityCredentialBuilder micBuilder = new ManagedIdentityCredentialBuilder ( ) ; builder . credential ( micBuilder . build ( ) ) ; } else { throw new IllegalArgumentException ( "No Configuration method was set for connecting to App Configuration" ) ; } }
public void test() { if ( tokenCredential != null ) { LOGGER . debug ( "Connecting to " + endpoint + " using AppConfigurationCredentialProvider." ) ; builder . credential ( tokenCredential ) ; } else-if ( ( connection . getClientId ( ) != null && StringUtils . isNotEmpty ( connection . getClientId ( ) ) ) && connection . getEndpoint ( ) != null ) { LOGGER . debug ( "Connecting to " + endpoint + " using Client ID from configuration file." ) ; ManagedIdentityCredentialBuilder micBuilder = new ManagedIdentityCredentialBuilder ( ) . clientId ( connection . getClientId ( ) ) ; builder . credential ( micBuilder . build ( ) ) ; } else-if ( StringUtils . isNotEmpty ( connection . getConnectionString ( ) ) ) { LOGGER . debug ( "Connecting to " + endpoint + " using Connecting String." ) ; builder . connectionString ( connection . getConnectionString ( ) ) ; } else-if ( connection . getEndpoint ( ) != null ) { LOGGER . debug ( "Connecting to " + endpoint + " using Azure Id from configuration file." ) ; ManagedIdentityCredentialBuilder micBuilder = new ManagedIdentityCredentialBuilder ( ) ; builder . credential ( micBuilder . build ( ) ) ; } else { throw new IllegalArgumentException ( "No Configuration method was set for connecting to App Configuration" ) ; } }
public boolean isServiceSettingTrue ( String serviceparameter ) { String value = getServiceSetting ( serviceparameter ) ; boolean isTrue = ( value != null ? value . equalsIgnoreCase ( "true" ) : false ) ; log . info ( "isServiceSettingTrue() => " + isTrue ) ; return isTrue ; }
public void test() { if ( map . containsKey ( key ) ) { log . warn ( "Duplicate key: " + key ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { File destination = new File ( chooser . getDirectory ( ) + chooser . getFile ( ) ) ; jTextFieldExportPath . setText ( destination . getAbsolutePath ( ) ) ; } catch ( Exception ex ) { LOG . error ( "" , ex ) ; } }
public void test() { try { jTextFieldExportPath . setText ( chooser . getSelectedFile ( ) . getAbsolutePath ( ) ) ; } catch ( Exception ex ) { LOG . error ( "Error" , ex ) ; } }
@ Override public void punsubscribeAll ( ) { logger . info ( "P punsubscribeAll called" ) ; }
public void test() { if ( ! ( target instanceof JavaBrooklynClassLoadingContext ) ) { log . warn ( "An installation of JavaBrooklynClassLoadingContext not supported by JavaBrooklynClassLoadingContext." ) ; } }
@ Override public Response setCollaborators ( CollaboratorsRepresentation collaborators , String entityId , String entityType ) { log . debug ( "Invoking setCollaborators" ) ; return this . setCollaborators ( collaborators . getCollaborators ( ) , entityId , entityType , true ) ; }
public void test() { try { jt . update ( "UPDATE config_info_beta SET content=?, md5 = ?, src_ip=?,src_user=?,gmt_modified=?,app_name=? WHERE " + "data_id=? AND group_id=? AND tenant_id=?" , configInfo . getContent ( ) , md5 , srcIp , srcUser , time , appNameTmp , configInfo . getDataId ( ) , configInfo . getGroup ( ) , tenantTmp ) ; } catch ( CannotGetJdbcConnectionException e ) { LogUtil . FATAL_LOG . error ( "[db-error] " + e . toString ( ) , e ) ; throw e ; } }
@ Override public void sendNCBIUploadExceptionEmail ( String adminEmailAddress , Exception rootCause , Long submissionId ) throws MailSendException { LOGGER . debug ( "Recieved notification with submissionId {}." , submissionId ) ; }
public void test() { if ( matrixPath . isEmpty ( ) || vectorPath . isEmpty ( ) || outputPath . isEmpty ( ) ) { LOG . info ( "No input path to " + vectorPath ) ; return ; } }
public void test() { if ( writeDelay > this . writeDelayLimit && currentTime - timestamps . lastChannelWriteAttempt ( ) > writeHangGracePeriod ) { final Optional < RntbdContext > rntbdContext = requestManager . rntbdContext ( ) ; final int pendingRequestCount = requestManager . pendingRequestCount ( ) ; logger . info ( "Write delay for {} ms, pending request count {}" , writeDelay , pendingRequestCount ) ; return promise . setSuccess ( Boolean . FALSE ) ; } }
public void test() { if ( readDelay > this . readDelayLimit && currentTime - timestamps . lastChannelWrite ( ) > readHangGracePeriod ) { final Optional < RntbdContext > rntbdContext = requestManager . rntbdContext ( ) ; final int pendingRequestCount = requestManager . pendingRequestCount ( ) ; logger . info ( "Subscribe request count {}" , pendingRequestCount ) ; return promise . setSuccess ( Boolean . FALSE ) ; } }
public void test() { try { return getConvertedPropertyValue ( securityContext , graphObject , key ) . getBytes ( Charset . forName ( "utf-8" ) ) . length ; } catch ( FrameworkException fex ) { logger . warn ( "" , fex ) ; } }
public void test() { try { c = getCachedClass ( this . className ) ; } catch ( ClassNotFoundException ex ) { LOG . error ( ex . getMessage ( ) , ex ) ; return ; } }
public void test() { try { s = newInstance ( c ) ; } catch ( IllegalArgumentException ex ) { LOGGER . log ( Level . SEVERE , "Could not create " + c , ex ) ; return ; } }
public void test() { try { InternalDataSerializer . _register ( s , false ) ; } catch ( IllegalArgumentException | IllegalStateException ex ) { logger . warn ( ex . getMessage ( ) ) ; } }
public void test() { try { InternalDataSerializer . register ( this . className , false , this . eventId , null , this . id ) ; } catch ( IllegalArgumentException | IllegalStateException ex ) { logger . warn ( "failed to register class {}" , this . className , ex ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { if ( ! metadataKeyResult . isComplete ( ) ) { LOGGER . warn ( "Metadata key {} was not complete properly" , metadataKeyResult . getPartialReason ( ) ) ; return failure ( newFailure ( ) . withMessage ( metadataKeyResult . getPartialReason ( ) ) . withFailureCode ( INVALID_METADATA_KEY ) . onComponent ( ) ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { if ( ! metadataKeyResult . isComplete ( ) ) { LOGGER . warn ( "Metadata key {} was not complete properly" , metadataKeyResult . getPartialReason ( ) ) ; return failure ( newFailure ( ) . withMessage ( metadataKeyResult . getPartialReason ( ) ) . withFailureCode ( INVALID_METADATA_KEY ) . onComponent ( ) ) ; } }
public void test() { if ( LOGGER . isWarnEnabled ( ) ) { LOGGER . warn ( "Unable to find parameter {}" , key ) ; } }
public void test() { if ( ! metadataKeyResult . isComplete ( ) ) { LOGGER . warn ( "Metadata key {} was not complete properly" , metadataKeyResult . getPartialReason ( ) ) ; return failure ( newFailure ( ) . withMessage ( metadataKeyResult . getPartialReason ( ) ) . withFailureCode ( INVALID_METADATA_KEY ) . onComponent ( ) ) ; } }
public void test() { if ( ! metadataKeyResult . isComplete ( ) ) { LOGGER . warn ( "Metadata key {} was not complete properly" , metadataKeyResult . getPartialReason ( ) ) ; return failure ( newFailure ( ) . withMessage ( metadataKeyResult . getPartialReason ( ) ) . withFailureCode ( INVALID_METADATA_KEY ) . onComponent ( ) ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { for ( String cache : pools ) { code_block = IfStatement ; } }
public static void log ( String s ) { log . info ( s ) ; }
public void test() { try { br = new BufferedReader ( new InputStreamReader ( new FileInputStream ( dispatchFile ) , "UTF-8" ) ) ; final String line = br . readLine ( ) ; return line ; } catch ( Exception continued ) { log . warn ( "Detected wrong command line, forwarding file: {}" , dispatchFile ) ; return defaultValue ; } finally { code_block = IfStatement ; } }
public void test() { if ( handler == null ) { logger . debug ( HANDLER_IS_NULL ) ; return ; } }
public void test() { try { code_block = ForStatement ; } catch ( Exception e ) { logger . error ( Messages . getInstance ( ) . getErrorString ( "Workspace.ERROR_0002_PROPS_EXCEPTION" ) , e ) ; } }
public void test() { try { m_transactedSession . rollback ( ) ; } catch ( final JMSException e ) { LOG . warn ( "UNRECOVERABLE ERROR, unable to rollback JMS connection." , e ) ; } }
public void test() { if ( ! maybeEndDate . isPresent ( ) ) { LOG . warn ( "Couldn't find end date for feed {}" , feed ) ; } }
@ Override public void initialize ( final Subject subject , final CallbackHandler callbackHandler , final Map < String , ? > sharedState , final Map < String , ? > options ) { super . initialize ( subject , callbackHandler , sharedState , options ) ; logger . debug ( "Initializing {}" , callbackHandler ) ; code_block = ForStatement ; code_block = IfStatement ; roleResolver = roleResolverFactory . createRoleResolver ( options ) ; logger . debug ( "Retrieved role resolver from factory: {}" , roleResolver ) ; searchRequest = roleResolverFactory . createSearchRequest ( options ) ; searchRequest . setReturnAttributes ( roleAttribute ) ; logger . debug ( "Retrieved search request from factory: {}" , searchRequest ) ; }
@ Override public void initialize ( final Subject subject , final CallbackHandler callbackHandler , final Map < String , ? > sharedState , final Map < String , ? > options ) { super . initialize ( subject , callbackHandler , sharedState , options ) ; code_block = ForStatement ; code_block = IfStatement ; logger . trace ( "roleResolverFactory = {}, roleFilter = {}, roleAttribute = {}, noResultsIsError = {}" , roleResolverFactory , roleFilter , Arrays . toString ( roleAttribute ) , noResultsIsError ) ; roleResolver = roleResolverFactory . createRoleResolver ( options ) ; logger . debug ( "Creating search request from factory: {}" , roleResolver ) ; searchRequest = roleResolverFactory . createSearchRequest ( options ) ; searchRequest . setReturnAttributes ( roleAttribute ) ; logger . debug ( "Retrieved search request from factory: {}" , searchRequest ) ; }
@ Override public void initialize ( final Subject subject , final CallbackHandler callbackHandler , final Map < String , ? > sharedState , final Map < String , ? > options ) { super . initialize ( subject , callbackHandler , sharedState , options ) ; code_block = ForStatement ; code_block = IfStatement ; logger . trace ( "roleResolverFactory = {}, roleFilter = {}, roleAttribute = {}, noResultsIsError = {}" , roleResolverFactory , roleFilter , Arrays . toString ( roleAttribute ) , noResultsIsError ) ; roleResolver = roleResolverFactory . createRoleResolver ( options ) ; logger . debug ( "Retrieved role resolver from factory: {}" , roleResolver ) ; searchRequest = roleResolverFactory . createSearchRequest ( options ) ; searchRequest . setReturnAttributes ( roleAttribute ) ; logger . trace ( "Search request for search request: {}" , searchRequest ) ; }
public void test() { try { Iterator < Record > result = this . neo4jConnectionManager . execute ( cypherQuery , interpreterContext ) . iterator ( ) ; Set < Node > nodes = new HashSet < > ( ) ; Set < Relationship > relationships = new HashSet < > ( ) ; List < String > columns = new ArrayList < > ( ) ; List < List < String > > lines = new ArrayList < List < String > > ( ) ; code_block = WhileStatement ; code_block = IfStatement ; } catch ( Exception e ) { LOGGER . error ( e . getMessage ( ) , e ) ; return new InterpreterResult ( Code . ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( _queryLogLevelInfo ) { _log . info ( sql ) ; } else { _log . debug ( sql ) ; } }
public void test() { if ( _queryLogLevelInfo ) { _log . info ( sql ) ; } else { _log . debug ( sql ) ; } }
private Location getVisionOffsets ( Head head , Location pickLocation ) throws Exception { Camera camera = null ; code_block = ForStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; head . moveToSafeZ ( ) ; Logger . debug ( "Move camera to pick location." ) ; camera . moveTo ( pickLocation ) ; VisionProvider visionProvider = camera . getVisionProvider ( ) ; Rectangle aoi = getVision ( ) . getAreaOfInterest ( ) ; Logger . debug ( "Perform template match." ) ; Point [ ] matchingPoints = visionProvider . locateTemplateMatches ( aoi . getX ( ) , aoi . getY ( ) , aoi . getWidth ( ) , aoi . getHeight ( ) , 0 , 0 , vision . getTemplateImage ( ) ) ; Logger . debug ( "Found match." ) ; Point match = matchingPoints [ 0 ] ; double imageWidth = camera . getWidth ( ) ; double imageHeight = camera . getHeight ( ) ; double templateWidth = vision . getTemplateImage ( ) . getWidth ( ) ; double templateHeight = vision . getTemplateImage ( ) . getHeight ( ) ; double matchX = match . x ; double matchY = match . y ; Logger . debug ( "matchX {}, matchY {}" , matchX , matchY ) ; matchX += ( templateWidth / 2 ) ; matchY += ( templateHeight / 2 ) ; Logger . debug ( "centered matchX {}, matchY {}" , matchX , matchY ) ; double offsetX = ( imageWidth / 2 ) - matchX ; double offsetY = ( imageHeight / 2 ) - matchY ; Logger . debug ( "offsetX {}, offsetY {}" , offsetX , offsetY ) ; offsetY *= - 1 ; Logger . debug ( "negated offsetX {}, offsetY {}" , offsetX , offsetY ) ; Location unitsPerPixel = camera . getUnitsPerPixel ( ) ; offsetX *= unitsPerPixel . getX ( ) ; offsetY *= unitsPerPixel . getY ( ) ; Logger . debug ( "final, in camera units offsetX {}, offsetY {}" , offsetX , offsetY ) ; return new Location ( unitsPerPixel . getUnits ( ) , offsetX , offsetY ,
private Location getVisionOffsets ( Head head , Location pickLocation ) throws Exception { Logger . debug ( "getVisionOffsets({}, {})" , head . getName ( ) , pickLocation ) ; Camera camera = null ; code_block = ForStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; head . moveToSafeZ ( ) ; camera . moveTo ( pickLocation ) ; VisionProvider visionProvider = camera . getVisionProvider ( ) ; Rectangle aoi = getVision ( ) . getAreaOfInterest ( ) ; Logger . debug ( "Perform template match." ) ; Point [ ] matchingPoints = visionProvider . locateTemplateMatches ( aoi . getX ( ) , aoi . getY ( ) , aoi . getWidth ( ) , aoi . getHeight ( ) , 0 , 0 , vision . getTemplateImage ( ) ) ; Logger . debug ( "Found match." ) ; Point match = matchingPoints [ 0 ] ; double imageWidth = camera . getWidth ( ) ; double imageHeight = camera . getHeight ( ) ; double templateWidth = vision . getTemplateImage ( ) . getWidth ( ) ; double templateHeight = vision . getTemplateImage ( ) . getHeight ( ) ; double matchX = match . x ; double matchY = match . y ; Logger . debug ( "matchX {}, matchY {}" , matchX , matchY ) ; matchX += ( templateWidth / 2 ) ; matchY += ( templateHeight / 2 ) ; Logger . debug ( "centered matchX {}, matchY {}" , matchX , matchY ) ; double offsetX = ( imageWidth / 2 ) - matchX ; double offsetY = ( imageHeight / 2 ) - matchY ; Logger . debug ( "offsetX {}, offsetY {}" , offsetX , offsetY ) ; offsetY *= - 1 ; Logger . debug ( "negated offsetX {}, offsetY {}" , offsetX , offsetY ) ; Location unitsPerPixel = camera . getUnitsPerPixel ( ) ; offsetX *= unitsPerPixel . getX ( ) ; offsetY *= unitsPerPixel . getY ( ) ; Logger . debug ( "final, in camera units offsetX {}, offsetY {}" , offsetX , offsetY ) ; return new Location ( unitsPerPixel .
private Location getVisionOffsets ( Head head , Location pickLocation ) throws Exception { Logger . debug ( "getVisionOffsets({}, {})" , head . getName ( ) , pickLocation ) ; Camera camera = null ; code_block = ForStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; head . moveToSafeZ ( ) ; Logger . debug ( "Move camera to pick location." ) ; camera . moveTo ( pickLocation ) ; VisionProvider visionProvider = camera . getVisionProvider ( ) ; Rectangle aoi = getVision ( ) . getAreaOfInterest ( ) ; Point [ ] matchingPoints = visionProvider . locateTemplateMatches ( aoi . getX ( ) , aoi . getY ( ) , aoi . getWidth ( ) , aoi . getHeight ( ) , 0 , 0 , vision . getTemplateImage ( ) ) ; Point match = matchingPoints [ 0 ] ; double imageWidth = camera . getWidth ( ) ; double imageHeight = camera . getHeight ( ) ; double templateWidth = vision . getTemplateImage ( ) . getWidth ( ) ; double templateHeight = vision . getTemplateImage ( ) . getHeight ( ) ; double matchX = match . x ; double matchY = match . y ; Logger . debug ( "matchX {}, matchY {}" , matchX , matchY ) ; matchX += ( templateWidth / 2 ) ; matchY += ( templateHeight / 2 ) ; Logger . debug ( "centered matchX {}, matchY {}" , matchX , matchY ) ; double offsetX = ( imageWidth / 2 ) - matchX ; double offsetY = ( imageHeight / 2 ) - matchY ; Logger . debug ( "offsetX {}, offsetY {}" , offsetX , offsetY ) ; offsetY *= - 1 ; Logger . debug ( "negated offsetX {}, offsetY {}" , offsetX , offsetY ) ; Location unitsPerPixel = camera . getUnitsPerPixel ( ) ; offsetX *= unitsPerPixel . getX ( ) ; offsetY *= unitsPerPixel . getY ( ) ; Logger . debug ( "final, in camera units offsetX {}, offsetY {}" , offsetX , offsetY ) ; return new Location ( unitsPerPixel . getUnits ( ) , offsetX *=
private Location getVisionOffsets ( Head head , Location pickLocation ) throws Exception { Logger . debug ( "getVisionOffsets({}, {})" , head . getName ( ) , pickLocation ) ; Camera camera = null ; code_block = ForStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; head . moveToSafeZ ( ) ; Logger . debug ( "Move camera to pick location." ) ; camera . moveTo ( pickLocation ) ; Logger . debug ( "Perform template match." ) ; Point [ ] matchingPoints = camera . getWidth ( ) ; double imageHeight = camera . getHeight ( ) ; double templateWidth = vision . getTemplateImage ( ) . getWidth ( ) ; double templateHeight = vision . getTemplateImage ( ) . getHeight ( ) ; Logger . debug ( "Perform match." ) ; Point match = matchingPoints [ 0 ] ; double imageWidth = camera . getWidth ( ) ; double imageHeight = camera . getHeight ( ) ; double templateWidth = vision . getTemplateImage ( ) . getWidth ( ) ; double templateHeight = vision . getTemplateImage ( ) . getHeight ( ) ; double matchX = match . x ; double matchY = match . y ; matchX += ( templateWidth / 2 ) ; matchY += ( templateHeight / 2 ) ; Logger . debug ( "centered matchX {}, matchY {}" , matchX , matchY ) ; double offsetX = ( imageWidth / 2 ) - matchX ; double offsetY = ( imageHeight / 2 ) - matchY ; Logger . debug ( "offsetX {}, offsetY {}" , offsetX , offsetY ) ; Logger . debug ( "offsetX {}, offsetY {}" , offsetX , offsetY ) ; offsetY *= - 1 ; Logger . debug ( "negated offsetX {}, offsetY {}" , offsetX , offsetY ) ; Location unitsPerPixel = camera . getUnitsPerPixel ( ) ; offsetX *= unitsPerPixel . getX ( ) ; offsetY *= unitsPerPixel . getY ( ) ; Logger . debug ( "final, in camera units offsetX {}, offsetY {}" , offsetX , offsetY ) ; return new Location ( unitsPerPixel . getUnits ( ) , offsetX , offsetY , 0 , 0 )
private Location getVisionOffsets ( Head head , Location pickLocation ) throws Exception { Logger . debug ( "getVisionOffsets({}, {})" , head . getName ( ) , pickLocation ) ; Camera camera = null ; code_block = ForStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; head . moveToSafeZ ( ) ; Logger . debug ( "Move camera to pick location." ) ; camera . moveTo ( pickLocation ) ; Logger . debug ( "Perform template match." ) ; Point [ ] matchingPoints = camera . getWidth ( ) ; double imageHeight = camera . getHeight ( ) ; double templateWidth = vision . getTemplateImage ( ) . getWidth ( ) ; double templateHeight = vision . getTemplateImage ( ) . getHeight ( ) ; Logger . debug ( "Perform template match." ) ; Point match = matchingPoints [ 0 ] ; double imageWidth = camera . getWidth ( ) ; double imageHeight = camera . getHeight ( ) ; double templateWidth = vision . getTemplateImage ( ) . getWidth ( ) ; double templateHeight = vision . getTemplateImage ( ) . getHeight ( ) ; double matchX = match . x ; double matchY = match . y ; Logger . debug ( "matchX {}, matchY {}" , matchX , matchY ) ; matchX += ( templateWidth / 2 ) ; matchY += ( templateWidth / 2 ) ; matchY += ( templateHeight / 2 ) ; double offsetX = ( imageWidth / 2 ) - matchX ; double offsetY = ( imageHeight / 2 ) - matchY ; Logger . debug ( "offsetX {}, offsetY {}" , offsetX , offsetY ) ; Logger . debug ( "offsetX {}, offsetY {}" , offsetX , offsetY ) ; offsetY *= - 1 ; Logger . debug ( "negated offsetX {}, offsetY {}" , offsetX , offsetY ) ; Location unitsPerPixel = camera . getUnitsPerPixel ( ) ; offsetX *= unitsPerPixel . getX ( ) ; offsetY *= unitsPerPixel . getY ( ) ; Logger . debug ( "final, in camera units offsetX {}, offsetY {}" , offsetX , offsetY ) ; return new Location ( unitsPerPixel . getUnits ( ) ,
private Location getVisionOffsets ( Head head , Location pickLocation ) throws Exception { Logger . debug ( "getVisionOffsets({}, {})" , head . getName ( ) , pickLocation ) ; Camera camera = null ; code_block = ForStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; head . moveToSafeZ ( ) ; Logger . debug ( "Move camera to pick location." ) ; camera . moveTo ( pickLocation ) ; Logger . debug ( "Perform template match." ) ; Point [ ] matchingPoints = camera . getWidth ( ) ; double imageHeight = camera . getHeight ( ) ; double templateWidth = vision . getTemplateImage ( ) . getWidth ( ) ; double templateHeight = vision . getTemplateImage ( ) . getHeight ( ) ; Logger . debug ( "Perform template match." ) ; Point match = matchingPoints [ 0 ] ; double imageWidth = camera . getWidth ( ) ; double imageHeight = camera . getHeight ( ) ; double templateWidth = vision . getTemplateImage ( ) . getWidth ( ) ; double templateHeight = vision . getTemplateImage ( ) . getHeight ( ) ; double matchY += ( templateHeight / 2 ) ; Logger . debug ( "Perform match." ) ; double offsetX = match . y ; double matchY = match . y ; Logger . debug ( "matchX {}, matchY {}" , matchX , matchY ) ; matchX += ( templateWidth / 2 ) ; matchY += ( templateHeight / 2 ) ; Logger . debug ( "centered matchX {}, matchY {}" , matchX , matchY ) ; double offsetX = ( imageWidth / 2 ) - matchX ; double offsetY = ( imageHeight / 2 ) - matchY ; offsetY *= - 1 ; Logger . debug ( "negated offsetX {}, offsetY {}" , offsetX , offsetY ) ; Location unitsPerPixel = camera . getUnitsPerPixel ( ) ; offsetX *= unitsPerPixel . getX ( ) ; offsetY *= unitsPerPixel . getY ( ) ; Logger . debug ( "final, in camera units offsetX {}, offsetY {}" , offsetX , offsetY ) ; return new Location ( unitsPerPixel . getUnits ( ) , offsetX , offsetY , 0 ,
private Location getVisionOffsets ( Head head , Location pickLocation ) throws Exception { Logger . debug ( "getVisionOffsets({}, {})" , head . getName ( ) , pickLocation ) ; Camera camera = null ; code_block = ForStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; head . moveToSafeZ ( ) ; Logger . debug ( "Move camera to pick location." ) ; camera . moveTo ( pickLocation ) ; Logger . debug ( "Perform template match." ) ; Point [ ] matchingPoints = camera . getWidth ( ) ; double imageHeight = camera . getHeight ( ) ; double templateWidth = vision . getTemplateImage ( ) . getWidth ( ) ; double templateHeight = vision . getTemplateImage ( ) . getHeight ( ) ; Logger . debug ( "Perform template match." ) ; Point match = matchingPoints [ 0 ] ; double imageWidth = camera . getWidth ( ) ; double imageHeight = camera . getHeight ( ) ; double templateWidth = vision . getTemplateImage ( ) . getWidth ( ) ; double templateHeight = vision . getTemplateImage ( ) . getHeight ( ) ; double matchY += ( templateHeight / 2 ) ; Logger . debug ( "Matching match." ) ; double offsetX = match . y ; double matchY = match . y ; Logger . debug ( "matchX {}, matchY {}" , matchX , matchY ) ; matchX += ( templateWidth / 2 ) ; matchY += ( templateHeight / 2 ) ; Logger . debug ( "centered matchX {}, matchY {}" , matchX , matchY ) ; double offsetX = ( imageWidth / 2 ) - matchX ; double offsetY = ( imageHeight / 2 ) - matchY ; Logger . debug ( "offsetX {}, offsetY {}" , offsetX , offsetY ) ; offsetY *= - 1 ; Location unitsPerPixel = camera . getUnitsPerPixel ( ) ; offsetX *= unitsPerPixel . getX ( ) ; offsetY *= unitsPerPixel . getY ( ) ; Logger . debug ( "final, in camera units offsetX {}, offsetY {}" , offsetX , offsetY ) ; return new Location ( unitsPerPixel . getUnits ( ) , offsetX , offsetY , 0 , 0 )
private Location getVisionOffsets ( Head head , Location pickLocation ) throws Exception { Logger . debug ( "getVisionOffsets({}, {})" , head . getName ( ) , pickLocation ) ; Camera camera = null ; code_block = ForStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; head . moveToSafeZ ( ) ; Logger . debug ( "Move camera to pick location." ) ; camera . moveTo ( pickLocation ) ; Logger . debug ( "Perform template match." ) ; Point [ ] matchingPoints = camera . getWidth ( ) ; double imageHeight = camera . getHeight ( ) ; double templateWidth = vision . getTemplateImage ( ) . getWidth ( ) ; double templateHeight = vision . getTemplateImage ( ) . getHeight ( ) ; Logger . debug ( "Perform template match." ) ; Point match = matchingPoints [ 0 ] ; double imageWidth = camera . getWidth ( ) ; double imageHeight = camera . getHeight ( ) ; double templateWidth = vision . getTemplateImage ( ) . getWidth ( ) ; double templateHeight = vision . getTemplateImage ( ) . getHeight ( ) ; double matchY += ( templateHeight / 2 ) ; Logger . debug ( "Matching image width of image width {} and template image height {}" , imageHeight , imageHeight ) ; Logger . debug ( "centered matchX {}, matchY {}" , matchX , matchY ) ; double matchX = match . x ; double matchY = match . y ; Logger . debug ( "matchX {}, matchY {}" , matchX , matchY ) ; matchX += ( templateWidth / 2 ) ; double offsetY = ( imageHeight / 2 ) - matchY ; Logger . debug ( "offsetX {}, offsetY {}" , offsetX , offsetY ) ; offsetY *= - 1 ; Logger . debug ( "negated offsetX {}, offsetY {}" , offsetX , offsetY ) ; Location unitsPerPixel = camera . getUnitsPerPixel ( ) ; offsetX *= unitsPerPixel . getX ( ) ; offsetY *= unitsPerPixel . getY ( ) ; return new Location ( unitsPerPixel . getUnits ( ) , offsetX , offsetY , 0 , 0 ) ; }
public void test() { try { validatorResults = validator . validate ( ) ; } catch ( ValidatorException e ) { log . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { channel . close ( ) ; } catch ( IOException e ) { log . error ( "Error closing channel" , e ) ; } }
public void test() { if ( ! routingContext . response ( ) . closed ( ) && ! routingContext . response ( ) . ended ( ) ) { routingContext . response ( ) . setStatusCode ( statusCode ) ; log . debug ( "[{}] Response: {}" , routingContext . get ( "request-id" ) , filename ) ; routingContext . response ( ) . putHeader ( HttpHeaderNames . CONTENT_TYPE , contentType ) . sendFile ( filename ) ; } else-if ( routingContext . response ( ) . ended ( ) ) { log . debug ( "[{}] Response: statusCode = {}" , routingContext . get ( "request-id" ) , statusCode ) ; } }
public void waitSensorActive ( @ Nonnull Sensor [ ] mSensors ) { LOG . debug ( "waitSensorActive() - waitSensorActive()" ) ; waitSensorState ( mSensors , Sensor . ACTIVE ) ; }
public void test() { try { File cacheDir = new File ( failoverDir ) ; code_block = IfStatement ; File [ ] files = cacheDir . listFiles ( ) ; code_block = IfStatement ; } catch ( Throwable e ) { LOG . error ( e ) ; } }
public void createView ( String name , String tableName , String ... columnNames ) throws SQLException { code_block = IfStatement ; StringBuilder statement = new StringBuilder ( ) ; statement . append ( "CREATE VIEW " ) ; statement . append ( quoteCaseSensitive ( name ) ) ; statement . append ( " (" ) ; statement . append ( Arrays . stream ( columnNames ) . collect ( Collectors . joining ( "\", \"" , "\"" , "\"" ) ) ) ; statement . append ( ") AS SELECT " ) ; statement . append ( Arrays . stream ( columnNames ) . collect ( Collectors . joining ( "\", \"" , "\"" , "\"" ) ) ) ; statement . append ( " FROM " ) ; statement . append ( quoteCaseSensitive ( tableName ) ) ; Statement stmt = conn . createStatement ( ) ; String statementStr = statement . toString ( ) ; LOGGER . debug ( "{}" , statementStr ) ; stmt . execute ( statementStr ) ; }
public void test() { try { iam . projects ( ) . serviceAccounts ( ) . keys ( ) . get ( keyName ) . execute ( ) ; return true ; } catch ( GoogleJsonResponseException e ) { code_block = IfStatement ; log . error ( e . getMessage ( ) , e ) ; throw e ; } }
public void test() { try { inputAsJSON = writer . writeValueAsString ( message ) ; logger . info ( "LCM Kit input message follows: {}" , inputAsJSON ) ; } catch ( JsonProcessingException e ) { logger . error ( "Error writing to JSON" , e ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { Configuration configuration = ApplicationProperties . get ( ) ; Properties properties = ConfigurationConverter . getProperties ( configuration . subset ( "atlas.authentication.method.ldap" ) ) ; ldapURL = properties . getProperty ( "url" ) ; ldapUserDNPattern = properties . getProperty ( "userDNpattern" ) ; ldapGroupSearchBase = properties . getProperty ( "groupSearchBase" ) ; ldapGroupSearchFilter = properties . getProperty ( "groupSearchFilter" ) ; ldapGroupRoleAttribute = properties . getProperty ( "groupRoleAttribute" ) ; ldapBindDN = properties . getProperty ( "bind.dn" ) ; ldapBindPassword = properties . getProperty ( "bind.password" ) ; ldapDefaultRole = properties . getProperty ( "default.role" ) ; ldapUserSearchFilter = properties . getProperty ( "user.searchfilter" ) ; ldapReferral = properties . getProperty ( "referral" ) ; ldapBase = properties . getProperty ( "base.dn" ) ; groupsFromUGI = configuration . getBoolean ( "atlas.authentication.method.ldap.ugi-groups" , true ) ; code_block = IfStatement ; } catch ( Exception e ) { LOG . error ( "Error while loading configuration" , e ) ; } }
public void test() { switch ( problem . getSeverity ( ) ) { case ERROR : case FATAL : throw new BootstrapMavenException ( "Settings problem encountered at " + problem . getLocation ( ) , problem . getException ( ) ) ; default : log . error ( "Settings problem encountered at {}" , problem . getLocation ( ) ) ; } }
@ Override public < K , V > Map < K , V > createLRUWeakCache ( int maximumCacheSize ) { LOG . trace ( "Creating LRUCache with maximumCacheSize: {}" , maximumCacheSize ) ; return new SimpleLRUCache < > ( maximumCacheSize ) ; }
public void test() { if ( handler == null ) { logger . debug ( HANDLER_IS_NULL ) ; return ; } }
@ Override public List < AccountReport > listReports ( String accountId , String report ) throws IOException { LOG . debug ( "Getting report for report {}" , report ) ; code_block = IfStatement ; String url = buildCanvasUrl ( "accounts/" + accountId + "/reports/" + report , Collections . emptyMap ( ) ) ; return getListFromCanvas ( url ) ; }
public void test() { if ( fieldBean != null ) { columnNames . add ( fieldBean . getColumnName ( ) ) ; } else-if ( keyBean != null ) { columnNames . add ( keyBean . getColumnName ( ) ) ; } else { LOGGER . warn ( "Field name {} not supported" , fieldBean . getName ( ) ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getCapabilities()" ) ; } }
public void test() { if ( configExpirationTime != tokenDefinedExirationTime ) { String msg = String . format ( "The configured expiration time for the token = '%s' changed from when the token was created." , token . getId ( ) ) ; logger . warn ( msg ) ; } }
public boolean pkgShow ( List < String > packages ) { boolean cmdOk = true ; code_block = IfStatement ; StringBuilder sb = new StringBuilder ( ) ; sb . append ( "****************************************" ) ; code_block = ForStatement ; LOGGER . info ( sb . toString ( ) ) ; return cmdOk ; }
public List findByExample ( NmbNotiz instance ) { log . debug ( "finding NmbNotiz instance by example" ) ; code_block = TryStatement ;  }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.NmbNotiz" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.NmbNotiz" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
public void test() { try { SerializingTranscoder transcoder = new SerializingTranscoder ( config . getMaxObjectSize ( ) ) ; transcoder . setCompressionThreshold ( Integer . MAX_VALUE ) ; OperationQueueFactory opQueueFactory ; int maxQueueSize = config . getMaxOperationQueueSize ( ) ; code_block = IfStatement ; String hostsStr = config . getHosts ( ) ; ConnectionFactory connectionFactory = new MemcachedConnectionFactoryBuilder ( ) . setProtocol ( ConnectionFactoryBuilder . Protocol . BINARY ) . setHashAlg ( DefaultHashAlgorithm . FNV1A_64_HASH ) . setLocatorType ( ConnectionFactoryBuilder . Locator . CONSISTENT ) . setDaemon ( true ) . setFailureMode ( FailureMode . Redistribute ) . setTranscoder ( transcoder ) . setShouldOptimize ( true ) . setOpQueueMaxBlockTime ( config . getTimeout ( ) ) . setOpTimeout ( config . getTimeout ( ) ) . setReadBufferSize ( config . getReadBufferSize ( ) ) . setOpQueueFactory ( opQueueFactory ) . build ( ) ; return new MemcachedCache ( new MemcachedClient ( new MemcachedConnectionFactory ( connectionFactory ) , getResolvedAddrList ( hostsStr ) ) , config , memcachedPrefix , timeToLive ) ; } catch ( IOException e ) { LOG . error ( "Unable to create memcached connection" , e ) ; throw Throwables . propagate ( e ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { { final String startTime = TimeUtil . getTimeWrtSystemTime ( - 20 ) ; String endTime = TimeUtil . getTimeWrtSystemTime ( 4000 ) ; bundles [ 1 ] . setProcessPeriodicity ( 1 , TimeUnit . days ) ; bundles [ 1 ] . setOutputFeedPeriodicity ( 1 , TimeUnit . days ) ; bundles [ 1 ] . setProcessValidity ( startTime , endTime ) ; bundles [ 1 ] . submitBundle ( prism ) ; AssertUtil . assertSucceeded ( cluster3 . getProcessHelper ( ) . schedule ( bundles [ 1 ] . getProcessData ( ) ) ) ; InstanceUtil . waitTillInstancesAreCreated ( cluster3OC , bundles [ 1 ] . getProcessData ( ) , 0 , 10 ) ; String oldBundleId = OozieUtil . getLatestBundleID ( cluster3OC , bundles [ 1 ] . getProcessName ( ) , EntityType . PROCESS ) ; waitForProcessToReachACertainState ( cluster3OC , bundles [ 1 ] , Job . Status . RUNNING ) ; List < String > oldNominalTimes = OozieUtil . getActionsNominalTime ( cluster3OC , oldBundleId , EntityType . PROCESS ) ; LOGGER . info ( "original process: " + oldNominalTimes ) ; ProcessMerlin updatedProcess = new ProcessMerlin ( bundles [ 1 ] . getProcessObject ( ) ) ; updatedProcess . setFrequency ( new Frequency ( "5" , TimeUnit . minutes ) ) ; LOGGER . info ( "updated process: " + updatedProcess ) ; ServiceResponse response = prism . getProcessHelper ( ) . update ( bundles [ 1 ] . getProcessData ( ) , updatedProcess . toString ( ) ) ; AssertUtil . assertSucceeded ( response ) ; InstanceUtil . waitTillInstancesAreCreated ( cluster3OC , bundles [ 1 ] . getProcessData ( ) , 1 , 10 ) ; String prismString = dualComparison ( prism , cluster2 , bundles [ 1 ] . getProcessData ( ) ) ; Assert . assertEquals ( new ProcessMerlin ( prismString ) . getFrequency ( ) , new Frequency ( "" + 5 , TimeUnit . minutes ) ) ; dualComparison (
public void test() { { final String startTime = TimeUtil . getTimeWrtSystemTime ( - 20 ) ; String endTime = TimeUtil . getTimeWrtSystemTime ( 4000 ) ; bundles [ 1 ] . setProcessPeriodicity ( 1 , TimeUnit . days ) ; bundles [ 1 ] . setOutputFeedPeriodicity ( 1 , TimeUnit . days ) ; bundles [ 1 ] . setProcessValidity ( startTime , endTime ) ; bundles [ 1 ] . submitBundle ( prism ) ; AssertUtil . assertSucceeded ( cluster3 . getProcessHelper ( ) . schedule ( bundles [ 1 ] . getProcessData ( ) ) ) ; InstanceUtil . waitTillInstancesAreCreated ( cluster3OC , bundles [ 1 ] . getProcessData ( ) , 0 , 10 ) ; String oldBundleId = OozieUtil . getLatestBundleID ( cluster3OC , bundles [ 1 ] . getProcessName ( ) , EntityType . PROCESS ) ; waitForProcessToReachACertainState ( cluster3OC , bundles [ 1 ] , Job . Status . RUNNING ) ; List < String > oldNominalTimes = OozieUtil . getActionsNominalTime ( cluster3OC , oldBundleId , EntityType . PROCESS ) ; LOGGER . info ( "original process: " + Util . prettyPrintXml ( bundles [ 1 ] . getProcessData ( ) ) ) ; ProcessMerlin updatedProcess = new ProcessMerlin ( bundles [ 1 ] . getProcessObject ( ) ) ; updatedProcess . setFrequency ( new Frequency ( "5" , TimeUnit . minutes ) ) ; LOGGER . info ( "updated process: " + Util . prettyPrintXml ( bundles [ 1 ] . getProcessData ( ) ) ) ; ServiceResponse response = prism . getProcessHelper ( ) . update ( bundles [ 1 ] . getProcessData ( ) , updatedProcess . toString ( ) ) ; AssertUtil . assertSucceeded ( response ) ; InstanceUtil . waitTillInstancesAreCreated ( cluster3OC , bundles [ 1 ] . getProcessData ( ) , 1 , 10 ) ; String prismString = dualComparison ( prism , cluster2 , bundles [ 1 ] . getProcessData ( ) ) ; Assert . assertEquals ( new Process
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
@ Override public void stsRevocationRegistryInstantiationError ( ) { LOGGER . warn ( "stsRevocationRegistry Instantiation error" ) ; }
public void test() { try { Release release = componentClient . getReleaseById ( releaseId , user ) ; JsonNode input = OBJECT_MAPPER . readValue ( request . getParameter ( SPDX_LICENSE_INFO ) , JsonNode . class ) ; JsonNode licenesIdsNode = input . get ( LICENSE_IDS ) ; code_block = IfStatement ; licenesIdsNode = input . get ( "otherLicenseIds" ) ; code_block = IfStatement ; result = componentClient . updateRelease ( release , user ) ; } catch ( TException | IOException e ) { log . error ( "Failed to update release" , e ) ; response . setProperty ( ResourceResponse . HTTP_STATUS_CODE , "500" ) ; } }
public void test() { try { response = new ResponseDTO ( faqService . getFAQSByWidget ( widgetId , domainId ) ) ; } catch ( ServiceException e ) { logger . error ( e . getMessage ( ) , e ) ; return complianceService . formatException ( e ) ; } }
public void test() { try { int count = 1 ; boolean reachable = isGithubReachable ( containerId ) ; code_block = WhileStatement ; String cmd = String . format ( "cd %s; bash get_unzip.sh %s" , TMP_DIR , repoLink ) ; runCmd ( containerId , cmd ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( ! responses . containsKey ( requestId ) ) { LOG . error ( "Received response for unknown response [{}]" , requestId ) ; return ; } }
public void test() { if ( queue != null ) { queue . put ( response ) ; } else { log . trace ( "No queue found for response {}" , response ) ; } }
public void test() { try { final ArrayBlockingQueue < Object > queue = responses . get ( requestId ) ; code_block = IfStatement ; } catch ( InterruptedException e ) { LOG . warn ( "Interrupted while waiting for responses" , e ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getCapabilities()" ) ; } }
@ Override public void move ( MoveOperationContext moveContext ) throws LdapException { LOG . debug ( ">>> Entering into the Administrative Interceptor, moveRequest" ) ; Entry entry = moveContext . getOriginalEntry ( ) ; Attribute adminPoint = entry . get ( directoryService . getAtProvider ( ) . getAdministrativeRole ( ) ) ; code_block = IfStatement ; String message = "Cannot move an Administrative Point in the current version" ; LOG . error ( message ) ; throw new LdapUnwillingToPerformException ( message ) ; }
public void test() { if ( componentId . missing ( ) ) { log . debug ( "Component [{}] missing" , componentId ) ; } }
public void test() { if ( verbose ) { Log . info ( "=> Scanning classpath element: " + uri ) ; } }
public void test() { if ( verbose ) { Log . info ( "=> Scanning classpath element: " + uri ) ; } }
public void test() { if ( verbose ) { Log . info ( "=> Scanning classpath element: " + uri ) ; } }
public void test() { if ( verbose ) { Log . info ( "=> Scanning classpath element: " + uri ) ; } }
public void test() { try { boolean cont = true ; int cnt = 0 ; code_block = WhileStatement ; } finally { LOG . debug ( "Exiting" ) ; } }
@ Override public void setup ( ProfilerPluginSetupContext context ) { DubboConfiguration config = new DubboConfiguration ( context . getConfig ( ) ) ; code_block = IfStatement ; logger . info ( "{} config:{}" , this . getClass ( ) . getSimpleName ( ) , config ) ; code_block = IfStatement ; logger . info ( "Adding Dubbo transformers" ) ; this . addTransformers ( ) ; }
public void test() { if ( ! context . registerApplicationType ( DubboConstants . DUBBO_PROVIDER_SERVICE_TYPE ) ) { logger . info ( "Application type [{}] already set, skipping [{}] registration." , context . getApplicationType ( ) , DubboConstants . DUBBO_PROVIDER_SERVICE_TYPE ) ; } }
public void test() { if ( pm != null && propertyMappings . contains ( pm ) ) { LOG . debug ( "Removing property: " + pm ) ; propertyMappings . remove ( pm ) ; } }
public void test() { if ( addProp . getLocalName ( ) . equals ( INVOCATION_TYPE ) ) { final String invocationType = addProp . getTextContent ( ) . trim ( ) ; LOGGER . debug ( "Invoking InvocationType: {}" , invocationType ) ; return invocationType ; } }
private void checkSystemRequestDTO ( final SystemRequestDTO system , final String origin ) { logger . debug ( "checkSystemRequestDTO started..." ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; final int validatedPort = system . getPort ( ) . intValue ( ) ; code_block = IfStatement ; }
private void adjustDhModulus ( DHEServerKeyExchangeMessage message ) { tlsContext . setServerDhModulus ( new BigInteger ( 1 , message . getModulus ( ) . getValue ( ) ) ) ; LOGGER . debug ( "ServerDHModulus: " + tlsContext . getServerDhModulus ( ) ) ; }
public void test() { try { cis . close ( ) ; } catch ( final IOException ioe ) { LOG . warn ( ioe . getMessage ( ) , ioe ) ; } }
@ Override public Boolean isDuplicateFile ( String filepath ) { logger . info ( "isDuplicateFile: {}" , filepath ) ; return false ; }
public void test() { if ( accuracyThreshold > 0 ) { logger . info ( "Location accuracy threshold check is enabled." ) ; } else { logger . debug ( "Location accuracy threshold check is disabled." ) ; } }
public void test() { if ( accuracyThreshold > 0 ) { logger . debug ( "Location accuracy is below required threshold: {}<={}" , accuracy , accuracyThreshold ) ; } else { logger . debug ( "Location accuracy is not required threshold: {}" , accuracyThreshold ) ; } }
public void test() { if ( accuracyThreshold >= accuracy || accuracyThreshold . intValue ( ) == 0 ) { code_block = IfStatement ; String regionName = ConfigHelper . getRegionName ( currentConfig ) ; PointType center = ConfigHelper . getRegionCenterLocation ( currentConfig ) ; State newLocation = message . getTrackerLocation ( ) ; code_block = IfStatement ; } else { logger . info ( String . format ( "Successfully updated location %s on location %s" , location , regionName ) ) ; } }
public void test() { if ( resource == null || resource . isEmpty ( ) ) { logger . warn ( "No resource found with path: {}" , path ) ; } else-if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Checking: {}" , resource ) ; } }
public void test() { if ( resource == null || resource . isEmpty ( ) ) { logger . warn ( "This resource has no resource identifier in the xacml response results!" ) ; } else-if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Resource identifier: " + resource + " = " + resource ) ; } }
public void test() { if ( ! conflictingNodeIds . isEmpty ( ) ) { final Set < String > fullNodeIdDescriptions = conflictingNodeIds . stream ( ) . map ( NodeIdentifier :: getFullDescription ) . collect ( Collectors . toSet ( ) ) ; conflictingNodeIds . forEach ( uuid -> removeNode ( uuid ) ) ; getLogger ( ) . warning ( String . format ( "Duplicate node %s detected: %s" , conflictingNodeIds , fullNodeIds ) ) ; } }
public void test() { for ( @ SuppressWarnings ( "rawtypes" ) Metric metric : record . metrics ( ) ) { LOG . info ( "Received metric {}" , metric . getName ( ) ) ; } }
public void test() { try { MetricsRecord record = ( MetricsRecord ) future . get ( ) ; code_block = IfStatement ; } catch ( InterruptedException ie ) { LOG . error ( ie . getMessage ( ) ) ; Thread . currentThread ( ) . interrupt ( ) ; } catch ( ExecutionException ee ) { LOG . warn ( ee . getCause ( ) ) ; } }
public void test() { try { MetricsRecord record = ( MetricsRecord ) future . get ( ) ; code_block = IfStatement ; } catch ( InterruptedException ie ) { LOG . warn ( ie ) ; Thread . currentThread ( ) . interrupt ( ) ; } catch ( ExecutionException ee ) { LOG . error ( ee . getMessage ( ) , ee ) ; } }
public void test() { try { MetaServerConsoleService . PreviousPrimaryDcMessage previousPrimaryDcMessage = commandFuture . get ( ) ; logger . info ( "[doPrevPrimaryDcMigrate][result]{},{},{},{}" , cluster , shard , dc , previousPrimaryDcMessage ) ; shardMigrationResult . setPreviousPrimaryDcMessage ( previousPrimaryDcMessage ) ; shardMigrationResult . updateStepResult ( ShardMigrationStep . MIGRATE_PREVIOUS_PRIMARY_DC , true , previousPrimaryDcMessage == null ? LogUtils . info ( "Succeed, return message null" ) : previousPrimaryDcMessage . getMessage ( ) ) ; } catch ( Exception e ) { logger . error ( "[doPrevPrimaryDcMigrate]" , e ) ; shardMigrationResult . updateStepResult ( ShardMigrationStep . MIGRATE_PREVIOUS_PRIMARY_DC , true , LogUtils . error ( "Ignored:" + e . getMessage ( ) ) ) ; } }
public void test() { try ( Checkpoint checkpoint = Checkpoint . create ( rocksdb ) ) { String tempPath = targetPath + "_temp" ; File tempFile = new File ( tempPath ) ; FileUtils . deleteDirectory ( tempFile ) ; FileUtils . forceMkdir ( tempFile . getParentFile ( ) ) ; checkpoint . createCheckpoint ( tempPath ) ; LOG . debug ( "Deleted checkpoint path {}" , tempPath ) ; File snapshotFile = new File ( targetPath ) ; FileUtils . deleteDirectory ( snapshotFile ) ; LOG . debug ( "Deleted stale directory {}" , snapshotFile ) ; code_block = IfStatement ; } catch ( Exception e ) { throw new BackendException ( "Failed to create checkpoint at path %s" , e , targetPath ) ; } }
public void test() { try ( Checkpoint checkpoint = Checkpoint . create ( rocksdb ) ) { String tempPath = targetPath + "_temp" ; File tempFile = new File ( tempPath ) ; FileUtils . deleteDirectory ( tempFile ) ; LOG . debug ( "Deleted temp directory {}" , tempFile ) ; FileUtils . forceMkdir ( tempFile . getParentFile ( ) ) ; checkpoint . createCheckpoint ( tempPath ) ; File snapshotFile = new File ( targetPath ) ; FileUtils . deleteDirectory ( snapshotFile ) ; LOG . debug ( "Deleted checkpoint {}" , snapshotFile ) ; code_block = IfStatement ; } catch ( Exception e ) { throw new BackendException ( "Failed to create checkpoint at path %s" , e , targetPath ) ; } }
public void test() { try { checkpointIds [ i ] = Integer . valueOf ( status [ i ] . getPath ( ) . getName ( ) ) ; } catch ( Throwable x ) { LOG . error ( "Fail to parse checkpoint status" , x ) ; } }
public void test() { if ( ! fs . delete ( status [ i ] . getPath ( ) , true ) ) { LOG . info ( "Could not delete old checkpoint " + status [ i ] . getPath ( ) ) ; } else { LOG . info ( "Delete old checkpoint " + status [ i ] . getPath ( ) + " failed " ) ; } }
public void test() { if ( ! fs . delete ( status [ i ] . getPath ( ) , true ) ) { LOG . warn ( "Delete path " + status [ i ] . getPath ( ) + " failed " ) ; } else { LOG . info ( "Deleted path " + status [ i ] . getPath ( ) ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Throwable x ) { LOG . warn ( "Exception while notifying listener {}" , listener , x ) ; } }
public void test() { if ( receiveBufferSize != expectedRecvBufferSize ) { log . error ( "Expected receive buffer size! {}, got {}" , receiveBufferSize , expectedRecvBufferSize ) ; } }
public void test() { if ( future . isSuccess ( ) ) { final Channel channel = future . channel ( ) ; channelReference . set ( channel ) ; LOG . debug ( "Started channel {}" , channel ) ; final ServerSocketChannelConfig channelConfig = ( ServerSocketChannelConfig ) channel . config ( ) ; final int receiveBufferSize = channelConfig . getReceiveBufferSize ( ) ; code_block = IfStatement ; } else { LOG . error ( "Failed to connect to server" , future . cause ( ) ) ; } }
public void test() { try { registerMBean ( bean , mBeanServer , objectName ) ; } catch ( Exception e ) { logger . error ( "Error registering MBean" , e ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( AssetCategoryServiceUtil . class , "getVocabularyCategoriesDisplay" , _getVocabularyCategoriesDisplayParameterTypes20 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , vocabularyId , start , end , orderByComparator ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . asset . kernel . model . AssetCategoryDisplay ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( StringUtils . isEmpty ( organisationIdentification ) ) { logger . info ( "OrganisationIdentification is null" ) ; } }
public void test() { if ( StringUtils . isEmpty ( userName ) ) { logger . info ( "Authentication with empty user name!" ) ; } }
public void test() { if ( StringUtils . isEmpty ( applicationName ) ) { log . error ( String . format ( "Application name not found in %s" , applicationName ) ) ; } }
public void test() { switch ( jainmgcpresponseevent . getObjectIdentifier ( ) ) { case Constants . RESP_ENDPOINT_CONFIGURATION : responseReceived = true ; break ; default : logger . debug ( "Response not found" ) ; break ; } }
@ Override public void init ( ) throws Exception { _logger . debug ( "Initializing folder: {}" , this . getResourceTrashRootDiskSubFolder ( ) ) ; this . checkTrashedResourceDiskFolder ( this . getResourceTrashRootDiskSubFolder ( ) ) ; _logger . debug ( "Folder trashed resources: {}" , this . getResourceTrashRootDiskSubFolder ( ) ) ; }
@ Override public void init ( ) throws Exception { _logger . debug ( "{} ready" , this . getClass ( ) . getName ( ) ) ; this . checkTrashedResourceDiskFolder ( this . getResourceTrashRootDiskSubFolder ( ) ) ; _logger . debug ( "{} ready" , this . getClass ( ) . getName ( ) ) ; }
public void test() { if ( dueTime > maximumDueTime ) { log . warn ( "    dueTime > maximumDueTime ) ; dueTime = maximumDueTime ; } }
public void test() { try { Collection < ? > c = ( Collection ) literal ; Iterator iterator = c . iterator ( ) ; List < Object > cast = new ArrayList < Object > ( ) ; code_block = WhileStatement ; return cast ; } catch ( Exception e ) { log . debug ( "Exception casting: " + e . getMessage ( ) , e ) ; } }
public void test() { { jcloudsLocation = ( JcloudsLocation ) managementContext . getLocationRegistry ( ) . getLocationManaged ( AWS_EC2_LOCATION_SPEC ) ; machine = createEc2Machine ( ImmutableMap . < String , Object > of ( ) ) ; assertSshable ( machine ) ; String locationAddress = machine . getAddress ( ) . getHostName ( ) ; InetAddress address = machine . getAddress ( ) ; Set < String > publicAddresses = machine . getPublicAddresses ( ) ; Set < String > privateAddresses = machine . getPrivateAddresses ( ) ; String subnetIp = machine . getSubnetIp ( ) ; String subnetHostname = machine . getSubnetHostname ( ) ; String hostname = machine . getHostname ( ) ; String msg = "locationAddress=" + locationAddress + "; address=" + address + "; publicAddrs=" + publicAddresses + "; privateAddrs=" + privateAddresses + "; subnetIp=" + subnetIp + "; hostname=" + hostname + "; subnetHostname=" + subnetHostname ; LOG . info ( msg ) ; assertReachable ( machine , locationAddress , msg ) ; assertReachableFromMachine ( machine , locationAddress , msg ) ; assertReachable ( machine , locationAddress , msg ) ; assertReachable ( machine , address , msg ) ; assertTrue ( publicAddresses . size ( ) > 0 , msg ) ; code_block = ForStatement ; assertTrue ( privateAddresses . size ( ) > 0 , msg ) ; code_block = ForStatement ; assertNotNull ( subnetIp , msg ) ; assertReachableFromMachine ( machine , subnetIp , msg ) ; assertNotNull ( hostname , msg ) ; assertReachableFromMachine ( machine , hostname , msg ) ; assertNotNull ( subnetHostname , msg ) ; assertReachable ( machine , subnetHostname , msg ) ; assertReachableFromMachine ( machine , subnetHostname , msg ) ; } }
public void test() { try ( final Collection coll = broker . openCollection ( XmldbURI . createInternal ( protectColl ) , collectionLockMode ) ) { docs = new DefaultDocumentSet ( ) ; coll . allDocs ( broker , docs , true , lockedDocuments , documentLockMode ) ; return lockedDocuments ; } catch ( final LockException e ) { log . error ( e . getMessage ( ) , e ) ; lockedDocuments . unlock ( ) ; } }
public void test() { try { session = sessionFactory . openSession ( ) ; tokenDto = ( EnrollmentTokenDto ) session . createQuery ( "from EnrollmentTokenDto " + " where enrollmentToken= :enrollmentToken" + " ORDER BY id DESC" ) . setString ( "enrollmentToken" , token ) . setMaxResults ( 1 ) . uniqueResult ( ) ; code_block = IfStatement ; } catch ( Exception e ) { LOG . error ( "StudyDAOImpl - enrollments() - ERROR" , e ) ; } finally { code_block = IfStatement ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public Structure update ( final Structure structure ) { LOGGER . debug ( "update() - structure: {}" , structure ) ; code_block = IfStatement ; validateStructure ( structure ) ; return structureRepository . save ( structure ) ; }
@ Override public void init ( ) { super . init ( ) ; Integer portStartingPoint ; Object rawPort = getAllConfigBag ( ) . getStringKey ( PORT_FORWARD_MANAGER_STARTING_PORT . getName ( ) ) ; code_block = IfStatement ; portReserved . set ( portStartingPoint ) ; LOG . info ( "Reserved port starting point: {}" , getName ( ) ) ; }
@ Override public void stop ( ) { LOGGER . warn ( "Plugin:SpringAppLoader stopping..." ) ; springCtxs . forEach ( context code_block = LoopStatement ; ) ; LOGGER . warn ( "Plugin:SpringAppLoader stoped.." ) ; }
public void test() { if ( protocol . equals ( "HTTP" ) ) { return Protocol . HTTP ; } else-if ( protocol . equals ( "HTTPS" ) ) { return Protocol . HTTPS ; } else { log . warn ( "Unsupported protocol: {}" , protocol ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
@ ApiOperation ( value = "get history by owner's id" ) @ GetMapping ( CommonConstants . PATH_LOGBOOK ) public LogbookOperationsResponseDto findHistoryById ( final @ PathVariable String id ) { LOGGER . debug ( "get logbook for owner with id :{}" , id ) ; ParameterChecker . checkParameter ( "Identifier is mandatory : " , id ) ; return service . findHistoryById ( buildUiHttpContext ( ) , id ) ; }
@ RequestMapping ( value = "/{id}" , method = RequestMethod . GET ) public RoleDto find ( @ PathVariable Long id ) { log . debug ( "find() - id: {}" , id ) ; return roleManagementService . findRole ( id ) ; }
public void test() { if ( connected . compareAndSet ( false , true ) ) { code_block = TryStatement ;  } else { logger . debug ( "Already connected to {}" , this ) ; } }
public void test() { try { catalogs = catalogService . latestInfraTemplates ( ) ; break ; } catch ( IOException e ) { LOG . warn ( e . getMessage ( ) , e ) ; code_block = TryStatement ;  } }
public void test() { try { code_block = IfStatement ; GrpcServerBuilder serverBuilder = GrpcServerBuilder . forAddress ( GrpcServerAddress . create ( mRpcConnectAddress . getHostName ( ) , mRpcBindAddress ) , ServerConfiguration . global ( ) , ServerUserState . global ( ) ) ; code_block = ForStatement ; LOG . info ( "Started gRPC server {}" , mRpcConnectAddress ) ; mGrpcServer = serverBuilder . build ( ) . start ( ) ; LOG . info ( "Started gRPC server on address {}" , mRpcConnectAddress ) ; mGrpcServer . awaitTermination ( ) ; } catch ( IOException e ) { throw new RuntimeException ( e ) ; } }
public void test() { try { code_block = IfStatement ; LOG . info ( "Starting gRPC server on address {}" , mRpcConnectAddress ) ; GrpcServerBuilder serverBuilder = GrpcServerBuilder . forAddress ( GrpcServerAddress . create ( mRpcConnectAddress . getHostName ( ) , mRpcBindAddress ) , ServerConfiguration . global ( ) , ServerUserState . global ( ) ) ; code_block = ForStatement ; mGrpcServer = serverBuilder . build ( ) . start ( ) ; mGrpcServer . awaitTermination ( ) ; LOG . info ( "GRPC server started" ) ; } catch ( IOException e ) { throw new RuntimeException ( e ) ; } }
public void test() { try { KeyPairGenerator keyPairGenerator = null ; keyPairGenerator = KeyPairGenerator . getInstance ( alg ) ; keyPairGenerator . initialize ( keySize ) ; return keyPairGenerator . generateKeyPair ( ) ; } catch ( NoSuchAlgorithmException e ) { logger . warn ( "Unable to generate keypair" ) ; } }
public void test() { try { MerkleEntityId id = fromContractId ( txn . getContractUpdateInstance ( ) . getContractID ( ) ) ; Timestamp expiry = lookupAccountExpiry ( id , view . accounts ( ) ) ; return usageEstimator . getContractUpdateTxFeeMatrices ( txn , expiry , sigUsage ) ; } catch ( Exception e ) { logger . warn ( "Unable to get contract update instance due to {}" , e . getMessage ( ) ) ; return FeeData . getDefaultInstance ( ) ; } }
public void test() { if ( e instanceof org . apache . airavata . model . error . InvalidRequestException ) { result . ire = ( org . apache . airavata . model . error . InvalidRequestException ) e ; result . setIreIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AiravataClientException ) { result . ace = ( org . apache . airavata . model . error . AiravataClientException ) e ; result . setAceIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AiravataSystemException ) { result . ase = ( org . apache . airavata . model . error . AiravataSystemException ) e ; result . setAseIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AuthorizationException ) { result . ae = ( org . apache . airavata . model . error . AuthorizationException ) e ; result . setAeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof org . apache . airavata . model . error . InvalidRequestException ) { result . ire = ( org . apache . airavata . model . error . InvalidRequestException ) e ; result . setIreIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AiravataClientException ) { result . ace = ( org . apache . airavata . model . error . AiravataClientException ) e ; result . setAceIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AiravataSystemException ) { result . ase = ( org . apache . airavata . model . error . AiravataSystemException ) e ; result . setAseIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AuthorizationException ) { result . ae = ( org . apache . airavata . model . error . AuthorizationException ) e ; result . setAeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof org . apache . airavata . model . error . InvalidRequestException ) { result . ire = ( org . apache . airavata . model . error . InvalidRequestException ) e ; result . setIreIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AiravataClientException ) { result . ace = ( org . apache . airavata . model . error . AiravataClientException ) e ; result . setAceIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AiravataSystemException ) { result . ase = ( org . apache . airavata . model . error . AiravataSystemException ) e ; result . setAseIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AuthorizationException ) { result . ae = ( org . apache . airavata . model . error . AuthorizationException ) e ; result . setAeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { try { fcall . sendResponse ( fb , msg , msgType , seqid ) ; } catch ( java . lang . Exception ex ) { _LOGGER . error ( "Exception writing to internal frame buffer" , ex ) ; fb . close ( ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( lastElementIndex != - 1 && ( currentElementIndex < lastElementIndex ) ) { logger . warn ( "Element " + currentElementIndex + " is invalid." ) ; return false ; } }
public void test() { try { if ( ! Services . getInstance ( ) . isShuttingDown ( ) && ! Services . getInstance ( ) . isShutdownDone ( ) ) info . messageCallback ( topic , message ) ; } catch ( FrameworkException e ) { logger . info ( "Unable to callback message callback." , e ) ; } }
public void test() { if ( update > 0 ) { logger . info ( "update retry message error by retryQueryCondition:{}, status:{}, sendStartTime:{}, sendEndTime:{}" , retryQueryCondition , status , update ) ; } else { logger . error ( "update retry message error by retryQueryCondition:{}, status:{}, sendStartTime:{}, sendEndTime:{}" , retryQueryCondition , status ) ; } }
public void test() { if ( update > 0 ) { logger . info ( "update retry message success by retryQueryCondition:{}, status:{}, sendStartTime:{}, sendEndTime:{}" , retryQueryCondition , status ) ; } else { logger . info ( "update retry message fail by retryQueryCondition:{}, status:{}, sendStartTime:{}" , retryQueryCondition , status ) ; } }
public void test() { try { com . liferay . commerce . model . CommerceOrder returnValue = CommerceOrderServiceUtil . updateCustomFields ( commerceOrderId , serviceContext ) ; return com . liferay . commerce . model . CommerceOrderSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( SegmentsEntryRelServiceUtil . class , "getSegmentsEntryRelsCount" , _getSegmentsEntryRelsCountParameterTypes7 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , classNameId , classPK ) ; Object returnObj = null ; code_block = TryStatement ;  return ( ( Integer ) returnObj ) . intValue ( ) ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( head != null ) { head . moveToBlocking ( neck , rothead , eyeX , eyeY , jaw , rollNeck ) ; } else { log . warn ( "moveToBlocking() without a head" ) ; } }
public void test() { try { file = new File ( url . toURI ( ) ) ; } catch ( URISyntaxException e ) { file = new File ( url . getPath ( ) ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; file = null ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
@ Test public void copyAllTranslationsWithMD5MatchBetweenRepositoriesNameRegex ( ) throws InterruptedException , ExecutionException , RepositoryNameAlreadyUsedException , AssetWithIdNotFoundException , RepositoryWithIdNotFoundException { TMTestData tmTestDataSource = new TMTestData ( testIdWatcher ) ; Repository sourceRepository = tmTestDataSource . repository ; logger . debug ( "Create the target repository" ) ; Repository targetRepository = repositoryService . createRepository ( testIdWatcher . getEntityName ( "targetRepository" ) ) ; TM tm = targetRepository . getTm ( ) ; Asset asset = assetService . createAssetWithContent ( targetRepository . getId ( ) , "fake_for_test" , "fake code_block = ForStatement ; ; Iterator < TMTextUnitVariant > itSource = Iterables . filter ( sourceTranslations , filterZuora ) . iterator ( ) ; Iterator < TMTextUnitVariant > itTarget = targetTranslations . iterator ( ) ; code_block = WhileStatement ; Assert . assertFalse ( itSource . hasNext ( ) ) ; }
public void test() { try { this . addFile ( ) ; this . outputStream . close ( ) ; this . outputStream = null ; } catch ( Exception ex ) { ex . printStackTrace ( ) ; LOGGER . log ( Level . SEVERE , "Error in addFile" , ex ) ; } finally { this . isClosed = true ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { EntityManager . instance . insertFeedback ( username , title , type , message , email , host , cal . getTime ( ) ) ; } catch ( Exception e ) { log . error ( e . getMessage ( ) ) ; } }
public void test() { if ( logger . isDebugable ( ) ) { logger . debug ( e . getMessage ( ) , e ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( segmentImp == null ) { LOG . warn ( "SegmentId:{} segmentImp is null" , segmentId ) ; return DEFAULT_CHANGE_NUMBER ; } }
public void test() { try { OperationFuture < Boolean > future = memcachedClient ( ) . delete ( key ) ; return getReturnCode ( future ) ; } catch ( Exception e ) { logger . error ( "Error inserting key: " + key , e ) ; return Status . ERROR ; } }
public void test() { if ( mainRenderContext == null ) { LOG . error ( "Cannot restore context, because it's not available in the context" ) ; return ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
@ Override public void run ( ) { HazelcastInstance instance = factory . newHazelcastInstance ( config ) ; instances . set ( 0 , instance ) ; IMap < String , String > map = instance . getMap ( MAP_NAME ) ; LOGGER . info ( "Map size on node 1: " + node1 . getName ( ) ) ; node1MapLoadingAboutToStart . countDown ( ) ; int sizeOnNode1 = map . size ( ) ; LOGGER . info ( "Map loading has been completed by now" ) ; LOGGER . info ( "Map size on node 1: " + sizeOnNode1 ) ; node1FinishedLoading . countDown ( ) ; }
@ Override public void run ( ) { HazelcastInstance instance = factory . newHazelcastInstance ( config ) ; instances . set ( 0 , instance ) ; IMap < String , String > map = instance . getMap ( MAP_NAME ) ; LOGGER . info ( "Getting the map from node1 -> " + node1 ) ; node1MapLoadingAboutToStart . countDown ( ) ; LOGGER . info ( "Getting the size of the map on node1 -> load is triggered" ) ; int sizeOnNode1 = map . size ( ) ; LOGGER . info ( "Map size on node 1: " + sizeOnNode1 ) ; node1FinishedLoading . countDown ( ) ; }
@ Override public void run ( ) { HazelcastInstance instance = factory . newHazelcastInstance ( config ) ; instances . set ( 0 , instance ) ; IMap < String , String > map = instance . getMap ( MAP_NAME ) ; LOGGER . info ( "Getting the map on node1 -> " + node1 ) ; node1MapLoadingAboutToStart . countDown ( ) ; LOGGER . info ( "Getting the size of the map on node1 -> load is triggered" ) ; int sizeOnNode1 = map . size ( ) ; LOGGER . info ( "Map loading has been completed by now" ) ; node1FinishedLoading . countDown ( ) ; }
public void test() { try { code_block = IfStatement ; } catch ( LifecycleException exception ) { log . error ( "Failed to process lifecycle state change" , exception ) ; } }
public void test() { if ( me . getClass ( ) . getName ( ) . endsWith ( ".SMTPAddressFailedException" ) || me . getClass ( ) . getName ( ) . endsWith ( ".SMTPAddressSucceededException" ) ) { LOGGER . info ( "Ignoring interrupt: {}" , me . getClass ( ) . getName ( ) ) ; } }
public void test() { if ( pref . isPresent ( ) ) { String json = pref . get ( ) . getTraits ( ) ; T result = JSONUtil . fromJsonString ( aKey . getTraitClass ( ) , json ) ; LOGGER . info ( "Found preference for key {} and user {}" , aKey , aUser ) ; return result ; } else { LOGGER . debug ( "No preferences found for key {} and user {}" , aKey , aUser ) ; return buildDefault ( aKey . getTraitClass ( ) ) ; } }
public void test() { if ( pref . isPresent ( ) ) { String json = pref . get ( ) . getTraits ( ) ; T result = JSONUtil . fromJsonString ( aKey . getTraitClass ( ) , json ) ; LOGGER . info ( "Loaded preferences for key {} and user {} and project {}: [{}]" , aKey , aUser , aProject , result ) ; return result ; } else { LOGGER . info ( "Could not find key {} and user {} and project {}" , aKey , aUser , aProject ) ; return buildDefault ( aKey . getTraitClass ( ) ) ; } }
public void test() { try { Optional < UserProjectPreference > pref = getUserProjectPreference ( aKey , aUser , aProject ) ; code_block = IfStatement ; } catch ( IOException e ) { Log . log ( e ) ; return buildDefault ( aKey . getTraitClass ( ) ) ; } }
public void test() { if ( CollectionUtils . isEmpty ( ini ) ) { LOG . error ( "Bad configuration file at " + path ) ; } }
public void test() { if ( ! isTestSuccess . get ( ) ) { LOG . error ( "Test case has exceeded the maximum allotted time to run of: " + getMaxTestTime ( ) + " ms." ) ; dumpAllThreads ( getName ( ) ) ; System . exit ( EXIT_ERROR ) ; } }
public void test() { if ( jdbcUrl . isPresent ( ) ) { connection = DriverManager . getConnection ( jdbcUrl . get ( ) ) ; } else { LOG . error ( "JdbcUrl already present" ) ; } }
public void test() { if ( time <= 0 ) { logger . debug ( "Waiting for index {}" , index ) ; } }
@ Before public void setup ( ) throws Exception { File agentDir = StagedInstall . getInstance ( ) . getStageDir ( ) ; File testDir = new File ( agentDir , TestSpooldirSource . class . getName ( ) ) ; assertTrue ( testDir . mkdirs ( ) ) ; File spoolParentDir = new File ( testDir , "spools" ) ; assertTrue ( "Unable to create sink output dir: " + spoolParentDir . getPath ( ) , spoolParentDir . mkdir ( ) ) ; final int NUM_SOURCES = 100 ; agentProps = new Properties ( ) ; List < String > spooldirSrcNames = Lists . newArrayList ( ) ; String channelName = "mem-01" ; code_block = ForStatement ; agentProps . put ( "agent.channels.mem-01.type" , "MEMORY" ) ; agentProps . put ( "agent.channels.mem-01.capacity" , String . valueOf ( 100000 ) ) ; sinkOutputDir = new File ( testDir , "out" ) ; assertTrue ( "Unable to create sink output dir: " + sinkOutputDir . getPath ( ) , sinkOutputDir . mkdir ( ) ) ; logger . info ( "Unable to create sink output dir: " + sinkOutputDir . getPath ( ) ) ; agentProps . put ( "agent.sinks.roll-01.channel" , channelName ) ; agentProps . put ( "agent.sinks.roll-01.type" , "FILE_ROLL" ) ; agentProps . put ( "agent.sinks.roll-01.sink.directory" , sinkOutputDir . getPath ( ) ) ; agentProps . put ( "agent.sinks.roll-01.sink.rollInterval" , "0" ) ; agentProps . put ( "agent.sources" , Joiner . on ( " " ) . join ( spooldirSrcNames ) ) ; agentProps . put ( "agent.channels" , channelName ) ; agentProps . put ( "agent.sinks" , "roll-01" ) ; }
public void test() { try { com . liferay . expando . kernel . model . ExpandoValue returnValue = ExpandoValueServiceUtil . addValue ( companyId , className , tableName , columnName , classPK , data ) ; return com . liferay . expando . kernel . model . ExpandoValueSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( filters . size ( ) > 0 ) { NeutralQuery query = new NeutralQuery ( ) ; code_block = ForStatement ; securityEventIds = Lists . newArrayList ( ( repository . findAllIds ( RESOURCE_NAME , query ) ) ) ; } else { LOGGER . debug ( "No security event found." ) ; } }
public void test() { try { boolean present = jdbcTemplate . queryForObject ( "SELECT COUNT(1) FROM " + getRepositoryName ( ) + " WHERE " + ID + " = ?" , Integer . class , key ) != 0 ; code_block = IfStatement ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Error adding to repository " + repositoryName + " with key " + key , e ) ; throw new RuntimeException ( "Error adding to repository " + repositoryName + " with key " + key , e ) ; } }
public void test() { if ( present ) { LOG . debug ( "Updating record with key {}" , key ) ; long version = exchange . getProperty ( VERSION_PROPERTY , Long . class ) ; update ( camelContext , correlationId , exchange , getRepositoryName ( ) , version ) ; } else { LOG . debug ( "Inserting record with key {}" , key ) ; insert ( camelContext , correlationId , exchange , getRepositoryName ( ) , 1L ) ; } }
public void test() { if ( present ) { long version = exchange . getProperty ( VERSION_PROPERTY , Long . class ) ; LOG . debug ( "Updating record with key {} and version {}" , key , version ) ; update ( camelContext , correlationId , exchange , getRepositoryName ( ) , version ) ; } else { LOG . debug ( "Creating record with key {} and version {}" , key , version ) ; insert ( camelContext , correlationId , exchange , getRepositoryName ( ) , 1L ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( currentLevel > params . length ) { logger . error ( "Current level is not longer than {} characters" , currentLevel ) ; return false ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( String . format ( "Index %s already exists" , indexName ) ) ; } }
public void test() { try { configurator . setInputStream ( new BufferedInputStream ( Files . newInputStream ( cacheConfig ) ) ) ; configurator . configure ( pluginName ) ; } catch ( Exception e ) { log . error ( "Error loading configurator from cacheConfig " + cacheConfig , e ) ; } }
public void test() { try { parameters . add ( URLDecoder . decode ( homeFile . getAbsolutePath ( ) , StandardCharsets . UTF_8 . name ( ) ) ) ; return commandService . runCommand ( new File ( command ) , parameters ) . isSuccessful ( ) ; } catch ( FileNotFoundException e ) { logger . error ( "File not found in deleteSymLink" , e ) ; return false ; } catch ( IOException e ) { logger . error ( "IOException in deleteSymLink" , e ) ; return false ; } }
public void test() { try { parameters . add ( URLDecoder . decode ( homeFile . getAbsolutePath ( ) , StandardCharsets . UTF_8 . name ( ) ) ) ; return commandService . runCommand ( new File ( command ) , parameters ) . isSuccessful ( ) ; } catch ( FileNotFoundException e ) { logger . error ( "FileNotFoundException in deleteSymLink" , e ) ; return false ; } catch ( IOException e ) { logger . error ( "IOException in deleteSymLink" , e ) ; return false ; } }
public void test() { if ( rs . next ( ) ) { Long id = rs . getLong ( "id" ) ; LOGGER . debug ( "Found id: {}" , id ) ; return id ; } else { return null ; } }
@ Override public void generate ( Model model , MolgenisOptions options ) throws Exception { Template template = createTemplate ( "/" + getClass ( ) . getSimpleName ( ) + ".java.ftl" ) ; Map < String , Object > templateArgs = createTemplateArguments ( options ) ; File target = new File ( this . getDocumentationPath ( options ) + "/tabledoc.html" ) ; boolean created = target . getParentFile ( ) . mkdirs ( ) ; code_block = IfStatement ; templateArgs . put ( "model" , model ) ; OutputStream targetOut = new FileOutputStream ( target ) ; template . process ( templateArgs , new OutputStreamWriter ( targetOut , Charset . forName ( "UTF-8" ) ) ) ; targetOut . close ( ) ; logger . info ( "generated " + target ) ; }
public void test() { if ( logger . isWarnEnabled ( ) ) { logger . warn ( e . getMessage ( ) , e ) ; } }
public void test() { try { Configuration configuration = new ConfigurationImpl ( ) ; configuration . setPersistenceEnabled ( false ) ; configuration . setJournalDirectory ( DEFAULT_DATA_DIRECTORY ) ; configuration . setSecurityEnabled ( false ) ; configuration . addAcceptorConfiguration ( "amqp" , "tcp://127.0.0.1:5672?protocols=AMQP" ) ; configuration . addConnectorConfiguration ( "connector" , "tcp://127.0.0.1:5672" ) ; JMSConfiguration jmsConfig = new JMSConfigurationImpl ( ) ; ConnectionFactoryConfiguration cfConfig = new ConnectionFactoryConfigurationImpl ( ) . setName ( "cf" ) . setConnectorNames ( Arrays . asList ( "connector" ) ) . setBindings ( "cf" ) ; jmsConfig . getConnectionFactoryConfigurations ( ) . add ( cfConfig ) ; jmsServer = new EmbeddedJMS ( ) . setConfiguration ( configuration ) . setJmsConfiguration ( jmsConfig ) . start ( ) ; code_block = IfStatement ; } catch ( RuntimeException e ) { throw e ; } catch ( Exception e ) { LOG . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { currentStream = streamProvider . openStream ( ) ; currentWriter = streamSupport . createWriter ( currentPath , currentStream ) ; } catch ( IOException e ) { logger . error ( "Unable to open stream" , e ) ; throw e ; } }
public void test() { try { connected = connect ( ip_board , port_board ) ; } catch ( ArrayIndexOutOfBoundsException outEx ) { LOG . error ( "The ethernet port is not open" ) ; throw new UnableToExecuteException ( ) ; } catch ( NumberFormatException numberFormatException ) { LOG . error ( port_board + " is not a valid ethernet port to connect to" ) ; throw new UnableToExecuteException ( ) ; } }
public void test() { try { connected = connect ( ip_board , port_board ) ; } catch ( ArrayIndexOutOfBoundsException outEx ) { LOG . error ( "The object address '" + c . getProperty ( "address" ) + "' is not properly formatted. Check it!" ) ; throw new UnableToExecuteException ( ) ; } catch ( NumberFormatException numberFormatException ) { LOG . error ( port_board + " is not a valid ethernet port to connect to" ) ; throw new UnableToExecuteException ( ) ; } }
public void test() { try { String reply = sendToBoard ( message ) ; code_block = IfStatement ; } catch ( IOException iOException ) { setDescription ( "Unable to send the message to host " + address [ 0 ] + " on port " + address [ 1 ] ) ; LOG . error ( "Unable to send the message to host " + address [ 0 ] + " on port " + address [ 1 ] ) ; throw new UnableToExecuteException ( ) ; } finally { disconnect ( ) ; } }
public void startMongoServer ( Runtime runtime , String startMongoServerCommand ) throws IOException , InterruptedException { logger . info ( "starting to start MongoServer..." ) ; runtime . exec ( startMongoServerCommand ) ; logger . info ( "started.............." ) ; Thread . sleep ( 90000 ) ; }
public void startMongoServer ( Runtime runtime , String startMongoServerCommand ) throws IOException , InterruptedException { logger . info ( "Starting mongo server at ..........." + startMongoServerCommand ) ; runtime . exec ( startMongoServerCommand ) ; logger . info ( "Mongo server started." ) ; Thread . sleep ( 90000 ) ; }
@ Test public void testDebugShortCircuit ( ) { inner . setLevel ( Level . OFF ) ; assertTrue ( handler . isEmpty ( ) ) ; logger . info ( "debug" ) ; }
public void test() { try { client . advance ( ) ; } catch ( RuntimeException e ) { LOG . warn ( "Exception while invoking client" , e ) ; } }
public void test() { if ( dist > 100 ) { logger . warn ( "not able to deliver {} to {}" , dist , this ) ; return ; } }
public void test() { switch ( evt . getEventType ( ) ) { case MCREvent . CREATE_EVENT : handleFileCreated ( evt , file ) ; break ; case MCREvent . UPDATE_EVENT : handleFileUpdated ( evt , file ) ; break ; case MCREvent . DELETE_EVENT : handleFileDeleted ( evt , file ) ; break ; case MCREvent . REPAIR_EVENT : handleFileRepaired ( evt , file ) ; break ; case MCREvent . INDEX_EVENT : updateFileIndex ( evt , file ) ; break ; default : LOGGER . warn ( "Can't find file data for event type {}" , evt . getEventType ( ) ) ; break ; } }
public void test() { try { listener . onStart ( context ) ; } catch ( Throwable t ) { log . error ( t . getMessage ( ) , t ) ; } }
public void test() { if ( currentResource == null ) { log . info ( "Creating {} {}" , newResource . getKind ( ) , newResource . getMetadata ( ) . getName ( ) ) ; operation . create ( newResource ) ; } else-if ( comparator . compare ( currentResource , newResource ) != 0 ) { log . info ( "Updating {} {}" , newResource . getKind ( ) , newResource . getMetadata ( ) . getName ( ) ) ; operation . createOrReplace ( newResource ) ; } }
public void test() { if ( currentResource == null ) { log . info ( "Creating {} {}" , newResource . getKind ( ) , newResource . getMetadata ( ) . getName ( ) ) ; operation . create ( newResource ) ; } else-if ( comparator . compare ( currentResource , newResource ) != 0 ) { log . info ( "{} {} already updating" , newResource . getKind ( ) , newResource . getMetadata ( ) . getName ( ) ) ; operation . createOrReplace ( newResource ) ; } }
public void test() { try { handle . reset ( ) ; } catch ( Throwable e ) { LOG . error ( e . toString ( ) , e ) ; } }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; final StringBuilder stringBuilder = new StringBuilder ( ) ; String buffer ; code_block = WhileStatement ; return stringBuilder . toString ( ) ; } catch ( Exception e ) { logger . error ( "Exception occurred" , e ) ; throw e ; } finally { code_block = IfStatement ; } }
public void test() { try { bufferedReader . close ( ) ; } catch ( IOException exception ) { logger . warn ( "Failed to release the buffered reader" , exception ) ; } }
public ConnectionBuilder db ( String dbName ) { LOGGER . info ( "Database: {}" , dbName ) ; this . dbName = dbName ; return this ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { DLAppServiceUtil . cancelCheckOut ( fileEntryId ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
@ Override protected void shutDown ( ) { log . info ( "Shutdown requested" ) ; }
public void test() { try { writer . writeStartElement ( ADD_IDENTIFIER ) ; } catch ( final XMLStreamException e ) { final String message = "couldn't finish Solr Update XML export successfully" ; LOG . error ( message , e ) ; final DMPConverterException converterException = new DMPConverterException ( message , e ) ; throw DMPConverterError . wrap ( converterException ) ; } }
private void echoStartWorkers ( ) { started = System . nanoTime ( ) ; LOGGER . info ( HORIZONTAL_RULER ) ; LOGGER . info ( "Starting {} workers..." , name ( ) ) ; LOGGER . info ( HORIZONTAL_RULER ) ; LOGGER . info ( format ( "Starting %d Workers (%d members, %d clients)..." , count ( memberDeploymentPlan ) + count ( clientDeploymentPlan ) , count ( memberDeploymentPlan ) , count ( clientDeploymentPlan ) ) ) ; }
private void echoStartWorkers ( ) { started = System . nanoTime ( ) ; LOGGER . info ( HORIZONTAL_RULER ) ; LOGGER . info ( "Starting Workers..." ) ; LOGGER . info ( HORIZONTAL_RULER . toString ( ) ) ; LOGGER . info ( HORIZONTAL_RULER ) ; }
public void test() { try { StringBuilder query = new StringBuilder ( isAdd ? ADD_PAGE_METADATA_START : UPDATE_PAGE_METADATA_START ) ; query . append ( tableName ) . append ( isAdd ? ADD_PAGE_METADATA_END : UPDATE_PAGE_METADATA_END ) ; stat = conn . prepareStatement ( query . toString ( ) ) ; int index = 1 ; code_block = IfStatement ; stat . setString ( index ++ , pageMetadata . getGroup ( ) ) ; stat . setString ( index ++ , pageMetadata . getTitles ( ) . toXml ( ) ) ; stat . setString ( index ++ , pageMetadata . getModel ( ) . getCode ( ) ) ; code_block = IfStatement ; String extraConfig = this . getExtraConfig ( pageMetadata ) ; stat . setString ( index ++ , extraConfig ) ; Date updatedAt = pageMetadata . getUpdatedAt ( ) != null ? pageMetadata . getUpdatedAt ( ) : new Date ( ) ; stat . setTimestamp ( index ++ , updatedAt != null ? new java . sql . Timestamp ( updatedAt . getTime ( ) ) : null ) ; code_block = IfStatement ; stat . executeUpdate ( ) ; } catch ( Throwable t ) { logger . error ( "Error while saving the page metadata record for table {}" , tableName , t ) ; throw new RuntimeException ( "Error while saving the page metadata record for table " + tableName , t ) ; } finally { closeDaoResources ( null , stat ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { for ( Logger logger : this . loggers ) { logger . info ( message , e ) ; } }
public void test() { if ( getBasicProperties ( ) . isSuppressGenerateTask ( ) ) { LOG . info ( "Skipping generate task" ) ; return false ; } }
@ Override protected boolean begin ( ) { code_block = IfStatement ; _log . info ( "+------------------------------------------+" ) ; _log . info ( "|                                         |" ) ; _log . info ( "|          " ) ; _log . info ( "|                                    |" ) ; _log . info ( "+------------------------------------------+" ) ; DfDBFluteTaskStatus . getInstance ( ) . setTaskType ( TaskType . Generate ) ; return true ; }
public void syncHive ( ) { HiveSyncConfig hiveSyncConfig = DataSourceUtils . buildHiveSyncConfig ( props , cfg . targetBasePath , cfg . baseFileFormat ) ; HiveConf hiveConf = new HiveConf ( conf , HiveConf . class ) ; LOG . info ( "Hive Conf => " + hiveConf . getAllProperties ( ) . toString ( ) ) ; LOG . info ( "Hive Sync Path => " + hiveSyncConfig . toString ( ) ) ; new HiveSyncTool ( hiveSyncConfig , hiveConf , fs ) . syncHoodieTable ( ) ; }
public void syncHive ( ) { HiveSyncConfig hiveSyncConfig = DataSourceUtils . buildHiveSyncConfig ( props , cfg . targetBasePath , cfg . baseFileFormat ) ; LOG . info ( "Syncing target hoodie table with hive table(" + hiveSyncConfig . tableName + "). Hive metastore URL :" + hiveSyncConfig . jdbcUrl + ", basePath :" + cfg . targetBasePath ) ; HiveConf hiveConf = new HiveConf ( conf , HiveConf . class ) ; LOG . info ( "Hive Sync Conf => " + hiveSyncConfig . toString ( ) ) ; new HiveSyncTool ( hiveSyncConfig , hiveConf , fs ) . syncHoodieTable ( ) ; LOG . info ( "Synced target hoodie table with hive table(" + hiveSyncConfig . tableName + "). Hive hdbcUrl :" + hiveSyncConfig . jdbcUrl ) ; }
public void syncHive ( ) { HiveSyncConfig hiveSyncConfig = DataSourceUtils . buildHiveSyncConfig ( props , cfg . targetBasePath , cfg . baseFileFormat ) ; LOG . info ( "Syncing target hoodie table with hive table(" + hiveSyncConfig . tableName + "). Hive metastore URL :" + hiveSyncConfig . jdbcUrl + ", basePath :" + cfg . targetBasePath ) ; HiveConf hiveConf = new HiveConf ( conf , HiveConf . class ) ; LOG . info ( "Hive Conf => " + hiveConf . getAllProperties ( ) . toString ( ) ) ; new HiveSyncTool ( hiveSyncConfig , hiveConf , fs ) . syncHoodieTable ( ) ; LOG . info ( "Synced target hoodie table with hive table(" + hiveSyncConfig . tableName + "). Hive metastore URL :" + hiveSyncConfig . jdbcUrl + ", basePath :" + cfg . targetBasePath ) ; }
public void test() { try { RecordMetadata metadata = result . get ( 10 , TimeUnit . MILLISECONDS ) ; LOGGER . info ( "send success metadata={}" , metadata ) ; } catch ( Throwable t ) { LOGGER . error ( "send success metadata={}" , result , t ) ; } }
public void test() { -> { log . info ( "Connect timeout to {} ms." , s ) ; super . setConnectTimeout ( s ) ; } }
@ Override public void recordEvent ( Event e ) { log . info ( "Record event: {}" , e . getMessage ( ) ) ; }
public void test() { if ( ab != null ) { list . add ( ab . getTitle ( ) ) ; } else { log . error ( "could not find ab" ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; } catch ( SQLException sqle ) { LOG . error ( "Unable to close connection" , sqle ) ; return null ; } }
public void test() { if ( cachedBeanMetaDataInstances < totalCreatedMetaDataInstances ) { log . debug ( "totalCreatedMetaDataInstances:" + totalCreatedMetaDataInstances ) ; log . debug ( "cachedBeanMetaDataInstances:" + cachedBeanMetaDataInstances ) ; log . debug ( "cachedBeanMetaDataInstances:" + cachedBeanMetaDataInstances ) ; break ; } }
public void test() { try { List < Object > memoryConsumer = new ArrayList < > ( ) ; code_block = ForStatement ; } catch ( OutOfMemoryError e ) { log . error ( "OutOfMemoryError" ) ; log . debug ( "totalCreatedMetaDataInstances:" + totalCreatedMetaDataInstances ) ; log . debug ( "cachedBeanMetaDataInstances:" + cachedBeanMetaDataInstances ) ; } }
@ Override public boolean connect ( ) { logger . debug ( "Connecting to {}" , configuration . getUrl ( ) ) ; InfluxDBClientOptions . Builder optionsBuilder = InfluxDBClientOptions . builder ( ) . url ( configuration . getUrl ( ) ) . org ( configuration . getDatabaseName ( ) ) . bucket ( configuration . getRetentionPolicy ( ) ) ; char [ ] token = configuration . getTokenAsCharArray ( ) ; code_block = IfStatement ; InfluxDBClientOptions clientOptions = optionsBuilder . build ( ) ; final InfluxDBClient createdClient = InfluxDBClientFactory . create ( clientOptions ) ; this . client = createdClient ; queryAPI = createdClient . getQueryApi ( ) ; writeAPI = createdClient . getWriteApi ( ) ; return checkConnectionStatus ( ) ; }
private void handleException ( FailureReason aReason , String aMessage ) { LOGGER . debug ( aReason . getMessage ( ) ) ; myURLDataSource . setFailureReason ( aReason ) ; myURLDataSource . setErrorMessage ( aMessage ) ; }
public void test() { try { String value = URLDecoder . decode ( cookie . getValue ( ) , "UTF-8" ) ; if ( StringUtils . isNotBlank ( value ) ) return value ; LOGGER . info ( "Found cookie {}" , value ) ; } catch ( UnsupportedEncodingException e ) { LOGGER . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { String value = URLDecoder . decode ( cookie . getValue ( ) , "UTF-8" ) ; LOGGER . debug ( "Cookie: " + value + " (" + httpRequest . getRequestURI ( ) + ")" ) ; if ( StringUtils . isNotBlank ( value ) ) return value ; } catch ( UnsupportedEncodingException e ) { LOGGER . error ( "Error while decoding cookie" , e ) ; } }
public void test() { try { type = Class . forName ( typeClass ) . asSubclass ( DiscreteIndexType . class ) . newInstance ( ) ; } catch ( Exception e ) { LOGGER . warn ( "Unable to create Gerete index type" , e ) ; } }
public void test() { try { connection = startRequestToUrl ( appendPath ( serverUrl , path ) ) ; return connectionHandler . withConnection ( connection ) ; } catch ( Exception e ) { expectedErrorCount = incrementAndGetErrorCount ( expectedErrorCount ) ; code_block = IfStatement ; previousException = e ; log . error ( "Failed to connect to {}" , serverUrl , e ) ; } finally { HttpUtils . consumeAndClose ( connection ) ; } }
public void test() { try { VersionInfo version = client . getVersion ( ) ; code_block = IfStatement ; log . debugf ( "Kubernetes Version: %s.%s" , version . getMajor ( ) , version . getMinor ( ) ) ; serverFound = true ; return Result . enabled ( ) ; } catch ( Exception e ) { log . warn ( e . toString ( ) , e ) ; code_block = IfStatement ; } finally { client . close ( ) ; } }
public void test() { if ( ! displays . containsKey ( source ) ) { log . error ( "cannot remove {}" , source ) ; return ; } }
@ Override public void run ( ) { code_block = IfStatement ; code_block = TryStatement ;  code_block = IfStatement ; nextSchedule = getNextSchedule ( nextSchedule , cronExpression ) ; final long delay = getDelay ( nextSchedule ) ; logger . debug ( "Finished schedule for {} with delay {}" , nextSchedule , delay ) ; flowEngine . schedule ( this , delay , TimeUnit . MILLISECONDS ) ; }
public void test() { try { MerkleTopic merkleTopic = view . topics ( ) . get ( MerkleEntityId . fromTopicId ( txn . getConsensusUpdateTopic ( ) . getTopicID ( ) ) ) ; long rbsIncrease = getUpdateTopicRbsIncrease ( txn . getTransactionID ( ) . getTransactionValidStart ( ) , JKey . mapJKey ( merkleTopic . getAdminKey ( ) ) , JKey . mapJKey ( merkleTopic . getSubmitKey ( ) ) , merkleTopic . getMemo ( ) , merkleTopic . hasAutoRenewAccountId ( ) , lookupExpiry ( merkleTopic ) , txn . getConsensusUpdateTopic ( ) ) ; return getConsensusUpdateTopicFee ( txn , rbsIncrease , sigUsage ) ; } catch ( Exception illegal ) { log . error ( "Wrong transaction!" ) ; throw new InvalidTxBodyException ( illegal ) ; } }
public void test() { if ( attr instanceof Attributes ) { Attributes attrs = ( Attributes ) attr ; delta = attrs . size ( ) ; log . trace ( "delta to " + attr + " is " + delta ) ; } else { log . trace ( "delta to " + attr + " is " + delta ) ; } }
public void test() { if ( attr instanceof Attributes ) { Attributes attrs = ( Attributes ) attr ; delta = attrs . size ( ) ; log . trace ( "delta to " + attrs + " is " + delta ) ; } else { log . warn ( "delta to " + attrs + " is not " + delta ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception ex ) { String msg = "An Exception was caught while trying to load the driver. %s" ; String msgArg = ex . getLocalizedMessage ( ) ; log . error ( msg , ex ) ; throw new SQLException ( String . format ( msg , msgArg ) ) ; } }
public void test() { try { managedChannel . shutdownNow ( ) ; managedChannel . awaitTermination ( 1000L , TimeUnit . MILLISECONDS ) ; } catch ( Exception e ) { logger . warn ( "Failed to close managed channel" , e ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
@ Transactional ( rollbackFor = ArrowheadException . class ) public void removePlanEntryById ( final long id ) { logger . debug ( "removePlanEntryById started..." ) ; code_block = TryStatement ;  }
public void test() { try { code_block = IfStatement ; choreographerPlanRepository . deleteById ( id ) ; choreographerPlanRepository . flush ( ) ; } catch ( final InvalidParameterException ex ) { throw ex ; } catch ( final Exception ex ) { logger . debug ( ex . getMessage ( ) , ex ) ; throw new ArrowheadException ( CoreCommonConstants . DATABASE_OPERATION_EXCEPTION_MSG ) ; } }
public void test() { try { logTimeseriesDeleted ( user , entityId , keys , t ) ; } catch ( ThingsboardException e ) { log . error ( "Failed to log timeseries delete" , e ) ; } }
public void test() { if ( sizeInBytes > logRequestSizeLimit ) { LOG . warn ( "{} was larger than the request size." , logRequestSizeLimit ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception ex ) { log . error ( ex ) ; } }
public void test() { try { roleDao . createUser ( tenant , user . getUsername ( ) , password , null , userRoles ) ; } catch ( AlreadyExistsException e ) { getLogger ( ) . info ( Messages . getInstance ( ) . getString ( "USER.Already.Exists" , user . getUsername ( ) ) ) ; code_block = TryStatement ;  } catch ( Exception e ) { getLogger ( ) . info ( Messages . getInstance ( ) . getString ( "USER.ERROR" ) , e ) ; } }
@ Override public void lifeCycleStopping ( @ Nullable LifeCycle arg0 ) { LOGGER . debug ( "Cycle stopping" ) ; }
@ Override public void runEx ( RunnableEx runnable ) throws Exception { log . debug ( "runEx" ) ; startTransactionIfNeeded ( ) ; runnable . run ( ) ; flushAndClear ( ) ; }
@ Override public void onNext ( final TaskStop event ) { log . log ( Level . FINEST , "Task {0}" , event ) ; NoopTask . this . stopTask ( ) ; }
public void test() { if ( AdminUtils . topicExists ( zkUtils , topic ) ) { log . info ( "Deleting topic {}" , topic ) ; AdminUtils . deleteTopic ( zkUtils , topic ) ; log . info ( "Deleted Zookeeper topic {}" , topic ) ; } else { log . info ( "Topic {} doesn't exist" , topic ) ; } }
public void test() { try { checkNotNull ( embeddedAgentModule ) ; embeddedAgentModule . waitForSimpleRepoModule ( ) ; embeddedAgentModule . initEmbeddedServer ( ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { executeSendCommandWithNoResponse ( task ) ; } catch ( PDUException e ) { logger . error ( "executeSendCommandWithNoResponse error" , e ) ; } }
public void test() { if ( printInBackground ) { LOG . debug ( String . format ( "Executing blocking process %s" , commandLine . toString ( ) ) ) ; resultHandler = new PrintResultHandler ( watchdog ) ; executor . execute ( commandLine , resultHandler ) ; } else { LOG . debug ( String . format ( "Executing blocking process %s" , commandLine . toString ( ) ) ) ; successExitValue = executor . execute ( commandLine ) ; resultHandler = new PrintResultHandler ( successExitValue ) ; } }
public void test() { if ( printInBackground ) { LOG . debug ( String . format ( "Executing non-blocking process %s" , commandLine . toString ( ) ) ) ; resultHandler = new PrintResultHandler ( watchdog ) ; executor . execute ( commandLine , resultHandler ) ; } else { LOG . debug ( String . format ( "Executing non-blocking process %s" , commandLine . toString ( ) ) ) ; successExitValue = executor . execute ( commandLine ) ; resultHandler = new PrintResultHandler ( successExitValue ) ; } }
private static void logAndPrint ( String s ) { LOG . info ( s ) ; System . out . println ( s ) ; }
@ Bean @ ConditionalOnProperty ( "sap.security.services.xsuaa.uaadomain" ) public JwtDecoder hybridJwtDecoder ( XsuaaServiceConfiguration xsuaaConfig , IdentityServiceConfiguration identityConfig ) { LOGGER . debug ( "Initializing hybrid JwtDecoder" ) ; return new JwtDecoderBuilder ( ) . withIasServiceConfiguration ( identityConfig ) . withXsuaaServiceConfiguration ( xsuaaConfig ) . build ( ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { String s = ".encodeClose(): Raw Bytes - " + getHexDump ( allocated ) ; logger . debug ( CLASS_NAME + s ) ; } }
public void test() { if ( scheduledEventslogger . isTraceEnabled ( ) ) { scheduledEventslogger . trace ( "Removing scheduled events" ) ; } }
public void test() { if ( getManagementSupport ( ) . isNoLongerManaged ( ) ) { log . debug ( "{} No longer managed; ignoring" , this ) ; return ; } }
public void test() { try { code_block = IfStatement ; connectSensors ( ) ; } catch ( Throwable e ) { LOG . warn ( "Problem connecting sensors " + e , e ) ; Exceptions . propagateIfFatal ( e ) ; } }
@ Override public void postProcess ( DataSetLookup lookup , DataSet dataSet ) { LOG . info ( "Running DataSet: " + dataSet ) ; numberOfRunningDataSetLookups . labels ( dataSet . getUUID ( ) ) . dec ( ) ; dataSetExecution . labels ( dataSet . getUUID ( ) ) . inc ( ) ; final Long start = ( Long ) lookup . getMetadata ( PROMETHEUS_META ) ; code_block = IfStatement ; }
public void test() { try { String inputPath = "../../data/a9a/a9a_123d_train.libsvm" ; String savePath = LOCAL_FS + TMP_PATH + "/FMmodel" ; String logPath = LOCAL_FS + TMP_PATH + "/FMlog" ; conf . setInt ( AngelConf . ANGEL_PS_NUMBER , 4 ) ; conf . set ( AngelConf . ANGEL_TRAIN_DATA_PATH , inputPath ) ; conf . set ( AngelConf . ANGEL_SAVE_MODEL_PATH , savePath ) ; conf . set ( AngelConf . ANGEL_LOG_PATH , logPath ) ; conf . set ( AngelConf . ANGEL_ACTION_TYPE , MLConf . ANGEL_ML_TRAIN ( ) ) ; GraphRunner runner = new GraphRunner ( ) ; runner . train ( conf ) ; } catch ( Exception x ) { LOG . error ( "run trainOnLocalClusterTest failed " , x ) ; throw x ; } }
public void test() { if ( new File ( pathToWrite ) . exists ( ) ) { LOGGER . info ( "Already Exists: " + pathToWrite ) ; } else { code_block = TryStatement ;  } }
public void test() { try ( PrintWriter writer = new PrintWriter ( pathToWrite , "UTF-8" ) ; ) { code_block = ForStatement ; log . info ( "Done!" ) ; } catch ( Exception e ) { e . printStackTrace ( ) ; } }
public void test() { try { code_block = WhileStatement ; } catch ( IOException e ) { LOG . error ( "Caught IOException" , e ) ; } }
public void test() { try { var extension = context . getBean ( LogLevelOverrideExtension . class ) ; return Optional . of ( extension ) ; } catch ( NoSuchBeanDefinitionException e ) { log . debug ( "Unable to find extension {}" , extension , e ) ; return Optional . empty ( ) ; } }
public void test() { try { savedSock . close ( ) ; } catch ( IOException ex ) { LOGGER . log ( Level . WARNING , "Failed to close {0}" , savedSock , ex ) ; return false ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
@ Override public void commit ( ) throws SailException { log . debug ( "Committing Transaction" ) ; super . commit ( ) ; resetDedupBuffer ( ) ; }
public void test() { try { java . util . List < com . liferay . journal . model . JournalArticle > returnValue = JournalArticleServiceUtil . getArticlesByLayoutUuid ( groupId , layoutUuid , start , end ) ; return com . liferay . journal . model . JournalArticleSoap . toSoapModels ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try { final MailboxSession mailboxSession = session . getMailboxSession ( ) ; final SelectedMailbox selectedMailbox = session . getSelected ( ) ; final boolean isSelectedMailbox = selectedMailbox != null && selectedMailbox . getMailboxId ( ) . equals ( mailbox . getId ( ) ) ; final ComposedMessageId messageId = mailbox . appendMessage ( MessageManager . AppendCommand . builder ( ) . withInternalDate ( datetime ) . withFlags ( flagsToBeSet ) . isRecent ( ! isSelectedMailbox ) . build ( message ) , mailboxSession ) . getId ( ) ; code_block = IfStatement ; UidValidity uidValidity = mailbox . getMailboxEntity ( ) . getUidValidity ( ) ; unsolicitedResponses ( session , responder , false ) ; okComplete ( request , ResponseCode . appendUid ( uidValidity , new UidRange [ ] code_block = "" ; ) , responder ) ; } catch ( MailboxNotFoundException e ) { tryCreate ( request , responder , e ) ; } catch ( MailboxException e ) { LOG . error ( e . getMessage ( ) , e ) ; no ( request , responder , HumanReadableText . SAVE_FAILED ) ; } }
public void test() { if ( ! file . delete ( ) ) { logger . warn ( "Unable to clean up file: " + file ) ; } }
public void test() { if ( entityName . endsWith ( "DO" ) ) { log . error ( msg ) ; } else { log . info ( msg ) ; } }
public void test() { if ( entityName . endsWith ( "DO" ) ) { log . error ( msg ) ; } else { log . trace ( msg ) ; } }
public void test() { if ( columnLengthFailedSet . contains ( getKey ( entityName , propertyName ) ) ) { log . debug ( "Column length failed for entity: {}" , entityName ) ; return null ; } }
@ Then ( "^the \"([^\"]*)\" meter reads gas result should be returned$" ) public void theMeterReadsGasResultShouldBeReturned ( final String periodType , final Map < String , String > settings ) throws Throwable { final PeriodicMeterReadsGasAsyncRequest asyncRequest = PeriodicMeterReadsGasRequestFactory . fromScenarioContext ( ) ; final PeriodicMeterReadsGasResponse response = this . responseClient . getResponse ( asyncRequest ) ; LOGGER . info ( "meter reads gas result is {}" , response ) ; assertThat ( response ) . as ( "PeriodicMeterReadsGasResponse should not be null" ) . isNotNull ( ) ; assertThat ( response . getPeriodType ( ) ) . as ( "PeriodType should match" ) . isEqualTo ( PeriodType . fromValue ( periodType ) ) ; assertThat ( response . getPeriodicMeterReadsGas ( ) ) . as ( "Expected periodic meter reads gas" ) . isNotNull ( ) ; }
public void test() { try ( Connection con = getDatasource ( ) . getConnection ( ) ; PreparedStatement stmt = con . prepareStatement ( query ) ; ) { stmt . setString ( 1 , status . name ( ) ) ; stmt . setString ( 2 , instanceId ) ; stmt . executeUpdate ( ) ; } catch ( SQLException e ) { logger . error ( "Caught SQLException: " , e ) ; throw e ; } }
private static void loadTpchTopic ( EmbeddedKafka embeddedKafka , TestingPrestoClient prestoClient , TpchTable < ? > table ) { long start = System . nanoTime ( ) ; log . info ( "Loading %s" , table . getTableName ( ) ) ; TestUtils . loadTpchTopic ( embeddedKafka , prestoClient , kafkaTopicName ( table ) , new QualifiedObjectName ( "tpch" , TINY_SCHEMA_NAME , table . getTableName ( ) . toLowerCase ( ENGLISH ) ) ) ; log . info ( "Imported %s in %s" , 0 , table . getTableName ( ) , nanosSince ( start ) . convertToMostSuccinctTimeUnit ( ) ) ; }
private static void loadTpchTopic ( EmbeddedKafka embeddedKafka , TestingPrestoClient prestoClient , TpchTable < ? > table ) { long start = System . nanoTime ( ) ; log . info ( "Running import for %s" , table . getTableName ( ) ) ; TestUtils . loadTpchTopic ( embeddedKafka , prestoClient , kafkaTopicName ( table ) , new QualifiedObjectName ( "tpch" , TINY_SCHEMA_NAME , table . getTableName ( ) . toLowerCase ( ENGLISH ) ) ) ; log . info ( "Imported %s in %s" , embeddedKafka , table . getTableName ( ) ) ; }
@ Override public void warn ( Marker marker , String message ) { getLogger ( ) . warn ( marker , message ) ; }
public void test() { if ( warnWhenFull ) { LOGGER . warn ( warnWhenFull ) ; } }
public void test() { if ( event . isPre ( ) ) { log . trace ( "Pre- (pre) " + event . getEntries ( ) . size ( ) + " pre from the cache " + event . getCache ( ) . getName ( ) ) ; } else { log . trace ( "Evicted " + event . getEntries ( ) . size ( ) + " entries from the cache " + event . getCache ( ) . getName ( ) ) ; } }
public void test() { if ( event . isPre ( ) ) { log . trace ( "Going to evict " + event . getEntries ( ) . size ( ) + " entries from the cache " + event . getCache ( ) . getName ( ) ) ; } else { log . debug ( "Going to evict from the cache " + event . getCache ( ) . getName ( ) ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( MBMessageServiceUtil . class , "addMessage" , _addMessageParameterTypes3 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , categoryId , subject , body , serviceContext ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . message . boards . model . MBMessage ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { long createdTime = setAndGetCreatedTime ( new Path ( file . toString ( ) ) , tableId . toString ( ) ) ; stat = Status . newBuilder ( stat ) . setCreatedTime ( createdTime ) . build ( ) ; value = ProtobufUtil . toValue ( stat ) ; log . debug ( "Status was lacking createdTime, set to {} for {}" , createdTime , file ) ; } catch ( IOException e ) { log . warn ( "Failed to write status mutation for replication, will retry" , e ) ; return false ; } catch ( MutationsRejectedException e ) { log . warn ( "Failed to write status mutation for replication, will retry" , e ) ; return false ; } }
public void test() { try { long createdTime = setAndGetCreatedTime ( new Path ( file . toString ( ) ) , tableId . toString ( ) ) ; stat = Status . newBuilder ( stat ) . setCreatedTime ( createdTime ) . build ( ) ; value = ProtobufUtil . toValue ( stat ) ; log . debug ( "Status was lacking createdTime, set to {} for {}" , createdTime , file ) ; } catch ( IOException e ) { log . warn ( "Failed to get file status, will retry" , e ) ; return false ; } catch ( MutationsRejectedException e ) { log . warn ( "Failed to remove file status, will retry" , e ) ; return false ; } }
public void runStep ( ChoreographerStep step , long sessionId ) throws InterruptedException { logger . debug ( "Running " + step . getId ( ) ) ; ChoreographerRunningStep runningStep = insertInitiatedRunningStep ( step . getId ( ) , sessionId ) ; ServiceQueryFormDTO serviceQuery = new ServiceQueryFormDTO ( ) ; serviceQuery . setServiceDefinitionRequirement ( step . getServiceName ( ) . toLowerCase ( ) ) ; code_block = IfStatement ; final OrchestrationFormRequestDTO orchestrationForm = new OrchestrationFormRequestDTO . Builder ( requesterSystem ) . requestedService ( serviceQuery ) . flag ( OrchestrationFlags . Flag . MATCHMAKING , true ) . flag ( OrchestrationFlags . Flag . OVERRIDE_STORE , true ) . flag ( OrchestrationFlags . Flag . TRIGGER_INTER_CLOUD , false ) . build ( ) ; final OrchestrationResponseDTO orchestrationResponse = queryOrchestrator ( orchestrationForm ) ; List < OrchestrationResultDTO > orchestrationResultList = orchestrationResponse . getResponse ( ) ; logger . debug ( orchestrationResultList ) ; ChoreographerSessionRunningStepDataDTO runningStepDataDTO = new ChoreographerSessionRunningStepDataDTO ( sessionId , runningStep . getId ( ) ) ; code_block = IfStatement ; }
public void test() { try { Home home = client . getHome ( serialNumber , sessionId ) ; code_block = IfStatement ; Map < String , String > map = home . getSerializedMap ( ) ; code_block = IfStatement ; OneTouch [ ] oneTouches = client . getOneTouch ( serialNumber , sessionId ) ; Auxiliary [ ] auxes = client . getAux ( serialNumber , sessionId ) ; code_block = IfStatement ; code_block = ForStatement ; code_block = ForStatement ; code_block = IfStatement ; } catch ( IOException e ) { logger . debug ( "Exception polling" , e ) ; code_block = IfStatement ; } catch ( NotAuthorizedException e ) { logger . debug ( "Login denied" , e ) ; clearPolling ( ) ; configure ( ) ; } }
public void test() { if ( duration > MAX_DURATION ) { LOG . info ( "Duration has been set for the duration: {}." , duration ) ; duration = MAX_DURATION ; } }
public void test() { if ( dataSourceClassName != null ) { LOGGER . log ( Level . INFO , "Using specified DataSourceClass name: " + dataSourceClassName ) ; } }
public void test() { if ( driverClassName != null ) { LOGGER . error ( "{} - cannot use driverClassName and dataSourceClassName together." , poolName ) ; throw new IllegalStateException ( "cannot use driverClassName and dataSourceClassName together." ) ; } else-if ( jdbcUrl != null ) { LOGGER . error ( "{} - cannot use jdbcUrl and dataSourceClassName together." , poolName ) ; } }
public void test() { if ( dataSource != null ) { code_block = IfStatement ; } else-if ( dataSourceClassName != null ) { code_block = IfStatement ; } else-if ( jdbcUrl != null || dataSourceJndiName != null ) { LOGGER . error ( "{} - jdbcUrl is required with jdbcUrl {}." , poolName , jdbcUrl ) ; } else-if ( driverClassName != null ) { throw new IllegalArgumentException ( "jdbcUrl is required with driverClassName." ) ; } else { LOGGER . error ( "{} - dataSource or dataSourceClassName or jdbcUrl is required." , poolName ) ; throw new IllegalArgumentException ( "dataSource or dataSourceClassName or jdbcUrl is required." ) ; } }
public void test() { if ( dataSource != null ) { code_block = IfStatement ; } else-if ( dataSourceClassName != null ) { code_block = IfStatement ; } else-if ( jdbcUrl != null || dataSourceJndiName != null ) { LOGGER . error ( "{} - jdbcUrl is required with jdbcUrl." , poolName ) ; } else-if ( driverClassName != null ) { LOGGER . error ( "{} - jdbcUrl is required with driverClassName." , poolName ) ; throw new IllegalArgumentException ( "jdbcUrl is required with driverClassName." ) ; } else { throw new IllegalArgumentException ( "dataSource or dataSourceClassName or jdbcUrl is required." ) ; } }
private HttpURLConnection connect ( URL url ) throws IOException { final URLConnectionFactory factory = new URLConnectionFactory ( settings ) ; final HttpURLConnection conn = factory . createHttpURLConnection ( url , useProxy ) ; conn . setDoOutput ( true ) ; conn . addRequestProperty ( "X-Result-Detail" , "info" ) ; final String username = settings . getString ( Settings . KEYS . ANALYZER_ARTIFACTORY_API_USERNAME ) ; final String apiToken = settings . getString ( Settings . KEYS . ANALYZER_ARTIFACTORY_API_TOKEN ) ; code_block = IfStatement ; LOG . debug ( "Connecting to {}" , username ) ; conn . connect ( ) ; return conn ; }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "JDBC::{}" , sql ) ; } }
public void test() { if ( e instanceof org . apache . airavata . service . profile . user . cpi . exception . UserProfileServiceException ) { result . upe = ( org . apache . airavata . service . profile . user . cpi . exception . UserProfileServiceException ) e ; result . setUpeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AuthorizationException ) { result . ae = ( org . apache . airavata . model . error . AuthorizationException ) e ; result . setAeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof org . apache . airavata . service . profile . user . cpi . exception . UserProfileServiceException ) { result . upe = ( org . apache . airavata . service . profile . user . cpi . exception . UserProfileServiceException ) e ; result . setUpeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AuthorizationException ) { result . ae = ( org . apache . airavata . model . error . AuthorizationException ) e ; result . setAeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof org . apache . airavata . service . profile . user . cpi . exception . UserProfileServiceException ) { result . upe = ( org . apache . airavata . service . profile . user . cpi . exception . UserProfileServiceException ) e ; result . setUpeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AuthorizationException ) { result . ae = ( org . apache . airavata . model . error . AuthorizationException ) e ; result . setAeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { try { fcall . sendResponse ( fb , msg , msgType , seqid ) ; } catch ( java . lang . Exception ex ) { _LOGGER . error ( "Exception writing to internal frame buffer" , ex ) ; fb . close ( ) ; } }
public void test() { try { List < String > protocolStrings = objectMapper . readValue ( protocols , new TypeReference < List < String > > ( ) code_block = "" ; ) ; code_block = ForStatement ; return protocolList ; } catch ( Exception e ) { logger . error ( "INVALID configured test protocols " + protocols ) ; throw new IllegalStateException ( "INVALID configured test protocols " + protocols ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( authscheme == null ) { logger . debug ( "Unknown OAuth scheme" ) ; return ; } }
public void test() { if ( method . getHostAuthState ( ) . isPreemptive ( ) ) { log . warn ( "Unexpected authentication attempt for host {}" , host ) ; } }
public void test() { if ( instanceId == null ) { return ; } }
public boolean cleanupNode ( final RegionAndId regionAndId ) { String instanceId = regionAndId . id ( ) ; InstanceStatus instanceStatus = Iterables . tryFind ( api . instanceApi ( ) . listInstanceStatus ( regionAndId . regionId ( ) ) . concat ( ) , new InstanceStatusPredicate ( instanceId ) ) . orNull ( ) ; if ( instanceStatus == null ) return true ; code_block = IfStatement ; logger . info ( "Removing instance " + instanceId ) ; api . instanceApi ( ) . delete ( instanceId ) ; return instanceTerminatedPredicate . apply ( RegionAndId . slashEncodeRegionAndId ( regionAndId ) ) ; }
public void test() { if ( localCoordinator != null ) { localCoordinator . audioSink . process ( audioStream ) ; } else { logger . warn ( "Coordinator is null." ) ; } }
public void onFailure ( Throwable failure ) { submitDialog . hide ( ) ; failure . printStackTrace ( System . err ) ; Dialog . error ( "Server-side Production Error" , failure . getMessage ( ) ) ; }
public void test() { try { final DeflaterOutputStream dzip = new DeflaterOutputStream ( out ) ; dzip . write ( stringified . getBytes ( ) ) ; dzip . close ( ) ; encodedResult = Base64 . encodeBase64String ( out . toByteArray ( ) ) ; } catch ( final IOException e ) { LOGGER . error ( "Error encoding base64 string" , e ) ; } }
public List < GenPolynomial < C > > GB ( int modv , List < GenPolynomial < C > > F ) { List < GenPolynomial < C > > G = normalizeZerosOnes ( F ) ; G = PolyUtil . < C > monic ( G ) ; code_block = IfStatement ; GenPolynomialRing < C > ring = G . get ( 0 ) . ring ; code_block = IfStatement ; PairList < C > pairlist = strategy . create ( modv , ring ) ; pairlist . put ( G ) ; logger . info ( "start   " + pairlist ) ; Terminator fin = new Terminator ( threads ) ; code_block = ForStatement ; fin . waitDone ( ) ; code_block = IfStatement ; logger . debug ( "parallel list = " + G . size ( ) ) ; G = minimalGB ( G ) ; logger . info ( "end   " + pairlist ) ; return G ; }
public List < GenPolynomial < C > > GB ( int modv , List < GenPolynomial < C > > F ) { List < GenPolynomial < C > > G = normalizeZerosOnes ( F ) ; G = PolyUtil . < C > monic ( G ) ; code_block = IfStatement ; GenPolynomialRing < C > ring = G . get ( 0 ) . ring ; code_block = IfStatement ; PairList < C > pairlist = strategy . create ( modv , ring ) ; pairlist . put ( G ) ; logger . info ( "start " + pairlist ) ; Terminator fin = new Terminator ( threads ) ; code_block = ForStatement ; fin . waitDone ( ) ; code_block = IfStatement ; logger . info ( "" ) ; G = minimalGB ( G ) ; logger . info ( "end   " + pairlist ) ; return G ; }
public List < GenPolynomial < C > > GB ( int modv , List < GenPolynomial < C > > F ) { List < GenPolynomial < C > > G = normalizeZerosOnes ( F ) ; G = PolyUtil . < C > monic ( G ) ; code_block = IfStatement ; GenPolynomialRing < C > ring = G . get ( 0 ) . ring ; code_block = IfStatement ; PairList < C > pairlist = strategy . create ( modv , ring ) ; pairlist . put ( G ) ; logger . info ( "start " + pairlist ) ; Terminator fin = new Terminator ( threads ) ; code_block = ForStatement ; fin . waitDone ( ) ; code_block = IfStatement ; logger . debug ( "parallel list = " + G . size ( ) ) ; G = minimalGB ( G ) ; logger . info ( "" + pairlist ) ; return G ; }
@ Test public void testLoginButton ( ) { logger . startPage ( GeoServerHomePage . class ) ; String html = tester . getLastResponseAsString ( ) ; assertTrue ( html . contains ( "<form style=\"display: inline-block;\" method=\"post\" action=\"../web/j_spring_oauth2_geonode_login\">" ) ) ; assertTrue ( html . contains ( "<img src=\"./wicket/resource/org.geoserver.web.security.oauth2.GeoNodeOAuth2AuthProviderPanel/geonode" ) ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try ( BufferedReader upgradeLogReader = new BufferedReader ( new FileReader ( FSFactoryProducer . getFSFactory ( ) . getFile ( UpgradeLog . getUpgradeLogPath ( ) ) ) ) ) { String line = null ; code_block = WhileStatement ; } catch ( IOException e ) { LOGGER . error ( "Unable to read upgrade log file" , e ) ; } finally { FSFactoryProducer . getFSFactory ( ) . getFile ( UpgradeLog . getUpgradeLogPath ( ) ) . delete ( ) ; } }
public String getClientID ( ) { logger . trace ( "getClientID()" ) ; return this . clientID ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { String [ ] parts = getPathParts ( request ) ; String pid = parts [ 1 ] ; resAttr = ResourceAttributes . getResources ( parts ) ; code_block = IfStatement ; code_block = IfStatement ; actions . put ( Constants . ACTION . ID . getURI ( ) , Constants . ACTION . EXPORT . getStringAttribute ( ) ) ; actions . put ( Constants . ACTION . API . getURI ( ) , Constants . ACTION . APIM . getStringAttribute ( ) ) ; req = getContextHandler ( ) . buildRequest ( getSubjects ( request ) , actions , resAttr , getEnvironment ( request ) ) ; LogUtil . statLog ( request . getRemoteUser ( ) , Constants . ACTION . EXPORT . uri , pid , null ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; throw new ServletException ( e . getMessage ( ) , e ) ; } }
protected List < String > createEntity ( ArrayNode entities ) throws AtlasServiceException { LOG . debug ( "Creating entities" ) ; ObjectNode response = callAPIWithBody ( API_V1 . CREATE_ENTITY , entities . toString ( ) ) ; List < String > results = extractEntityResult ( response ) . getCreatedEntities ( ) ; LOG . debug ( "Create entities returned: {}" , results ) ; return results ; }
protected List < String > createEntity ( ArrayNode entities ) throws AtlasServiceException { LOG . debug ( "Creating entities: {}" , entities ) ; ObjectNode response = callAPIWithBody ( API_V1 . CREATE_ENTITY , entities . toString ( ) ) ; List < String > results = extractEntityResult ( response ) . getCreatedEntities ( ) ; LOG . debug ( "Created {} entities" , results ) ; return results ; }
public void test() { try { LocalDate actionDate = LocalDate . from ( DateUtils . LRS_ACTIONS_DATE . parse ( billActionMatcher . group ( 1 ) ) ) ; String actionText = billActionMatcher . group ( 2 ) ; Chamber actionChamber = StringUtils . isAllUpperCase ( actionText . replaceAll ( "[^a-zA-Z]+" , "" ) ) ? Chamber . SENATE : Chamber . ASSEMBLY ; billActions . add ( new BillAction ( actionDate , actionText , actionChamber , ++ sequenceNo , daybreakBill . getBaseBillId ( ) ) ) ; } catch ( DateTimeParseException ex ) { logger . error ( "Error while getting Bill Action!" ) ; logger . error ( ex . getMessage ( ) ) ; } }
public void test() { try { lastFlow = flow ; flow . setCandidates ( result ) ; flow . setSpec ( allocationSpec ) ; flow . setTrigger ( this ) ; flow . setPaginationInfo ( paginationInfo ) ; flow . allocate ( ) ; } catch ( OperationFailureException ofe ) { code_block = IfStatement ; } catch ( Throwable t ) { completion . fail ( inerr ( t . getMessage ( ) ) ) ; log . error ( t . getMessage ( ) , t ) ; } }
public void test() { try { ifaces = NetworkInterface . getNetworkInterfaces ( ) ; } catch ( SocketException e ) { logger . error ( "Error while getting network interfaces" , e ) ; } }
public void test() { try { _changesetCollectionLocalService . deleteChangesetCollection ( changesetCollection . getChangesetCollectionId ( ) ) ; } catch ( PortalException portalException ) { _log . error ( portalException , portalException ) ; } }
public void test() { switch ( _dataType ) { case BOOL : final char first = _value . charAt ( 0 ) ; return first == '0' ? "false" : "true" ; case ERROR : logger . warn ( "Error-cell occurred: {}" , _value ) ; return _value . toString ( ) ; case FORMULA : return _value . toString ( ) ; case INLINESTR : final XSSFRichTextString rtsi = new XSSFRichTextString ( _value . toString ( ) ) ; return rtsi . toString ( ) ; case SSTINDEX : final String sstIndex = _value . toString ( ) ; final int idx = Integer . parseInt ( sstIndex ) ; final RichTextString item = _sharedStringTable . getItemAt ( idx ) ; return item . getString ( ) ; case NUMBER : final String numberString = _value . toString ( ) ; code_block = IfStatement ; default : logger . warn ( "Unknown data type: {}" , _value ) ; return "" ; } }
@ Override public void onSuccess ( @ Nullable final Collection < SgtInfo > result ) { final Integer counter = Optional . ofNullable ( result ) . map ( Collection :: size ) . orElse ( 0 ) ; LOG . debug ( "ping successful, outcome: {}" , counter ) ; storeOutcome ( true , counter , null ) ; }
public void test() { try { code_block = IfStatement ; } catch ( ParseException pe ) { log . error ( pe . getMessage ( ) ) ; } }
@ Test public void testSlowAppendWithoutTimeout ( ) throws InterruptedException , LifecycleException , EventDeliveryException , IOException { log . info ( "Starting..." ) ; slowAppendTestHelper ( 0 ) ; }
public void test() { try { return new BufferedWriter ( new OutputStreamWriter ( fs . create ( new Path ( filePath ) ) ) ) ; } catch ( IOException e ) { log . error ( "Exception creating file " + filePath , e ) ; return null ; } }
public void test() { if ( t instanceof RuntimeException ) { logger . error ( "Error happened during task execution" , t ) ; exceptionHolder . setExpected ( false ) ; exceptionHolder . setException ( ( Exception ) t ) ; } else-if ( t instanceof Exception ) { logger . debug ( "Error happened during task execution" , t ) ; exceptionHolder . setExpected ( true ) ; exceptionHolder . setException ( ( Exception ) t ) ; } else { String msg = "A throwable was thrown while executing the task, this is most likely a severe issue." ; logger . error ( msg , t ) ; exceptionHolder . setExpected ( false ) ; exceptionHolder . setException ( new Exception ( msg , t ) ) ; } }
public void test() { if ( t instanceof RuntimeException ) { logger . error ( "Unexpected error happened while executing the task " + "(if the error is known to happen it should be caught and wrapped " + "into an checked exception to stop logging it as an error)" , t ) ; exceptionHolder . setExpected ( false ) ; exceptionHolder . setException ( ( Exception ) t ) ; } else-if ( t instanceof Exception ) { logger . info ( "Exception happened while executing the task " + "(if the error is known to happen it should be caught and wrapped " + "into an checked exception to stop logging it as an error)" , t ) ; exceptionHolder . setExpected ( true ) ; exceptionHolder . setException ( ( Exception ) t ) ; } else { String msg = "A throwable was thrown while executing the task, this is most likely a severe issue." ; logger . error ( msg , t ) ; exceptionHolder . setExpected ( false ) ; exceptionHolder . setException ( new Exception ( msg , t ) ) ; } }
public void test() { if ( t instanceof RuntimeException ) { logger . error ( "Unexpected error happened while executing the task " + "(if the error is known to happen it should be caught and wrapped " + "into an checked exception to stop logging it as an error)" , t ) ; exceptionHolder . setExpected ( false ) ; exceptionHolder . setException ( ( Exception ) t ) ; } else-if ( t instanceof Exception ) { logger . debug ( "Error happened during task execution" , t ) ; exceptionHolder . setExpected ( true ) ; exceptionHolder . setException ( ( Exception ) t ) ; } else { String msg = "A throwable was thrown while executing the task, this is most likely a severe issue." ; logger . error ( msg , t ) ; exceptionHolder . setExpected ( false ) ; exceptionHolder . setException ( new Exception ( msg , t ) ) ; } }
private void waitUntilServiceIsReady ( ) throws Exception { log . debug ( "Wait until service is ready" ) ; getProvisionedServiceItem ( ) . expandServiceItem ( ) ; selenium . takeScreenShot ( ) ; selenium . getDriverWait ( ) . withTimeout ( Duration . ofMinutes ( 5 ) ) . until ( ExpectedConditions . numberOfElementsToBe ( By . className ( "alert-info" ) , 0 ) ) ; selenium . takeScreenShot ( ) ; }
public void test() { if ( isEnabled ( LogLevel . DEBUG ) ) { this . logger . debug ( buildMessage ( message ) ) ; } }
public void run ( ) { log . info ( "Started event send for read" ) ; code_block = TryStatement ;  log . info ( "Completed event send for read" ) ; }
public void test() { try { code_block = WhileStatement ; } catch ( RuntimeException ex ) { log . error ( "Exception encountered: " + ex . getMessage ( ) , ex ) ; exception = ex ; } }
public void run ( ) { log . info ( "Started event send for read" ) ; code_block = TryStatement ;  log . info ( "Completed event send for read" ) ; }
public void test() { if ( log . isInfoEnabled ( ) ) { log . info ( msg ) ; } }
public void test() { while ( mat . find ( ) ) { String found = mat . group ( ) ; found = "mailto://" + found ; LOGGER . debug ( "*******6 mailfound=\"{}\" after cleanup 6" , found ) ; String host = hostFromUriStr ( found ) ; LOGGER . debug ( "*******6 mailfound=\"{}\" after cleanup 6" , host ) ; code_block = IfStatement ; } }
@ RequestMapping ( value = Constants . UDR_GET_BY_AUTHOR_TITLE , method = RequestMethod . GET ) public JsonServiceResponse getUDR ( @ PathVariable String authorId , @ PathVariable String title ) { logger . debug ( "Getting UDR for : {}" , authorId ) ; UdrSpecification resp = udrService . fetchUdr ( authorId , title ) ; return new JsonServiceResponse ( Status . SUCCESS , "GET UDR was successful" , resp ) ; }
@ Test @ Order ( 2 ) void testWhenCaptured ( ) { log . warn ( "Captured error: " + logCapture . getMessage ( 3 ) ) ; assertEquals ( "captured error" , logCapture . getMessage ( 3 ) ) ; }
public void test() { try { code_block = IfStatement ; } catch ( Throwable e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { dataSources = OBJECT_MAPPER . readValue ( dataSourcesUrl , DataSources . class ) ; } catch ( IOException e ) { LOG . error ( "Couldn't parse " + dataSourcesUrl , e ) ; } }
@ Override public void startBundles ( String symbolicName , String ... additionalSymbolicNames ) { Set < String > toStart = toSet ( symbolicName , additionalSymbolicNames ) ; String bundlesNames = String . join ( ", " , toStart ) ; LOGGER . info ( "Starting bundles: {}" , bundlesNames ) ; long startTime = System . currentTimeMillis ( ) ; code_block = ForStatement ; LOGGER . info ( "Finished starting bundles in [{}] ms" , ( System . currentTimeMillis ( ) - startTime ) ) ; }
@ Override public void startBundles ( String symbolicName , String ... additionalSymbolicNames ) { Set < String > toStart = toSet ( symbolicName , additionalSymbolicNames ) ; String bundlesNames = String . join ( ", " , toStart ) ; LOGGER . info ( "Starting the following bundles:[{}]" , bundlesNames ) ; long startTime = System . currentTimeMillis ( ) ; code_block = ForStatement ; LOGGER . info ( "Started the following bundles: {}" , System . currentTimeMillis ( ) - startTime ) ; }
public void test() { try { List < ApiDefinitionEntity > apis = sentinelApiClient . fetchApis ( app , ip , port ) . get ( ) ; repository . saveAll ( apis ) ; return Result . ofSuccess ( apis ) ; } catch ( Throwable throwable ) { logger . error ( "Error fetching apis for app={}, ip={}, port={}" , app , ip , port , throwable ) ; return Result . ofThrowable ( - 1 , throwable ) ; } }
public void test() { try { String fieldName = fieldNameNode . getIdentifier ( ) ; String txtIndex = fieldName . substring ( fieldName . indexOf ( '(' ) + 1 , fieldName . indexOf ( ')' ) ) ; return Integer . parseInt ( txtIndex ) ; } catch ( Exception e ) { log . error ( e . getMessage ( ) ) ; return null ; } }
public void test() { if ( paths . isEmpty ( ) ) { LOG . error ( "No paths specified!" ) ; System . exit ( - 1 ) ; } }
public void test() { try { demo . run ( paths ) ; } catch ( Throwable t ) { LOG . error ( " demo failed" , t ) ; } }
@ Test public void testSendTextMessage ( ) throws Exception { String expectedBody = "Hello there!" ; template . sendBodyAndHeader ( startEndpointUri , expectedBody , "cheese" , 123 ) ; listener . assertMessagesArrived ( 1 , 5000 ) ; List < Message > list = listener . flushMessages ( ) ; assertFalse ( list . isEmpty ( ) , "Should have received some messages!" ) ; Message message = list . get ( 0 ) ; LOG . info ( "Received: {}" , message ) ; TextMessage textMessage = assertIsInstanceOf ( TextMessage . class , message ) ; assertEquals ( expectedBody , textMessage . getText ( ) , "Text message body: " + textMessage ) ; }
public static void errorLogging ( Logger logger , short responseCommand ) { logger . warn ( "Gateway response {} ({}) cannot be handled" , Command . get ( responseCommand ) . toString ( ) , new CommandNumber ( responseCommand ) . toString ( ) ) ; }
public static void errorLogging ( Logger logger , short responseCommand ) { logger . error ( "setResponse(): error message={}" , new CommandNumber ( responseCommand ) . toString ( ) ) ; logger . trace ( "setResponse(): cannot handle response {} ({})." , Command . get ( responseCommand ) . toString ( ) , new CommandNumber ( responseCommand ) . toString ( ) ) ; }
public void test() { switch ( next ) { case NORMAL : logger . debug ( LogMarker . DISK_STORE_MONITOR_MARKER , "The disk volume {} for log files has exceeded the system usage threshold and is {} full." , args ) ; break ; case WARN : case CRITICAL : logger . warn ( LogMarker . DISK_STORE_MONITOR_MARKER , "The disk volume {} for log files has exceeded the warning usage threshold and is {} full." , args ) ; break ; } }
public void test() { switch ( next ) { case NORMAL : logger . info ( LogMarker . DISK_STORE_MONITOR_MARKER , "The disk volume {} for log files has returned to normal usage levels and is {} full." , args ) ; break ; case WARN : case CRITICAL : logger . warn ( LogMarker . DISK_STORE_MONITOR_MARKER , "The disk volume {} for log files has returned to normal usage levels and is {} full." , args ) ; break ; } }
public void test() { if ( isTrace ) { StringBuilder txt = new StringBuilder ( ) ; txt . append ( "Ending dispatch." ) ; txt . append ( " dispatched type: " ) . append ( oldde . type ) ; txt . append ( ",   remaining nesting levels: " ) . append ( dispatches . size ( ) ) ; code_block = IfStatement ; LOG . debug ( txt . toString ( ) ) ; } }
public void test() { for ( String cache : pools ) { code_block = IfStatement ; } }
public void test() { for ( String cache : pools ) { code_block = IfStatement ; } }
public void test() { for ( String cache : pools ) { code_block = IfStatement ; } }
public void test() { try { LoggingFactory . init ( Level . INFO ) ; Platform . setVirtual ( false ) ; String port = "COM7" ; int pin = 22 ; boolean useHobbyServo = true ; SwingGui gui = ( SwingGui ) Runtime . start ( "gui" , "SwingGui" ) ; Arduino mega = ( Arduino ) Runtime . start ( "mega" , "Arduino" ) ; ServoControl servo = null ; servo = ( ServoControl ) Runtime . start ( "servo" , "Servo" ) ; Service . sleep ( 500 ) ; gui . setActiveTab ( "servo" ) ; code_block = IfStatement ; mega . connect ( port ) ; servo . setPin ( pin ) ; log . info ( "rest is {}" , servo . getRest ( ) ) ; servo . attach ( mega ) ; servo . moveTo ( 10.3 ) ; servo . moveTo ( 110.3 ) ; servo . moveToBlocking ( 113.0 ) ; servo . setSpeed ( 2.0 ) ; servo . moveTo ( 140.0 ) ; TestCatcher catcher = ( TestCatcher ) Runtime . start ( "catcher" , "TestCatcher" ) ; catcher . exportAll ( "export.py" ) ; log . info ( "converted" ) ; } catch ( Exception e ) { log . error ( "main threw" , e ) ; } }
public void test() { try { LoggingFactory . init ( Level . INFO ) ; log . info ( "{}" , Serial . getPorts ( ) ) ; Platform . setVirtual ( false ) ; String port = "COM7" ; int pin = 22 ; boolean useHobbyServo = true ; SwingGui gui = ( SwingGui ) Runtime . start ( "gui" , "SwingGui" ) ; Arduino mega = ( Arduino ) Runtime . start ( "mega" , "Arduino" ) ; ServoControl servo = null ; servo = ( ServoControl ) Runtime . start ( "servo" , "Servo" ) ; Service . sleep ( 500 ) ; gui . setActiveTab ( "servo" ) ; code_block = IfStatement ; mega . connect ( port ) ; servo . setPin ( pin ) ; servo . attach ( mega ) ; servo . moveTo ( 10.3 ) ; servo . moveTo ( 110.3 ) ; servo . moveToBlocking ( 113.0 ) ; servo . setSpeed ( 2.0 ) ; servo . moveTo ( 140.0 ) ; TestCatcher catcher = ( TestCatcher ) Runtime . start ( "catcher" , "TestCatcher" ) ; catcher . exportAll ( "export.py" ) ; log . info ( "{}" , Serial . getPorts ( ) ) ; } catch ( Exception e ) { log . error ( "main threw" , e ) ; } }
public void test() { try { LoggingFactory . init ( Level . INFO ) ; log . info ( "{}" , Serial . getPorts ( ) ) ; Platform . setVirtual ( false ) ; String port = "COM7" ; int pin = 22 ; boolean useHobbyServo = true ; SwingGui gui = ( SwingGui ) Runtime . start ( "gui" , "SwingGui" ) ; Arduino mega = ( Arduino ) Runtime . start ( "mega" , "Arduino" ) ; ServoControl servo = null ; servo = ( ServoControl ) Runtime . start ( "servo" , "Servo" ) ; Service . sleep ( 500 ) ; gui . setActiveTab ( "servo" ) ; code_block = IfStatement ; mega . connect ( port ) ; servo . setPin ( pin ) ; log . info ( "rest is {}" , servo . getRest ( ) ) ; servo . attach ( mega ) ; servo . moveTo ( 10.3 ) ; servo . moveTo ( 110.3 ) ; servo . moveToBlocking ( 113.0 ) ; servo . setSpeed ( 2.0 ) ; servo . moveTo ( 140.0 ) ; TestCatcher catcher = ( TestCatcher ) Runtime . start ( "catcher" , "TestCatcher" ) ; catcher . exportAll ( "export.py" ) ; } catch ( Exception e ) { log . error ( "error" , e ) ; } }
public void test() { try { d = _cache . load ( id ) ; } catch ( Exception e ) { LOG . trace ( "IGNORED" , e ) ; } }
public void test() { if ( m != null ) { return m . object ; } else { LOGGER . debug ( "Could not find mapping for key {}" , key ) ; return null ; } }
@ Test public void testGetResource ( ) throws Exception { ResourcesResourceTest . LOG . debug ( "start get resource test" ) ; final String resourceJSON = resourceUploadInteral ( resourceFile , expectedResource ) ; ResourcesResourceTest . LOG . debug ( "created resource = '{}'" , resourceJSON ) ; final Resource resource = objectMapper . readValue ( resourceJSON , Resource . class ) ; Assert . assertNotNull ( "resource shouldn't be null" , resource ) ; Assert . assertNotNull ( "resource id shouldn't be null" , resource . getUuid ( ) ) ; ResourcesResourceTest . LOG . debug ( "try to retrieve resource '{}'" , resource . getUuid ( ) ) ; final Response response = target ( String . valueOf ( resource . getUuid ( ) ) ) . request ( ) . accept ( MediaType . APPLICATION_JSON_TYPE ) . get ( Response . class ) ; final String responseResource = response . readEntity ( String . class ) ; Assert . assertEquals ( "200 OK was expected" , 200 , response . getStatus ( ) ) ; Assert . assertEquals ( "resource JSONs are not equal" , resourceJSON , responseResource ) ; ResourcesResourceTest . LOG . debug ( "end get resource test" ) ; }
@ Test public void testGetResource ( ) throws Exception { ResourcesResourceTest . LOG . debug ( "start get resource test" ) ; final String resourceJSON = resourceUploadInteral ( resourceFile , expectedResource ) ; ResourcesResourceTest . LOG . debug ( "resource JSON = '{}'" , resourceJSON ) ; final Resource resource = objectMapper . readValue ( resourceJSON , Resource . class ) ; Assert . assertNotNull ( "resource shouldn't be null" , resource ) ; Assert . assertNotNull ( "resource id shouldn't be null" , resource . getUuid ( ) ) ; ResourcesResourceTest . LOG . debug ( "try to retrieve resource '{}'" , resource . getUuid ( ) ) ; final Response response = target ( String . valueOf ( resource . getUuid ( ) ) ) . request ( ) . accept ( MediaType . APPLICATION_JSON_TYPE ) . get ( Response . class ) ; final String responseResource = response . readEntity ( String . class ) ; Assert . assertEquals ( "200 OK was expected" , 200 , response . getStatus ( ) ) ; Assert . assertEquals ( "resource JSONs are not equal" , resourceJSON , responseResource ) ; ResourcesResourceTest . LOG . debug ( "end get resource test" ) ; }
@ Test public void testGetResource ( ) throws Exception { ResourcesResourceTest . LOG . debug ( "start get resource test" ) ; final String resourceJSON = resourceUploadInteral ( resourceFile , expectedResource ) ; ResourcesResourceTest . LOG . debug ( "created resource = '{}'" , resourceJSON ) ; final Resource resource = objectMapper . readValue ( resourceJSON , Resource . class ) ; Assert . assertNotNull ( "resource shouldn't be null" , resource ) ; Assert . assertNotNull ( "resource id shouldn't be null" , resource . getUuid ( ) ) ; ResourcesResourceTest . LOG . debug ( "response = {}" , response ) ; final Response response = target ( String . valueOf ( resource . getUuid ( ) ) ) . request ( ) . accept ( MediaType . APPLICATION_JSON_TYPE ) . get ( Response . class ) ; final String responseResource = response . readEntity ( String . class ) ; Assert . assertEquals ( "200 OK was expected" , 200 , response . getStatus ( ) ) ; Assert . assertEquals ( "resource JSONs are not equal" , resourceJSON , responseResource ) ; ResourcesResourceTest . LOG . debug ( "end get resource test" ) ; }
@ Test public void testGetResource ( ) throws Exception { ResourcesResourceTest . LOG . debug ( "start get resource test" ) ; final String resourceJSON = resourceUploadInteral ( resourceFile , expectedResource ) ; ResourcesResourceTest . LOG . debug ( "created resource = '{}'" , resourceJSON ) ; final Resource resource = objectMapper . readValue ( resourceJSON , Resource . class ) ; Assert . assertNotNull ( "resource shouldn't be null" , resource ) ; Assert . assertNotNull ( "resource id shouldn't be null" , resource . getUuid ( ) ) ; ResourcesResourceTest . LOG . debug ( "try to retrieve resource '{}'" , resource . getUuid ( ) ) ; final Response response = target ( String . valueOf ( resource . getUuid ( ) ) ) . request ( ) . accept ( MediaType . APPLICATION_JSON_TYPE ) . get ( Response . class ) ; final String responseResource = response . readEntity ( String . class ) ; Assert . assertEquals ( "200 OK was expected" , 200 , response . getStatus ( ) ) ; Assert . assertEquals ( "resource JSONs are not equal" , resourceJSON , responseResource ) ; ResourcesResourceTest . LOG . debug ( "end get resource test" ) ; }
public void test() { try { List < WidgetType > types = this . getWidgetManager ( ) . getWidgetTypes ( ) ; List < WidgetDto > dtoList = dtoBuilder . convert ( types ) ; List < WidgetDto > resultList = new WidgetTypeListProcessor ( restListReq , dtoList ) . filterAndSort ( ) . toList ( ) ; List < WidgetDto > sublist = restListReq . getSublist ( resultList ) ; SearcherDaoPaginatedResult < WidgetDto > paginatedResult = new SearcherDaoPaginatedResult < > ( resultList . size ( ) , sublist ) ; PagedMetadata < WidgetDto > pagedMetadata = new PagedMetadata < > ( restListReq , paginatedResult ) ; pagedMetadata . setBody ( sublist ) ; return pagedMetadata ; } catch ( Throwable t ) { logger . error ( "error in get widgets" , t ) ; throw new RestServerError ( "error in get widgets" , t ) ; } }
public void test() { try { listener . componentSecretUpdated ( subdomain , secret ) ; } catch ( Exception e ) { logger . warn ( "Exception at componentSecretUpdated" , e ) ; } }
public void test() { if ( StringUtils . isEmpty ( serviceName ) || StringUtils . isEmpty ( checksum ) ) { log . warn ( String . format ( "Cannot validate request without checksum %s" , checksum ) ) ; return ; } }
public void test() { try { code_block = WhileStatement ; Thread . sleep ( 10L ) ; } catch ( IOException ignore1 ) { } catch ( InterruptedException ignore2 ) { } catch ( Throwable t ) { logger . error ( "" , t ) ; } }
public void test() { try { com . liferay . blogs . model . BlogsEntry returnValue = BlogsEntryServiceUtil . getEntry ( entryId ) ; return com . liferay . blogs . model . BlogsEntrySoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try { bean . reloadConfiguration ( ) ; } catch ( Exception e ) { log . error ( "Error reloading bean " + bean , e ) ; } }
public void test() { try { id = Integer . parseInt ( strId ) ; } catch ( NumberFormatException e ) { log . warn ( "Bad value to " + strId + " (" + strId + ")" ) ; id = 0 ; } }
public List < Long > doInHibernate ( final Session session ) throws HibernateException { StringBuilder sb = new StringBuilder ( ) ; sb . append ( "SELECT DISTINCT child.id AS child_id FROM quota AS father" ) ; sb . append ( " JOIN quota AS child" ) ; sb . append ( " ON child.domain_parent_id = father.domain_id" ) ; sb . append ( " AND child.quota_type = :domainType " ) ; sb . append ( " AND father.domain_parent_id = :domainId " ) ; sb . append ( " AND father.default_max_file_size_override = false" ) ; sb . append ( " WHERE father.quota_type = :domainType" ) ; code_block = IfStatement ; sb . append ( " AND child.default_max_file_size_override = false" ) ; sb . append ( ";" ) ; @ SuppressWarnings ( "unchecked" ) final NativeQuery < Long > query = session . createNativeQuery ( sb . toString ( ) ) ; query . setParameter ( "domainId" , domain . getPersistenceId ( ) ) ; query . addScalar ( "child_id" , LongType . INSTANCE ) ; query . setParameter ( "domainType" , type . name ( ) ) ; code_block = IfStatement ; List < Long > res = query . list ( ) ; logger . debug ( "child_ids :" + res ) ; return res ; }
public void test() { try { listener . gotUserDetail ( user ) ; } catch ( Exception e ) { logger . warn ( "Exception at getUserDetail" , e ) ; } }
@ Override public void ping ( TCredentials credentials ) { log . info ( "ping" ) ; }
public void test() { try { SnappyOutputStream snappyOutputStream = new SnappyOutputStream ( baos ) ; snappyOutputStream . write ( uncompressedBytes ) ; snappyOutputStream . close ( ) ; baos . close ( ) ; } catch ( IOException e ) { logger . error ( "Unable to write to byte array" , e ) ; } }
public void test() { try { GitAccess . getInstance ( ) . setBranch ( branchName ) ; } catch ( CheckoutConflictException ex ) { restoreCurrentBranchSelectionInMenu ( ) ; BranchesUtil . showBranchSwitchErrorMessage ( ) ; } catch ( GitAPIException | JGitInternalException ex ) { LOG . error ( ex , ex ) ; restoreCurrentBranchSelectionInMenu ( ) ; PluginWorkspaceProvider . getPluginWorkspace ( ) . showErrorMessage ( ex . getMessage ( ) , ex ) ; } }
public void test() { -> { code_block = IfStatement ; } }
public void test() { try { final Boolean skipDevDependencies = getSettings ( ) . getBoolean ( Settings . KEYS . ANALYZER_NODE_AUDIT_SKIPDEV , false ) ; final JsonObject lockJson = fetchYarnAuditJson ( dependency , skipDevDependencies ) ; final JsonReader packageReader = Json . createReader ( FileUtils . openInputStream ( packageFile ) ) ; final JsonObject packageJson = packageReader . readObject ( ) ; final JsonObject payload = NpmPayloadBuilder . build ( lockJson , packageJson , dependencyMap , skipDevDependencies ) ; return getSearcher ( ) . submitPackage ( payload ) ; } catch ( URLConnectionFailureException e ) { this . setEnabled ( false ) ; throw new AnalysisException ( "Failed to connect to the NPM Audit API (YarnAuditAnalyzer); the analyzer " + "is being disabled and may result in false negatives." , e ) ; } catch ( IOException e ) { LOGGER . debug ( "Error reading dependency or connecting to NPM Audit API" , e ) ; this . setEnabled ( false ) ; throw new AnalysisException ( "Failed to read results from the NPM Audit API (YarnAuditAnalyzer); " + "the analyzer is being disabled and may result in false negatives." , e ) ; } catch ( JsonException e ) { throw new AnalysisException ( String . format ( "Failed to parse %s file from the NPM Audit API " + "(YarnAuditAnalyzer)." , lockFile . getPath ( ) ) , e ) ; } catch ( SearchException ex ) { LOGGER . error ( "YarnAuditAnalyzer failed on {}" , dependency . getActualFilePath ( ) ) ; throw ex ; } }
public void test() { try { final Boolean skipDevDependencies = getSettings ( ) . getBoolean ( Settings . KEYS . ANALYZER_NODE_AUDIT_SKIPDEV , false ) ; final JsonObject lockJson = fetchYarnAuditJson ( dependency , skipDevDependencies ) ; final JsonReader packageReader = Json . createReader ( FileUtils . openInputStream ( packageFile ) ) ; final JsonObject packageJson = packageReader . readObject ( ) ; final JsonObject payload = NpmPayloadBuilder . build ( lockJson , packageJson , dependencyMap , skipDevDependencies ) ; return getSearcher ( ) . submitPackage ( payload ) ; } catch ( URLConnectionFailureException e ) { this . setEnabled ( false ) ; throw new AnalysisException ( "Failed to connect to the NPM Audit API (YarnAuditAnalyzer); the analyzer " + "is being disabled and may result in false negatives." , e ) ; } catch ( IOException e ) { LOGGER . debug ( "Error reading dependency or connecting to NPM Audit API" , e ) ; this . setEnabled ( false ) ; throw new AnalysisException ( "Failed to read results from the NPM Audit API (YarnAuditAnalyzer); " + "the analyzer is being disabled and may result in false negatives." , e ) ; } catch ( JsonException e ) { throw new AnalysisException ( String . format ( "Failed to parse %s file from the NPM Audit API " + "(YarnAuditAnalyzer)." , lockFile . getPath ( ) ) , e ) ; } catch ( SearchException ex ) { LOGGER . error ( "Search error in {}" , dependency ) ; throw ex ; } }
private void executeStatement ( String sql , Statement stmt ) throws SQLException { LOGGER . info ( sql ) ; stmt . execute ( sql ) ; }
@ Override public void onStatusChanged ( final UaSubscription subscription , final StatusCode status ) { logger . debug ( "onStatusChanged {}" , status ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { date1 = format . parse ( dateString1 ) ; date2 = format . parse ( dateString2 ) ; } catch ( ParseException e ) { logger . error ( e . getMessage ( ) ) ; return 0 ; } }
protected void leaked ( LeakDetector . LeakInfo leakInfo ) { LOGGER . warn ( "F leaked: {}" , leakInfo . getStackTrace ( ) ) ; }
public void test() { try { return propertiesProvider . getConfiguration ( RABBITMQ_CONFIGURATION_NAME ) ; } catch ( FileNotFoundException e ) { logger . error ( "Could not find RABBITMQ config." , e ) ; throw new RuntimeException ( e ) ; } }
public void test() { if ( StringAttribute . identifier . equals ( type ) ) { return StringAttribute . getInstance ( value ) ; } else-if ( AnyURIAttribute . identifier . equals ( type ) ) { return AnyURIAttribute . getInstance ( value ) ; } else-if ( DateTimeAttribute . identifier . equals ( type ) ) { return DateTimeAttribute . getInstance ( value ) ; } else-if ( DateAttribute . identifier . equals ( type ) ) { return DateAttribute . getInstance ( value ) ; } else-if ( TimeAttribute . identifier . equals ( type ) ) { return TimeAttribute . getInstance ( value ) ; } else-if ( IntegerAttribute . identifier . equals ( type ) ) { return IntegerAttribute . getInstance ( value ) ; } else-if ( BooleanAttribute . identifier . equals ( type ) ) { return BooleanAttribute . getInstance ( value ) ; } else { logger . warn ( "Unsupported attribute {}" , type ) ; return StringAttribute . getInstance ( value ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { logger . error ( LoggingAnchor . FOUR , MessageEnum . RA_CONNECTION_NOT_FOUND , "SDNC" , ErrorCode . DataError . getValue ( ) , ": " + e . getMessage ( ) , e ) ; return null ; } }
public void test() { try { this . oslpChannelHandler . send ( this . createAddress ( ipAddress ) , oslpRequest , oslpResponseHandler , deviceRequest . getDeviceIdentification ( ) ) ; } catch ( final RuntimeException e ) { LOGGER . error ( "Unable to send oslp channel." , e ) ; throw new IOException ( e . getMessage ( ) ) ; } }
public void test() { try { profiler . getAttribute ( ) ; LOG . info ( "Profile loaded. Profiles will not be available from this worker." ) ; return ProfilingState . PROFILING_PRESENT ; } catch ( UnsatisfiedLinkError e ) { LOG . info ( "Profiling Agent not found. Profiles will not be available from this worker." ) ; return ProfilingState . PROFILING_ABSENT ; } }
public void test() { try { profiler . getAttribute ( ) ; LOG . info ( "Profiling Agent found. Per-step profiling is enabled." ) ; return ProfilingState . PROFILING_PRESENT ; } catch ( UnsatisfiedLinkError e ) { LOG . debug ( "Failed to read profiler profile information: {}" , e . getMessage ( ) ) ; return ProfilingState . PROFILING_ABSENT ; } }
public void test() { try { String path = getResourceRoot ( ) + fs + serviceType + ".png" ; return Files . readAllBytes ( Paths . get ( path ) ) ; } catch ( Exception e ) { logger . debug ( "Failed to load screenshot" , e ) ; } }
public void deactivate ( ) { logger . debug ( "deactivate()" ) ; httpService . unregister ( path ) ; this . bridgeHandler = null ; }
public void test() { while ( iter . hasNext ( ) ) { Map . Entry pairs = ( Map . Entry ) iter . next ( ) ; LOG . debug ( "\t" + pairs . length ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
@ Override public void service ( final WebdavRequest request , final WebdavResponse response ) throws WebdavException , IOException { LOG . trace ( "service: {}" , request ) ; requestInfo . setRequest ( request ) ; super . service ( request , response ) ; }
public org . springframework . data . domain . Page < ModuleDto > findAllModules ( final Pageable pageable , final ModuleSearchForm moduleSearchForm ) { LOG . debug ( "findAllModules() - pageable={}" , pageable ) ; final PlatformUser platformAuthorizedUser = securityService . getAuthorizedUser ( ) ; final User authorizedUser = userService . find ( platformAuthorizedUser . getId ( ) ) ; final org . springframework . data . domain . Page < Module > modules = moduleService . findAll ( pageable , moduleSearchForm , authorizedUser ) ; return moduleToModuleDtoConverter . convertToPage ( modules ) ; }
public void test() { if ( lock == null ) { logger . debug ( "No lock found for key {}" , key ) ; return ; } }
public void test() { if ( ! lock . lock ( 30 ) ) { logger . info ( "[doRun]" ) ; return ; } }
public void test() { try { scanStalledVMInTransitionStateOnDisconnectedHosts ( ) ; final List < VMInstanceVO > instances = _vmDao . findVMInTransition ( new Date ( DateUtil . currentGMTTime ( ) . getTime ( ) - AgentManager . Wait . value ( ) * 1000 ) , State . Starting , State . Stopping ) ; code_block = ForStatement ; } catch ( final Exception e ) { s_logger . error ( "Exception when stopping vm instances" , e ) ; } finally { lock . unlock ( ) ; } }
private void getDropletActions ( Exchange exchange ) throws Exception { Actions actions = getEndpoint ( ) . getDigitalOceanClient ( ) . getAvailableDropletActions ( dropletId , configuration . getPage ( ) , configuration . getPerPage ( ) ) ; LOG . trace ( "Available actions : " + actions . getActions ( ) ) ; exchange . getMessage ( ) . setBody ( actions . getActions ( ) ) ; }
public void test() { try { slackApi . call ( message ) ; } catch ( SlackChannelNotFoundException ex ) { LOG . warn ( ex . getMessage ( ) ) ; } }
@ Test public void testHeartBeat4 ( ) throws Exception { connection . close ( ) ; ClientStompFrame frame = conn . createFrame ( "CONNECT" ) ; frame . addHeader ( "host" , "127.0.0.1" ) ; frame . addHeader ( "login" , this . defUser ) ; frame . addHeader ( "passcode" , this . defPass ) ; frame . addHeader ( "heart-beat" , "500,500" ) ; frame . addHeader ( "accept-version" , "1.1,1.2" ) ; ClientStompFrame reply = conn . sendFrame ( frame ) ; instanceLog . debug ( "Reply: " + reply . toString ( ) ) ; assertEquals ( "CONNECTED" , reply . getCommand ( ) ) ; RemotingConnection remotingConnection = null ; StompConnection stompConnection = null ; Iterator < RemotingConnection > iterator = server . getRemotingService ( ) . getConnections ( ) . iterator ( ) ; code_block = WhileStatement ; StompFrameHandlerV11 stompFrameHandler = ( StompFrameHandlerV11 ) stompConnection . getStompVersionHandler ( ) ; instanceLog . debug ( "========== start pinger!" ) ; conn . startPinger ( 100 ) ; ClientStompFrame subFrame = conn . createFrame ( "SUBSCRIBE" ) ; subFrame . addHeader ( "destination" , getTopicPrefix ( ) + getTopicName ( ) ) ; subFrame . addHeader ( "id" , "0" ) ; ClientStompFrame f = conn . sendFrame ( subFrame ) ; f = conn . sendFrame ( subFrame ) ; f = conn . sendFrame ( subFrame ) ; f = conn . receiveFrame ( 1000 ) ; instanceLog . debug ( "========== finish pinger!" ) ; Assert . assertTrue ( f . getCommand ( ) . equals ( "ERROR" ) ) ; conn . stopPinger ( ) ; Thread . sleep ( 2000 ) ; Wait . waitFor ( ( ) code_block = LoopStatement ; ) ; Assert . assertFalse ( "HeartBeater is still running!!" , stompFrameHandler . getHeartBeater ( ) . isStarted ( ) ) ; }
public void persist ( CmMasState transientInstance ) { log . debug ( "persisting CmMasState instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . persist ( transientInstance ) ; log . debug ( "persist successful" ) ; } catch ( RuntimeException re ) { log . error ( "persist failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . persist ( transientInstance ) ; log . debug ( "persist successful" ) ; } catch ( RuntimeException re ) { log . error ( "persist failed" , re ) ; throw re ; } }
@ Test public void testTimedFlush ( ) throws Exception { EntityManager em = app . getEntityManager ( ) ; assertNotNull ( em ) ; UUID user1 = UUID . randomUUID ( ) ; UUID user2 = UUID . randomUUID ( ) ; Event event ; code_block = ForStatement ; Thread . sleep ( 30000 ) ; final long totalCount = returnCounts ( em , "visits" ) ; logger . debug ( "Returning count: " + totalCount ) ; assertEquals ( 200 , totalCount ) ; }
private void addToPollQueue ( InsteonDevice d , long time ) { long texp = findNextExpirationTime ( d , time ) ; PQEntry ne = new PQEntry ( d , texp ) ; pollQueue . add ( ne ) ; logger . trace ( "Added to poll queue: {}" , ne ) ; }
private void handleOnCompleted ( PinpointGrpcServer pinpointGrpcServer , AgentInfo agentInfo ) { Objects . requireNonNull ( pinpointGrpcServer , "pinpointGrpcServer" ) ; Objects . requireNonNull ( agentInfo , "agentInfo" ) ; logger . info ( "{} completed" , agentInfo ) ; pinpointGrpcServer . disconnected ( ) ; }
public void test() { if ( indexRef == null ) { reset ( ) ; } else { LOGGER . info ( "IndexRefreshed: {}" , indexRef . getName ( ) ) ; } }
public void test() { try { DeployedJar deployedJar = deployedJars . remove ( artifactId ) ; code_block = IfStatement ; logger . debug ( "JarDeployer deployedJars list after remove: {}" , Arrays . toString ( deployedJars . keySet ( ) . toArray ( ) ) ) ; ClassPathLoader . getLatest ( ) . unloadClassloaderForArtifact ( artifactId ) ; deleteAllVersionsOfJar ( deployedJar . getFile ( ) . getName ( ) ) ; logger . debug ( "JarDeployer deployedJars list after remove: {}" , Arrays . toString ( deployedJars . keySet ( ) . toArray ( ) ) ) ; return deployedJar . getFileCanonicalPath ( ) ; } finally { lock . unlock ( ) ; } }
public void test() { try { logger . debug ( "JarDeployer deployedJars list before remove: {}" , Arrays . toString ( deployedJars . keySet ( ) . toArray ( ) ) ) ; DeployedJar deployedJar = deployedJars . remove ( artifactId ) ; code_block = IfStatement ; ClassPathLoader . getLatest ( ) . unloadClassloaderForArtifact ( artifactId ) ; deleteAllVersionsOfJar ( deployedJar . getFile ( ) . getName ( ) ) ; logger . debug ( "JarDeployer deployedJars list after remove: {}" , deployedJar . getFile ( ) . getName ( ) ) ; return deployedJar . getFileCanonicalPath ( ) ; } finally { lock . unlock ( ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( JournalArticleServiceUtil . class , "getLatestArticle" , _getLatestArticleParameterTypes47 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , className , classPK ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . journal . model . JournalArticle ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public ArrayList < FileMatch > getTargetFileMatches ( CommandDirectories commandDirectories , FileType fileType , String sourceLocale , String sourcePathFilterRegex ) throws CommandException { log . debug ( "getTargets()" ) ; FileFinder fileFinder = getFileFinder ( commandDirectories , fileType , sourceLocale , sourcePathFilterRegex ) ; return fileFinder . getTargets ( ) ; }
public Long cascadeDefaultAccountQuotaToSubDomainsAccountQuota ( AbstractDomain domain , Long accountQuota , List < Long > quotaIdList ) { code_block = IfStatement ; HibernateCallback < Long > action = new HibernateCallback < Long > ( ) code_block = "" ; ; long updatedCounter = getHibernateTemplate ( ) . execute ( action ) ; logger . debug ( " {} default_account_quota of ContainerQuota have been updated." , updatedCounter ) ; return updatedCounter ; }
public void test() { if ( log . isInfoEnabled ( ) ) { log . info ( msg ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { X500Name x500name = new JcaX509CertificateHolder ( certificate ) . getSubject ( ) ; RDN cn = x500name . getRDNs ( BCStyle . CN ) [ 0 ] ; String cnStr = IETFUtils . valueToString ( cn . getFirst ( ) . getValue ( ) ) ; trustStore . setCertificateEntry ( cnStr , certificate ) ; resultList . add ( Collections . singletonMap ( "success" , true ) ) ; } catch ( CertificateEncodingException e ) { LOG . error ( e . getMessage ( ) , e ) ; resultList . add ( Collections . singletonMap ( "success" , false ) ) ; } }
public void test() { try { decodedUrl = new String ( Base64 . getDecoder ( ) . decode ( url ) , "UTF-8" ) ; socket = createNonVerifyingSslSocket ( decodedUrl ) ; socket . startHandshake ( ) ; X509Certificate [ ] peerCertificateChain = ( X509Certificate [ ] ) socket . getSession ( ) . getPeerCertificates ( ) ; code_block = ForStatement ; Path trustStoreFile = Paths . get ( SecurityConstants . getTruststorePath ( ) ) ; code_block = IfStatement ; String keyStorePassword = SecurityConstants . getTruststorePassword ( ) ; fos = Files . newOutputStream ( trustStoreFile ) ; trustStore . store ( fos , keyStorePassword . toCharArray ( ) ) ; } catch ( IOException | GeneralSecurityException e ) { LOG . error ( "Exception thrown" , e ) ; } finally { IOUtils . closeQuietly ( socket ) ; IOUtils . closeQuietly ( fos ) ; } }
public void test() { switch ( permission ) { case ADMINISTER : break ; case CREATE : case DELETE : case UPDATE : hasPermission = isRegionOwner ( authentication , region , trustedRegionContainer , trustedDomainObject ) || isRegionMember ( authentication , region , trustedRegionContainer , trustedDomainObject , true ) ; break ; case READ : hasPermission = isRegionOwner ( authentication , region , trustedRegionContainer , trustedDomainObject ) || isRegionMember ( authentication , region , trustedRegionContainer , trustedDomainObject , false ) ; default : log . warn ( "unknown permission: " + permission ) ; break ; } }
public void test() { try { String value = dpr . getValue ( key ) . toString ( ) ; return FormattingSwitchHelper . applyFormattingSwitch ( context . getWmlPackage ( ) , model , value ) ; } catch ( FieldValueException e ) { if ( e . getMessage ( ) . contains ( "No value found code_block = ForStatement ; logger . error ( e . getMessage ( ) ) ; throw new TransformerException ( e ) ; } catch ( Docx4JException e ) { throw new TransformerException ( e ) ; } }
public void test() { try { LogManager . resetConfiguration ( ) ; PaxLoggingConfigurator configurator = new PaxLoggingConfigurator ( m_bundleContext ) ; configurator . doConfigure ( extracted , LogManager . getLoggerRepository ( ) ) ; proxies = configurator . getProxies ( ) ; emptyConfiguration . set ( false ) ; } catch ( Exception e ) { LOG . error ( "Error logging configuration" , e ) ; problem = e ; } }
public void test() { if ( auth == null ) { log . error ( "Authorization {} does not exist." , auth . getAuthorizableId ( ) ) ; return false ; } else { currentMembership = new Membership ( auth . getID ( ) ) ; return true ; } }
public void test() { -> { logger . debug ( "Starting transaction maintenance task" ) ; int originalSize = transactions . size ( ) ; code_block = TryStatement ;  logger . debug ( "Transaction maintenance task finished. originalSize={}, currentSize={}" , originalSize , transactions . size ( ) ) ; } }
public void test() { try { code_block = ForStatement ; } catch ( Exception e ) { logger . error ( Messages . getInstance ( ) . getErrorString ( "Workspace.ERROR_0002_PROPS_EXCEPTION" ) , e ) ; } }
public void test() { -> { int originalSize = transactions . size ( ) ; logger . trace ( "Transaction maintenance task started." ) ; code_block = TryStatement ;  logger . trace ( "Transaction maintenance task finished." ) ; } }
@ Override public void onFailure ( Throwable caught ) { caught . printStackTrace ( System . err ) ; productionRequests = new DtoProductionRequest [ 0 ] ; fillRequestList ( ) ; updateRequestDetails ( ) ; LOG . info ( productionRequests . toString ( ) ) ; }
public void test() { try { oldVolume = getVolume ( sinkId ) ; } catch ( IOException e ) { logger . warn ( "Failed to update volume for sink {}" , sinkId , e ) ; } }
public void test() { try { setVolume ( volume , sinkId ) ; } catch ( IOException e ) { logger . log ( Level . WARNING , "Volume Provisioning failed" , e ) ; } }
public void test() { try { sink . process ( audioStream ) ; } catch ( UnsupportedAudioFormatException | UnsupportedAudioStreamException e ) { logger . warn ( "Unsupported audio stream: {}" , e . getMessage ( ) ) ; } finally { code_block = IfStatement ; } }
public void test() { try { setVolume ( oldVolume , sinkId ) ; } catch ( IOException e ) { logger . warn ( "Failed to set sink {} to {}" , sinkId , e ) ; } }
public void test() { if ( sink != null ) { PercentType oldVolume = null ; code_block = TryStatement ;  code_block = IfStatement ; code_block = TryStatement ;  } }
public void test() { try { MojitoAppUserInfo result = new MojitoAppUserInfo ( ) ; BoxAPIConnection apiConnection = boxAPIConnectionProvider . getConnection ( ) ; BoxFolder parentFolder = new BoxFolder ( apiConnection , BoxFolder . getRootFolder ( apiConnection ) . getID ( ) ) ; BoxFolder . Info mojitoFolder = parentFolder . createFolder ( MOJITO_FOLDER_NAME ) ; logger . debug ( "Created Project Requests Folder: " + mojitoFolder . getID ( ) ) ; result . setRootFolderId ( mojitoFolder . getID ( ) ) ; BoxFolder . Info projectRequestFolder = mojitoFolder . getResource ( ) . createFolder ( PROJECT_REQUESTS_FOLDER_NAME ) ; logger . debug ( "Created Project Requests Folder: " + projectRequestFolder . getID ( ) ) ; result . setDropsFolderId ( projectRequestFolder . getID ( ) ) ; return result ; } catch ( BoxAPIException e ) { throw new BoxSDKServiceException ( "Can't creating Mojito Folder Structure." , e ) ; } }
public void test() { try { MojitoAppUserInfo result = new MojitoAppUserInfo ( ) ; BoxAPIConnection apiConnection = boxAPIConnectionProvider . getConnection ( ) ; BoxFolder parentFolder = new BoxFolder ( apiConnection , BoxFolder . getRootFolder ( apiConnection ) . getID ( ) ) ; BoxFolder . Info mojitoFolder = parentFolder . createFolder ( MOJITO_FOLDER_NAME ) ; logger . debug ( "Created Mojito Folder: " + mojitoFolder . getID ( ) ) ; result . setRootFolderId ( mojitoFolder . getID ( ) ) ; BoxFolder . Info projectRequestFolder = mojitoFolder . getResource ( ) . createFolder ( PROJECT_REQUESTS_FOLDER_NAME ) ; result . setDropsFolderId ( projectRequestFolder . getID ( ) ) ; logger . debug ( "Modified project request folder: " + projectRequestFolder . getID ( ) ) ; return result ; } catch ( BoxAPIException e ) { throw new BoxSDKServiceException ( "Can't creating Mojito Folder Structure." , e ) ; } }
@ Test public void testSet ( ) { log . info ( "------  testSet  ------" ) ; Set < String > set = new HashSet < > ( ) ; set . add ( "1" ) ; }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
@ Override protected void shutDown ( ) { publisher . close ( ) ; listener . close ( ) ; curator . close ( ) ; log . info ( "Stopped" ) ; }
public synchronized void initServerInfo ( Id server , NodeRole role ) { E . checkArgument ( server != null && role != null , "The server id or role can't be null" ) ; this . selfServerId = server ; this . selfServerRole = role ; LOG . info ( "Initializing server {} with role {}" , server , role ) ; HugeServerInfo existed = this . serverInfo ( server ) ; E . checkArgument ( existed == null || ! existed . alive ( ) , "The server with name '%s' already in cluster" , server ) ; code_block = IfStatement ; HugeServerInfo serverInfo = new HugeServerInfo ( server , role ) ; serverInfo . maxLoad ( this . calcMaxLoad ( ) ) ; this . save ( serverInfo ) ; }
public void run ( ) { Thread . currentThread ( ) . setName ( "ChannelBufferManager Redis subscription Thread" ) ; logger . info ( "Entering thread: " + Thread . currentThread ( ) . getName ( ) ) ; code_block = TryStatement ;  Thread . currentThread ( ) . interrupt ( ) ; logger . info ( "Exiting thread: " + Thread . currentThread ( ) . getName ( ) ) ; }
public void test() { try { subscriberJedis . psubscribe ( aidrSubscriber , channelRegEx ) ; } catch ( JedisConnectionException e ) { logger . error ( "AIDR Predict Channel pSubscribing failed for channel = " + channelRegEx , e ) ; stopSubscription ( ) ; Thread . currentThread ( ) . interrupt ( ) ; } }
public void run ( ) { Thread . currentThread ( ) . setName ( "ChannelBufferManager Redis subscription Thread" ) ; logger . info ( "New thread <" + Thread . currentThread ( ) . getName ( ) + "> created for subscribing to redis channel: " + channelRegEx ) ; code_block = TryStatement ;  Thread . currentThread ( ) . interrupt ( ) ; logger . info ( "Exiting thread: " + Thread . currentThread ( ) . getName ( ) ) ; }
@ Override public void runCompareTest ( ) throws Exception { logger . info ( "Running test: " + targetDir ) ; assertFalse ( "Dir should not exist: " + targetDir . getAbsolutePath ( ) , targetDir . exists ( ) ) ; assertTrue ( "Unable to create: " + targetDir . getAbsolutePath ( ) , targetDir . mkdirs ( ) ) ; assertTrue ( new File ( targetDir , "0_deltas_go_here.txt" ) . createNewFile ( ) ) ; preRunHook ( ) ; int failedQueries = 0 ; code_block = ForStatement ; assertThat ( "Number of failed queries" , failedQueries , is ( 0 ) ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public String getInstanceState ( String instanceId ) { LOGGER . debug ( "getInstanceState('{}')" , instanceId ) ; DescribeInstancesResult result = getEC2 ( ) . describeInstances ( new DescribeInstancesRequest ( ) . withInstanceIds ( instanceId ) ) ; List < Reservation > reservations = result . getReservations ( ) ; Set < Instance > instances = new HashSet < Instance > ( ) ; code_block = ForStatement ; LOGGER . debug ( "getInstanceState('{}') left" , instanceId ) ; return null ; }
public void test() { if ( instances . size ( ) > 0 ) { String state = instances . iterator ( ) . next ( ) . getState ( ) . getName ( ) ; logger . info ( "state: " + state ) ; return state ; } }
public String getInstanceState ( String instanceId ) { LOGGER . debug ( "getInstanceState('{}') entered" , instanceId ) ; DescribeInstancesResult result = getEC2 ( ) . describeInstances ( new DescribeInstancesRequest ( ) . withInstanceIds ( instanceId ) ) ; List < Reservation > reservations = result . getReservations ( ) ; Set < Instance > instances = new HashSet < Instance > ( ) ; code_block = ForStatement ; LOGGER . debug ( "getInstanceState('{}') finished" , instanceId ) ; return null ; }
public void test() { if ( Log . isErrorEnabled ( ) ) { Log . error ( "Error while getting logs." , e ) ; } }
public void test() { if ( sentTime == null ) { LOGGER . warn ( "sendTime is null in session: {}" , session ) ; } }
@ Override public void update ( Object args , Observable observable ) { logger . debug ( "Update {}" , args ) ; this . currentState . refresh ( ) ; }
@ ExceptionHandler ( InvalidTokenException . class ) public ResponseEntity < OAuth2Exception > handleException ( Exception e ) throws Exception { logger . debug ( "" , e ) ; InvalidTokenException e400 = new InvalidTokenException ( e . getMessage ( ) ) code_block = "" ; ; return exceptionTranslator . translate ( e400 ) ; }
@ Test public void runBothDates ( ) throws IOException { File createTempDir = Files . createTempDir ( ) ; String randomString = UUID . randomUUID ( ) . toString ( ) ; File testOutFile = new File ( createTempDir , randomString + ".txt" ) ; String listCommand = "-p net.sourceforge.seqware.pipeline.plugins.WorkflowRunReporter " + "-- --output-filename " + testOutFile . getName ( ) + " --workflow-accession 2861 --time-period 2012-01:2012-01-15 " ; String listOutput = ITUtility . runSeqWareJar ( listCommand , ReturnValue . SUCCESS , createTempDir ) ; log . info ( listOutput ) ; File retrievedFile = new File ( createTempDir , testOutFile . getName ( ) ) ; Assert . assertTrue ( "output file does not exist" , retrievedFile . exists ( ) ) ; List < String > readLines = FileUtils . readLines ( testOutFile , StandardCharsets . UTF_8 ) ; Assert . assertTrue ( "incorrect number of lines " , readLines . size ( ) == 4 ) ; long checksumCRC32 = FileUtils . checksumCRC32 ( testOutFile ) ; Assert . assertTrue ( "incorrect output checksum " + checksumCRC32 + " " + FileUtils . readFileToString ( retrievedFile , StandardCharsets . UTF_8 ) , checksumCRC32 == 562223107L || checksumCRC32 == 4072825873L ) ; }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( _log . isInfoEnabled ( ) ) { _log . info ( StringBundler . concat ( "Removing " , companyId , " from company " , companyId , " to " , company . getCompanyId ( ) , " using " , company . getCompanyId ( ) ) ) ; } }
public void test() { try { ReturnT returnT = GsonTool . fromJson ( resultJson , ReturnT . class , returnTargClassOfT ) ; return returnT ; } catch ( Exception e ) { logger . error ( "xxl-rpc remoting (url=" + url + ") response content invalid(" + resultJson + ")." , e ) ; return new ReturnT < String > ( ReturnT . FAIL_CODE , "xxl-rpc remoting (url=" + url + ") response content invalid(" + resultJson + ")." ) ; } }
public void test() { try { URL realUrl = new URL ( url ) ; connection = ( HttpURLConnection ) realUrl . openConnection ( ) ; boolean useHttps = url . startsWith ( "https" ) ; code_block = IfStatement ; connection . setRequestMethod ( "POST" ) ; connection . setDoOutput ( true ) ; connection . setDoInput ( true ) ; connection . setUseCaches ( false ) ; connection . setReadTimeout ( timeout * 1000 ) ; connection . setConnectTimeout ( 3 * 1000 ) ; connection . setRequestProperty ( "connection" , "Keep-Alive" ) ; connection . setRequestProperty ( "Content-Type" , "application/json;charset=UTF-8" ) ; connection . setRequestProperty ( "Accept-Charset" , "application/json;charset=UTF-8" ) ; code_block = IfStatement ; connection . connect ( ) ; code_block = IfStatement ; int statusCode = connection . getResponseCode ( ) ; code_block = IfStatement ; bufferedReader = new BufferedReader ( new InputStreamReader ( connection . getInputStream ( ) , "UTF-8" ) ) ; StringBuilder result = new StringBuilder ( ) ; String line ; code_block = WhileStatement ; String resultJson = result . toString ( ) ; code_block = TryStatement ;  } catch ( Exception e ) { logger . error ( "xxl-rpc remoting error" , e ) ; return new ReturnT < String > ( ReturnT . FAIL_CODE , "xxl-rpc remoting error(" + e . getMessage ( ) + "), for url : " + url ) ; } finally { code_block = TryStatement ;  } }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; } catch ( Exception e2 ) { log . error ( e2 . getMessage ( ) , e2 ) ; } }
public void test() { try { futureTimeToAlertTs = System . currentTimeMillis ( ) + interval ; info ( "starting watchdog timer %s - expecting checkpoint in %d ms" , name , interval ) ; code_block = WhileStatement ; } catch ( Exception e2 ) { logger . error ( "Exception in watchdog timer " + name , e2 ) ; deactivate ( ) ; } }
public void test() { -> { CreateDirectoryPOptions mergedOptions = FileSystemOptions . createDirectoryDefaults ( mFsContext . getPathConf ( path ) ) . toBuilder ( ) . mergeFrom ( options ) . build ( ) ; client . createDirectory ( path , mergedOptions ) ; LOG . info ( "Successfully created directory: {}" , path ) ; return null ; } }
public void test() { try { runQuery ( query , Collections . singletonList ( query . replace ( "'mytable'" , "mytable" ) ) ) ; } catch ( Exception e ) { LOGGER . error ( "Getting erro for query : {}" , query ) ; LOGGER . error ( "Exception thrown" , e ) ; } }
public void test() { try { LOGGER . info ( "Trying to send query : {}" , query ) ; runQuery ( query , Collections . singletonList ( query . replace ( "'mytable'" , "mytable" ) ) ) ; } catch ( Exception e ) { LOGGER . error ( "Could not send query : {}" , query , e ) ; } }
@ Test public void testGenerateJson ( ) { BandInfoParameters object = new BandInfoParameters ( ) ; List < BandInfoParameters > objects = new ArrayList < BandInfoParameters > ( ) ; objects . add ( object ) ; logger . debug ( "GeneratedJson" ) ; }
public void test() { try { return _assetEntryService . getEntriesCount ( assetEntryQuery ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; } }
public void test() { if ( log . isErrorEnabled ( ) ) { log . error ( throwable . getMessage ( ) ) ; } }
public void test() { if ( terminationEarlySuccessful ) { LOG . log ( Level . FINE , "Termination early" ) ; } }
public void test() { try { subEquipmentConfiguration . setAliveTagId ( Long . parseLong ( getTagValue ( subEquipmentElement , ALIVE_TAG_ID_ELEMENT ) ) ) ; } catch ( NullPointerException e ) { log . error ( "Failed to get alive tag id" , e ) ; } }
public void test() { try { subEquipmentConfiguration . setAliveInterval ( Long . parseLong ( getTagValue ( subEquipmentElement , ALIVE_INTERVAL_ELEMENT ) ) ) ; } catch ( NullPointerException e ) { log . debug ( "Unable to find alive interval" ) ; } }
public void test() { try { PhaseWorkerCall call = new PhaseWorkerCall ( ) ; do code_block = "" ; while ( LIFECYCLE_EXECUTOR . getActiveCount ( ) <= getMinThreads ( ) ) ; } catch ( Throwable t ) { logger . error ( t . getMessage ( ) , t ) ; } }
public void handleSetRandomisationSettingsResponse ( final DeviceMessageMetadata deviceMessageMetadata , final ResponseMessageResultType deviceResult , final OsgpException exception ) { LOGGER . info ( "handleSetRandomisationSettingsResponse for MessageType: {}" , deviceMessageMetadata . getMessageType ( ) ) ; ResponseMessageResultType result = deviceResult ; code_block = IfStatement ; final ResponseMessage responseMessage = ResponseMessage . newResponseMessageBuilder ( ) . withCorrelationUid ( deviceMessageMetadata . getCorrelationUid ( ) ) . withOrganisationIdentification ( deviceMessageMetadata . getOrganisationIdentification ( ) ) . withDeviceIdentification ( deviceMessageMetadata . getDeviceIdentification ( ) ) . withResult ( result ) . withOsgpException ( exception ) . withMessagePriority ( deviceMessageMetadata . getMessagePriority ( ) ) . build ( ) ; this . webServiceResponseMessageSender . send ( responseMessage , deviceMessageMetadata . getMessageType ( ) ) ; }
public void test() { if ( exception != null ) { LOGGER . error ( "Response not ok." , exception ) ; result = ResponseMessageResultType . NOT_OK ; } }
public void test() { if ( debug ) { logger . debug ( "Expected error: " + e . getMessage ( ) ) ; } }
public void test() { if ( ex != null ) { LOGGER . warn ( "Failed to send message to topic '{}'" , topic , ex ) ; } }
public void test() { try { link = new URI ( uri ) ; entry . setLink ( link ) ; } catch ( URISyntaxException e ) { LOG . error ( e . getMessage ( ) ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getCapabilities()" ) ; } }
@ Override public ListIterator < Object > listIterator ( ) { Collections . shuffle ( cachedEntityList , workingRandom ) ; logger . info ( "++++++++++++" ) ; return cachedEntityList . listIterator ( ) ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
@ POST @ JacksonSerialized @ Produces ( MediaType . APPLICATION_JSON ) public Response addWorkerContainer ( ConnectWorkerContainer connectWorkerContainer ) { this . workerAdministrationManagement . register ( connectWorkerContainer ) ; LOG . info ( "Connected to {}" , connectWorkerContainer ) ; return ok ( Notifications . success ( "Worker Container sucessfully added" ) ) ; }
public void test() { if ( retries == 0 ) { LOG . warn ( "Failed to write records: {}" , records ) ; } }
public void test() { try { Thread . sleep ( 1000 ) ; } catch ( InterruptedException e1 ) { logger . error ( "Interrupted while looping." , e1 ) ; } }
public void test() { try { UserGroupInformation ugi = UgiFactory . getUgi ( user ) ; final long startTime = System . nanoTime ( ) ; String id = queueAsUser ( ugi , args ) ; long elapsed = ( ( System . nanoTime ( ) - startTime ) / ( ( int ) 1e6 ) ) ; LOG . info ( "Launched job {} took {} ms" , id , elapsed ) ; if ( id == null ) throw new QueueException ( "Unable to get job id" ) ; registerJob ( id , user , callback , userArgs ) ; return new EnqueueBean ( id ) ; } catch ( InterruptedException e ) { throw new QueueException ( "Unable to launch job " + e ) ; } }
@ PUT public Response put ( @ PathParam ( "path" ) final String externalPath ) throws Exception { LOGGER . debug ( "PUT: {}" , externalPath ) ; final FedoraId id = identifierConverter ( ) . pathToInternalId ( externalPath ) ; return doRequest ( id ) ; }
public void test() { try ( InputStream is = new ByteArrayInputStream ( readAttempt . result ( ) . getBytes ( ) ) ) { networkConfig . load ( is ) ; } catch ( final IOException e ) { LOGGER . warn ( "Unable to load network config" , e ) ; } }
public void test() { if ( readAttempt . succeeded ( ) ) { code_block = TryStatement ;  } else { LOG . debug ( "Failed to connect to ZooKeeper server" , readAttempt . cause ( ) ) ; } }
public void test() { if ( debug ) { logger . debug ( "Expected error: " + e . getMessage ( ) ) ; } }
@ Override public void save ( Device profile ) { LOGGER . info ( "Saving device {}" , profile ) ; deviceService . save ( profile ) ; Audit . logSave ( profile ) ; }
public void test() { try { task . run ( ) ; } catch ( Exception e ) { logger . warn ( "Task [{}] failed" , task . getName ( ) , e ) ; } }
public void setConfigFile ( String file ) { log . debug ( "setting config file: " + file ) ; configFile = file ; }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { try { KeyStoreManager keyStoreManager = KeyStoreManager . getInstance ( MultitenantConstants . SUPER_TENANT_ID ) ; this . publicCert = keyStoreManager . getDefaultPrimaryCertificate ( ) ; } catch ( Exception e ) { String error = "Error in obtaining keystore" ; log . error ( error ) ; } }
@ Test public void TestCreateVfModuleFailure_5000 ( ) { new MockAAIGenericVnfSearch ( wireMockServer ) ; MockAAICreateGenericVnf ( wireMockServer ) ; MockAAIVfModulePUT ( wireMockServer , true ) ; Map < String , Object > variables = new HashMap < > ( ) ; variables . put ( "isDebugLogEnabled" , "true" ) ; variables . put ( "isVidRequest" , "false" ) ; variables . put ( "vnfId" , "a27ce5a9-29c4-4c22-a017-6615ac73c721" ) ; variables . put ( "serviceId" , "99999999-9999-9999-9999-9999999999999999" ) ; variables . put ( "personaModelId" , "973ed047-d251-4fb9-bf1a-65b8949e0a73" ) ; variables . put ( "personaModelVersion" , "1.0" ) ; variables . put ( "vfModuleName" , "STMTN5MMSC21-PCRF::module-1-0" ) ; variables . put ( "vfModuleModelName" , "STMTN5MMSC21-PCRF::model-1-0" ) ; variables . put ( "mso-request-id" , UUID . randomUUID ( ) . toString ( ) ) ; String processId = invokeSubProcess ( "CreateAAIVfModule" , variables ) ; WorkflowException exception = BPMNUtil . getRawVariable ( processEngine , "CreateAAIVfModule" , "WorkflowException" , processId ) ; logger . debug ( exception ) ; Assert . assertEquals ( 5000 , exception . getErrorCode ( ) ) ; Assert . assertEquals ( true , exception . getErrorMessage ( ) . contains ( "<messageId>SVC3002</messageId>" ) ) ; }
public void attachDirty ( MBstnStatus instance ) { log . debug ( "attaching dirty MBstnStatus instance" ) ; code_block = TryStatement ;  }
public void test() { try { getSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { getSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
@ Test public void findProjectsByFuzzyOwnerTest ( ) { code_block = IfStatement ; GetUser getUserRequest = GetUser . newBuilder ( ) . setEmail ( authClientInterceptor . getClient1Email ( ) ) . build ( ) ; UserInfo testUser1 = uacServiceStub . getUser ( getUserRequest ) ; String testUser1UserName = testUser1 . getVertaInfo ( ) . getUsername ( ) ; Value stringValue = Value . newBuilder ( ) . setStringValue ( testUser1UserName . substring ( 0 , 2 ) ) . build ( ) ; KeyValueQuery keyValueQuery = KeyValueQuery . newBuilder ( ) . setKey ( "owner" ) . setValue ( stringValue ) . setOperator ( OperatorEnum . Operator . CONTAIN ) . build ( ) ; FindProjects findProjects = FindProjects . newBuilder ( ) . addPredicates ( keyValueQuery ) . build ( ) ; FindProjects . Response response = projectServiceStub . findProjects ( findProjects ) ; LOGGER . info ( "FindProjects Response : " + response . getProjectsList ( ) ) ; assertEquals ( "Project count not match with expected project count" , 4 , response . getProjectsList ( ) . size ( ) ) ; assertEquals ( "Total records count not matched with expected records count" , 4 , response . getTotalRecords ( ) ) ; keyValueQuery = KeyValueQuery . newBuilder ( ) . setKey ( "owner" ) . setValue ( stringValue ) . setOperator ( OperatorEnum . Operator . NOT_CONTAIN ) . build ( ) ; findProjects = FindProjects . newBuilder ( ) . addPredicates ( keyValueQuery ) . build ( ) ; response = projectServiceStub . findProjects ( findProjects ) ; LOGGER . info ( "FindProjects Response : " + findProjects ) ; assertEquals ( "Total records count not matched with expected records count" , 0 , response . getTotalRecords ( ) ) ; assertEquals ( "Project count not match with expected project count" , 0 , response . getProjectsCount ( ) ) ; stringValue = Value . newBuilder ( ) . setStringValue ( "asdasdasd" ) . build ( ) ; keyValueQuery
@ Test public void findProjectsByFuzzyOwnerTest ( ) { LOGGER . info ( "FindProjects by owner fuzzy search test start................................" ) ; code_block = IfStatement ; GetUser getUserRequest = GetUser . newBuilder ( ) . setEmail ( authClientInterceptor . getClient1Email ( ) ) . build ( ) ; UserInfo testUser1 = uacServiceStub . getUser ( getUserRequest ) ; String testUser1UserName = testUser1 . getVertaInfo ( ) . getUsername ( ) ; Value stringValue = Value . newBuilder ( ) . setStringValue ( testUser1UserName . substring ( 0 , 2 ) ) . build ( ) ; KeyValueQuery keyValueQuery = KeyValueQuery . newBuilder ( ) . setKey ( "owner" ) . setValue ( stringValue ) . setOperator ( OperatorEnum . Operator . CONTAIN ) . build ( ) ; FindProjects findProjects = FindProjects . newBuilder ( ) . addPredicates ( keyValueQuery ) . build ( ) ; FindProjects . Response response = projectServiceStub . findProjects ( findProjects ) ; LOGGER . info ( "FindProjects Response : " + response . getProjectsList ( ) ) ; assertEquals ( "Project count not match with expected project count" , 4 , response . getProjectsList ( ) . size ( ) ) ; assertEquals ( "Total records count not matched with expected records count" , 4 , response . getTotalRecords ( ) ) ; keyValueQuery = KeyValueQuery . newBuilder ( ) . setKey ( "owner" ) . setValue ( stringValue ) . setOperator ( OperatorEnum . Operator . NOT_CONTAIN ) . build ( ) ; findProjects = FindProjects . newBuilder ( ) . addPredicates ( keyValueQuery ) . build ( ) ; response = projectServiceStub . findProjects ( findProjects ) ; LOGGER . info ( "FindProjects Response : " + findProjects ) ; assertEquals ( "Total records count not matched with expected records count" , 0 , response . getTotalRecords ( ) ) ; assertEquals ( "Project count not match with expected project count" , 0 , response . getProjectsCount ( ) ) ; stringValue = Value . newBuilder ( ) . setString
@ Test public void testQuery003 ( ) throws Exception { log . info ( "------  testQuery003  ------" ) ; Set < String > expected = new HashSet < > ( ) ; String query = CarField . COLOR . name ( ) + " =~ 'bl.*s.*' and " + CarField . WHEELS . name ( ) + " == '4'" ; runTest ( query , expected ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { try { Foreach internalForeach = createForeach ( ) ; Processor nestedEventProcessor = event code_block = LoopStatement ; ; internalForeach . setMessageProcessors ( singletonList ( nestedEventProcessor ) ) ; Foreach nestedForeach = createForeach ( ) ; nestedForeach . setMessageProcessors ( singletonList ( internalForeach ) ) ; initialiseIfNeeded ( nestedForeach , muleContext ) ; nestedForeach . process ( nestedForeachEvent ) ; } catch ( Throwable t ) { logger . error ( "" , t ) ; } }
public void test() { try { return helper . fetchPage ( sqlCount . toString ( ) , sql . toString ( ) , paramList . toArray ( ) , pageNo , pageSize , CONFIG_INFO_ROW_MAPPER ) ; } catch ( CannotGetJdbcConnectionException e ) { LogUtil . FATAL_LOG . error ( "[db-error] " + e . toString ( ) , e ) ; throw e ; } }
public void test() { if ( debug ) { logger . debug ( "Expected error: " + e . getMessage ( ) ) ; } }
public void test() { try { code_block = IfStatement ; task . rollback ( ) ; } catch ( Exception e ) { log . warn ( e . getMessage ( ) , e ) ; errors = true ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { try { hotDeployListener . invokeDeploy ( hotDeployEvent ) ; } catch ( HotDeployException hotDeployException ) { _log . error ( hotDeployException , hotDeployException ) ; } finally { PortletClassLoaderUtil . setServletContextName ( null ) ; } }
public void test() { try { handleSendFailure ( e , Event . Type . SEND_EVENT_REQUEST , buffer ) ; } catch ( Exception e1 ) { logger . error ( "Failed to send event" , e1 ) ; } }
public void test() { try { this . handleEvent ( eventQueue . remove ( 0 ) ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { { log . info ( Color . BLUE + "4-Network-1 max_size = 1" + Color . NORMAL ) ; Context context = initValidatorContext ( ) ; Assert . assertNotNull ( fullparameters , "no parameters for test" ) ; context . put ( VALIDATION_REPORT , new ValidationReport ( ) ) ; bean1 . setRegistrationNumber ( "azerty" ) ; bean2 . setRegistrationNumber ( "az234ZDER" ) ; fullparameters . getNetwork ( ) . getRegistrationNumber ( ) . setPattern ( AbstractValidation . PATTERN_OPTION . lower . ordinal ( ) ) ; context . put ( VALIDATION , fullparameters ) ; ValidationData data = new ValidationData ( ) ; data . getNetworks ( ) . addAll ( beansFor4 ) ; context . put ( VALIDATION_DATA , data ) ; checkPoint . validate ( context , null ) ; ValidationReport report = ( ValidationReport ) context . get ( VALIDATION_REPORT ) ; checkReportForTest ( report , "4-Network-1" , 1 ) ; } }
public void test() { try code_block = "" ; catch ( RuntimeException e1 ) { log . error ( e1 . getMessage ( ) , e1 ) ; } }
public void test() { try { LOG . info ( "opening connection" ) ; m_impl . close ( ) ; LOG . info ( "connection closed" ) ; } catch ( JMSException e ) { BEANS . get ( MomExceptionHandler . class ) . handle ( e ) ; } finally { m_impl = null ; } }
public void test() { try { LOG . info ( "closing connection" ) ; m_impl . close ( ) ; } catch ( JMSException e ) { LOG . info ( "failed to close connection" , e ) ; BEANS . get ( MomExceptionHandler . class ) . handle ( e ) ; } finally { m_impl = null ; } }
public void test() { try { return migration . migrate ( s ) ; } catch ( Exception e ) { LOG . warn ( "Unable to migrate migration {}" , s , e ) ; return s ; } }
public void test() { try { if ( rwFileChannel != null ) rwFileChannel . close ( ) ; rwFileChannel = null ; if ( rwRaf != null ) rwRaf . close ( ) ; rwRaf = null ; rwFile = null ; File parent = new File ( baseDir ) ; code_block = IfStatement ; rwFile = FileUtils . getFile ( baseDir , SystemClock . now ( ) + "" ) ; rwRaf = new RandomAccessFile ( rwFile , "rw" ) ; rwFileChannel = rwRaf . getChannel ( ) ; rwMap = rwFileChannel . map ( FileChannel . MapMode . READ_WRITE , 0 , pageSize ) ; } catch ( Exception ex ) { logger . error ( ex ) ; } }
public void test() { if ( StringUtils . isNotBlank ( defaultFSProp ) ) { Map < String , String > fsRelatedProps = PropertyUtils . getPrefixedProperties ( "fs." ) ; configuration . set ( Constants . FS_DEFAULTFS , defaultFSProp ) ; fsRelatedProps . forEach ( ( key , value ) -> configuration . set ( key , value ) ) ; } else { logger . error ( String . format ( "property: %s can not to be empty, please set!" , Constants . FS_DEFAULTFS ) ) ; throw new RuntimeException ( String . format ( "property: %s can not to be empty, please set!" , Constants . FS_DEFAULTFS ) ) ; } }
public void test() { if ( defaultFS . startsWith ( "file" ) ) { String defaultFSProp = PropertyUtils . getString ( Constants . FS_DEFAULTFS ) ; code_block = IfStatement ; } else { log . error ( "Unsupported FS [" + defaultFS + "]" ) ; } }
public void test() { try { configuration = new HdfsConfiguration ( ) ; String resourceStorageType = PropertyUtils . getUpperCaseString ( Constants . RESOURCE_STORAGE_TYPE ) ; ResUploadType resUploadType = ResUploadType . valueOf ( resourceStorageType ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Failed to load configuration file" , e ) ; } }
@ Test public void wildcardResourcesAreOrderedAlphabetically ( ) { final WroModel model = new WroModel ( ) ; final String uri = String . format ( ClasspathUriLocator . PREFIX + "%s/expander/order/**.js" , WroUtil . toPackageAsFolder ( getClass ( ) ) ) ; model . addGroup ( new Group ( "group" ) . addResource ( Resource . create ( uri , ResourceType . JS ) ) ) ; Mockito . when ( decoratedFactory . create ( ) ) . thenReturn ( model ) ; final WroModel changedModel = transformer . transform ( model ) ; LOG . debug ( "model: {}" , uri ) ; Assert . assertEquals ( 7 , changedModel . getGroupByName ( "group" ) . getResources ( ) . size ( ) ) ; final List < Resource > resources = changedModel . getGroupByName ( "group" ) . getResources ( ) ; Assert . assertEquals ( "01-xyc.js" , FilenameUtils . getName ( resources . get ( 0 ) . getUri ( ) ) ) ; Assert . assertEquals ( "02-xyc.js" , FilenameUtils . getName ( resources . get ( 1 ) . getUri ( ) ) ) ; Assert . assertEquals ( "03-jquery-ui.js" , FilenameUtils . getName ( resources . get ( 2 ) . getUri ( ) ) ) ; Assert . assertEquals ( "04-xyc.js" , FilenameUtils . getName ( resources . get ( 3 ) . getUri ( ) ) ) ; Assert . assertEquals ( "05-xyc.js" , FilenameUtils . getName ( resources . get ( 4 ) . getUri ( ) ) ) ; Assert . assertEquals ( "06-xyc.js" , FilenameUtils . getName ( resources . get ( 5 ) . getUri ( ) ) ) ; Assert . assertEquals ( "07-jquery-impromptu.js" , FilenameUtils . getName ( resources . get ( 6 ) . getUri ( ) ) ) ; }
public void test() { try { t . join ( ) ; } catch ( InterruptedException e ) { Thread . currentThread ( ) . interrupt ( ) ; logger . warn ( "Got interrupted while waiting for Login thread to " + t ) ; } }
@ Override public void contextDestroyed ( ServletContextEvent servletContextEvent ) { LOGGER . log ( Level . FINE , "Destroying Web application" ) ; WebApplicationContext ac = WebApplicationContextUtils . getRequiredWebApplicationContext ( servletContextEvent . getServletContext ( ) ) ; ConfigurableApplicationContext gwac = ( ConfigurableApplicationContext ) ac ; gwac . close ( ) ; LOGGER . log ( Level . FINE , "Web application destroyed" ) ; }
public void test() { if ( log . isWarnEnabled ( ) ) { log . warn ( "Unhandled exception: " + e . getMessage ( ) ) ; } }
public void test() { if ( ! unsanitizedDn . equals ( decodedDn ) ) { logger . warn ( "Dn with invalid Dn: {}" , decodedDn ) ; } }
public void test() { if ( t != null ) { logger . error ( t . getMessage ( ) , t ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { switch ( status ) { case FAILURE : LOGGER . info ( "Failure device message status received: {}" , status ) ; throw new TechnicalException ( ComponentType . PROTOCOL_OSLP , "Device reports failure" ) ; case REJECTED : LOGGER . info ( "Rejected device message status received: {}" , status ) ; throw new TechnicalException ( ComponentType . PROTOCOL_OSLP , "Device reports rejected" ) ; case OK : LOGGER . info ( "OK device message status received: {}" , status ) ; break ; default : LOGGER . warn ( "Unknown device message status received: {}" , status ) ; break ; } }
public void test() { switch ( status ) { case FAILURE : LOGGER . info ( "Failure device message status received: {}" , status ) ; throw new TechnicalException ( ComponentType . PROTOCOL_OSLP , "Device reports failure" ) ; case REJECTED : LOGGER . error ( "Device reports rejected" ) ; throw new TechnicalException ( ComponentType . PROTOCOL_OSLP , "Device reports rejected" ) ; case OK : LOGGER . info ( "OK device message status received: {}" , status ) ; break ; default : LOGGER . warn ( "Unknown device message status received: {}" , status ) ; break ; } }
public void test() { switch ( status ) { case FAILURE : LOGGER . info ( "Failure device message status received: {}" , status ) ; throw new TechnicalException ( ComponentType . PROTOCOL_OSLP , "Device reports failure" ) ; case REJECTED : LOGGER . info ( "Rejected device message status received: {}" , status ) ; throw new TechnicalException ( ComponentType . PROTOCOL_OSLP , "Device reports rejected" ) ; case OK : LOGGER . debug ( "Device message status received: {}" , status ) ; break ; default : LOGGER . warn ( "Unknown device message status received: {}" , status ) ; break ; } }
public void test() { switch ( status ) { case FAILURE : LOGGER . info ( "Failure device message status received: {}" , status ) ; throw new TechnicalException ( ComponentType . PROTOCOL_OSLP , "Device reports failure" ) ; case REJECTED : LOGGER . info ( "Rejected device message status received: {}" , status ) ; throw new TechnicalException ( ComponentType . PROTOCOL_OSLP , "Device reports rejected" ) ; case OK : LOGGER . info ( "OK device message status received: {}" , status ) ; break ; default : LOGGER . warn ( "Unhandled device message status: {}" , status ) ; break ; } }
public void test() { if ( file . createNewFile ( ) ) { logger . debug ( "File created: {}" , file . getName ( ) ) ; final boolean append = true ; BufferedWriter headerWriter = new BufferedWriter ( new OutputStreamWriter ( new FileOutputStream ( csvFile , append ) , StandardCharsets . UTF_8 ) ) ; final String headerToAppend = "ContainerId,Timestamp[uts],ReceivedMessages[msgs/sec],SentMessages[msgs/sec]" ; headerWriter . write ( headerToAppend ) ; headerWriter . newLine ( ) ; headerWriter . close ( ) ; } else { logger . debug ( "File already exists. writting data to: {}" , csvFile ) ; } }
public void test() { if ( file . createNewFile ( ) ) { logger . debug ( "File created: {}" , file . getName ( ) ) ; final boolean append = true ; BufferedWriter headerWriter = new BufferedWriter ( new OutputStreamWriter ( new FileOutputStream ( csvFile , append ) , StandardCharsets . UTF_8 ) ) ; final String headerToAppend = "ContainerId,Timestamp[uts],ReceivedMessages[msgs/sec],SentMessages[msgs/sec]" ; headerWriter . write ( headerToAppend ) ; headerWriter . newLine ( ) ; headerWriter . close ( ) ; } else { logger . debug ( "File already exists." ) ; } }
@ Override public void exitLegacy_change_substitution ( Legacy_change_substitutionContext ctx ) { LOGGER . debug ( "Leaving legacy_change_substitution" ) ; LegacyLocation location = ( LegacyLocation ) getValue ( ctx . legacy_point_location ( ) ) ; String from = ctx . NT_STRING ( 0 ) . getText ( ) ; String to = ctx . NT_STRING ( 1 ) . getText ( ) ; setValue ( ctx , new LegacySubstitution ( location , from , to ) ) ; }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { String bootstrapHostnames = podNames . stream ( ) . map ( podName -> KafkaCluster . podDnsName ( this . namespace , this . cluster , podName ) + ":" + KafkaCluster . REPLICATION_PORT ) . collect ( Collectors . joining ( "," ) ) ; return adminClientProvider . createAdminClient ( bootstrapHostnames , this . clusterCaCertSecret , this . coKeySecret , "cluster-operator" ) ; } catch ( KafkaException e ) { code_block = IfStatement ; } catch ( RuntimeException e ) { LOG . debug ( "An error while try to create an admin client with bootstrap brokers " + podNames , e ) ; throw new ForceableProblem ( "An error while try to create an admin client with bootstrap brokers " + podNames , e ) ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( "Closing connection {}" , connection ) ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( "Closing connection {}" , connection ) ; } }
public void test() { if ( resource == null ) { LOG . debug ( "Resource is null." ) ; continue ; } }
public void test() { if ( index == null ) { LOG . warn ( "Index [{}] not found for table [{}]" , indexName , table ) ; continue ; } }
public void test() { try { configDescriptionProvider . add ( bundle , configDescription ) ; } catch ( RuntimeException e ) { LOG . warn ( "Problem registering {} config description provider" , configDescriptionProvider , e ) ; } }
public void replay ( RemoveTopicRecord record ) { TopicControlInfo topic = topics . remove ( record . topicId ( ) ) ; code_block = IfStatement ; topicsByName . remove ( topic . name ) ; configurationControl . deleteTopicConfigs ( topic . name ) ; code_block = ForStatement ; brokersToIsrs . removeTopicEntryForBroker ( topic . id , NO_LEADER ) ; controllerMetrics . setGlobalTopicsCount ( topics . size ( ) ) ; controllerMetrics . setGlobalPartitionCount ( globalPartitionCount . get ( ) ) ; controllerMetrics . setOfflinePartitionCount ( brokersToIsrs . offlinePartitionCount ( ) ) ; controllerMetrics . setPreferredReplicaImbalanceCount ( preferredReplicaImbalanceCount . get ( ) ) ; logger . info ( "Added topic {} to broker {}" , topic . getName ( ) , brokerMetrics . getBrokerName ( ) ) ; }
@ Override public void onFailedConnection ( Address address ) { logger . info ( "Connection to " + address . getHostAddress ( ) + " failed" ) ; }
public void test() { try { code_block = IfStatement ; } catch ( IllegalStateException e ) { log . debug ( "Access denied" , e ) ; } catch ( AccessControlException e ) { } }
public void test() { if ( ! equal ) { log . error ( "Expected " + expected ) ; log . error ( "Actual   " + JexlStringBuildingVisitor . buildQuery ( actualScript ) ) ; log . error ( "Expected " + PrintingVisitor . formattedQueryString ( expectedScript ) ) ; log . error ( "Actual   " + PrintingVisitor . formattedQueryString ( actualScript ) ) ; } }
public void test() { if ( ! equal ) { log . error ( "Expected " + JexlStringBuildingVisitor . buildQuery ( expectedScript ) ) ; log . error ( "Expected " + PrintingVisitor . formattedQueryString ( expectedScript ) ) ; log . error ( "Actual   " + PrintingVisitor . formattedQueryString ( actualScript ) ) ; log . error ( "Actual   " + PrintingVisitor . formattedQueryString ( actualScript ) ) ; } }
public void test() { if ( ! equal ) { log . error ( "Expected " + JexlStringBuildingVisitor . buildQuery ( expectedScript ) ) ; log . error ( "Expected " + JexlStringBuildingVisitor . buildQuery ( expectedScript ) ) ; log . error ( "Expected   " + JexlStringBuildingVisitor . buildQuery ( expectedScript ) ) ; log . error ( "Actual   " + PrintingVisitor . formattedQueryString ( actualScript ) ) ; } }
public void test() { if ( ! equal ) { log . error ( "Expected " + JexlStringBuildingVisitor . buildQuery ( expectedScript ) ) ; log . error ( "Actual   " + JexlStringBuildingVisitor . buildQuery ( actualScript ) ) ; log . error ( "Expected " + PrintingVisitor . formattedQueryString ( expectedScript ) ) ; log . error ( "Actual " + PrintingVisitor . formattedQueryString ( actualScript ) ) ; } }
public void test() { try { final ReferenceTerm referenceTerm = new ReferenceTermImpl ( new URL ( referenceValue . getReference ( ) ) , Set . copyOf ( referenceValue . getEntityTypes ( ) ) ) ; return persistentEntityResolver . resolveByUri ( Set . of ( referenceTerm ) ) . getOrDefault ( referenceTerm , Collections . emptyList ( ) ) ; } catch ( MalformedURLException e ) { LOG . error ( "The input values are invalid" , e ) ; throw new IllegalArgumentException ( "The input values are invalid" , e ) ; } }
public void test() { if ( cause != null ) { logger . error ( cause . getMessage ( ) , cause ) ; } }
public void test() { try { String inputPath = "../../data/census/census_148d_train.dummy" ; conf . set ( AngelConf . ANGEL_TRAIN_DATA_PATH , inputPath ) ; conf . set ( AngelConf . ANGEL_ACTION_TYPE , MLConf . ANGEL_ML_TRAIN ( ) ) ; conf . set ( AngelConf . ANGEL_LOAD_MODEL_PATH , LOCAL_FS + TMP_PATH + "/model/DeepFM" ) ; conf . set ( AngelConf . ANGEL_SAVE_MODEL_PATH , LOCAL_FS + TMP_PATH + "/model/DeepFM_new" ) ; GraphRunner runner = new GraphRunner ( ) ; runner . train ( conf ) ; } catch ( Exception x ) { LOG . error ( "run trainOnLocalClusterTest failed " , x ) ; throw x ; } }
public void test() { -> { LOGGER . info ( "Loaded {} results" , result . size ( ) ) ; } }
public void test() { try { final ConnectableObservable < Response > writeResponse = internalServiceFactory . getInternalGDMGraphService ( ) . updateObject ( dataModel . getUuid ( ) , connectableResult2 . observeOn ( GDM_SCHEDULER ) . onBackpressureBuffer ( 100000 ) , updateFormat , enableVersioning ) . onBackpressureBuffer ( 100000 ) . doOnSubscribe ( ( ) -> LOG . debug ( "subscribed to write response observable" ) ) . publish ( ) ; connectableResult2 . connect ( ) ; writeResponse . ignoreElements ( ) . cast ( Void . class ) . doOnError ( e code_block = LoopStatement ; ) ; final BlockingObservable < Response > blockingObservable = writeResponse . toBlocking ( ) ; writeResponse . connect ( ) ; connectableSource . connect ( ) ; blockingObservable . firstOrDefault ( null ) ; } catch ( final DMPPersistenceException e ) { final String message = String . format ( "couldn't persist the converted data of data model '%s'" , dataModel . getUuid ( ) ) ; ConverterEventRecorder . LOG . error ( message , e ) ; throw new DMPControllerException ( String . format ( "%s %s" , message , e . getMessage ( ) ) , e ) ; } }
public void test() { try { final ConnectableObservable < Response > writeResponse = internalServiceFactory . getInternalGDMGraphService ( ) . updateObject ( dataModel . getUuid ( ) , connectableResult2 . observeOn ( GDM_SCHEDULER ) . onBackpressureBuffer ( 100000 ) , updateFormat , enableVersioning ) . onBackpressureBuffer ( 100000 ) . doOnSubscribe ( ( ) -> LOG . debug ( "subscribed to write response observable" ) ) . publish ( ) ; connectableResult2 . connect ( ) ; writeResponse . ignoreElements ( ) . cast ( Void . class ) . doOnError ( e code_block = LoopStatement ; ) ; final BlockingObservable < Response > blockingObservable = writeResponse . toBlocking ( ) ; writeResponse . connect ( ) ; connectableSource . connect ( ) ; blockingObservable . firstOrDefault ( null ) ; LOG . debug ( "processed {} data resource into data model '{}'" , type , dataModel . getUuid ( ) ) ; } catch ( final DMPPersistenceException e ) { final String message = String . format ( "couldn't persist the converted data of data model '%s'" , dataModel . getUuid ( ) ) ; LOG . error ( "" , e ) ; throw new DMPControllerException ( String . format ( "%s %s" , message , e . getMessage ( ) ) , e ) ; } }
public void test() { if ( forceRegistration ) { LOG . debug ( "Unregistering MBean with ObjectName: {}" , name ) ; server . unregisterMBean ( name ) ; } else { LOG . debug ( "MBean already registered with ObjectName: {}" , name ) ; } }
public void test() { if ( forceRegistration ) { LOG . info ( "ForceRegistration enabled, unregistering existing MBean with ObjectName: {}" , name ) ; server . unregisterMBean ( name ) ; } else { LOG . info ( "ForceRegistration disabled, unregistering existing MBean with ObjectName: {}" , name ) ; } }
public void test() { if ( this . deregisterJDBCDriver ) { cleanupDrivers ( this . providedDrivers ) ; } else { logger . info ( "JDBC Driver has already been removed" ) ; } }
public boolean doPairing ( ) throws InterruptedException { logger . trace ( "Starting pairing openHAB Client with Bosch Smart Home Controller!" ) ; ContentResponse contentResponse ; code_block = TryStatement ;  logger . trace ( "Done pairing openHAB Client with Bosch Smart Home Controller!" ) ; }
public void test() { try { String publicCert = getCertFromSslContextFactory ( ) ; Map < String , String > items = new HashMap < > ( ) ; items . put ( "@type" , "client" ) ; items . put ( "id" , BoschSslUtil . getBoschShcClientId ( ) ) ; items . put ( "name" , "oss_OpenHAB_Binding" ) ; items . put ( "primaryRole" , "ROLE_RESTRICTED_CLIENT" ) ; items . put ( "certificate" , "-----BEGIN CERTIFICATE-----\r" + publicCert + "\r-----END CERTIFICATE-----" ) ; String url = this . getPairingUrl ( ) ; Request request = this . createRequest ( url , HttpMethod . POST , items ) . header ( "Systempassword" , Base64 . getEncoder ( ) . encodeToString ( this . systemPassword . getBytes ( StandardCharsets . UTF_8 ) ) ) ; contentResponse = request . send ( ) ; logger . trace ( "Pairing response complete: {} - return code: {}" , contentResponse . getContentAsString ( ) , contentResponse . getStatus ( ) ) ; code_block = IfStatement ; } catch ( TimeoutException | CertificateEncodingException | KeyStoreException | NullPointerException e ) { logger . warn ( "Pairing failed with exception {}" , e . getMessage ( ) ) ; logger . warn ( "Stack trace:" , e ) ; return false ; } catch ( ExecutionException e ) { logger . trace ( "Pairing failed - Details: {}" , e . getMessage ( ) ) ; logger . warn ( "Pairing failed. Was the Bosch Smart Home Controller button pressed?" ) ; return false ; } }
public void test() { try { String publicCert = getCertFromSslContextFactory ( ) ; logger . trace ( "Pairing with SHC {}" , ipAddress ) ; Map < String , String > items = new HashMap < > ( ) ; items . put ( "@type" , "client" ) ; items . put ( "id" , BoschSslUtil . getBoschShcClientId ( ) ) ; items . put ( "name" , "oss_OpenHAB_Binding" ) ; items . put ( "primaryRole" , "ROLE_RESTRICTED_CLIENT" ) ; items . put ( "certificate" , "-----BEGIN CERTIFICATE-----\r" + publicCert + "\r-----END CERTIFICATE-----" ) ; String url = this . getPairingUrl ( ) ; Request request = this . createRequest ( url , HttpMethod . POST , items ) . header ( "Systempassword" , Base64 . getEncoder ( ) . encodeToString ( this . systemPassword . getBytes ( StandardCharsets . UTF_8 ) ) ) ; contentResponse = request . send ( ) ; code_block = IfStatement ; } catch ( TimeoutException | CertificateEncodingException | KeyStoreException | NullPointerException e ) { logger . warn ( "Pairing failed with exception {}" , e . getMessage ( ) ) ; logger . warn ( "" , e ) ; return false ; } catch ( ExecutionException e ) { logger . trace ( "Pairing failed - Details: {}" , e . getMessage ( ) ) ; logger . warn ( "Pairing failed. Was the Bosch Smart Home Controller button pressed?" ) ; return false ; } }
public void test() { if ( 201 == contentResponse . getStatus ( ) ) { logger . debug ( "Pairing started with id {}." , contentResponse . getStatus ( ) ) ; return true ; } else { logger . info ( "Pairing failed with response status {}." , contentResponse . getStatus ( ) ) ; return false ; } }
public void test() { if ( 201 == contentResponse . getStatus ( ) ) { logger . debug ( "Pairing successful." ) ; return true ; } else { logger . warn ( "Could not process record with status: {}" , contentResponse . getStatus ( ) ) ; return false ; } }
public void test() { try { String publicCert = getCertFromSslContextFactory ( ) ; logger . trace ( "Pairing with SHC {}" , ipAddress ) ; Map < String , String > items = new HashMap < > ( ) ; items . put ( "@type" , "client" ) ; items . put ( "id" , BoschSslUtil . getBoschShcClientId ( ) ) ; items . put ( "name" , "oss_OpenHAB_Binding" ) ; items . put ( "primaryRole" , "ROLE_RESTRICTED_CLIENT" ) ; items . put ( "certificate" , "-----BEGIN CERTIFICATE-----\r" + publicCert + "\r-----END CERTIFICATE-----" ) ; String url = this . getPairingUrl ( ) ; Request request = this . createRequest ( url , HttpMethod . POST , items ) . header ( "Systempassword" , Base64 . getEncoder ( ) . encodeToString ( this . systemPassword . getBytes ( StandardCharsets . UTF_8 ) ) ) ; contentResponse = request . send ( ) ; logger . trace ( "Pairing response complete: {} - return code: {}" , contentResponse . getContentAsString ( ) , contentResponse . getStatus ( ) ) ; code_block = IfStatement ; } catch ( TimeoutException | CertificateEncodingException | KeyStoreException | NullPointerException e ) { logger . warn ( "Pairing failed: {}" , e . getMessage ( ) ) ; return false ; } catch ( ExecutionException e ) { logger . trace ( "Pairing failed - Details: {}" , e . getMessage ( ) ) ; logger . warn ( "Pairing failed. Was the Bosch Smart Home Controller button pressed?" ) ; return false ; } }
public void test() { try { String publicCert = getCertFromSslContextFactory ( ) ; logger . trace ( "Pairing with SHC {}" , ipAddress ) ; Map < String , String > items = new HashMap < > ( ) ; items . put ( "@type" , "client" ) ; items . put ( "id" , BoschSslUtil . getBoschShcClientId ( ) ) ; items . put ( "name" , "oss_OpenHAB_Binding" ) ; items . put ( "primaryRole" , "ROLE_RESTRICTED_CLIENT" ) ; items . put ( "certificate" , "-----BEGIN CERTIFICATE-----\r" + publicCert + "\r-----END CERTIFICATE-----" ) ; String url = this . getPairingUrl ( ) ; Request request = this . createRequest ( url , HttpMethod . POST , items ) . header ( "Systempassword" , Base64 . getEncoder ( ) . encodeToString ( this . systemPassword . getBytes ( StandardCharsets . UTF_8 ) ) ) ; logger . trace ( "Sending {}" , url ) ; contentResponse = request . send ( ) ; logger . trace ( "Pairing response complete: {} - return code: {}" , contentResponse . getContentAsString ( ) , contentResponse . getStatus ( ) ) ; code_block = IfStatement ; } catch ( TimeoutException | CertificateEncodingException | KeyStoreException | NullPointerException e ) { logger . warn ( "Pairing failed with exception {}" , e . getMessage ( ) ) ; return false ; } catch ( ExecutionException e ) { logger . warn ( "Pairing failed. Was the Bosch Smart Home Controller button pressed?" ) ; return false ; } }
public void test() { try { String publicCert = getCertFromSslContextFactory ( ) ; logger . trace ( "Pairing with SHC {}" , ipAddress ) ; Map < String , String > items = new HashMap < > ( ) ; items . put ( "@type" , "client" ) ; items . put ( "id" , BoschSslUtil . getBoschShcClientId ( ) ) ; items . put ( "name" , "oss_OpenHAB_Binding" ) ; items . put ( "primaryRole" , "ROLE_RESTRICTED_CLIENT" ) ; items . put ( "certificate" , "-----BEGIN CERTIFICATE-----\r" + publicCert + "\r-----END CERTIFICATE-----" ) ; String url = this . getPairingUrl ( ) ; Request request = this . createRequest ( url , HttpMethod . POST , items ) . header ( "Systempassword" , Base64 . getEncoder ( ) . encodeToString ( this . systemPassword . getBytes ( StandardCharsets . UTF_8 ) ) ) ; logger . trace ( "Pairing {}" , url ) ; contentResponse = request . send ( ) ; logger . trace ( "Pairing response complete: {} - return code: {}" , contentResponse . getContentAsString ( ) , contentResponse . getStatus ( ) ) ; code_block = IfStatement ; } catch ( TimeoutException | CertificateEncodingException | KeyStoreException | NullPointerException e ) { logger . warn ( "Pairing failed with exception {}" , e . getMessage ( ) ) ; return false ; } catch ( ExecutionException e ) { logger . trace ( "Pairing failed - Details: {}" , e . getMessage ( ) ) ; return false ; } }
public void test() { try { LOG . debug ( "Safe close on stream using {}" , onClose ) ; onClose . run ( ) ; isClosed = true ; } catch ( Exception e ) { LOG . warn ( "safe close failed" , e ) ; } }
public void test() { if ( temperatureFormat . isPresent ( ) ) { updateState ( CHANNEL_AUX_TEMP , new QuantityType < > ( temperatureFormat . get ( ) . omniToFormat ( status . getTemperature ( ) ) , temperatureFormat . get ( ) . getFormatNumber ( ) == 1 ? ImperialUnits . FAHRENHEIT : SIUnits . CELSIUS ) ) ; updateState ( CHANNEL_AUX_LOW_SETPOINT , new QuantityType < > ( temperatureFormat . get ( ) . omniToFormat ( status . getCoolSetpoint ( ) ) , temperatureFormat . get ( ) . getFormatNumber ( ) == 1 ? ImperialUnits . FAHRENHEIT : SIUnits . CELSIUS ) ) ; updateState ( CHANNEL_AUX_HIGH_SETPOINT , new QuantityType < > ( temperatureFormat . get ( ) . omniToFormat ( status . getHeatSetpoint ( ) ) , temperatureFormat . get ( ) . getFormatNumber ( ) == 1 ? ImperialUnits . FAHRENHEIT : SIUnits . CELSIUS ) ) ; } else { logger . debug ( "Could not find temperature {}" , status ) ; } }
public void test() { if ( bridgeHandler != null ) { Optional < TemperatureFormat > temperatureFormat = bridgeHandler . getTemperatureFormat ( ) ; code_block = IfStatement ; } else { logger . debug ( "No bridge handler available." ) ; } }
public void test() { if ( serviceName . equals ( "jndi/serviceA" ) ) { LOGGER . info ( "Looking up service A and creating new service for a" ) ; return new ServiceImpl ( "jndi/serviceA" ) ; } else-if ( serviceName . equals ( "jndi/serviceB" ) ) { LOGGER . info ( "Looking up service B and creating new service for B" ) ; return new ServiceImpl ( "jndi/serviceB" ) ; } else { return null ; } }
public void test() { if ( serviceName . equals ( "jndi/serviceA" ) ) { LOGGER . info ( "Looking up service A and creating new service for A" ) ; return new ServiceImpl ( "jndi/serviceA" ) ; } else-if ( serviceName . equals ( "jndi/serviceB" ) ) { LOGGER . info ( "Looking up service B and creating new service for B" ) ; return new ServiceImpl ( "jndi/serviceB" ) ; } else { return null ; } }
@ SuppressWarnings ( "deprecation" ) @ Test public void testAddNewLocationDefinition ( ) { String yaml = Joiner . on ( "\n" ) . join ( ImmutableList . of ( "brooklyn.catalog:" , "  symbolicName: " + locationName , "  version: " + locationVersion , "" , "brooklyn.locations:" , "- type: " + "aws-ec2:us-east-1" , "  brooklyn.config:" , "    identity: bob" , "    credential: CR3dential" ) ) ; ClientResponse response = client ( ) . resource ( "/v1/catalog" ) . post ( ClientResponse . class , yaml ) ; assertEquals ( response . getStatus ( ) , Response . Status . CREATED . getStatusCode ( ) ) ; URI addedCatalogItemUri = response . getLocation ( ) ; log . info ( " added catalog item: " + addedCatalogItemUri ) ; CatalogLocationSummary locationItem = client ( ) . resource ( "/v1/catalog/locations/" + locationName + "/" + locationVersion ) . get ( CatalogLocationSummary . class ) ; log . info ( " item: " + locationItem ) ; LocationSummary locationSummary = client ( ) . resource ( URI . create ( "/v1/locations/" + locationName + "/" ) ) . get ( LocationSummary . class ) ; log . info ( " summary: " + locationSummary ) ; Assert . assertEquals ( locationSummary . getSpec ( ) , "brooklyn.catalog:" + locationName + ":" + locationVersion ) ; JcloudsLocation l = ( JcloudsLocation ) getManagementContext ( ) . getLocationRegistry ( ) . resolve ( locationName ) ; Assert . assertEquals ( l . getProvider ( ) , "aws-ec2" ) ; Assert . assertEquals ( l . getRegion ( ) , "us-east-1" ) ; Assert . assertEquals ( l . getIdentity ( ) , "bob" ) ; Assert . assertEquals ( l . getCredential ( ) , "CR3dential" ) ; }
@ SuppressWarnings ( "deprecation" ) @ Test public void testAddNewLocationDefinition ( ) { String yaml = Joiner . on ( "\n" ) . join ( ImmutableList . of ( "brooklyn.catalog:" , "  symbolicName: " + locationName , "  version: " + locationVersion , "" , "brooklyn.locations:" , "- type: " + "aws-ec2:us-east-1" , "  brooklyn.config:" , "    identity: bob" , "    credential: CR3dential" ) ) ; ClientResponse response = client ( ) . resource ( "/v1/catalog" ) . post ( ClientResponse . class , yaml ) ; assertEquals ( response . getStatus ( ) , Response . Status . CREATED . getStatusCode ( ) ) ; URI addedCatalogItemUri = response . getLocation ( ) ; log . info ( "added, at: " + addedCatalogItemUri ) ; CatalogLocationSummary locationItem = client ( ) . resource ( "/v1/catalog/locations/" + locationName + "/" + locationVersion ) . get ( CatalogLocationSummary . class ) ; log . info ( " item: " + locationItem ) ; LocationSummary locationSummary = client ( ) . resource ( URI . create ( "/v1/locations/" + locationName + "/" ) ) . get ( LocationSummary . class ) ; log . info ( " summary: " + locationSummary ) ; Assert . assertEquals ( locationSummary . getSpec ( ) , "brooklyn.catalog:" + locationName + ":" + locationVersion ) ; JcloudsLocation l = ( JcloudsLocation ) getManagementContext ( ) . getLocationRegistry ( ) . resolve ( locationName ) ; Assert . assertEquals ( l . getProvider ( ) , "aws-ec2" ) ; Assert . assertEquals ( l . getRegion ( ) , "us-east-1" ) ; Assert . assertEquals ( l . getIdentity ( ) , "bob" ) ; Assert . assertEquals ( l . getCredential ( ) , "CR3dential" ) ; }
@ SuppressWarnings ( "deprecation" ) @ Test public void testAddNewLocationDefinition ( ) { String yaml = Joiner . on ( "\n" ) . join ( ImmutableList . of ( "brooklyn.catalog:" , "  symbolicName: " + locationName , "  version: " + locationVersion , "" , "brooklyn.locations:" , "- type: " + "aws-ec2:us-east-1" , "  brooklyn.config:" , "    identity: bob" , "    credential: CR3dential" ) ) ; ClientResponse response = client ( ) . resource ( "/v1/catalog" ) . post ( ClientResponse . class , yaml ) ; assertEquals ( response . getStatus ( ) , Response . Status . CREATED . getStatusCode ( ) ) ; URI addedCatalogItemUri = response . getLocation ( ) ; log . info ( "added, at: " + addedCatalogItemUri ) ; CatalogLocationSummary locationItem = client ( ) . resource ( "/v1/catalog/locations/" + locationName + "/" + locationVersion ) . get ( CatalogLocationSummary . class ) ; log . info ( " item: " + locationItem ) ; LocationSummary locationSummary = client ( ) . resource ( URI . create ( "/v1/locations/" + locationName + "/" ) ) . get ( LocationSummary . class ) ; log . info ( " summary: " + locationSummary ) ; Assert . assertEquals ( locationSummary . getSpec ( ) , "brooklyn.catalog:" + locationName + ":" + locationVersion ) ; JcloudsLocation l = ( JcloudsLocation ) getManagementContext ( ) . getLocationRegistry ( ) . resolve ( locationName ) ; Assert . assertEquals ( l . getProvider ( ) , "aws-ec2" ) ; Assert . assertEquals ( l . getRegion ( ) , "us-east-1" ) ; Assert . assertEquals ( l . getIdentity ( ) , "bob" ) ; Assert . assertEquals ( l . getCredential ( ) , "CR3dential" ) ; }
@ Override public void initialize ( ) { code_block = TryStatement ;  LOGGER . debug ( "Start the initialize phase for the type " + entityClass . getSimpleName ( ) ) ; final Collection < T > loadedFiles = loadFiles ( ) ; code_block = ForStatement ; LOGGER . debug ( "Finished the initialize phase for the type " + entityClass . getSimpleName ( ) ) ; postInitiate ( ) ; }
public void test() { try { this . jaxbContext = JAXBContext . newInstance ( entityClass ) ; } catch ( JAXBException e ) { LOGGER . error ( "Unable to create a JAXB instance" , e ) ; throw new IllegalStateException ( "Unable to create a new JAXB instance" , e ) ; } }
public void test() { try { final RevisionInfo revisionInfo = policyToDelete . getRevision ( ) ; final Long version = revisionInfo == null ? 0 : revisionInfo . getVersion ( ) ; client . target ( createURL ( "policies/" + policyToDelete . getIdentifier ( ) ) ) . queryParam ( "version" , version . longValue ( ) ) . request ( ) . header ( "Authorization" , "Bearer " + adminAuthToken ) . delete ( ) ; } catch ( Exception e ) { LOGGER . error ( "Failed to delete policy {}" , policyToDelete , e ) ; } }
public void test() { if ( ! failedReporter . getFailedTests ( ) . isEmpty ( ) ) { LOGGER . error ( "Failed to start JAR file." ) ; System . exit ( 1 ) ; } }
public void test() { if ( context . getElement ( ) . isPresent ( ) ) { AnnotatedElement e = context . getElement ( ) . get ( ) ; Description description = Description . createSuiteDescription ( "LDAP" , e . getAnnotations ( ) ) ; directoryService = DSAnnotationProcessor . getDirectoryService ( description ) ; DSAnnotationProcessor . applyLdifs ( description , directoryService ) ; log . trace ( "Creating ldap server" ) ; ldapServer = ServerAnnotationProcessor . createLdapServer ( description , directoryService ) ; log . trace ( "Done creating ldap server" ) ; } }
public void test() { if ( context . getElement ( ) . isPresent ( ) ) { AnnotatedElement e = context . getElement ( ) . get ( ) ; Description description = Description . createSuiteDescription ( "LDAP" , e . getAnnotations ( ) ) ; log . trace ( "Creating directory service" ) ; directoryService = DSAnnotationProcessor . getDirectoryService ( description ) ; DSAnnotationProcessor . applyLdifs ( description , directoryService ) ; ldapServer = ServerAnnotationProcessor . createLdapServer ( description , directoryService ) ; log . trace ( "Server started" ) ; } }
private void forwardRpcRequestToDeviceActor ( ToDeviceRpcRequest request , Consumer < FromDeviceRpcResponse > responseConsumer ) { log . debug ( "Forwarding RPC request [{}]" , request . getId ( ) ) ; UUID requestId = request . getId ( ) ; toDeviceRpcRequests . put ( requestId , responseConsumer ) ; sendRpcRequestToDevice ( request ) ; scheduleTimeout ( request , requestId ) ; }
private void encryptVPNPassword ( Connection conn ) { s_logger . debug ( "Encrypting vpn_users password" ) ; List < PreparedStatement > pstmt2Close = new ArrayList < PreparedStatement > ( ) ; PreparedStatement pstmt = null ; ResultSet rs = null ; code_block = TryStatement ;  s_logger . debug ( "Done encrypting vpn_users password" ) ; }
public void attachDirty ( FilterCol instance ) { log . debug ( "attaching dirty FilterCol instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { LOGGER . info ( "Received message of type: {}" , message . getJMSType ( ) ) ; final ObjectMessage objectMessage = ( ObjectMessage ) message ; final String messageType = objectMessage . getJMSType ( ) ; final RequestMessage requestMessage = ( RequestMessage ) objectMessage . getObject ( ) ; this . messageProcessor . processMessage ( requestMessage , messageType ) ; } catch ( final JMSException e ) { LOGGER . error ( "Exception: {}, StackTrace: {}" , e . getMessage ( ) , e . getStackTrace ( ) , e ) ; } catch ( final UnknownMessageTypeException e ) { LOGGER . error ( "UnknownMessageTypeException" , e ) ; } }
public void test() { try { LOGGER . info ( "Received message" ) ; final ObjectMessage objectMessage = ( ObjectMessage ) message ; final String messageType = objectMessage . getJMSType ( ) ; final RequestMessage requestMessage = ( RequestMessage ) objectMessage . getObject ( ) ; this . messageProcessor . processMessage ( requestMessage , messageType ) ; } catch ( final JMSException e ) { LOGGER . error ( "JMSException" , e ) ; } catch ( final UnknownMessageTypeException e ) { LOGGER . error ( "UnknownMessageTypeException" , e ) ; } }
public void test() { try { LOGGER . info ( "Received message" ) ; final ObjectMessage objectMessage = ( ObjectMessage ) message ; final String messageType = objectMessage . getJMSType ( ) ; final RequestMessage requestMessage = ( RequestMessage ) objectMessage . getObject ( ) ; this . messageProcessor . processMessage ( requestMessage , messageType ) ; } catch ( final JMSException e ) { LOGGER . error ( "Exception: {}, StackTrace: {}" , e . getMessage ( ) , e . getStackTrace ( ) , e ) ; } catch ( final UnknownMessageTypeException e ) { LOGGER . error ( "UnknownMessageType" , e ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( ! CollectionUtils . isEmpty ( listServerCertificatesMetadata ) ) { code_block = ForStatement ; iamCertificateVH . put ( account + delimiter + accountName , iamCerttList ) ; } else { log . info ( InventoryConstants . KEY_NOT_FOUND ) ; } }
public void test() { try { amazonIdentityManagement = AmazonIdentityManagementClientBuilder . standard ( ) . withCredentials ( new AWSStaticCredentialsProvider ( temporaryCredentials ) ) . withRegion ( InventoryConstants . REGION_US_WEST_2 ) . build ( ) ; listServerCertificatesMetadata = amazonIdentityManagement . listServerCertificates ( new ListServerCertificatesRequest ( ) ) . getServerCertificateMetadataList ( ) ; List < IAMCertificateVH > iamCerttList = new ArrayList < > ( ) ; code_block = IfStatement ; } catch ( Exception e ) { log . error ( "Error in uploading certificate" , e ) ; ErrorManageUtil . uploadError ( account , "" , "IAMCertificate" , e . getMessage ( ) ) ; } }
public org . talend . mdm . webservice . WSViewPK putView ( org . talend . mdm . webservice . WSPutView arg0 ) { LOG . info ( "Executing operation putView" ) ; System . out . println ( arg0 ) ; code_block = TryStatement ;  }
public void test() { try { return repositoriesContainer . getRepositoryNames ( Collections . unmodifiableMap ( factoryParams ) ) ; } catch ( RepositoryException e ) { logger . warn ( e , "Could not retrieve repository names" ) ; return Collections . emptySet ( ) ; } }
public void test() { if ( ! ObjectUtils . isEmpty ( errors ) ) { return handleRequestValidationErrors ( messageContext , errors ) ; } else-if ( logger . isDebugEnabled ( ) ) { logger . debug ( "No errors found" ) ; } }
public void test() { try { currentTransaction . success ( ) ; currentTransaction . close ( ) ; } catch ( Throwable t ) { log . error ( t . getMessage ( ) , t ) ; } finally { currentTransaction = null ; } }
public void test() { try { tryDestroy ( ) ; } catch ( Exception e ) { LOGGER . log ( Level . WARNING , "Error destroying security manager" , e ) ; } }
public void test() { if ( statusCode != HttpURLConnection . HTTP_OK ) { LOGGER . debug ( "Request failed: {}" , statusCode ) ; } }
public void test() { if ( null == startKey && null == endKey ) { Preconditions . checkState ( 1 == locations . size ( ) ) ; logger . debug ( "Found no start key on server {}: {}" , serverName , regionInfo ) ; return b . put ( new KeyRange ( FOUR_ZERO_BYTES , FOUR_ZERO_BYTES ) , serverName ) . build ( ) ; } else-if ( null == startKey ) { logger . debug ( "Found HRegionInfo with null startKey on server {}: {}" , serverName , regionInfo ) ; Preconditions . checkState ( null == nullStart ) ; nullStart = location ; StaticBuffer endBuf = StaticArrayBuffer . of ( zeroExtend ( endKey ) ) ; b . put ( new KeyRange ( FOUR_ZERO_BYTES , endBuf ) , serverName ) ; } else-if ( null == endKey ) { logger . debug ( "Found HRegionInfo with null endKey on server {}: {}" , serverName , regionInfo ) ; Preconditions . checkState ( null == nullEnd ) ; nullEnd = location ; b . put ( new KeyRange ( StaticArrayBuffer . of ( zeroExtend ( startKey ) ) , FOUR_ZERO_BYTES ) , serverName ) ; } else { Preconditions . checkState ( null != startKey ) ; Preconditions . checkState ( null != endKey ) ; StaticBuffer startBuf = StaticArrayBuffer . of ( zeroExtend ( startKey ) ) ; StaticBuffer endBuf = StaticArrayBuffer . of ( zeroExtend ( endKey ) ) ; KeyRange kr = new KeyRange ( startBuf , endBuf ) ; b . put ( kr , serverName ) ; logger . debug ( "Found HRegionInfo with non-null end and start keys on server {}: {}" , serverName , regionInfo ) ; } }
public void test() { if ( null == startKey && null == endKey ) { Preconditions . checkState ( 1 == locations . size ( ) ) ; logger . debug ( "HBase table {} has a single region {}" , tableName , regionInfo ) ; return b . put ( new KeyRange ( FOUR_ZERO_BYTES , FOUR_ZERO_BYTES ) , serverName ) . build ( ) ; } else-if ( null == startKey ) { Preconditions . checkState ( null == nullStart ) ; nullStart = location ; StaticBuffer endBuf = StaticArrayBuffer . of ( zeroExtend ( endKey ) ) ; b . put ( new KeyRange ( FOUR_ZERO_BYTES , endBuf ) , serverName ) ; logger . debug ( "Found HRegionInfo with null startKey on server {}: {}" , serverName , regionInfo ) ; } else-if ( null == endKey ) { logger . debug ( "Found HRegionInfo with null endKey on server {}: {}" , serverName , regionInfo ) ; Preconditions . checkState ( null == nullEnd ) ; nullEnd = location ; b . put ( new KeyRange ( StaticArrayBuffer . of ( zeroExtend ( startKey ) ) , FOUR_ZERO_BYTES ) , serverName ) ; } else { Preconditions . checkState ( null != startKey ) ; Preconditions . checkState ( null != endKey ) ; StaticBuffer startBuf = StaticArrayBuffer . of ( zeroExtend ( startKey ) ) ; StaticBuffer endBuf = StaticArrayBuffer . of ( zeroExtend ( endKey ) ) ; KeyRange kr = new KeyRange ( startBuf , endBuf ) ; b . put ( kr , serverName ) ; logger . debug ( "Found HRegionInfo with non-null end and start keys on server {}: {}" , serverName , regionInfo ) ; } }
public void test() { if ( null == startKey && null == endKey ) { Preconditions . checkState ( 1 == locations . size ( ) ) ; logger . debug ( "HBase table {} has a single region {}" , tableName , regionInfo ) ; return b . put ( new KeyRange ( FOUR_ZERO_BYTES , FOUR_ZERO_BYTES ) , serverName ) . build ( ) ; } else-if ( null == startKey ) { logger . debug ( "Found HRegionInfo with null startKey on server {}: {}" , serverName , regionInfo ) ; Preconditions . checkState ( null == nullStart ) ; nullStart = location ; StaticBuffer endBuf = StaticArrayBuffer . of ( zeroExtend ( endKey ) ) ; b . put ( new KeyRange ( FOUR_ZERO_BYTES , endBuf ) , serverName ) ; } else-if ( null == endKey ) { Preconditions . checkState ( null == nullEnd ) ; nullEnd = location ; logger . debug ( "Found null end key on server {}: {}" , serverName , regionInfo ) ; b . put ( new KeyRange ( StaticArrayBuffer . of ( zeroExtend ( startKey ) ) , FOUR_ZERO_BYTES ) , serverName ) ; } else { Preconditions . checkState ( null != startKey ) ; Preconditions . checkState ( null != endKey ) ; StaticBuffer startBuf = StaticArrayBuffer . of ( zeroExtend ( startKey ) ) ; StaticBuffer endBuf = StaticArrayBuffer . of ( zeroExtend ( endKey ) ) ; KeyRange kr = new KeyRange ( startBuf , endBuf ) ; b . put ( kr , serverName ) ; logger . debug ( "Found HRegionInfo with non-null end and start keys on server {}: {}" , serverName , regionInfo ) ; } }
public void test() { if ( null == startKey && null == endKey ) { Preconditions . checkState ( 1 == locations . size ( ) ) ; logger . debug ( "HBase table {} has a single region {}" , tableName , regionInfo ) ; return b . put ( new KeyRange ( FOUR_ZERO_BYTES , FOUR_ZERO_BYTES ) , serverName ) . build ( ) ; } else-if ( null == startKey ) { logger . debug ( "Found HRegionInfo with null startKey on server {}: {}" , serverName , regionInfo ) ; Preconditions . checkState ( null == nullStart ) ; nullStart = location ; StaticBuffer endBuf = StaticArrayBuffer . of ( zeroExtend ( endKey ) ) ; b . put ( new KeyRange ( FOUR_ZERO_BYTES , endBuf ) , serverName ) ; } else-if ( null == endKey ) { logger . debug ( "Found HRegionInfo with null endKey on server {}: {}" , serverName , regionInfo ) ; Preconditions . checkState ( null == nullEnd ) ; nullEnd = location ; b . put ( new KeyRange ( StaticArrayBuffer . of ( zeroExtend ( startKey ) ) , FOUR_ZERO_BYTES ) , serverName ) ; } else { Preconditions . checkState ( null != startKey ) ; Preconditions . checkState ( null != endKey ) ; logger . debug ( "Found HRegionInfo with startKey on server {}: {}" , serverName , regionInfo ) ; StaticBuffer startBuf = StaticArrayBuffer . of ( zeroExtend ( startKey ) ) ; StaticBuffer endBuf = StaticArrayBuffer . of ( zeroExtend ( endKey ) ) ; KeyRange kr = new KeyRange ( startBuf , endBuf ) ; b . put ( kr , serverName ) ; } }
@ Override public IApsEntity extractEntityType ( String typeCode , Class entityClass , String configItemName , IEntityTypeDOM entityTypeDom , String entityManagerName , IApsEntityDOM entityDom ) throws ApsSystemException { String xml = this . getConfigManager ( ) . getConfigItem ( configItemName ) ; logger . debug ( "Extracting entity type '{}'" , configItemName ) ; return entityTypeDom . extractEntityType ( typeCode , xml , entityClass , entityDom , entityManagerName ) ; }
public void test() { try { xceiverClientManager . close ( ) ; } catch ( Exception ex ) { logger . error ( ex . getMessage ( ) , ex ) ; } }
public void attachClean ( MbPrioritaet instance ) { log . debug ( "attaching clean MbPrioritaet instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . lock ( instance , LockMode . NONE ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { try { pem = SshKeyGen . writePublicKey ( entry . getValue ( ) ) ; break ; } catch ( Exception e ) { LOG . warn ( "Could not write key to file " + entry . getKey ( ) , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try ( BufferedWriter writer = new BufferedWriter ( new FileWriter ( idFile ) ) ) { writer . write ( String . valueOf ( identifier ) ) ; } catch ( IOException e ) { log . error ( "Unable to write file " + identifier , e ) ; } }
public void test() { try { return resp . getBody ( ComponentTypesHash . class ) ; } catch ( Exception e ) { log . error ( "Recieved Message Exception." , e ) ; return null ; } }
protected void doDependency ( Index index , final EntryStatistics es [ ] , final IterablePosting ips [ ] , ResultSet rs , final double [ ] phraseTermWeights , boolean SD ) throws IOException { final int numPhraseTerms = phraseTerms . length ; final boolean [ ] postingListFinished = new boolean [ numPhraseTerms ] ; code_block = ForStatement ; this . setCollectionStatistics ( index . getCollectionStatistics ( ) , index ) ; determineGlobalStatistics ( phraseTerms , es , SD ) ; final int [ ] docids = rs . getDocids ( ) ; final double [ ] scores = rs . getScores ( ) ; final short [ ] occurrences = rs . getOccurrences ( ) ; int altered = 0 ; MultiSort . ascendingHeapSort ( docids , scores , occurrences , docids . length ) ; final int docidsLength = docids . length ; boolean allZero = true ; code_block = ForStatement ; DOC : code_block = ForStatement ; code_block = ForStatement ; logger . info ( "Completed: " + altered ) ; }
private void parseSaltLength ( PasswordSaltExtensionMessage msg ) { msg . setSaltLength ( parseIntField ( ExtensionByteLength . PASSWORD_SALT ) ) ; LOGGER . debug ( "SaltLength: " + msg . getSaltLength ( ) . getValue ( ) ) ; }
public void test() { try { code_block = ForStatement ; input . put ( InputConverterUnitTest . END_ROW ) ; } catch ( InterruptedException e ) { logger . warn ( "Thread interrupted!" , e ) ; } }
public void test() { try { consumer . join ( ) ; } catch ( InterruptedException e ) { logger . error ( "Error while stopping consumer: {}" , e . getMessage ( ) ) ; } }
public void test() { try { LOG . debug ( "Closing writer" ) ; closeWriter ( ) ; commitTransaction ( ) ; } catch ( EventDeliveryException ex ) { rollbackTransaction ( ) ; LOG . debug ( "Exception follows." , ex ) ; } }
@ Test public void testInvalidConfigStatus ( ) { ThingMock t = new ThingMock ( ) ; HashMap < String , Object > properties = new HashMap < String , Object > ( ) ; properties . put ( "sensorid" , - 1 ) ; t . setConfiguration ( properties ) ; PMHandlerExtension pmHandler = new PMHandlerExtension ( t ) ; pmHandler . initialize ( ) ; int retryCount = 0 ; code_block = WhileStatement ; logger . debug ( "Handler Configuration status = " + retryCount ) ; assertEquals ( ConfigStatus . SENSOR_ID_NEGATIVE , pmHandler . getConfigStatus ( ) , "Handler Configuration status" ) ; }
public void test() { try { Thread . sleep ( 500 ) ; retryCount ++ ; } catch ( InterruptedException e ) { logger . error ( "" , e ) ; } }
public void test() { if ( _Employee . LOG . isDebugEnabled ( ) ) { _Employee . LOG . debug ( "updating exemptions from " + exemptions ( ) + " to " + value ) ; } }
public void test() { if ( content . length > smallCellMetadataWarningThreshold ) { getLogger ( ) . warn ( "Found small cell metadata warning." + content . length + ", smallCellMetadataWarningThreshold " + smallCellMetadataWarningThreshold ) ; } }
public void test() { try { int hueId = Integer . parseInt ( metadata . getValue ( ) ) ; code_block = IfStatement ; } catch ( NumberFormatException e ) { LOGGER . debug ( "Could not parse hue id {}" , metadata . getValue ( ) , e ) ; } }
@ Test public void testMarker ( ) { Logger logger = LoggerFactory . getLogger ( "testMarker" ) ; Marker blue = MarkerFactory . getMarker ( "BLUE" ) ; logger . debug ( blue , "hello" ) ; logger . info ( blue , "hello" ) ; logger . warn ( blue , "hello" ) ; logger . error ( blue , "hello" ) ; logger . debug ( blue , "hello {}" , "world" ) ; logger . info ( blue , "hello {}" , "world" ) ; logger . warn ( blue , "hello {}" , "world" ) ; logger . error ( blue , "hello {}" , "world" ) ; logger . debug ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . info ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . warn ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . error ( blue , "hello {} and {} " , "world" , "universe" ) ; }
@ Test public void testMarker ( ) { Logger logger = LoggerFactory . getLogger ( "testMarker" ) ; Marker blue = MarkerFactory . getMarker ( "BLUE" ) ; logger . debug ( blue , "hello" ) ; logger . info ( blue , "hello" ) ; logger . warn ( blue , "hello" ) ; logger . error ( blue , "hello" ) ; logger . debug ( blue , "hello {}" , "world" ) ; logger . info ( blue , "hello {}" , "world" ) ; logger . warn ( blue , "hello {}" , "world" ) ; logger . error ( blue , "hello {}" , "world" ) ; logger . debug ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . info ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . warn ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . error ( blue , "hello {} and {} " , "world" , "universe" ) ; }
@ Test public void testMarker ( ) { Logger logger = LoggerFactory . getLogger ( "testMarker" ) ; Marker blue = MarkerFactory . getMarker ( "BLUE" ) ; logger . debug ( blue , "hello" ) ; logger . info ( blue , "hello" ) ; logger . warn ( blue , "hello" ) ; logger . error ( blue , "hello" ) ; logger . debug ( blue , "hello {}" , "world" ) ; logger . info ( blue , "hello {}" , "world" ) ; logger . warn ( blue , "hello {}" , "world" ) ; logger . error ( blue , "hello {}" , "world" ) ; logger . debug ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . info ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . warn ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . error ( blue , "hello {} and {} " , "world" , "universe" ) ; }
@ Test public void testMarker ( ) { Logger logger = LoggerFactory . getLogger ( "testMarker" ) ; Marker blue = MarkerFactory . getMarker ( "BLUE" ) ; logger . debug ( blue , "hello" ) ; logger . info ( blue , "hello" ) ; logger . warn ( blue , "hello" ) ; logger . warn ( blue , "hello" ) ; logger . debug ( blue , "hello {}" , "world" ) ; logger . info ( blue , "hello {}" , "world" ) ; logger . warn ( blue , "hello {}" , "world" ) ; logger . error ( blue , "hello {}" , "world" ) ; logger . debug ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . info ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . warn ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . error ( blue , "hello {} and {} " , "world" , "universe" ) ; }
@ Test public void testMarker ( ) { Logger logger = LoggerFactory . getLogger ( "testMarker" ) ; Marker blue = MarkerFactory . getMarker ( "BLUE" ) ; logger . debug ( blue , "hello" ) ; logger . info ( blue , "hello" ) ; logger . warn ( blue , "hello" ) ; logger . error ( blue , "hello" ) ; logger . debug ( blue , "hello {}" , "world" ) ; logger . info ( blue , "hello {}" , "world" ) ; logger . warn ( blue , "hello {}" , "world" ) ; logger . error ( blue , "hello {}" , "world" ) ; logger . debug ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . info ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . warn ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . error ( blue , "hello {} and {} " , "world" , "universe" ) ; }
@ Test public void testMarker ( ) { Logger logger = LoggerFactory . getLogger ( "testMarker" ) ; Marker blue = MarkerFactory . getMarker ( "BLUE" ) ; logger . debug ( blue , "hello" ) ; logger . info ( blue , "hello" ) ; logger . warn ( blue , "hello" ) ; logger . error ( blue , "hello" ) ; logger . debug ( blue , "hello {}" , "world" ) ; logger . warn ( blue , "hello {}" , "world" ) ; logger . error ( blue , "hello {}" , "world" ) ; logger . debug ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . info ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . warn ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . error ( blue , "hello {} and {} " , "world" , "universe" ) ; }
@ Test public void testMarker ( ) { Logger logger = LoggerFactory . getLogger ( "testMarker" ) ; Marker blue = MarkerFactory . getMarker ( "BLUE" ) ; logger . debug ( blue , "hello" ) ; logger . info ( blue , "hello" ) ; logger . warn ( blue , "hello" ) ; logger . error ( blue , "hello" ) ; logger . debug ( blue , "hello {}" , "world" ) ; logger . info ( blue , "hello {}" , "world" ) ; logger . error ( blue , "hello {}" , "world" ) ; logger . debug ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . info ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . warn ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . warn ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . error ( blue , "hello {} and {} " , "world" , "universe" ) ; }
@ Test public void testMarker ( ) { Logger logger = LoggerFactory . getLogger ( "testMarker" ) ; Marker blue = MarkerFactory . getMarker ( "BLUE" ) ; logger . debug ( blue , "hello" ) ; logger . info ( blue , "hello" ) ; logger . warn ( blue , "hello" ) ; logger . error ( blue , "hello" ) ; logger . debug ( blue , "hello {}" , "world" ) ; logger . info ( blue , "hello {}" , "world" ) ; logger . warn ( blue , "hello {}" , "world" ) ; logger . debug ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . info ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . warn ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . error ( blue , "hello {} and {} " , "world" , "universe" ) ; }
@ Test public void testMarker ( ) { Logger logger = LoggerFactory . getLogger ( "testMarker" ) ; Marker blue = MarkerFactory . getMarker ( "BLUE" ) ; logger . debug ( blue , "hello" ) ; logger . info ( blue , "hello" ) ; logger . warn ( blue , "hello" ) ; logger . error ( blue , "hello" ) ; logger . debug ( blue , "hello {}" , "world" ) ; logger . info ( blue , "hello {}" , "world" ) ; logger . warn ( blue , "hello {}" , "world" ) ; logger . error ( blue , "hello {}" , "world" ) ; logger . debug ( blue , "hello {}" , "world" ) ; logger . info ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . warn ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . error ( blue , "hello {} and {} " , "world" , "universe" ) ; }
@ Test public void testMarker ( ) { Logger logger = LoggerFactory . getLogger ( "testMarker" ) ; Marker blue = MarkerFactory . getMarker ( "BLUE" ) ; logger . debug ( blue , "hello" ) ; logger . info ( blue , "hello" ) ; logger . warn ( blue , "hello" ) ; logger . error ( blue , "hello" ) ; logger . debug ( blue , "hello {}" , "world" ) ; logger . info ( blue , "hello {}" , "world" ) ; logger . warn ( blue , "hello {}" , "world" ) ; logger . error ( blue , "hello {}" , "world" ) ; logger . debug ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . warn ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . error ( blue , "hello {} and {} " , "world" , "universe" ) ; }
@ Test public void testMarker ( ) { Logger logger = LoggerFactory . getLogger ( "testMarker" ) ; Marker blue = MarkerFactory . getMarker ( "BLUE" ) ; logger . debug ( blue , "hello" ) ; logger . info ( blue , "hello" ) ; logger . warn ( blue , "hello" ) ; logger . error ( blue , "hello" ) ; logger . debug ( blue , "hello {}" , "world" ) ; logger . info ( blue , "hello {}" , "world" ) ; logger . warn ( blue , "hello {}" , "world" ) ; logger . error ( blue , "hello {}" , "world" ) ; logger . debug ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . info ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . error ( blue , "hello {} and {} " , "world" , "universe" ) ; }
@ Test public void testMarker ( ) { Logger logger = LoggerFactory . getLogger ( "testMarker" ) ; Marker blue = MarkerFactory . getMarker ( "BLUE" ) ; logger . debug ( blue , "hello" ) ; logger . info ( blue , "hello" ) ; logger . warn ( blue , "hello" ) ; logger . warn ( blue , "hello" ) ; logger . error ( blue , "hello" ) ; logger . debug ( blue , "hello {}" , "world" ) ; logger . info ( blue , "hello {}" , "world" ) ; logger . warn ( blue , "hello {}" , "world" ) ; logger . error ( blue , "hello {}" , "world" ) ; logger . debug ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . info ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . warn ( blue , "hello {} and {} " , "world" , "universe" ) ; }
public void test() { if ( document == null ) { logger . warn ( "Missing document: " + summary ) ; } else { final File indexDir ; code_block = IfStatement ; final IndexableDocument doc = new IndexableDocument ( document , summary , indexDir ) ; indexableDocs . add ( doc ) ; } }
public void test() { try { indexTask . reIndex ( indexableDocs , CommitPreference . PREVENT_COMMIT ) ; } catch ( final IOException ioe ) { logger . error ( "Failed to re-index some Provenance Events. " + "Some Provenance Events may not be available for querying. See logs for more information." , ioe ) ; eventReporter . reportEvent ( Severity . ERROR , EVENT_CATEGORY , "Failed to re-index some Provenance Events. " + "Some Provenance Events may not be available for querying. See logs for more information." ) ; } }
@ Override @ SuppressWarnings ( "deprecation" ) public RequestCtx handleRequest ( SOAPMessageContext context ) throws OperationHandlerException { logger . debug ( "handleRequest!" ) ; RequestCtx req = null ; Object oMap = null ; String pid = null ; String sDefPid = null ; String methodName = null ; String asOfDateTime = null ; code_block = TryStatement ;  code_block = TryStatement ;  logger . debug ( "Extracted SOAP Request Objects" ) ; Map < URI , AttributeValue > actions = new HashMap < URI , AttributeValue > ( ) ; Map < URI , AttributeValue > resAttr ; code_block = TryStatement ;  return req ; }
public void test() { try { oMap = getSOAPRequestObjects ( context ) ; logger . debug ( "Retrieved SOAP Request Objects" ) ; } catch ( SoapFault af ) { logger . error ( "Error obtaining SOAP Request Objects" , af ) ; throw new OperationHandlerException ( "Error obtaining SOAP Request Objects" , af ) ; } }
public void test() { try { pid = ( String ) callGetter ( "getPid" , oMap ) ; sDefPid = ( String ) callGetter ( "getServiceDefinitionPid" , oMap ) ; methodName = ( String ) callGetter ( "getMethodName" , oMap ) ; asOfDateTime = ( String ) callGetter ( "getAsOfDateTime" , oMap ) ; } catch ( Exception e ) { logger . error ( "Error obtaining parameters" , e ) ; throw new OperationHandlerException ( "Error obtaining parameters." , e ) ; } }
public void test() { try { resAttr = ResourceAttributes . getResources ( pid ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; actions . put ( Constants . ACTION . ID . getURI ( ) , Constants . ACTION . GET_DISSEMINATION . getStringAttribute ( ) ) ; actions . put ( Constants . ACTION . API . getURI ( ) , Constants . ACTION . APIA . getStringAttribute ( ) ) ; req = getContextHandler ( ) . buildRequest ( getSubjects ( context ) , actions , resAttr , getEnvironment ( context ) ) ; LogUtil . statLog ( getUser ( context ) , Constants . ACTION . GET_DISSEMINATION . uri , pid , null ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; throw new OperationHandlerException ( e . getMessage ( ) , e ) ; } }
@ Override public void unRegisterService ( String serviceType , String baseUrl ) throws ServiceRegistryException { logger . info ( "Unregistering service type: {}" , baseUrl ) ; setOnlineStatus ( serviceType , baseUrl , null , false , null ) ; cleanRunningJobs ( serviceType , baseUrl ) ; }
public synchronized void producersConnected ( final Wire [ ] wires ) { logger . trace ( "Connected" ) ; this . wireSupport . producersConnected ( wires ) ; }
private void logCallInfo ( URI _uri , Map < String , String > _params ) { final StringBuilder b = new StringBuilder ( ) ; b . append ( "Calling [" ) . append ( _uri . toString ( ) ) . append ( "]" ) ; code_block = IfStatement ; log . debug ( b . toString ( ) ) ; }
@ Override public Reader loadResource ( String relativePath ) { logger . info ( "Loaded resource " + relativePath ) ; InputStream is = this . classLoader . getResourceAsStream ( relativePath ) ; return is != null ? new InputStreamReader ( is ) : null ; }
public List < String > lookupAssigningAuthorities ( String homeCommunityId ) { logger . debug ( "getAssigningAuthorities by homeCommunityId:" + homeCommunityId ) ; return mappingDao . getAssigningAuthoritiesByHomeCommunity ( homeCommunityId ) ; }
public void test() { try { m3uaManagementEventListener . onAsCreated ( as ) ; } catch ( Throwable ee ) { logger . error ( ee . getMessage ( ) , ee ) ; } }
@ Test public void deleteNotExistingUserExpectError ( ) throws Exception { LOG . info ( "deleteNotExistingUserExpectError" ) ; assertEquals ( deleteUser ( adminConnectionCorrect , NEW_USER_SDN ) . getStatus ( ) , HttpStatus . NOT_FOUND_404 ) ; }
public void test() { try { code_block = IfStatement ; MessageResourcesFactory factory = ( MessageResourcesFactory ) clazz . newInstance ( ) ; return ( factory ) ; } catch ( Throwable t ) { logger . error ( "Error creating message resources factory" , t ) ; return ( null ) ; } }
public void test() { try { fileChannel . close ( ) ; } catch ( IOException e ) { logger . warn ( "Exception while closing channel for log file:" + logId ) ; } }
public void test() { if ( n1 == null ) { log . warn ( "no n1 to " + n1 . toString ( ) ) ; return Integer . MAX_VALUE ; } }
public void test() { if ( n2 == null ) { log . warn ( "Warning: n2 is null" ) ; return Integer . MAX_VALUE ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { job . dead ( execution ) ; jobAccessor . save ( execution ) ; } catch ( Exception t ) { logger . error ( "Error dead job" , t ) ; } }
public void test() { if ( LOG . isWarnEnabled ( ) ) { LOG . warn ( "Unhandled exception: " , e ) ; } }
public void test() { try { code_block = TryStatement ;  } catch ( final ValidationUserException e ) { final UiMessageStack uiMessageStack = routeContext . getUiMessageStack ( ) ; e . flushToUiMessageStack ( uiMessageStack ) ; return sendJsonUiMessageStack ( SC_UNPROCESSABLE_ENTITY , uiMessageStack , response ) ; } catch ( final VUserException e ) { final UiMessageStack uiMessageStack = routeContext . getUiMessageStack ( ) ; uiMessageStack . error ( e . getMessage ( ) ) ; LOGGER . error ( "Failed to execute request" , e ) ; return sendJsonUiMessageStack ( SC_UNPROCESSABLE_ENTITY , uiMessageStack , response ) ; } catch ( final SessionException e ) { return sendJsonError ( HttpServletResponse . SC_UNAUTHORIZED , e , response ) ; } catch ( final VSecurityException e ) { return sendJsonError ( HttpServletResponse . SC_FORBIDDEN , e , response ) ; } catch ( final JsonSyntaxException e ) { return sendJsonError ( HttpServletResponse . SC_BAD_REQUEST , e , response ) ; } catch ( final TooManyRequestException e ) { return sendJsonError ( SC_TOO_MANY_REQUEST , e , response ) ; } catch ( final Throwable e ) { LOGGER . error ( "Internal Server Error" , e ) ; return sendJsonError ( HttpServletResponse . SC_INTERNAL_SERVER_ERROR , e , response ) ; } }
public void test() { try { code_block = TryStatement ;  } catch ( final ValidationUserException e ) { final UiMessageStack uiMessageStack = routeContext . getUiMessageStack ( ) ; e . flushToUiMessageStack ( uiMessageStack ) ; return sendJsonUiMessageStack ( SC_UNPROCESSABLE_ENTITY , uiMessageStack , response ) ; } catch ( final VUserException e ) { final UiMessageStack uiMessageStack = routeContext . getUiMessageStack ( ) ; uiMessageStack . error ( e . getMessage ( ) ) ; return sendJsonUiMessageStack ( SC_UNPROCESSABLE_ENTITY , uiMessageStack , response ) ; } catch ( final SessionException e ) { return sendJsonError ( HttpServletResponse . SC_UNAUTHORIZED , e , response ) ; } catch ( final VSecurityException e ) { return sendJsonError ( HttpServletResponse . SC_FORBIDDEN , e , response ) ; } catch ( final JsonSyntaxException e ) { LOGGER . info ( "JsonSyntaxException" , e ) ; return sendJsonError ( HttpServletResponse . SC_BAD_REQUEST , e , response ) ; } catch ( final TooManyRequestException e ) { LOGGER . info ( "JsonSyntaxException" , e ) ; return sendJsonError ( SC_TOO_MANY_REQUEST , e , response ) ; } catch ( final Throwable e ) { return sendJsonError ( HttpServletResponse . SC_INTERNAL_SERVER_ERROR , e , response ) ; } }
public void test() { if ( jobCounter . get ( ) != currentAlertCount ) { LOG . info ( "Job count {}" , currentAlertCount ) ; } }
public void test() { try { int currentAlertCount = jobCounter . get ( ) ; jobCounter . addAndGet ( service . executeScheduledAlerts ( 50 , timeout ) ) ; code_block = IfStatement ; Thread . sleep ( POLL_INTERVAL_MS ) ; } catch ( InterruptedException ex ) { LOGGER . debug ( "Alerter thread was interrupted." ) ; Thread . currentThread ( ) . interrupt ( ) ; break ; } catch ( Throwable ex ) { LOGGER . error ( "Exception in alerter: {}" , ExceptionUtils . getFullStackTrace ( ex ) ) ; } }
public void test() { try { int currentAlertCount = jobCounter . get ( ) ; jobCounter . addAndGet ( service . executeScheduledAlerts ( 50 , timeout ) ) ; code_block = IfStatement ; Thread . sleep ( POLL_INTERVAL_MS ) ; } catch ( InterruptedException ex ) { LOGGER . info ( "Execution was interrupted." ) ; Thread . currentThread ( ) . interrupt ( ) ; break ; } catch ( Throwable ex ) { LOGGER . warn ( "Error occurred while executing scheduled alert." , ex ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { try { v = Short . parseShort ( value . trim ( ) ) ; } catch ( NumberFormatException e ) { Logger logger = Logger . getLogger ( StringTools . class ) ; logger . debug ( "Must be of type short!" , e ) ; throw new IllegalArgumentException ( "Must be of type short!" , e ) ; } }
@ Override public void lifeCycleStarted ( LifeCycle event ) { final Set < SslReload > reloaders = new HashSet < > ( ) ; reloaders . addAll ( getReloaders ( environment . getApplicationContext ( ) ) ) ; reloaders . addAll ( getReloaders ( environment . getAdminContext ( ) ) ) ; reloadTask . setReloaders ( reloaders ) ; LOG . debug ( "security reloaded" ) ; }
public void test() { if ( totalError > lastTotalError ) { logger . warn ( msg ) ; lastTotalError = totalError ; } else { logger . info ( msg ) ; } }
public void test() { if ( totalError > lastTotalError ) { logger . warn ( "Exception in dict\n {}" , msg ) ; lastTotalError = totalError ; } else { logger . debug ( "Error in dict\n {}" , msg ) ; } }
@ Override public void onConnectGPRSRequest ( ConnectGPRSRequest ind ) { this . logger . debug ( "ConnectGPRSRequest" ) ; TestEvent te = TestEvent . createReceivedEvent ( EventType . ConnectGPRSRequest , ind , sequence ++ ) ; this . observerdEvents . add ( te ) ; }
@ Override public void onError ( final Task task , final Throwable throwable ) { LOG . error ( "Task failed" , throwable ) ; errorReference . compareAndSet ( null , throwable ) ; }
public void test() { try { return file . toURI ( ) . toURL ( ) ; } catch ( MalformedURLException e ) { Log . log ( e ) ; return null ; } }
public void test() { { final String user = SecurityContextHolder . getContext ( ) . getAuthentication ( ) . getName ( ) ; logger . info ( "Adding node {} to replicaSet." , nodeID ) ; streamingService . addNodeToReplicaSet ( replicaSetID , nodeID ) ; } }
public void test() { if ( read != slotFile . length ( ) && logger . isWarnEnabled ( ) ) { logger . warn ( "Read offset : " + read ) ; } }
public void test() { try ( FileInputStream fileInputStream = new FileInputStream ( slotFile ) ; BufferedInputStream bufferedInputStream = new BufferedInputStream ( fileInputStream ) ) { byte [ ] bytes = new byte [ ( int ) slotFile . length ( ) ] ; int read = bufferedInputStream . read ( bytes ) ; code_block = IfStatement ; deserialize ( ByteBuffer . wrap ( bytes ) ) ; return true ; } catch ( Exception e ) { logger . warn ( "Could not read slot file: " + slotFile , e ) ; return false ; } }
public void test() { try { Log . debug ( "Annotating Workflow " + workflowSWID ) ; Workflow obj = ll . findWorkflow ( "/" + workflowSWID ) ; code_block = IfStatement ; code_block = IfStatement ; ll . updateWorkflow ( "/" + workflowSWID , obj ) ; } catch ( IOException ex ) { Log . error ( "IOException while updating study " + workflowSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } catch ( JAXBException ex ) { Log . error ( "JAXBException while updating study " + workflowSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } catch ( ResourceException ex ) { Log . error ( "ResourceException while updating study " + workflowSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } }
public void test() { if ( skip != null ) { Log . info ( "Processing does not have a skip column!" ) ; } }
public void test() { try { Log . debug ( "Annotating WorkflowRun " + workflowSWID + " with skip=" + skip + ", Att = " + att ) ; Workflow obj = ll . findWorkflow ( "/" + workflowSWID ) ; code_block = IfStatement ; code_block = IfStatement ; ll . updateWorkflow ( "/" + workflowSWID , obj ) ; } catch ( IOException ex ) { Log . error ( "IOException while updating study " + workflowSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } catch ( JAXBException ex ) { Log . error ( "JAXBException while updating study " + workflowSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } catch ( ResourceException ex ) { Log . error ( "ResourceException while updating study " + workflowSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } }
public void test() { try { Log . debug ( "Annotating WorkflowRun " + workflowSWID + " with skip=" + skip + ", Att = " + att ) ; Workflow obj = ll . findWorkflow ( "/" + workflowSWID ) ; code_block = IfStatement ; code_block = IfStatement ; ll . updateWorkflow ( "/" + workflowSWID , obj ) ; } catch ( IOException ex ) { Log . error ( "IOException while updating study " + workflowSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } catch ( JAXBException ex ) { Log . error ( "JAXBException while updating study " + workflowSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } catch ( ResourceException ex ) { Log . error ( "ResourceException while updating study " + workflowSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } }
public void test() { try { Log . debug ( "Annotating WorkflowRun " + workflowSWID + " with skip=" + skip + ", Att = " + att ) ; Workflow obj = ll . findWorkflow ( "/" + workflowSWID ) ; code_block = IfStatement ; code_block = IfStatement ; ll . updateWorkflow ( "/" + workflowSWID , obj ) ; } catch ( IOException ex ) { Log . error ( "IOException while updating study " + workflowSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } catch ( JAXBException ex ) { Log . error ( "JAXBException while updating study " + workflowSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } catch ( ResourceException ex ) { Log . error ( "ResourceException while updating study " + workflowSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } }
public void test() { if ( pTO . getIdentifiers ( ) . size ( ) == 1 ) { IdentifierTO ito = new IdentifierTO ( ) ; ito . setValue ( pTO . getIdentifiers ( ) . get ( 0 ) . getValue ( ) ) ; person . setIdentifier ( ito . getValue ( ) ) ; } else-if ( pTO . getIdentifiers ( ) . size ( ) > 1 ) { log . warn ( "Multiple IdentifierTO found for person with name '" + person . getIdentifier ( ) + "' and type '" + pTO . getIdentifiers ( ) . get ( 0 ) . getValue ( ) + "'" ) ; } }
public void test() { try { detector = getDefaultLanguageDetector ( ) ; detector . loadModels ( ) ; } catch ( IOException e ) { logger . error ( "fail to load language detector" , e ) ; } }
public void test() { if ( configuration . isValid ( ) ) { return ; } }
public void test() { if ( Log . isErrorEnabled ( ) ) { Log . error ( "Error while getting logs." , e ) ; } }
@ Override protected void tearDown ( ) throws Exception { LOG . info ( "tearDown" ) ; connection . close ( ) ; service . stop ( ) ; context . close ( ) ; super . tearDown ( ) ; }
protected final void initOutput ( BatchSinkContext context , BigQuery bigQuery , String outputName , String tableName , @ Nullable Schema tableSchema , String bucket , FailureCollector collector ) throws IOException { List < BigQueryTableFieldSchema > fields = getBigQueryTableFields ( bigQuery , tableName , tableSchema , getConfig ( ) . isAllowSchemaRelaxation ( ) , collector ) ; Configuration configuration = new Configuration ( baseConfiguration ) ; String temporaryGcsPath = BigQuerySinkUtils . getTemporaryGcsPath ( bucket , runUUID . toString ( ) , tableName ) ; LOG . info ( "Creating output path '{}'" , temporaryGcsPath ) ; BigQuerySinkUtils . configureOutput ( configuration , getConfig ( ) . getDatasetProject ( ) , getConfig ( ) . getDataset ( ) , tableName , temporaryGcsPath , fields ) ; List < String > fieldNames = fields . stream ( ) . map ( BigQueryTableFieldSchema :: getName ) . collect ( Collectors . toList ( ) ) ; recordLineage ( context , outputName , tableSchema , fieldNames ) ; context . addOutput ( Output . of ( outputName , getOutputFormatProvider ( configuration , tableName , tableSchema ) ) ) ; }
public void test() { if ( ! authorizationResponseStr . contains ( "" ) ) { Wait < WebDriver > wait = new FluentWait < WebDriver > ( driver ) . withTimeout ( Duration . ofSeconds ( WAIT_OPERATION_TIMEOUT ) ) . pollingEvery ( Duration . ofMillis ( 500 ) ) . ignoring ( NoSuchElementException . class ) ; WebElement allowButton = wait . until ( new Function < WebDriver , WebElement > ( ) code_block = "" ; ) ; JavascriptExecutor jse = ( JavascriptExecutor ) driver ; jse . executeScript ( "scroll(0, 1000)" ) ; String previousURL = driver . getCurrentUrl ( ) ; Actions actions = new Actions ( driver ) ; actions . click ( allowButton ) . perform ( ) ; authorizationResponseStr = driver . getCurrentUrl ( ) ; authorizationResponse = new AuthorizationResponse ( authorizationResponseStr ) ; log . info ( "Authorization completed successfully." ) ; } else { fail ( "The authorization form was expected to be shown." ) ; } }
public void test() { try { AtlasAuthorizer authorizer = AtlasAuthorizerFactory . getAtlasAuthorizer ( ) ; request . setUser ( getCurrentUserName ( ) , getCurrentUserGroups ( ) ) ; request . setClientIPAddress ( RequestContext . get ( ) . getClientIPAddress ( ) ) ; request . setForwardedAddresses ( RequestContext . get ( ) . getForwardedAddresses ( ) ) ; request . setRemoteIPAddress ( RequestContext . get ( ) . getClientIPAddress ( ) ) ; authorizer . filterTypesDef ( request ) ; } catch ( AtlasAuthorizationException e ) { LOG . error ( "Exception in AtlasSimpleAuthorizer" , e ) ; } }
@ Test void parse ( ) throws Exception { JavaClassSource clazz = ( JavaClassSource ) Roaster . parse ( new File ( "src/test/java/org/apache/camel/parser/java/MySimpleToDRoute.java" ) ) ; MethodSource < JavaClassSource > method = CamelJavaParserHelper . findConfigureMethod ( clazz ) ; List < CamelEndpointDetails > details = new ArrayList < > ( ) ; RouteBuilderParser . parseRouteBuilderEndpoints ( clazz , "." , "src/test/java/org/apache/camel/parser/java/MySimpleToDRoute.java" , details ) ; LOG . info ( "{}" , details ) ; List < ParserResult > list = CamelJavaParserHelper . parseCamelConsumerUris ( method , true , true ) ; code_block = ForStatement ; assertEquals ( "direct:start" , list . get ( 0 ) . getElement ( ) ) ; list = CamelJavaParserHelper . parseCamelProducerUris ( method , true , true ) ; code_block = ForStatement ; assertEquals ( "toD" , list . get ( 0 ) . getNode ( ) ) ; assertEquals ( "log:a" , list . get ( 0 ) . getElement ( ) ) ; assertEquals ( "to" , list . get ( 1 ) . getNode ( ) ) ; assertEquals ( "log:b" , list . get ( 1 ) . getElement ( ) ) ; assertEquals ( "to" , list . get ( 2 ) . getNode ( ) ) ; assertEquals ( "log:c" , list . get ( 2 ) . getElement ( ) ) ; assertEquals ( 3 , list . size ( ) ) ; assertEquals ( 4 , details . size ( ) ) ; assertEquals ( "direct:start" , details . get ( 0 ) . getEndpointUri ( ) ) ; assertEquals ( "log:a" , details . get ( 1 ) . getEndpointUri ( ) ) ; assertEquals ( "log:b" , details . get ( 2 ) . getEndpointUri ( ) ) ; assertEquals ( "log:c" , details . get ( 3 ) . getEndpointUri ( ) ) ; }
public void test() { for ( ParserResult result : list ) { LOG . debug ( "Parser: " + result . getMessage ( ) ) ; } }
public void test() { for ( ParserResult result : list ) { LOG . debug ( "Parser: " + result . getMessage ( ) ) ; } }
public void test() { if ( t != null ) { service . unregisterChannel ( channel ) ; channel . close ( true ) ; buffer . putByte ( ( byte ) 0x5b ) ; } else { logger . error ( "Failed to close TCP socket." , t ) ; buffer . putByte ( ( byte ) 0x5a ) ; } }
public Object call ( ) throws Exception { LOG . info ( "Waiting {} ms" , getEndpoint ( ) . getDelay ( ) ) ; Thread . sleep ( getEndpoint ( ) . getDelay ( ) ) ; int count = counter . incrementAndGet ( ) ; code_block = IfStatement ; LOG . info ( "Callback done(false)" ) ; callback . done ( false ) ; return null ; }
public void test() { if ( getEndpoint ( ) . getFailFirstAttempts ( ) >= count ) { LOG . info ( "Simulated error at attempt " + count ) ; exchange . setException ( new CamelExchangeException ( "Simulated error at attempt " + count , exchange ) ) ; } else { String reply = getEndpoint ( ) . getReply ( ) ; exchange . getMessage ( ) . setBody ( reply ) ; exchange . getMessage ( ) . setHeaders ( exchange . getIn ( ) . getHeaders ( ) ) ; LOG . info ( "Setting reply " + reply ) ; } }
public void test() { if ( getEndpoint ( ) . getFailFirstAttempts ( ) >= count ) { LOG . info ( "Simulating a failure at attempt " + count ) ; exchange . setException ( new CamelExchangeException ( "Simulated error at attempt " + count , exchange ) ) ; } else { String reply = getEndpoint ( ) . getReply ( ) ; LOG . debug ( "Received reply: {}" , reply ) ; exchange . getMessage ( ) . setBody ( reply ) ; exchange . getMessage ( ) . setHeaders ( exchange . getIn ( ) . getHeaders ( ) ) ; } }
public void test() { if ( label . getLang ( ) . equals ( Locale . getDefault ( ) . getLanguage ( ) ) ) { final String labelText = label . getLabel ( ) ; if ( labelText != null ) logger . debug ( "label text: {}" , labelText ) ; return labelText ; } }
public void test() { try { Message prototype = ( Message ) super . getEntityPrototype ( formTypeCode ) ; Integer version = 1 ; code_block = IfStatement ; prototype . setVersionType ( version ) ; String modelXml = new FormTypeDOM ( ) . getXml ( prototype ) ; StepsConfig stepsConfig = this . getStepsConfig ( formTypeCode ) ; String stepsXml = new StepConfigsDOM ( ) . createConfigXml ( stepsConfig ) ; TypeVersionGuiConfig versionConfig = new TypeVersionGuiConfig ( ) ; versionConfig . setFormTypeCode ( formTypeCode ) ; versionConfig . setPrototypeXml ( modelXml ) ; versionConfig . setPrototype ( prototype ) ; versionConfig . setStepsConfigXml ( stepsXml ) ; versionConfig . setStepsConfig ( stepsConfig ) ; Map < String , StepGuiConfig > workGuiConfigs = this . getFormTypeGuiDAO ( ) . getWorkGuiConfigs ( formTypeCode ) ; List < StepGuiConfig > guiConfigs = new ArrayList < StepGuiConfig > ( ) ; guiConfigs . addAll ( workGuiConfigs . values ( ) ) ; versionConfig . setGuiConfigs ( guiConfigs ) ; versionConfig . setVersion ( prototype . getVersionType ( ) ) ; this . generateLabels ( versionConfig ) ; this . getFormTypeGuiDAO ( ) . addTypeVersionGui ( versionConfig ) ; this . getLastVersionConfigs ( ) . put ( formTypeCode , versionConfig ) ; this . updateEntityPrototype ( prototype ) ; } catch ( Throwable t ) { _logger . error ( "Error generating new version type - {}" , formTypeCode , t ) ; throw new ApsSystemException ( "Error generating new version type - " + formTypeCode , t ) ; } }
private void sendRequest ( Socket socket , Integer port , String request , String arguments , Logger logger ) throws IOException { logger . debug ( "Sending command to NiFi instance..." ) ; socket . setSoTimeout ( 60000 ) ; socket . connect ( new InetSocketAddress ( "localhost" , port ) ) ; logger . debug ( "Established connection to NiFi instance." ) ; socket . setSoTimeout ( 60000 ) ; logger . debug ( "Sending {} Command to port {}" , request , port ) ; final OutputStream socketOut = socket . getOutputStream ( ) ; final Properties nifiProps = loadProperties ( logger ) ; final String secretKey = nifiProps . getProperty ( "secret.key" ) ; code_block = IfStatement ; socketOut . flush ( ) ; }
private void sendRequest ( Socket socket , Integer port , String request , String arguments , Logger logger ) throws IOException { logger . debug ( "Connecting to NiFi instance" ) ; socket . setSoTimeout ( 60000 ) ; socket . connect ( new InetSocketAddress ( "localhost" , port ) ) ; socket . setSoTimeout ( 60000 ) ; logger . debug ( "Sending {} Command to port {}" , request , port ) ; final OutputStream socketOut = socket . getOutputStream ( ) ; final Properties nifiProps = loadProperties ( logger ) ; final String secretKey = nifiProps . getProperty ( "secret.key" ) ; code_block = IfStatement ; logger . debug ( "Sending {} Command to port {}" , request , port ) ; socketOut . flush ( ) ; }
public void test() { if ( db . isFile ( ) ) { final File temp = settings . getTempDirectory ( ) ; final File tempDB = new File ( temp , db . getName ( ) ) ; Files . copy ( db . toPath ( ) , tempDB . toPath ( ) ) ; settings . setString ( Settings . KEYS . H2_DATA_DIRECTORY , temp . getPath ( ) ) ; final String connStr = settings . getString ( Settings . KEYS . DB_CONNECTION_STRING ) ; code_block = IfStatement ; database = new CveDB ( settings ) ; } else { logger . error ( "Unable to open database - configured database file does not exist: " + db . toString ( ) ) ; throw new DatabaseException ( "Unable to open database - configured database file does not exist: " + db . toString ( ) ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public Job run ( ) throws Exception { Job job = Job . getInstance ( getConf ( ) ) ; job . setJobName ( name ) ; job . setJarByClass ( RollupPhaseFourJob . class ) ; job . setMapperClass ( RollupPhaseFourMapper . class ) ; job . setInputFormatClass ( SequenceFileInputFormat . class ) ; job . setMapOutputKeyClass ( BytesWritable . class ) ; job . setMapOutputValueClass ( BytesWritable . class ) ; job . setCombinerClass ( RollupPhaseFourReducer . class ) ; job . setReducerClass ( RollupPhaseFourReducer . class ) ; job . setOutputKeyClass ( BytesWritable . class ) ; job . setOutputValueClass ( BytesWritable . class ) ; job . setOutputFormatClass ( SequenceFileOutputFormat . class ) ; String numReducers = props . getProperty ( "num.reducers" ) ; job . setNumReduceTasks ( 1 ) ; LOGGER . info ( "Num reducers : " + numReducers ) ; Configuration configuration = job . getConfiguration ( ) ; String inputPathDir = getAndSetConfiguration ( configuration , ROLLUP_PHASE4_INPUT_PATH ) ; getAndSetConfiguration ( configuration , ROLLUP_PHASE4_CONFIG_PATH ) ; getAndSetConfiguration ( configuration , ROLLUP_PHASE4_OUTPUT_PATH ) ; LOGGER . info ( "Input path dir: " + inputPathDir ) ; code_block = ForStatement ; FileOutputFormat . setOutputPath ( job , new Path ( getAndCheck ( ROLLUP_PHASE4_OUTPUT_PATH . toString ( ) ) ) ) ; job . waitForCompletion ( true ) ; return job ; }
public Job run ( ) throws Exception { Job job = Job . getInstance ( getConf ( ) ) ; job . setJobName ( name ) ; job . setJarByClass ( RollupPhaseFourJob . class ) ; job . setMapperClass ( RollupPhaseFourMapper . class ) ; job . setInputFormatClass ( SequenceFileInputFormat . class ) ; job . setMapOutputKeyClass ( BytesWritable . class ) ; job . setMapOutputValueClass ( BytesWritable . class ) ; job . setCombinerClass ( RollupPhaseFourReducer . class ) ; job . setReducerClass ( RollupPhaseFourReducer . class ) ; job . setOutputKeyClass ( BytesWritable . class ) ; job . setOutputValueClass ( BytesWritable . class ) ; job . setOutputFormatClass ( SequenceFileOutputFormat . class ) ; String numReducers = props . getProperty ( "num.reducers" ) ; job . setNumReduceTasks ( 1 ) ; LOGGER . info ( "Setting number of reducers : " + job . getNumReduceTasks ( ) ) ; Configuration configuration = job . getConfiguration ( ) ; String inputPathDir = getAndSetConfiguration ( configuration , ROLLUP_PHASE4_INPUT_PATH ) ; getAndSetConfiguration ( configuration , ROLLUP_PHASE4_CONFIG_PATH ) ; getAndSetConfiguration ( configuration , ROLLUP_PHASE4_OUTPUT_PATH ) ; LOGGER . info ( "Input path dir: " + inputPathDir ) ; code_block = ForStatement ; FileOutputFormat . setOutputPath ( job , new Path ( getAndCheck ( ROLLUP_PHASE4_OUTPUT_PATH . toString ( ) ) ) ) ; job . waitForCompletion ( true ) ; return job ; }
public void test() { for ( String inputPath : inputPathDir . split ( "," ) ) { LOGGER . info ( "Adding input:" + inputPath ) ; Path input = new Path ( inputPath ) ; FileInputFormat . addInputPath ( job , input ) ; } }
private void getHouseStatusCommsJob ( ) { logger . trace ( "getHouseStatusCommsJob() started." ) ; code_block = IfStatement ; logger . trace ( "getHouseStatusCommsJob() initiated by {} has finished." , Thread . currentThread ( ) ) ; }
public void test() { if ( new VeluxBridgeGetHouseStatus ( ) . evaluateState ( thisBridge ) ) { logger . trace ( "getHouseStatusCommsJob(): => GetHouseStatus() => ready state" ) ; syncChannelsWithProducts ( ) ; } else { logger . trace ( "getHouseStatusCommsJob(): => GetHouseStatus() => no updates" ) ; } }
public void test() { if ( new VeluxBridgeGetHouseStatus ( ) . evaluateState ( thisBridge ) ) { logger . trace ( "getHouseStatusCommsJob(): => GetHouseStatus() => updates received => synchronizing" ) ; syncChannelsWithProducts ( ) ; } else { logger . trace ( "getHouseStatusCommsJob(): => Nothing to do" ) ; } }
public void test() { if ( this . logger . isDebugEnabled ( ) ) { this . logger . debug ( "Retrieved possible data from [" + source + "]" ) ; } }
public void test() { try { componentCSVRecord = new ComponentCSVRecordBuilder ( input ) . build ( ) ; } catch ( Exception e ) { log . error ( "Broken component: " + input , e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( KBCommentServiceUtil . class , "getKBCommentsCount" , _getKBCommentsCountParameterTypes9 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( ( Integer ) returnObj ) . intValue ( ) ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( log . isErrorEnabled ( ) ) { log . error ( throwable . getMessage ( ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
private static boolean isFieldQueried ( Path field , ValueComparisonExpression q , Path context ) { LOGGER . debug ( "Checking if field {} is queried by queried field {}" , field , q ) ; return isFieldQueried ( field , q . getField ( ) , context ) ; }
public void test() { try { } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( state == null ) { log . debug ( "Resetting offset for partition {} to position {}." , tp , position ) ; } else-if ( ! state . awaitingReset ( ) ) { log . debug ( "Skipping reset of partition {} since reset is no longer needed" , tp ) ; } else-if ( requestedResetStrategy != state . resetStrategy ) { log . debug ( "Skipping reset of partition {} since an alternative reset has been requested" , tp ) ; } else { log . info ( "Resetting offset for partition {} to position {}." , tp , position ) ; state . seekUnvalidated ( position ) ; } }
public void test() { if ( state == null ) { log . debug ( "Skipping reset of partition {} since it is no longer assigned" , tp ) ; } else-if ( ! state . awaitingReset ( ) ) { log . debug ( "Resetting offset for partition {} to position {}." , tp , position ) ; } else-if ( requestedResetStrategy != state . resetStrategy ) { log . debug ( "Skipping reset of partition {} since an alternative reset has been requested" , tp ) ; } else { log . info ( "Resetting offset for partition {} to position {}." , tp , position ) ; state . seekUnvalidated ( position ) ; } }
public void test() { if ( state == null ) { log . debug ( "Skipping reset of partition {} since it is no longer assigned" , tp ) ; } else-if ( ! state . awaitingReset ( ) ) { log . debug ( "Skipping reset of partition {} since reset is no longer needed" , tp ) ; } else-if ( requestedResetStrategy != state . resetStrategy ) { log . debug ( "Resetting offset for partition {} to position {}." , tp , position ) ; } else { log . info ( "Resetting offset for partition {} to position {}." , tp , position ) ; state . seekUnvalidated ( position ) ; } }
public void test() { if ( state == null ) { log . debug ( "Skipping reset of partition {} since it is no longer assigned" , tp ) ; } else-if ( ! state . awaitingReset ( ) ) { log . debug ( "Skipping reset of partition {} since reset is no longer needed" , tp ) ; } else-if ( requestedResetStrategy != state . resetStrategy ) { log . debug ( "Skipping reset of partition {} since an alternative reset has been requested" , tp ) ; } else { state . seekUnvalidated ( position ) ; log . debug ( "Resetting reset of partition {} since reset has been requested" , tp ) ; } }
public void test() { try { stopSyncInternal ( syncPoint ) ; } catch ( InvalidPathException e ) { LOG . error ( "Failed to stop sync point {}" , syncPoint , e ) ; return ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { code_block = IfStatement ; String exceptionMessage = exception . getMessage ( ) != null ? exception . getMessage ( ) : "Exception occurred during function execution" ; code_block = IfStatement ; writeFunctionExceptionResponse ( msg , exceptionMessage , exception ) ; this . lastResultReceived = true ; } catch ( IOException ignoreAsSocketIsClosed ) { logger . trace ( "IGNORED" , ignoreAsSocketIsClosed ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { flatPushMessageInformationDao . update ( pushMessageInformation ) ; } catch ( Exception e ) { logger . error ( "Failed to update PushMessage" , e ) ; logger . debug ( "Details:" , e ) ; } }
public void test() { try { flatPushMessageInformationDao . update ( pushMessageInformation ) ; logger . info ( "Updated PushMessageInformation" ) ; } catch ( Exception e ) { logger . info ( "Failed to save pushMessageInformation: {}" , e . getMessage ( ) ) ; } }
public void test() { try { LOG . info ( "A dead datanode is detected. {}" , datanodeDetails ) ; destroyPipelines ( datanodeDetails ) ; closeContainers ( datanodeDetails , publisher ) ; code_block = IfStatement ; } catch ( NodeNotFoundException ex ) { LOG . warn ( ex . getMessage ( ) ) ; } }
public void test() { if ( exists == null ) { LOGGER . warn ( "Database does not exist." ) ; return - 1 ; } }
public void test() { if ( snapshotString == null || snapshotString . size ( ) == 0 ) { logger . info ( "No snapshot found in database" ) ; } else { code_block = ForStatement ; } }
public void test() { try { AnomalyNotifiedStatus status = OBJECT_MAPPER . readValue ( statusString , AnomalyNotifiedStatus . class ) ; snapshot . put ( entry . getKey ( ) , status ) ; } catch ( IOException e ) { LOG . error ( "Could not parse " + statusString , e ) ; } }
public void test() { try { ManagementFactory . getRuntimeMXBean ( ) . getInputArguments ( ) . stream ( ) . forEach ( s -> logger . debug ( "Parameter: {}" , s ) ) ; System . getProperties ( ) . entrySet ( ) . stream ( ) . forEach ( e -> logger . debug ( "Property: {}={}" , e . getKey ( ) , e . getValue ( ) ) ) ; System . getenv ( ) . entrySet ( ) . forEach ( e -> logger . debug ( "Env: {}={}" , e . getKey ( ) , e . getValue ( ) ) ) ; } catch ( final Exception e ) { ManagementFactory . getRuntimeMXBean ( ) . getInputArguments ( ) . stream ( ) . forEach ( e -> logger . debug ( "Error" , e ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "SuggestCreator is stopped." ) ; } else-if ( logger . isInfoEnabled ( ) ) { logger . info ( "SuggestCreator is stopped." ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "SuggestCreator is stopped." , e ) ; } else-if ( logger . isInfoEnabled ( ) ) { logger . info ( "SuggestCreator is stopped." ) ; } }
public void test() { try { SingletonLaContainerFactory . setConfigPath ( "app.xml" ) ; SingletonLaContainerFactory . setExternalContext ( new GenericExternalContext ( ) ) ; SingletonLaContainerFactory . setExternalContextComponentDefRegister ( new GenericExternalContextComponentDefRegister ( ) ) ; SingletonLaContainerFactory . init ( ) ; final Thread shutdownCallback = new Thread ( "ShutdownHook" ) code_block = "" ; ; Runtime . getRuntime ( ) . addShutdownHook ( shutdownCallback ) ; systemMonitorTask = TimeoutManager . getInstance ( ) . addTimeoutTarget ( new SystemMonitorTarget ( ) , ComponentUtil . getFessConfig ( ) . getSuggestSystemMonitorIntervalAsInteger ( ) , true ) ; exitCode = process ( options ) ; } catch ( final ContainerNotAvailableException e ) { code_block = IfStatement ; exitCode = Constants . EXIT_FAIL ; } catch ( final Throwable t ) { exitCode = Constants . EXIT_FAIL ; LOG . error ( "Container initialization error" , t ) ; } finally { code_block = IfStatement ; destroyContainer ( ) ; } }
private void cancelAllJobs ( ) { LOGGER . info ( "Cancel all jobs" ) ; JOBS . keySet ( ) . forEach ( this :: cancelJob ) ; }
@ Secured ( ServicesData . ROLE_GET_AGENCIES ) @ GetMapping ( path = RestApi . PATH_REFERENTIAL_ID ) public AgencyDto getOne ( final @ PathVariable ( "identifier" ) String identifier ) { LOGGER . debug ( "Get agency identifier={}" , identifier ) ; ParameterChecker . checkParameter ( "Identifier is mandatory : " , identifier ) ; return agencyExternalService . getOne ( identifier ) ; }
public void test() { try { st . push ( new FlowVariable ( Scope . Local . getPrefix ( ) + "(drop) " + f , child . toURI ( ) . toURL ( ) . toString ( ) , Scope . Local ) ) ; } catch ( MalformedURLException mue ) { LOGGER . error ( "Unable to create Flow variable" , mue ) ; } }
public void test() { try { checkNotNull ( "file" , file ) ; checkNotNull ( "attrs" , attrs ) ; code_block = IfStatement ; } catch ( final Exception e ) { logger . error ( "An unexpected exception was thrown: " , e ) ; return FileVisitResult . TERMINATE ; } }
public void test() { if ( systemMetadata . has ( "uri" ) ) { this . uri = systemMetadata . getString ( "uri" ) ; metadata . setUri ( this . uri ) ; } else { LOGGER . warn ( "systemMetadata has no system metadata" ) ; } }
public void test() { if ( systemMetadata . has ( "id" ) ) { this . id = systemMetadata . getInt ( "id" ) ; metadata . setId ( this . id ) ; } else { LOGGER . warn ( "System metadata has no id" ) ; } }
public void test() { if ( info == null ) { return true ; } }
private static void print ( String content ) { log . info ( content ) ; }
public void test() { try { AttachmentId attachmentId = AttachmentId . from ( attachment . getBlobId ( ) . getRawValue ( ) ) ; return OptionalUtils . executeIfEmpty ( Optional . ofNullable ( attachmentsById . get ( attachmentId ) ) . map ( attachmentMetadata -> MessageAttachmentMetadata . builder ( ) . attachment ( attachmentMetadata ) . name ( attachment . getName ( ) . orElse ( null ) ) . cid ( attachment . getCid ( ) . map ( Cid :: from ) . orElse ( null ) ) . isInline ( attachment . isIsInline ( ) ) . build ( ) ) , ( ) -> LOGGER . error ( String . format ( "Attachment %s not found" , attachment . getBlobId ( ) ) ) ) ; } catch ( IllegalStateException e ) { LOGGER . error ( e . getMessage ( ) , e ) ; return Optional . empty ( ) ; } }
private void updateBrokerState ( final BrokerState newState ) { brokerState = newState . getCode ( ) ; logger . debug ( "Broker state updated: {}" , newState ) ; heartbeat ( ) ; }
public void test() { if ( recoveryValue . getObjectName ( ) == null || recoveryValue . getObjectName ( ) . isEmpty ( ) ) { this . lowerBoundIdValue = selectLowerBoundValue ( sourceConnect ) ; log . debug ( "START object '{}' from = {}" , operationParams . tableName , recoveryValue ) ; } else-if ( recoveryValue . getProcessedObject ( ) != null && ! recoveryValue . getProcessedObject ( ) . isEmpty ( ) && isContain ( recoveryValue . getProcessedObject ( ) , currentTableName ) ) { log . debug ( "START object '{}' from = {}" , operationParams . tableName , recoveryValue ) ; return true ; } else { code_block = IfStatement ; } }
public void test() { if ( recoveryValue . getObjectName ( ) != null && recoveryValue . getObjectName ( ) . equalsIgnoreCase ( currentTableName ) && ! isContain ( recoveryValue . getProcessedObject ( ) , currentTableName ) ) { this . lowerBoundIdValue = recoveryValue . getLastColumnValue ( ) ; log . debug ( "GO NEXT object '{}' by = {}" , operationParams . tableName , recoveryValue ) ; } else { this . lowerBoundIdValue = selectLowerBoundValue ( sourceConnect ) ; log . debug ( "GO NEXT object '{}' by = {}" , operationParams . tableName , recoveryValue ) ; } }
public void test() { if ( recoveryValue . getObjectName ( ) != null && recoveryValue . getObjectName ( ) . equalsIgnoreCase ( currentTableName ) && ! isContain ( recoveryValue . getProcessedObject ( ) , currentTableName ) ) { this . lowerBoundIdValue = recoveryValue . getLastColumnValue ( ) ; log . debug ( "RESTORED object '{}' from = {}" , operationParams . tableName , recoveryValue ) ; } else { this . lowerBoundIdValue = selectLowerBoundValue ( sourceConnect ) ; log . debug ( "RESTORED object '{}' from = {}" , operationParams . tableName , sourceConnect ) ; } }
public void test() { try ( PreparedStatement query = prepareStatement ( "SELECT commits.id, commits.elasticId, commits.refid, commits.timestamp, committype.name, creator FROM commits JOIN committype ON commits.committype = committype.id WHERE elasticId = ?" ) ) { query . setString ( 1 , commitId ) ; code_block = TryStatement ;  } catch ( Exception e ) { logger . warn ( String . format ( "%s" , LogUtil . getStackTrace ( e ) ) ) ; } finally { close ( ) ; } }
public void test() { try { DeleteResult response = collection . deleteMany ( new BasicDBObject ( ) ) ; return response . getDeletedCount ( ) ; } catch ( MongoException e ) { LOGGER . error ( "Error while delete documents" , e ) ; throw new DatabaseException ( "Error while delete documents" , e ) ; } }
private String bratRenderLaterCommand ( ) { logger . debug ( "bratRenderLaterCommand() called" ) ; return "Wicket.$('" + vis . getMarkupId ( ) + "').dispatcher.post('current', " + "[" + toJson ( getCollection ( ) ) + ", '1234', {}, true]);" ; }
public void skip ( String containerId , Number taskId , String userId ) { containerId = context . getContainerId ( containerId , new ByTaskIdContainerLocator ( taskId . longValue ( ) ) ) ; userId = getUser ( userId ) ; logger . debug ( "Skipping task with id '{}' as user '{}'" , taskId , userId ) ; userTaskService . skip ( containerId , taskId . longValue ( ) , userId ) ; }
public void test() { try { newMessage = builder . buildQueryResponse ( results , ( String ) task . getParameters ( ) . get ( TaskRequestsConstants . P_QUERY_NUMBER ) , this . plugin . getName ( ) ) ; } catch ( IOException ex ) { LOG . error ( "Error while building query response" , ex ) ; } }
public void test() { try { String t = Text . decode ( s . getBytes ( ) , 0 , s . getBytes ( ) . length ) ; return t . getBytes ( ) ; } catch ( CharacterCodingException e ) { log . warn ( "Exception: " , e ) ; return null ; } }
public void test() { if ( null == topicConfig ) { LOG . error ( "Topic config is not valid " + topicName ) ; continue ; } }
@ Override public void progress ( float percent ) { synchronized ( this ) code_block = "" ; assertTrue ( percent >= 0 ) ; assertTrue ( percent <= 100 ) ; adaptee . progress ( percent ) ; LOGGER . info ( "progress: " + percent ) ; }
public void test() { try { File hookDir = Controller . HOOKS_DIR . resolve ( subdir ) . toFile ( ) ; code_block = IfStatement ; } catch ( IOException e ) { LOG . warn ( "Cannot load hook directory: " + subdir , e ) ; } }
public void test() { try { java . util . List < com . liferay . commerce . product . model . CPDefinitionLink > returnValue = CPDefinitionLinkServiceUtil . getCPDefinitionLinks ( cpDefinitionId ) ; return com . liferay . commerce . product . model . CPDefinitionLinkSoap . toSoapModels ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Refreshing." ) ; } }
public void test() { if ( log . isTraceEnabled ( ) && ! list . isEmpty ( ) ) { log . trace ( this . toString ( ) ) ; } }
public void test() { if ( glVCB != - 1 && tId != - 1 ) { int textureUnitId = JOGLUtils . getTextureUnitConst ( i ) ; gl . glClientActiveTexture ( textureUnitId ) ; gl . glActiveTexture ( textureUnitId ) ; gl . glEnable ( GL . GL_GL_2D ) ; gl . glEnableClientState ( GL . GL_VIDEO_COORD_ARRAY ) ; gl . glBindBufferARB ( GL . GL_ARRAY_BUFFER_ARB , glVCB ) ; gl . glTexCoordPointer ( 2 , GL . GL_FLOAT , 0 , 0 ) ; gl . glBindTexture ( GL . GL_VIDEO_2D , tId ) ; gl . glTexParameteri ( GL . GL_VIDEO_2D , GL . GL_SOURCE_WRAP_S , GL . GL_CLAMP_TO_EDGE ) ; gl . glTexParameteri ( GL . GL_VIDEO_2D , GL . GL_Texture_WRAP_T , GL . GL_CLAMP_TO_EDGE ) ; } else { LOGGER . info ( "Color is not available" ) ; } }
public void test() { if ( glVCB != - 1 && tId != - 1 ) { gl . glClientActiveTexture ( GL . GL_VIDEO0 ) ; gl . glActiveTexture ( GL . GL_TEXTURE0 ) ; gl . glEnable ( GL . GL_VERSION_2D ) ; gl . glEnableClientState ( GL . GL_SOURCE_COORD_ARRAY ) ; gl . glBindBufferARB ( GL . GL_ARRAY_BUFFER_ARB , glVCB ) ; gl . glTexCoordPointer ( 2 , GL . GL_FLOAT , 0 , 0 ) ; gl . glBindTexture ( GL . GL_VIDEO_2D , tId ) ; gl . glTexParameteri ( GL . GL_VIDEO_2D , GL . GL_Texture_WRAP_S , GL . GL_CLAMP_TO_EDGE ) ; gl . glTexParameteri ( GL . GL_VIDEO_2D , GL . GL_Texture_WRAP_T , GL . GL_CLAMP_TO_EDGE ) ; code_block = ForStatement ; shaderProgram . useProgram ( gl ) ; code_block = ForStatement ; } else { LOGGER . warn ( "Color is not available!" ) ; } }
public void test() { try { messageProcessor . writeHeader ( transactionContext ) ; messageProcessor . writeMessageEnd ( ) ; transport . flush ( ) ; String infoLog = "response[seqId:" + transactionContext . seqId ( ) + ", respCode:" + soaHeader . getRespCode ( ) . get ( ) + "]:" + "service[" + soaHeader . getServiceName ( ) + "]:version[" + soaHeader . getVersionName ( ) + "]:method[" + soaHeader . getMethodName ( ) + "]" + ( soaHeader . getOperatorId ( ) . isPresent ( ) ? " operatorId:" + soaHeader . getOperatorId ( ) . get ( ) : "" ) + ( soaHeader . getUserId ( ) . isPresent ( ) ? " userId:" + soaHeader . getUserId ( ) . get ( ) : "" ) ; LOGGER . info ( infoLog ) ; } catch ( Throwable e ) { LOGGER . error ( e . getMessage ( ) , e ) ; } finally { container . requestCounter ( ) . decrementAndGet ( ) ; MDC . remove ( SoaSystemEnvProperties . KEY_LOGGER_SESSION_TID ) ; } }
public void test() { try { messageProcessor . writeHeader ( transactionContext ) ; messageProcessor . writeMessageEnd ( ) ; transport . flush ( ) ; String infoLog = "response[seqId:" + transactionContext . seqId ( ) + ", respCode:" + soaHeader . getRespCode ( ) . get ( ) + "]:" + "service[" + soaHeader . getServiceName ( ) + "]:version[" + soaHeader . getVersionName ( ) + "]:method[" + soaHeader . getMethodName ( ) + "]" + ( soaHeader . getOperatorId ( ) . isPresent ( ) ? " operatorId:" + soaHeader . getOperatorId ( ) . get ( ) : "" ) + ( soaHeader . getUserId ( ) . isPresent ( ) ? " userId:" + soaHeader . getUserId ( ) . get ( ) : "" ) ; LOGGER . info ( getClass ( ) + " " + infoLog + ", payload:\n" + soaException . getMessage ( ) ) ; } catch ( Throwable e ) { LOGGER . error ( "Exception while writing transaction" , e ) ; } finally { container . requestCounter ( ) . decrementAndGet ( ) ; MDC . remove ( SoaSystemEnvProperties . KEY_LOGGER_SESSION_TID ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceCurrencyServiceUtil . class , "updateExchangeRate" , _updateExchangeRateParameterTypes12 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , commerceCurrencyId , exchangeRateProviderKey ) ; code_block = TryStatement ;  } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { code_block = IfStatement ; } catch ( ClassCastException e ) { log . debug ( "Encountered unexpected exception while casting values" , e ) ; rollbackWithException ( ActionStatus . INVALID_YAML ) ; } }
@ Test public void testSetupWorked ( ) throws IOException { LOG . info ( "Test: testSetupWorked" ) ; Path [ ] inputFiles = FileUtil . stat2Paths ( getFileSystem ( ) . listStatus ( input , new OutputLogFilter ( ) ) ) ; Assert . assertEquals ( testWarcs . length , inputFiles . length ) ; }
public void test() { try { org . apache . commons . io . FileUtils . deleteDirectory ( new File ( execLocalPath ) ) ; logger . info ( "exec local path: {} cleared." , execLocalPath ) ; } catch ( IOException e ) { logger . warn ( "Could not clear exec local path: {}" , execLocalPath , e ) ; } }
public void test() { if ( dv . getArchivalCopyLocation ( ) != null ) { logger . info ( "DatasetVersion id=" + ds . getGlobalId ( ) . toString ( ) + " v" + versionNumber + " submitted to Archive at: " + dv . getArchivalCopyLocation ( ) ) ; } else { logger . severe ( "Error submitting version due to conflict/error at Archive" ) ; } }
public void test() { { long start = System . currentTimeMillis ( ) ; code_block = ForStatement ; long end = System . currentTimeMillis ( ) ; long duration = end - start ; LOG . debug ( "duration: {}" , duration ) ; totalTime . addAndGet ( duration ) ; activeCount . decrementAndGet ( ) ; } }
public static void main ( String [ ] args ) throws InterruptedException { CouchbaseSampleEntryManager couchbaseSampleEntryManager = new CouchbaseSampleEntryManager ( ) ; final CouchbaseEntryManager couchbaseEntryManager = couchbaseSampleEntryManager . createCouchbaseEntryManager ( ) ; int countUsers = 1000000 ; int threadCount = 200 ; int threadIterationCount = 10 ; long totalStart = System . currentTimeMillis ( ) ; code_block = TryStatement ;  long totalEnd = System . currentTimeMillis ( ) ; long duration = totalEnd - totalStart ; logger . info ( "Total: " + duration ) ; System . out . println ( String . format ( "successResult: '%d', failedResult: '%d', errorResult: '%d'" , successResult . get ( ) , failedResult . get ( ) , errorResult . get ( ) ) ) ; }
public void test() { switch ( operation ) { case ASSIGN_CONTENT : assignContent ( in ) ; break ; case ASSIGN_EXPRESSION : assignExpression ( in ) ; break ; case EVAL : eval ( in ) ; break ; case VOID_EVAL : voidEval ( in ) ; break ; case PARSE_AND_EVAL : parseAndEval ( in ) ; break ; default : LOG . warn ( "Unknown operation: " + operation ) ; break ; } }
public void test() { if ( awaked ) { log . warn ( "File creation failed" ) ; return ; } }
public void test() { if ( log . isInfoEnabled ( ) ) { log . info ( msg ) ; } }
private void prepareServerNameListLength ( ClientEsniInner msg ) { msg . setServerNameListLength ( msg . getServerNameListBytes ( ) . getValue ( ) . length ) ; LOGGER . debug ( "ServerNameListLength: " + msg . getServerNameListLength ( ) . getValue ( ) ) ; }
public void test() { try { DistributedLockService service = DistributedLockService . getServiceNamed ( serviceName ) ; code_block = WhileStatement ; } catch ( VirtualMachineError e ) { SystemFailure . initiateFailure ( e ) ; throw e ; } catch ( Throwable t ) { logger . error ( "error in DistributedLockService" , t ) ; fail ( t . getMessage ( ) ) ; } finally { return lockCount . get ( ) ; } }
public void test() { if ( ! loggingOff ) { log . info ( message ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { long start = System . nanoTime ( ) ; writer . addDocument ( document ) ; log . debug ( "Added document [" + document . getDocumentId ( ) + "]" ) ; } else { writer . addDocument ( document ) ; } }
public void test() { try { Document document = getFinishedDocument ( doc ) ; code_block = IfStatement ; } catch ( IOException e ) { exceptions . add ( e ) ; LOG . warn ( "Failed to retrieve document: {}" , e . getMessage ( ) ) ; } finally { latch . countDown ( ) ; } }
public void test() { try { return execute ( ) ; } catch ( LdapException e ) { code_block = IfStatement ; String host = hosts [ currentConnectionIndex ] ; LOG . info ( "Failed to perform ldap host:" + host + "." ) ; performFailover ( ) ; host = hosts [ currentConnectionIndex ] ; LOG . info ( "Failover to ldap host:" + host + "." ) ; return execute ( ) ; } }
@ Override public void accept ( String propertyName , Object value ) { var stateCommand = typeConverter . toStateCommand ( value ) ; logger . debug ( "Updating channel state to {}: {}" , channel . getUID ( ) , stateCommand ) ; channelHandler . updateItemState ( channel . getUID ( ) , stateCommand ) ; }
public void test() { if ( _log . isInfoEnabled ( ) ) { _log . info ( StringBundler . concat ( "Removing " , companyId , " from company " , companyId , " to " , company . getCompanyId ( ) , " using " , company . getCompanyId ( ) ) ) ; } }
private boolean download ( final File jarFile ) throws IOException { log . info ( "downloading: " + jarFile ) ; final File tempFile = File . createTempFile ( "JFlex" , "zip" ) ; tempFile . deleteOnExit ( ) ; code_block = IfStatement ; copyIntoFileAndCloseStream ( new URL ( DOWNLOAD_URL ) . openStream ( ) , tempFile ) ; log . info ( "finished downloading. Now extracting to " + downloadTo ) ; final ZipFile zipFile = new ZipFile ( tempFile ) ; code_block = TryStatement ;  return true ; }
public boolean teamExists ( Integer teamId ) { boolean exists = getHibernateTemplate ( ) . get ( ProgramTeam . class , teamId ) != null ; LOG . debug ( "teamExists={}" , teamId ) ; return exists ; }
public void test() { try { previousLocalListing = WatcherCommon . initStorage ( currentLocalListingFile , config . localDir ) ; } catch ( IOException e ) { logger . error ( "Could not initialize storage directory '{}'" , currentLocalListingFile , e ) ; updateStatus ( ThingStatus . OFFLINE , ThingStatusDetail . CONFIGURATION_ERROR , e . getMessage ( ) ) ; return ; } }
public void test() { if ( i % 1000000 == 0 ) { log . info ( "Processed {} samples" , i ) ; } }
public void test() { try { code_block = IfStatement ; headerSplitInfoRef . set ( splitInfo ) ; } catch ( IllegalStateException e ) { error . set ( true ) ; LOG . error ( "Error getting split info" , e ) ; } }
public void test() { if ( getLogger ( ) . isDebugEnabled ( ) ) { getLogger ( ) . debug ( "Shutting down GeoServer" ) ; } }
private void recordAudit ( HttpServletRequest httpRequest , String whenISO9601 ) { final String who = getUserFromRequest ( httpRequest ) ; final String fromHost = httpRequest . getRemoteHost ( ) ; final String fromAddress = httpRequest . getRemoteAddr ( ) ; final String whatURL = Servlets . getRequestURL ( httpRequest ) ; final String whatAddrs = httpRequest . getLocalAddr ( ) ; GenericAlert . audit ( who , fromAddress , fromHost , whatURL , whatAddrs , whenISO9601 ) ; LOGGER . debug ( "Audit: {}" , who ) ; }
@ Path ( "/getMetadataShouldSucceed" ) @ POST public void getMetadataShouldSucceed ( ) { LOG . debug ( "" ) ; String uri = String . format ( URI_FORMAT , SwiftConstants . GET_METADATA ) ; Map < ? , ? > metadata = template . requestBodyAndHeader ( uri , null , SwiftConstants . CONTAINER_NAME , CONTAINER_NAME , Map . class ) ; assertNotNull ( metadata ) ; assertEquals ( "2000" , metadata . get ( NAME_YEAR ) ) ; assertEquals ( "TestBook" , metadata . get ( NAME_BOOK ) ) ; }
public void test() { try { path = new PartialPath ( pathString ) ; } catch ( IllegalPathException e ) { logger . warn ( "Illegal path " + pathString , e ) ; } }
public void test() { try { Mongo mongo = this . createNewMongo ( getProperties ( ) ) ; return mongo != null ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; return false ; } }
public void testGetFile ( ) throws Exception { ByteArrayContainerResponseWriter writer = new ByteArrayContainerResponseWriter ( ) ; String requestPath = SERVICE_URI + "item/" + fileId ; ContainerResponse response = launcher . service ( HttpMethod . GET , requestPath , BASE_URI , null , null , writer , null ) ; log . info ( new String ( writer . getBody ( ) ) ) ; assertEquals ( "Error: " + response . getEntity ( ) , 200 , response . getStatus ( ) ) ; Item item = ( Item ) response . getEntity ( ) ; assertEquals ( ItemType . FILE , item . getItemType ( ) ) ; assertEquals ( fileId , item . getId ( ) ) ; assertEquals ( filePath , item . getPath ( ) ) ; validateLinks ( item ) ; }
public void test() { if ( values . length == 2 ) { securityLogger . audit ( "Adding mapping: {} = {} to matchAllMap." , values [ 1 ] . trim ( ) , values [ 0 ] . trim ( ) ) ; matchAllMap . put ( values [ 1 ] . trim ( ) , values [ 0 ] . trim ( ) ) ; } else { LOGGER . info ( "Invalid mapping: {} = {}" , values [ 1 ] . trim ( ) , values [ 0 ] . trim ( ) ) ; } }
public void test() { { String targetPackageId = "target2" ; addTargetPackage ( targetPackageId ) ; ResourceIdentifier id = ResourceIdentifier . create ( ResourceType . ENTITY_TYPE , ENTITY_TYPE_A ) ; TestProgress progress = new TestProgress ( ) ; copyService . copy ( singletonList ( id ) , targetPackageId , progress ) ; await ( ) . atMost ( 5 , TimeUnit . SECONDS ) . until ( copyJobFinished ( progress ) ) ; waitForWorkToBeFinished ( indexService , LOG ) ; Package targetPackage = metadataService . getPackage ( targetPackageId ) . get ( ) ; List < Package > packages = newArrayList ( targetPackage . getChildren ( ) ) ; List < EntityType > entityTypes = newArrayList ( targetPackage . getEntityTypes ( ) ) ; assertEquals ( 0 , packages . size ( ) ) ; assertEquals ( 1 , entityTypes . size ( ) ) ; EntityType entityTypeACopy = entityTypes . get ( 0 ) ; assertEquals ( "EntityType A" , entityTypeACopy . getLabel ( ) ) ; assertEquals ( ENTITY_TYPE_B , entityTypeA . getAttribute ( "xref_attr" ) . getRefEntity ( ) . getId ( ) ) ; assertEquals ( ENTITY_TYPE_B , entityTypeACopy . getAttribute ( "xref_attr" ) . getRefEntity ( ) . getId ( ) ) ; assertEquals ( 1 , progress . getProgress ( ) ) ; List < Object > entitiesOfA = dataService . findAll ( entityTypeACopy . getId ( ) ) . map ( Entity :: getIdValue ) . collect ( toList ( ) ) ; assertEquals ( asList ( "0" , "1" , "2" ) , entitiesOfA ) ; cleanupTargetPackage ( targetPackageId ) ; LOG . info ( "Copy completed successfully." ) ; } }
public void test() { try { listener . destroyedFavorite ( status ) ; } catch ( Exception e ) { logger . warn ( "Exception at destroyFavorite" , e ) ; } }
@ Override public void processMessage ( final ObjectMessage message ) throws JMSException { LOGGER . debug ( "Processing microgrids response message" ) ; String correlationUid = null ; String messageType = null ; int messagePriority = MessagePriorityEnum . DEFAULT . getPriority ( ) ; String organisationIdentification = null ; String deviceIdentification = null ; ResponseMessage responseMessage ; ResponseMessageResultType responseMessageResultType = null ; OsgpException osgpException = null ; code_block = TryStatement ;  code_block = TryStatement ;  }
public void test() { try { correlationUid = message . getJMSCorrelationID ( ) ; messageType = message . getJMSType ( ) ; messagePriority = message . getJMSPriority ( ) ; organisationIdentification = message . getStringProperty ( Constants . ORGANISATION_IDENTIFICATION ) ; deviceIdentification = message . getStringProperty ( Constants . DEVICE_IDENTIFICATION ) ; responseMessage = ( ResponseMessage ) message . getObject ( ) ; responseMessageResultType = responseMessage . getResult ( ) ; osgpException = responseMessage . getOsgpException ( ) ; } catch ( final JMSException e ) { LOGGER . error ( "UNRECOVERABLE ERROR, unable to read ObjectMessage instance, giving up." , e ) ; LOGGER . debug ( "correlationUid: {}" , correlationUid ) ; LOGGER . debug ( "messageType: {}" , messageType ) ; LOGGER . debug ( "messagePriority: {}" , messagePriority ) ; LOGGER . debug ( "organisationIdentification: {}" , organisationIdentification ) ; LOGGER . debug ( "deviceIdentification: {}" , deviceIdentification ) ; LOGGER . debug ( "responseMessageResultType: {}" , responseMessageResultType ) ; LOGGER . debug ( "deviceIdentification: {}" , deviceIdentification ) ; LOGGER . debug ( "osgpException" , osgpException ) ; return ; } }
public void test() { try { correlationUid = message . getJMSCorrelationID ( ) ; messageType = message . getJMSType ( ) ; messagePriority = message . getJMSPriority ( ) ; organisationIdentification = message . getStringProperty ( Constants . ORGANISATION_IDENTIFICATION ) ; deviceIdentification = message . getStringProperty ( Constants . DEVICE_IDENTIFICATION ) ; responseMessage = ( ResponseMessage ) message . getObject ( ) ; responseMessageResultType = responseMessage . getResult ( ) ; osgpException = responseMessage . getOsgpException ( ) ; } catch ( final JMSException e ) { LOGGER . error ( "UNRECOVERABLE ERROR, unable to read ObjectMessage instance, giving up." , e ) ; LOGGER . debug ( "correlationUid: {}" , correlationUid ) ; LOGGER . debug ( "messageType: {}" , messageType ) ; LOGGER . debug ( "messagePriority: {}" , messagePriority ) ; LOGGER . debug ( "organisationIdentification: {}" , organisationIdentification ) ; LOGGER . debug ( "deviceIdentification: {}" , deviceIdentification ) ; LOGGER . debug ( "responseMessageResultType: {}" , responseMessageResultType ) ; LOGGER . debug ( "deviceIdentification: {}" , deviceIdentification ) ; LOGGER . debug ( "osgpException" , osgpException ) ; return ; } }
public void test() { try { correlationUid = message . getJMSCorrelationID ( ) ; messageType = message . getJMSType ( ) ; messagePriority = message . getJMSPriority ( ) ; organisationIdentification = message . getStringProperty ( Constants . ORGANISATION_IDENTIFICATION ) ; deviceIdentification = message . getStringProperty ( Constants . DEVICE_IDENTIFICATION ) ; responseMessage = ( ResponseMessage ) message . getObject ( ) ; responseMessageResultType = responseMessage . getResult ( ) ; osgpException = responseMessage . getOsgpException ( ) ; } catch ( final JMSException e ) { LOGGER . error ( "UNRECOVERABLE ERROR, unable to read ObjectMessage instance, giving up." , e ) ; LOGGER . debug ( "correlationUid: {}" , correlationUid ) ; LOGGER . debug ( "messageType: {}" , messageType ) ; LOGGER . debug ( "messagePriority: {}" , messagePriority ) ; LOGGER . debug ( "organisationIdentification: {}" , organisationIdentification ) ; LOGGER . debug ( "deviceIdentification: {}" , deviceIdentification ) ; LOGGER . debug ( "responseMessageResultType: {}" , responseMessageResultType ) ; LOGGER . debug ( "deviceIdentification: {}" , deviceIdentification ) ; LOGGER . debug ( "osgpException" , osgpException ) ; return ; } }
public void test() { try { correlationUid = message . getJMSCorrelationID ( ) ; messageType = message . getJMSType ( ) ; messagePriority = message . getJMSPriority ( ) ; organisationIdentification = message . getStringProperty ( Constants . ORGANISATION_IDENTIFICATION ) ; deviceIdentification = message . getStringProperty ( Constants . DEVICE_IDENTIFICATION ) ; responseMessage = ( ResponseMessage ) message . getObject ( ) ; responseMessageResultType = responseMessage . getResult ( ) ; osgpException = responseMessage . getOsgpException ( ) ; } catch ( final JMSException e ) { LOGGER . error ( "UNRECOVERABLE ERROR, unable to read ObjectMessage instance, giving up." , e ) ; LOGGER . debug ( "correlationUid: {}" , correlationUid ) ; LOGGER . debug ( "messageType: {}" , messageType ) ; LOGGER . debug ( "messagePriority: {}" , messagePriority ) ; LOGGER . debug ( "organisationIdentification: {}" , organisationIdentification ) ; LOGGER . debug ( "deviceIdentification: {}" , deviceIdentification ) ; LOGGER . debug ( "responseMessageResultType: {}" , responseMessageResultType ) ; LOGGER . debug ( "deviceIdentification: {}" , deviceIdentification ) ; LOGGER . debug ( "osgpException" , osgpException ) ; return ; } }
public void test() { try { correlationUid = message . getJMSCorrelationID ( ) ; messageType = message . getJMSType ( ) ; messagePriority = message . getJMSPriority ( ) ; organisationIdentification = message . getStringProperty ( Constants . ORGANISATION_IDENTIFICATION ) ; deviceIdentification = message . getStringProperty ( Constants . DEVICE_IDENTIFICATION ) ; responseMessage = ( ResponseMessage ) message . getObject ( ) ; responseMessageResultType = responseMessage . getResult ( ) ; osgpException = responseMessage . getOsgpException ( ) ; } catch ( final JMSException e ) { LOGGER . error ( "UNRECOVERABLE ERROR, unable to read ObjectMessage instance, giving up." , e ) ; LOGGER . debug ( "correlationUid: {}" , correlationUid ) ; LOGGER . debug ( "messageType: {}" , messageType ) ; LOGGER . debug ( "messagePriority: {}" , messagePriority ) ; LOGGER . debug ( "organisationIdentification: {}" , organisationIdentification ) ; LOGGER . debug ( "deviceIdentification: {}" , deviceIdentification ) ; LOGGER . debug ( "responseMessageResultType: {}" , responseMessageResultType ) ; LOGGER . debug ( "deviceIdentification: {}" , deviceIdentification ) ; LOGGER . debug ( "osgpException" , osgpException ) ; return ; } }
public void test() { try { correlationUid = message . getJMSCorrelationID ( ) ; messageType = message . getJMSType ( ) ; messagePriority = message . getJMSPriority ( ) ; organisationIdentification = message . getStringProperty ( Constants . ORGANISATION_IDENTIFICATION ) ; deviceIdentification = message . getStringProperty ( Constants . DEVICE_IDENTIFICATION ) ; responseMessage = ( ResponseMessage ) message . getObject ( ) ; responseMessageResultType = responseMessage . getResult ( ) ; osgpException = responseMessage . getOsgpException ( ) ; } catch ( final JMSException e ) { LOGGER . error ( "UNRECOVERABLE ERROR, unable to read ObjectMessage instance, giving up." , e ) ; LOGGER . debug ( "correlationUid: {}" , correlationUid ) ; LOGGER . debug ( "messageType: {}" , messageType ) ; LOGGER . debug ( "messagePriority: {}" , messagePriority ) ; LOGGER . debug ( "organisationIdentification: {}" , organisationIdentification ) ; LOGGER . debug ( "deviceIdentification: {}" , deviceIdentification ) ; LOGGER . debug ( "responseMessageResultType: {}" , responseMessageResultType ) ; LOGGER . debug ( "deviceIdentification: {}" , deviceIdentification ) ; LOGGER . debug ( "osgpException" , osgpException ) ; return ; } }
public void test() { try { correlationUid = message . getJMSCorrelationID ( ) ; messageType = message . getJMSType ( ) ; messagePriority = message . getJMSPriority ( ) ; organisationIdentification = message . getStringProperty ( Constants . ORGANISATION_IDENTIFICATION ) ; deviceIdentification = message . getStringProperty ( Constants . DEVICE_IDENTIFICATION ) ; responseMessage = ( ResponseMessage ) message . getObject ( ) ; responseMessageResultType = responseMessage . getResult ( ) ; osgpException = responseMessage . getOsgpException ( ) ; } catch ( final JMSException e ) { LOGGER . error ( "UNRECOVERABLE ERROR, unable to read ObjectMessage instance, giving up." , e ) ; LOGGER . debug ( "correlationUid: {}" , correlationUid ) ; LOGGER . debug ( "messageType: {}" , messageType ) ; LOGGER . debug ( "messagePriority: {}" , messagePriority ) ; LOGGER . debug ( "organisationIdentification: {}" , organisationIdentification ) ; LOGGER . debug ( "deviceIdentification: {}" , deviceIdentification ) ; LOGGER . debug ( "responseMessageResultType: {}" , responseMessageResultType ) ; LOGGER . debug ( "deviceIdentification: {}" , deviceIdentification ) ; LOGGER . debug ( "osgpException" , osgpException ) ; return ; } }
public SysExportNotiz findById ( sernet . gs . reveng . SysExportNotizId id ) { log . debug ( "getting SysExportNotiz instance with id: " + id ) ; code_block = TryStatement ;  }
public void test() { if ( instance == null ) { log . debug ( "get successful, no instance found" ) ; } else { log . debug ( "get successful, instance found" ) ; } }
public void test() { if ( instance == null ) { log . debug ( "get successful, no instance found" ) ; } else { log . debug ( "get successful, instance found" ) ; } }
public void test() { try { SysExportNotiz instance = ( SysExportNotiz ) sessionFactory . getCurrentSession ( ) . get ( "sernet.gs.reveng.SysExportNotiz" , id ) ; code_block = IfStatement ; return instance ; } catch ( RuntimeException re ) { log . error ( "get failed" , re ) ; throw re ; } }
public void test() { -> { log . error ( "Unknown error" , t ) ; System . exit ( 1 ) ; } }
public void test() { if ( SRV_LOG . isDebugEnabled ( ) ) { SRV_LOG . debug ( "tcp check, ip:" + ip + " to " + ip ) ; } }
public void test() { if ( ! ip . markChecking ( ) ) { SRV_LOG . warn ( "[TaskTracker-{}] check started before last finished, service: {}" , task . getCluster ( ) . getName ( ) , task . getCluster ( ) . getName ( ) ) ; healthCheckCommon . reEvaluateCheckRT ( task . getCheckRtNormalized ( ) * 2 , task , switchDomain . getMysqlHealthParams ( ) ) ; continue ; } }
public void agentLost ( Protos . AgentID agentId ) { log . info ( "notifying listener {}" , agentId ) ; agentAndRackManager . agentLost ( agentId ) ; }
public void test() { if ( channel == null ) { logger . warn ( "handleCommand(): unsupported channel type {}" , channelUID ) ; return null ; } }
public void test() { if ( channelTypeUID == null ) { logger . debug ( "Channel type is null." ) ; return null ; } }
public void test() { if ( channelTypeId == null ) { logger . debug ( "ChannelTypeId is null" ) ; return null ; } }
public void test() { try { String response = HttpJsonHelper . requestString ( apiEndPoint + "/internal/sso/server/" + token , "GET" , null , Pair . of ( "clienturl" , URLEncoder . encode ( apiEndPoint , "UTF-8" ) ) ) ; JsonValue value = JsonHelper . parseJson ( response ) ; return new UserImpl ( value . getElement ( "name" ) . getStringValue ( ) , value . getElement ( "id" ) . getStringValue ( ) , value . getElement ( "token" ) . getStringValue ( ) , Collections . < String > emptySet ( ) , value . getElement ( "temporary" ) . getBooleanValue ( ) ) ; } catch ( ForbiddenException | UnauthorizedException | ServerException un ) { logger . warn ( un ) ; return null ; } catch ( ConflictException | NotFoundException | IOException | JsonParseException e ) { throw new ServletException ( e . getMessage ( ) , e ) ; } }
public void test() { if ( loginConfig != null ) { loginConfig . setAuthMethod ( "KEYCLOAK-SAML" ) ; } else { logger . warn ( "WARNING: Unable to retrieve login configuration." ) ; } }
@ ApiOperation ( value = "get history by profile's id" ) @ GetMapping ( CommonConstants . PATH_LOGBOOK ) public LogbookOperationsResponseDto findHistoryById ( final @ PathVariable String id ) { LOGGER . debug ( "get logbook for profile with id :{}" , id ) ; ParameterChecker . checkParameter ( "The Identifier is a mandatory parameter: " , id ) ; return service . findHistoryById ( buildUiHttpContext ( ) , id ) ; }
protected void deleteTestTable ( String tableName ) { LOG . debug ( "Removing table " + tableName ) ; KuduClient client = ikc . getClient ( ) ; code_block = TryStatement ;  LOG . trace ( "Table " + tableName + " removed." ) ; }
public void test() { try { client . deleteTable ( tableName ) ; } catch ( Exception e ) { LOG . warn ( "Unable to delete table {}" , tableName , e ) ; } }
@ Override public Set < String > queryNames ( ) { long start = System . currentTimeMillis ( ) ; Set < String > names = withAllReams ( realm code_block = LoopStatement ; ) . collect ( Collectors . toSet ( ) ) ; LOG . debug ( "Query names: {}" , names . size ( ) ) ; return names ; }
private void createOrUpdateIndexerSearchEnties ( SearchCacheEntry searchCacheEntry ) { Stopwatch stopwatch = Stopwatch . createStarted ( ) ; int countEntities = 0 ; code_block = ForStatement ; log . info ( "Indexer search entities created in {} ms" , stopwatch . elapsed ( TimeUnit . MILLISECONDS ) ) ; }
public void test() { try { final Collection < ValidationResult > instanceResults = instance . validate ( context ) ; code_block = IfStatement ; } catch ( final Exception e ) { final ComponentLog logger = getLogger ( ) ; final String message = "Unable to validate the script Processor: " + e ; logger . error ( message ) ; final Collection < ValidationResult > results = new HashSet < > ( ) ; results . add ( new ValidationResult . Builder ( ) . subject ( "Validation" ) . valid ( false ) . explanation ( "An error occurred calling validate in the configured script Processor." ) . input ( context . getProperty ( ScriptingComponentUtils . SCRIPT_FILE ) . getValue ( ) ) . build ( ) ) ; return results ; } }
public void test() { while ( budget . timeLeft ( ) >= 0 && ! isUserActive ( foundUser ) ) { log . info ( "Waiting for user {}" , name ) ; Thread . sleep ( 1000 ) ; foundUser = client . withName ( name ) . get ( ) ; } }
public void test() { try { localhost = InetAddress . getLocalHost ( ) . getCanonicalHostName ( ) ; } catch ( UnknownHostException e ) { LOGGER . warn ( "Unable to get canonical host name, using defaults" , e ) ; localhost = LOCALHOST ; } }
public void test() { if ( LOGGER . isTraceEnabled ( ) ) { LOGGER . trace ( "Weak listener list status:{}" , System . lineSeparator ( ) ) ; } }
public void test() { try { partitionManager . initializeResultPartitionReader ( jobId , rsId , partition , noc ) ; } catch ( HyracksException e ) { LOGGER . error ( "Initializing PartitionWriteInterface" , e ) ; noc . abort ( AbstractChannelWriteInterface . REMOTE_ERROR_CODE ) ; } }
@ Override String createOid ( String ... keys ) { String oid = "IG_" ; String crfName = keys [ 0 ] ; String itemGroupLabel = keys [ 1 ] ; logger . debug ( crfName ) ; logger . debug ( itemGroupLabel ) ; crfName = truncateToXChars ( capitalize ( stripNonAlphaNumeric ( crfName ) ) , 5 ) ; itemGroupLabel = truncateToXChars ( capitalize ( stripNonAlphaNumeric ( itemGroupLabel ) ) , 26 ) ; oid = oid + crfName + "_" + itemGroupLabel ; code_block = IfStatement ; logger . debug ( "OID : " + oid ) ; return oid ; }
@ Override String createOid ( String ... keys ) { String oid = "IG_" ; String crfName = keys [ 0 ] ; String itemGroupLabel = keys [ 1 ] ; logger . debug ( crfName ) ; crfName = truncateToXChars ( capitalize ( stripNonAlphaNumeric ( crfName ) ) , 5 ) ; itemGroupLabel = truncateToXChars ( capitalize ( stripNonAlphaNumeric ( itemGroupLabel ) ) , 26 ) ; oid = oid + crfName + "_" + itemGroupLabel ; logger . debug ( "ItemGroupLabel : " + itemGroupLabel ) ; code_block = IfStatement ; logger . debug ( "OID : " + oid ) ; return oid ; }
@ Override String createOid ( String ... keys ) { String oid = "IG_" ; String crfName = keys [ 0 ] ; String itemGroupLabel = keys [ 1 ] ; logger . debug ( crfName ) ; logger . debug ( itemGroupLabel ) ; crfName = truncateToXChars ( capitalize ( stripNonAlphaNumeric ( crfName ) ) , 5 ) ; itemGroupLabel = truncateToXChars ( capitalize ( stripNonAlphaNumeric ( itemGroupLabel ) ) , 26 ) ; oid = oid + crfName + "_" + itemGroupLabel ; code_block = IfStatement ; logger . debug ( "OID : " + oid ) ; return oid ; }
public void test() { if ( timeUnit == null || ! SUPPORTED_TIME_UNITS . contains ( timeUnit ) ) { getLogger ( ) . warn ( "The time unit '" + timeUnit + "' is not supported in " + timeUnit ) ; return false ; } }
public void test() { try { configDescriptionValidator . validate ( config , new URI ( CONFIG_DESC_URI_KEY ) ) ; } catch ( URISyntaxException | ConfigValidationException e ) { LOGGER . error ( "Exception while validating URI." , e ) ; return false ; } }
public void test() { for ( String h : headers . getRequestHeader ( key ) ) { logger . info ( "{}" , h ) ; } }
public void test() { for ( Commit commit : list ) { log . debug ( commit . toString ( ) ) ; } }
private void writeSessionIdHit ( SSL2ServerHelloMessage message ) { appendByte ( message . getSessionIdHit ( ) . getValue ( ) ) ; LOGGER . debug ( "SessionId hit: " + message . getSessionIdHit ( ) . getValue ( ) ) ; }
public void test() { try { EntityParam recipient = new EntityParam ( entityId ) ; String entityName = getEntityLabel ( recipient ) ; Map < String , String > params = new HashMap < > ( ) ; params . put ( UserNotificationTemplateDef . USER , entityName == null ? "" : entityName ) ; notificationProducer . sendNotification ( recipient , templateId , params , cfg . getDefaultLocale ( ) . toString ( ) , null , false ) ; } catch ( Exception e ) { LOG . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { list = this . getIdeaDAO ( ) . searchIdea ( instanceCode , status , text , category , order ) ; } catch ( Throwable t ) { _logger . error ( "Error in searchIdeas" , t ) ; throw new ApsSystemException ( "Error in searchIdeas" , t ) ; } }
public void test() { -> { LOGGER . info ( "Loaded {} results" , result . size ( ) ) ; } }
public void test() { -> { log . warn ( "Unable to find annotated type {}" , annotatedType ) ; return null ; } }
public void test() { try { EndpointBuilder instance ; code_block = IfStatement ; return Optional . of ( instance ) ; } catch ( CitrusRuntimeException e ) { log . warn ( String . format ( "Failed to parse endpoint builder instance %s" , instance ) , e ) ; } }
public void test() { try { String canonicalURL = _portal . getCanonicalURL ( _getCompleteURL ( themeDisplay ) , themeDisplay , layout , false , false ) ; LayoutSEOLink layoutSEOLink = _layoutSEOLinkManager . getCanonicalLayoutSEOLink ( layout , locale , canonicalURL , _portal . getAlternateURLs ( canonicalURL , themeDisplay , layout ) ) ; return layoutSEOLink . getHref ( ) ; } catch ( PortalException portalException ) { _log . error ( portalException , portalException ) ; return StringPool . BLANK ; } }
public void test() { try { String authString = clientId . concat ( ":" ) . concat ( oauth2ClientSecret ) ; byte [ ] authEncBytes = Base64 . encodeBase64 ( authString . getBytes ( ) ) ; clientCredentials = "Basic " + new String ( authEncBytes ) ; } catch ( Exception exception ) { _log . error ( "Client Validation Failed!!!" , exception ) ; return response ( false , "Client Validation Failed!!!" ) ; } }
public void test() { try { String url = System . getenv ( DOMAIN_URL ) + "/oauth/token" ; Map < String , String > headers = Maps . newHashMap ( ) ; headers . put ( HttpHeaders . CONTENT_TYPE , MediaType . APPLICATION_FORM_URLENCODED_VALUE ) ; String clientCredentials = null ; code_block = TryStatement ;  headers . put ( HttpHeaders . AUTHORIZATION , clientCredentials ) ; String accessToken = doHttpPost ( url , requestBodyUrl , headers ) ; accessTokenDetails = mapper . readValue ( accessToken , new TypeReference < HashMap < String , Object > > ( ) code_block = "" ; ) ; code_block = IfStatement ; } catch ( Exception exception ) { log . error ( "Unexpected Error Occurred:" , exception ) ; return response ( false , "Unexpected Error Occured!!!" ) ; } }
public void test() { if ( offHeapQueue == null ) { int remaining = inMemoryQueue . remainingCapacity ( ) ; LOG . debug ( "offheapQueue is null, returning {}" , remaining ) ; return remaining <= 0 ; } }
public void test() { try { Thread . sleep ( 1000 ) ; } catch ( InterruptedException e ) { logger . error ( "" , e ) ; } }
private void waitUntilServerIsInitialized ( ) { int i = 0 ; code_block = ForStatement ; logger . info ( "Server is initialized." ) ; shutdown = true ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( sipServletMapping . getMatchingRule ( ) . matches ( sipServletRequest ) ) { return sipServletMapping ; } else { logger . error ( "Could not find sip servlet mapping: " + sipServletMapping ) ; } }
@ Override public ConnectionTestResult runTests ( final List < TestType > testTypes ) throws JargonException { log . info ( "runTests()" ) ; code_block = IfStatement ; ConnectionTestResult testResult = new ConnectionTestResult ( ) ; testResult . setConfiguration ( connectionTesterConfiguration ) ; testResult . setIrodsAccount ( getIrodsAccount ( ) ) ; code_block = ForStatement ; return testResult ; }
public void test() { try { UserManager . getUserProvider ( ) . createUser ( username , StringUtils . randomString ( 8 ) , null , null ) ; } catch ( UserAlreadyExistsException uaee ) { LOGGER . info ( "User already exists!" , uaee ) ; } }
public void test() { try { final List vStatusList = appConfigValuesService . getConfigValuesByModuleAndKey ( "EGF" , "APPROVEDVOUCHERSTATUS" ) ; code_block = IfStatement ; else throw new ApplicationRuntimeException ( "APPROVEDVOUCHERSTATUS" + MISSINGMSG ) ; createVoucher . createVoucherFromPreApprovedVoucher ( vouhcerheaderid , voucherStatus ) ; } catch ( final ApplicationRuntimeException e ) { LOGGER . error ( e ) ; throw new ApplicationRuntimeException ( e . getMessage ( ) ) ; } }
public void test() { try { serverSocket = new ServerSocket ( seyrenConfig . getGraphiteCarbonPicklePort ( ) ) ; serverSocket . setReuseAddress ( true ) ; serverSocket . setReceiveBufferSize ( 1024 * 1024 ) ; code_block = WhileStatement ; } catch ( IOException e ) { log . error ( "IO Exception thrown: " + e . getMessage ( ) ) ; } finally { code_block = IfStatement ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Refreshing." ) ; } }
public void test() { try { quantumClock . stopDeamon ( ) ; registryUpdateDeamon . stopDeamon ( ) ; dataTransferManagerDTP . stopDataTransferServer ( ) ; ManagementUtil . unregisterMBean ( "Container" ) ; super . unregister ( ) ; executor . terminate ( ) ; log . info ( "Stopped" ) ; } catch ( RemoteException e ) { log . debug ( "Stop Container" , e ) ; throw new ServerException ( "Cannot stop container" , e ) ; } }
public void test() { try { quantumClock . stopDeamon ( ) ; registryUpdateDeamon . stopDeamon ( ) ; dataTransferManagerDTP . stopDataTransferServer ( ) ; log . info ( "Stopping container..." ) ; ManagementUtil . unregisterMBean ( "Container" ) ; super . unregister ( ) ; log . info ( "Container succesfully stopped!" ) ; executor . terminate ( ) ; } catch ( RemoteException e ) { throw new ServerException ( "Cannot stop container" , e ) ; } }
public void test() { try { final CliFrontend cli = new CliFrontend ( configuration , customCommandLines ) ; SecurityUtils . install ( new SecurityConfiguration ( cli . configuration ) ) ; retCode = SecurityUtils . getInstalledContext ( ) . runSecured ( ( ) -> cli . parseAndRun ( args ) ) ; } catch ( Throwable t ) { final Throwable strippedThrowable = ExceptionUtils . stripException ( t , UndeclaredThrowableException . class ) ; strippedThrowable . printStackTrace ( ) ; logger . warn ( "Unable to run '" + cli . getName ( ) + "'" , strippedThrowable ) ; } finally { System . exit ( retCode ) ; } }
public void test() { try { CLIENT . getService ( SyncopeService . class ) ; LOG . info ( getName ( ) + " completed successfully!" ) ; } catch ( Exception e ) { LOG . error ( getName ( ) + " failed." , e ) ; } }
public void test() { try { propValue = getPropAccessor ( ) . getProperty ( NhincConstants . GATEWAY_PROPERTY_FILE , propertyName ) ; } catch ( PropertyAccessException ex ) { LOG . error ( "Failed to access properties file {}: {}" , NhincConstants . GATEWAY_PROPERTY_FILE , ex . getLocalizedMessage ( ) , ex ) ; } }
public void test() { if ( eofSenderFuture == null ) { log . info ( "CFDPDP received EOF ACK" ) ; } else { log . info ( "CFDPDP received EOF ACK" ) ; eofSenderFuture . cancel ( true ) ; } }
public void test() { if ( eofSenderFuture == null ) { log . error ( "EOF ACK received but EOF not sent" ) ; } else { log . debug ( "Stopping EOF sender" ) ; eofSenderFuture . cancel ( true ) ; } }
public void test() { try { String [ ] resultFieldsArray = resultFields . getItem ( ) . toArray ( EMPTY_STRING_ARRAY ) ; org . fcrepo . server . search . FieldSearchResult result = m_access . findObjects ( context , resultFieldsArray , maxResults . intValue ( ) , TypeUtility . convertGenFieldSearchQueryToFieldSearchQuery ( query ) ) ; return TypeUtility . convertFieldSearchResultToGenFieldSearchResult ( result ) ; } catch ( Throwable th ) { LOG . error ( "Error finding objects" , th ) ; throw CXFUtility . getFault ( th ) ; } }
public void test() { try { onFinish ( ) ; } catch ( final Exception e ) { logger . warn ( "Failed to perform tasks when enumerator was finished" , e ) ; } }
public void test() { if ( encoding == null ) { LOGGER . debug ( "Setting encoding: {}" , encoding ) ; encoding = Charset . defaultCharset ( ) . name ( ) ; configuration . setEncoding ( encoding ) ; } }
public void test() { if ( nextToProcess == - 1 ) { LOG . debug ( "{}: got seq={} in {}" , requests . getName ( ) , seqNum , this ) ; nextToProcess = seqNum ; } else { LOG . debug ( "{}: got seq={} in {}" , requests . getName ( ) , seqNum , this ) ; } }
public void test() { if ( nextToProcess == - 1 ) { nextToProcess = seqNum ; LOG . debug ( "{}: got seq={} (first request), set nextToProcess in {}" , requests . getName ( ) , seqNum , this ) ; } else { LOG . debug ( "{}: got seq={}, nextToProcess in {}" , requests . getName ( ) , seqNum , this ) ; } }
public void test() { try { WatchdogServiceModVerIdLookup lookup = watchdogModVerIdLookupRepository . findOneByDistributionId ( distributionId ) ; String serviceModelVersionId = "" ; code_block = IfStatement ; logger . debug ( "ASDC Notification ServiceModelInvariantUUID : {}" , serviceModelInvariantUUID ) ; logger . debug ( "ASDC Notification ServiceModelVersionId : {}" , serviceModelVersionId ) ; code_block = IfStatement ; AAIResourceUri aaiUri = AAIUriFactory . createResourceUri ( AAIFluentTypeBuilder . serviceDesignAndCreation ( ) . model ( serviceModelInvariantUUID ) . modelVer ( serviceModelVersionId ) ) ; aaiUri . depth ( Depth . ZERO ) ; logger . debug ( "Target A&AI Resource URI: {}" , aaiUri . build ( ) . toString ( ) ) ; Map < String , String > payload = new HashMap < > ( ) ; payload . put ( "distribution-status" , distributionStatus ) ; getAaiClient ( ) . update ( aaiUri , payload ) ; logger . debug ( "A&AI UPDATE MODEL Version is success!" ) ; } catch ( Exception e ) { logger . debug ( "Exception occurred on executePatchAAI : {}" , e . getMessage ( ) ) ; logger . error ( "Exception occurred" , e ) ; throw new Exception ( e ) ; } }
public void test() { try { WatchdogServiceModVerIdLookup lookup = watchdogModVerIdLookupRepository . findOneByDistributionId ( distributionId ) ; String serviceModelVersionId = "" ; code_block = IfStatement ; logger . debug ( "Executed RequestDB getWatchdogServiceModVerIdLookup with distributionId: {} " + "and serviceModelVersionId: {}" , distributionId , serviceModelVersionId ) ; code_block = IfStatement ; AAIResourceUri aaiUri = AAIUriFactory . createResourceUri ( AAIFluentTypeBuilder . serviceDesignAndCreation ( ) . model ( serviceModelInvariantUUID ) . modelVer ( serviceModelVersionId ) ) ; aaiUri . depth ( Depth . ZERO ) ; logger . debug ( "Target A&AI Resource URI: {}" , aaiUri . build ( ) . toString ( ) ) ; Map < String , String > payload = new HashMap < > ( ) ; payload . put ( "distribution-status" , distributionStatus ) ; getAaiClient ( ) . update ( aaiUri , payload ) ; logger . debug ( "A&AI UPDATE MODEL Version is success!" ) ; } catch ( Exception e ) { logger . debug ( "Exception occurred on executePatchAAI : {}" , e . getMessage ( ) ) ; logger . error ( "Exception occurred" , e ) ; throw new Exception ( e ) ; } }
public void test() { if ( serviceModelInvariantUUID == null || "" . equals ( serviceModelVersionId ) ) { String error = "No Service found with serviceModelInvariantUUID: " + serviceModelInvariantUUID ; logger . error ( error ) ; throw new Exception ( error ) ; } }
public void test() { try { WatchdogServiceModVerIdLookup lookup = watchdogModVerIdLookupRepository . findOneByDistributionId ( distributionId ) ; String serviceModelVersionId = "" ; code_block = IfStatement ; logger . debug ( "Executed RequestDB getWatchdogServiceModVerIdLookup with distributionId: {} " + "and serviceModelVersionId: {}" , distributionId , serviceModelVersionId ) ; logger . debug ( "ASDC Notification ServiceModelInvariantUUID : {}" , serviceModelInvariantUUID ) ; code_block = IfStatement ; AAIResourceUri aaiUri = AAIUriFactory . createResourceUri ( AAIFluentTypeBuilder . serviceDesignAndCreation ( ) . model ( serviceModelInvariantUUID ) . modelVer ( serviceModelVersionId ) ) ; aaiUri . depth ( Depth . ZERO ) ; Map < String , String > payload = new HashMap < > ( ) ; payload . put ( "distribution-status" , distributionStatus ) ; logger . debug ( "A&AI UPDATE MODEL Version is success!" ) ; getAaiClient ( ) . update ( aaiUri , payload ) ; logger . debug ( "A&AI UPDATE MODEL Version is success!" ) ; } catch ( Exception e ) { logger . debug ( "Exception occurred on executePatchAAI : {}" , e . getMessage ( ) ) ; logger . error ( "Exception occurred" , e ) ; throw new Exception ( e ) ; } }
public void test() { try { WatchdogServiceModVerIdLookup lookup = watchdogModVerIdLookupRepository . findOneByDistributionId ( distributionId ) ; String serviceModelVersionId = "" ; code_block = IfStatement ; logger . debug ( "Executed RequestDB getWatchdogServiceModVerIdLookup with distributionId: {} " + "and serviceModelVersionId: {}" , distributionId , serviceModelVersionId ) ; logger . debug ( "ASDC Notification ServiceModelInvariantUUID : {}" , serviceModelInvariantUUID ) ; code_block = IfStatement ; AAIResourceUri aaiUri = AAIUriFactory . createResourceUri ( AAIFluentTypeBuilder . serviceDesignAndCreation ( ) . model ( serviceModelInvariantUUID ) . modelVer ( serviceModelVersionId ) ) ; aaiUri . depth ( Depth . ZERO ) ; logger . debug ( "Target A&AI Resource URI: {}" , aaiUri . build ( ) . toString ( ) ) ; Map < String , String > payload = new HashMap < > ( ) ; payload . put ( "distribution-status" , distributionStatus ) ; getAaiClient ( ) . update ( aaiUri , payload ) ; logger . debug ( "updatePatchAAI : {}" , payload ) ; } catch ( Exception e ) { logger . debug ( "Exception occurred on executePatchAAI : {}" , e . getMessage ( ) ) ; logger . error ( "Exception occurred" , e ) ; throw new Exception ( e ) ; } }
public void test() { try { WatchdogServiceModVerIdLookup lookup = watchdogModVerIdLookupRepository . findOneByDistributionId ( distributionId ) ; String serviceModelVersionId = "" ; code_block = IfStatement ; logger . debug ( "Executed RequestDB getWatchdogServiceModVerIdLookup with distributionId: {} " + "and serviceModelVersionId: {}" , distributionId , serviceModelVersionId ) ; logger . debug ( "ASDC Notification ServiceModelInvariantUUID : {}" , serviceModelInvariantUUID ) ; code_block = IfStatement ; AAIResourceUri aaiUri = AAIUriFactory . createResourceUri ( AAIFluentTypeBuilder . serviceDesignAndCreation ( ) . model ( serviceModelInvariantUUID ) . modelVer ( serviceModelVersionId ) ) ; aaiUri . depth ( Depth . ZERO ) ; logger . debug ( "Target A&AI Resource URI: {}" , aaiUri . build ( ) . toString ( ) ) ; Map < String , String > payload = new HashMap < > ( ) ; payload . put ( "distribution-status" , distributionStatus ) ; getAaiClient ( ) . update ( aaiUri , payload ) ; logger . debug ( "A&AI UPDATE MODEL Version is success!" ) ; } catch ( Exception e ) { logger . error ( "Exception occurred" , e ) ; throw new Exception ( e ) ; } }
public void test() { try { WatchdogServiceModVerIdLookup lookup = watchdogModVerIdLookupRepository . findOneByDistributionId ( distributionId ) ; String serviceModelVersionId = "" ; code_block = IfStatement ; logger . debug ( "Executed RequestDB getWatchdogServiceModVerIdLookup with distributionId: {} " + "and serviceModelVersionId: {}" , distributionId , serviceModelVersionId ) ; logger . debug ( "ASDC Notification ServiceModelInvariantUUID : {}" , serviceModelInvariantUUID ) ; code_block = IfStatement ; AAIResourceUri aaiUri = AAIUriFactory . createResourceUri ( AAIFluentTypeBuilder . serviceDesignAndCreation ( ) . model ( serviceModelInvariantUUID ) . modelVer ( serviceModelVersionId ) ) ; aaiUri . depth ( Depth . ZERO ) ; logger . debug ( "Target A&AI Resource URI: {}" , aaiUri . build ( ) . toString ( ) ) ; Map < String , String > payload = new HashMap < > ( ) ; payload . put ( "distribution-status" , distributionStatus ) ; getAaiClient ( ) . update ( aaiUri , payload ) ; logger . debug ( "A&AI UPDATE MODEL Version is success!" ) ; } catch ( Exception e ) { logger . debug ( "Exception occurred on executePatchAAI : {}" , e . getMessage ( ) ) ; logger . debug ( "Exception occurred on executePatchAAI : {}" , e . getMessage ( ) ) ; throw new Exception ( e ) ; } }
public void test() { { long ts1 = System . currentTimeMillis ( ) ; logger . info ( "event=lp_receive vto=120 wt=20" ) ; List < Message > messages = cqs1 . receiveMessage ( receiveMessageRequest ) . getMessages ( ) ; assertTrue ( "Expected 1 message, instead found " + messages . size ( ) , messages . size ( ) == 1 ) ; long ts2 = System . currentTimeMillis ( ) ; logger . info ( "event=lp_receive vto=100" ) ; } }
public void test() { try { final String queueUrl = getQueueUrl ( 1 , USR . USER1 ) ; Thread . sleep ( 1000 ) ; final ReceiveMessageRequest receiveMessageRequest = new ReceiveMessageRequest ( queueUrl ) ; receiveMessageRequest . setVisibilityTimeout ( 120 ) ; receiveMessageRequest . setMaxNumberOfMessages ( 10 ) ; receiveMessageRequest . setWaitTimeSeconds ( 20 ) ; ( new Thread ( ) code_block = "" ; ) . start ( ) ; Thread . sleep ( 100 ) ; cqs1 . sendMessage ( new SendMessageRequest ( queueUrl , "This is my message text. " + ( new Random ( ) ) . nextInt ( ) ) ) ; Thread . sleep ( 100 ) ; List < Message > messages = null ; long ts = System . currentTimeMillis ( ) ; code_block = ForStatement ; code_block = ForStatement ; logger . info ( "message not found " + ts ) ; fail ( "message not found any more" ) ; } catch ( AmazonServiceException ase ) { logger . error ( "test failed" , ase ) ; fail ( ase . getMessage ( ) ) ; } }
public void test() { try { final String queueUrl = getQueueUrl ( 1 , USR . USER1 ) ; Thread . sleep ( 1000 ) ; final ReceiveMessageRequest receiveMessageRequest = new ReceiveMessageRequest ( queueUrl ) ; receiveMessageRequest . setVisibilityTimeout ( 120 ) ; receiveMessageRequest . setMaxNumberOfMessages ( 10 ) ; receiveMessageRequest . setWaitTimeSeconds ( 20 ) ; ( new Thread ( ) code_block = "" ; ) . start ( ) ; Thread . sleep ( 100 ) ; logger . info ( "event=send_message queue_url=" + queueUrl ) ; cqs1 . sendMessage ( new SendMessageRequest ( queueUrl , "This is my message text. " + ( new Random ( ) ) . nextInt ( ) ) ) ; Thread . sleep ( 100 ) ; List < Message > messages = null ; long ts = System . currentTimeMillis ( ) ; code_block = ForStatement ; code_block = ForStatement ; fail ( "message not found any more" ) ; } catch ( AmazonServiceException ase ) { logger . error ( "error in receive_message" , ase ) ; fail ( ase . getMessage ( ) ) ; } }
public void test() { if ( member != getDistributionManagerId ( ) ) { String relationship = areInSameZone ( getDistributionManagerId ( ) , member ) ? "" : "not " ; logger . debug ( relationship ) ; } }
@ Override public RuleResult execute ( Map < String , String > ruleParam , Map < String , String > resourceAttributes ) { MDC . put ( "executionId" , ruleParam . get ( "executionId" ) ) ; MDC . put ( "ruleId" , ruleParam . get ( PacmanSdkConstants . RULE_ID ) ) ; logger . debug ( "======== Scan Policy Evaluation Rule started=========" ) ; String severity = ruleParam . get ( PacmanRuleConstants . SEVERITY ) ; String category = ruleParam . get ( PacmanRuleConstants . CATEGORY ) ; String resourceId = resourceAttributes . get ( PacmanRuleConstants . RESOURCE_ID ) . toLowerCase ( ) ; String pacmanHost = PacmanUtils . getPacmanHost ( PacmanRuleConstants . ES_URI ) ; String policyDefinitionName = ruleParam . get ( "policyDefinitionName" ) ; String azurePolicyEvaluationResultsURl = ruleParam . get ( "azurePolicyEvaluationResults" ) ; Map < String , Object > policyEvaluationResultsMap = new HashMap < > ( ) ; code_block = TryStatement ;  logger . debug ( "======== Azure Policy Evaluation Rule ended=========" ) ; return new RuleResult ( PacmanSdkConstants . STATUS_SUCCESS , PacmanRuleConstants . SUCCESS_MESSAGE ) ; }
public void test() { if ( ! isCompliant == true ) { List < LinkedHashMap < String , Object > > issueList = new ArrayList < > ( ) ; LinkedHashMap < String , Object > issue = new LinkedHashMap < > ( ) ; Annotation annotation = null ; annotation = Annotation . buildAnnotation ( ruleParam , Annotation . Type . ISSUE ) ; annotation . put ( PacmanSdkConstants . DESCRIPTION , policyEvaluationResultsMap . get ( "policyDescription" ) . toString ( ) ) ; annotation . put ( PacmanRuleConstants . SEVERITY , severity ) ; annotation . put ( PacmanRuleConstants . CATEGORY , category ) ; annotation . put ( PacmanRuleConstants . AZURE_SUBSCRIPTION , resourceAttributes . get ( PacmanRuleConstants . AZURE_SUBSCRIPTION ) ) ; annotation . put ( PacmanRuleConstants . AZURE_SUBSCRIPTION_NAME , resourceAttributes . get ( PacmanRuleConstants . AZURE_SUBSCRIPTION_NAME ) ) ; issue . put ( "resourceId" , resourceId ) ; issue . put ( "policyDescription" , policyEvaluationResultsMap . get ( "policyDescription" ) . toString ( ) ) ; issue . put ( "policyName" , policyEvaluationResultsMap . get ( "policyName" ) . toString ( ) ) ; issueList . add ( issue ) ; annotation . put ( PacmanRuleConstants . ISSUE_DETAILS , issueList . toString ( ) ) ; logger . debug ( "========CheckRule ended with annotation {} :=========" , annotation ) ; return new RuleResult ( PacmanSdkConstants . STATUS_FAILURE , PacmanRuleConstants . FAILURE_MESSAGE , annotation ) ; } }
public void test() { try { policyEvaluationResultsMap = PacmanUtils . getAzurePolicyEvaluationResults ( pacmanHost + azurePolicyEvaluationResultsURl , resourceId , policyDefinitionName ) ; code_block = IfStatement ; } catch ( Exception exception ) { logger . error ( exception . getMessage ( ) ) ; throw new RuleExecutionFailedExeption ( exception . getMessage ( ) ) ; } }
@ Override public RuleResult execute ( Map < String , String > ruleParam , Map < String , String > resourceAttributes ) { logger . debug ( "======== Azure Policy Evaluation Rule started =========" ) ; MDC . put ( "executionId" , ruleParam . get ( "executionId" ) ) ; MDC . put ( "ruleId" , ruleParam . get ( PacmanSdkConstants . RULE_ID ) ) ; String severity = ruleParam . get ( PacmanRuleConstants . SEVERITY ) ; String category = ruleParam . get ( PacmanRuleConstants . CATEGORY ) ; String resourceId = resourceAttributes . get ( PacmanRuleConstants . RESOURCE_ID ) . toLowerCase ( ) ; String pacmanHost = PacmanUtils . getPacmanHost ( PacmanRuleConstants . ES_URI ) ; String policyDefinitionName = ruleParam . get ( "policyDefinitionName" ) ; String azurePolicyEvaluationResultsURl = ruleParam . get ( "azurePolicyEvaluationResults" ) ; Map < String , Object > policyEvaluationResultsMap = new HashMap < > ( ) ; code_block = TryStatement ;  logger . debug ( "======== Azure Policy Evaluation Rule ended =========" ) ; return new RuleResult ( PacmanSdkConstants . STATUS_SUCCESS , PacmanRuleConstants . SUCCESS_MESSAGE ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( enabled ) { log . debug ( "ReplicationLive is stopping message: {}" , finalMessage ) ; return sendReplicatePacket ( new ReplicationLiveIsStoppingMessage ( finalMessage ) ) ; } }
public void test() { if ( ! DISK_OUT_OF_SPACE . equals ( e . getResult ( ) ) ) { log . warn ( "Unable to find space in task configuration: {}" , DISK_OUT_OF_SPACE ) ; } }
public void test() { if ( lang == null ) { return null ; } }
@ Override public List < CommunicationChannel > getCommunicationChannelsForUser ( String userId ) throws IOException { LOG . debug ( "Getting communication channels for user {}" , userId ) ; String url = buildCanvasUrl ( String . format ( "users/%s/communication_channels" , userId ) , emptyMap ( ) ) ; return getListFromCanvas ( url ) ; }
public void test() { try { servletDescriptor . register ( httpService ) ; } catch ( RuntimeException e ) { LOG . error ( "Unable to register servlet on mount point '{}', either it was already registered under the same alias or the init method throws an exception" , servletDescriptor . getAlias ( ) , e ) ; } catch ( ServletException e ) { LOG . error ( "Unable to mount servlet on mount point '{}', either it was already registered under the same alias or the init method throws an exception" , servletDescriptor . getAlias ( ) , e ) ; } catch ( NamespaceException e ) { LOG . error ( "Unable to mount servlet on mount point '{}', another resource is already bound to this alias" , servletDescriptor . getAlias ( ) , e ) ; } }
public void test() { try { servletDescriptor . register ( httpService ) ; } catch ( RuntimeException e ) { LOG . error ( "Registration of ServletDescriptor under mountpoint {} fails with unexpected RuntimeException!" , servletDescriptor . getAlias ( ) , e ) ; } catch ( ServletException e ) { LOG . error ( "Unable to register servlet on mount point '{}', another resource is already bound to this alias" , servletDescriptor . getAlias ( ) , e ) ; } catch ( NamespaceException e ) { LOG . error ( "Unable to mount servlet on mount point '{}', another resource is already bound to this alias" , servletDescriptor . getAlias ( ) , e ) ; } }
public void test() { try { servletDescriptor . register ( httpService ) ; } catch ( RuntimeException e ) { LOG . error ( "Registration of ServletDescriptor under mountpoint {} fails with unexpected RuntimeException!" , servletDescriptor . getAlias ( ) , e ) ; } catch ( ServletException e ) { LOG . error ( "Unable to mount servlet on mount point '{}', either it was already registered under the same alias or the init method throws an exception" , servletDescriptor . getAlias ( ) , e ) ; } catch ( NamespaceException e ) { LOG . error ( "Unable to mount servlet on mount point '{}', either it was already registered under the same alias or the init method throws an exception" , servletDescriptor . getAlias ( ) , e ) ; } }
@ Override public GetDataSystemIdentifierDto getData ( final SystemFilterDto systemFilter , final Iec61850Client client , final DeviceConnection connection ) throws NodeException { final int logicalDeviceIndex = systemFilter . getId ( ) ; LOGGER . debug ( "Get data for device {}" , logicalDeviceIndex ) ; final List < MeasurementDto > measurements = new ArrayList < > ( ) ; code_block = ForStatement ; final List < ProfileDto > profiles = new ArrayList < > ( ) ; code_block = ForStatement ; return new GetDataSystemIdentifierDto ( systemFilter . getId ( ) , systemFilter . getSystemType ( ) , measurements , profiles ) ; }
public void terminate ( ) { goon = false ; code_block = IfStatement ; int svs = 0 ; List < DHTBroadcaster < K > > scopy = null ; code_block = IfStatement ; logger . debug ( "DHTBroadcaster terminated" ) ; long enc = DHTTransport . etime - etime ; long dec = DHTTransport . dtime - dtime ; long encr = DHTTransport . ertime - ertime ; long decr = DHTTransport . drtime - drtime ; long drest = ( encr * dec ) / ( enc + 1 ) ; logger . info ( "DHT time: encode = " + enc + ", decode = " + dec + ", enc raw = " + encr + ", dec raw wait = " + decr + ", dec raw wait = " + decr + ", dec raw est = " + drest + ", sum est = " + ( enc + dec + encr + drest ) ) ; code_block = IfStatement ; code_block = TryStatement ;  hellread = null ; logger . info ( "terminated" ) ; logger . info ( "terminated" ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void terminate ( ) { goon = false ; logger . info ( "terminating" ) ; code_block = IfStatement ; int svs = 0 ; List < DHTBroadcaster < K > > scopy = null ; code_block = IfStatement ; logger . debug ( "DHTBroadcaster terminated" ) ; long enc = DHTTransport . etime - etime ; long dec = DHTTransport . dtime - dtime ; long encr = DHTTransport . ertime - ertime ; long decr = DHTTransport . drtime - drtime ; long drest = ( encr * dec ) / ( enc + 1 ) ; code_block = IfStatement ; logger . info ( "DHTBroadcaster terminated" ) ; code_block = TryStatement ;  hellread = null ; logger . info ( "terminated" ) ; }
public void test() { if ( logger . isWarnEnabled ( ) ) { logger . warn ( e . getMessage ( ) , e ) ; } }
public void test() { if ( CompanyThreadLocal . isDeleteInProcess ( ) ) { _log . debug ( "Skip indexing because the delete is already active" ) ; } else-if ( IndexWriterHelperUtil . isIndexReadOnly ( ) ) { _log . debug ( "Skip indexing because the index is read only" ) ; } }
public void test() { if ( CompanyThreadLocal . isDeleteInProcess ( ) ) { _log . debug ( "Skip indexing because company delete is in process" ) ; } else-if ( IndexWriterHelperUtil . isIndexReadOnly ( ) ) { _log . error ( "Skip indexing because read only" ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
private String getDateBeforeDays ( int numberOfDays ) { ZonedDateTime dateBeforeTwoDays = ZonedDateTime . now ( ) . minus ( Duration . ofDays ( numberOfDays ) ) ; String result = DateTimeFormatter . ofPattern ( "yyyy-MM-dd" ) . format ( dateBeforeTwoDays ) ; LOGGER . debug ( "DateBeforeTwoDays: " + result ) ; return result ; }
public void test() { if ( LOG . isDebugEnabled ( ) && ! ( command instanceof WaitIOCommand ) ) { LOG . debug ( "Invalid command: {}" , command ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) && ! ( command instanceof WaitIOCommand ) ) { LOG . debug ( "Invalid command: {}" , command ) ; } }
public void test() { if ( LOG . isInfoEnabled ( ) ) { LOG . info ( String . format ( "Call to '%s' on file '%s'" , uri . toString ( ) , file ) ) ; } }
protected void onUnexpectedStatusCode ( String urlStr , int statusCode , String contentType , String description ) { logger . debug ( "Unexpected status code {}" , statusCode ) ; }
@ Override protected AmazonSQSClient createClient ( final ProcessContext context , final AWSCredentialsProvider credentialsProvider , final ClientConfiguration config ) { getLogger ( ) . info ( "Creating client with credentials provider" ) ; return new AmazonSQSClient ( credentialsProvider , config ) ; }
public void test() { try { Set < URI > unassignFrom = new HashSet < URI > ( ) ; unassignFrom . add ( id ) ; _log . info ( "No Errors found proceeding further {}, {}, {}" , new Object [ ] code_block = "" ; ) ; controller . unassignFilePolicy ( filePolicyUri , unassignFrom , task ) ; auditOp ( OperationTypeEnum . UNASSIGN_FILE_POLICY , true , "BEGIN" , fp . getId ( ) . toString ( ) , fp . getFilePolicyName ( ) ) ; } catch ( BadRequestException e ) { op = _dbClient . error ( FilePolicy . class , fp . getId ( ) , task , e ) ; _log . error ( "Error Unassigning File policy {}, {}" , e . getMessage ( ) , e ) ; throw e ; } catch ( Exception e ) { _log . error ( "Error Unassigning File policy {}, {}" , e . getMessage ( ) , e ) ; throw APIException . badRequests . unableToProcessRequest ( e . getMessage ( ) ) ; } }
private void addKeyFile ( String username , String keyPairName , String filePath ) { logger . debug ( "Adding key: {}" , keyPairName ) ; ProviderParams params = providerParamsDao . getByUser ( username ) ; params . getKeys ( ) . put ( keyPairName , filePath ) ; save ( params ) ; }
@ Test public void testBasicApi ( ) { LogListener lsnr = LogListener . matches ( Pattern . compile ( "a[a-z]+" ) ) . andMatches ( "Exception message." ) . andMatches ( ".java:" ) . build ( ) ; log . debug ( "Listener: {}" , lsnr ) ; log . registerListener ( lsnr ) ; assertFalse ( lsnr . check ( ) ) ; log . error ( "There was an error." , new RuntimeException ( "Exception message." ) ) ; assertTrue ( lsnr . check ( ) ) ; }
@ Test public void testBasicApi ( ) { LogListener lsnr = LogListener . matches ( Pattern . compile ( "a[a-z]+" ) ) . andMatches ( "Exception message." ) . andMatches ( ".java:" ) . build ( ) ; log . info ( "Listener: " + lsnr ) ; log . info ( "Something new." ) ; assertFalse ( lsnr . check ( ) ) ; assertTrue ( lsnr . check ( ) ) ; }
public void test() { try { entity = this . createEntityType ( element , entityClass ) ; entity . setEntityDOM ( entityDom ) ; this . fillEntityType ( entity , element ) ; entity . setDefaultLang ( this . getLangManager ( ) . getDefaultLang ( ) . getCode ( ) ) ; _logger . debug ( "Entity Type '{}' defined" , entity . getTypeCode ( ) ) ; } catch ( Throwable t ) { _logger . error ( "Configuration error of the Entity Type detected" , t ) ; throw new ApsSystemException ( "Configuration error of the Entity Type detected" , t ) ; } }
public void test() { try { code_block = IfStatement ; final String json = command . getCommand ( this ) ; logger . trace ( "Sending command to: {}" , json ) ; websocketEndpoint . sendMessage ( json ) ; } catch ( final BitfinexCommandException e ) { logger . error ( "Got Exception while sending command" , e ) ; } }
public void test() { try { code_block = IfStatement ; final String json = command . getCommand ( this ) ; logger . debug ( "Sent: {}" , command ) ; websocketEndpoint . sendMessage ( json ) ; } catch ( final BitfinexCommandException e ) { logger . warn ( "Failed to send command: {}" , command , e ) ; } }
public void test() { try { PSAgentContext . get ( ) . getMasterClient ( ) . updateClock ( request . getTaskIndex ( ) , request . getMatrixId ( ) , request . getClock ( ) ) ; } catch ( ServiceException e ) { LOG . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( debugEnabled ) { log . debug ( "destroy({}) Destroy ({})" , this , channel ) ; } }
public void test() { if ( debugEnabled ) { log . debug ( "destroy({}) Destroy ({})" , this , channel ) ; } }
public void test() { if ( debugEnabled ) { log . debug ( "destroy({}) Destroy ({})" , this , channel ) ; } }
public void test() { if ( debugEnabled ) { log . debug ( "destroy({}) Destroy ({})" , this , channel ) ; } }
public void test() { if ( debugEnabled ) { log . debug ( "destroy({}) Destroy ({})" , this , channel ) ; } }
public void test() { try { String [ ] objArr = child . getTextTrim ( ) . split ( "/" ) ; String objStr = objArr [ objArr . length - 1 ] . toUpperCase ( ) ; BaseObjectType . valueOf ( objStr ) ; myModule . setObjectType ( objStr ) ; } catch ( IllegalArgumentException ex ) { LOGGER . error ( "IO error" , ex ) ; myModule . setObjectType ( "NOTE" ) ; } }
public void test() { try { File fileToDiscard = new File ( selectedRepository , file . getFileLocation ( ) ) ; FileUtils . forceDelete ( fileToDiscard ) ; deletedFilesParentDirs . add ( fileToDiscard . getParentFile ( ) ) ; } catch ( IOException e1 ) { logger . error ( "Unable to delete file {}" , file ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { if ( migrationFolderFile == null ) { LOG . info ( "Can't find migration folder {}" , folderName ) ; return ; } }
public void test() { if ( fileExt . equalsIgnoreCase ( "json" ) ) { String reportTemplateStr = readFileAndCreateJson ( file ) ; JsonObject reportTemplateJson = jsonParser . parse ( reportTemplateStr ) . getAsJsonObject ( ) ; int reportId = saveTemplateReport ( reportTemplateJson ) ; returnMessage = "Report Template Id created " + reportId ; } else { log . error ( "Invalid Report Template file format. " ) ; throw new InsightsCustomException ( "Invalid Report Template file format." ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception ex ) { log . error ( ex . getMessage ( ) , ex ) ; throw new InsightsCustomException ( ex . getMessage ( ) ) ; } }
public void test() { if ( potentialModification ) { int transId = msg != null ? msg . getTransactionId ( ) : Integer . MIN_VALUE ; logger . warn ( String . format ( "%s: transId: %s" , serverConnection . getName ( ) , transId ) , e ) ; } else { logger . warn ( String . format ( "%s: Unexpected IOException: " , serverConnection . getName ( ) ) , e ) ; } }
public void test() { if ( potentialModification ) { int transId = msg != null ? msg . getTransactionId ( ) : Integer . MIN_VALUE ; logger . warn ( String . format ( "%s: Unexpected IOException during operation for region: %s key: %s messId: %s" , serverConnection . getName ( ) , serverConnection . getModRegion ( ) , serverConnection . getModKey ( ) , transId ) , e ) ; } else { logger . warn ( String . format ( "%s: Unexpected transaction for region: %s key: %s messId: %s" , serverConnection . getName ( ) , serverConnection . getModRegion ( ) , serverConnection . getModKey ( ) , e ) ) ; } }
public void test() { try { serviceInstance = extractPojosForBB . extractByKey ( execution , ResourceKey . SERVICE_INSTANCE_ID ) ; vnf = extractPojosForBB . extractByKey ( execution , ResourceKey . GENERIC_VNF_ID ) ; vfModule = extractPojosForBB . extractByKey ( execution , ResourceKey . VF_MODULE_ID ) ; Customer customer = gBBInput . getCustomer ( ) ; CloudRegion cloudRegion = gBBInput . getCloudRegion ( ) ; SDNCRequest sdncRequest = new SDNCRequest ( ) ; GenericResourceApiVfModuleOperationInformation req = sdncVfModuleResources . activateVfModule ( vfModule , vnf , serviceInstance , customer , cloudRegion , requestContext , buildCallbackURI ( sdncRequest ) ) ; sdncRequest . setSDNCPayload ( req ) ; sdncRequest . setTopology ( SDNCTopology . VFMODULE ) ; execution . setVariable ( SDNC_REQUEST , sdncRequest ) ; } catch ( Exception ex ) { logger . error ( "Exception occurred" , ex ) ; exceptionUtil . buildAndThrowWorkflowException ( execution , 7000 , ex ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { if ( this . agentStatus == AgentStatus . INITIALIZING ) { changeStatus ( AgentStatus . RUNNING ) ; } else { logger . debug ( "Agent {} is not running" , agentId ) ; return ; } }
@ Override public String getWorkflowRunReport ( int workflowRunSWID ) { logger . info ( "No metadata connection" ) ; return "" ; }
public void test() { try { JsonNode crNode = OBJECT_MAPPER . readValue ( request . getParameter ( CLEARING_REQUEST ) , JsonNode . class ) ; clearingRequest = OBJECT_MAPPER . convertValue ( crNode , ClearingRequest . class ) ; clearingRequest . setRequestingUser ( user . getEmail ( ) ) ; clearingRequest . setClearingState ( ClearingRequestState . NEW ) ; LiferayPortletURL projectUrl = createDetailLinkTemplate ( request ) ; projectUrl . setParameter ( PROJECT_ID , clearingRequest . getProjectId ( ) ) ; projectUrl . setParameter ( PAGENAME , PAGENAME_DETAIL ) ; ProjectService . Iface client = thriftClients . makeProjectClient ( ) ; requestSummary = client . createClearingRequest ( clearingRequest , user , projectUrl . toString ( ) ) ; } catch ( IOException | TException e ) { log . error ( "Could not create clearing request" , e ) ; response . setProperty ( ResourceResponse . HTTP_STATUS_CODE , "500" ) ; } }
public void test() { try { JsonGenerator jsonGenerator = JSON_FACTORY . createGenerator ( response . getWriter ( ) ) ; jsonGenerator . writeStartObject ( ) ; jsonGenerator . writeStringField ( RESULT , requestSummary . getRequestStatus ( ) . toString ( ) ) ; code_block = IfStatement ; jsonGenerator . writeEndObject ( ) ; jsonGenerator . close ( ) ; } catch ( IOException e ) { log . error ( e . getMessage ( ) , e ) ; response . setProperty ( ResourceResponse . HTTP_STATUS_CODE , "500" ) ; } }
public void test() { try { lifeCycle . manager = this ; lifeCycle . start ( ) ; lifeCycles . put ( lifeCycle . getDomainName ( ) , lifeCycle ) ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; } }
private void getTableLineage ( ASTNode queryNode ) { ASTNode fromNode = ( ASTNode ) queryNode . getFirstChildWithType ( HiveParser . TOK_FROM ) ; ASTNode insertNode = ( ASTNode ) queryNode . getFirstChildWithType ( HiveParser . TOK_INSERT ) ; ASTNode insertDestinationNode = ( ASTNode ) insertNode . getFirstChildWithType ( HiveParser . TOK_DESTINATION ) ; if ( insertDestinationNode == null ) insertDestinationNode = ( ASTNode ) insertNode . getFirstChildWithType ( HiveParser . TOK_INSERT_INTO ) ; ASTNode nextToDestinationNode = ( ASTNode ) insertDestinationNode . getChild ( 0 ) ; ASTNode selectNode = ( ASTNode ) insertNode . getFirstChildWithType ( HiveParser . TOK_SELECT ) ; if ( selectNode == null ) selectNode = ( ASTNode ) insertNode . getFirstChildWithType ( HiveParser . TOK_SELECTDI ) ; ASTNode nextNodeOfFrom = ( ASTNode ) fromNode . getChild ( 0 ) ; if ( nextNodeOfFrom . getType ( ) == HiveParser . TOK_SUBQUERY ) handleTokSubQuery ( nextNodeOfFrom ) ; else-if ( nextNodeOfFrom . getType ( ) == HiveParser . TOK_TABREF ) handleTokTabRef ( nextNodeOfFrom ) ; else-if ( nextNodeOfFrom . getType ( ) == HiveParser . TOK_JOIN || nextNodeOfFrom . getType ( ) == HiveParser . TOK_LEFTOUTERJOIN || nextNodeOfFrom . getType ( ) == HiveParser . TOK_RIGHTOUTERJOIN || nextNodeOfFrom . getType ( ) == HiveParser . TOK_FULLOUTERJOIN || nextNodeOfFrom . getType ( ) == HiveParser . TOK_LEFTSEMIJOIN || nextNodeOfFrom . getType ( ) == HiveParser . TOK_UNIQUEJOIN ) handleTokJoin ( nextNodeOfFrom ) ; LOGGER . info ( "nextToDestinationNode to process : " + nextToDestinationNode . toString ( ) ) ; if ( nextToDestinationNode . getType ( ) == HiveParser . TOK_LEFT
private void getTableLineage ( ASTNode queryNode ) { ASTNode fromNode = ( ASTNode ) queryNode . getFirstChildWithType ( HiveParser . TOK_FROM ) ; ASTNode insertNode = ( ASTNode ) queryNode . getFirstChildWithType ( HiveParser . TOK_INSERT ) ; ASTNode insertDestinationNode = ( ASTNode ) insertNode . getFirstChildWithType ( HiveParser . TOK_DESTINATION ) ; if ( insertDestinationNode == null ) insertDestinationNode = ( ASTNode ) insertNode . getFirstChildWithType ( HiveParser . TOK_INSERT_INTO ) ; ASTNode nextToDestinationNode = ( ASTNode ) insertDestinationNode . getChild ( 0 ) ; ASTNode selectNode = ( ASTNode ) insertNode . getFirstChildWithType ( HiveParser . TOK_SELECT ) ; if ( selectNode == null ) selectNode = ( ASTNode ) insertNode . getFirstChildWithType ( HiveParser . TOK_SELECTDI ) ; ASTNode nextNodeOfFrom = ( ASTNode ) fromNode . getChild ( 0 ) ; LOGGER . info ( "nextNodeOfFrom to process : " + nextNodeOfFrom . toString ( ) ) ; if ( nextNodeOfFrom . getType ( ) == HiveParser . TOK_SUBQUERY ) handleTokSubQuery ( nextNodeOfFrom ) ; else-if ( nextNodeOfFrom . getType ( ) == HiveParser . TOK_TABREF ) handleTokTabRef ( nextNodeOfFrom ) ; else-if ( nextNodeOfFrom . getType ( ) == HiveParser . TOK_JOIN || nextNodeOfFrom . getType ( ) == HiveParser . TOK_LEFTOUTERJOIN || nextNodeOfFrom . getType ( ) == HiveParser . TOK_RIGHTOUTERJOIN || nextNodeOfFrom . getType ( ) == HiveParser . TOK_FULLOUTERJOIN || nextNodeOfFrom . getType ( ) == HiveParser . TOK_LEFTSEMIJOIN || nextNodeOfFrom . getType ( ) == HiveParser . TOK_UNIQUEJOIN ) handleTokJoin ( nextNodeOfFrom ) ; if ( nextToDestinationNode . getType ( ) == HiveParser . TOK_JOIN || next
private void getTableLineage ( ASTNode queryNode ) { ASTNode fromNode = ( ASTNode ) queryNode . getFirstChildWithType ( HiveParser . TOK_FROM ) ; ASTNode insertNode = ( ASTNode ) queryNode . getFirstChildWithType ( HiveParser . TOK_INSERT ) ; ASTNode insertDestinationNode = ( ASTNode ) insertNode . getFirstChildWithType ( HiveParser . TOK_DESTINATION ) ; if ( insertDestinationNode == null ) insertDestinationNode = ( ASTNode ) insertNode . getFirstChildWithType ( HiveParser . TOK_INSERT_INTO ) ; ASTNode nextToDestinationNode = ( ASTNode ) insertDestinationNode . getChild ( 0 ) ; ASTNode selectNode = ( ASTNode ) insertNode . getFirstChildWithType ( HiveParser . TOK_SELECT ) ; if ( selectNode == null ) selectNode = ( ASTNode ) insertNode . getFirstChildWithType ( HiveParser . TOK_SELECTDI ) ; ASTNode nextNodeOfFrom = ( ASTNode ) fromNode . getChild ( 0 ) ; LOGGER . info ( "nextNodeOfFrom to process : " + nextNodeOfFrom . toString ( ) ) ; if ( nextNodeOfFrom . getType ( ) == HiveParser . TOK_SUBQUERY ) handleTokSubQuery ( nextNodeOfFrom ) ; else-if ( nextNodeOfFrom . getType ( ) == HiveParser . TOK_TABREF ) handleTokTabRef ( nextNodeOfFrom ) ; else-if ( nextNodeOfFrom . getType ( ) == HiveParser . TOK_JOIN || nextNodeOfFrom . getType ( ) == HiveParser . TOK_LEFTOUTERJOIN || nextNodeOfFrom . getType ( ) == HiveParser . TOK_RIGHTOUTERJOIN || nextNodeOfFrom . getType ( ) == HiveParser . TOK_FULLOUTERJOIN || nextNodeOfFrom . getType ( ) == HiveParser . TOK_LEFTSEMIJOIN || nextNodeOfFrom . getType ( ) == HiveParser . TOK_UNIQUEJOIN ) handleTokJoin ( nextNodeOfFrom ) ; LOGGER . info ( "nextToDestinationNode to process : " + nextToDestinationNode ) ;
public void test() { if ( this . serialNumber == null ) { logger . debug ( "sendCommand getSerialNumber :: {}" , Arrays . toString ( reply ) ) ; byte [ ] reply ; CommConnection commAtConnection = openSerialPort ( getAtPort ( ) ) ; code_block = IfStatement ; code_block = TryStatement ;  closeSerialPort ( commAtConnection ) ; code_block = IfStatement ; } }
public void test() { try { long totalBases = sequencingObject . getFiles ( ) . stream ( ) . mapToLong ( f code_block = LoopStatement ; ) . sum ( ) ; CoverageQCEntry coverageQCEntry = new CoverageQCEntry ( sequencingObject , totalBases ) ; qcEntryRepository . save ( coverageQCEntry ) ; } catch ( EntityNotFoundException e ) { LOG . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { final Response response = remoteWebResource . request ( MediaType . TEXT_PLAIN ) . post ( Entity . text ( "" ) ) ; checkResponseStatus ( response ) ; } catch ( final Exception c ) { logger . error ( c . getMessage ( ) , c ) ; } }
public void test() { if ( irodsDescriptionValue . getTagData ( ) . equals ( currentIrodsDescriptionValue . getTagData ( ) ) ) { LOGGER . debug ( "skipping, ignore" ) ; return ; } }
@ Override public void checkAndUpdateDescriptionOnCollection ( final String collectionAbsolutePath , final IRODSTagValue irodsDescriptionValue ) throws JargonException { log . info ( "checkAndUpdateDescriptionOnCollection()" ) ; IRODSTagValue currentIrodsDescriptionValue = getDescriptionOnCollectionForLoggedInUser ( collectionAbsolutePath ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; deleteDescriptionFromCollection ( collectionAbsolutePath , currentIrodsDescriptionValue ) ; addDescriptionToCollection ( collectionAbsolutePath , irodsDescriptionValue ) ; }
public void test() { try { insertPreparedStmt = connection . prepareStatement ( insertQuery ) ; } catch ( SQLException e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( message . toString ( ) . contains ( "ERROR" ) ) { LOG . error ( message ) ; } }
public void test() { if ( handler == null ) { logger . debug ( HANDLER_IS_NULL ) ; return ; } }
public void test() { try { handler . handleChannelData ( action , message ) ; } catch ( final BitfinexClientException e ) { logger . warn ( "Unable to handle channel data" , e ) ; } }
public void test() { try { com . liferay . commerce . price . list . model . CommercePriceListChannelRel returnValue = CommercePriceListChannelRelServiceUtil . addCommercePriceListChannelRel ( commercePriceListId , commerceChannelId , order , serviceContext ) ; return com . liferay . commerce . price . list . model . CommercePriceListChannelRelSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try { InsightsContentConfig existingContentConfig = reportConfigDAL . getContentConfig ( contentId ) ; code_block = IfStatement ; } catch ( Exception e ) { log . error ( e ) ; } }
public void test() { try { boolean interrupted = false ; code_block = WhileStatement ; code_block = IfStatement ; } catch ( JMSException e ) { log . error ( "Error while listening to messages" , e ) ; throw new RuntimeException ( "JMSException : Error while listening to messages" , e ) ; } catch ( IOException e ) { log . error ( "Error while writing message to file" , e ) ; throw new RuntimeException ( "IOException : Error while writing message to file\"" , e ) ; } }
public void test() { try { boolean interrupted = false ; code_block = WhileStatement ; code_block = IfStatement ; } catch ( JMSException e ) { log . error ( "Error while receiving messages " , e ) ; throw new RuntimeException ( "JMSException : Error while listening to messages" , e ) ; } catch ( IOException e ) { log . error ( "IOException : Error while writing message to file\"" , e ) ; throw new RuntimeException ( "IOException : Error while writing message to file\"" , e ) ; } }
@ Override public void run ( ) { log . debug ( "Running timer task" ) ; eventHandler . handleEvent ( new TimerEvent ( customResourceUid , TimerEventSource . this ) ) ; }
public void test() { try { long checksum = 0 ; long remaining = length ; code_block = WhileStatement ; return new WriteSummary ( length , checksum ) ; } catch ( Throwable throwable ) { LOG . error ( "Failed to write file." , throwable ) ; fail ( "Failed to write file." + throwable . getMessage ( ) ) ; throw throwable ; } }
public void test() { if ( nextEvent != null ) { logger . debug ( "Processing event: {}" , event ) ; event = nextEvent ; nextEvent = null ; } else-if ( textUnitsIterator . hasNext ( ) ) { logger . debug ( "There are TextUnitDTOs available, create a text unit and return the text unit event" ) ; event = new Event ( EventType . TEXT_UNIT , getNextTextUnit ( ) ) ; } else { logger . debug ( "No more TextUnitDTO, create end document event and return it" ) ; event = new Event ( EventType . END_DOCUMENT ) ; finished = true ; } }
public void test() { if ( nextEvent != null ) { logger . debug ( "Return existing event, type: {}" , nextEvent . getEventType ( ) . toString ( ) ) ; event = nextEvent ; nextEvent = null ; } else-if ( textUnitsIterator . hasNext ( ) ) { logger . debug ( "No TextUnitDTO, create next text unit" ) ; event = new Event ( EventType . TEXT_UNIT , getNextTextUnit ( ) ) ; } else { logger . debug ( "No more TextUnitDTO, create end document event and return it" ) ; event = new Event ( EventType . END_DOCUMENT ) ; finished = true ; } }
public void test() { if ( nextEvent != null ) { logger . debug ( "Return existing event, type: {}" , nextEvent . getEventType ( ) . toString ( ) ) ; event = nextEvent ; nextEvent = null ; } else-if ( textUnitsIterator . hasNext ( ) ) { logger . debug ( "There are TextUnitDTOs available, create a text unit and return the text unit event" ) ; event = new Event ( EventType . TEXT_UNIT , getNextTextUnit ( ) ) ; } else { logger . debug ( "No DOCUMENT DTOs available" ) ; event = new Event ( EventType . END_DOCUMENT ) ; finished = true ; } }
@ Test public void testUserHostAndPortSerialization ( ) throws Exception { String result = checkSerializesAs ( UserAndHostAndPort . fromParts ( "testHostUser" , "1.2.3.4" , 22 ) , null ) ; Assert . assertFalse ( result . contains ( "error" ) , "Shouldn't have had an error, instead got: " + result ) ; Assert . assertEquals ( Strings . collapseWhitespace ( result , "" ) , "{\"user\":\"testHostUser\",\"hostAndPort\":{\"host\":\"1.2.3.4\",\"port\":22,\"hasBracketlessColons\":false}}" ) ; logger . info ( result ) ; }
public void test() { if ( GenericUtils . isNegativeOrNull ( idleTimeout ) ) { idleTimeout = CoreModuleProperties . IDLE_TIMEOUT . getRequiredDefault ( ) ; LOGGER . info ( "The default idle timeout is not set. Using default value: {}" , idleTimeout ) ; } }
public void test() { if ( traceEnabled ) { logger . trace ( "{} closing" , this ) ; } }
public List findByExample ( CmZosState instance ) { log . debug ( "finding CmZosState instance by example" ) ; code_block = TryStatement ;  }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.CmZosState" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.CmZosState" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
public void processClaimedUser ( ShadowUser shadowUser , RequestContext context ) { String orgId = getOrgId ( shadowUser , context ) ; Map < String , Object > esUser = ( Map < String , Object > ) ElasticSearchHelper . getResponseFromFuture ( elasticSearchService . getDataByIdentifier ( ProjectUtil . EsType . user . getTypeName ( ) , shadowUser . getUserId ( ) , context ) ) ; String userId = ( String ) esUser . get ( JsonKey . ID ) ; String rootOrgId = ( String ) esUser . get ( JsonKey . ROOT_ORG_ID ) ; logger . info ( context , "ShadowUserProcessor:processClaimedUser:started: flag value got from es " + esUser . get ( JsonKey . FLAGS_VALUE ) ) ; logger . info ( context , "ShadowUserProcessor:processClaimedUser:started: " + rootOrgId ) ; int flagsValue = null != esUser . get ( JsonKey . FLAGS_VALUE ) ? ( int ) esUser . get ( JsonKey . FLAGS_VALUE ) : 0 ; logger . info ( context , "ShadowUserProcessor:processClaimedUser:Got Flag Value " + flagsValue ) ; code_block = IfStatement ; deleteUserFromOrganisations ( shadowUser , rootOrgId , ( List < Map < String , Object > > ) esUser . get ( JsonKey . ORGANISATIONS ) , context ) ; code_block = IfStatement ; syncUserToES ( userId , context ) ; updateUserInShadowDb ( userId , shadowUser , ClaimStatus . CLAIMED . getValue ( ) , null , context ) ; }
public void processClaimedUser ( ShadowUser shadowUser , RequestContext context ) { logger . info ( context , "ShadowUserProcessor:processClaimedUser:started claming shadow user with processId: " + shadowUser . getProcessId ( ) ) ; String orgId = getOrgId ( shadowUser , context ) ; Map < String , Object > esUser = ( Map < String , Object > ) ElasticSearchHelper . getResponseFromFuture ( elasticSearchService . getDataByIdentifier ( ProjectUtil . EsType . user . getTypeName ( ) , shadowUser . getUserId ( ) , context ) ) ; String userId = ( String ) esUser . get ( JsonKey . ID ) ; String rootOrgId = ( String ) esUser . get ( JsonKey . ROOT_ORG_ID ) ; int flagsValue = null != esUser . get ( JsonKey . FLAGS_VALUE ) ? ( int ) esUser . get ( JsonKey . FLAGS_VALUE ) : 0 ; logger . info ( context , "ShadowUserProcessor:processClaimedUser:Got Flag Value " + flagsValue ) ; code_block = IfStatement ; deleteUserFromOrganisations ( shadowUser , rootOrgId , ( List < Map < String , Object > > ) esUser . get ( JsonKey . ORGANISATIONS ) , context ) ; code_block = IfStatement ; syncUserToES ( userId , context ) ; updateUserInShadowDb ( userId , shadowUser , ClaimStatus . CLAIMED . getValue ( ) , null , context ) ; logger . info ( context , "ShadowUserProcessor:processClaimedUser:finished claming shadow user with processId: " + shadowUser . getProcessId ( ) ) ; }
public void processClaimedUser ( ShadowUser shadowUser , RequestContext context ) { logger . info ( context , "ShadowUserProcessor:processClaimedUser:started claming shadow user with processId: " + shadowUser . getProcessId ( ) ) ; String orgId = getOrgId ( shadowUser , context ) ; Map < String , Object > esUser = ( Map < String , Object > ) ElasticSearchHelper . getResponseFromFuture ( elasticSearchService . getDataByIdentifier ( ProjectUtil . EsType . user . getTypeName ( ) , shadowUser . getUserId ( ) , context ) ) ; String userId = ( String ) esUser . get ( JsonKey . ID ) ; logger . info ( context , "ShadowUserProcessor:processClaimedUser:started claming shadow user with processId: " + shadowUser . getProcessId ( ) ) ; String rootOrgId = ( String ) esUser . get ( JsonKey . ROOT_ORG_ID ) ; logger . info ( context , "ShadowUserProcessor:processClaimedUser:started: flag value got from es " + esUser . get ( JsonKey . FLAGS_VALUE ) ) ; int flagsValue = null != esUser . get ( JsonKey . FLAGS_VALUE ) ? ( int ) esUser . get ( JsonKey . FLAGS_VALUE ) : 0 ; code_block = IfStatement ; deleteUserFromOrganisations ( shadowUser , rootOrgId , ( List < Map < String , Object > > ) esUser . get ( JsonKey . ORGANISATIONS ) , context ) ; code_block = IfStatement ; syncUserToES ( userId , context ) ; updateUserInShadowDb ( userId , shadowUser , ClaimStatus . CLAIMED . getValue ( ) , null , context ) ; }
public void test() { try { messageMetadata = MessageMetadata . fromMessage ( message ) ; publicKey = ( String ) message . getObject ( ) ; } catch ( final JMSException e ) { LOGGER . error ( "UNRECOVERABLE ERROR, unable to read ObjectMessage instance, giving up." , e ) ; return ; } }
public void test() { try { final CloseableHttpClient httpclient = HttpClients . createDefault ( ) ; uri = "http://search.maven.org/solrsearch/select?q=1:<SHA1>&rows=20&wt=json" . replaceAll ( "<SHA1>" , _digest ) ; final HttpGet method = new HttpGet ( uri ) ; if ( ConnectionUtil . getProxyConfig ( ) != null ) method . setConfig ( ConnectionUtil . getProxyConfig ( ) ) ; final CloseableHttpResponse response = httpclient . execute ( method ) ; code_block = TryStatement ;  } catch ( ClientProtocolException e ) { log . error ( "HTTP GET [uri=" + uri + "] caused an exception: " + e . getMessage ( ) , e ) ; } catch ( Exception e ) { log . error ( "HTTP GET [uri=" + uri + "] caused an exception: " + e . getMessage ( ) , e ) ; } }
public void test() { try { final CloseableHttpClient httpclient = HttpClients . createDefault ( ) ; uri = "http://search.maven.org/solrsearch/select?q=1:<SHA1>&rows=20&wt=json" . replaceAll ( "<SHA1>" , _digest ) ; final HttpGet method = new HttpGet ( uri ) ; if ( ConnectionUtil . getProxyConfig ( ) != null ) method . setConfig ( ConnectionUtil . getProxyConfig ( ) ) ; final CloseableHttpResponse response = httpclient . execute ( method ) ; code_block = TryStatement ;  } catch ( ClientProtocolException e ) { log . error ( "HTTP GET [uri=" + uri + "] caused an exception: " + e . getMessage ( ) ) ; } catch ( Exception e ) { log . error ( "HTTP GET [uri=" + uri + "] caused an exception: " + e . getMessage ( ) ) ; } }
public void execute ( ) throws IOException { log . info ( "Executing matchThreshold" ) ; Set < Map < String , String > > resultSet = match ( this . matchThreshold , this . scoreJena ) ; code_block = ForStatement ; log . info ( "Found " + resultSet . size ( ) + " links between Vivo and the Input model" ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; this . inputJena . sync ( ) ; }
public void test() { for ( Map < String , String > entry : resultSet ) { String sInputURI = entry . get ( "sInputURI" ) ; String sVivoURI = entry . get ( "sVivoURI" ) ; log . trace ( "vivo: " + sVivoURI ) ; log . trace ( "sVivo: " + sVivoURI ) ; String score = entry . get ( "score" ) ; log . trace ( "score: " + score ) ; log . debug ( "Match found: <" + sInputURI + "> in Input matched with <" + sVivoURI + "> in Vivo" ) ; } }
public void test() { for ( Map < String , String > entry : resultSet ) { String sInputURI = entry . get ( "sInputURI" ) ; log . trace ( "input: " + sInputURI ) ; String sVivoURI = entry . get ( "sVivoURI" ) ; log . trace ( "vivo: " + sVivoURI ) ; String score = entry . get ( "score" ) ; log . debug ( "Score found: <" + score ) ; log . debug ( "Match found: <" + sInputURI + "> in Input matched with <" + sVivoURI + "> in Vivo" ) ; } }
public void test() { for ( Map < String , String > entry : resultSet ) { String sInputURI = entry . get ( "sInputURI" ) ; log . trace ( "input: " + sInputURI ) ; String sVivoURI = entry . get ( "sVivoURI" ) ; log . trace ( "vivo: " + sVivoURI ) ; log . trace ( "vivo: " + sVivoURI ) ; String score = entry . get ( "score" ) ; log . trace ( "score: " + score ) ; } }
public void execute ( ) throws IOException { log . info ( "Finding matches" ) ; Set < Map < String , String > > resultSet = match ( this . matchThreshold , this . scoreJena ) ; code_block = ForStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; this . inputJena . sync ( ) ; log . info ( "Done" ) ; }
public void test() { try { conn . close ( ) ; } catch ( SQLException e ) { logger . warn ( "failed to close connection" , e ) ; } finally { conn = null ; } }
public void test() { try { Webcam webcamx = ( Webcam ) Runtime . start ( "webcam" , "Webcam" ) ; webcamx . capture ( "/dev/video0" , "jpg" , 30 , 640 , 480 , 0.5 ) ; WebGui webgui = ( WebGui ) Runtime . create ( "webgui" , "WebGui" ) ; webgui . autoStartBrowser ( false ) ; webgui . startService ( ) ; } catch ( Exception e ) { log . error ( "main threw" , e ) ; } }
public void test() { if ( client . getSecurityGroupServices ( ) . describeSecurityGroupsInRegion ( region , groupName ) . size ( ) > 0 ) { logger . debug ( ">> deleting securityGroup(%s)" , groupName ) ; client . getSecurityGroupServices ( ) . deleteSecurityGroupInRegion ( region , groupName ) ; securityGroupMap . invalidate ( new RegionNameAndIngressRules ( region , groupName , null , false ) ) ; logger . debug ( "<< deleted securityGroup(%s)" , groupName ) ; } }
public void test() { if ( client . getSecurityGroupServices ( ) . describeSecurityGroupsInRegion ( region , groupName ) . size ( ) > 0 ) { logger . debug ( ">> deleting securityGroup(%s)" , groupName ) ; client . getSecurityGroupServices ( ) . deleteSecurityGroupInRegion ( region , groupName ) ; securityGroupMap . invalidate ( new RegionNameAndIngressRules ( region , groupName , null , false ) ) ; logger . debug ( "<< deleted securityGroup(%s)" , groupName ) ; } }
@ Test public void testMultiMapSerialization ( ) throws Exception { Multimap < String , Integer > m = MultimapBuilder . hashKeys ( ) . arrayListValues ( ) . build ( ) ; m . put ( "bob" , 24 ) ; m . put ( "bob" , 25 ) ; String result = checkSerializesAs ( m , null ) ; log . debug ( "testMultiMapSerialization: " + result ) ; Assert . assertFalse ( result . contains ( "error" ) , "Shouldn't have had an error, instead got: " + result ) ; Assert . assertEquals ( Strings . collapseWhitespace ( result , "" ) , "{\"bob\":[24,25]}" ) ; }
public void test() { try { super . close ( ) ; } catch ( IOException e ) { log . warn ( "Can't close stream" , e ) ; } }
public void test() { if ( token . getJwt ( ) . getJWTClaimsSet ( ) . getIssueTime ( ) . before ( validToDate ) ) { logger . debug ( "Revoking expired access token: {}" , token ) ; tokenService . revokeAccessToken ( token ) ; OAuth2AccessTokenEntity newToken = connectTokenService . createResourceAccessToken ( client ) ; tokenService . saveAccessToken ( newToken ) ; return newToken ; } else { return token ; } }
public void test() { try { Date validToDate = new Date ( System . currentTimeMillis ( ) - config . getRegTokenLifeTime ( ) * 1000 ) ; code_block = IfStatement ; } catch ( ParseException e ) { logger . error ( "Invalid token" , e ) ; return token ; } }
public void test() { if ( retryCounter > 0 ) { LOG . warn ( "Netconf WRITE transaction failed to {}. Restarting transaction ... " , e . getMessage ( ) ) ; return write ( mountpoint , data , -- retryCounter ) ; } else { LOG . error ( "Netconf WRITE transaction failed to {}" , e . getMessage ( ) ) ; return false ; } }
private void validateExchange ( final PkiMessage < ? > req , final CertRep res ) throws TransactionException { LOGGER . debug ( "Validating message exchange" ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; LOGGER . debug ( "SCEP message exchange validated successfully" ) ; }
public void test() { if ( ! res . getTransactionId ( ) . equals ( req . getTransactionId ( ) ) ) { throw new TransactionException ( "Transaction ID Mismatch" ) ; } else { log . debug ( "Received transaction message: {}" , req . getTransactionId ( ) ) ; } }
public void test() { if ( ! res . getRecipientNonce ( ) . equals ( req . getSenderNonce ( ) ) ) { throw new InvalidNonceException ( req . getSenderNonce ( ) , res . getRecipientNonce ( ) ) ; } else { logger . debug ( req . getSenderNonce ( ) ) ; } }
public void test() { if ( res . getSenderNonce ( ) == null ) { logger . warn ( "SenderNonce is null" ) ; return ; } }
public ManagedConnectionFactory getManagedConnectionFactory ( ) { LOG . trace ( "getManagedConnectionFactory()" ) ; return this . managedConnectionFactory ; }
public void test() { try { clientResp = restClient . get ( relativeURL , queryParams ) ; } catch ( Exception e ) { LOG . error ( "Failed to get response, Error is : " + e . getMessage ( ) ) ; } }
public void test() { try { response = kv . get ( storeKey ) . get ( ) ; } catch ( InterruptedException | ExecutionException e ) { log . error ( "{}" , e . getMessage ( ) , e ) ; } }
public void test() { try { this . writeValue ( from ? HeliosEasyControlsBindingConstants . BYPASS_FROM_DAY : HeliosEasyControlsBindingConstants . BYPASS_TO_DAY , Integer . toString ( bypassDate . getDay ( ) ) ) ; this . writeValue ( from ? HeliosEasyControlsBindingConstants . BYPASS_FROM_MONTH : HeliosEasyControlsBindingConstants . BYPASS_TO_MONTH , Integer . toString ( bypassDate . getMonth ( ) ) ) ; } catch ( HeliosException e ) { logger . warn ( "{} encountered Exception when trying to read first pass date: {}" , HeliosEasyControlsHandler . class . getSimpleName ( ) , e . getMessage ( ) ) ; } }
public void test() { if ( dropCommandStrategy . shouldDropCommand ( commandId , address , redelivery ) ) { LOG . debug ( "The command {} is dropped." , commandId ) ; writeBuffer . flip ( ) ; getReplayBuffer ( ) . addBuffer ( commandId , writeBuffer ) ; } else { super . sendWriteBuffer ( commandId , address , writeBuffer , redelivery ) ; } }
@ PostConstruct public void initialise ( ) { logger . info ( "Starting initialisation vehicle service proxy." ) ; service = serviceLocator . builder ( VehicleStatelessAsync . class , "io.joynr.examples.statelessasync.carsim" ) . withUseCase ( "jee-consumer-test" ) . build ( ) ; logger . info ( "FINISHED initialisation vehicle service proxy." ) ; }
@ PostConstruct public void initialise ( ) { logger . info ( "START initialisation of vehicle service proxy." ) ; service = serviceLocator . builder ( VehicleStateStatelessAsync . class , "io.joynr.examples.statelessasync.carsim" ) . withUseCase ( "jee-consumer-test" ) . build ( ) ; logger . info ( "FINISHED initialisation of vehicle service proxy." ) ; }
@ Transactional ( readOnly = false ) public int undoSoftDeleteOnCascade ( int id , Set < CardType > types , Set < EventType > filteredEvents ) { LOG . debug ( " undoSoftDeleteOnCascade: id={}, types={}" , id , types ) ; return queries . undoSoftDeleteOnCascade ( id , toStringList ( types ) , toStringList ( filteredEvents ) ) ; }
public void test() { try { return delegate . next ( ) ; } finally { log . debug ( sb . a ( ", after=" + toString ( ) + ']' ) . toString ( ) ) ; } }
public void test() { if ( ! ( rawHookId instanceof String ) ) { LOGGER . error ( "Got unexpected raw hookId: " + rawHookId ) ; return ; } }
public void test() { if ( registeredHooks . containsKey ( hookId ) ) { LOGGER . debug ( "Registered hook {}" , hookId ) ; return ; } }
public synchronized void bindHook ( ServiceReference < DeploymentHook > hook ) { final Object rawHookId = hook . getProperty ( ConfigurationService . KURA_SERVICE_PID ) ; code_block = IfStatement ; final String hookId = ( String ) rawHookId ; code_block = IfStatement ; this . registeredHooks . put ( hookId , getBundleContext ( ) . getService ( hook ) ) ; updateAssociations ( ) ; LOGGER . info ( "hook registered" ) ; }
public void test() { if ( result . getError ( ) . isPresent ( ) ) { LOGGER . error ( "HealthCheck failed for {} : {}" , result . getComponentName ( ) . getName ( ) , result . getCause ( ) . orElse ( "" ) ) ; } else { LOGGER . error ( "HealthCheck failed for {} : {}" , result . getComponentName ( ) . getName ( ) , result . getCause ( ) . orElse ( "" ) ) ; } }
public void test() { if ( result . getError ( ) . isPresent ( ) ) { LOGGER . error ( "HealthCheck failed for {} : {}" , result . getComponentName ( ) . getName ( ) , result . getCause ( ) . orElse ( "" ) , result . getError ( ) . get ( ) ) ; } else { LOGGER . debug ( "HealthCheck succeeded for {}" , result . getComponentName ( ) . getName ( ) ) ; } }
public void test() { if ( result . getError ( ) . isPresent ( ) ) { LOGGER . error ( "HealthCheck is unstable for {} : {}" , result . getComponentName ( ) . getName ( ) , result . getError ( ) . getMessage ( ) ) ; } else { LOGGER . warn ( "HealthCheck is unstable for {} : {}" , result . getComponentName ( ) . getName ( ) , result . getCause ( ) . orElse ( "" ) ) ; } }
public void test() { if ( result . getError ( ) . isPresent ( ) ) { LOGGER . warn ( "HealthCheck is unstable for {} : {}" , result . getComponentName ( ) . getName ( ) , result . getCause ( ) . orElse ( "" ) , result . getError ( ) . get ( ) ) ; } else { LOGGER . info ( "HealthCheck is unstable for {}" , result . getComponentName ( ) . getName ( ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( LogMarker . PERSIST_ADVISOR_VERBOSE ) ) { logger . debug ( LogMarker . PERSIST_ADVISOR_VERBOSE , "{}-{}: Not waiting for {} because local member knows more about it" , shortDiskStoreId ( ) , regionPath , id ) ; } }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; } catch ( DiskAccessException e ) { log . warn ( "Unable to truncate file: " + file . getAbsolutePath ( ) ) ; } }
private void cleanupTemporaryFile ( String temporaryFile ) { LOGGER . info ( "Cleaning up temporary file: " + temporaryFile ) ; Script . runSimpleBashScript ( "rm -f " + temporaryFile ) ; }
public void test() { if ( ! dhcpServiceProvider . removeDhcpSupportForSubnet ( network ) ) { LOG . warn ( "Failed to remove DHCP support for subnet: {}" , network ) ; } }
public void test() { try { final NicIpAliasVO ipAlias = _nicIpAliasDao . findByGatewayAndNetworkIdAndState ( nic . getIPv4Gateway ( ) , network . getId ( ) , NicIpAlias . State . active ) ; code_block = IfStatement ; } catch ( final ResourceUnavailableException e ) { s_logger . warn ( "Unable to find ip alias" , e ) ; } }
public void test() { try { String url = getTDMURL ( ) + "/api/bundle/deploy/status/" + id + "/list" ; String json = _remoteConnectionService . getContent ( url ) ; _log . debug ( "response:" + json ) ; return Response . ok ( json ) . build ( ) ; } catch ( Exception e ) { _log . error ( e . getMessage ( ) ) ; return Response . serverError ( ) . build ( ) ; } }
public void test() { try { String url = getTDMURL ( ) + "/api/bundle/deploy/status/" + id + "/list" ; _log . debug ( "requesting:" + url ) ; String json = _remoteConnectionService . getContent ( url ) ; return Response . ok ( json ) . build ( ) ; } catch ( Exception e ) { _log . error ( e . getMessage ( ) ) ; return Response . serverError ( ) . build ( ) ; } }
public void test() { if ( ! ( reservation . getState ( ) . equals ( lastState ) ) ) { s_logger . info ( "reshing reservation " + reservation . getId ( ) + " from " + reservation . getState ( ) ) ; lastState = reservation . getState ( ) ; } }
public void test() { if ( connectedShardDataSourceMap . containsKey ( shardId ) ) { TransactionalDataSource dataSource = connectedShardDataSourceMap . get ( shardId ) ; dataSource . update ( dbVersion ) ; return dataSource ; } else { logger . warn ( "DataSource cannot be found: {}" , shardId ) ; return createShardDatasource ( shardId , dbVersion ) ; } }
@ Override public void assign ( byte [ ] regionName ) throws MasterNotRunningException , ZooKeeperConnectionException , IOException { log . debug ( "ignoring assign" ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { code_block = IfStatement ; devices = deviceTypeDAO . getDeviceTypeDAO ( ) . getAllDevices ( ) ; } catch ( DeviceMgtPluginException e ) { String msg = "Error while fetching all ${rootArtifactId} devices." ; log . error ( msg , e ) ; throw new DeviceManagementException ( msg , e ) ; } }
@ AfterClass public static void finalization ( ) { balancer . stop ( ) ; logger . info ( "Stopping SMPP server!" ) ; server . destroy ( ) ; logger . info ( "SMPP server stopped!" ) ; executor . shutdownNow ( ) ; monitorExecutor . shutdownNow ( ) ; logger . info ( "Shutdown complete!" ) ; balancer . stop ( ) ; logger . info ( "Done. Exiting" ) ; }
@ AfterClass public static void finalization ( ) { logger . info ( "Stopping LB!" ) ; balancer . stop ( ) ; server . destroy ( ) ; logger . info ( "SMPP server stopped!" ) ; executor . shutdownNow ( ) ; monitorExecutor . shutdownNow ( ) ; logger . info ( "Stopping SMPP server..." ) ; balancer . stop ( ) ; logger . info ( "Done. Exiting" ) ; }
@ AfterClass public static void finalization ( ) { logger . info ( "Stopping LB!" ) ; balancer . stop ( ) ; logger . info ( "Stopping SMPP server!" ) ; server . destroy ( ) ; executor . shutdownNow ( ) ; monitorExecutor . shutdownNow ( ) ; logger . info ( "Stopping SMPP server!" ) ; balancer . stop ( ) ; logger . info ( "Done. Exiting" ) ; }
@ AfterClass public static void finalization ( ) { logger . info ( "Stopping LB!" ) ; balancer . stop ( ) ; logger . info ( "Stopping SMPP server!" ) ; server . destroy ( ) ; logger . info ( "SMPP server stopped!" ) ; executor . shutdownNow ( ) ; monitorExecutor . shutdownNow ( ) ; balancer . stop ( ) ; logger . info ( "Stopped" ) ; }
public void parseStatus ( int bus , int address , int value ) throws java . io . IOException { log . debug ( "parseStatus({})" , bus ) ; java . util . List < SystemConnectionMemo > list = InstanceManager . getList ( SystemConnectionMemo . class ) ; SystemConnectionMemo memo ; code_block = TryStatement ;  String sensorName = memo . getSystemPrefix ( ) + "S" + address ; this . initSensor ( sensorName ) ; code_block = IfStatement ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { configRepository . updateUserConfig ( userConfig , version ) ; } catch ( DuplicateUsernameException e ) { logger . debug ( e . getMessage ( ) , e ) ; throw new JsonServiceException ( CONFLICT , "username" ) ; } }
public final void openModule ( WorkbenchModule module ) { code_block = IfStatement ; logger . info ( "Opening " + module . getValue ( ) ) ; activeModule . setValue ( module ) ; }
public void test() { { String primaryEmailTrim = primaryEmail . trim ( ) ; emailManager . reactivatePrimaryEmail ( orcid , primaryEmailTrim ) ; LOGGER . info ( "Processing record orcid={} from record" , orcid ) ; code_block = IfStatement ; ProfileEntity profileEntity = profileDao . find ( orcid ) ; profileEntity . setDeactivationDate ( null ) ; profileEntity . setClaimed ( true ) ; profileEntity . setIndexingStatus ( IndexingStatus . PENDING ) ; code_block = IfStatement ; profileDao . merge ( profileEntity ) ; code_block = IfStatement ; LOGGER . info ( "Record orcid={} successfully reactivated" , orcid ) ; return true ; } }
public void test() { { LOGGER . info ( "About to reactivate record, orcid={}" , orcid ) ; String primaryEmailTrim = primaryEmail . trim ( ) ; emailManager . reactivatePrimaryEmail ( orcid , primaryEmailTrim ) ; code_block = IfStatement ; ProfileEntity profileEntity = profileDao . find ( orcid ) ; profileEntity . setDeactivationDate ( null ) ; profileEntity . setClaimed ( true ) ; profileEntity . setIndexingStatus ( IndexingStatus . PENDING ) ; code_block = IfStatement ; profileDao . merge ( profileEntity ) ; LOGGER . info ( "Updated record" ) ; code_block = IfStatement ; return true ; } }
public void test() { try { stopRejectingRpcServer ( ) ; GrpcServerBuilder serverBuilder = GrpcServerBuilder . forAddress ( GrpcServerAddress . create ( mRpcConnectAddress . getHostName ( ) , mRpcBindAddress ) , ServerConfiguration . global ( ) , ServerUserState . global ( ) ) ; registerServices ( serverBuilder , mJobMaster . getServices ( ) ) ; serverBuilder . addService ( alluxio . grpc . ServiceType . JOURNAL_MASTER_CLIENT_SERVICE , new GrpcService ( new JournalMasterClientServiceHandler ( new DefaultJournalMaster ( JournalDomain . JOB_MASTER , mJournalSystem ) ) ) ) ; LOG . info ( "Started Alluxio job master gRPC server on address {}" , mRpcConnectAddress ) ; mGrpcServer = serverBuilder . build ( ) . start ( ) ; LOG . info ( "Started Alluxio job master gRPC server on address {}" , mRpcConnectAddress ) ; mGrpcServer . awaitTermination ( ) ; } catch ( IOException e ) { throw new RuntimeException ( e ) ; } }
public void test() { try { stopRejectingRpcServer ( ) ; LOG . info ( "Starting Alluxio job master gRPC server on address {}" , mRpcBindAddress ) ; GrpcServerBuilder serverBuilder = GrpcServerBuilder . forAddress ( GrpcServerAddress . create ( mRpcConnectAddress . getHostName ( ) , mRpcBindAddress ) , ServerConfiguration . global ( ) , ServerUserState . global ( ) ) ; registerServices ( serverBuilder , mJobMaster . getServices ( ) ) ; serverBuilder . addService ( alluxio . grpc . ServiceType . JOURNAL_MASTER_CLIENT_SERVICE , new GrpcService ( new JournalMasterClientServiceHandler ( new DefaultJournalMaster ( JournalDomain . JOB_MASTER , mJournalSystem ) ) ) ) ; mGrpcServer = serverBuilder . build ( ) . start ( ) ; mGrpcServer . awaitTermination ( ) ; LOG . info ( "Alluxio job master gRPC server started" ) ; } catch ( IOException e ) { throw new RuntimeException ( e ) ; } }
public void test() { switch ( responseStatus ) { case CACHE_HIT : LOGGER . debug ( "The response was generated directly" ) ; break ; case CACHE_MODULE_RESPONSE : LOGGER . debug ( "The response was generated directly by the " + "caching module" ) ; break ; case CACHE_MISS : LOGGER . debug ( "The response came from an upstream server" ) ; break ; case VALIDATED : LOGGER . debug ( "The response was generated from the cache " + "after validating the entry with the origin server" ) ; break ; } }
public void test() { switch ( responseStatus ) { case CACHE_HIT : LOGGER . debug ( "A response was generated from the cache with " + "no requests sent upstream" ) ; break ; case CACHE_MODULE_RESPONSE : LOGGER . debug ( "The response came from an upstream server" ) ; break ; case CACHE_MISS : LOGGER . debug ( "The response came from an upstream server" ) ; break ; case VALIDATED : LOGGER . debug ( "The response was generated from the cache " + "after validating the entry with the origin server" ) ; break ; } }
public void test() { switch ( responseStatus ) { case CACHE_HIT : LOGGER . debug ( "A response was generated from the cache with " + "no requests sent upstream" ) ; break ; case CACHE_MODULE_RESPONSE : LOGGER . debug ( "The response was generated directly by the " + "caching module" ) ; break ; case CACHE_MISS : LOGGER . debug ( "The response was generated from the cache with " + "no cache entry" ) ; break ; case VALIDATED : LOGGER . debug ( "The response was generated from the cache " + "after validating the entry with the origin server" ) ; break ; } }
public void test() { switch ( responseStatus ) { case CACHE_HIT : LOGGER . debug ( "A response was generated from the cache with " + "no requests sent upstream" ) ; break ; case CACHE_MODULE_RESPONSE : LOGGER . debug ( "The response was generated directly by the " + "caching module" ) ; break ; case CACHE_MISS : LOGGER . debug ( "The response came from an upstream server" ) ; break ; case VALIDATED : LOGGER . debug ( "Successfully retrieved response" ) ; break ; } }
@ Override public void entryAdded ( EntryEvent < Long , Whiteboards > event ) { LOG . debug ( "Entry added" ) ; onlineWbs . put ( event . getKey ( ) , event . getValue ( ) ) ; }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; } catch ( IOException e ) { LOG . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { new ExtGithub ( this . farm ) . value ( ) . repos ( ) . get ( new Coordinates . Simple ( "zerocracy/farm" ) ) . json ( ) ; this . output . set ( "OK" ) ; } catch ( final IOException err ) { this . output . set ( err . getMessage ( ) ) ; Logger . warn ( "Failed to poll email" , err ) ; } }
public void test() { if ( LOGGER . isTraceEnabled ( ) ) { LOGGER . trace ( "Weak listener list status:{}" , System . lineSeparator ( ) ) ; } }
@ Test public void testFindUsersByBadHeaderValueThenReturnBadRequest ( ) { LOGGER . debug ( "executing testFindUsersByBadHeaderValueThenReturnBadRequest" ) ; final HttpHeaders headers = new HttpHeaders ( ) ; headers . add ( CommonConstants . X_TENANT_ID_HEADER , "%ds</><!-sdq" ) ; super . performGet ( "/archive" , ImmutableMap . of ( "page" , 1 , "size" , 20 , "orderBy" , "id" ) , headers , status ( ) . isBadRequest ( ) ) ; }
public void test() { try { final ConfigurableFCMSender sender = new ConfigurableFCMSender ( androidVariant . getGoogleKey ( ) ) ; PrometheusExporter . instance ( ) . increasetotalPushAndroidRequests ( ) ; processFCM ( androidVariant , pushTargets , fcmMessage , sender ) ; logger . debug ( "Message batch to FCM has been submitted" ) ; callback . onSuccess ( ) ; } catch ( Exception e ) { logger . error ( "Error sending payload to FCM server." , e ) ; callback . onError ( String . format ( "Error sending payload to FCM server: %s" , e . getMessage ( ) ) ) ; } }
public void test() { try { logger . debug ( "Sending transformed FCM payload: {}" , fcmMessage ) ; final ConfigurableFCMSender sender = new ConfigurableFCMSender ( androidVariant . getGoogleKey ( ) ) ; PrometheusExporter . instance ( ) . increasetotalPushAndroidRequests ( ) ; processFCM ( androidVariant , pushTargets , fcmMessage , sender ) ; callback . onSuccess ( ) ; } catch ( Exception e ) { logger . error ( "Error sending payload to FCM" , e ) ; callback . onError ( String . format ( "Error sending payload to FCM server: %s" , e . getMessage ( ) ) ) ; } }
public void test() { try { runTestAsSubject ( new TestOperation ( ) code_block = "" ; ) ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; } finally { policyFilePath . delete ( ) ; } }
public static Process exec ( String ... cmd ) throws IOException { logger . info ( "Executing: " + cmd ) ; Process p = java . lang . Runtime . getRuntime ( ) . exec ( cmd ) ; return p ; }
public void test() { if ( dependencyDescriptor == null ) { logger . warn ( "Can't find dependency with id {}" , dependencyId ) ; return false ; } }
public void test() { if ( dependencyConfiguration == null ) { LOG . error ( "Missing dependency configuration for {}" , dependency ) ; return false ; } }
public void test() { if ( dependency == null ) { _log . error ( "Dependency {} has no dependency name" , dependency ) ; return false ; } }
public void test() { if ( dependentValues != null && ! dependentValues . contains ( dependencyValue ) ) { log . debug ( "Skipping non-existing dependency value " + dependencyValue ) ; return false ; } }
public void test() { try { cswSubscribe . deleteRecordsSubscription ( filterlessSubscriptionId ) ; } catch ( CswException e ) { LOGGER . debug ( "Failed to delete csw subscriber for filterless subscription {}" , filterlessSubscriptionId , e ) ; } }
public void test() { try { conn = this . getConnection ( ) ; StringBuffer sbBuffer = new StringBuffer ( ) ; Iterator < String > it = groupNames . iterator ( ) ; boolean appendWhere = true ; String q = SEARCH_IDEAINSTANCES_ID ; code_block = IfStatement ; code_block = IfStatement ; stat = conn . prepareStatement ( q ) ; int index = 1 ; code_block = IfStatement ; code_block = IfStatement ; res = stat . executeQuery ( ) ; code_block = WhileStatement ; } catch ( Throwable t ) { _logger . error ( "error in searchIdeaInstances" , t ) ; throw new RuntimeException ( "error in searchIdeaInstances" , t ) ; } finally { closeDaoResources ( res , stat , conn ) ; } }
public String createSecurityGroup ( String clusterName , String groupName , Nova nova , Set < String > ports ) { logger . info ( String . format ( "Creating security group '%s' ..." , groupName ) ) ; String securityGroupUniqueName = NovaSetting . NOVA_UNIQUE_GROUP_NAME ( clusterName , groupName ) ; SecGroupExtension group = this . novaContext . getCompute ( ) . securityGroups ( ) . create ( securityGroupUniqueName , String . format ( "Security group for hops cluster %s, node group %s" , clusterName , groupName ) ) ; code_block = IfStatement ; logger . info ( String . format ( "Security group '%s' was created :)" , securityGroupUniqueName ) ) ; return group . getId ( ) ; }
public String createSecurityGroup ( String clusterName , String groupName , Nova nova , Set < String > ports ) { String securityGroupUniqueName = NovaSetting . NOVA_UNIQUE_GROUP_NAME ( clusterName , groupName ) ; logger . info ( String . format ( "Creating security group '%s' ..." , securityGroupUniqueName ) ) ; SecGroupExtension group = this . novaContext . getCompute ( ) . securityGroups ( ) . create ( securityGroupUniqueName , String . format ( "Security group for hops cluster %s, node group %s" , clusterName , groupName ) ) ; code_block = IfStatement ; logger . info ( String . format ( "Security group '%s' successfully created." , group . getName ( ) ) ) ; return group . getId ( ) ; }
@ Test public void testAggregateProcessInstancesEmpty ( ) throws Exception { String xml1 = read ( this . getClass ( ) . getResourceAsStream ( "/jaxb/process-instance-empty.xml" ) ) ; String xml2 = read ( this . getClass ( ) . getResourceAsStream ( "/jaxb/process-instance-empty.xml" ) ) ; JaxbXMLResponseAggregator aggregate = new JaxbXMLResponseAggregator ( ) ; List < String > data = new ArrayList < > ( ) ; data . add ( xml1 ) ; data . add ( xml2 ) ; String result = aggregate . aggregate ( data ) ; logger . debug ( result ) ; Document xml = toXml ( result ) ; assertNotNull ( xml ) ; NodeList processes = xml . getElementsByTagName ( "process-instance-list" ) ; assertNotNull ( processes ) ; assertEquals ( 1 , processes . getLength ( ) ) ; NodeList processInstances = xml . getElementsByTagName ( "process-instance" ) ; assertNotNull ( processInstances ) ; assertEquals ( 0 , processInstances . getLength ( ) ) ; }
public void test() { try { return helper . fetchPageLimit ( sqlCountRows , sqlFetchRows , new Object [ ] code_block = "" ; , pageNo , pageSize , CONFIG_INFO_ROW_MAPPER ) ; } catch ( CannotGetJdbcConnectionException e ) { LogUtil . FATAL_LOG . error ( "[db-error] " + e . toString ( ) , e ) ; throw e ; } }
public void doSendServiceCheck ( String serviceCheckName , String status , String message , String [ ] tags ) { String tagString = "" ; code_block = IfStatement ; logger . info ( "Sending Service Check {}" , serviceCheckName ) ; Map < String , Object > sc = new HashMap < String , Object > ( ) ; sc . put ( "name" , serviceCheckName ) ; sc . put ( "status" , status ) ; sc . put ( "message" , message ) ; sc . put ( "tags" , tags ) ; serviceChecks . add ( sc ) ; }
public void test() { try { MethodKey methodKey = new MethodKey ( CPTaxCategoryServiceUtil . class , "getCPTaxCategory" , _getCPTaxCategoryParameterTypes8 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , cpTaxCategoryId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . commerce . product . model . CPTaxCategory ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void onClusterChange ( ) { ILogger logger = loggingService . getLogger ( HazelcastInstance . class ) ; logger . info ( "Cluster change" ) ; dispose ( onClusterChangeDisposables ) ; clusterService . reset ( ) ; partitionService . reset ( ) ; connectionManager . reset ( ) ; }
public void test() { if ( fileName == null ) { LOGGER . error ( "Unable to determine if file name is null." ) ; return ; } }
public void test() { try ( InputStream stream = new FileInputStream ( fileName ) ) { properties . load ( stream ) ; System . getProperties ( ) . putAll ( properties ) ; } catch ( FileNotFoundException e ) { LOG . error ( "The given test properties file does not exist" ) ; fail ( "The given test properties file does not exist" ) ; } catch ( IOException e ) { LOG . error ( "I/O error reading the test properties at {}: {}" , fileName , e . getMessage ( ) , e ) ; fail ( "Unable to read the test properties file" ) ; } }
public void test() { try ( InputStream stream = new FileInputStream ( fileName ) ) { properties . load ( stream ) ; System . getProperties ( ) . putAll ( properties ) ; } catch ( FileNotFoundException e ) { LOG . error ( "Test properties provided at {} does not exist, therefore aborting the test execution" , fileName ) ; fail ( "The given test properties file does not exist" ) ; } catch ( IOException e ) { LOG . error ( "Unable to read the test properties file" , e ) ; fail ( "Unable to read the test properties file" ) ; } }
public void test() { if ( timerFuture . status != HashedWheelTimerFuture . WAITING ) { log . warn ( "Timed out waiting for the timer. Exiting..." ) ; return true ; } }
public void test() { if ( timerFuture . totalTicks < currentTick ) { logger . warn ( "Timed out waiting for {} ms to avoid heartbeat" , currentTick ) ; } }
private void autoStart ( ) throws TException { String [ ] servicesToSchedule = ScheduleConstants . autostartServices ; log . info ( "Starting auto-start..." ) ; code_block = ForStatement ; log . info ( "Auto-start completed." ) ; }
private void autoStart ( ) throws TException { log . info ( "Auto-starting scheduling tasks in schedule service..." ) ; String [ ] servicesToSchedule = ScheduleConstants . autostartServices ; code_block = ForStatement ; log . info ( "Auto-starting scheduling tasks in schedule service...done" ) ; }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceChannelServiceUtil . class , "fetchCommerceChannel" , _fetchCommerceChannelParameterTypes3 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , commerceChannelId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . commerce . product . model . CommerceChannel ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( rc != BKException . Code . OK ) { log . error ( "[{}] Failed to close channel" , ledgerId ) ; channel . close ( ) ; return ; } }
public void test() { try { final FileSystem fileSystem = zip . getFileSystem ( configuration ) ; long size = 0L ; final byte [ ] buffer = new byte [ 1 << 13 ] ; progressable . progress ( ) ; code_block = TryStatement ;  progressable . progress ( ) ; return size ; } catch ( IOException | RuntimeException exception ) { LOGGER . error ( "Error generating zip" , exception ) ; throw exception ; } }
public void test() { try { autoPingManager . release ( ) ; bookmarkManager . release ( ) ; mediaFileManager . release ( ) ; fileContentManager . release ( ) ; pingTargetManager . release ( ) ; pingQueueManager . release ( ) ; pluginManager . release ( ) ; threadManager . release ( ) ; userManager . release ( ) ; weblogManager . release ( ) ; } catch ( Exception e ) { logger . error ( "Error during weblog: " , e ) ; } }
@ ExceptionHandler ( InvalidSearchParamException . class ) @ ResponseStatus ( value = HttpStatus . BAD_REQUEST ) public ViewObjectErrorResponse invalidSearchParamExceptionHandler ( InvalidSearchParamException ex ) { log . error ( ex . getMessage ( ) , ex ) ; return new ViewObjectErrorResponse ( ErrorCode . SEARCH_ERROR , ex . getMessage ( ) ) ; }
public void attachClean ( TmpItvSel instance ) { log . debug ( "attaching clean TmpItvSel instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . lock ( instance , LockMode . NONE ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
@ Override public MigrationPublishResult doMigrationPublish ( String clusterName , String shardName , String primaryDcName , InetSocketAddress newMaster ) throws OuterClientException { String startTime = DateTimeUtils . currentTimeAsString ( ) ; MigrationPublishResult res = new MigrationPublishResult ( "default-addr" , clusterName , primaryDcName , Arrays . asList ( newMaster ) ) ; String endTime = DateTimeUtils . currentTimeAsString ( ) ; res . setSuccess ( true ) ; res . setMessage ( "default-success" ) ; res . setStartTime ( startTime ) ; res . setEndTime ( endTime ) ; logger . info ( "doMigrationPublish({},{}): {}" , clusterName , shardName , endTime ) ; return res ; }
public void test() { try { return mapper . writeValueAsString ( mapper . createObjectNode ( ) . put ( "message" , getMessage ( ) ) ) ; } catch ( JsonProcessingException e ) { logger . error ( "unable to serialize message" , e ) ; throw new RuntimeException ( e ) ; } }
private void writePSKIdentityHintLength ( PskEcDheServerKeyExchangeMessage msg ) { appendInt ( msg . getIdentityHintLength ( ) . getValue ( ) , HandshakeByteLength . PSK_IDENTITY_LENGTH ) ; LOGGER . debug ( "SerializedPSKIdentityHintLength: " + msg . getIdentityHintLength ( ) . getValue ( ) ) ; }
public void test() { try { return this . createEntityFromAPI ( getApiUrl ( ) + SESSION_URL + id , token ) ; } catch ( JsonSyntaxException e ) { LOGGER . log ( Level . SEVERE , "Error creating entity" , e ) ; return new GenericEntity ( ) ; } }
@ ApiOperation ( value = "get history by user's id" ) @ GetMapping ( CommonConstants . PATH_LOGBOOK ) public LogbookOperationsResponseDto findHistoryById ( final @ PathVariable String id ) { LOGGER . debug ( "get logbook for user with id :{}" , id ) ; SanityChecker . check ( id ) ; return service . findHistoryById ( buildUiHttpContext ( ) , id ) ; }
public void test() { try { return InetAddress . getByName ( ip ) . getCanonicalHostName ( ) ; } catch ( UnknownHostException ex ) { LOGGER . warn ( "Unable to resolve canonical ip {}" , ip ) ; return ip ; } }
public void test() { try { SwingUtilities . invokeAndWait ( this :: createFrame ) ; } catch ( InterruptedException | InvocationTargetException ex ) { log . error ( "Exception creating frame" , ex ) ; } }
public void testDatasourceUpdate ( ) throws Exception { TestContext context = new TestContext ( ) ; Map < String , String > overlay = context . getUniqueOverlay ( ) ; String filePath = TestContext . overlayParametersOverTemplate ( TestContext . CLUSTER_TEMPLATE , overlay ) ; context . setCluster ( filePath ) ; LOG . info ( "entity -submit -type cluster -file " + filePath ) ; Assert . assertEquals ( TestContext . executeWithURL ( "entity -submit -type cluster -file " + filePath ) , 0 ) ; String dsName = "datasource-test-1" ; overlay . put ( DATASOURCE_NAME_KEY , dsName ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . DATASOURCE_TEMPLATE1 , overlay ) ; LOG . info ( "Submit datatsource entity {} via entity -submit -type datasource -file {}" , dsName , filePath ) ; Assert . assertEquals ( TestContext . executeWithURL ( "entity -submit -type datasource -file " + filePath ) , 0 ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . FEED_TEMPLATE3 , overlay ) ; LOG . info ( "Submit feed with datasource {} via entity -submitAndSchedule -type feed -file {}" , dsName , filePath ) ; Assert . assertEquals ( 0 , TestContext . executeWithURL ( "entity -submitAndSchedule -type feed -file " + filePath ) ) ; overlay . put ( DATASOURCE_NAME_KEY , dsName ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . DATASOURCE_TEMPLATE5 , overlay ) ; LOG . info ( "update datasource {} via -update -type datasource -file {}" , dsName , filePath ) ; Assert . assertEquals ( 0 , TestContext . executeWithURL ( "entity -update -type datasource -file " + filePath ) ) ; }
public void testDatasourceUpdate ( ) throws Exception { TestContext context = new TestContext ( ) ; Map < String , String > overlay = context . getUniqueOverlay ( ) ; String filePath = TestContext . overlayParametersOverTemplate ( TestContext . CLUSTER_TEMPLATE , overlay ) ; context . setCluster ( filePath ) ; LOG . info ( "entity -submit -type cluster -file " + filePath ) ; Assert . assertEquals ( TestContext . executeWithURL ( "entity -submit -type cluster -file " + filePath ) , 0 ) ; String dsName = "datasource-test-1" ; overlay . put ( DATASOURCE_NAME_KEY , dsName ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . DATASOURCE_TEMPLATE1 , overlay ) ; LOG . info ( "Submit datatsource entity {} via entity -submit -type datasource -file {}" , dsName , filePath ) ; Assert . assertEquals ( TestContext . executeWithURL ( "entity -submit -type datasource -file " + filePath ) , 0 ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . FEED_TEMPLATE3 , overlay ) ; LOG . info ( "Submit feed with datasource {} via entity -submitAndSchedule -type feed -file {}" , dsName , filePath ) ; Assert . assertEquals ( 0 , TestContext . executeWithURL ( "entity -submitAndSchedule -type feed -file " + filePath ) ) ; overlay . put ( DATASOURCE_NAME_KEY , dsName ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . DATASOURCE_TEMPLATE5 , overlay ) ; LOG . info ( "update datasource {} via -update -type datasource -file {}" , dsName , filePath ) ; Assert . assertEquals ( 0 , TestContext . executeWithURL ( "entity -update -type datasource -file " + filePath ) ) ; }
public void testDatasourceUpdate ( ) throws Exception { TestContext context = new TestContext ( ) ; Map < String , String > overlay = context . getUniqueOverlay ( ) ; String filePath = TestContext . overlayParametersOverTemplate ( TestContext . CLUSTER_TEMPLATE , overlay ) ; context . setCluster ( filePath ) ; LOG . info ( "entity -submit -type cluster -file " + filePath ) ; Assert . assertEquals ( TestContext . executeWithURL ( "entity -submit -type cluster -file " + filePath ) , 0 ) ; String dsName = "datasource-test-1" ; overlay . put ( DATASOURCE_NAME_KEY , dsName ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . DATASOURCE_TEMPLATE1 , overlay ) ; LOG . info ( "Submit datatsource entity {} via entity -submit -type datasource -file {}" , dsName , filePath ) ; Assert . assertEquals ( TestContext . executeWithURL ( "entity -submit -type datasource -file " + filePath ) , 0 ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . FEED_TEMPLATE3 , overlay ) ; LOG . info ( "submit feed {} via -submitAndSchedule -type feed -file {}" , dsName , filePath ) ; Assert . assertEquals ( 0 , TestContext . executeWithURL ( "entity -submitAndSchedule -type feed -file " + filePath ) ) ; overlay . put ( DATASOURCE_NAME_KEY , dsName ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . DATASOURCE_TEMPLATE5 , overlay ) ; LOG . info ( "update datasource {} via -update -type datasource -file {}" , dsName , filePath ) ; Assert . assertEquals ( 0 , TestContext . executeWithURL ( "entity -update -type datasource -file " + filePath ) ) ; }
public void testDatasourceUpdate ( ) throws Exception { TestContext context = new TestContext ( ) ; Map < String , String > overlay = context . getUniqueOverlay ( ) ; String filePath = TestContext . overlayParametersOverTemplate ( TestContext . CLUSTER_TEMPLATE , overlay ) ; context . setCluster ( filePath ) ; LOG . info ( "entity -submit -type cluster -file " + filePath ) ; Assert . assertEquals ( TestContext . executeWithURL ( "entity -submit -type cluster -file " + filePath ) , 0 ) ; String dsName = "datasource-test-1" ; overlay . put ( DATASOURCE_NAME_KEY , dsName ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . DATASOURCE_TEMPLATE1 , overlay ) ; LOG . info ( "Submit datatsource entity {} via entity -submit -type datasource -file {}" , dsName , filePath ) ; Assert . assertEquals ( TestContext . executeWithURL ( "entity -submit -type datasource -file " + filePath ) , 0 ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . FEED_TEMPLATE3 , overlay ) ; LOG . info ( "Submit feed with datasource {} via entity -submitAndSchedule -type feed -file {}" , dsName , filePath ) ; Assert . assertEquals ( 0 , TestContext . executeWithURL ( "entity -submitAndSchedule -type feed -file " + filePath ) ) ; overlay . put ( DATASOURCE_NAME_KEY , dsName ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . DATASOURCE_TEMPLATE5 , overlay ) ; LOG . info ( "Submit update entity {} via entity -update -type datasource -file {}" , dsName , filePath ) ; Assert . assertEquals ( 0 , TestContext . executeWithURL ( "entity -update -type datasource -file " + filePath ) ) ; }
public void test() { try { executeStmt ( nodeStateDeleteSQL , new Object [ ] code_block = "" ; ) ; } catch ( Exception e ) { String msg = "failed to delete node state: " + state . getNodeId ( ) ; log . error ( msg ) ; throw new ItemStateException ( msg , e ) ; } }
public void test() { if ( isReservedWord ( name ) ) { LOGGER . warn ( name + " (reserved word) cannot be used as name. Renamed to " + name + "_" ) ; name = name + "_" ; } }
public void test() { try { String loginToken = session . login ( session . getAccessToken ( ) ) ; LOG . info ( "Refreshed token " + loginToken ) ; assertTrue ( onLoginTriggered , "SalesforceSessionListener onLogin NOT called" ) ; onLoginTriggered = false ; loginToken = session . login ( loginToken ) ; LOG . info ( "Refreshed token " + loginToken ) ; assertTrue ( onLogoutTriggered , "SalesforceSessionListener onLogout NOT called" ) ; assertTrue ( onLoginTriggered , "SalesforceSessionListener onLogin NOT called" ) ; } finally { session . logout ( ) ; } }
public void test() { try { LOG . info ( "First token " + loginToken ) ; String loginToken = session . login ( session . getAccessToken ( ) ) ; LOG . info ( "First token " + loginToken ) ; assertTrue ( onLoginTriggered , "SalesforceSessionListener onLogin NOT called" ) ; onLoginTriggered = false ; loginToken = session . login ( loginToken ) ; assertTrue ( onLogoutTriggered , "SalesforceSessionListener onLogout NOT called" ) ; assertTrue ( onLoginTriggered , "SalesforceSessionListener onLogin NOT called" ) ; } finally { session . logout ( ) ; } }
public void test() { if ( protectionSystems . isEmpty ( ) ) { log . warn ( "No protection system specified." ) ; } }
public void test() { try { log . info ( "Saving position in {}" , filename ) ; Thread . sleep ( 2000 ) ; FileIO . toFile ( filename , CodecUtils . toJson ( positions ) . getBytes ( ) ) ; } catch ( Exception e ) { log . error ( "could not save servo positions" , e ) ; } }
public void test() { try { Thread . sleep ( 2000 ) ; log . debug ( "saving {} positions of {} servos" , filename , positions . size ( ) ) ; FileIO . toFile ( filename , CodecUtils . toJson ( positions ) . getBytes ( ) ) ; } catch ( Exception e ) { log . error ( "exception saving positions" , e ) ; } }
public void test() { if ( FORMAT_RAW . equals ( preferredTargetFormat ) ) { getLogger ( ) . debug ( "Using " + preferredTargetFormat + " instead." ) ; return null ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( LOGGER . isTraceEnabled ( ) ) { LOGGER . trace ( "Weak listener list status:{}" , System . lineSeparator ( ) ) ; } }
@ Test public void testQuery ( ) throws Exception { com . google . api . services . calendar . model . FreeBusyRequest request = new FreeBusyRequest ( ) ; List < FreeBusyRequestItem > items = new ArrayList < > ( ) ; items . add ( new FreeBusyRequestItem ( ) . setId ( getCalendar ( ) . getId ( ) ) ) ; request . setItems ( items ) ; request . setTimeMin ( DateTime . parseRfc3339 ( "2014-11-10T20:45:30-00:00" ) ) ; request . setTimeMax ( DateTime . parseRfc3339 ( "2014-11-10T21:45:30-00:00" ) ) ; final com . google . api . services . calendar . model . FreeBusyResponse result = requestBody ( "direct://QUERY" , request ) ; assertNotNull ( result , "query result" ) ; LOG . debug ( "query: " + result ) ; }
protected void processAffinityGroup ( AffinityGroupVMMapVO vmGroupMapping , DeploymentPlan plan , VirtualMachine vm ) { LOG . info ( "Removing affinity group " + plan . getName ( ) ) ; AffinityGroupVO group = _affinityGroupDao . findById ( vmGroupMapping . getAffinityGroupId ( ) ) ; List < Long > groupVMIds = _affinityGroupVMMapDao . listVmIdsByAffinityGroup ( group . getId ( ) ) ; groupVMIds . remove ( vm . getId ( ) ) ; List < Long > preferredHosts = getPreferredHostsFromGroupVMIds ( groupVMIds ) ; plan . setPreferredHosts ( preferredHosts ) ; }
public void test() { try { BaremetalRctResponse rsp = vlanMgr . addRct ( this ) ; this . setResponseObject ( rsp ) ; } catch ( Exception e ) { s_logger . warn ( "Exception: " , e ) ; throw new ServerApiException ( ApiErrorCode . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { try { Thread . sleep ( 5000 ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( duration > 1000 ) { LOG . debug ( "{} - Unlocking {} after {}ms" , name , requestId , duration ) ; } else { LOG . trace ( "{} - Unlocking {} after {}ms" , name , requestId , duration ) ; } }
public void test() { if ( duration > 1000 ) { LOG . debug ( "{} - Unlocking {} after {}ms" , name , requestId , duration ) ; } else { LOG . debug ( "{} - Unlocking {}" , name , requestId ) ; } }
public void test() { if ( ! nonEmptyDirs . isEmpty ( ) ) { LOG . error ( "Non-empty Dirs are not empty" ) ; throw new InvalidCookieException ( ) ; } }
public void test() { try { code_block = TryStatement ;  } catch ( IOException e ) { String errMsg = "Trouble copying inputstream " + in + " to outputstream " + out ; log . warn ( errMsg , e ) ; throw new IOFailure ( errMsg , e ) ; } }
public void test() { if ( ! file . delete ( ) ) { logger . warn ( "Unable to clean up file: " + file ) ; } }
public void test() { try { VALUE value = TypeCastUtility . castValue ( XmlUtility . getObjectAttribute ( x , "value" ) , getHolderType ( ) ) ; setValue ( value ) ; } catch ( Exception e ) { LOG . error ( "" , e ) ; } }
public void test() { try { return new URL ( defaultUrl . get ( ) ) ; } catch ( IllegalArgumentException e1 ) { LOGGER . error ( "Could not resolve default url, falling back to default." , e1 ) ; throw Throwables . propagate ( e1 ) ; } }
public void test() { try { luceneQuery = inQueryParser . parse ( nativeSearchString ) ; } catch ( ParseException e ) { String message = "Unable to parse query: '" + nativeSearchString + "'" ; LOGGER . error ( message ) ; throw new RuntimeException ( message ) ; } }
@ Override public synchronized void run ( ) { log . debug ( "** Fail start" ) ; session . getConnection ( ) . fail ( new ActiveMQNotConnectedException ( "oops" ) ) ; log . debug ( "** Fail complete" ) ; cancel ( ) ; executed = true ; }
@ Override public synchronized void run ( ) { log . debug ( "** Failing connection" ) ; session . getConnection ( ) . fail ( new ActiveMQNotConnectedException ( "oops" ) ) ; cancel ( ) ; log . debug ( "** Failing" ) ; executed = true ; }
public void test() { try { testUser = TestUtils . setupUser ( "wtTestUser" ) ; testWeblog = TestUtils . setupWeblog ( "wtTestWeblog" , testUser ) ; TestUtils . endSession ( true ) ; } catch ( Exception ex ) { log . error ( ex ) ; throw new Exception ( "Test setup failed" , ex ) ; } }
public void test() { try { CopyWriter copyWriter = sourceBlob . copyTo ( destination ) ; copied = copyWriter . getResult ( ) ; log . debug ( "Successfully copied bucket {} to {}" , destPath . getBucket ( ) , copied ) ; } catch ( StorageException e ) { throw new RuntimeException ( String . format ( "Unable to access source bucket %s. " , destPath . getBucket ( ) ) + "Ensure you entered the correct bucket path." , e ) ; } }
public void test() { try { return Optional . of ( generateReport ( metacard ) ) ; } catch ( ValidationExceptionImpl e ) { LOGGER . error ( "Failed to generate report" , e ) ; return Optional . empty ( ) ; } }
public void test() { if ( currentOp . getState ( ) == OperationState . COMPLETE ) { getLogger ( ) . debug ( "Completed read op: %s and giving the next %d " + "bytes" , currentOp , rbuf . remaining ( ) ) ; Operation op = node . removeCurrentReadOp ( ) ; assert op == currentOp : "Expected to pop " + currentOp + " got " + op ; code_block = IfStatement ; } else-if ( currentOp . getState ( ) == OperationState . RETRY ) { handleRetryInformation ( currentOp . getErrorMsg ( ) ) ; getLogger ( ) . debug ( "Recovering retry information" , currentOp ) ; ( ( VBucketAware ) currentOp ) . addNotMyVbucketNode ( currentOp . getHandlingNode ( ) ) ; Operation op = node . removeCurrentReadOp ( ) ; assert op == currentOp : "Expected to pop " + currentOp + " got " + op ; retryOperation ( currentOp ) ; metrics . markMeter ( OVERALL_RESPONSE_RETRY_METRIC ) ; } }
public void test() { -> { logger . warn ( "Checkout conflicts warning" ) ; EventHandler handler = event -> quickStashSave ( ) ; this . notificationPaneController . addNotification ( "You can't switch to that branch because there would be a merge conflict. " + "Stash your changes or resolve conflicts first." , "stash" , handler ) ; } }
@ Override public void processRecord ( ARCRecord sar , OutputStream os ) { LOG . debug ( "processRecord" ) ; Map < String , String > fieldsread = new HashMap < String , String > ( ) ; fieldsread . put ( "A" , sar . getMetaData ( ) . getUrl ( ) ) ; fieldsread . put ( "e" , sar . getMetaData ( ) . getIp ( ) ) ; fieldsread . put ( "b" , sar . getMetaData ( ) . getDate ( ) ) ; fieldsread . put ( "m" , sar . getMetaData ( ) . getMimetype ( ) ) ; fieldsread . put ( "n" , Long . toString ( sar . getMetaData ( ) . getLength ( ) ) ) ; fieldsread . put ( "v" , Long . toString ( sar . getMetaData ( ) . getOffset ( ) ) ) ; fieldsread . put ( "g" , sar . getMetaData ( ) . getArcFile ( ) . getName ( ) ) ; code_block = IfStatement ; printFields ( fieldsread , os ) ; }
public void test() { try { Map < String , Object > params = new HashMap < > ( ) ; params . put ( "tableName" , tableName ) ; params . put ( "fieldName" , fieldName ) ; params . put ( "fieldVal" , fieldVal ) ; params . put ( "equalFieldName" , equalFieldName ) ; params . put ( "equalFieldVal" , equalFieldVal ) ; params . put ( "dataId" , dataId ) ; count = systemMapper . duplicateCheck ( params ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; } finally { sqlSession . close ( ) ; } }
public void test() { if ( count == null || count == 0 ) { LOG . info ( "This value is available is available!" ) ; return new JsonResponse . Builder < > ( Response . Status . OK ) . message ( "This value is available!" ) . success ( true ) . build ( ) ; } else { LOG . info ( "This value already exists is not available!" ) ; return new JsonResponse . Builder < > ( Response . Status . OK ) . message ( "This value already exists is not available!" ) . success ( false ) . build ( ) ; } }
public void test() { if ( count == null || count == 0 ) { LOG . info ( "This value is available" ) ; return new JsonResponse . Builder < > ( Response . Status . OK ) . message ( "This value is available!" ) . success ( true ) . build ( ) ; } else { LOG . info ( "This value already exists is not available!" ) ; return new JsonResponse . Builder < > ( Response . Status . OK ) . message ( "This value already exists is not available!" ) . success ( false ) . build ( ) ; } }
public void test() { if ( resolvedMoveThreadCount > availableProcessorCount ) { LOG . debug ( "The resolvedMoveThreadCount ({}) is greater than the availableProcessorCount ({})." , resolvedMoveThreadCount , availableProcessorCount ) ; } }
public void test() { try { String body = message . getBody ( ) ; code_block = IfStatement ; String from = message . getFrom ( ) ; code_block = IfStatement ; long fromUserId = UserLocalServiceUtil . getUserIdByScreenName ( _companyId , JabberUtil . getScreenName ( from ) ) ; EntryLocalServiceUtil . addEntry ( fromUserId , _userId , body ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; } }
public void test() { if ( getPort ( ) == getPortDefaultValue ( ) ) { log . info ( "server listens on standard secure port [{}:{}]" , bindAddress , server . actualPort ( ) ) ; } else { log . warn ( "server listens on non-standard secure port [{}:{}], default is {}" , bindAddress , server . actualPort ( ) , getPortDefaultValue ( ) ) ; } }
public void test() { if ( getPort ( ) == getPortDefaultValue ( ) ) { log . info ( "server listens on standard secure port [{}:{}]" , bindAddress , server . actualPort ( ) ) ; } else { log . warn ( "server listens on non-standard secure port [{}:{}], default is {}" , bindAddress , server . actualPort ( ) , getPortDefaultValue ( ) ) ; } }
public void test() { try { MDC . put ( "destination" , destination ) ; applyResult = applySnapshotToDB ( lastPosition , false ) ; } catch ( Throwable e ) { LOG . error ( "Failed to apply snapshot to DB." , e ) ; } }
public void test() { try { MDC . put ( "destination" , destination ) ; code_block = IfStatement ; } catch ( Throwable e ) { log . error ( "send() :: failed" , e ) ; } }
public void test() { try { ConfigurationRequest . Builder builder = ConfigurationRequest . newBuilder ( ) . setClusterName ( settings . getClusterName ( ) ) ; code_block = IfStatement ; ConfigurationResponse response = stub . call ( builder . build ( ) ) ; String responseUuid = response . getUuid ( ) ; code_block = IfStatement ; response . getConfigTableList ( ) . forEach ( config code_block = LoopStatement ; ) ; this . uuid = responseUuid ; } catch ( Exception e ) { LOG . error ( "" , e ) ; } }
@ Test public void testRangeOpsInDiffSubTree ( ) throws Exception { log . info ( "------  testRangeOpsInDiffSubTree  ------" ) ; String city = TestCities . rome . name ( ) ; String query = "(" + CityField . NUM . name ( ) + LTE_OP + "100" + AND_OP + CityField . CITY . name ( ) + EQ_OP + "'" + city + "')" + AND_OP + CityField . NUM . name ( ) + GTE_OP + "100" ; String expected = CityField . NUM . name ( ) + LTE_OP + "'+cE1'" + JEXL_AND_OP + CityField . CITY . name ( ) + EQ_OP + "'" + city + "'" + JEXL_AND_OP + CityField . NUM . name ( ) + GTE_OP + "'+cE1'" ; String plan = getPlan ( query , true , true ) ; assertPlanEquals ( expected , plan ) ; expected = CityField . NUM . name ( ) + LTE_OP + "'+cE1'" + JEXL_AND_OP + CityField . CITY . name ( ) + EQ_OP + "'" + city + "'" + JEXL_AND_OP + CityField . NUM . name ( ) + GTE_OP + "'+cE1'" ; plan = getPlan ( query , true , false ) ; assertPlanEquals ( expected , plan ) ; expected = CityField . NUM . name ( ) + LTE_OP + "'+cE1'" + JEXL_AND_OP + CityField . CITY . name ( ) + EQ_OP + "'" + city + "'" + JEXL_AND_OP + CityField . NUM . name ( ) + GTE_OP + "'+cE1'" ; plan = getPlan ( query , false , true ) ; assertPlanEquals ( expected , plan ) ; expected = query ; runTest ( query , expected ) ; }
public void test() { if ( LOGGER . isTraceEnabled ( ) ) { LOGGER . trace ( "Weak listener list status:{}" , System . lineSeparator ( ) ) ; } }
public void test() { try { String result = this . checkMethod ( ) ; if ( null != result ) return result ; this . resetMethodStatus ( this . extractMethod ( ) ) ; } catch ( Throwable t ) { _logger . error ( "error in method" , t ) ; return FAILURE ; } }
public void test() { if ( joinRecords . size ( ) > 0 ) { AsyncJobJoinMapVO joinRecord = joinRecords . get ( 0 ) ; code_block = ForStatement ; } else { LOG . warn ( "No join records found for job: {}" , jobName ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void shutdown ( ) { logger . info ( "Shutdown requested" ) ; shutdownLatch . countDown ( ) ; }
public static void destroyInstance ( ) { checkState ( instance != null , "createInstance() must be called prior to destroyInstance()" ) ; instance . stop ( ) ; LOG . info ( "Stopping instance [{}]" , instance ) ; instance = null ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debugf ( "Previous: %s" , previous ) ; log . debugf ( "Current: %s" , current ) ; log . debugf ( "Partition: %s" , partition ) ; log . debugf ( "Added: %s" , addedRemoved . get ( "added" ) ) ; log . debugf ( "Removed: %s" , addedRemoved . get ( "removed" ) ) ; log . debug ( "Removed: %s" , addedRemoved . get ( "removed" ) ) ; } }
public void test() { try { jwt = jwt . substring ( jwt . indexOf ( ' ' ) + 1 ) ; Jws < Claims > claims = Jwts . parser ( ) . setSigningKey ( ShiroJwtProvider . SIGNING_KEY ) . parseClaimsJws ( jwt ) ; String user = claims . getBody ( ) . getSubject ( ) ; return Strings . hasText ( user ) ? Optional . of ( user ) : Optional . empty ( ) ; } catch ( JwtException | IllegalArgumentException e ) { LOG . warn ( "Could not parse user from JWT" ) ; LOG . debug ( "exception" , e ) ; } }
public void test() { try { jwt = jwt . substring ( jwt . indexOf ( ' ' ) + 1 ) ; Jws < Claims > claims = Jwts . parser ( ) . setSigningKey ( ShiroJwtProvider . SIGNING_KEY ) . parseClaimsJws ( jwt ) ; String user = claims . getBody ( ) . getSubject ( ) ; LOG . debug ( "Validating user {}" , user ) ; return Strings . hasText ( user ) ? Optional . of ( user ) : Optional . empty ( ) ; } catch ( JwtException | IllegalArgumentException e ) { LOG . error ( "Failed validating JWT {} from {}" , jwt , WebUtils . toHttp ( req ) . getRemoteAddr ( ) ) ; } }
private void updateTwcsDtcsGcSeconds ( ) throws Exception { logger . info ( "Updating gc_grace_seconds on TWCS/DTCS tables..." ) ; code_block = ForStatement ; logger . info ( "updating gc_grace_seconds on TWCS/DTCS tables - complete" ) ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { if ( datasetWithEtag . isDeleted ( ) ) { logger . debug ( "sending status deleted" ) ; return new ResponseEntity < > ( headers , HttpStatus . GONE ) ; } else-if ( datasetWithEtag . isNotFound ( ) ) { logger . debug ( "sending status 404 NOT FOUND" ) ; return new ResponseEntity < > ( headers , HttpStatus . NOT_FOUND ) ; } else-if ( datasetWithEtag . isForbidden ( ) ) { int statusCode = HttpStatus . FORBIDDEN . value ( ) ; Optional < AclEvalResult > accResult = WonAclRequestHelper . getWonAclEvaluationContext ( request ) . getCombinedResults ( ) ; code_block = IfStatement ; HttpStatus status = HttpStatus . valueOf ( statusCode ) ; logger . debug ( "sending status {}" , status ) ; return new ResponseEntity < > ( headers , status ) ; } else-if ( datasetWithEtag . isChanged ( ) ) { logger . debug ( "sending status 200 OK" ) ; return new ResponseEntity < > ( datasetWithEtag . getData ( ) , headers , HttpStatus . OK ) ; } else { logger . debug ( "sending status 304 NOT MODIFIED" ) ; return new ResponseEntity < > ( headers , HttpStatus . NOT_MODIFIED ) ; } }
public void test() { if ( datasetWithEtag . isDeleted ( ) ) { logger . debug ( "sending status 410 GONE" ) ; return new ResponseEntity < > ( headers , HttpStatus . GONE ) ; } else-if ( datasetWithEtag . isNotFound ( ) ) { logger . debug ( "sending status 404 not found" ) ; return new ResponseEntity < > ( headers , HttpStatus . NOT_FOUND ) ; } else-if ( datasetWithEtag . isForbidden ( ) ) { int statusCode = HttpStatus . FORBIDDEN . value ( ) ; Optional < AclEvalResult > accResult = WonAclRequestHelper . getWonAclEvaluationContext ( request ) . getCombinedResults ( ) ; code_block = IfStatement ; HttpStatus status = HttpStatus . valueOf ( statusCode ) ; logger . debug ( "sending status {}" , status ) ; return new ResponseEntity < > ( headers , status ) ; } else-if ( datasetWithEtag . isChanged ( ) ) { logger . debug ( "sending status 200 OK" ) ; return new ResponseEntity < > ( datasetWithEtag . getData ( ) , headers , HttpStatus . OK ) ; } else { logger . debug ( "sending status 304 NOT MODIFIED" ) ; return new ResponseEntity < > ( headers , HttpStatus . NOT_MODIFIED ) ; } }
public void test() { if ( datasetWithEtag . isDeleted ( ) ) { logger . debug ( "sending status 410 GONE" ) ; return new ResponseEntity < > ( headers , HttpStatus . GONE ) ; } else-if ( datasetWithEtag . isNotFound ( ) ) { logger . debug ( "sending status 404 NOT FOUND" ) ; return new ResponseEntity < > ( headers , HttpStatus . NOT_FOUND ) ; } else-if ( datasetWithEtag . isForbidden ( ) ) { logger . debug ( "sending status 403" ) ; int statusCode = HttpStatus . FORBIDDEN . value ( ) ; Optional < AclEvalResult > accResult = WonAclRequestHelper . getWonAclEvaluationContext ( request ) . getCombinedResults ( ) ; code_block = IfStatement ; HttpStatus status = HttpStatus . valueOf ( statusCode ) ; return new ResponseEntity < > ( headers , status ) ; } else-if ( datasetWithEtag . isChanged ( ) ) { logger . debug ( "sending status 200 OK" ) ; return new ResponseEntity < > ( datasetWithEtag . getData ( ) , headers , HttpStatus . OK ) ; } else { logger . debug ( "sending status 304 NOT MODIFIED" ) ; return new ResponseEntity < > ( headers , HttpStatus . NOT_MODIFIED ) ; } }
public void test() { if ( datasetWithEtag . isDeleted ( ) ) { logger . debug ( "sending status 410 GONE" ) ; return new ResponseEntity < > ( headers , HttpStatus . GONE ) ; } else-if ( datasetWithEtag . isNotFound ( ) ) { logger . debug ( "sending status 404 NOT FOUND" ) ; return new ResponseEntity < > ( headers , HttpStatus . NOT_FOUND ) ; } else-if ( datasetWithEtag . isForbidden ( ) ) { int statusCode = HttpStatus . FORBIDDEN . value ( ) ; Optional < AclEvalResult > accResult = WonAclRequestHelper . getWonAclEvaluationContext ( request ) . getCombinedResults ( ) ; code_block = IfStatement ; HttpStatus status = HttpStatus . valueOf ( statusCode ) ; logger . debug ( "sending status {}" , status ) ; return new ResponseEntity < > ( headers , status ) ; } else-if ( datasetWithEtag . isChanged ( ) ) { logger . debug ( "sending status 304 NOT MODIFIED" ) ; return new ResponseEntity < > ( datasetWithEtag . getData ( ) , headers , HttpStatus . OK ) ; } else { logger . debug ( "sending status 304 NOT MODIFIED" ) ; return new ResponseEntity < > ( headers , HttpStatus . NOT_MODIFIED ) ; } }
public void test() { if ( datasetWithEtag . isDeleted ( ) ) { logger . debug ( "sending status 410 GONE" ) ; return new ResponseEntity < > ( headers , HttpStatus . GONE ) ; } else-if ( datasetWithEtag . isNotFound ( ) ) { logger . debug ( "sending status 404 NOT FOUND" ) ; return new ResponseEntity < > ( headers , HttpStatus . NOT_FOUND ) ; } else-if ( datasetWithEtag . isForbidden ( ) ) { int statusCode = HttpStatus . FORBIDDEN . value ( ) ; Optional < AclEvalResult > accResult = WonAclRequestHelper . getWonAclEvaluationContext ( request ) . getCombinedResults ( ) ; code_block = IfStatement ; HttpStatus status = HttpStatus . valueOf ( statusCode ) ; logger . debug ( "sending status {}" , status ) ; return new ResponseEntity < > ( headers , status ) ; } else-if ( datasetWithEtag . isChanged ( ) ) { logger . debug ( "sending status 200 OK" ) ; return new ResponseEntity < > ( datasetWithEtag . getData ( ) , headers , HttpStatus . OK ) ; } else { logger . debug ( "sending status 204 not found" ) ; return new ResponseEntity < > ( headers , HttpStatus . NOT_MODIFIED ) ; } }
@ BeforeClass public static void setUp ( ) throws Exception { log . info ( "Starting test cluster..." ) ; new File ( "target/test-logs" ) . mkdirs ( ) ; System . setProperty ( "hadoop.log.dir" , "target/test-logs" ) ; System . setProperty ( "javax.xml.parsers.SAXParserFactory" , "com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl" ) ; Configuration conf = new Configuration ( ) ; dfsCluster = new MiniDFSCluster ( conf , 1 , true , null ) ; dfsCluster . getFileSystem ( ) . makeQualified ( input ) ; dfsCluster . getFileSystem ( ) . makeQualified ( output ) ; mrCluster = new MiniMRCluster ( 1 , getFileSystem ( ) . getUri ( ) . toString ( ) , 1 ) ; code_block = ForStatement ; log . warn ( "Spun up test cluster." ) ; }
@ BeforeClass public static void setUp ( ) throws Exception { log . warn ( "Spinning up test cluster..." ) ; new File ( "target/test-logs" ) . mkdirs ( ) ; System . setProperty ( "hadoop.log.dir" , "target/test-logs" ) ; System . setProperty ( "javax.xml.parsers.SAXParserFactory" , "com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl" ) ; Configuration conf = new Configuration ( ) ; dfsCluster = new MiniDFSCluster ( conf , 1 , true , null ) ; dfsCluster . getFileSystem ( ) . makeQualified ( input ) ; dfsCluster . getFileSystem ( ) . makeQualified ( output ) ; mrCluster = new MiniMRCluster ( 1 , getFileSystem ( ) . getUri ( ) . toString ( ) , 1 ) ; log . warn ( "Spinning up test cluster." ) ; code_block = ForStatement ; }
public void test() { try { AbstractOutputWriter writer = getWriter ( ) ; code_block = IfStatement ; String [ ] tok = new String [ fileToken . length ] ; code_block = ForStatement ; code_block = ForStatement ; StringBuilder sb = new StringBuilder ( ) ; code_block = ForStatement ; writer . writeLine ( sb . toString ( ) , bundle ) ; } catch ( Exception ex ) { LOG . warn ( ex . toString ( ) ) ; throw DataChannelError . promote ( ex ) ; } }
public ModelAndView getOntologyOwlApi ( HttpServletRequest request , HttpServletResponse response ) throws Exception { String syntax ; OutputStream out = null ; InputStream is = null ; int headerBufferSize = 8096 ; syntax = request . getParameter ( "type" ) ; response . setHeader ( "Content-Type" , "application/rdf+xml" ) ; response . setContentType ( "application/rdf+xml" ) ; response . setHeader ( "Content-Disposition" , "attachment;filename=eegdatabase.owl" ) ; log . debug ( "Creating output stream" ) ; response . setStatus ( HttpServletResponse . SC_OK ) ; response . setBufferSize ( headerBufferSize ) ; log . debug ( "Sending output stream" ) ; out = response . getOutputStream ( ) ; log . debug ( "Generating OWL" ) ; is = semanticFactory . getOntologyOwlApi ( syntax ) ; copy ( is , out ) ; out . flush ( ) ; out . close ( ) ; return null ; }
public ModelAndView getOntologyOwlApi ( HttpServletRequest request , HttpServletResponse response ) throws Exception { log . debug ( "Controller for transforming POJO object to resources of semantic web" ) ; String syntax ; OutputStream out = null ; InputStream is = null ; int headerBufferSize = 8096 ; syntax = request . getParameter ( "type" ) ; response . setHeader ( "Content-Type" , "application/rdf+xml" ) ; response . setContentType ( "application/rdf+xml" ) ; response . setHeader ( "Content-Disposition" , "attachment;filename=eegdatabase.owl" ) ; response . setStatus ( HttpServletResponse . SC_OK ) ; response . setBufferSize ( headerBufferSize ) ; out = response . getOutputStream ( ) ; log . debug ( "Generating OWL" ) ; is = semanticFactory . getOntologyOwlApi ( syntax ) ; copy ( is , out ) ; out . flush ( ) ; out . close ( ) ; log . debug ( "Model created: " + syntax ) ; return null ; }
public ModelAndView getOntologyOwlApi ( HttpServletRequest request , HttpServletResponse response ) throws Exception { log . debug ( "Controller for transforming POJO object to resources of semantic web" ) ; String syntax ; OutputStream out = null ; InputStream is = null ; int headerBufferSize = 8096 ; syntax = request . getParameter ( "type" ) ; response . setHeader ( "Content-Type" , "application/rdf+xml" ) ; response . setContentType ( "application/rdf+xml" ) ; response . setHeader ( "Content-Disposition" , "attachment;filename=eegdatabase.owl" ) ; log . debug ( "Creating output stream" ) ; response . setStatus ( HttpServletResponse . SC_OK ) ; response . setBufferSize ( headerBufferSize ) ; out = response . getOutputStream ( ) ; log . debug ( "Sending output stream of type {}" , syntax ) ; is = semanticFactory . getOntologyOwlApi ( syntax ) ; copy ( is , out ) ; out . flush ( ) ; out . close ( ) ; return null ; }
public void partition ( ) { disableDiscovery ( ) ; installNewView ( ) ; assertPartitionFormed ( ) ; log . trace ( "New views installed" ) ; log . trace ( "New views installed" ) ; }
public void partition ( ) { log . trace ( "Partition forming" ) ; disableDiscovery ( ) ; installNewView ( ) ; assertPartitionFormed ( ) ; log . trace ( "PartitionFormatted" ) ; }
public void test() { try { IPage newPage = null ; code_block = IfStatement ; return this . getDtoBuilder ( ) . convert ( newPage ) ; } catch ( ValidationGenericException e ) { throw e ; } catch ( ApsSystemException e ) { logger . error ( "error in update page status" , e ) ; throw new RestServerError ( "error in update page status" , e ) ; } }
public void test() { if ( cachedRowSet . size ( ) == 0 ) { return Optional . empty ( ) ; } else-if ( cachedRowSet . size ( ) > 1 ) { LOGGER . warn ( MessageFormat . format ( "Found multiple implementations for ScriptDesignTrace {0}. Returning first implementation" , scriptName ) ) ; } }
private void prepareUsername ( PWDProtectExtensionMessage msg ) throws CryptoException { LOGGER . debug ( "Preparing client public key" ) ; Config config = chooser . getConfig ( ) ; EllipticCurve curve = CurveFactory . getCurve ( config . getDefaultPWDProtectGroup ( ) ) ; Point generator = curve . getBasePoint ( ) ; Point serverPublicKey = config . getDefaultServerPWDProtectPublicKey ( ) ; HKDFAlgorithm hkdfAlgorithm ; code_block = IfStatement ; BigInteger clientPublicKey = curve . mult ( config . getDefaultServerPWDProtectRandomSecret ( ) , generator ) . getX ( ) . getData ( ) ; BigInteger sharedSecret = curve . mult ( config . getDefaultServerPWDProtectRandomSecret ( ) , serverPublicKey ) . getX ( ) . getData ( ) ; byte [ ] key = HKDFunction . expand ( hkdfAlgorithm , HKDFunction . extract ( hkdfAlgorithm , null , ArrayConverter . bigIntegerToByteArray ( sharedSecret ) ) , new byte [ 0 ] , curve . getModulus ( ) . bitLength ( ) / Bits . IN_A_BYTE ) ; byte [ ] ctrKey = Arrays . copyOfRange ( key , 0 , key . length / 2 ) ; byte [ ] macKey = Arrays . copyOfRange ( key , key . length / 2 , key . length ) ; SivMode AES_SIV = new SivMode ( ) ; byte [ ] protectedUsername = AES_SIV . encrypt ( ctrKey , macKey , chooser . getClientPWDUsername ( ) . getBytes ( ) ) ; msg . setUsername ( ArrayConverter . concatenate ( ArrayConverter . bigIntegerToByteArray ( clientPublicKey , curve . getModulus ( ) . bitLength ( ) / Bits . IN_A_BYTE , true ) , protectedUsername ) ) ; LOGGER . debug ( "Username: " + ArrayConverter . bytesToHexString ( msg . getUsername ( ) ) ) ; }
private void prepareUsername ( PWDProtectExtensionMessage msg ) throws CryptoException { Config config = chooser . getConfig ( ) ; EllipticCurve curve = CurveFactory . getCurve ( config . getDefaultPWDProtectGroup ( ) ) ; Point generator = curve . getBasePoint ( ) ; Point serverPublicKey = config . getDefaultServerPWDProtectPublicKey ( ) ; HKDFAlgorithm hkdfAlgorithm ; code_block = IfStatement ; BigInteger clientPublicKey = curve . mult ( config . getDefaultServerPWDProtectRandomSecret ( ) , generator ) . getX ( ) . getData ( ) ; BigInteger sharedSecret = curve . mult ( config . getDefaultServerPWDProtectRandomSecret ( ) , serverPublicKey ) . getX ( ) . getData ( ) ; byte [ ] key = HKDFunction . expand ( hkdfAlgorithm , HKDFunction . extract ( hkdfAlgorithm , null , ArrayConverter . bigIntegerToByteArray ( sharedSecret ) ) , new byte [ 0 ] , curve . getModulus ( ) . bitLength ( ) / Bits . IN_A_BYTE ) ; LOGGER . debug ( "Username encryption key: " + ArrayConverter . bytesToHexString ( key ) ) ; byte [ ] ctrKey = Arrays . copyOfRange ( key , 0 , key . length / 2 ) ; byte [ ] macKey = Arrays . copyOfRange ( key , key . length / 2 , key . length ) ; SivMode AES_SIV = new SivMode ( ) ; byte [ ] protectedUsername = AES_SIV . encrypt ( ctrKey , macKey , chooser . getClientPWDUsername ( ) . getBytes ( ) ) ; LOGGER . debug ( "Client public key: " + ArrayConverter . bytesToHexString ( clientPublicKey ) ) ; msg . setUsername ( ArrayConverter . concatenate ( ArrayConverter . bigIntegerToByteArray ( clientPublicKey , curve . getModulus ( ) . bitLength ( ) / Bits . IN_A_BYTE , true ) , protectedUsername ) ) ; }
@ MCRCommand ( syntax = "build google sitemap" , help = "Creates the google sitemap(s) in webapps directory." , order = 10 ) public static void buildSitemap ( ) throws Exception { LOGGER . debug ( "Build Google sitemap request..." ) ; final long start = System . currentTimeMillis ( ) ; File webappBaseDir = new File ( MCRConfiguration2 . getStringOrThrow ( "MCR.WebApplication.basedir" ) ) ; MCRGoogleSitemapCommon common = new MCRGoogleSitemapCommon ( webappBaseDir ) ; common . removeSitemapFiles ( ) ; int number = common . checkSitemapFile ( ) ; LOGGER . debug ( "Build Google number of URL files {}." , Integer . toString ( number ) ) ; code_block = IfStatement ; LOGGER . debug ( "Google sitemap request took {}ms." , System . currentTimeMillis ( ) - start ) ; }
@ MCRCommand ( syntax = "build google sitemap" , help = "Creates the google sitemap(s) in webapps directory." , order = 10 ) public static void buildSitemap ( ) throws Exception { LOGGER . debug ( "Build Google sitemap start." ) ; final long start = System . currentTimeMillis ( ) ; File webappBaseDir = new File ( MCRConfiguration2 . getStringOrThrow ( "MCR.WebApplication.basedir" ) ) ; MCRGoogleSitemapCommon common = new MCRGoogleSitemapCommon ( webappBaseDir ) ; common . removeSitemapFiles ( ) ; int number = common . checkSitemapFile ( ) ; LOGGER . debug ( "Build Google sitemap common: {}" , number ) ; code_block = IfStatement ; LOGGER . debug ( "Google sitemap request took {}ms." , System . currentTimeMillis ( ) - start ) ; }
@ MCRCommand ( syntax = "build google sitemap" , help = "Creates the google sitemap(s) in webapps directory." , order = 10 ) public static void buildSitemap ( ) throws Exception { LOGGER . debug ( "Build Google sitemap start." ) ; final long start = System . currentTimeMillis ( ) ; File webappBaseDir = new File ( MCRConfiguration2 . getStringOrThrow ( "MCR.WebApplication.basedir" ) ) ; MCRGoogleSitemapCommon common = new MCRGoogleSitemapCommon ( webappBaseDir ) ; common . removeSitemapFiles ( ) ; int number = common . checkSitemapFile ( ) ; LOGGER . debug ( "Build Google number of URL files {}." , Integer . toString ( number ) ) ; code_block = IfStatement ; LOGGER . debug ( "Build Google sitemap complete." ) ; }
public void test() { try { return renderService . fromMetadata ( byteArrayInputStream , RenderFormat . JSON ) ; } catch ( IOException ex ) { String message = "Exception occurred while trying to do JSON conversion while parsing class list map" ; throw new MetadataSyncServiceException ( message , ex ) ; } catch ( Exception ex ) { LOG . error ( ex . getMessage ( ) , ex ) ; throw new MetadataSyncServiceException ( ex . getMessage ( ) , ex ) ; } }
protected void runIngestTest ( long defaultRunTime , long keysPerServerPerIter , int colsPerKey , int recordSize , int writeThreads , int readThreads ) throws Exception { LOG . info ( "Running test" ) ; LOG . info ( "Cluster size:" + util . getHBaseClusterInterface ( ) . getClusterMetrics ( ) . getLiveServerMetrics ( ) . size ( ) ) ; long start = System . currentTimeMillis ( ) ; String runtimeKey = String . format ( RUN_TIME_KEY , this . getClass ( ) . getSimpleName ( ) ) ; long runtime = util . getConfiguration ( ) . getLong ( runtimeKey , defaultRunTime ) ; long startKey = 0 ; long numKeys = getNumKeys ( keysPerServerPerIter ) ; code_block = WhileStatement ; }
public void test() { if ( 0 != ret ) { String errorMsg = "Verification failed with error code " + ret ; log . info ( errorMsg ) ; Threads . sleep ( 1000 * 60 ) ; ret = loadTool . run ( getArgsForLoadTestTool ( "-read" , String . format ( "100:%d" , readThreads ) , startKey , numKeys ) ) ; code_block = IfStatement ; Assert . fail ( errorMsg ) ; } }
public void test() { if ( 0 != ret ) { log . error ( "Expected value not found: " + value ) ; } }
public void test() { try { uninstall = DOMCommandsParser . parse ( df ) ; } catch ( Exception e ) { logger . warn ( e . getMessage ( ) ) ; } }
public void test() { try { Map < String , Object > parameters = new HashMap < > ( ) ; parameters . put ( "contentId" , contentId ) ; return getUniqueResult ( "FROM InsightsContentConfig CC WHERE CC.contentId = :contentId " , InsightsContentConfig . class , parameters ) ; } catch ( Exception e ) { log . error ( e ) ; throw e ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { code_block = TryStatement ;  } catch ( final IOException e ) { LOGGER . warn ( "Failed to close the streams." , e ) ; } finally { code_block = TryStatement ;  stopped = true ; } }
public void test() { try { listener . gotUserListMemberships ( lists ) ; } catch ( Exception e ) { logger . warn ( "Exception at getUserListMemberships" , e ) ; } }
@ Override public boolean matches ( final ConditionContext context , final AnnotatedTypeMetadata metadata ) { final boolean enabled = SQS . equals ( RpcStrategy . getRpcStrategy ( ) ) ; LOGGER . debug ( "Enabled {}" , enabled ) ; return enabled ; }
public void test() { if ( kuduClient . tableExists ( userTable ) ) { kuduClient . deleteTable ( userTable ) ; logger . info ( "Deleted table {}" , userTable ) ; } }
public void test() { if ( options . basicIndexOptions . getPartitionStrategy ( ) . equals ( PartitionStrategy . NONE ) ) { LOGGER . warn ( "Partition strategy is deprecated and will use default partition strategy" ) ; } }
public void test() { try { return solrRequester . getByQuery ( query , limit , offset ) ; } catch ( SolrServerException e ) { log . error ( e . getMessage ( ) ) ; throw new SearchException ( e ) ; } }
public void test() { if ( queryProfile . isDetailProfileEnable ( ) ) { String stepName = getQueryStepName ( ) ; StreamingQueryProfile . ProfileStep profileStep = queryProfile . finishStep ( stepName ) ; profileStep . stepInfo ( "scan_count" , String . valueOf ( scanCnt ) ) . stepInfo ( "filter_count" , String . valueOf ( filterCnt ) ) ; logger . info ( "Started the query : {}" , stepName ) ; } }
public void attachDirty ( StgMPersbezTxt instance ) { log . debug ( "attaching dirty StgMPersbezTxt instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { lifeCycles = checkAndUpdateLifeCycle ( lifeCycles , type ) ; validateNotEmpty ( "entityName" , entity ) ; validateInstanceFilterByClause ( filterBy ) ; Entity entityObject = EntityUtil . getEntity ( type , entity ) ; AbstractWorkflowEngine wfEngine = getWorkflowEngine ( entityObject ) ; return getInstanceResultSubset ( wfEngine . getRunningInstances ( entityObject , lifeCycles ) , filterBy , orderBy , sortOrder , offset , numResults , "" ) ; } catch ( Throwable e ) { LOG . error ( "Failed to get running instances for running instances" , e ) ; throw FalconWebException . newAPIException ( e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceApplicationBrandServiceUtil . class , "deleteCommerceApplicationBrand" , _deleteCommerceApplicationBrandParameterTypes1 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , commerceApplicationBrandId ) ; code_block = TryStatement ;  } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( ActiveMQRALogger . LOGGER . isTraceEnabled ( ) ) { ActiveMQRALogger . LOGGER . trace ( "execute()" ) ; } }
public void test() { try { RenderRequest renderRequest = ( RenderRequest ) httpServletRequest . getAttribute ( JavaConstants . JAVAX_PORTLET_REQUEST ) ; CommerceVirtualOrderItemEditDisplayContext commerceVirtualOrderItemEditDisplayContext = new CommerceVirtualOrderItemEditDisplayContext ( _commerceOrderService , _commerceOrderItemService , getCommerceVirtualOrderItem ( httpServletRequest ) , _dlAppService , _itemSelector , renderRequest ) ; httpServletRequest . setAttribute ( WebKeys . PORTLET_DISPLAY_CONTEXT , commerceVirtualOrderItemEditDisplayContext ) ; } catch ( PortalException portalException ) { _log . error ( portalException , portalException ) ; } }
public void test() { if ( ! ( exception instanceof BulkValidationException ) ) { LoggerFactory . getLogger ( BulkSaveService . class ) . error ( "Bulk save failed" , exception ) ; } }
public void test() { if ( iterationCondition == null ) { return null ; } else-if ( iterationCondition instanceof Text ) { return iterationCondition . toString ( ) ; } else { LOGGER . warn ( MessageFormat . format ( this . getActionExecution ( ) . getAction ( ) . getType ( ) + " does not accept {0} as type for iteration condition" , iterationCondition . getClass ( ) ) ) ; return iterationCondition . toString ( ) ; } }
public void test() { if ( message . getError ( ) != null ) { LOG . error ( "Error: " + message . getError ( ) ) ; return ; } }
public void test() { try { log . trace ( "Bouncing a message stanza." ) ; final Message errorResponse = message . createCopy ( ) ; errorResponse . setError ( PacketError . Condition . service_unavailable ) ; errorResponse . setFrom ( message . getTo ( ) ) ; errorResponse . setTo ( message . getFrom ( ) ) ; route ( errorResponse ) ; } catch ( Exception e ) { log . error ( "An exception occurred while trying to bounce a message stanza." , e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceDiscountRelServiceUtil . class , "getCPDefinitionsByCommerceDiscountId" , _getCPDefinitionsByCommerceDiscountIdParameterTypes12 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , commerceDiscountId , name , languageId , start , end ) ; Object returnObj = null ; code_block = TryStatement ;  return ( java . util . List < com . liferay . commerce . discount . model . CommerceDiscountRel > ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( debug ) { logger . debug ( "Expected error: " + e . getMessage ( ) ) ; } }
public void test() { try { GenPolynomial < C > g = e1 . baseGcd ( P , S ) ; code_block = IfStatement ; return g ; } catch ( PreemptingException e ) { throw new RuntimeException ( "GCDProxy e1 pre " + e ) ; } catch ( Exception e ) { logger . info ( "GCDProxy e1 " + e ) ; logger . info ( "GCDProxy P = " + P ) ; logger . info ( "GCDProxy S = " + S ) ; throw new RuntimeException ( "GCDProxy e1 " + e ) ; } }
public void test() { if ( debug ) { logger . debug ( "Expected error: " + e . getMessage ( ) ) ; } }
public void test() { try { GenPolynomial < C > g = e2 . baseGcd ( P , S ) ; code_block = IfStatement ; return g ; } catch ( PreemptingException e ) { throw new RuntimeException ( "GCDProxy e2 pre " + e ) ; } catch ( Exception e ) { logger . info ( "GCDProxy e2 " + e ) ; logger . info ( "GCDProxy P = " + P ) ; logger . info ( "GCDProxy S = " + S ) ; throw new RuntimeException ( "GCDProxy e2 " + e ) ; } }
public void test() { try { g = pool . invokeAny ( cs ) ; } catch ( InterruptedException ignored ) { logger . info ( "InterruptedException " + ignored ) ; Thread . currentThread ( ) . interrupt ( ) ; } catch ( ExecutionException e ) { logger . info ( "ExecutionException " + e ) ; Thread . currentThread ( ) . interrupt ( ) ; } }
public void test() { try { g = pool . invokeAny ( cs ) ; } catch ( InterruptedException ignored ) { logger . info ( "InterruptedException " + ignored ) ; Thread . currentThread ( ) . interrupt ( ) ; } catch ( ExecutionException e ) { logger . info ( "ExecutionException " + e ) ; Thread . currentThread ( ) . interrupt ( ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { try { final String stringValue = getConvertedPropertyValue ( securityContext , graphObject , key ) ; final byte [ ] bytes = stringValue . getBytes ( Charset . forName ( "utf-8" ) ) ; dst . put ( bytes ) ; return bytes . length ; } catch ( FrameworkException fex ) { logger . warn ( "" , fex ) ; } }
@ Test public void testMovingTarget ( ) throws Exception { logger . debug ( "testMovingTarget" ) ; EntityManager em = app . getEntityManager ( ) ; assertNotNull ( em ) ; Map < String , Object > properties = new LinkedHashMap < String , Object > ( ) code_block = "" ; ; Entity user = em . create ( "user" , properties ) ; assertNotNull ( user ) ; app . waitForQueueDrainAndRefreshIndex ( ) ; final double lat = 37.776753 ; final double lon = - 122.407846 ; Query query = Query . fromQL ( "select * where location within 100 of " + lat + "," + lon ) ; Results listResults = em . searchCollection ( em . getApplicationRef ( ) , "users" , query ) ; assertEquals ( 1 , listResults . size ( ) ) ; updatePos ( em , user , 37.428526 , - 122.140916 ) ; listResults = em . searchCollection ( em . getApplicationRef ( ) , "users" , query ) ; assertEquals ( 0 , listResults . size ( ) ) ; em . delete ( user ) ; }
public void test() { if ( key != null ) { code_block = IfStatement ; } else { LOGGER . error ( "Key not found in memory" ) ; } }
public void test() { try { con = DbConnectionManager . getConnection ( ) ; pstmt = con . prepareStatement ( LOAD_PROPERTIES ) ; pstmt . setString ( 1 , name ) ; rs = pstmt . executeQuery ( ) ; code_block = WhileStatement ; } catch ( SQLException sqle ) { Log . error ( sqle . getMessage ( ) , sqle ) ; } finally { DbConnectionManager . closeConnection ( rs , pstmt , con ) ; } }
@ Override public void run ( ) { int count = 0 ; long start = System . currentTimeMillis ( ) ; running = true ; code_block = WhileStatement ; logger . info ( "Shutdown hook started." ) ; }
public void test() { try { Thread . sleep ( 10 ) ; } catch ( InterruptedException e ) { logger . warn ( "" , e ) ; } }
public void test() { if ( count % 100 == 0 ) { double rate = 1000.0 * count / ( System . currentTimeMillis ( ) - start ) ; log . info ( "Yolo Classification Rate : {}" , rate ) ; } }
public void test() { try { count ++ ; lastResult = dl4j . classifyImageMiniEXCEPTION ( lastImage , confidence ) ; code_block = IfStatement ; invoke ( "publishClassification" , lastResult ) ; if ( lastResult != null && lastResult . size ( ) > 0 ) log . info ( formatResultString ( lastResult ) ) ; } catch ( IOException e ) { log . error ( "Unable to publish image!" , e ) ; } }
public void handle ( PortletContainerException e ) { log . error ( "Unable to remove portlet container" , e ) ; }
@ Test public void retrievePutConcurrent ( ) throws Exception { LOG . info ( "Starting retrievePutConcurrent" ) ; final File f = createFile ( 0 , loader , cache , folder ) ; File f2 = copyToFile ( randomStream ( 1 , 4 * 1024 ) , folder . newFile ( ) ) ; ListeningExecutorService executorService = MoreExecutors . listeningDecorator ( Executors . newFixedThreadPool ( 2 ) ) ; closer . register ( new ExecutorCloser ( executorService , 5 , TimeUnit . MILLISECONDS ) ) ; CountDownLatch thread1Start = new CountDownLatch ( 1 ) ; SettableFuture < File > future1 = retrieveThread ( executorService , ID_PREFIX + 0 , cache , thread1Start ) ; CountDownLatch thread2Start = new CountDownLatch ( 1 ) ; SettableFuture < Boolean > future2 = putThread ( executorService , 1 , f2 , cache , thread2Start ) ; thread1Start . countDown ( ) ; thread2Start . countDown ( ) ; future1 . get ( ) ; future2 . get ( ) ; LOG . info ( "Async tasks finished" ) ; assertCacheIfPresent ( 0 , cache , f ) ; assertCacheIfPresent ( 1 , cache , copyToFile ( randomStream ( 1 , 4 * 1024 ) , folder . newFile ( ) ) ) ; assertCacheStats ( cache , 2 , 8 * 1024 , 1 , 1 ) ; LOG . info ( "Finished retrievePutConcurrent" ) ; }
@ Test public void retrievePutConcurrent ( ) throws Exception { LOG . info ( "Started retrievePutConcurrent" ) ; final File f = createFile ( 0 , loader , cache , folder ) ; File f2 = copyToFile ( randomStream ( 1 , 4 * 1024 ) , folder . newFile ( ) ) ; LOG . info ( "Finished retrievePutConcurrent" ) ; ListeningExecutorService executorService = MoreExecutors . listeningDecorator ( Executors . newFixedThreadPool ( 2 ) ) ; closer . register ( new ExecutorCloser ( executorService , 5 , TimeUnit . MILLISECONDS ) ) ; CountDownLatch thread1Start = new CountDownLatch ( 1 ) ; SettableFuture < File > future1 = retrieveThread ( executorService , ID_PREFIX + 0 , cache , thread1Start ) ; CountDownLatch thread2Start = new CountDownLatch ( 1 ) ; SettableFuture < Boolean > future2 = putThread ( executorService , 1 , f2 , cache , thread2Start ) ; thread1Start . countDown ( ) ; thread2Start . countDown ( ) ; future1 . get ( ) ; future2 . get ( ) ; assertCacheIfPresent ( 0 , cache , f ) ; assertCacheIfPresent ( 1 , cache , copyToFile ( randomStream ( 1 , 4 * 1024 ) , folder . newFile ( ) ) ) ; assertCacheStats ( cache , 2 , 8 * 1024 , 1 , 1 ) ; LOG . info ( "Finished retrievePutConcurrent" ) ; }
@ Test public void retrievePutConcurrent ( ) throws Exception { LOG . info ( "Started retrievePutConcurrent" ) ; final File f = createFile ( 0 , loader , cache , folder ) ; File f2 = copyToFile ( randomStream ( 1 , 4 * 1024 ) , folder . newFile ( ) ) ; ListeningExecutorService executorService = MoreExecutors . listeningDecorator ( Executors . newFixedThreadPool ( 2 ) ) ; closer . register ( new ExecutorCloser ( executorService , 5 , TimeUnit . MILLISECONDS ) ) ; CountDownLatch thread1Start = new CountDownLatch ( 1 ) ; SettableFuture < File > future1 = retrieveThread ( executorService , ID_PREFIX + 0 , cache , thread1Start ) ; CountDownLatch thread2Start = new CountDownLatch ( 1 ) ; SettableFuture < Boolean > future2 = putThread ( executorService , 1 , f2 , cache , thread2Start ) ; thread1Start . countDown ( ) ; thread2Start . countDown ( ) ; future1 . get ( ) ; future2 . get ( ) ; LOG . info ( "Async tasks finished" ) ; assertCacheIfPresent ( 0 , cache , f ) ; assertCacheIfPresent ( 1 , cache , copyToFile ( randomStream ( 1 , 4 * 1024 ) , folder . newFile ( ) ) ) ; assertCacheStats ( cache , 2 , 8 * 1024 , 1 , 1 ) ; LOG . info ( "Completed retrievePutConcurrent" ) ; }
public void test() { try { registerResources ( pluginAnnotation , classLoader ) ; } catch ( IOException e ) { logger . error ( "Unable to register resources for plugin " + pluginAnnotation , e ) ; return false ; } }
public void test() { try { return catalogsCache . get ( "" , ( ) code_block = LoopStatement ; ) ; } catch ( ExecutionException executionException ) { logger . error ( "Unable to retrieve catalog from catalog" , executionException ) ; return delegate . getCatalogs ( ) ; } }
public void test() { try { url = new URL ( posterimageUrlOpt ) ; } catch ( Exception e ) { Logger . log ( e ) ; } }
public void test() { try { File coverImageFile = getWorkspace ( ) . get ( url . toURI ( ) ) ; return coverImageFile . getPath ( ) ; } catch ( NotFoundException e ) { logger . warn ( "Couldn't find cover image" , e ) ; return null ; } catch ( IOException e ) { logger . warn ( "Error getting poster image: {}" , e . getMessage ( ) ) ; return null ; } catch ( URISyntaxException e ) { logger . warn ( "Given URL '{}' is not a valid URI" , url ) ; return null ; } }
public void test() { try { File coverImageFile = getWorkspace ( ) . get ( url . toURI ( ) ) ; return coverImageFile . getPath ( ) ; } catch ( NotFoundException e ) { logger . warn ( "Poster image could not be found at '{}'" , url ) ; return null ; } catch ( IOException e ) { logger . warn ( "Failed to get poster image from '{}'" , url , e ) ; return null ; } catch ( URISyntaxException e ) { logger . warn ( "Given URL '{}' is not a valid URI" , url ) ; return null ; } }
public void test() { try { File coverImageFile = getWorkspace ( ) . get ( url . toURI ( ) ) ; return coverImageFile . getPath ( ) ; } catch ( NotFoundException e ) { logger . warn ( "Poster image could not be found at '{}'" , url ) ; return null ; } catch ( IOException e ) { logger . warn ( "Error getting poster image: {}" , e . getMessage ( ) ) ; return null ; } catch ( URISyntaxException e ) { logger . warn ( "Invalid document format: {}" , e . getMessage ( ) ) ; return null ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceOrderServiceUtil . class , "getPendingCommerceOrdersCount" , _getPendingCommerceOrdersCountParameterTypes21 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , commerceAccountId , keywords ) ; Object returnObj = null ; code_block = TryStatement ;  return ( ( Integer ) returnObj ) . intValue ( ) ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { try { return terminalInfoAvp . getGrouped ( ) . getAvp ( Avp . TGPP2_MEID ) != null ; } catch ( AvpDataException ex ) { logger . debug ( "Failure trying to obtain (Terminal-Information) AVP value" , ex ) ; } }
public void test() { try { return Class . forName ( interfaceName ) ; } catch ( ClassNotFoundException e1 ) { logger . warn ( "Failed to load class {}" , interfaceName , e1 ) ; } }
public void test() { if ( value != null ) { log . debug ( "Using cached attribute: " + attributeName ) ; } else { value = element . getAttribute ( attributeName ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
private ReportResult generateReport ( Path userCsvFolder , Profile profile , DashBoard dash , long now ) throws Exception { int fetchCount = ( int ) report . reportType . getFetchCount ( report . granularityType ) ; long startFrom = now - TimeUnit . DAYS . toMillis ( report . reportType . getDuration ( ) ) ; startFrom = ( startFrom / report . granularityType . period ) * report . granularityType . period ; Path output = Paths . get ( userCsvFolder . toString ( ) + ".zip" ) ; boolean hasData = generateReport ( output , profile , dash , fetchCount , startFrom ) ; log . info ( "Generated data: " + hasData ) ; code_block = IfStatement ; return ReportResult . NO_DATA ; }
public void test() { if ( ! createDirectories ( destination ) ) { logger . error ( "Unable to create directory [" + destination . getAbsolutePath ( ) + "]" ) ; return false ; } }
public void test() { if ( ERROR . equals ( when ) && log . isErrorEnabled ( ) ) { log . error ( sb . toString ( ) , when ) ; } else-if ( log . isTraceEnabled ( ) ) { log . trace ( sb . toString ( ) ) ; } }
public void test() { if ( ERROR . equals ( when ) && log . isErrorEnabled ( ) ) { log . error ( sb . toString ( ) , t ) ; } else-if ( log . isTraceEnabled ( ) ) { log . trace ( sb . toString ( ) ) ; } }
public void test() { try { out = new PrintWriter ( new BufferedWriter ( new FileWriter ( logFile , true ) ) ) ; out . println ( sb . toString ( ) ) ; } catch ( Exception e ) { log . warn ( "Failed to write to file " + logFile , e ) ; } finally { IOUtils . closeQuietly ( out ) ; } }
public void test() { try { StringBuilder sb = new StringBuilder ( ) ; appendToLine ( sb , Long . toString ( context . getEvaluationId ( ) ) ) ; appendToLine ( sb , DateUtil . formatDate ( new Date ( logTime ) , "yyyy-MM-dd HH:mm:ss.S" ) ) ; appendToLine ( sb , StringUtils . repeat ( ">" , context . getEvaluationLevel ( ) ) ) ; appendToLine ( sb , when ) ; appendToLine ( sb , eventCode ) ; code_block = IfStatement ; sb . append ( message ) ; code_block = IfStatement ; code_block = IfStatement ; } catch ( Exception e ) { LOGGER . error ( "Exception in getLogTime" , e ) ; } }
private static String [ ] expandFileNames ( String fileName ) throws IOException { code_block = IfStatement ; code_block = IfStatement ; fileName = new File ( fileName ) . getCanonicalPath ( ) ; LOG . debug ( "Scanning for {}" , fileName ) ; DirectoryScanner scanner = new DirectoryScanner ( ) ; scanner . setIncludes ( new String [ ] code_block = "" ; ) ; scanner . scan ( ) ; return scanner . getIncludedFiles ( ) ; }
public void test() { if ( IS_DEBUG ) { LOG . debug ( "CSV1 created" ) ; } }
public void test() { while ( true ) { Thread . sleep ( 1000 ) ; logger . info ( "Pump row key: " , output . getReceivedTuples ( ) ) ; logger . info ( "Tuple row key: " , output . getReceivedTuples ( ) ) ; code_block = IfStatement ; code_block = IfStatement ; } }
public void testOrFilter2 ( ) throws Exception { logger . info ( "executing test case testOrFilter2" ) ; String req = "{\"filter\":{\"or\":[{\"term\":{\"color\":\"blue\",\"_noOptimize\":false}},{\"term\":{\"color\":\"red\",\"_noOptimize\":false}}]}}" ; JSONObject res = search ( new JSONObject ( req ) ) ; assertEquals ( "numhits is wrong" , 3264 , res . getInt ( "numhits" ) ) ; }
public void test() { try { value = parseItemValue ( item . getObject ( ) , item . getClassType ( ) ) ; } catch ( Exception ex ) { logger . error ( "Error parsing item value" , ex ) ; continue ; } }
public void test() { if ( ! cacheEnabled ( region ) ) { Cache cache = buildCache ( region , cacheProperties ) ; code_block = IfStatement ; } else { CacheManager . logger . warn ( Messages . getInstance ( ) . getString ( "CacheManager.WARN_0002_REGION_ALREADY_EXIST" , region ) ) ; } }
public void test() { if ( cacheEnabled ) { code_block = IfStatement ; } else { CacheManager . logger . warn ( Messages . getInstance ( ) . getString ( "CacheManager.WARN_0001_CACHE_NOT_ENABLED" ) ) ; } }
@ Override public void addIP ( LogicalDevice logicalDevice , IPProtocolEndpoint ip ) throws CapabilityException { log . info ( "Start of addIP call" ) ; if ( ( ip . getIPv4Address ( ) != null ) && ( ip . getSubnetMask ( ) != null ) && ! ( ip . getIPv4Address ( ) . isEmpty ( ) ) && ! ( ip . getSubnetMask ( ) . isEmpty ( ) ) ) addIPv4 ( logicalDevice , ip ) ; else-if ( ip . getIPv6Address ( ) != null && ! ip . getIPv6Address ( ) . isEmpty ( ) ) addIPv6 ( logicalDevice , ip ) ; else throw new CapabilityException ( "IP address not set." ) ; log . info ( "End of addIP call" ) ; }
@ Override public void addIP ( LogicalDevice logicalDevice , IPProtocolEndpoint ip ) throws CapabilityException { log . info ( "Start of addIP call" ) ; if ( ( ip . getIPv4Address ( ) != null ) && ( ip . getSubnetMask ( ) != null ) && ! ( ip . getIPv4Address ( ) . isEmpty ( ) ) && ! ( ip . getSubnetMask ( ) . isEmpty ( ) ) ) addIPv4 ( logicalDevice , ip ) ; else-if ( ip . getIPv6Address ( ) != null && ! ip . getIPv6Address ( ) . isEmpty ( ) ) addIPv6 ( logicalDevice , ip ) ; else throw new CapabilityException ( "IP address not set." ) ; log . info ( "End of addIP call" ) ; }
public void test() { if ( group != null && "api" . equals ( group . getName ( ) ) ) { LOGGER . info ( "Group {} is a api" , group . getName ( ) ) ; return true ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { try { writeToLog ( new Checkpoint ( txnId ) ) ; } catch ( final JournalException e ) { LOG . error ( "Failed to write checkpoint {}." , txnId , e ) ; } }
public void test() { try { channel . close ( ) ; } catch ( final IOException e ) { log . error ( "Failed to close channel" , e ) ; } }
public void test() { try { String taskId = taskFile . getName ( ) ; TaskAnnouncement taskAnnouncement = jsonMapper . readValue ( taskFile , TaskAnnouncement . class ) ; code_block = IfStatement ; } catch ( IOException ex ) { log . warn ( "Failed to parse task announce" , ex ) ; } }
public void test() { if ( ! completedTasks . isEmpty ( ) ) { LOG . info ( "Cleanup completed tasks for completed task {}" , completedTasks . size ( ) ) ; } }
public void test() { try { checkNodes ( expCnt ) ; return true ; } catch ( AssertionError e ) { log . warn ( "EXPECTED " + e . toString ( ) ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( DLFileEntryServiceUtil . class , "deleteFileEntry" , _deleteFileEntryParameterTypes8 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , fileEntryId ) ; code_block = TryStatement ;  } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { server . stop ( ) ; } catch ( Exception ex ) { LOGGER . log ( Level . SEVERE , null , ex ) ; } }
public static List < CoordinatorAction > getProcessInstanceListFromAllBundles ( OozieClient oozieClient , String processName , EntityType entityType ) throws OozieClientException { List < CoordinatorAction > list = new ArrayList < > ( ) ; final List < String > bundleIds = OozieUtil . getBundles ( oozieClient , processName , entityType ) ; LOGGER . info ( "Bundles: " + bundleIds ) ; code_block = ForStatement ; String coordId = OozieUtil . getLatestCoordinatorID ( oozieClient , processName , entityType ) ; LOGGER . info ( "default coordID: " + coordId ) ; return list ; }
public static List < CoordinatorAction > getProcessInstanceListFromAllBundles ( OozieClient oozieClient , String processName , EntityType entityType ) throws OozieClientException { List < CoordinatorAction > list = new ArrayList < > ( ) ; final List < String > bundleIds = OozieUtil . getBundles ( oozieClient , processName , entityType ) ; LOGGER . info ( "bundle size for process is " + bundleIds . size ( ) ) ; code_block = ForStatement ; String coordId = OozieUtil . getLatestCoordinatorID ( oozieClient , processName , entityType ) ; LOGGER . info ( "getProcessInstanceListFromAllBundles: " + list ) ; return list ; }
public void test() { try { return some ( IOUtils . toString ( is ) ) ; } catch ( Exception e ) { LOG . warn ( e . getMessage ( ) , e ) ; return none ( ) ; } }
public void test() { if ( fieldExpType == null ) { logger . debug ( "fieldExpType is null" ) ; } }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; } catch ( ClassCastException ex ) { logger . error ( ex . getMessage ( ) , ex ) ; } }
public void test() { if ( mountFolderPath != null ) { final Path relativePathParent = relativePath . getParent ( ) ; code_block = IfStatement ; } else { logger . warn ( "Failed to mount folder: {}" , path ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Failed to clean up." , e ) ; } }
public void test() { try { admin . createTopicIfNotExists ( topic ) ; ProjectTopicName topicName = ProjectTopicName . of ( pubSubSettings . getProjectId ( ) , topic ) ; Publisher publisher = Publisher . newBuilder ( topicName ) . setCredentialsProvider ( pubSubSettings . getCredentialsProvider ( ) ) . build ( ) ; publisherMap . put ( topic , publisher ) ; return publisher ; } catch ( IOException e ) { logger . error ( e . getMessage ( ) , e ) ; throw new RuntimeException ( "Failed to create Publisher for the topic." , e ) ; } }
public void test() { try { code_block = IfStatement ; acceptComponentRequestCall = true ; RequestLifeCycle . begin ( this ) ; super . start ( ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; } finally { RequestLifeCycle . end ( ) ; } }
public void test() { if ( Boolean . TRUE . equals ( getAttribute ( BrooklynNode . WEB_CONSOLE_ACCESSIBLE ) ) ) { queueShutdownTask ( ) ; queueWaitExitTask ( ) ; } else { LOG . debug ( "WebSocket interface " + getAttribute ( BrooklynNode . WEB_CONSOLE_ACCESSIBLE ) ) ; } }
public void test() { if ( _log . isInfoEnabled ( ) ) { _log . info ( StringBundler . concat ( "Removing " , companyId , " from company " , companyId , " to " , company . getCompanyId ( ) , " using " , company . getCompanyId ( ) ) ) ; } }
public void test() { if ( _log . isInfoEnabled ( ) ) { _log . info ( StringBundler . concat ( "Removing " , companyId , " from company " , companyId , " to " , company . getCompanyId ( ) , " using " , company . getCompanyId ( ) ) ) ; } }
public void test() { try { outputPlans = initialOptimizer . optimize ( appModel , MMtoOptModelTransformer . transformModel ( suitableCloudOffer ) , benchmarkPlatforms , NUMBER_OF_PLANS_GENERATED , HYSTERESIS_PROPORTION ) ; previousPlans = outputPlans ; } catch ( Exception exc ) { log . error ( "Error optimizing the initial deployment" ) ; outputPlans = initialOptimizer . optimize ( appModel , suitableCloudOffer , benchmarkPlatforms , NUMBER_OF_PLANS_GENERATED , HYSTERESIS_PROPORTION ) ; previousPlans = outputPlans ; } catch ( Error E ) { log . error ( "Error optimizing the initial deployment" ) ; E . printStackTrace ( ) ; } }
public void test() { try { outputPlans = initialOptimizer . optimize ( appModel , MMtoOptModelTransformer . transformModel ( suitableCloudOffer ) , benchmarkPlatforms , NUMBER_OF_PLANS_GENERATED , HYSTERESIS_PROPORTION ) ; previousPlans = outputPlans ; } catch ( Exception exc ) { log . warn ( "Optimizer did not work in its expected input. Exception name was " + exc . getClass ( ) . getName ( ) + " Trying with the assumption of former versions of Input " ) ; outputPlans = initialOptimizer . optimize ( appModel , suitableCloudOffer , benchmarkPlatforms , NUMBER_OF_PLANS_GENERATED , HYSTERESIS_PROPORTION ) ; previousPlans = outputPlans ; } catch ( Error E ) { log . warn ( "Optimizer did not work in its expected input. Exception name was " + E . getMessage ( ) ) ; E . printStackTrace ( ) ; } }
public void method ( String string ) { LOGGER . info ( string ) ; }
@ Override public void elementOpened ( OpenAction action ) { logger . debug ( "Element opened" ) ; }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getCapabilities()" ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Refreshing." ) ; } }
public void test() { if ( this . proxyRequests ) { log . info ( "Will proxy non-getMap requests to backend." ) ; } else { log . info ( "Will NOT proxy non-getMap requests to backend." ) ; } }
public void test() { if ( this . proxyRequests ) { log . info ( "Will proxy requests to backend that are not getmap or getcapabilities." ) ; } else { log . info ( "Will not proxy requests to backend." ) ; } }
public void test() { try { sizeofByType . putIfAbsent ( genericType , SIZE_RECURSIVE_MARKER ) ; Sizeof newSizeof = computeSizeof ( type , genericType ) ; return sizeofByType . compute ( genericType , ( key , value ) -> value == null || value == SIZE_RECURSIVE_MARKER ? newSizeof : value ) ; } catch ( Exception | StackOverflowError t ) { logger . warn ( "unable to compute size of " + objectHeaderSize , t ) ; return Sizeof . constant ( objectHeaderSize ) ; } }
private OrgDisambiguatedEntity createDisambiguatedOrg ( RDFOrganization organization ) { String orgType = getOrgType ( organization ) ; Iso3166Country country = StringUtils . isNotBlank ( organization . country ) ? Iso3166Country . fromValue ( organization . country ) : null ; LOGGER . debug ( "createDisambiguated org {}" , orgType ) ; OrgDisambiguatedEntity orgDisambiguatedEntity = new OrgDisambiguatedEntity ( ) ; orgDisambiguatedEntity . setName ( organization . name ) ; orgDisambiguatedEntity . setCountry ( country == null ? null : country . name ( ) ) ; orgDisambiguatedEntity . setCity ( organization . city ) ; orgDisambiguatedEntity . setRegion ( organization . stateCode ) ; orgDisambiguatedEntity . setOrgType ( orgType ) ; orgDisambiguatedEntity . setSourceId ( organization . doi ) ; orgDisambiguatedEntity . setSourceUrl ( organization . doi ) ; code_block = IfStatement ; code_block = IfStatement ; orgDisambiguatedEntity . setSourceType ( OrgDisambiguatedSourceType . FUNDREF . name ( ) ) ; orgDisambiguatedManager . createOrgDisambiguated ( orgDisambiguatedEntity ) ; return orgDisambiguatedEntity ; }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getCapabilities()" ) ; } }
public void test() { try { paths . add ( pathElement . toURI ( ) . toURL ( ) ) ; } catch ( MalformedURLException e ) { LOG . warn ( "Path '{}' is malformed: {}" , pathElement , e . getMessage ( ) ) ; } }
public void test() { if ( cache . getCacheConfiguration ( ) . jmxStatistics ( ) . enabled ( ) ) { return cache . getAdvancedCache ( ) . getStats ( ) ; } else { log . info ( "Cache is disabled." ) ; return new UnavailableStats ( - 1 ) ; } }
public void test() { try { Cache < Object , Object > cache = cacheManager . getCache ( cacheName ) ; code_block = IfStatement ; } catch ( Exception e ) { LOG . error ( "Unable to get cache" , e ) ; return new UnavailableStats ( - 2 ) ; } }
@ Override public void process ( Channel channel , Command command ) { Preconditions . checkArgument ( CommandType . TASK_EXECUTE_ACK == command . getType ( ) , String . format ( "invalid command type : %s" , command . getType ( ) ) ) ; TaskExecuteAckCommand taskAckCommand = JSONUtils . parseObject ( command . getBody ( ) , TaskExecuteAckCommand . class ) ; taskInstanceCacheManager . cacheTaskInstance ( taskAckCommand ) ; String workerAddress = ChannelUtils . toAddress ( channel ) . getAddress ( ) ; ExecutionStatus ackStatus = ExecutionStatus . of ( taskAckCommand . getStatus ( ) ) ; TaskResponseEvent taskResponseEvent = TaskResponseEvent . newAck ( ackStatus , taskAckCommand . getStartTime ( ) , workerAddress , taskAckCommand . getExecutePath ( ) , taskAckCommand . getLogPath ( ) , taskAckCommand . getTaskInstanceId ( ) , channel ) ; LOG . debug ( "Received ack from {}" , taskResponseEvent ) ; taskResponseService . addResponse ( taskResponseEvent ) ; }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { SearchExecutor executor = new SearchExecutor ( ) ; executor . setBaseDn ( this . userSearchBaseDn ) ; result = executor . search ( cf , filter ) . getResult ( ) ; log . info ( "User query successful: {}" , result ) ; } catch ( LdapException e ) { log . error ( "Error running {}" , filter , e ) ; log . error ( "Error running {}" , filter ) ; throw new IllegalStateException ( e ) ; } }
public void test() { try { SearchExecutor executor = new SearchExecutor ( ) ; executor . setBaseDn ( this . userSearchBaseDn ) ; log . info ( "Running search '{}'" , filter ) ; result = executor . search ( cf , filter ) . getResult ( ) ; log . info ( "User query successful: {}" , result ) ; } catch ( LdapException e ) { log . error ( "Error while executing query" , e ) ; throw new IllegalStateException ( e ) ; } }
public void test() { try { new LdifWriter ( writer ) . write ( result ) ; } catch ( IOException e ) { log . error ( "" , e ) ; throw new IllegalStateException ( e ) ; } }
@ Override public String exportAsLdif ( @ NonNull Account account ) { log . info ( "Exporting personal user data for account {}" , account . getUid ( ) ) ; ConnectionConfig connConfig = new ConnectionConfig ( this . ldapUrl ) ; connConfig . setConnectionInitializer ( new BindConnectionInitializer ( this . ldapUserName , new Credential ( this . ldapPassword ) ) ) ; ConnectionFactory cf = new DefaultConnectionFactory ( connConfig ) ; SearchResult result ; final String filter = String . format ( "(uid=%s)" , account . getUid ( ) ) ; code_block = TryStatement ;  StringWriter writer = new StringWriter ( ) ; code_block = TryStatement ;  String ldifContents = writer . toString ( ) ; log . info ( "Export finished" ) ; return ldifContents ; }
public void test() { try { logger . info ( "Starting to fetch segment files" ) ; Path path = new Path ( JobHelper . getURIFromSegment ( segment . getSegment ( ) ) ) ; logger . info ( "Fetch segment files from [%s]" , path ) ; File dir = FileUtils . createTempDir ( ) ; tmpSegmentDirs . add ( dir ) ; logger . info ( "Locally storing fetched segment at [%s]" , dir ) ; JobHelper . unzipNoGuava ( path , context . getConfiguration ( ) , dir , context , null ) ; logger . info ( "finished fetching segment files" ) ; QueryableIndex index = HadoopDruidIndexerConfig . INDEX_IO . loadIndex ( dir ) ; indexes . add ( index ) ; numRows += index . getNumRows ( ) ; return new WindowedStorageAdapter ( new QueryableIndexStorageAdapter ( index ) , segment . getInterval ( ) ) ; } catch ( IOException ex ) { throw new RuntimeException ( ex ) ; } }
public void test() { try { logger . info ( "Getting storage path for segment [%s]" , segment . getSegment ( ) . getId ( ) ) ; Path path = new Path ( JobHelper . getURIFromSegment ( segment . getSegment ( ) ) ) ; logger . info ( "Using temp dir: [%s]" , path ) ; File dir = FileUtils . createTempDir ( ) ; tmpSegmentDirs . add ( dir ) ; logger . info ( "Locally storing fetched segment at [%s]" , dir ) ; JobHelper . unzipNoGuava ( path , context . getConfiguration ( ) , dir , context , null ) ; logger . info ( "finished fetching segment files" ) ; QueryableIndex index = HadoopDruidIndexerConfig . INDEX_IO . loadIndex ( dir ) ; indexes . add ( index ) ; numRows += index . getNumRows ( ) ; return new WindowedStorageAdapter ( new QueryableIndexStorageAdapter ( index ) , segment . getInterval ( ) ) ; } catch ( IOException ex ) { throw new RuntimeException ( ex ) ; } }
public void test() { try { logger . info ( "Getting storage path for segment [%s]" , segment . getSegment ( ) . getId ( ) ) ; Path path = new Path ( JobHelper . getURIFromSegment ( segment . getSegment ( ) ) ) ; logger . info ( "Fetch segment files from [%s]" , path ) ; File dir = FileUtils . createTempDir ( ) ; tmpSegmentDirs . add ( dir ) ; JobHelper . unzipNoGuava ( path , context . getConfiguration ( ) , dir , context , null ) ; logger . info ( "finished fetching segment files" ) ; QueryableIndex index = HadoopDruidIndexerConfig . INDEX_IO . loadIndex ( dir ) ; indexes . add ( index ) ; numRows += index . getNumRows ( ) ; logger . info ( "finished reading segment files" ) ; return new WindowedStorageAdapter ( new QueryableIndexStorageAdapter ( index ) , segment . getInterval ( ) ) ; } catch ( IOException ex ) { throw new RuntimeException ( ex ) ; } }
public void test() { try { logger . info ( "Getting storage path for segment [%s]" , segment . getSegment ( ) . getId ( ) ) ; Path path = new Path ( JobHelper . getURIFromSegment ( segment . getSegment ( ) ) ) ; logger . info ( "Fetch segment files from [%s]" , path ) ; File dir = FileUtils . createTempDir ( ) ; tmpSegmentDirs . add ( dir ) ; logger . info ( "Locally storing fetched segment at [%s]" , dir ) ; JobHelper . unzipNoGuava ( path , context . getConfiguration ( ) , dir , context , null ) ; QueryableIndex index = HadoopDruidIndexerConfig . INDEX_IO . loadIndex ( dir ) ; indexes . add ( index ) ; numRows += index . getNumRows ( ) ; logger . info ( "Using index for segment [%s]" , segment . getSegment ( ) . getId ( ) ) ; return new WindowedStorageAdapter ( new QueryableIndexStorageAdapter ( index ) , segment . getInterval ( ) ) ; } catch ( IOException ex ) { throw new RuntimeException ( ex ) ; } }
public void test() { try { return keycloakclient . isUserExist ( authzToken . getAccessToken ( ) , gatewayId , username ) ; } catch ( Exception ex ) { String msg = "Error while checking if user account exists, reason: " + ex . getMessage ( ) ; logger . error ( msg , ex ) ; throw new IamAdminServicesException ( msg ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { if ( logError ) { _log . error ( webDAVException , webDAVException ) ; } else-if ( _log . isWarnEnabled ( ) ) { _log . warn ( webDAVException , webDAVException ) ; } }
public void test() { if ( logError ) { _log . error ( webDAVException , webDAVException ) ; } else-if ( _log . isWarnEnabled ( ) ) { _log . warn ( webDAVException . getMessage ( ) ) ; } }
public void test() { try { code_block = IfStatement ; WebDAVStorage storage = getStorage ( httpServletRequest ) ; code_block = IfStatement ; code_block = IfStatement ; PermissionChecker permissionChecker = null ; String remoteUser = httpServletRequest . getRemoteUser ( ) ; code_block = IfStatement ; MethodFactory methodFactory = storage . getMethodFactory ( ) ; Method method = methodFactory . create ( httpServletRequest ) ; code_block = TryStatement ;  } catch ( Exception exception ) { _log . error ( exception , exception ) ; } finally { httpServletResponse . setStatus ( status ) ; code_block = IfStatement ; } }
@ Override public void connectionClosed ( final IOException e ) { LOGGER . info ( "Connection closed" , e ) ; this . iec60870ConnectionRegistry . unregisterConnection ( this . connection ) ; }
public void test() { try { addBranchIfNeeded ( clientDn ) ; permission . setDn ( getDn ( clientDn , permission . getTicket ( ) ) ) ; ldapEntryManager . persist ( permission ) ; } catch ( Exception e ) { logger . error ( "Could not create client permission" , e ) ; } }
public void test() { try { SipProvider p = null ; Boolean isIpv6 = false ; InetAddress address = InetAddress . getByName ( messageChannel . getHost ( ) ) ; code_block = IfStatement ; ResponseEvent event = new ResponseEvent ( new BalancerAppContent ( p , isIpv6 ) , null , null , response ) ; balancerRunner . balancerContext . forwarder . processResponse ( event ) ; } catch ( Exception e ) { logger . error ( "Failed to forward response" , e ) ; return false ; } }
public void enforce ( IAgreement agreement , Date since , boolean isLastExecution ) { final Date now = new Date ( ) ; logger . debug ( "Evaluating agreement" ) ; checkInitialized ( true ) ; Map < IGuaranteeTerm , IMetricsRetrieverV2 . RetrievalItem > retrievalItems = buildRetrievalItems ( agreement , since , now , isLastExecution ) ; Map < IGuaranteeTerm , List < IMonitoringMetric > > metricsMap ; code_block = IfStatement ; enforce ( agreement , metricsMap ) ; }
public void test() { try { String vmIp = getFirstReachableAddress ( node , setup ) ; int port = node . getLoginPort ( ) ; inferredHostAndPort = HostAndPort . fromParts ( vmIp , port ) ; } catch ( Exception e ) { s_logger . debug ( "Unable to determine if host " + node . getHost ( ) + ":" + e . toString ( ) ) ; } }
public void test() { if ( isWindows ( node , setup ) ) { LOG . warn ( "Failed to connect to host " + node + "; skipping" ) ; } else { HostAndPort hostAndPortToUse = sshHostAndPort . isPresent ( ) ? sshHostAndPort . get ( ) : inferredHostAndPort ; code_block = TryStatement ;  } }
public void test() { try { return Maybe . of ( getHostnameAws ( hostAndPortToUse , userCredentials . get ( ) , setup ) ) ; } catch ( Exception e ) { LOG . warn ( "Failed to get hostnameAws for host {}" , hostAndPortToUse , e ) ; } }
public void test() { try { application = new AccessInternalMain ( ACCESS_CONF , AccessInternalResourceImplTest . class , null ) ; application . start ( ) ; RestAssured . port = port ; RestAssured . basePath = ACCESS_RESOURCE_URI ; LOGGER . debug ( "Beginning tests" ) ; } catch ( final VitamApplicationServerException e ) { LOGGER . error ( e ) ; throw new IllegalStateException ( "Cannot start the Access Application Server" , e ) ; } }
public void test() { try { application = new AccessInternalMain ( ACCESS_CONF , AccessInternalResourceImplTest . class , null ) ; application . start ( ) ; RestAssured . port = port ; RestAssured . basePath = ACCESS_RESOURCE_URI ; LOGGER . debug ( "Beginning tests" ) ; } catch ( final VitamApplicationServerException e ) { LOGGER . error ( e ) ; throw new IllegalStateException ( "Cannot start the Access Application Server" , e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( DLAppServiceUtil . class , "checkOutFileEntry" , _checkOutFileEntryParameterTypes14 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , fileEntryId , owner , expirationTime , serviceContext ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . portal . kernel . repository . model . FileEntry ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( t == null ) { logger . debug ( msg ) ; } else { logger . debug ( msg , t ) ; } }
public void test() { if ( t == null ) { logger . debug ( msg ) ; } else { logger . debug ( msg , t ) ; } }
public void test() { if ( t == null ) { logger . info ( msg ) ; } else { logger . info ( msg , t ) ; } }
public void test() { if ( t == null ) { logger . info ( msg ) ; } else { logger . warn ( msg , t ) ; } }
public void test() { if ( t == null ) { logger . warn ( msg ) ; } else { logger . warn ( msg , t ) ; } }
public void test() { if ( t == null ) { logger . warn ( msg ) ; } else { logger . warn ( msg , t ) ; } }
public void test() { if ( t == null ) { logger . error ( msg ) ; } else { logger . error ( msg , t ) ; } }
public void test() { if ( t == null ) { logger . error ( msg ) ; } else { logger . error ( msg , t ) ; } }
@ Override public void disposeModel ( ) { logger . debug ( "Disposing model" ) ; model = null ; }
public void test() { try { processor . processTimeout ( entry ) ; } catch ( Exception e ) { LOG . error ( "process sync request error" , e ) ; } }
public void test() { try { String targetNode = this . getTargetNode ( ) ; Boolean open = this . getOpen ( ) ; code_block = IfStatement ; } catch ( Throwable t ) { _logger . error ( "error in viewPreview" , t ) ; } }
public static void inoutdemo ( int in1 , int [ ] inout1 , int [ ] out1 ) { logger . info ( "inoutdemo" ) ; inout1 [ 0 ] = 1 ; out1 [ 0 ] = 2 ; }
@ Test public void testInsertEmptyWhere ( ) throws Exception { logger . debug ( "executing testInsertEmptyWhere" ) ; StringBuilder update = new StringBuilder ( ) ; update . append ( getNamespaceDeclarations ( ) ) ; update . append ( "INSERT code_block = "" ; WHERE code_block = "" ;
@ Test @ Atomic public void test09 ( ) { TxIntrospector txIntrospector = FenixFramework . getTransaction ( ) . getTxIntrospector ( ) ; printTest ( "Create a new book and modify its 1-* relation with '" + LITTLE + "' twice\n\t" + "(New: [ '" + ECLIPSE + "']; DM: []; M: ['" + LITTLE + "']; RCL: ['PublisherWithBooks'])" ) ; VampireBook eclipse = createEclipse ( txIntrospector ) ; Publisher little = getPublisherByName ( LITTLE ) ; eclipse . setPublisher ( little ) ; little . addPublishedBook ( eclipse ) ; assertFalse ( txIntrospector . getModifiedObjects ( ) . contains ( eclipse ) ) ; assertFalse ( txIntrospector . getDirectlyModifiedObjects ( ) . contains ( eclipse ) ) ; assertTrue ( txIntrospector . getModifiedObjects ( ) . contains ( little ) ) ; assertFalse ( txIntrospector . getDirectlyModifiedObjects ( ) . contains ( little ) ) ; logger . trace ( txIntrospector . toString ( ) ) ; }
public void test() { try { lucenePartitionRepositoryManager . computeRepository ( bucketId ) ; } catch ( PrimaryBucketException | AlreadyClosedException e ) { LOGGER . error ( e . getMessage ( ) , e ) ; } }
@ ModelAttribute ( "researchGroupTitle" ) private String fillResearchGroupTitleForExperiment ( @ RequestParam ( "experimentId" ) String idString ) { int experimentId = Integer . parseInt ( idString ) ; Experiment experiment = ( Experiment ) experimentDao . read ( experimentId ) ; logger . info ( "Gave group title is " + experiment . getResearchGroup ( ) . getTitle ( ) ) ; return experiment . getResearchGroup ( ) . getTitle ( ) ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceCurrencyServiceUtil . class , "setActive" , _setActiveParameterTypes9 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , commerceCurrencyId , active ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . commerce . currency . model . CommerceCurrency ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( number != allocatedNumber ) { log . warn ( "The slot " + this . getName ( ) + " has already been allocated." ) ; } }
@ ResponseStatus ( HttpStatus . OK ) @ RequestMapping ( value = "/bundles/{bundleId}/uninstallconfig" , method = RequestMethod . POST ) public void uninstallBundleWithConfig ( @ PathVariable long bundleId ) throws BundleException { logger . info ( "uninstalling bundle with id {}" , bundleId ) ; moduleAdminService . uninstallBundle ( bundleId , true ) ; }
public void test() { try { String prefix = CONFIG_PROVIDERS_CONFIG + "." + entry . getKey ( ) + CONFIG_PROVIDERS_PARAM ; Map < String , ? > configProperties = configProviderProperties ( prefix , providerConfigProperties ) ; ConfigProvider provider = Utils . newInstance ( entry . getValue ( ) , ConfigProvider . class ) ; provider . configure ( configProperties ) ; configProviderInstances . put ( entry . getKey ( ) , provider ) ; } catch ( ClassNotFoundException e ) { LOG . error ( "Invalid config:" + entry . getValue ( ) , e ) ; throw new ConfigException ( "Invalid config:" + entry . getValue ( ) + " ClassNotFoundException exception occurred" , e ) ; } }
public static void reset ( ) { log . debug ( "Resetting freemarker configuration" ) ; freemarkerConfig = null ; freemarkerConfig = new Configuration ( ) ; freemarkerConfig . setObjectWrapper ( new DefaultObjectWrapper ( ) ) ; freemarkerConfig . clearTemplateCache ( ) ; }
public void test() { try { code_block = WhileStatement ; message . saveChanges ( ) ; } catch ( MessagingException e ) { LOGGER . error ( "MessagingException in recoverAttachment" , e ) ; } catch ( IOException e ) { LOGGER . error ( "IOException in recoverAttachment" , e ) ; } }
public void test() { try { code_block = WhileStatement ; message . saveChanges ( ) ; } catch ( MessagingException e ) { LOGGER . error ( "MessagingException in recoverAttachment" , e ) ; } catch ( IOException e ) { LOGGER . error ( "IOException in recoverAttachment" , e ) ; } }
public void test() { if ( vm == null ) { logger . warn ( "vm " + node . getId ( ) + " is not found." ) ; } else { node . setVmMobId ( vm . getId ( ) ) ; } }
public void test() { if ( propertyMap instanceof Map ) { String excludePrefix = ServiceHelper . getContext ( ) . getPrefix ( ) ; Map < String , Object > properties = ( Map < String , Object > ) propertyMap ; Map < String , Object > filtered = properties . entrySet ( ) . stream ( ) . filter ( p -> ! ( p == null || p . getKey ( ) . startsWith ( excludePrefix ) ) ) . collect ( Collectors . toMap ( p -> p . getKey ( ) , p -> p . getValue ( ) ) ) ; ServiceHelper . getContext ( ) . addProperties ( ( Map ) propertyMap ) ; } else { LOGGER . warn ( "Skipping non-Map property: {}" , propertyMap . getClass ( ) ) ; } }
public void test() { try { pstmt . executeQuery ( ) ; } catch ( SQLException e ) { logger . error ( "SQL Exception:" , e ) ; return ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { try { fcall . sendResponse ( fb , msg , msgType , seqid ) ; } catch ( java . lang . Exception ex ) { _LOGGER . error ( "Exception writing to internal frame buffer" , ex ) ; fb . close ( ) ; } }
private void prepareDistinguishedNames ( CertificateRequestMessage msg ) { msg . setDistinguishedNames ( chooser . getConfig ( ) . getDistinguishedNames ( ) ) ; LOGGER . debug ( "DistinguishedNames: " + ArrayConverter . bytesToHexString ( msg . getDistinguishedNames ( ) . getValue ( ) ) ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { k . cancel ( ) ; } catch ( Exception e ) { LOG . warn ( "Exception while stopping kafka store" , e ) ; } }
@ AfterGroups ( groups = "sharedHBase" ) public void afterGroups ( ITestContext context ) throws Exception { code_block = IfStatement ; getClient ( context ) . close ( ) . get ( ) ; getTSO ( context ) . stopAndWait ( ) ; TestUtils . waitForSocketNotListening ( "localhost" , 1234 , 1000 ) ; LOGGER . info ( "Wait for reconnect" ) ; }
public void dataSource ( DataSource dataSource ) { ConnectionDataSourceConfig connectionConfig = new ConnectionDataSourceConfig ( dataSource ) ; connectionConfig . setDbName ( dbName ) ; connectionConfig . setEnvironment ( environment ) ; connectionConfig . setTesting ( testing ) ; logger . debug ( "connectionConfig: " + connectionConfig ) ; DBConfiguration . addConnectionConfig ( connectionConfig ) ; }
public void test() { if ( ! isFailOnConnectionTimeout ) { log . warn ( "could not connect to {}" , host ) ; } }
public void test() { try { closeableHttpClient . close ( ) ; } catch ( IOException e ) { logger . error ( "HttpClient close failed" , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { String response = caseManagementServiceBase . startCase ( containerId , caseDefId , payload , type ) ; logger . debug ( "Returning OK response with content '{}'" , response ) ; return createResponse ( response , v , Response . Status . CREATED , customHeaders ) ; } catch ( CaseDefinitionNotFoundException e ) { return notFound ( MessageFormat . format ( CASE_DEFINITION_NOT_FOUND , caseDefId , containerId ) , v , customHeaders ) ; } }
public void test() { try { helper . addTokenFromUserToJobConf ( ugi , jobConf ) ; } catch ( IOException e ) { LOG . error ( "Cannot add token from user to job conf. " , e ) ; return ; } }
public void test() { try { logTimeseriesDeleted ( user , entityId , new ArrayList < > ( keys ) , null ) ; } catch ( ThingsboardException e ) { log . error ( "Failed to log timeseries delete" , e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Failed to clean up." , e ) ; } }
public void test() { if ( result . wasAcknowledged ( ) ) { log . info ( "Removed user {} from tracks" , user ) ; } else { log . error ( "Error removing user {} from tracks: {}" , user , result ) ; } }
public void test() { if ( result . wasAcknowledged ( ) ) { log . debug ( "Removed user {} from {} tracks" , user , result . getN ( ) ) ; } else { log . warn ( "Could not remove user {} from {} tracks" , user , result . getN ( ) ) ; } }
public void test() { if ( ! tadCache . get ( deviceId ) . containsKey ( descriptor ) ) { log . trace ( "Getting TAD and shape info data buffer..." ) ; Pair < DataBuffer , DataBuffer > buffers = super . getTADOnlyShapeInfo ( array , dimension ) ; if ( buffers . getFirst ( ) != array . shapeInfoDataBuffer ( ) ) AtomicAllocator . getInstance ( ) . moveToConstant ( buffers . getFirst ( ) ) ; if ( buffers . getSecond ( ) != null ) AtomicAllocator . getInstance ( ) . moveToConstant ( buffers . getSecond ( ) ) ; tadCache . get ( deviceId ) . put ( descriptor , buffers ) ; bytes . addAndGet ( ( buffers . getFirst ( ) . length ( ) * 4 ) ) ; if ( buffers . getSecond ( ) != null ) bytes . addAndGet ( buffers . getSecond ( ) . length ( ) * 8 ) ; log . trace ( "Using TAD from cache..." ) ; } }
public void test() { if ( ! tadCache . get ( deviceId ) . containsKey ( descriptor ) ) { log . trace ( "Creating new TAD..." ) ; Pair < DataBuffer , DataBuffer > buffers = super . getTADOnlyShapeInfo ( array , dimension ) ; log . trace ( "Creating new TAD only shapes" ) ; if ( buffers . getFirst ( ) != array . shapeInfoDataBuffer ( ) ) AtomicAllocator . getInstance ( ) . moveToConstant ( buffers . getFirst ( ) ) ; if ( buffers . getSecond ( ) != null ) AtomicAllocator . getInstance ( ) . moveToConstant ( buffers . getSecond ( ) ) ; tadCache . get ( deviceId ) . put ( descriptor , buffers ) ; bytes . addAndGet ( ( buffers . getFirst ( ) . length ( ) * 4 ) ) ; if ( buffers . getSecond ( ) != null ) bytes . addAndGet ( buffers . getSecond ( ) . length ( ) * 8 ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { connection = createPooledConnection ( ) ; code_block = IfStatement ; getPoolStats ( ) . incPrefillConnect ( ) ; availableConnectionManager . addLast ( connection , false ) ; code_block = IfStatement ; return true ; } catch ( ServerConnectivityException ex ) { logger . debug ( "Can't connect to server: {}" , ex . getMessage ( ) ) ; return false ; } finally { code_block = IfStatement ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { BulkByScrollResponse bulkResponse = client . deleteByQuery ( request , RequestOptions . DEFAULT ) ; logger . info ( "Deleted all models in the '{}' index." , index ) ; } catch ( IOException e ) { throw new IndexingException ( String . format ( "Error deleting all models in the '%s' index." , index ) , e ) ; } }
public void test() { try { String filePath = base + "_" + WorkerContext . get ( ) . getWorkerAttemptId ( ) + "_" + taskIndex + "_" + currentWriteFileIndex ++ ; Path newPath = lDirAlloc . getLocalPathForWrite ( filePath , maxSizePerFile , WorkerContext . get ( ) . getConf ( ) ) ; return newPath . toUri ( ) . toString ( ) ; } catch ( Exception e ) { log . error ( "task UnInitializtionException" , e ) ; throw new IOException ( "task UnInitializtionException" , e ) ; } }
@ Test public void testStringResult ( ) throws ServiceFailureException { LOGGER . info ( "------  testStringResult  ------" ) ; ObservationDao doa = service . observations ( ) ; Observation b1 = new Observation ( "fourty two" , DATASTREAMS . get ( 0 ) ) ; doa . create ( b1 ) ; OBSERVATIONS . add ( b1 ) ; Observation found ; found = doa . find ( b1 . getId ( ) ) ; String message = "Expected result to be a String." ; Assert . assertEquals ( message , b1 . getResult ( ) , found . getResult ( ) ) ; }
public void test() { if ( ! java . util . Objects . deepEquals ( byteBufferOut , expectedByteBuffer ) ) { logger . info ( name . getMethodName ( ) + " - callback - invalid content" ) ; subscribeBroadcastWithSingleByteBufferParameterCallbackResult = false ; } else { logger . info ( name . getMethodName ( ) + " - callback - content OK" ) ; subscribeBroadcastWithSingleByteBufferParameterCallbackResult = true ; } }
public void test() { if ( ! java . util . Objects . deepEquals ( byteBufferOut , expectedByteBuffer ) ) { logger . info ( name . getMethodName ( ) + " - callback - invalid content" ) ; subscribeBroadcastWithSingleByteBufferParameterCallbackResult = false ; } else { logger . info ( name . getMethodName ( ) + " - callback - content OK" ) ; subscribeBroadcastWithSingleByteBufferParameterCallbackResult = true ; } }
public void test() { { logger . info ( name . getMethodName ( ) + " - callback - invalid" ) ; subscribeBroadcastWithSingleByteBufferParameterCallbackResult = false ; resultsAvailable . release ( ) ; } }
public void test() { try { subscriptionIdFuture = testInterfaceProxy . subscribeToBroadcastWithSingleByteBufferParameterBroadcast ( new BroadcastWithSingleByteBufferParameterBroadcastAdapter ( ) code_block = "" ; , new MulticastSubscriptionQos ( ) , partitions ) ; subscriptionId = subscriptionIdFuture . get ( 10000 ) ; logger . info ( name . getMethodName ( ) + " - subscription successful" ) ; logger . info ( name . getMethodName ( ) + " - Invoking fire method" ) ; testInterfaceProxy . methodToFireBroadcastWithSingleByteBufferParameter ( expectedByteBuffer , partitions ) ; logger . info ( name . getMethodName ( ) + " - fire method invoked" ) ; Assert . assertTrue ( name . getMethodName ( ) + " - FAILED - callback was not received in time" , resultsAvailable . tryAcquire ( 2 , TimeUnit . SECONDS ) ) ; logger . info ( name . getMethodName ( ) + " - results received" ) ; Assert . assertTrue ( name . getMethodName ( ) + " - FAILED - callback got called but received unexpected error or publication event" , subscribeBroadcastWithSingleByteBufferParameterCallbackResult ) ; code_block = TryStatement ;  } catch ( Exception e ) { fail ( name . getMethodName ( ) + " - FAILED - caught unexpected exception: " + e . getMessage ( ) ) ; } }
public void test() { try { subscriptionIdFuture = testInterfaceProxy . subscribeToBroadcastWithSingleByteBufferParameterBroadcast ( new BroadcastWithSingleByteBufferParameterBroadcastAdapter ( ) code_block = "" ; , new MulticastSubscriptionQos ( ) , partitions ) ; subscriptionId = subscriptionIdFuture . get ( 10000 ) ; logger . info ( name . getMethodName ( ) + " - subscription successful, subscriptionId = " + subscriptionId ) ; logger . info ( name . getMethodName ( ) + " - fire method invoked" ) ; testInterfaceProxy . methodToFireBroadcastWithSingleByteBufferParameter ( expectedByteBuffer , partitions ) ; logger . info ( name . getMethodName ( ) + " - fire method invoked" ) ; Assert . assertTrue ( name . getMethodName ( ) + " - FAILED - callback was not received in time" , resultsAvailable . tryAcquire ( 2 , TimeUnit . SECONDS ) ) ; logger . info ( name . getMethodName ( ) + " - results received" ) ; Assert . assertTrue ( name . getMethodName ( ) + " - FAILED - callback got called but received unexpected error or publication event" , subscribeBroadcastWithSingleByteBufferParameterCallbackResult ) ; code_block = TryStatement ;  } catch ( Exception e ) { fail ( name . getMethodName ( ) + " - FAILED - caught unexpected exception: " + e . getMessage ( ) ) ; } }
public void test() { try { subscriptionIdFuture = testInterfaceProxy . subscribeToBroadcastWithSingleByteBufferParameterBroadcast ( new BroadcastWithSingleByteBufferParameterBroadcastAdapter ( ) code_block = "" ; , new MulticastSubscriptionQos ( ) , partitions ) ; subscriptionId = subscriptionIdFuture . get ( 10000 ) ; logger . info ( name . getMethodName ( ) + " - subscription successful, subscriptionId = " + subscriptionId ) ; logger . info ( name . getMethodName ( ) + " - Invoking fire method" ) ; testInterfaceProxy . methodToFireBroadcastWithSingleByteBufferParameter ( expectedByteBuffer , partitions ) ; logger . info ( name . getMethodName ( ) + " - Waiting for callback" ) ; Assert . assertTrue ( name . getMethodName ( ) + " - FAILED - callback was not received in time" , resultsAvailable . tryAcquire ( 2 , TimeUnit . SECONDS ) ) ; logger . info ( name . getMethodName ( ) + " - results received" ) ; Assert . assertTrue ( name . getMethodName ( ) + " - FAILED - callback got called but received unexpected error or publication event" , subscribeBroadcastWithSingleByteBufferParameterCallbackResult ) ; code_block = TryStatement ;  } catch ( Exception e ) { fail ( name . getMethodName ( ) + " - FAILED - caught unexpected exception: " + e . getMessage ( ) ) ; } }
public void test() { try { subscriptionIdFuture = testInterfaceProxy . subscribeToBroadcastWithSingleByteBufferParameterBroadcast ( new BroadcastWithSingleByteBufferParameterBroadcastAdapter ( ) code_block = "" ; , new MulticastSubscriptionQos ( ) , partitions ) ; subscriptionId = subscriptionIdFuture . get ( 10000 ) ; logger . info ( name . getMethodName ( ) + " - subscription successful, subscriptionId = " + subscriptionId ) ; logger . info ( name . getMethodName ( ) + " - Invoking fire method" ) ; testInterfaceProxy . methodToFireBroadcastWithSingleByteBufferParameter ( expectedByteBuffer , partitions ) ; logger . info ( name . getMethodName ( ) + " - fire method invoked" ) ; Assert . assertTrue ( name . getMethodName ( ) + " - FAILED - callback was not received in time" , resultsAvailable . tryAcquire ( 2 , TimeUnit . SECONDS ) ) ; logger . info ( name . getMethodName ( ) + " - callback got called" ) ; Assert . assertTrue ( name . getMethodName ( ) + " - FAILED - callback got called but received unexpected error or publication event" , subscribeBroadcastWithSingleByteBufferParameterCallbackResult ) ; code_block = TryStatement ;  } catch ( Exception e ) { fail ( name . getMethodName ( ) + " - FAILED - caught unexpected exception: " + e . getMessage ( ) ) ; } }
public void test() { try { testInterfaceProxy . unsubscribeFromBroadcastWithSingleByteBufferParameterBroadcast ( subscriptionId ) ; logger . info ( name . getMethodName ( ) + " - unsubscribe successful" ) ; } catch ( Exception e ) { fail ( name . getMethodName ( ) + " - FAILED - caught unexpected exception on unsubscribe: " + e . getMessage ( ) ) ; } }
public void test() { try { validator . validate ( xmlFile ) ; report . setValid ( errorHandler . getErrors ( ) . isEmpty ( ) ) ; code_block = ForStatement ; } catch ( SAXException e ) { log . warn ( e . getMessage ( ) , e ) ; report . setValid ( false ) ; code_block = ForStatement ; } }
public void test() { try { root = new File ( new URI ( location ) ) ; code_block = IfStatement ; } catch ( Exception e ) { LOGGER . error ( "Unable to create file " + location , e ) ; } }
public void test() { try { URL url = file . toURI ( ) . toURL ( ) ; deployment . urls = new ArrayList < URL > ( ) ; ctx . deploy ( url ) ; deployment . urls . add ( url ) ; log . info ( "Deployed " + url ) ; } catch ( Exception e ) { log . error ( "Failed to deploy: " + file , e ) ; } }
public void test() { try { URL url = file . toURI ( ) . toURL ( ) ; log . info ( "Deploying external component: " + url ) ; deployment . urls = new ArrayList < URL > ( ) ; ctx . deploy ( url ) ; deployment . urls . add ( url ) ; } catch ( Exception e ) { log . error ( "Could not deploy component" , e ) ; } }
public void test() { try { return JsonHandler . getFromString ( objectList , List . class , JsonNode . class ) ; } catch ( InvalidParseOperationException e ) { logger . error ( e . getMessage ( ) , e ) ; throw new IllegalArgumentException ( e ) ; } }
public void test() { if ( e . getCause ( ) instanceof NoClassDefFoundError ) { log . warn ( "Can't find enclosing class {} in classpath" , name ) ; } else { throw e ; } }
@ Override public void start ( ) { Status status = MongoDBRiverHelper . getRiverStatus ( esClient , riverName . getName ( ) ) ; code_block = IfStatement ; code_block = IfStatement ; statusThread = EsExecutors . daemonThreadFactory ( settings . globalSettings ( ) , "mongodb_river_status:" + definition . getIndexName ( ) ) . newThread ( new StatusChecker ( this , definition , context ) ) ; statusThread . start ( ) ; LOGGER . info ( "Started!" ) ; }
public void test() { if ( status == Status . IMPORT_FAILED || status == Status . INITIAL_IMPORT_FAILED || status == Status . SCRIPT_IMPORT_FAILED || status == Status . START_FAILED ) { LOG . info ( "Skipping invalid status" ) ; return ; } }
public void test() { if ( status == Status . STOPPED ) { context . setStatus ( Status . STOPPED ) ; logger . info ( "Already started" ) ; } else { context . setStatus ( Status . START_PENDING ) ; MongoDBRiverHelper . setRiverStatus ( esClient , riverName . getName ( ) , Status . RUNNING ) ; logger . info ( "Startup pending" ) ; } }
public void test() { if ( status == Status . STOPPED ) { context . setStatus ( Status . STOPPED ) ; logger . info ( "River is currently disabled and will not be started" ) ; } else { context . setStatus ( Status . START_PENDING ) ; MongoDBRiverHelper . setRiverStatus ( esClient , riverName . getName ( ) , Status . RUNNING ) ; logger . info ( "Starting migration" ) ; } }
public void test() { if ( e instanceof SymjaMMANotFoundException ) { LOGGER . error ( e . getLocalizedMessage ( ) ) ; r = TryResult . createError ( e . getLocalizedMessage ( ) ) ; } else { e . printStackTrace ( ) ; r = TryResult . createError ( e . getLocalizedMessage ( ) ) ; } }
private void startZookeeper ( ) { logger . info ( "Starting Zookeeper client" ) ; client = CuratorUtils . newCuratorFrameworkClient ( connectString , logger ) ; client . start ( ) ; int startupTimeOutMs = Integer . parseInt ( System . getProperty ( Constants . Properties . ZK_STARTUP_TIMEOUT , "30000" ) ) ; code_block = TryStatement ;  code_block = IfStatement ; logger . info ( "CuratorFramework client started successfully" ) ; }
public void test() { try { logger . info ( "Waiting to connect to zookeeper, startupTimeout : {}" , startupTimeOutMs ) ; client . blockUntilConnected ( startupTimeOutMs , TimeUnit . MILLISECONDS ) ; } catch ( InterruptedException ex ) { logger . error ( ex . getMessage ( ) , ex ) ; } }
private void startZookeeper ( ) { logger . info ( "Starting Zookeeper" ) ; client = CuratorUtils . newCuratorFrameworkClient ( connectString , logger ) ; client . start ( ) ; logger . info ( "Curator framework start operation invoked" ) ; int startupTimeOutMs = Integer . parseInt ( System . getProperty ( Constants . Properties . ZK_STARTUP_TIMEOUT , "30000" ) ) ; code_block = TryStatement ;  code_block = IfStatement ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
protected List < NodeRef > processNodes ( final Long batchSize , Long maxNodeId , final Pair < Long , QName > recordAspectPair , Long totalNumberOfRecordsToProcess , final BufferedWriter out , final boolean attach ) { logger . info ( MESSAGE_PROCESSING_END ) ; final Long maxRecordsToProcess = totalNumberOfRecordsToProcess ; final List < NodeRef > processedNodes = new ArrayList < > ( ) ; code_block = ForStatement ; logger . info ( MESSAGE_PROCESSING_END ) ; return processedNodes ; }
public void test() { for ( Long nodeId : nodeIds ) { code_block = IfStatement ; NodeRef record = nodeDAO . getNodePair ( nodeId ) . getSecond ( ) ; String recordName = ( String ) nodeService . getProperty ( record , ContentModel . PROP_NAME ) ; logger . info ( MessageFormat . format ( MESSAGE_PROCESSING_RECORD_END_TEMPLATE , recordName ) ) ; processNode ( record ) ; logger . info ( MessageFormat . format ( MESSAGE_PROCESSING_RECORD_END_TEMPLATE , recordName ) ) ; processedNodes . add ( record ) ; code_block = IfStatement ; } }
public void test() { for ( Long nodeId : nodeIds ) { code_block = IfStatement ; NodeRef record = nodeDAO . getNodePair ( nodeId ) . getSecond ( ) ; String recordName = ( String ) nodeService . getProperty ( record , ContentModel . PROP_NAME ) ; logger . info ( MessageFormat . format ( MESSAGE_PROCESSING_RECORD_BEGIN_TEMPLATE , recordName ) ) ; processNode ( record ) ; logger . info ( MessageFormat . format ( MESSAGE_PROCESSING_RECORD_END_TEMPLATE , recordName ) ) ; processedNodes . add ( record ) ; code_block = IfStatement ; } }
protected List < NodeRef > processNodes ( final Long batchSize , Long maxNodeId , final Pair < Long , QName > recordAspectPair , Long totalNumberOfRecordsToProcess , final BufferedWriter out , final boolean attach ) { final Long maxRecordsToProcess = totalNumberOfRecordsToProcess ; final List < NodeRef > processedNodes = new ArrayList < > ( ) ; logger . info ( MESSAGE_PROCESSING_BEGIN ) ; code_block = ForStatement ; logger . info ( MESSAGE_PROCESSING_END ) ; return processedNodes ; }
public void test() { try { code_block = IfStatement ; lastOffset = offset ; return true ; } catch ( EventHubException ex ) { logger . warn ( "Failed to reset topic {}" , topic , ex ) ; return false ; } }
public void test() { if ( verbose > 0 ) { Log . info ( "\t\t" + n ) ; } }
public void test() { if ( currentEmailTemplate == null ) { LOGGER . warn ( "No email template specified" ) ; return false ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
@ GET @ Timed @ Produces ( APPLICATION_JSON_WITH_CHARSET ) public String get ( @ Context GraphManager manager , @ PathParam ( "graph" ) String graph , @ QueryParam ( "source" ) String sourceV , @ QueryParam ( "direction" ) String direction , @ QueryParam ( "label" ) String edgeLabel , @ QueryParam ( "max_depth" ) int depth , @ QueryParam ( "max_degree" ) @ DefaultValue ( DEFAULT_MAX_DEGREE ) long maxDegree , @ QueryParam ( "capacity" ) @ DefaultValue ( DEFAULT_CAPACITY ) long capacity , @ QueryParam ( "limit" ) @ DefaultValue ( DEFAULT_PATHS_LIMIT ) long limit ) { LOG . debug ( "Graph [{}] get paths" , edgeLabel ) ; Id source = VertexAPI . checkAndParseVertexId ( sourceV ) ; Directions dir = Directions . convert ( EdgeAPI . parseDirection ( direction ) ) ; HugeGraph g = graph ( manager , graph ) ; SubGraphTraverser traverser = new SubGraphTraverser ( g ) ; HugeTraverser . PathSet paths = traverser . rays ( source , dir , edgeLabel , depth , maxDegree , capacity , limit ) ; return manager . serializer ( g ) . writePaths ( "rays" , paths , false ) ; }
@ Override public void geoServerGetStyleCommand ( final org . locationtech . geowave . service . grpc . protobuf . GeoServerGetStyleCommandParametersProtos request , final StreamObserver < org . locationtech . geowave . service . grpc . protobuf . GeoWaveReturnTypesProtos . StringResponseProtos > responseObserver ) { final GeoServerGetStyleCommand cmd = new GeoServerGetStyleCommand ( ) ; final Map < FieldDescriptor , Object > m = request . getAllFields ( ) ; GeoWaveGrpcServiceCommandUtil . setGrpcToCommandFields ( m , cmd ) ; final File configFile = GeoWaveGrpcServiceOptions . geowaveConfigFile ; final OperationParams params = new ManualOperationParams ( ) ; params . getContext ( ) . put ( ConfigOptions . PROPERTIES_FILE_CONTEXT , configFile ) ; cmd . prepare ( params ) ; LOGGER . info ( "Executing GeoServerGetStyleCommand..." ) ; code_block = TryStatement ;  }
public void test() { try { final String result = cmd . computeResults ( params ) ; final StringResponseProtos resp = StringResponseProtos . newBuilder ( ) . setResponseValue ( result ) . build ( ) ; responseObserver . onNext ( resp ) ; responseObserver . onCompleted ( ) ; } catch ( final Exception e ) { LOGGER . error ( "Exception encountered executing command" , e ) ; responseObserver . onError ( e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( MDRActionServiceUtil . class , "updateAction" , _updateActionParameterTypes6 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , actionId , nameMap , descriptionMap , type , typeSettingsUnicodeProperties , serviceContext ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . mobile . device . rules . model . MDRAction ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { createQueue ( config . setTemporary ( false ) . setTransient ( false ) . setAutoCreated ( false ) . setConfigurationManaged ( true ) . setAutoCreateAddress ( true ) , false ) ; } catch ( ActiveMQQueueExistsException e ) { log . warn ( "Failed to create queue" , e ) ; } }
public void test() { try { directProducer . sendBodyAndHeader ( "Message " + i , RabbitMQConstants . ROUTING_KEY , "rk3" ) ; } catch ( CamelExecutionException e ) { failedMessages ++ ; LOG . error ( "Got exception trying to send message " + i , e ) ; } }
public void test() { if ( _log . isInfoEnabled ( ) ) { _log . info ( StringBundler . concat ( "Removing " , companyId , " from company " , companyId , " to " , company . getCompanyId ( ) , " using " , company . getCompanyId ( ) ) ) ; } }
public void test() { try { addresses = singletonList ( nominatimRestClient . exchange ( url , GET , getHttpEntity ( ) , Address . class ) . getBody ( ) ) ; } catch ( RestClientException e ) { LOG . error ( "Failed to fetch mapping addresses" , e ) ; addresses = null ; } }
public void test() { try { conn = this . getConnection ( ) ; conn . setAutoCommit ( false ) ; this . insertIdeaInstance ( ideainstance , conn ) ; this . insertIdeaInstanceGroups ( ideainstance . getCode ( ) , ideainstance . getGroups ( ) , conn ) ; conn . commit ( ) ; } catch ( Throwable t ) { this . executeRollback ( conn ) ; _logger . error ( "Error creating ideainstance" , t ) ; throw new RuntimeException ( "Error creating ideainstance" , t ) ; } finally { this . closeDaoResources ( null , stat , conn ) ; } }
public void test() { if ( tlsTestsEnabled ) { final String uri = "natsTlsAuth:test?sslContextParameters=ssl&secure=true" ; from ( uri ) . routeId ( "tls-auth" ) . bean ( natsResource , "storeMessage" ) ; LOG . info ( "TLS tests enabled, starting the TLS auth route" ) ; } else { LOG . info ( "TLS tests NOT enabled, so NOT starting the TLS auth route" ) ; } }
public void test() { if ( tlsTestsEnabled ) { LOG . info ( "TLS tests enabled so starting the TLS auth route" ) ; final String uri = "natsTlsAuth:test?sslContextParameters=ssl&secure=true" ; from ( uri ) . routeId ( "tls-auth" ) . bean ( natsResource , "storeMessage" ) ; } else { LOG . info ( "No TLS tests enabled so will not start the TLS auth route" ) ; } }
public void test() { try ( Transaction tx = ignite . transactions ( ) . txStart ( PESSIMISTIC , REPEATABLE_READ , timeout , 0 ) ) { int key1 = threadNum ; log . info ( ">>> Performs put [node=" + ( ( IgniteKernal ) ignite ) . localNode ( ) + ", tx=" + tx + ", key=" + key1 + ']' ) ; cache . put ( key1 , 0 ) ; barrier . await ( ) ; code_block = IfStatement ; tx . commit ( ) ; } catch ( Exception e ) { if ( hasCause ( e , TransactionTimeoutException . class ) ) timedOut . set ( true ) ; if ( hasCause ( e , TransactionDeadlockException . class ) ) deadlock . set ( true ) ; } }
public void test() { if ( threadNum == threads ) { log . info ( ">>> Performs sleep. [node=" + ( ( IgniteKernal ) ignite ) . localNode ( ) + ", tx=" + tx + ']' ) ; U . sleep ( timeout * 3 ) ; } else { int key2 = threadNum + 1 ; log . info ( ">>> Performs put [node=" + ( ( IgniteKernal ) ignite ) . localNode ( ) + ", tx=" + tx + ", key2=" + key2 + ']' ) ; cache . put ( key2 , 1 ) ; } }
public void test() { if ( threadNum == threads ) { log . info ( ">>> Performs sleep. [node=" + ( ( IgniteKernal ) ignite ) . localNode ( ) + ", tx=" + tx + ']' ) ; U . sleep ( timeout * 3 ) ; } else { int key2 = threadNum + 1 ; log . info ( ">>> Performs put [node=" + ( ( IgniteKernal ) ignite ) . localNode ( ) + ", tx=" + tx + ", key2=" + key2 + ']' ) ; cache . put ( key2 , 1 ) ; } }
public void test() { if ( ActiveMQRALogger . LOGGER . isTraceEnabled ( ) ) { ActiveMQRALogger . LOGGER . trace ( "execute()" ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( BookmarksEntryServiceUtil . class , "getEntry" , _getEntryParameterTypes6 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , entryId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . bookmarks . model . BookmarksEntry ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { URL url = new URL ( purlServerBaseURL + PURL_PATH + purl ) ; LOGGER . debug ( url . toString ( ) ) ; String data = "target=" + URLEncoder . encode ( target , StandardCharsets . UTF_8 ) ; data += "&maintainers=" + maintainers ; data += "&type=" + type ; LOGGER . debug ( data ) ; conn = ( HttpURLConnection ) url . openConnection ( ) ; conn . setRequestProperty ( COOKIE_HEADER_PARAM , cookie ) ; conn . setRequestMethod ( "POST" ) ; conn . setDoOutput ( true ) ; code_block = TryStatement ;  response = conn . getResponseCode ( ) ; code_block = IfStatement ; } catch ( Exception e ) { LOGGER . error ( e ) ; } finally { code_block = IfStatement ; } }
public void test() { while ( ( line = rd . readLine ( ) ) != null ) { LOG . info ( line ) ; } }
public void test() { try { URL url = new URL ( purlServerBaseURL + PURL_PATH + purl ) ; LOGGER . debug ( url . toString ( ) ) ; String data = "target=" + URLEncoder . encode ( target , StandardCharsets . UTF_8 ) ; data += "&maintainers=" + maintainers ; data += "&type=" + type ; LOGGER . debug ( data ) ; conn = ( HttpURLConnection ) url . openConnection ( ) ; conn . setRequestProperty ( COOKIE_HEADER_PARAM , cookie ) ; conn . setRequestMethod ( "POST" ) ; conn . setDoOutput ( true ) ; code_block = TryStatement ;  response = conn . getResponseCode ( ) ; code_block = IfStatement ; } catch ( Exception e ) { LOGGER . error ( "error" , e ) ; } finally { code_block = IfStatement ; } }
@ Override public void onCommunicationFailure ( BinaryLogClient client , Exception ex ) { LOGGER . log ( Level . WARNING , "Connection lost." , ex ) ; logReaderState ( ) ; code_block = TryStatement ;  BinlogReader . this . failed ( ex ) ; }
public void test() { try { client . disconnect ( ) ; } catch ( final Exception e ) { logger . warn ( "Couldn't disconnect FTP Client" , e ) ; } }
public void test() { try { searchResult . close ( ) ; } catch ( IOException e ) { logger . warn ( "Failed to close the search result" , e ) ; } }
@ Test public void testApiHTML ( ) throws Exception { MockHttpServletResponse response = getAsMockHttpServletResponse ( "ogc/images/api?f=text/html" , 200 ) ; assertEquals ( "text/html" , response . getContentType ( ) ) ; String html = response . getContentAsString ( ) ; LOGGER . info ( html ) ; assertThat ( html , containsString ( "<link rel=\"icon\" type=\"image/png\" href=\"http://localhost:8080/geoserver/swagger-ui/favicon-32x32.png\" sizes=\"32x32\" />" ) ) ; assertThat ( html , containsString ( "<link rel=\"icon\" type=\"image/png\" href=\"http://localhost:8080/geoserver/swagger-ui/favicon-16x16.png\" sizes=\"16x16\" />" ) ) ; assertThat ( html , containsString ( "<script src=\"http://localhost:8080/geoserver/swagger-ui/swagger-ui-bundle.js\">" ) ) ; assertThat ( html , containsString ( "<script src=\"http://localhost:8080/geoserver/swagger-ui/swagger-ui-standalone-preset.js\">" ) ) ; assertThat ( html , containsString ( "url: \"http://localhost:8080/geoserver/ogc/images/api?f=application%2Fvnd.oai.openapi%2Bjson%3Bversion%3D3.0\"" ) ) ; }
public void test() { try { Map < byte [ ] , byte [ ] > dataAll = jedisClient . hgetAll ( key . getBytes ( ) ) ; code_block = IfStatement ; int count = 0 ; code_block = ForStatement ; } catch ( JedisException e ) { logger . error ( "Error in getMemInfo" , e ) ; } }
public void test() { -> { log . warn ( "Unable to find annotated type {}" , annotatedType ) ; return null ; } }
public void test() { if ( schemaManager . add ( syntaxChecker ) ) { LOG . info ( "Added " + syntaxChecker ) ; } else { String msg = I18n . err ( I18n . ERR_386 , entry . getDn ( ) . getName ( ) , Strings . listToString ( schemaManager . getErrors ( ) ) ) ; LOG . info ( msg ) ; throw new LdapUnwillingToPerformException ( ResultCodeEnum . UNWILLING_TO_PERFORM , msg ) ; } }
public void test() { if ( schemaManager . add ( syntaxChecker ) ) { LOG . debug ( "Added {} into the enabled schema {}" , dn . getName ( ) , schemaName ) ; } else { String msg = I18n . err ( I18n . ERR_386 , entry . getDn ( ) . getName ( ) , Strings . listToString ( schemaManager . getErrors ( ) ) ) ; LOG . info ( msg ) ; throw new LdapUnwillingToPerformException ( ResultCodeEnum . UNWILLING_TO_PERFORM , msg ) ; } }
public void test() { if ( schema . isEnabled ( ) && syntaxChecker . isEnabled ( ) ) { code_block = IfStatement ; } else { LOG . info ( "Schema " + schema + " has been disabled." ) ; } }
public void test() { if ( log . isInfoEnabled ( ) ) { log . info ( msg ) ; } }
public void test() { try { synchronized ( this ) code_block = "" ; } catch ( IOException ex ) { log . error ( "Failed to write to file: " + file , ex ) ; } }
public void test() { try { LiferayPortletURL portletURL = PortletURLFactoryUtil . create ( portletRequest , PortletConfigurationSharingPortletKeys . PORTLET_CONFIGURATION_SHARING , PortletRequest . RENDER_PHASE ) ; portletURL . setParameter ( "netvibesURL" , getWidgetURL ( portletRequest ) ) ; portletURL . setWindowState ( LiferayWindowState . POP_UP ) ; return portletURL . toString ( ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; return StringPool . BLANK ; } }
public void test() { if ( cachedSources . containsKey ( radio . getId ( ) ) ) { LOG . debug ( "Retrieving sources for internet radio {}..." , radio . getStreamUrl ( ) ) ; sources = cachedSources . get ( radio . getId ( ) ) ; } else { LOG . debug ( "Retrieving sources for internet radio {}..." , radio . getStreamUrl ( ) ) ; code_block = TryStatement ;  cachedSources . put ( radio . getId ( ) , sources ) ; } }
public void test() { if ( cachedSources . containsKey ( radio . getId ( ) ) ) { LOG . debug ( "Got cached sources for internet {}!" , radio . getStreamUrl ( ) ) ; sources = cachedSources . get ( radio . getId ( ) ) ; } else { code_block = TryStatement ;  cachedSources . put ( radio . getId ( ) , sources ) ; LOG . debug ( "Adding cached sources for internet {}!" , radio . getStreamUrl ( ) ) ; } }
public void test() { if ( sources . isEmpty ( ) ) { LOG . debug ( "No playlist for internet radio {}." , radio . getStreamUrl ( ) ) ; } else { LOG . info ( "Retrieved playlist for internet radio {}, got {} sources." , radio . getStreamUrl ( ) , sources . size ( ) ) ; } }
public void test() { if ( sources . isEmpty ( ) ) { LOG . warn ( "No entries found for internet radio {}." , radio . getStreamUrl ( ) ) ; } else { LOG . info ( "Found {} entries for internet radio {}." , sources . size ( ) , sources ) ; } }
public void test() { try { sources = retrieveInternetRadioSources ( radio ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; sources = new ArrayList < > ( ) ; } }
public void test() { if ( page . getContent ( ) != null ) { LOG . info ( "limit:" + page . getContent ( ) . limit ( ) ) ; } else { LOG . info ( "no content found" ) ; assertNull ( page . getContent ( ) ) ; } }
public void test() { if ( page . getContent ( ) != null ) { LOG . info ( "url:" + page . getUrl ( ) . toString ( ) ) ; } else { LOG . info ( "no content found" ) ; assertNull ( page . getContent ( ) ) ; } }
public void test() { try { messageHandler . handleMessage ( bytes ) ; } catch ( Throwable t ) { LOG . error ( "Error processing a message handler" , t ) ; } }
public void test() { -> { logger . info ( "Update Message : {}" , entityType ) ; MqttBatchResult < JSONObject > result = mqttHelper . executeRequests ( getUpdatePatchEntityAction ( entityType ) , mqttHelper . getTopic ( entityType , IDS . get ( entityType ) ) ) ; assertJsonEqualsWithLinkResolving ( result . getActionResult ( ) , result . getMessages ( ) . values ( ) . iterator ( ) . next ( ) , mqttHelper . getTopic ( entityType , IDS . get ( entityType ) ) ) ; } }
public void test() { try { port = Integer . parseInt ( portSubstring ) ; } catch ( NumberFormatException nfe ) { log . error ( "Invalid port number {}" , portSubstring ) ; } }
@ SuppressWarnings ( "squid:S1126" ) private boolean checkSystemIfUniqueValidationNeeded ( final System system , final String validatedSystemName , final String validatedAddress , final Integer validatedPort ) { logger . debug ( "checkSystemIfUniqueValidationNeeded started..." ) ; final String actualSystemName = system . getSystemName ( ) ; final String actualAddress = system . getAddress ( ) ; final int actualPort = system . getPort ( ) ; code_block = IfStatement ; }
public void test() { try { node . nodeRemovedNotify ( ) ; code_block = IfStatement ; } catch ( Exception t ) { LOG . error ( "Failed to notify node {}" , node , t ) ; } }
public void test() { try { mdConsumer = new SimpleConsumer ( broker . split ( ":" ) [ 0 ] , Integer . parseInt ( broker . split ( ":" ) [ 1 ] ) , timeout , bufferSize , mdClientId ) ; List < String > topics = new ArrayList < String > ( 1 ) ; topics . add ( topic ) ; kafka . javaapi . TopicMetadataRequest req = new kafka . javaapi . TopicMetadataRequest ( topics ) ; TopicMetadataResponse resp = mdConsumer . send ( req ) ; List < TopicMetadata > metaData = resp . topicsMetadata ( ) ; code_block = ForStatement ; } catch ( NumberFormatException e ) { throw new IllegalArgumentException ( "Wrong format for broker url, should be \"broker1:port1\"" ) ; } catch ( Exception e ) { logger . error ( "Wrong format for broker url, should be \"broker1:port1\"" , e ) ; } }
List < VirtualAssetTextUnit > getLocalizedTextUnitsForTargetLocale ( Asset asset , RepositoryLocale repositoryLocale , InheritanceMode inheritanceMode ) { logger . debug ( "getLocalizedTextUnitsForTargetLocale" ) ; List < VirtualAssetTextUnit > virtualAssetTextUnits = new ArrayList < > ( ) ; Long lastSuccessfulAssetExtractionId = asset . getLastSuccessfulAssetExtraction ( ) . getId ( ) ; List < AssetTextUnitDTO > findByAssetExtractionAssetId = findByAssetExtractionIdAndDoNotTranslateFilter ( lastSuccessfulAssetExtractionId , null ) ; TranslatorWithInheritance translatorWithInheritance = new TranslatorWithInheritance ( asset , repositoryLocale , inheritanceMode ) ; code_block = ForStatement ; return virtualAssetTextUnits ; }
public void test() { if ( translation == null && InheritanceMode . REMOVE_UNTRANSLATED . equals ( inheritanceMode ) ) { logger . debug ( "No translation for text unit with name: {}, translation: {}" , assetTextUnit . getName ( ) , translation ) ; } else { logger . debug ( "Set translation for text unit with name: {}, translation: {}" , assetTextUnit . getName ( ) , translation ) ; VirtualAssetTextUnit virtualAssetTextUnit = convertAssetTextUnitDTOToVirtualAssetTextUnit ( assetTextUnit ) ; virtualAssetTextUnit . setContent ( translation ) ; virtualAssetTextUnits . add ( virtualAssetTextUnit ) ; } }
public void test() { if ( translation == null && InheritanceMode . REMOVE_UNTRANSLATED . equals ( inheritanceMode ) ) { logger . debug ( "Remove untranslated text unit" ) ; } else { logger . debug ( "Add untranslated text unit" ) ; VirtualAssetTextUnit virtualAssetTextUnit = convertAssetTextUnitDTOToVirtualAssetTextUnit ( assetTextUnit ) ; virtualAssetTextUnit . setContent ( translation ) ; virtualAssetTextUnits . add ( virtualAssetTextUnit ) ; } }
public void test() { try ( ResultSet rs = pstmt . executeQuery ( ) ; ) { code_block = WhileStatement ; } catch ( Exception e ) { s_logger . debug ( "listPodIdsHavingVmsforAccount:Exception: " + e . getMessage ( ) ) ; throw new CloudRuntimeException ( "listPodIdsHavingVmsforAccount:Exception: " + e . getMessage ( ) , e ) ; } }
public void test() { try ( PreparedStatement pstmt = txn . prepareStatement ( sql ) ) { pstmt . setLong ( 1 , zoneId ) ; pstmt . setLong ( 2 , accountId ) ; code_block = TryStatement ;  txn . commit ( ) ; return result ; } catch ( Exception e ) { s_logger . error ( "listPodIdsHavingVmsforAccount:Exception: " + e . getMessage ( ) , e ) ; throw new CloudRuntimeException ( "listPodIdsHavingVmsforAccount:Exception: " + e . getMessage ( ) , e ) ; } finally { code_block = TryStatement ;  } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Failed to clean up." , e ) ; } }
@ Test public void testDeepWalk ( ) throws Exception { Heartbeat . getInstance ( ) . disableHeartbeat ( ) ; AbstractCache < Blogger > vocabCache = new AbstractCache . Builder < Blogger > ( ) . build ( ) ; Graph < Blogger , Double > graph = buildGraph ( ) ; GraphWalker < Blogger > walker = new PopularityWalker . Builder < > ( graph ) . setNoEdgeHandling ( NoEdgeHandling . RESTART_ON_DISCONNECTED ) . setWalkLength ( 40 ) . setWalkDirection ( WalkDirection . FORWARD_UNIQUE ) . setRestartProbability ( 0.05 ) . setPopularitySpread ( 10 ) . setPopularityMode ( PopularityMode . MAXIMUM ) . setSpreadSpectrum ( SpreadSpectrum . PROPORTIONAL ) . build ( ) ; GraphTransformer < Blogger > graphTransformer = new GraphTransformer . Builder < > ( graph ) . setGraphWalker ( walker ) . shuffleOnReset ( true ) . setVocabCache ( vocabCache ) . build ( ) ; Blogger blogger = graph . getVertex ( 0 ) . getValue ( ) ; assertEquals ( 119 , blogger . getElementFrequency ( ) , 0.001 ) ; AbstractSequenceIterator < Blogger > sequenceIterator = new AbstractSequenceIterator . Builder < > ( graphTransformer ) . build ( ) ; WeightLookupTable < Blogger > lookupTable = new InMemoryLookupTable . Builder < Blogger > ( ) . lr ( 0.025 ) . vectorLength ( 150 ) . useAdaGrad ( false ) . cache ( vocabCache ) . seed ( 42 ) . build ( ) ; lookupTable . resetWeights ( true ) ; SequenceVectors < Blogger > vectors = new SequenceVectors . Builder < Blogger > ( new VectorsConfiguration ( ) ) . lookupTable ( lookupTable ) . iterate ( sequenceIterator ) . vocabCache ( vocabCache ) . batchSize ( 1000 ) . iterations ( 1 ) . epochs ( 10 ) . resetModel ( false ) . trainElementsRepresentation ( true ) . trainSequencesRepresentation ( false ) . elementsLearningAlgorithm ( new SkipGram < Blogger > ( ) ) . learningRate ( 0.025 ) . layerSize ( 150 ) . reset
@ Test public void testDeepWalk ( ) throws Exception { Heartbeat . getInstance ( ) . disableHeartbeat ( ) ; AbstractCache < Blogger > vocabCache = new AbstractCache . Builder < Blogger > ( ) . build ( ) ; Graph < Blogger , Double > graph = buildGraph ( ) ; GraphWalker < Blogger > walker = new PopularityWalker . Builder < > ( graph ) . setNoEdgeHandling ( NoEdgeHandling . RESTART_ON_DISCONNECTED ) . setWalkLength ( 40 ) . setWalkDirection ( WalkDirection . FORWARD_UNIQUE ) . setRestartProbability ( 0.05 ) . setPopularitySpread ( 10 ) . setPopularityMode ( PopularityMode . MAXIMUM ) . setSpreadSpectrum ( SpreadSpectrum . PROPORTIONAL ) . build ( ) ; GraphTransformer < Blogger > graphTransformer = new GraphTransformer . Builder < > ( graph ) . setGraphWalker ( walker ) . shuffleOnReset ( true ) . setVocabCache ( vocabCache ) . build ( ) ; Blogger blogger = graph . getVertex ( 0 ) . getValue ( ) ; assertEquals ( 119 , blogger . getElementFrequency ( ) , 0.001 ) ; logger . info ( "Blogger: " + blogger ) ; AbstractSequenceIterator < Blogger > sequenceIterator = new AbstractSequenceIterator . Builder < > ( graphTransformer ) . build ( ) ; WeightLookupTable < Blogger > lookupTable = new InMemoryLookupTable . Builder < Blogger > ( ) . lr ( 0.025 ) . vectorLength ( 150 ) . useAdaGrad ( false ) . cache ( vocabCache ) . seed ( 42 ) . build ( ) ; lookupTable . resetWeights ( true ) ; SequenceVectors < Blogger > vectors = new SequenceVectors . Builder < Blogger > ( new VectorsConfiguration ( ) ) . lookupTable ( lookupTable ) . iterate ( sequenceIterator ) . vocabCache ( vocabCache ) . batchSize ( 1000 ) . iterations ( 1 ) . epochs ( 10 ) . resetModel ( false ) . trainElementsRepresentation ( true ) . trainSequencesRepresentation ( false ) . elementsLearningAlgorithm ( new SkipGram < Blogger > ( ) ) { Heartbeat
public void test() { try { setUpClass ( ) ; } catch ( RuntimeException | IOException | InterruptedException ex ) { LOGGER . warn ( "Could not setup up class" , ex ) ; } }
public void test() { try { zigBeeGateway . processInputLine ( command , out ) ; } catch ( final Exception e ) { LOGGER . error ( "Error: " + e . getMessage ( ) , e ) ; out . println ( "Error: " + e . getMessage ( ) ) ; } }
public void test() { if ( ! this . rsaKeyFile . exists ( ) ) { logger . error ( "File {} does not exist" , this . rsaKeyFile . getAbsolutePath ( ) ) ; return null ; } }
public void test() { try ( FileInputStream input = new FileInputStream ( rsaKeyFile ) ) { return readInputStream ( input ) ; } catch ( IOException e ) { String msg = String . format ( "Failed to read private key {%s}." , this . rsaKeyFile . getAbsolutePath ( ) ) ; logger . error ( msg , e ) ; throw new EncryptionException ( msg , e ) ; } }
public void test() { try ( OutputStream propertiesOutputStream = fileSystemClient . newOutputStream ( catalogPath . getPropertiesPath ( ) ) ; OutputStream metadataOutputStream = fileSystemClient . newOutputStream ( catalogPath . getMetadataPath ( ) ) ) { Map < String , CatalogFileInputStream . InputStreamWithType > inputStreams = configFiles . getInputStreams ( ) ; code_block = ForStatement ; Properties metadata = new Properties ( ) ; metadata . put ( "createdTime" , String . valueOf ( catalogInfo . getCreatedTime ( ) ) ) ; metadata . put ( "version" , String . valueOf ( catalogInfo . getVersion ( ) ) ) ; metadata . put ( "catalogFiles" , LIST_CODEC . toJson ( configFiles . getCatalogFileNames ( ) ) ) ; metadata . put ( "globalFiles" , LIST_CODEC . toJson ( configFiles . getGlobalFileNames ( ) ) ) ; metadata . store ( metadataOutputStream , "The metadata of dynamic catalog" ) ; Map < String , String > catalogProperties = rewriteFilePathProperties ( catalogName , catalogInfo . getProperties ( ) , configFiles . getCatalogFileNames ( ) , configFiles . getGlobalFileNames ( ) ) ; Properties properties = new Properties ( ) ; properties . putAll ( catalogProperties ) ; properties . put ( CATALOG_NAME_PROPERTY , catalogInfo . getConnectorName ( ) ) ; properties . store ( propertiesOutputStream , "The properties of dynamic catalog" ) ; } catch ( IOException ex ) { LOG . warn ( ex . toString ( ) , ex ) ; deleteCatalog ( catalogName , false ) ; throw ex ; } }
public void close ( Socket s ) { LOG . info ( "Closing client connection {}" , s ) ; SafeClose . close ( s ) ; }
public void test() { try { currentStream . close ( ) ; currentStream = null ; } catch ( IOException ioe ) { LOG . warn ( "Could not close piped output stream" , ioe ) ; } }
public void test() { try { in = Files . openFileStream ( thisFilename ) ; filename = thisFilename . replaceAll ( "\\.gz$" , "" ) ; } catch ( IOException ioe ) { log . error ( "Unable to open stream: " + ioe . getMessage ( ) ) ; } }
@ Override public void mapPort ( final int port , final String address , final PortMapProtocol protocol , final String mappingDescription ) { final Protocol resolvedProtocol = resolveProtocol ( protocol ) ; final PortMapping portMapping = new PortMapping ( port , address , resolvedProtocol , mappingDescription ) ; LOG . debug ( "Port: [{}] for address: [{}] with description: [{}] has been mapped." , port , address , mappingDescription ) ; mappingServicesRegistrar . registerPortMapping ( portMapping ) ; LOG . debug ( "Port: [{}] for address: [{}] with description: [{}] has been mapped." , port , address , mappingDescription ) ; }
@ Override public void mapPort ( final int port , final String address , final PortMapProtocol protocol , final String mappingDescription ) { LOG . debug ( "Mapping port: [{}] for address: [{}] with description: [{}]." , port , address , mappingDescription ) ; final Protocol resolvedProtocol = resolveProtocol ( protocol ) ; final PortMapping portMapping = new PortMapping ( port , address , resolvedProtocol , mappingDescription ) ; mappingServicesRegistrar . registerPortMapping ( portMapping ) ; LOG . debug ( "Registered port: [{}] for address: [{}]." , port , address ) ; }
public void test() { try { SecorConfig config = SecorConfig . load ( ) ; String stagingDirectoryPath = config . getLocalPath ( ) + '/' + IdUtil . getLocalMessageDir ( ) ; LOG . info ( "Starting Ostraint {}" , stagingDirectoryPath ) ; ShutdownHookRegistry . registerHook ( 10 , new StagingDirectoryCleaner ( stagingDirectoryPath ) ) ; MetricCollector metricCollector = ReflectionUtil . createMetricCollector ( config . getMetricsCollectorClass ( ) ) ; metricCollector . initialize ( config ) ; OstrichAdminService ostrichService = new OstrichAdminService ( config ) ; ostrichService . start ( ) ; FileUtil . configure ( config ) ; LogFileDeleter logFileDeleter = new LogFileDeleter ( config ) ; logFileDeleter . deleteOldLogs ( ) ; RateLimitUtil . configure ( config ) ; LinkedList < Consumer > consumers = new LinkedList < Consumer > ( ) ; code_block = ForStatement ; code_block = ForStatement ; } catch ( Throwable t ) { LOG . error ( "Consumer failed" , t ) ; System . exit ( 1 ) ; } }
public void test() { try { SecorConfig config = SecorConfig . load ( ) ; String stagingDirectoryPath = config . getLocalPath ( ) + '/' + IdUtil . getLocalMessageDir ( ) ; ShutdownHookRegistry . registerHook ( 10 , new StagingDirectoryCleaner ( stagingDirectoryPath ) ) ; MetricCollector metricCollector = ReflectionUtil . createMetricCollector ( config . getMetricsCollectorClass ( ) ) ; metricCollector . initialize ( config ) ; OstrichAdminService ostrichService = new OstrichAdminService ( config ) ; ostrichService . start ( ) ; FileUtil . configure ( config ) ; LogFileDeleter logFileDeleter = new LogFileDeleter ( config ) ; logFileDeleter . deleteOldLogs ( ) ; RateLimitUtil . configure ( config ) ; LOG . info ( "starting {} consumer threads" , config . getConsumerThreads ( ) ) ; LinkedList < Consumer > consumers = new LinkedList < Consumer > ( ) ; code_block = ForStatement ; code_block = ForStatement ; } catch ( Throwable t ) { LOG . error ( "Error starting consumer" , t ) ; System . exit ( 1 ) ; } }
public void test() { try { interceptInitEntity ( ) ; } catch ( RuntimeException ex ) { LOGGER . error ( "Error intercepting entity" , ex ) ; } }
public void test() { try { a . initAttribute ( ) ; } catch ( RuntimeException ex ) { log . error ( "Exception while initializing attribute" , ex ) ; } }
public void test() { try { e . initEntity ( ) ; } catch ( RuntimeException ex ) { LOGGER . log ( Level . SEVERE , "Could not init Entity" , ex ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
@ Override public void createShare ( final String irodsAbsolutePath , final String shareName ) throws ShareAlreadyExistsException , FileNotFoundException , JargonException { log . info ( "createShare()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "irodsAbsolutePath:{}" , irodsAbsolutePath ) ; log . info ( "deciding whether a file or collection..." ) ; ObjStat objStat = getObjStatForAbsolutePath ( irodsAbsolutePath ) ; log . info ( "seeing if share already present.." ) ; IRODSSharedFileOrCollection currentSharedFile = findSharedGivenObjStat ( irodsAbsolutePath , objStat ) ; code_block = IfStatement ; MetadataDomain metadataDomain ; code_block = IfStatement ; IRODSSharedFileOrCollection irodsSharedFileOrCollection = new IRODSSharedFileOrCollection ( metadataDomain , irodsAbsolutePath , shareName , irodsAccount . getUserName ( ) , irodsAccount . getZone ( ) , new ArrayList < ShareUser > ( ) ) ; log . info ( "adding share tag" ) ; AvuData avuData = buildAVUBUBasedOnShare ( irodsSharedFileOrCollection ) ; log . info ( "setting inheritance and ACL" ) ; code_block = IfStatement ; log . info ( "share created" ) ; }
@ Override public void createShare ( final String irodsAbsolutePath , final String shareName ) throws ShareAlreadyExistsException , FileNotFoundException , JargonException { log . info ( "createShare()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "deciding whether a file or collection..." ) ; ObjStat objStat = getObjStatForAbsolutePath ( irodsAbsolutePath ) ; log . info ( "seeing if share already present.." ) ; IRODSSharedFileOrCollection currentSharedFile = findSharedGivenObjStat ( irodsAbsolutePath , objStat ) ; code_block = IfStatement ; MetadataDomain metadataDomain ; code_block = IfStatement ; log . info ( "no share found" ) ; IRODSSharedFileOrCollection irodsSharedFileOrCollection = new IRODSSharedFileOrCollection ( metadataDomain , irodsAbsolutePath , shareName , irodsAccount . getUserName ( ) , irodsAccount . getZone ( ) , new ArrayList < ShareUser > ( ) ) ; log . info ( "adding share tag" ) ; AvuData avuData = buildAVUBUBasedOnShare ( irodsSharedFileOrCollection ) ; log . info ( "setting inheritance and ACL" ) ; code_block = IfStatement ; log . info ( "share created" ) ; }
@ Override public void createShare ( final String irodsAbsolutePath , final String shareName ) throws ShareAlreadyExistsException , FileNotFoundException , JargonException { log . info ( "createShare()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "irodsAbsolutePath:{}" , irodsAbsolutePath ) ; ObjStat objStat = getObjStatForAbsolutePath ( irodsAbsolutePath ) ; log . info ( "seeing if share already present.." ) ; IRODSSharedFileOrCollection currentSharedFile = findSharedGivenObjStat ( irodsAbsolutePath , objStat ) ; code_block = IfStatement ; log . info ( "got shareName:{}" , shareName ) ; MetadataDomain metadataDomain ; code_block = IfStatement ; IRODSSharedFileOrCollection irodsSharedFileOrCollection = new IRODSSharedFileOrCollection ( metadataDomain , irodsAbsolutePath , shareName , irodsAccount . getUserName ( ) , irodsAccount . getZone ( ) , new ArrayList < ShareUser > ( ) ) ; log . info ( "adding share tag" ) ; AvuData avuData = buildAVUBasedOnShare ( irodsSharedFileOrCollection ) ; log . info ( "setting inheritance and ACL" ) ; code_block = IfStatement ; log . info ( "share created" ) ; }
@ Override public void createShare ( final String irodsAbsolutePath , final String shareName ) throws ShareAlreadyExistsException , FileNotFoundException , JargonException { log . info ( "createShare()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "irodsAbsolutePath:{}" , irodsAbsolutePath ) ; log . info ( "deciding whether a file or collection..." ) ; ObjStat objStat = getObjStatForAbsolutePath ( irodsAbsolutePath ) ; IRODSSharedFileOrCollection currentSharedFile = findSharedGivenObjStat ( irodsAbsolutePath , objStat ) ; code_block = IfStatement ; log . info ( "getting shareName:{}" , shareName ) ; MetadataDomain metadataDomain ; code_block = IfStatement ; IRODSSharedFileOrCollection irodsSharedFileOrCollection = new IRODSSharedFileOrCollection ( metadataDomain , irodsAbsolutePath , shareName , irodsAccount . getUserName ( ) , irodsAccount . getZone ( ) , new ArrayList < ShareUser > ( ) ) ; log . info ( "adding share tag" ) ; AvuData avuData = buildAVUBasedOnShare ( irodsSharedFileOrCollection ) ; log . info ( "setting inheritance and ACL" ) ; code_block = IfStatement ; log . info ( "share created" ) ; }
@ Override public void createShare ( final String irodsAbsolutePath , final String shareName ) throws ShareAlreadyExistsException , FileNotFoundException , JargonException { log . info ( "createShare()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "irodsAbsolutePath:{}" , irodsAbsolutePath ) ; log . info ( "deciding whether a file or collection..." ) ; ObjStat objStat = getObjStatForAbsolutePath ( irodsAbsolutePath ) ; log . info ( "seeing if share already present.." ) ; IRODSSharedFileOrCollection currentSharedFile = findSharedGivenObjStat ( irodsAbsolutePath , objStat ) ; code_block = IfStatement ; MetadataDomain metadataDomain ; code_block = IfStatement ; IRODSSharedFileOrCollection irodsSharedFileOrCollection = new IRODSSharedFileOrCollection ( metadataDomain , irodsAbsolutePath , shareName , irodsAccount . getUserName ( ) , irodsAccount . getZone ( ) , new ArrayList < ShareUser > ( ) ) ; log . info ( "getting avu data..." ) ; AvuData avuData = buildAVUBasedOnShare ( irodsSharedFileOrCollection ) ; log . info ( "setting inheritance and ACL" ) ; code_block = IfStatement ; log . info ( "share created" ) ; }
@ Override public void createShare ( final String irodsAbsolutePath , final String shareName ) throws ShareAlreadyExistsException , FileNotFoundException , JargonException { log . info ( "createShare()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "irodsAbsolutePath:{}" , irodsAbsolutePath ) ; log . info ( "deciding whether a file or collection..." ) ; ObjStat objStat = getObjStatForAbsolutePath ( irodsAbsolutePath ) ; log . info ( "seeing if share already present.." ) ; IRODSSharedFileOrCollection currentSharedFile = findSharedGivenObjStat ( irodsAbsolutePath , objStat ) ; code_block = IfStatement ; MetadataDomain metadataDomain ; code_block = IfStatement ; log . info ( "getting shareName:{}" , shareName ) ; IRODSSharedFileOrCollection irodsSharedFileOrCollection = new IRODSSharedFileOrCollection ( metadataDomain , irodsAbsolutePath , shareName , irodsAccount . getUserName ( ) , irodsAccount . getZone ( ) , new ArrayList < ShareUser > ( ) ) ; log . info ( "adding share tag" ) ; AvuData avuData = buildAVUBasedOnShare ( irodsSharedFileOrCollection ) ; code_block = IfStatement ; log . info ( "share created" ) ; }
@ Override public void createShare ( final String irodsAbsolutePath , final String shareName ) throws ShareAlreadyExistsException , FileNotFoundException , JargonException { log . info ( "createShare()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "irodsAbsolutePath:{}" , irodsAbsolutePath ) ; log . info ( "deciding whether a file or collection..." ) ; ObjStat objStat = getObjStatForAbsolutePath ( irodsAbsolutePath ) ; log . info ( "Lord if share already present.." ) ; IRODSSharedFileOrCollection currentSharedFile = findSharedGivenObjStat ( irodsAbsolutePath , objStat ) ; code_block = IfStatement ; MetadataDomain metadataDomain ; code_block = IfStatement ; IRODSSharedFileOrCollection irodsSharedFileOrCollection = new IRODSSharedFileOrCollection ( metadataDomain , irodsAbsolutePath , shareName , irodsAccount . getUserName ( ) , irodsAccount . getZone ( ) , new ArrayList < ShareUser > ( ) ) ; log . info ( "adding share tag" ) ; AvuData avuData = buildAVUBasedOnShare ( irodsSharedFileOrCollection ) ; log . info ( "setting inheritance and ACL" ) ; code_block = IfStatement ; log . info ( "using avu data" ) ; }
private synchronized void stopTask ( ) { this . isRunning = false ; this . notify ( ) ; LOG . info ( "Stopped" ) ; }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { MCRCategory linkedCategory = link . getCategory ( ) ; StringBuilder debugMessage = new StringBuilder ( "Adding Link from " ) . append ( linkedCategory . getId ( ) ) ; code_block = IfStatement ; debugMessage . append ( "to " ) . append ( objectReference ) ; LOGGER . debug ( debugMessage . toString ( ) ) ; } }
@ Override public QueryResult execute ( String query , String language ) throws RepositoryException { final Query jcrQuery = getLocalSession ( ) . getSession ( ) . getWorkspace ( ) . getQueryManager ( ) . createQuery ( query , language ) ; log . debug ( "Query: {}" , jcrQuery ) ; return jcrQuery . execute ( ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { { PortablePreconditions . checkNotNull ( "spaceName" , spaceName ) ; PortablePreconditions . checkNotNull ( "projectName" , projectName ) ; final String id = newId ( ) ; final InstallProjectRequest jobRequest = new InstallProjectRequest ( ) ; jobRequest . setStatus ( JobStatus . ACCEPTED ) ; jobRequest . setJobId ( id ) ; jobRequest . setSpaceName ( spaceName ) ; jobRequest . setProjectName ( projectName ) ; jobRequest . setBranchName ( branchName ) ; addAcceptedJobResult ( id ) ; jobRequestObserver . installProjectRequest ( jobRequest ) ; logger . info ( "Applying project {} to project {}" , spaceName , projectName ) ; return createAcceptedStatusResponse ( jobRequest ) ; } }
private void report ( ) { final long deviceConnectionDuration = noOfDeviceConnections . get ( ) * Duration . between ( startInstant . getAndSet ( Instant . now ( ) ) , Instant . now ( ) ) . toMillis ( ) ; logger . debug ( "Application connection took {} ms." , deviceConnectionDuration ) ; recorder . accept ( deviceConnectionDuration ) ; }
public void test() { if ( provenanceEventJmsWriter == null ) { LOGGER . warn ( "Unable to create provenanceEventJmsWriter" ) ; } }
public void test() { for ( Long executionId : execIds ) { ExecutionActionResult result = cancelExecutionService . requestCancelExecution ( executionId ) ; logger . info ( "Cancel action " + result ) ; } }
public void test() { for ( String cache : pools ) { code_block = IfStatement ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { this . camelContext . stop ( ) ; } catch ( Exception e ) { LOG . error ( "Can't stop camel context" , e ) ; } }
public void start ( ) { code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; startTimerAcquisitionThread ( ) ; startResetExpiredJobsThread ( ) ; isActive = true ; executeTemporaryJobs ( ) ; log . info ( "Executioner started" ) ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { try { return IOUtils . toString ( InputStream . class . cast ( getService ( RoleService . class ) . getAnyLayout ( roleKey ) . getEntity ( ) ) , StandardCharsets . UTF_8 ) ; } catch ( Exception e ) { log . warn ( "Unable to read role key {}" , roleKey ) ; return StringUtils . EMPTY ; } }
public void test() { if ( variantSourceObj instanceof VariantFileMetadata ) { sortedSampleNames = ( ( VariantFileMetadata ) variantSourceObj ) . getSampleIds ( ) ; } else-if ( variantSourceObj instanceof Map ) { sortedSampleNames = new ObjectMap ( ( Map ) variantSourceObj ) . getAsStringList ( "sampleIds" ) ; } else { logger . error ( "Unable to map variant source: " + variantSourceObj . getClass ( ) . getName ( ) ) ; } }
public void test() { if ( alignmentHeaderObj instanceof Map ) { sortedSampleNames = getSampleFromAlignmentHeader ( ( Map ) alignmentHeaderObj ) ; } else { LOGGER . error ( "Error converting sample name to map" ) ; } }
public void testStateTransfer ( ) throws Exception { CyclicBarrier barrier = new CyclicBarrier ( 2 ) ; blockDataContainerIteration ( cache ( 0 ) , barrier ) ; Set < Object > keys = new HashSet < > ( ) ; code_block = ForStatement ; addClusterEnabledCacheManager ( TestDataSCI . INSTANCE , dccc ) ; waitForClusterToForm ( ) ; barrier . await ( 10 , TimeUnit . SECONDS ) ; log . trace ( "Checking the values from caches..." ) ; code_block = ForStatement ; barrier . await ( 10 , TimeUnit . SECONDS ) ; log . trace ( "Checking the remaining values from caches..." ) ; cache ( 0 ) . getAdvancedCache ( ) . getAsyncInterceptorChain ( ) . removeInterceptor ( BlockingInterceptor . class ) ; code_block = ForStatement ; }
public void testStateTransfer ( ) throws Exception { CyclicBarrier barrier = new CyclicBarrier ( 2 ) ; blockDataContainerIteration ( cache ( 0 ) , barrier ) ; Set < Object > keys = new HashSet < > ( ) ; code_block = ForStatement ; log . trace ( "State transfer happens here" ) ; addClusterEnabledCacheManager ( TestDataSCI . INSTANCE , dccc ) ; waitForClusterToForm ( ) ; log . trace ( "Waiting for cluster to form" ) ; barrier . await ( 10 , TimeUnit . SECONDS ) ; code_block = ForStatement ; barrier . await ( 10 , TimeUnit . SECONDS ) ; cache ( 0 ) . getAdvancedCache ( ) . getAsyncInterceptorChain ( ) . removeInterceptor ( BlockingInterceptor . class ) ; code_block = ForStatement ; }
public void test() { try { studio . getModel ( ) . clearModuleInfo ( ) ; unregisterArtifactInProjectDescriptor ( childArtefact ) ; childArtefact . delete ( ) ; repositoryTreeState . refreshSelectedNode ( ) ; resetStudioModel ( ) ; WebStudioUtils . addInfoMessage ( "Element was deleted successfully." ) ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; WebStudioUtils . addErrorMessage ( "Error deleting." , e . getMessage ( ) ) ; } }
public void test() { try { signature = generateSignature ( selectedSignatureHashAlgo ) ; } catch ( CryptoException E ) { LOGGER . warn ( "Could not generate signature" , E ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { TaskExitState exitState = new TaskExitState ( ) ; exitState . setInput ( inputMeter . count ( ) ) ; exitState . setTotalEmitted ( outputMeter . count ( ) ) ; exitState . setMeanRate ( outputMeter . meanRate ( ) ) ; Files . write ( CodecJSON . INSTANCE . encode ( exitState ) , new File ( "job.exit" ) ) ; } catch ( Exception ex ) { log . error ( "Could not write task exit state to " + ex . getMessage ( ) , ex ) ; } }
public void test() { try { InputStreamReader inputStreamReader = new InputStreamReader ( response . getEntity ( ) . getContent ( ) ) ; BufferedReader reader = new BufferedReader ( inputStreamReader ) ; ObjectMapper objectMapper = new ObjectMapper ( ) ; Map < String , Map < String , Object > > responseMap = null ; code_block = TryStatement ;  code_block = IfStatement ; } catch ( JsonParseException e ) { LOGGER . error ( "SBIMOPS reconciliation, error while parsing the response content" , e ) ; } catch ( IOException e ) { LOGGER . error ( "SBIMOPS reconciliation, error while reading the response content" , e ) ; } }
public void test() { try { InputStreamReader inputStreamReader = new InputStreamReader ( response . getEntity ( ) . getContent ( ) ) ; BufferedReader reader = new BufferedReader ( inputStreamReader ) ; ObjectMapper objectMapper = new ObjectMapper ( ) ; Map < String , Map < String , Object > > responseMap = null ; code_block = TryStatement ;  code_block = IfStatement ; } catch ( JsonParseException e ) { LOGGER . error ( "SBIMOPS reconciliation, error while parsing the response content" , e ) ; } catch ( IOException e ) { LOGGER . error ( "SBIMOPS reconciliation, error while reading the response content" , e ) ; } }
public void test() { if ( verboseLogs ) { log . info ( "Waiting for contexts to complete" ) ; } }
public void test() { if ( cachedRowSet . size ( ) == 0 ) { return Optional . empty ( ) ; } else-if ( cachedRowSet . size ( ) > 1 ) { LOGGER . warn ( MessageFormat . format ( "Found multiple implementations for ScriptDesignTrace {0}. Returning first implementation" , scriptName ) ) ; } }
public void test() { try { return Optional . of ( Identities . builder ( ) . identities ( identities ) . build ( ) ) ; } catch ( IllegalArgumentException | NoSuchElementException e ) { LOG . warn ( "Unable to get identities" , e ) ; return Optional . empty ( ) ; } }
public void test() { try { WorkflowRun wr = ll . findWorkflowRun ( "/" + workflowRunAccession ) ; Workflow w = ll . findWorkflowByWorkflowRun ( workflowRunAccession ) ; wr . setWorkflow ( w ) ; return ( wr ) ; } catch ( IOException | JAXBException ex ) { Log . error ( ex ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
@ Test public void test1 ( ) throws Exception { LOG . info ( "------  test1  ------" ) ; myTestExecution ( DatesPage1 . class , "DatesPage1_ExpectedResult.html" ) ; }
protected long sweep ( GarbageCollectorFileState fs , long markStart , boolean forceBlobRetrieve ) throws Exception { long earliestRefAvailTime ; earliestRefAvailTime = GarbageCollectionType . get ( blobStore ) . mergeAllMarkedReferences ( blobStore , fs , clock , maxLastModifiedInterval , sweepIfRefsPastRetention ) ; earliestRefAvailTime = ( earliestRefAvailTime < markStart ? earliestRefAvailTime : markStart ) ; ( new BlobIdRetriever ( fs , forceBlobRetrieve ) ) . call ( ) ; difference ( fs ) ; long count = 0 ; long deleted = 0 ; long maxModifiedTime = getMaxModifiedTime ( earliestRefAvailTime ) ; LOG . debug ( "Starting sweep phase of the garbage collector" ) ; LOG . debug ( "Sweeping blobs with modified time > than the configured max deleted time ({}). " , timestampToString ( maxModifiedTime ) ) ; BufferedWriter removesWriter = null ; LineIterator iterator = null ; long deletedSize = 0 ; int numDeleted = 0 ; code_block = TryStatement ;  code_block = IfStatement ; BlobCollectionType . get ( blobStore ) . handleRemoves ( blobStore , fs . getGarbage ( ) , fs . getMarkedRefs ( ) ) ; code_block = IfStatement ; code_block = IfStatement ; statsCollector . updateNumCandidates ( count ) ; statsCollector . updateNumDeleted ( deleted ) ; statsCollector . updateTotalSizeDeleted ( deletedSize ) ; LOG . debug ( "Sweeping blobs with size {} and total size {} were deleted." , deleted , deletedSize ) ; GarbageCollectionType . get ( blobStore ) . removeAllMarkedReferences ( blobStore ) ; LOG . debug ( "Ending sweep phase of the garbage collector" ) ; return deleted ; }
protected long sweep ( GarbageCollectorFileState fs , long markStart , boolean forceBlobRetrieve ) throws Exception { long earliestRefAvailTime ; earliestRefAvailTime = GarbageCollectionType . get ( blobStore ) . mergeAllMarkedReferences ( blobStore , fs , clock , maxLastModifiedInterval , sweepIfRefsPastRetention ) ; LOG . debug ( "Earliest reference available for timestamp [{}]" , earliestRefAvailTime ) ; earliestRefAvailTime = ( earliestRefAvailTime < markStart ? earliestRefAvailTime : markStart ) ; ( new BlobIdRetriever ( fs , forceBlobRetrieve ) ) . call ( ) ; difference ( fs ) ; long count = 0 ; long deleted = 0 ; long maxModifiedTime = getMaxModifiedTime ( earliestRefAvailTime ) ; LOG . debug ( "Sweeping blobs with modified time > than the configured max deleted time ({}). " , timestampToString ( maxModifiedTime ) ) ; BufferedWriter removesWriter = null ; LineIterator iterator = null ; long deletedSize = 0 ; int numDeletedSizeAvailable = 0 ; code_block = TryStatement ;  code_block = IfStatement ; BlobCollectionType . get ( blobStore ) . handleRemoves ( blobStore , fs . getGarbage ( ) , fs . getMarkedRefs ( ) ) ; code_block = IfStatement ; code_block = IfStatement ; statsCollector . updateNumCandidates ( count ) ; statsCollector . updateNumDeleted ( deleted ) ; statsCollector . updateTotalSizeDeleted ( deletedSize ) ; LOG . debug ( "Sweeping for the garbage collector of the garbage collector" ) ; GarbageCollectionType . get ( blobStore ) . removeAllMarkedReferences ( blobStore ) ; LOG . debug ( "Ending sweep phase of the garbage collector" ) ; return deleted ; }
protected long sweep ( GarbageCollectorFileState fs , long markStart , boolean forceBlobRetrieve ) throws Exception { long earliestRefAvailTime ; earliestRefAvailTime = GarbageCollectionType . get ( blobStore ) . mergeAllMarkedReferences ( blobStore , fs , clock , maxLastModifiedInterval , sweepIfRefsPastRetention ) ; LOG . debug ( "Earliest reference available for timestamp [{}]" , earliestRefAvailTime ) ; earliestRefAvailTime = ( earliestRefAvailTime < markStart ? earliestRefAvailTime : markStart ) ; ( new BlobIdRetriever ( fs , forceBlobRetrieve ) ) . call ( ) ; difference ( fs ) ; long count = 0 ; long deleted = 0 ; long maxModifiedTime = getMaxModifiedTime ( earliestRefAvailTime ) ; LOG . debug ( "Starting sweep phase of the garbage collector" ) ; BufferedWriter removesWriter = null ; LineIterator iterator = null ; long deletedSize = 0 ; int numDeletedSizeAvailable = 0 ; code_block = TryStatement ;  code_block = IfStatement ; BlobCollectionType . get ( blobStore ) . handleRemoves ( blobStore , fs . getGarbage ( ) , fs . getMarkedRefs ( ) ) ; code_block = IfStatement ; code_block = IfStatement ; statsCollector . updateNumCandidates ( count ) ; statsCollector . updateNumDeleted ( deleted ) ; statsCollector . updateTotalSizeDeleted ( deletedSize ) ; LOG . debug ( "Removed {} from blob store" , deleted ) ; GarbageCollectionType . get ( blobStore ) . removeAllMarkedReferences ( blobStore ) ; LOG . debug ( "Ending sweep phase of the garbage collector" ) ; return deleted ; }
public void test() { if ( count != deleted ) { LOGGER . warn ( "Deleted {} db entries before was deleted" , deleted ) ; } }
public void test() { if ( deletedSize > 0 ) { LOG . info ( "Deleted {} blob with size {}" , deletedSize , blobStore . size ( ) ) ; } }
protected long sweep ( GarbageCollectorFileState fs , long markStart , boolean forceBlobRetrieve ) throws Exception { long earliestRefAvailTime ; earliestRefAvailTime = GarbageCollectionType . get ( blobStore ) . mergeAllMarkedReferences ( blobStore , fs , clock , maxLastModifiedInterval , sweepIfRefsPastRetention ) ; LOG . debug ( "Earliest reference available for timestamp [{}]" , earliestRefAvailTime ) ; earliestRefAvailTime = ( earliestRefAvailTime < markStart ? earliestRefAvailTime : markStart ) ; ( new BlobIdRetriever ( fs , forceBlobRetrieve ) ) . call ( ) ; difference ( fs ) ; long count = 0 ; long deleted = 0 ; long maxModifiedTime = getMaxModifiedTime ( earliestRefAvailTime ) ; LOG . debug ( "Starting sweep phase of the garbage collector" ) ; LOG . debug ( "Sweeping blobs with modified time > than the configured max deleted time ({}). " , timestampToString ( maxModifiedTime ) ) ; BufferedWriter removesWriter = null ; LineIterator iterator = null ; long deletedSize = 0 ; int numDeleted = 0 ; code_block = TryStatement ;  code_block = IfStatement ; BlobCollectionType . get ( blobStore ) . handleRemoves ( blobStore , fs . getGarbage ( ) , fs . getMarkedRefs ( ) ) ; code_block = IfStatement ; code_block = IfStatement ; statsCollector . updateNumCandidates ( count ) ; statsCollector . updateNumDeleted ( deleted ) ; statsCollector . updateTotalSizeDeleted ( deletedSize ) ; GarbageCollectionType . get ( blobStore ) . removeAllMarkedReferences ( blobStore ) ; LOG . debug ( "Removed " + deleted + " from the blob store" ) ; return deleted ; }
public void test() { try { String langCode = this . getCurrentLang ( ) . getCode ( ) ; String nodeRootCode = this . getIdeaManager ( ) . getCategoryRoot ( ) ; categories = this . getCategoryLeaf ( nodeRootCode , langCode , completeTitle ) ; } catch ( Throwable t ) { _logger . error ( "Error loading categories" , t ) ; throw new RuntimeException ( "Error loading categories" ) ; } }
@ Test public void testTypes ( ) throws Exception { logger . info ( "" + Schema . getDefaultSchema ( ) . getEntityType ( SampleEntity . class ) ) ; SampleEntity entity = new SampleEntity ( ) ; logger . info ( entity . getType ( ) ) ; logger . info ( entity ) ; }
@ Test public void testTypes ( ) throws Exception { logger . info ( "" + Schema . getDefaultSchema ( ) . getEntityClass ( "sample_entity" ) ) ; SampleEntity entity = new SampleEntity ( ) ; logger . info ( entity . getType ( ) ) ; logger . info ( entity . toString ( ) ) ; }
@ Test public void testTypes ( ) throws Exception { logger . info ( "" + Schema . getDefaultSchema ( ) . getEntityClass ( "sample_entity" ) ) ; logger . info ( "" + Schema . getDefaultSchema ( ) . getEntityType ( SampleEntity . class ) ) ; SampleEntity entity = new SampleEntity ( ) ; logger . info ( entity . toString ( ) ) ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( message , exception ) ; } else { _log . error ( message ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( message , exception ) ; } else { _log . error ( message ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( registration == null ) { localLogger . error ( "sequencerRunDAOHibernate insert registration is null" ) ; } else-if ( registration . isLIMSAdmin ( ) || sequencerRun . givesPermission ( registration ) ) { localLogger . info ( "insert sequencer run object" ) ; insert ( sequencerRun ) ; return ( sequencerRun . getSwAccession ( ) ) ; } else { localLogger . error ( "sequencerRunDAOHibernate insert not authorized" ) ; } }
public void test() { if ( registration == null ) { localLogger . error ( "SequencerRunDAOHibernate insert SequencerRun registration is null" ) ; } else-if ( registration . isLIMSAdmin ( ) || sequencerRun . givesPermission ( registration ) ) { localLogger . info ( "inserting sequence" ) ; insert ( sequencerRun ) ; return ( sequencerRun . getSwAccession ( ) ) ; } else { localLogger . error ( "sequencerRunDAOHibernate insert not authorized" ) ; } }
public void test() { if ( registration == null ) { localLogger . error ( "SequencerRunDAOHibernate insert SequencerRun registration is null" ) ; } else-if ( registration . isLIMSAdmin ( ) || sequencerRun . givesPermission ( registration ) ) { localLogger . info ( "insert sequencer run object" ) ; insert ( sequencerRun ) ; return ( sequencerRun . getSwAccession ( ) ) ; } else { localLogger . error ( "SequencerRunDAOHibernate insert not authorized" ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { if ( type == null ) { logger . warn ( "Cannot update event type {}" , event . getClass ( ) . getName ( ) ) ; return ; } }
public void test() { try { linkTableInstance . create ( from , to , type , attr ) ; } catch ( Exception e ) { log . warn ( "connectTableInstance failed" , e ) ; } }
public void test() { try { fs = hdfsPath . getFileSystem ( conf ) ; } catch ( IOException e ) { LOG . error ( "IOException: " , e ) ; } }
@ Test public void testDateFolderDistributor ( ) throws Exception { log . info ( "------  testDateFolderDistributor  ------" ) ; File f = setUpFlagDir ( ) ; createTestFiles ( 5 , 5 ) ; createBogusTestFiles ( 5 , 5 ) ; createCopyingTestFiles ( 5 , 5 ) ; fmc . setDistributorType ( "folderdate" ) ; FlagMaker instance = new TestWrappedFlagMaker ( fmc ) ; instance . processFlags ( ) ; File [ ] flags = f . listFiles ( pathname -> pathname . toString ( ) . endsWith ( "flag" ) ) ; assertEquals ( "Incorrect number of flags: " + Arrays . toString ( flags ) , 5 , flags . length ) ; HashSet < Long > buckets = new HashSet < > ( ) ; DateUtils du = new DateUtils ( ) ; code_block = ForStatement ; }
public void test() { try { response = new ResponseData ( vulnerabilityService . getVulnerabilityByAppAndEnv ( assetGroup , "tags.Application.keyword" , "" ) ) ; } catch ( Exception e ) { LOGGER . error ( e . getMessage ( ) ) ; return ResponseUtils . buildFailureResponse ( e ) ; } }
public void test() { if ( null == user ) { _logger . error ( "Null user" ) ; return false ; } }
@ Override public void createPages ( ) { Stopwatch stopwatch = Stopwatch . createStarted ( ) ; createModel ( ) ; Display display = getSite ( ) . getShell ( ) . getDisplay ( ) ; code_block = IfStatement ; getContainer ( ) . addControlListener ( new ControlAdapter ( ) code_block = "" ; ) ; display . asyncExec ( this :: updateProblemIndication ) ; log . debug ( "Created " + stopwatch . toString ( ) ) ; }
public void test() { if ( isConnected ( ) ) { logger . info ( "Trying to connect to TV" ) ; } else { sslContextFactory . setTrustAll ( true ) ; sslContextFactory . setEndpointIdentificationAlgorithm ( null ) ; WebSocketClient client = this . client ; code_block = IfStatement ; TibberWebSocketListener socket = this . socket ; code_block = IfStatement ; ClientUpgradeRequest newRequest = new ClientUpgradeRequest ( ) ; newRequest . setHeader ( "Authorization" , "Bearer " + tibberConfig . getToken ( ) ) ; newRequest . setSubProtocols ( "graphql-subscriptions" ) ; code_block = TryStatement ;  code_block = TryStatement ;  } }
public void test() { try { logger . debug ( "Starting Websocket connection" ) ; client . start ( ) ; } catch ( Exception e ) { logger . debug ( "WebSocket connection Error" , e ) ; } }
public void test() { try { logger . debug ( "Connecting Websocket connection" ) ; sessionFuture = client . connect ( socket , new URI ( SUBSCRIPTION_URL ) , newRequest ) ; } catch ( IOException e ) { logger . warn ( "Websocket URI Exception: {}" , e . getMessage ( ) ) ; } catch ( URISyntaxException e ) { logger . warn ( "Websocket URI Exception: {}" , e . getMessage ( ) ) ; } }
public void test() { try { logger . debug ( "Connecting Websocket connection" ) ; sessionFuture = client . connect ( socket , new URI ( SUBSCRIPTION_URL ) , newRequest ) ; } catch ( IOException e ) { logger . warn ( "Websocket Connect Exception: {}" , e . getMessage ( ) ) ; } catch ( URISyntaxException e ) { logger . warn ( "Websocket Connect Exception: {}" , e . getMessage ( ) ) ; } }
@ Override public void executeUnit ( Person unit ) { LOGGER . warn ( "Executing: " + unit ) ; executed . add ( unit . getId ( ) ) ; }
public void test() { if ( d == null ) { _logger . error ( "Unable to find datanode " + d . getName ( ) ) ; return false ; } }
public void test() { if ( d . getProviderNo ( ) == null || d . getProviderNo ( ) . equals ( "" ) ) { _log . error ( "invalid/provider no=" + d . getId ( ) ) ; return false ; } }
public void test() { if ( d . getDemographicId ( ) == null || d . getDemographicId ( ) < 0 ) { log . error ( "Unable to find Demographic '" + d . getDemographicId ( ) + "'" ) ; return false ; } }
public void test() { if ( d . getRxDate ( ) == null ) { log . error ( "rxDate is null" ) ; return false ; } }
public void test() { if ( d . getEndDate ( ) == null || d . getRxDate ( ) . after ( d . getEndDate ( ) ) ) { _log . error ( "end date (" + d . getRxDate ( ) + ") has ended" ) ; return false ; } }
@ Test public void testMarkerFiltering ( ) throws Exception { Slf4jLogger log = new Slf4jLogger ( LoggerFactory . getLogger ( Slf4jLoggerMarkerTest . class ) ) ; log . warning ( "IGNORE_ME" , "Ignored warning" , null ) ; log . info ( "IGNORE_ME" , "Ignored info" ) ; log . debug ( "IGNORE_ME" , "Ignored debug" ) ; log . trace ( "IGNORE_ME" , "Ignored trace" ) ; log . error ( "ACCEPT_ME" , "Accepted error" , null ) ; log . warning ( "ACCEPT_ME" , "Accepted warning" , null ) ; log . info ( "ACCEPT_ME" , "Accepted info" ) ; log . debug ( "ACCEPT_ME" , "Accepted debug" ) ; log . trace ( "ACCEPT_ME" , "Accepted trace" ) ; File allFile = U . resolveIgnitePath ( LOG_ALL ) ; assertNotNull ( allFile ) ; String all = U . readFileToString ( allFile . getPath ( ) , "UTF-8" ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored error" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored warning" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored info" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored debug" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored trace" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted error" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted warning" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted info" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted debug" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted trace" ) ) ; File filteredFile = U . resolveIgnitePath ( LOG_FILTERED ) ; assertNotNull ( filteredFile ) ; String filtered = U . readFileToString ( filteredFile ) ; String
@ Test public void testMarkerFiltering ( ) throws Exception { Slf4jLogger log = new Slf4jLogger ( LoggerFactory . getLogger ( Slf4jLoggerMarkerTest . class ) ) ; log . error ( "IGNORE_ME" , "Ignored error" , null ) ; log . warning ( "IGNORE_ME" , "Ignored warning" , null ) ; log . info ( "IGNORE_ME" , "Ignored debug" ) ; log . trace ( "IGNORE_ME" , "Ignored trace" ) ; log . error ( "ACCEPT_ME" , "Accepted error" , null ) ; log . warning ( "ACCEPT_ME" , "Accepted warning" , null ) ; log . info ( "ACCEPT_ME" , "Accepted info" ) ; log . debug ( "ACCEPT_ME" , "Accepted debug" ) ; log . trace ( "ACCEPT_ME" , "Accepted trace" ) ; File allFile = U . resolveIgnitePath ( LOG_ALL ) ; assertNotNull ( allFile ) ; String all = U . readFileToString ( allFile . getPath ( ) , "UTF-8" ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored error" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored warning" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored info" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored debug" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored trace" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted error" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted warning" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted info" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted debug" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted trace" ) ) ; File filteredFile = U . resolveIgnitePath ( LOG_FILTERED ) ; assertNotNull ( filteredFile ) ; String filtered = U . readFileToString ( allFile .
@ Test public void testMarkerFiltering ( ) throws Exception { Slf4jLogger log = new Slf4jLogger ( LoggerFactory . getLogger ( Slf4jLoggerMarkerTest . class ) ) ; log . error ( "IGNORE_ME" , "Ignored error" , null ) ; log . warning ( "IGNORE_ME" , "Ignored warning" , null ) ; log . info ( "IGNORE_ME" , "Ignored info" ) ; log . trace ( "IGNORE_ME" , "Ignored trace" ) ; log . error ( "ACCEPT_ME" , "Accepted error" , null ) ; log . warning ( "ACCEPT_ME" , "Accepted warning" , null ) ; log . info ( "ACCEPT_ME" , "Accepted info" ) ; log . debug ( "ACCEPT_ME" , "Accepted debug" ) ; log . trace ( "ACCEPT_ME" , "Accepted trace" ) ; File allFile = U . resolveIgnitePath ( LOG_ALL ) ; assertNotNull ( allFile ) ; String all = U . readFileToString ( allFile . getPath ( ) , "UTF-8" ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored error" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored warning" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored info" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored debug" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored trace" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted error" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted warning" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted info" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted debug" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted trace" ) ) ; File filteredFile = U . resolveIgnitePath ( LOG_FILTERED ) ; assertNotNull ( filteredFile ) ; String filtered = U . readFileToString ( allFile .
@ Test public void testMarkerFiltering ( ) throws Exception { Slf4jLogger log = new Slf4jLogger ( LoggerFactory . getLogger ( Slf4jLoggerMarkerTest . class ) ) ; log . error ( "IGNORE_ME" , "Ignored error" , null ) ; log . warning ( "IGNORE_ME" , "Ignored warning" , null ) ; log . info ( "IGNORE_ME" , "Ignored info" ) ; log . debug ( "IGNORE_ME" , "Ignored debug" ) ; log . error ( "ACCEPT_ME" , "Accepted error" , null ) ; log . warning ( "ACCEPT_ME" , "Accepted warning" , null ) ; log . info ( "ACCEPT_ME" , "Accepted info" ) ; log . debug ( "ACCEPT_ME" , "Accepted debug" ) ; log . trace ( "ACCEPT_ME" , "Accepted trace" ) ; File allFile = U . resolveIgnitePath ( LOG_ALL ) ; assertNotNull ( allFile ) ; String all = U . readFileToString ( allFile . getPath ( ) , "UTF-8" ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored error" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored warning" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored info" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored debug" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored trace" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted error" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted warning" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted info" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted debug" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted trace" ) ) ; File filteredFile = U . resolveIgnitePath ( LOG_FILTERED ) ; assertNotNull ( filteredFile ) ; String filtered = U . readFileToString ( allFile .
@ Test public void testMarkerFiltering ( ) throws Exception { Slf4jLogger log = new Slf4jLogger ( LoggerFactory . getLogger ( Slf4jLoggerMarkerTest . class ) ) ; log . error ( "IGNORE_ME" , "Ignored error" , null ) ; log . warning ( "IGNORE_ME" , "Ignored warning" , null ) ; log . info ( "IGNORE_ME" , "Ignored info" ) ; log . debug ( "IGNORE_ME" , "Ignored debug" ) ; log . trace ( "IGNORE_ME" , "Ignored trace" ) ; log . warning ( "ACCEPT_ME" , "Accepted warning" , null ) ; log . info ( "ACCEPT_ME" , "Accepted info" ) ; log . debug ( "ACCEPT_ME" , "Accepted debug" ) ; log . trace ( "ACCEPT_ME" , "Accepted trace" ) ; File allFile = U . resolveIgnitePath ( LOG_ALL ) ; assertNotNull ( allFile ) ; String all = U . readFileToString ( allFile . getPath ( ) , "UTF-8" ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored error" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored warning" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored info" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored debug" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored trace" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted error" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted warning" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted info" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted debug" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted trace" ) ) ; File filteredFile = U . resolveIgnitePath ( LOG_FILTERED ) ; assertNotNull ( filteredFile ) ; String filtered = U . readFileToString ( filteredFile ) ; String
@ Test public void testMarkerFiltering ( ) throws Exception { Slf4jLogger log = new Slf4jLogger ( LoggerFactory . getLogger ( Slf4jLoggerMarkerTest . class ) ) ; log . error ( "IGNORE_ME" , "Ignored error" , null ) ; log . warning ( "IGNORE_ME" , "Ignored warning" , null ) ; log . info ( "IGNORE_ME" , "Ignored info" ) ; log . debug ( "IGNORE_ME" , "Ignored debug" ) ; log . trace ( "IGNORE_ME" , "Ignored trace" ) ; log . error ( "ACCEPT_ME" , "Accepted error" , null ) ; log . warning ( "ACCEPT_ME" , "Accepted warning" , null ) ; log . debug ( "ACCEPT_ME" , "Accepted debug" ) ; log . trace ( "ACCEPT_ME" , "Accepted trace" ) ; File allFile = U . resolveIgnitePath ( LOG_ALL ) ; assertNotNull ( allFile ) ; String all = U . readFileToString ( allFile . getPath ( ) , "UTF-8" ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored error" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored warning" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored info" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored debug" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored trace" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted error" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted warning" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted info" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted debug" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted trace" ) ) ; File filteredFile = U . resolveIgnitePath ( LOG_FILTERED ) ; assertNotNull ( filteredFile ) ; String filtered = U . readFileToString ( allFile .
@ Test public void testMarkerFiltering ( ) throws Exception { Slf4jLogger log = new Slf4jLogger ( LoggerFactory . getLogger ( Slf4jLoggerMarkerTest . class ) ) ; log . error ( "IGNORE_ME" , "Ignored error" , null ) ; log . warning ( "IGNORE_ME" , "Ignored warning" , null ) ; log . info ( "IGNORE_ME" , "Ignored info" ) ; log . debug ( "IGNORE_ME" , "Ignored debug" ) ; log . trace ( "IGNORE_ME" , "Ignored trace" ) ; log . error ( "ACCEPT_ME" , "Accepted error" , null ) ; log . warning ( "ACCEPT_ME" , "Accepted warning" , null ) ; log . info ( "ACCEPT_ME" , "Accepted info" ) ; log . trace ( "ACCEPT_ME" , "Accepted trace" ) ; File allFile = U . resolveIgnitePath ( LOG_ALL ) ; assertNotNull ( allFile ) ; String all = U . readFileToString ( allFile . getPath ( ) , "UTF-8" ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored error" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored warning" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored info" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored debug" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored trace" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted error" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted warning" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted info" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted debug" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted trace" ) ) ; File filteredFile = U . resolveIgnitePath ( LOG_FILTERED ) ; assertNotNull ( filteredFile ) ; String filtered = U . readFileToString ( allFile .
@ Test public void testMarkerFiltering ( ) throws Exception { Slf4jLogger log = new Slf4jLogger ( LoggerFactory . getLogger ( Slf4jLoggerMarkerTest . class ) ) ; log . error ( "IGNORE_ME" , "Ignored error" , null ) ; log . warning ( "IGNORE_ME" , "Ignored warning" , null ) ; log . info ( "IGNORE_ME" , "Ignored info" ) ; log . debug ( "IGNORE_ME" , "Ignored debug" ) ; log . trace ( "IGNORE_ME" , "Ignored trace" ) ; log . error ( "ACCEPT_ME" , "Accepted error" , null ) ; log . warning ( "ACCEPT_ME" , "Accepted warning" , null ) ; log . info ( "ACCEPT_ME" , "Accepted info" ) ; log . debug ( "ACCEPT_ME" , "Accepted debug" ) ; File allFile = U . resolveIgnitePath ( LOG_ALL ) ; assertNotNull ( allFile ) ; String all = U . readFileToString ( allFile . getPath ( ) , "UTF-8" ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored error" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored warning" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored info" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored debug" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored trace" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted error" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted warning" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted info" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted debug" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted trace" ) ) ; File filteredFile = U . resolveIgnitePath ( LOG_FILTERED ) ; assertNotNull ( filteredFile ) ; String filtered = U . readFileToString ( allFile .
public void test() { try { int returnValue = CommerceDiscountRelServiceUtil . getCategoriesByCommerceDiscountIdCount ( commerceDiscountId , name ) ; return returnValue ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try { return file . getCanonicalPath ( ) ; } catch ( IOException e ) { log . warn ( "Unable to get canonical path" , e ) ; return file . getAbsolutePath ( ) ; } }
@ Override public void execute ( ) throws Exception { logger . debug ( "Executing samples command line" ) ; String subCommandString = getParsedSubCommand ( samplesCommandOptions . jCommander ) ; RestResponse queryResponse = null ; code_block = SwitchStatement ; createOutput ( queryResponse ) ; }
public void test() { switch ( subCommandString ) { case "create" : queryResponse = create ( ) ; break ; case "load" : queryResponse = load ( ) ; break ; case "info" : queryResponse = info ( ) ; break ; case "search" : queryResponse = search ( ) ; break ; case "update" : queryResponse = update ( ) ; break ; case "delete" : queryResponse = delete ( ) ; break ; case "stats" : queryResponse = stats ( ) ; break ; case "acl" : queryResponse = acl ( ) ; break ; case "acl-update" : queryResponse = updateAcl ( ) ; break ; case "annotation-sets-update" : queryResponse = updateAnnotations ( ) ; break ; default : logger . error ( "Subcommand not valid" ) ; break ; } }
public void test() { try { code_block = IfStatement ; } catch ( InterruptedException e ) { Log . warn ( e . toString ( ) , e ) ; } }
public void test() { if ( jobRequest != null && jobRequest . getRequestor ( ) != null ) { logger . debug ( "Returning existing job request" ) ; return jobRequest ; } else { logger . debug ( "Creating new job request" ) ; jobRequest = new JobRequest ( getJobUser ( request ) ) ; request . getSession ( ) . setAttribute ( JOB_REQUEST_ATTR , jobRequest ) ; return jobRequest ; } }
public void test() { if ( jobRequest != null && jobRequest . getRequestor ( ) != null ) { logger . debug ( "Getting old job request" ) ; return jobRequest ; } else { logger . debug ( "Getting new job request" ) ; jobRequest = new JobRequest ( getJobUser ( request ) ) ; request . getSession ( ) . setAttribute ( JOB_REQUEST_ATTR , jobRequest ) ; return jobRequest ; } }
@ Test public void testReadEntity ( ) throws Exception { final TestOlingo4ResponseHandler < ClientEntity > responseHandler = new TestOlingo4ResponseHandler < > ( ) ; olingoApp . read ( edm , TEST_AIRLINE , null , null , responseHandler ) ; ClientEntity entity = responseHandler . await ( ) ; assertEquals ( "Shanghai Airline" , entity . getProperty ( "Name" ) . getValue ( ) . toString ( ) ) ; LOG . info ( "Single Person:  {}" , prettyPrint ( entity ) ) ; responseHandler . reset ( ) ; olingoApp . read ( edm , TEST_PEOPLE , null , null , responseHandler ) ; entity = responseHandler . await ( ) ; assertEquals ( "Russell" , entity . getProperty ( "FirstName" ) . getValue ( ) . toString ( ) ) ; LOG . info ( "Single Entry:  {}" , prettyPrint ( entity ) ) ; responseHandler . reset ( ) ; final Map < String , String > queryParams = new HashMap < > ( ) ; queryParams . put ( SystemQueryOptionKind . EXPAND . toString ( ) , TRIPS ) ; olingoApp . read ( edm , TEST_PEOPLE , queryParams , null , responseHandler ) ; ClientEntity entityExpanded = responseHandler . await ( ) ; LOG . info ( "Single People Entiry with expanded Trips relation:  {}" , prettyPrint ( entityExpanded ) ) ; }
@ Test public void testReadEntity ( ) throws Exception { final TestOlingo4ResponseHandler < ClientEntity > responseHandler = new TestOlingo4ResponseHandler < > ( ) ; olingoApp . read ( edm , TEST_AIRLINE , null , null , responseHandler ) ; ClientEntity entity = responseHandler . await ( ) ; assertEquals ( "Shanghai Airline" , entity . getProperty ( "Name" ) . getValue ( ) . toString ( ) ) ; LOG . info ( "Single Entity:  {}" , prettyPrint ( entity ) ) ; responseHandler . reset ( ) ; olingoApp . read ( edm , TEST_PEOPLE , null , null , responseHandler ) ; entity = responseHandler . await ( ) ; assertEquals ( "Russell" , entity . getProperty ( "FirstName" ) . getValue ( ) . toString ( ) ) ; responseHandler . reset ( ) ; final Map < String , String > queryParams = new HashMap < > ( ) ; queryParams . put ( SystemQueryOptionKind . EXPAND . toString ( ) , TRIPS ) ; olingoApp . read ( edm , TEST_PEOPLE , queryParams , null , responseHandler ) ; ClientEntity entityExpanded = responseHandler . await ( ) ; LOG . info ( "Single People Entiry with expanded Trips relation:  {}" , prettyPrint ( entityExpanded ) ) ; LOG . info ( "Single People Entiry with expanded Trips relation:  {}" , prettyPrint ( queryParams ) ) ; }
@ Test public void testReadEntity ( ) throws Exception { final TestOlingo4ResponseHandler < ClientEntity > responseHandler = new TestOlingo4ResponseHandler < > ( ) ; olingoApp . read ( edm , TEST_AIRLINE , null , null , responseHandler ) ; ClientEntity entity = responseHandler . await ( ) ; assertEquals ( "Shanghai Airline" , entity . getProperty ( "Name" ) . getValue ( ) . toString ( ) ) ; LOG . info ( "Single Entity:  {}" , prettyPrint ( entity ) ) ; responseHandler . reset ( ) ; olingoApp . read ( edm , TEST_PEOPLE , null , null , responseHandler ) ; entity = responseHandler . await ( ) ; assertEquals ( "Russell" , entity . getProperty ( "FirstName" ) . getValue ( ) . toString ( ) ) ; LOG . info ( "Single Entry:  {}" , prettyPrint ( entity ) ) ; responseHandler . reset ( ) ; final Map < String , String > queryParams = new HashMap < > ( ) ; queryParams . put ( SystemQueryOptionKind . EXPAND . toString ( ) , TRIPS ) ; olingoApp . read ( edm , TEST_PEOPLE , queryParams , null , responseHandler ) ; ClientEntity entityExpanded = responseHandler . await ( ) ; LOG . info ( "Single Entities:  {}" , prettyPrint ( entityExpanded ) ) ; }
@ Override public void deleteBuildGroup ( ) throws RepositoryManagerException { logger . info ( "BEGIN: Removing build aggregation group: {}" , buildContentId ) ; userLog . info ( "Removing build aggregation group" ) ; StopWatch stopWatch = StopWatch . createStarted ( ) ; code_block = TryStatement ;  stopWatch . reset ( ) ; logger . info ( "END: Removing build aggregation group: {}" , buildContentId ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { Result result = buffer . pollLast ( waitTime . get ( ) , TimeUnit . MILLISECONDS ) ; code_block = IfStatement ; } catch ( InterruptedException e ) { LOG . error ( "Interrupted while waiting for the result." , e ) ; } catch ( JsonProcessingException e ) { LOG . error ( "Failed to generate the JSON for a result." , e ) ; } catch ( IOException e ) { LOG . error ( "Failed to write JSON to output stream for a result" , e ) ; } }
public void test() { try { Result result = buffer . pollLast ( waitTime . get ( ) , TimeUnit . MILLISECONDS ) ; code_block = IfStatement ; } catch ( InterruptedException e ) { LOG . error ( "ResultLog thread interrupted." , e ) ; } catch ( JsonProcessingException e ) { LOG . error ( "Failed to parse result log." , e ) ; } catch ( IOException e ) { LOG . error ( "Failed to write JSON to output stream for a result" , e ) ; } }
public void test() { try { Result result = buffer . pollLast ( waitTime . get ( ) , TimeUnit . MILLISECONDS ) ; code_block = IfStatement ; } catch ( InterruptedException e ) { LOG . error ( "ResultLog thread interrupted." , e ) ; } catch ( JsonProcessingException e ) { LOG . error ( "Failed to generate the JSON for a result." , e ) ; } catch ( IOException e ) { LOG . error ( "Failed to generate the result." , e ) ; } }
public void joinTournament ( String engineEndpoint , int numOfPlayers ) { this . getStatus ( ) . setState ( PlayerStatus . State . Requesting ) ; Client client = ClientBuilder . newClient ( new ClientConfig ( ) . register ( LoggingFeature . class ) ) ; WebTarget webTarget = client . target ( engineEndpoint ) . path ( "api/v0.1/tournament/join" ) ; Invocation . Builder invocationBuilder = webTarget . request ( MediaType . APPLICATION_JSON ) ; GameTicket ticket = new GameTicket ( this , numOfPlayers ) ; Response response = invocationBuilder . put ( Entity . entity ( ticket , MediaType . APPLICATION_JSON ) ) ; GameStatusResponse status = response . readEntity ( GameStatusResponse . class ) ; logger . info ( "Game: " + result . toString ( ) ) ; code_block = IfStatement ; }
public void test() { try { x . close ( ) ; } catch ( Exception e ) { logger . warn ( "close error" , e ) ; } }
public void test() { try { ThemeDisplay themeDisplay = ( ThemeDisplay ) httpServletRequest . getAttribute ( WebKeys . THEME_DISPLAY ) ; return StringBundler . concat ( "<iframe data-video-liferay height=\"315\" frameborder=\"0\" " , "src=\"" , _dlURLHelper . getPreviewURL ( fileVersion . getFileEntry ( ) , fileVersion , themeDisplay , "&videoEmbed=true" , true , false ) , "\" width=\"560\"></iframe>" ) ; } catch ( PortalException portalException ) { _log . error ( portalException , portalException ) ; return null ; } }
public void test() { if ( ( se . getErrorCode ( ) == 50000 ) && ( se . getSQLState ( ) . equals ( "XJ015" ) ) ) { LOG . info ( "Derby shutdown complete." ) ; } else { LOG . info ( "Derby shutdown complete abnormally. - message:" + se . getMessage ( ) ) ; } }
public void test() { if ( ( se . getErrorCode ( ) == 50000 ) && ( se . getSQLState ( ) . equals ( "XJ015" ) ) ) { LOG . info ( "Derby shutdown complete normally." ) ; } else { LOG . error ( "Exception while executing derby" , se . getErrorCode ( ) ) ; } }
public void test() { try { int returnValue = SiteNavigationMenuServiceUtil . getSiteNavigationMenusCount ( groupIds ) ; return returnValue ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try { String insertPNSP = "INSERT INTO `cloud`.`physical_network_service_providers` (`uuid`, `physical_network_id` , `provider_name`, `state` ," + "`destination_physical_network_id`, `vpn_service_provided`, `dhcp_service_provided`, `dns_service_provided`, `gateway_service_provided`," + "`firewall_service_provided`, `source_nat_service_provided`, `load_balance_service_provided`, `static_nat_service_provided`," + "`port_forwarding_service_provided`, `user_data_service_provided`, `security_group_service_provided`) VALUES (?,?,?,?,0,1,1,1,1,1,1,1,1,1,0)" ; String routerUUID = UUID . randomUUID ( ) . toString ( ) ; pstmtUpdate = conn . prepareStatement ( insertPNSP ) ; pstmtUpdate . setString ( 1 , routerUUID ) ; pstmtUpdate . setLong ( 2 , physicalNetworkId ) ; pstmtUpdate . setString ( 3 , "VirtualRouter" ) ; pstmtUpdate . setString ( 4 , "Enabled" ) ; pstmtUpdate . executeUpdate ( ) ; pstmtUpdate . close ( ) ; String fetchNSPid = "SELECT id from `cloud`.`physical_network_service_providers` where physical_network_id=" + physicalNetworkId + " AND provider_name = 'VirtualRouter' AND uuid = ?" ; pstmt2 = conn . prepareStatement ( fetchNSPid ) ; pstmt2 . setString ( 1 , routerUUID ) ; ResultSet rsNSPid = pstmt2 . executeQuery ( ) ; rsNSPid . next ( ) ; long nspId = rsNSPid . getLong ( 1 ) ; pstmt2 . close ( ) ; String insertRouter = "INSERT INTO `cloud`.`virtual_router_providers` (`nsp_id`, `uuid` , `type` , `enabled`) " + "VALUES (?,?,?,?)" ; pstmtUpdate = conn . prepareStatement ( fetchNSP
public void test() { switch ( child . getName ( ) ) { case "NUMBERS" : formatOption = child . toString ( ) ; break ; case "ABDRUCK_NAME" : nameOption = child . toString ( ) ; break ; case "ALL_VERSIONS_HIGHLIGHT_COLOR" : highlightColors . put ( PrintBlockSignature . ALL_VERSIONS , checkHighlightColor ( child . toString ( ) ) ) ; break ; case "DRAFT_ONLY_HIGHLIGHT_COLOR" : highlightColors . put ( PrintBlockSignature . DRAFT_ONLY , checkHighlightColor ( child . toString ( ) ) ) ; break ; case "NOT_IN_ORIGINAL_HIGHLIGHT_COLOR" : highlightColors . put ( PrintBlockSignature . NOT_IN_ORIGINAL , checkHighlightColor ( child . toString ( ) ) ) ; break ; case "ORIGINAL_ONLY_HIGHLIGHT_COLOR" : highlightColors . put ( PrintBlockSignature . ORIGINAL_ONLY , checkHighlightColor ( child . toString ( ) ) ) ; break ; case "COPY_ONLY_HIGHLIGHT_COLOR" : highlightColors . put ( PrintBlockSignature . COPY_ONLY , checkHighlightColor ( child . toString ( ) ) ) ; break ; default : LOG . warn ( "Unknown child type: " + child . getName ( ) ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( NodeNotFoundException e ) { LOGGER . error ( e ) ; } finally { format = NumberFormat . valueOf ( formatOption . toUpperCase ( ) ) ; LOGGER . debug ( "\"Verwende Zahlenformat '{}' aus Attribut NUMBERS.\"" , format ) ; copyName = nameOption ; LOGGER . debug ( "Verwende ABDRUCK_NAME '{}'" , copyName ) ; } }
public void test() { try { List < ConfidenceData > objList = ( List < ConfidenceData > ) criteria . list ( ) ; return objList ; } catch ( HibernateException e ) { logger . error ( "exception" , e ) ; e . printStackTrace ( ) ; } }
@ Override protected Map < String , String > modifyIniFile ( String commaSeparatedFilePaths , String commaSeparatedParentAccessions ) { Map < String , String > iniFileMap = new TreeMap < > ( ) ; iniFileMap . put ( ReservedIniKeys . INPUT_FILE . getKey ( ) , commaSeparatedFilePaths ) ; LOG . info ( "Modify ini file: {}" , commaSeparatedFilePaths ) ; return iniFileMap ; }
public void test() { try { handleCommandInternal ( channelUID , command ) ; updateModuleStatus ( ) ; } catch ( SmartherIllegalPropertyValueException e ) { logger . debug ( "Smarther not supported" , e ) ; } catch ( SmartherGatewayException e ) { updateStatus ( ThingStatus . OFFLINE , ThingStatusDetail . COMMUNICATION_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { try { executors . execute ( new Runnable ( ) code_block = "" ; ) ; } catch ( Exception e ) { logger . error ( "Failed to execute method" , e ) ; } }
public void test() { try { final List < Principal > users = createTestNodes ( testUserType , 3 ) ; final List < TestThree > testThrees = new LinkedList < > ( ) ; final Random random = new Random ( ) ; String uuid = null ; int count = 0 ; code_block = TryStatement ;  code_block = TryStatement ;  RestAssured . given ( ) . contentType ( "application/json; charset=UTF-8" ) . filter ( ResponseLoggingFilter . logResponseIfStatusCodeIs ( 500 ) ) . expect ( ) . statusCode ( 200 ) . when ( ) . get ( concat ( "/test_threes?sort=createdDate&owner=" + uuid + "&enumProperty=" + TestEnum . Status1 ) ) ; } catch ( FrameworkException ex ) { logger . info ( "" , ex ) ; ex . printStackTrace ( ) ; fail ( "Unexpected exception" ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "to be updated object" ) ; logger . debug ( objectAsXmlString ( uocCommon , UocCommon . class ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "to be created, uoc common" ) ; logger . debug ( objectAsXmlString ( uoc , ToStringBuilder . class ) ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Failed to clean up." , e ) ; } }
public void runGenomeInfo ( ) throws IOException , InterruptedException { logger . info ( "Running generation..." ) ; String outputFolder = downloadFolder . getParent ( ) . toAbsolutePath ( ) . toString ( ) + "/generated_json/" ; code_block = IfStatement ; }
@ Test public void resolveOfEmailWithTagsReturnsEntity ( ) throws Exception { idsMan . addEntity ( new IdentityParam ( EmailIdentity . ID , "a+zzz@ex.com" ) , "cr-pass" , EntityState . valid ) ; HttpClient client = getClient ( ) ; HttpHost host = new HttpHost ( "localhost" , 53456 , "https" ) ; HttpContext localcontext = getClientContext ( host ) ; HttpGet resolve = new HttpGet ( "/restadm/v1/resolve/email/a+foo@ex.com" ) ; HttpResponse response = client . execute ( host , resolve , localcontext ) ; String contents = EntityUtils . toString ( response . getEntity ( ) ) ; LOG . info ( contents ) ; assertEquals ( contents , Status . OK . getStatusCode ( ) , response . getStatusLine ( ) . getStatusCode ( ) ) ; }
public IRODSMidLevelProtocol currentConnectionCheckRenewalOfSocket ( final IRODSAccount irodsAccount ) throws AuthenticationException , JargonException { log . debug ( "currentConnectionCheckRenewalOfSocket()" ) ; code_block = IfStatement ; IRODSMidLevelProtocol irodsMidLevelProtocol = currentConnection ( irodsAccount ) ; log . debug ( "evaluate conn for renewal:{}" , irodsAccount ) ; boolean shutdown = evaluateConnectionForRenewal ( irodsMidLevelProtocol ) ; code_block = IfStatement ; }
public IRODSMidLevelProtocol currentConnectionCheckRenewalOfSocket ( final IRODSAccount irodsAccount ) throws AuthenticationException , JargonException { log . debug ( "renewConnection()" ) ; code_block = IfStatement ; log . debug ( "renewalance()" ) ; IRODSMidLevelProtocol irodsMidLevelProtocol = currentConnection ( irodsAccount ) ; boolean shutdown = evaluateConnectionForRenewal ( irodsMidLevelProtocol ) ; code_block = IfStatement ; }
public void test() { try { action . accept ( aTarget ) ; } catch ( Exception e ) { LOGGER . error ( "Error: " + e . getMessage ( ) , e ) ; Page page = ( Page ) PageRequestHandlerTracker . getLastHandler ( RequestCycle . get ( ) ) . getPage ( ) ; page . error ( "Error: " + e . getMessage ( ) ) ; aTarget . addChildren ( page , IFeedback . class ) ; } }
public void test() { try { client . close ( ) ; } catch ( IOException e ) { logger . error ( "IOException while closing client" , e ) ; } }
public void test() { if ( Boolean . getBoolean ( "zookeeper.jmx.log4j.disable" ) ) { LOGGER . info ( "zookeeper.jmx.log4j.disable is disabled" ) ; } else { code_block = TryStatement ;  } }
public void test() { try { Class . forName ( "org.apache.log4j.jmx.HierarchyDynamicMBean" ) ; enabled = true ; LOG . info ( "Log4j 1.2 jmx support found" ) ; } catch ( ClassNotFoundException e ) { LOG . info ( "Log4j 1.2 jmx support not found; jmx disabled." ) ; } }
public void test() { try { Class . forName ( "org.apache.log4j.jmx.HierarchyDynamicMBean" ) ; enabled = true ; LOG . info ( "Log4j 1.2 jmx support found and enabled." ) ; } catch ( ClassNotFoundException e ) { LOG . info ( "Log4j 1.2 jmx support not enabled." ) ; } }
public void test() { try { pool . invokeAll ( tasks , thinkTime , TimeUnit . SECONDS ) ; pool . awaitTermination ( 1 , TimeUnit . SECONDS ) ; pool . shutdownNow ( ) ; } catch ( InterruptedException | RejectedExecutionException ex ) { logger . warn ( ex . getMessage ( ) ) ; } }
public void test() { if ( USE_MULTIPLE_THREADS ) { ExecutorService pool = Executors . newFixedThreadPool ( poolSize ) ; List < MCTSExecutor > tasks = new ArrayList < > ( ) ; code_block = ForStatement ; code_block = TryStatement ;  int simCount = 0 ; code_block = ForStatement ; tasks . clear ( ) ; totalThinkTime += thinkTime ; totalSimulations += simCount ; logger . info ( "Total: Simulated " + totalSimulations + " games in " + totalThinkTime + " seconds - Average: " + totalSimulations / totalThinkTime ) ; MCTSNode . logHitMiss ( ) ; } else { long startTime = System . nanoTime ( ) ; long endTime = startTime + ( thinkTime * 1000000000l ) ; logger . info ( "Running: {}ms" , thinkTime ) ; MCTSNode current ; int simCount = 0 ; code_block = WhileStatement ; logger . info ( "Simulated " + simCount + " games - nodes in tree: " + root . size ( ) ) ; } }
public void test() { if ( USE_MULTIPLE_THREADS ) { ExecutorService pool = Executors . newFixedThreadPool ( poolSize ) ; List < MCTSExecutor > tasks = new ArrayList < > ( ) ; code_block = ForStatement ; code_block = TryStatement ;  int simCount = 0 ; code_block = ForStatement ; tasks . clear ( ) ; totalThinkTime += thinkTime ; totalSimulations += simCount ; logger . info ( "Player: " + name + " Simulated " + simCount + " games in " + thinkTime + " seconds - nodes in tree: " + root . size ( ) ) ; MCTSNode . logHitMiss ( ) ; } else { long startTime = System . nanoTime ( ) ; long endTime = startTime + ( thinkTime * 1000000000l ) ; logger . info ( "Player: " + name ) ; MCTSNode current ; int simCount = 0 ; code_block = WhileStatement ; logger . info ( "Simulated " + simCount + " games - nodes in tree: " + root . size ( ) ) ; } }
public void test() { if ( USE_MULTIPLE_THREADS ) { ExecutorService pool = Executors . newFixedThreadPool ( poolSize ) ; List < MCTSExecutor > tasks = new ArrayList < > ( ) ; code_block = ForStatement ; code_block = TryStatement ;  int simCount = 0 ; code_block = ForStatement ; tasks . clear ( ) ; totalThinkTime += thinkTime ; totalSimulations += simCount ; logger . info ( "Player: " + name + " Simulated " + simCount + " games in " + thinkTime + " seconds - nodes in tree: " + root . size ( ) ) ; logger . info ( "Total: Simulated " + totalSimulations + " games in " + totalThinkTime + " seconds - Average: " + totalSimulations / totalThinkTime ) ; MCTSNode . logHitMiss ( ) ; } else { long startTime = System . nanoTime ( ) ; long endTime = startTime + ( thinkTime * 1000000000l ) ; logger . info ( "Player: " + name ) ; MCTSNode current ; int simCount = 0 ; code_block = WhileStatement ; } }
void enrichTextUnitsWithUsages ( List < GitBlameWithUsage > gitBlameWithUsages ) { logger . debug ( "Begin Enrich text units with usages" ) ; Map < Long , GitBlameWithUsage > assetTextUnitIdToGitBlameWithUsage = new HashMap < > ( ) ; code_block = ForStatement ; logger . debug ( "Fetch the asset text unit information" ) ; List < AssetTextUnit > assetTextUnits = assetTextUnitRepository . findByIdIn ( new ArrayList < Long > ( assetTextUnitIdToGitBlameWithUsage . keySet ( ) ) ) ; code_block = ForStatement ; logger . debug ( "End Enrich text units with usages" ) ; }
void enrichTextUnitsWithUsages ( List < GitBlameWithUsage > gitBlameWithUsages ) { logger . debug ( "Enrich text units with usages" ) ; logger . debug ( "Enrich text units with usages" ) ; Map < Long , GitBlameWithUsage > assetTextUnitIdToGitBlameWithUsage = new HashMap < > ( ) ; code_block = ForStatement ; logger . debug ( "Fetch the asset text unit information" ) ; List < AssetTextUnit > assetTextUnits = assetTextUnitRepository . findByIdIn ( new ArrayList < Long > ( assetTextUnitIdToGitBlameWithUsage . keySet ( ) ) ) ; code_block = ForStatement ; }
public void test() { try { CalendarBookingServiceUtil . deleteCalendarBookingInstance ( calendarBookingId , startTime , allFollowing ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public CheckPolicyRequestType transformPatientDiscoveryEntityToCheckPolicy ( RespondingGatewayPRPAIN201305UV02RequestType event ) { LOG . debug ( "Begin -- PatientDiscoveryPolicyTransformHelper.transformPatientDiscoveryEntityToCheckPolicy()" ) ; code_block = IfStatement ; CheckPolicyRequestType checkPolicyRequest = transformPRPAIN201305UV02ToCheckPolicy ( event . getPRPAIN201305UV02 ( ) , event . getAssertion ( ) ) ; LOG . debug ( "End -- PatientDiscoveryPolicyTransformHelper.transformPatientDiscoveryEntityToCheckPolicy()" ) ; return checkPolicyRequest ; }
public CheckPolicyRequestType transformPatientDiscoveryEntityToCheckPolicy ( RespondingGatewayPRPAIN201305UV02RequestType event ) { LOG . debug ( "Begin -- PatientDiscoveryPolicyTransformHelper.transformPatientDiscoveryEntityToCheckPolicy()" ) ; code_block = IfStatement ; CheckPolicyRequestType checkPolicyRequest = transformPRPAIN201305UV02ToCheckPolicy ( event . getPRPAIN201305UV02 ( ) , event . getAssertion ( ) ) ; LOG . debug ( "End -- PatientDiscoveryPolicyTransformHelper.transformPatientDiscoveryEntityToCheckPolicy()" ) ; return checkPolicyRequest ; }
@ Test public void testRedefineTerms ( ) throws Exception { logger . info ( "  testRedefineTerms" ) ; EntityManager em = app . getEntityManager ( ) ; assertNotNull ( em ) ; Map < String , Object > properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "username" , "edanuff" ) ; properties . put ( "email" , "ed@anuff.com" ) ; em . create ( "user" , properties ) ; app . waitForQueueDrainAndRefreshIndex ( ) ; String s = "select {name: username, email: email} where username = 'edanuff'" ; Query query = Query . fromQL ( s ) ; Results r = em . searchCollection ( em . getApplicationRef ( ) , "users" , query ) ; assertTrue ( r . size ( ) == 1 ) ; }
public void test() { if ( ( isSatisfiedBy != null ) && isSatisfiedBy . apply ( key ) ) { logger . debug ( "Unable to remove field: " + timeField ) ; unsatisfied . remove ( timeField ) ; } }
public org . talend . mdm . webservice . WSBoolean isItemModifiedByOther ( org . talend . mdm . webservice . WSIsItemModifiedByOther arg0 ) { LOG . info ( "Executing operation isItemModifiedByOther" ) ; System . out . println ( arg0 ) ; code_block = TryStatement ;  }
@ Override public int getNumberOfResources ( ) { logger . info ( "Getting number of resources" ) ; return ResourceManager . getTotalNumberOfWorkers ( ) ; }
public void test() { try { Thread . sleep ( ClusterUtils . START_UP_CHECK_TIME_INTERVAL_MS ) ; } catch ( InterruptedException e ) { Thread . currentThread ( ) . interrupt ( ) ; logger . error ( "Heartbeat check failed" , e ) ; } }
public void test() { try { report . lastReportAt = generateReport ( ) ; code_block = IfStatement ; } catch ( Exception e ) { log . debug ( "Error generating report" , e ) ; ctx . writeAndFlush ( illegalCommand ( message . id ) , ctx . voidPromise ( ) ) ; } }
public void test() { if ( deviceValue == null ) { logger . warn ( "DeviceType is null!" ) ; return UnDefType . UNDEF ; } }
public void test() { if ( deviceValue instanceof String ) { double d = Double . parseDouble ( ( String ) deviceValue ) ; d *= 3.6 ; return new DecimalType ( d ) ; } else-if ( deviceValue instanceof Long ) { double d = ( ( Long ) deviceValue ) . longValue ( ) ; d *= 3.6 ; return new DecimalType ( d ) ; } else-if ( deviceValue instanceof BigDecimal ) { double d = ( ( BigDecimal ) deviceValue ) . doubleValue ( ) ; d *= 3.6 ; return new DecimalType ( d ) ; } else-if ( deviceValue instanceof Number ) { double d = ( ( Number ) deviceValue ) . doubleValue ( ) ; d *= 3.6 ; return new DecimalType ( d ) ; } else { logger . warn ( "Unsupported device value type {}" , deviceValue . getClass ( ) ) ; return UnDefType . UNDEF ; } }
public void test() { try { ctx = getContext ( getSecurityManager ( ) . getCurrentSubject ( ) ) ; SearchResult searchResult = findGroupByGroupName ( ctx , removeDomainPostfix ( name ) ) ; code_block = IfStatement ; final LDAPSearchContext search = ensureContextFactory ( ) . getSearch ( ) ; final String dnGroup = ( String ) searchResult . getAttributes ( ) . get ( search . getSearchGroup ( ) . getSearchAttribute ( LDAPSearchAttributeKey . DN ) ) . get ( ) ; final SearchAttribute sa = new SearchAttribute ( search . getSearchAccount ( ) . getSearchAttribute ( LDAPSearchAttributeKey . MEMBER_OF ) , escapeSearchAttribute ( dnGroup ) ) ; final String searchFilter = buildSearchFilter ( search . getSearchAccount ( ) . getSearchFilterPrefix ( ) , sa ) ; final SearchControls searchControls = new SearchControls ( ) ; searchControls . setSearchScope ( SearchControls . SUBTREE_SCOPE ) ; searchControls . setReturningAttributes ( new String [ ] code_block = "" ; ) ; final NamingEnumeration < SearchResult > results = ctx . search ( search . getBase ( ) , searchFilter , searchControls ) ; code_block = WhileStatement ; } catch ( final NamingException ne ) { LOG . error ( new AuthenticationException ( AuthenticationException . UNNOWN_EXCEPTION , ne . getMessage ( ) ) ) ; } finally { code_block = IfStatement ; } }
public void test() { try { defaultMQAdminExt . createAndUpdateTopicConfig ( addr , topicConfig ) ; } catch ( Exception e ) { logger . error ( "create topic config fail" , e ) ; } }
public void test() { if ( pharmacophoreMolecule . getAtomCount ( ) < pharmacophoreQuery . getAtomCount ( ) ) { logger . error ( "Sha parameters are incorrect." ) ; return false ; } }
public void test() { try { bundle . getString ( key ) ; return getStringPropertyArray ( bundle , key ) ; } catch ( MissingResourceException e ) { logger . info ( "resource not found: " + key ) ; printOutArrayProperty ( key , defaultValue , true ) ; return defaultValue ; } }
public HTTPRequest filterLogAndConvertRe ( HttpRequest request ) { LOGGER . debug ( "" ) ; code_block = ForStatement ; checkRequestHasContentLengthOrChunkedEncoding ( request , "After filtering, the request has neither chunked encoding nor content length: " + request ) ; wirePayloadIfEnabled ( wire , request ) ; HTTPRequest nativeRequest = convertToGaeRequest . apply ( request ) ; utils . logRequest ( headerLog , request , ">>" ) ; return nativeRequest ; }
public void test() { if ( bk . disableEnsembleChangeFeature . isAvailable ( ) ) { blockAddCompletions . decrementAndGet ( ) ; unsetSuccessAndSendWriteRequest ( failedBookies . keySet ( ) ) ; LOG . info ( "Ensemble change failure detected. {}" , failedBookies . keySet ( ) ) ; return ; } }
public void test() { try { stream = asStream ( ) ; code_block = IfStatement ; br = new BufferedReader ( new InputStreamReader ( stream , "UTF-8" ) ) ; StringBuilder buf = new StringBuilder ( ) ; String line ; code_block = WhileStatement ; this . responseAsString = buf . toString ( ) ; stream . close ( ) ; streamConsumed = true ; } catch ( IOException ioe ) { logger . warn ( ioe . getMessage ( ) , ioe ) ; throw new FacebookException ( ioe . getMessage ( ) , ioe ) ; } finally { code_block = IfStatement ; code_block = IfStatement ; disconnectForcibly ( ) ; } }
public void test() { try { exists = BackendConnector . getInstance ( ) . isAppExisting ( CoreConfiguration . buildGoalContextFromGlobalConfiguration ( ) , _app ) ; } catch ( BackendConnectionException e ) { LOG . debug ( "BackendConnectionException:" , e ) ; } }
public void test() { if ( lr . getStateProvince ( ) != null && stateProvinceCentrePoints . coordinatesMatchCentre ( lr . getStateProvince ( ) , lr . getDecimalLatitude ( ) , lr . getDecimalLongitude ( ) ) ) { addIssue ( lr , ALAOccurrenceIssue . COORDINATES_CENTRE_OF_STATEPROVINCE . name ( ) ) ; } else-if ( log . isTraceEnabled ( ) ) { log . trace ( "No stateProvince for location " + lr . getStateProvince ( ) + " -> no stateProvince" ) ; } }
@ Override public BasisFunctionFactory getFactory ( final DataTableSpec spec ) { LOGGER . debug ( "getFactory: " + spec ) ; LOGGER . debug ( "shrink       : " + Shrink . SHRINKS [ m_shrink ] ) ; return new FuzzyBasisFunctionFactory ( m_norm , m_shrink , spec , getTargetColumns ( ) , getDistance ( ) ) ; }
@ Override public BasisFunctionFactory getFactory ( final DataTableSpec spec ) { LOGGER . debug ( "fuzzy_norm   : " + norm . NORMS [ m_norm ] ) ; LOGGER . debug ( "fuzzy_size : " + getDistance ( ) ) ; return new FuzzyBasisFunctionFactory ( m_norm , m_shrink , spec , getTargetColumns ( ) , getDistance ( ) ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { AngelApplicationMaster angelAppMaster = LocalClusterContext . get ( ) . getMaster ( ) . getAppMaster ( ) ; assertTrue ( angelAppMaster != null ) ; AMTaskManager taskManager = angelAppMaster . getAppContext ( ) . getTaskManager ( ) ; Worker worker = LocalClusterContext . get ( ) . getWorker ( worker0Attempt0Id ) . getWorker ( ) ; PSAgentMatrixMetaManager matrixMetaManager = worker . getPSAgent ( ) . getMatrixMetaManager ( ) ; int w1Id = matrixMetaManager . getMatrixId ( "w1" ) ; int w2Id = matrixMetaManager . getMatrixId ( "w2" ) ; MasterClient masterClient = worker . getPSAgent ( ) . getMasterClient ( ) ; AMTask task0 = taskManager . getTask ( task0Id ) ; AMTask task1 = taskManager . getTask ( task1Id ) ; masterClient . updateClock ( task0Id . getIndex ( ) , w1Id , 1 ) ; masterClient . updateClock ( task0Id . getIndex ( ) , w2Id , 1 ) ; Int2IntOpenHashMap matrixClocks = task0 . getMatrixClocks ( ) ; assertEquals ( matrixClocks . size ( ) , 2 ) ; assertEquals ( matrixClocks . get ( w1Id ) , 1 ) ; assertEquals ( matrixClocks . get ( w2Id ) , 1 ) ; masterClient . updateClock ( task0Id . getIndex ( ) , w1Id , 2 ) ; assertEquals ( task0 . getMatrixClock ( w1Id ) , 2 ) ; assertEquals ( task0 . getMatrixClock ( w2Id ) , 1 ) ; masterClient . updateClock ( task1Id . getIndex ( ) , w1Id , 1 ) ; masterClient . updateClock ( task1Id . getIndex ( ) , w2Id , 1 ) ; matrixClocks = task1 . getMatrixClocks ( ) ; assertEquals ( matrixClocks . size ( ) , 2 ) ; assertEquals ( matrixClocks . get ( w1Id ) , 1 ) ; assertEquals ( matrixClocks . get ( w2Id ) , 1 ) ; masterClient . updateClock ( task1Id . getIndex ( ) , w2
public void test() { try { LOG . info ( "===========================testTaskMatrixClock===============================" ) ; AngelApplicationMaster angelAppMaster = LocalClusterContext . get ( ) . getMaster ( ) . getAppMaster ( ) ; assertTrue ( angelAppMaster != null ) ; AMTaskManager taskManager = angelAppMaster . getAppContext ( ) . getTaskManager ( ) ; Worker worker = LocalClusterContext . get ( ) . getWorker ( worker0Attempt0Id ) . getWorker ( ) ; PSAgentMatrixMetaManager matrixMetaManager = worker . getPSAgent ( ) . getMatrixMetaManager ( ) ; int w1Id = matrixMetaManager . getMatrixId ( "w1" ) ; int w2Id = matrixMetaManager . getMatrixId ( "w2" ) ; MasterClient masterClient = worker . getPSAgent ( ) . getMasterClient ( ) ; AMTask task0 = taskManager . getTask ( task0Id ) ; AMTask task1 = taskManager . getTask ( task1Id ) ; masterClient . updateClock ( task0Id . getIndex ( ) , w1Id , 1 ) ; masterClient . updateClock ( task0Id . getIndex ( ) , w2Id , 1 ) ; Int2IntOpenHashMap matrixClocks = task0 . getMatrixClocks ( ) ; assertEquals ( matrixClocks . size ( ) , 2 ) ; assertEquals ( matrixClocks . get ( w1Id ) , 1 ) ; assertEquals ( matrixClocks . get ( w2Id ) , 1 ) ; masterClient . updateClock ( task0Id . getIndex ( ) , w1Id , 2 ) ; assertEquals ( task0 . getMatrixClock ( w1Id ) , 2 ) ; assertEquals ( task0 . getMatrixClock ( w2Id ) , 1 ) ; masterClient . updateClock ( task1Id . getIndex ( ) , w1Id , 1 ) ; masterClient . updateClock ( task1Id . getIndex ( ) , w2Id , 1 ) ; matrixClocks = task1 . getMatrixClocks ( ) ; assertEquals ( matrixClocks . size ( ) , 2 ) ; assertEquals ( matrixClocks . get ( w1Id ) , 1 ) ; assertEquals ( matrixClocks . get ( w2Id ) , 1 )
private void testLoginFromConfig ( ) { log . debug ( "testLoginFromConfig called." ) ; if ( ccm . loginFromConfig ( ) == null ) throw new AssertionError ( "loginFromConfig returned null." ) ; if ( ! ccm . logout ( ) ) throw new AssertionError ( "logout returned false." ) ; log . debug ( "testLoginFromConfig success." ) ; }
private void testLoginFromConfig ( ) { log . debug ( "Starting testLoginFromConfig." ) ; if ( ccm . loginFromConfig ( ) == null ) throw new AssertionError ( "loginFromConfig returned null." ) ; log . debug ( "Logging out." ) ; if ( ! ccm . logout ( ) ) throw new AssertionError ( "logout returned false." ) ; }
public void test() { try { MochawesomeSpecRunReport specRunReport = objectMapper . readValue ( path . toFile ( ) , MochawesomeSpecRunReport . class ) ; specRunReport . fillInTestResults ( results ) ; } catch ( JsonMappingException e ) { log . warn ( "Unable to deserialize spec run report" , e ) ; } }
public void test() { if ( baudRate == 9600 ) { serial . write ( new byte [ ] code_block = "" ; ) ; } else-if ( baudRate == 19200 ) { serial . write ( new byte [ ] code_block = "" ; ) ; } else-if ( baudRate == 38400 ) { serial . write ( new byte [ ] code_block = "" ; ) ; } else { log . warn ( "Not supported on serial port {}" , baudRate ) ; } }
public void test() { try { commandOptional = service . applianceCommand ( appliance ) ; } catch ( IOException e ) { logger . debug ( e . getMessage ( ) , e ) ; return null ; } }
@ Override public void updateUserInfo ( final String userName , final String userInfo ) throws DataNotFoundException , JargonException { log . info ( "updateUserInfo()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "userName:{}" , userName ) ; log . info ( "userInfo:{}" , userInfo ) ; User user = findByName ( userName ) ; log . info ( "looked up user:{}" , user ) ; user . setInfo ( userInfo ) ; this . updateUserInfo ( user ) ; log . info ( "updated info" ) ; }
@ Override public void updateUserInfo ( final String userName , final String userInfo ) throws DataNotFoundException , JargonException { log . info ( "updateUserInfo()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "userName:{}" , userName ) ; log . info ( "userName:{}" , userName ) ; User user = findByName ( userName ) ; log . info ( "looked up user:{}" , user ) ; user . setInfo ( userInfo ) ; this . updateUserInfo ( user ) ; log . info ( "updated info" ) ; }
@ Override public void updateUserInfo ( final String userName , final String userInfo ) throws DataNotFoundException , JargonException { log . info ( "updateUserInfo()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "userName:{}" , userName ) ; log . info ( "userInfo:{}" , userInfo ) ; User user = findByName ( userName ) ; log . info ( "looked up user:{}" , user ) ; user . setInfo ( userInfo ) ; this . updateUserInfo ( user ) ; log . info ( "updated info" ) ; }
@ Override public void updateUserInfo ( final String userName , final String userInfo ) throws DataNotFoundException , JargonException { log . info ( "updateUserInfo()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "userName:{}" , userName ) ; log . info ( "userInfo:{}" , userInfo ) ; User user = findByName ( userName ) ; user . setInfo ( userInfo ) ; log . info ( "set info" ) ; this . updateUserInfo ( user ) ; log . info ( "updated info" ) ; }
@ Override public void updateUserInfo ( final String userName , final String userInfo ) throws DataNotFoundException , JargonException { log . info ( "updateUserInfo()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "userName:{}" , userName ) ; log . info ( "userInfo:{}" , userInfo ) ; User user = findByName ( userName ) ; log . info ( "looked up user:{}" , user ) ; user . setInfo ( userInfo ) ; this . updateUserInfo ( user ) ; log . info ( "updateUserInfo()" ) ; }
public void test() { if ( ! moduleName . matches ( "^[a-zA-Z_]*$" ) ) { LOGGER . error ( "^[a-zA-Z_]*$ not supported" ) ; return false ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( RedirectEntryServiceUtil . class , "deleteRedirectEntry" , _deleteRedirectEntryParameterTypes2 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , redirectEntryId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . redirect . model . RedirectEntry ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { lh = bkc . createLedger ( digestType , ledgerPassword ) ; ledgerId = lh . getId ( ) ; code_block = ForStatement ; synchronized ( sync ) code_block = "" ; long length = numEntriesToWrite * 4 ; assertTrue ( "Ledger length before closing: " + lh . getLength ( ) , lh . getLength ( ) == length ) ; LOG . debug ( "*** WRITE COMPLETE ***" ) ; lh . close ( ) ; lh = bkc . openLedger ( ledgerId , digestType , ledgerPassword ) ; assertTrue ( "Ledger length after opening: " + lh . getLength ( ) , lh . getLength ( ) == length ) ; LOG . debug ( "*** WRITE COMPLETE ***" ) ; lh . close ( ) ; } catch ( BKException e ) { LOG . error ( "Test failed" , e ) ; fail ( "Test failed due to BookKeeper exception" ) ; } catch ( InterruptedException e ) { LOG . error ( "Test failed" , e ) ; fail ( "Test failed due to interruption" ) ; } }
public void test() { try { lh = bkc . createLedger ( digestType , ledgerPassword ) ; ledgerId = lh . getId ( ) ; LOG . info ( "Ledger ID: " + lh . getId ( ) ) ; code_block = ForStatement ; synchronized ( sync ) code_block = "" ; LOG . info ( "Number of entries to write: " + numEntriesToWrite ) ; long length = numEntriesToWrite * 4 ; assertTrue ( "Ledger length before closing: " + lh . getLength ( ) , lh . getLength ( ) == length ) ; lh . close ( ) ; lh = bkc . openLedger ( ledgerId , digestType , ledgerPassword ) ; assertTrue ( "Ledger length after opening: " + lh . getLength ( ) , lh . getLength ( ) == length ) ; lh . close ( ) ; } catch ( BKException e ) { LOG . error ( "Test failed" , e ) ; fail ( "Test failed due to BookKeeper exception" ) ; } catch ( InterruptedException e ) { LOG . error ( "Test failed" , e ) ; fail ( "Test failed due to interruption" ) ; } }
public void test() { if ( receiver == null ) { RecordLog . warn ( "[HeartbeatSenderNotRegistered]" ) ; return ; } }
public void test() { if ( ! scheduler . isTerminated ( ) ) { LOG . info ( "Periodic progress reporting stopped." ) ; scheduler . awaitTermination ( Long . MAX_VALUE , TimeUnit . SECONDS ) ; LOG . info ( "Periodic progress reporting terminated." ) ; } }
@ Override public void onDisconnect ( InternalDistributedSystem sys ) { logger . debug ( "Initiating AdminDistributedSystemImplonDisconnect" ) ; disconnect ( ) ; logger . debug ( "Completed AdminDistributedSystemImplonDisconnect" ) ; }
@ Override public void onDisconnect ( InternalDistributedSystem sys ) { logger . debug ( "Calling AdminDistributedSystemImplonDisconnect" ) ; disconnect ( ) ; logger . debug ( "Completed AdminDistributedSystemImplonDisconnect" ) ; }
public void test() { try ( LoggingContext loggingContext = LoggingContext . forConnector ( connName ) ) { log . trace ( "Creating connector for {}" , connName ) ; WorkerConnector workerConnector = connectors . get ( connName ) ; code_block = IfStatement ; ClassLoader savedLoader = plugins . currentThreadLoader ( ) ; code_block = TryStatement ;  } }
public void test() { if ( workerConnector == null ) { log . warn ( "Can't find worker connector with id {}" , workerId ) ; return ; } }
public void test() { try { URL url = new URL ( current_uri . toString ( ) ) ; HttpsURLConnection urlConn = ( HttpsURLConnection ) url . openConnection ( ) ; urlConn . setRequestMethod ( "GET" ) ; urlConn . setDoOutput ( true ) ; urlConn . setRequestProperty ( REQUEST_PROPERTY_ACCEPT , REQUEST_VALUE_TEXT_JSON ) ; return fillMarketoRecordResultFromReader ( getReaderFromHttpResponse ( urlConn ) , schema ) ; } catch ( IOException e ) { LOG . error ( "{}" , e . getMessage ( ) ) ; throw new MarketoException ( REST , e . getMessage ( ) ) ; } }
private void execInContainer ( String containerId , String execCommand , boolean logout ) throws DockerException , InterruptedException { final String [ ] command = code_block = "" ; ; LOGGER . info ( "Executing {}" , execCommand ) ; final ExecCreation execCreation = docker . execCreate ( containerId , command , DockerClient . ExecCreateParam . attachStdout ( ) , DockerClient . ExecCreateParam . attachStderr ( ) ) ; LogStream logStream = docker . execStart ( execCreation . id ( ) ) ; code_block = WhileStatement ; }
@ Test ( timeOut = 10_000 ) public void testCleanupWithDeleteRow ( ITestContext context ) throws Exception { TransactionManager tm = newTransactionManager ( context ) ; TTable tt = new TTable ( hbaseConf , TEST_TABLE ) ; Transaction t1 = tm . begin ( ) ; LOG . info ( "Transaction created " + t1 ) ; int rowcount = 10 ; int count = 0 ; byte [ ] fam = Bytes . toBytes ( TEST_FAMILY ) ; byte [ ] col = Bytes . toBytes ( "testdata" ) ; byte [ ] data1 = Bytes . toBytes ( "testWrite-1" ) ; byte [ ] data2 = Bytes . toBytes ( "testWrite-2" ) ; byte [ ] modrow = Bytes . toBytes ( "test-del" + 3 ) ; code_block = ForStatement ; tm . commit ( t1 ) ; Transaction t2 = tm . begin ( ) ; LOG . info ( "Transaction created " + t2 ) ; Delete d = new Delete ( modrow ) ; tt . delete ( t2 , d ) ; ResultScanner rs = tt . getScanner ( t2 , new Scan ( ) ) ; Result r = rs . next ( ) ; count = 0 ; code_block = WhileStatement ; assertEquals ( count , rowcount - 1 , "Wrong count" ) ; Transaction t3 = tm . begin ( ) ; LOG . info ( "Transaction created " + t3 ) ; Put p = new Put ( modrow ) ; p . add ( fam , col , data2 ) ; tt . put ( t3 , p ) ; tm . commit ( t3 ) ; boolean aborted = false ; code_block = TryStatement ;  assertTrue ( aborted , "Didn't raise exception" ) ; Transaction tscan = tm . begin ( ) ; rs = tt . getScanner ( tscan , new Scan ( ) ) ; r = rs . next ( ) ; count = 0 ; code_block = WhileStatement ; assertEquals ( count , rowcount , "Wrong count" ) ; }
@ Test ( timeOut = 10_000 ) public void testCleanupWithDeleteRow ( ITestContext context ) throws Exception { TransactionManager tm = newTransactionManager ( context ) ; TTable tt = new TTable ( hbaseConf , TEST_TABLE ) ; Transaction t1 = tm . begin ( ) ; LOG . info ( "Transaction created " + t1 ) ; int rowcount = 10 ; int count = 0 ; byte [ ] fam = Bytes . toBytes ( TEST_FAMILY ) ; byte [ ] col = Bytes . toBytes ( "testdata" ) ; byte [ ] data1 = Bytes . toBytes ( "testWrite-1" ) ; byte [ ] data2 = Bytes . toBytes ( "testWrite-2" ) ; byte [ ] modrow = Bytes . toBytes ( "test-del" + 3 ) ; code_block = ForStatement ; tm . commit ( t1 ) ; Transaction t2 = tm . begin ( ) ; LOG . info ( "Deleting " + t2 ) ; Delete d = new Delete ( modrow ) ; tt . delete ( t2 , d ) ; ResultScanner rs = tt . getScanner ( t2 , new Scan ( ) ) ; Result r = rs . next ( ) ; count = 0 ; code_block = WhileStatement ; assertEquals ( count , rowcount - 1 , "Wrong count" ) ; Transaction t3 = tm . begin ( ) ; LOG . info ( "Transaction created " + t3 ) ; Put p = new Put ( modrow ) ; p . add ( fam , col , data2 ) ; tt . put ( t3 , p ) ; tm . commit ( t3 ) ; boolean aborted = false ; code_block = TryStatement ;  assertTrue ( aborted , "Didn't raise exception" ) ; Transaction tscan = tm . begin ( ) ; rs = tt . getScanner ( tscan , new Scan ( ) ) ; r = rs . next ( ) ; count = 0 ; code_block = WhileStatement ; assertEquals ( count , rowcount , "Wrong count" ) ; }
@ Test ( timeOut = 10_000 ) public void testCleanupWithDeleteRow ( ITestContext context ) throws Exception { TransactionManager tm = newTransactionManager ( context ) ; TTable tt = new TTable ( hbaseConf , TEST_TABLE ) ; Transaction t1 = tm . begin ( ) ; LOG . info ( "Transaction created " + t1 ) ; int rowcount = 10 ; int count = 0 ; byte [ ] fam = Bytes . toBytes ( TEST_FAMILY ) ; byte [ ] col = Bytes . toBytes ( "testdata" ) ; byte [ ] data1 = Bytes . toBytes ( "testWrite-1" ) ; byte [ ] data2 = Bytes . toBytes ( "testWrite-2" ) ; byte [ ] modrow = Bytes . toBytes ( "test-del" + 3 ) ; code_block = ForStatement ; tm . commit ( t1 ) ; Transaction t2 = tm . begin ( ) ; LOG . info ( "Transaction created " + t2 ) ; Delete d = new Delete ( modrow ) ; tt . delete ( t2 , d ) ; ResultScanner rs = tt . getScanner ( t2 , new Scan ( ) ) ; Result r = rs . next ( ) ; count = 0 ; code_block = WhileStatement ; assertEquals ( count , rowcount - 1 , "Wrong count" ) ; Transaction t3 = tm . begin ( ) ; LOG . info ( "Transaction created " + t3 ) ; Put p = new Put ( modrow ) ; p . add ( fam , col , data2 ) ; tt . put ( t3 , p ) ; tm . commit ( t3 ) ; boolean aborted = false ; code_block = TryStatement ;  assertTrue ( aborted , "Didn't raise exception" ) ; Transaction tscan = tm . begin ( ) ; rs = tt . getScanner ( tscan , new Scan ( ) ) ; r = rs . next ( ) ; count = 0 ; code_block = WhileStatement ; assertEquals ( count , rowcount , "Wrong count" ) ; }
@ Override public void run ( ) throws Exception { log . info ( "Starting ..." ) ; log . info ( "Username: {}" , getOpts ( ) . getUsername ( ) ) ; log . info ( "Source language: {}" , DEFAULT_SOURCE_LANG ) ; log . info ( "Translation language: {}" , getOpts ( ) . getTransLang ( ) ) ; code_block = IfStatement ; log . info ( "Glossary file: {}" , getOpts ( ) . getFile ( ) ) ; log . info ( "Batch size: {}" , getOpts ( ) . getBatchSize ( ) ) ; File glossaryFile = getOpts ( ) . getFile ( ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; String fileExtension = validateFileExtensionWithTransLang ( ) ; String project = getOpts ( ) . getProject ( ) ; String qualifiedName ; code_block = TryStatement ;  AbstractGlossaryPushReader reader = getReader ( fileExtension ) ; log . info ( "Pushing glossary document [{}] to server" , glossaryFile . getName ( ) ) ; Reader inputStreamReader = new InputStreamReader ( new FileInputStream ( glossaryFile ) , "UTF-8" ) ; BufferedReader br = new BufferedReader ( inputStreamReader ) ; Map < LocaleId , List < GlossaryEntry > > glossaries = reader . extractGlossary ( br , qualifiedName ) ; int totalEntries = 0 ; code_block = ForStatement ; int totalDone = 0 ; code_block = ForStatement ; }
@ Override public void run ( ) throws Exception { log . info ( "Server: {}" , getOpts ( ) . getUrl ( ) ) ; log . info ( "Source language: {}" , DEFAULT_SOURCE_LANG ) ; log . info ( "Translation language: {}" , getOpts ( ) . getTransLang ( ) ) ; code_block = IfStatement ; log . info ( "Glossary file: {}" , getOpts ( ) . getFile ( ) ) ; log . info ( "Batch size: {}" , getOpts ( ) . getBatchSize ( ) ) ; File glossaryFile = getOpts ( ) . getFile ( ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; String fileExtension = validateFileExtensionWithTransLang ( ) ; String project = getOpts ( ) . getProject ( ) ; String qualifiedName ; log . info ( "Extension of project: {}" , project ) ; code_block = TryStatement ;  AbstractGlossaryPushReader reader = getReader ( fileExtension ) ; log . info ( "Pushing glossary document [{}] to server" , glossaryFile . getName ( ) ) ; Reader inputStreamReader = new InputStreamReader ( new FileInputStream ( glossaryFile ) , "UTF-8" ) ; BufferedReader br = new BufferedReader ( inputStreamReader ) ; Map < LocaleId , List < GlossaryEntry > > glossaries = reader . extractGlossary ( br , qualifiedName ) ; int totalEntries = 0 ; code_block = ForStatement ; int totalDone = 0 ; code_block = ForStatement ; }
@ Override public void run ( ) throws Exception { log . info ( "Server: {}" , getOpts ( ) . getUrl ( ) ) ; log . info ( "Username: {}" , getOpts ( ) . getUsername ( ) ) ; log . info ( "Translation language: {}" , getOpts ( ) . getTransLang ( ) ) ; code_block = IfStatement ; log . info ( "Glossary file: {}" , getOpts ( ) . getFile ( ) ) ; log . info ( "Batch size: {}" , getOpts ( ) . getBatchSize ( ) ) ; File glossaryFile = getOpts ( ) . getFile ( ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; String fileExtension = validateFileExtensionWithTransLang ( ) ; String project = getOpts ( ) . getProject ( ) ; String qualifiedName ; log . info ( "Extension of project: {}" , project ) ; code_block = TryStatement ;  AbstractGlossaryPushReader reader = getReader ( fileExtension ) ; log . info ( "Pushing glossary document [{}] to server" , glossaryFile . getName ( ) ) ; Reader inputStreamReader = new InputStreamReader ( new FileInputStream ( glossaryFile ) , "UTF-8" ) ; BufferedReader br = new BufferedReader ( inputStreamReader ) ; Map < LocaleId , List < GlossaryEntry > > glossaries = reader . extractGlossary ( br , qualifiedName ) ; int totalEntries = 0 ; code_block = ForStatement ; int totalDone = 0 ; code_block = ForStatement ; }
@ Override public void run ( ) throws Exception { log . info ( "Server: {}" , getOpts ( ) . getUrl ( ) ) ; log . info ( "Username: {}" , getOpts ( ) . getUsername ( ) ) ; log . info ( "Source language: {}" , DEFAULT_SOURCE_LANG ) ; code_block = IfStatement ; log . info ( "Glossary file: {}" , getOpts ( ) . getFile ( ) ) ; log . info ( "Batch size: {}" , getOpts ( ) . getBatchSize ( ) ) ; File glossaryFile = getOpts ( ) . getFile ( ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; String fileExtension = validateFileExtensionWithTransLang ( ) ; String project = getOpts ( ) . getProject ( ) ; String qualifiedName ; log . info ( "Extracted glossary document [{}] to server" , qualifiedName ) ; code_block = TryStatement ;  AbstractGlossaryPushReader reader = getReader ( fileExtension ) ; log . info ( "Pushing glossary document [{}] to server" , glossaryFile . getName ( ) ) ; Reader inputStreamReader = new InputStreamReader ( new FileInputStream ( glossaryFile ) , "UTF-8" ) ; BufferedReader br = new BufferedReader ( inputStreamReader ) ; Map < LocaleId , List < GlossaryEntry > > glossaries = reader . extractGlossary ( br , qualifiedName ) ; int totalEntries = 0 ; code_block = ForStatement ; int totalDone = 0 ; code_block = ForStatement ; }
public void test() { if ( StringUtils . isNotBlank ( getOpts ( ) . getProject ( ) ) ) { log . info ( "Project: {}" , getOpts ( ) . getProject ( ) ) ; } }
@ Override public void run ( ) throws Exception { log . info ( "Server: {}" , getOpts ( ) . getUrl ( ) ) ; log . info ( "Username: {}" , getOpts ( ) . getUsername ( ) ) ; log . info ( "Source language: {}" , DEFAULT_SOURCE_LANG ) ; log . info ( "Translation language: {}" , getOpts ( ) . getTransLang ( ) ) ; code_block = IfStatement ; log . info ( "Batch size: {}" , getOpts ( ) . getBatchSize ( ) ) ; File glossaryFile = getOpts ( ) . getFile ( ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; String fileExtension = validateFileExtensionWithTransLang ( ) ; String project = getOpts ( ) . getProject ( ) ; String qualifiedName ; code_block = TryStatement ;  AbstractGlossaryPushReader reader = getReader ( fileExtension ) ; log . info ( "Pushing glossary document [{}] to server" , glossaryFile . getName ( ) ) ; Reader inputStreamReader = new InputStreamReader ( new FileInputStream ( glossaryFile ) , "UTF-8" ) ; BufferedReader br = new BufferedReader ( inputStreamReader ) ; Map < LocaleId , List < GlossaryEntry > > glossaries = reader . extractGlossary ( br , qualifiedName ) ; log . info ( "Extracted glossary entries: {}" , totalEntries ) ; int totalEntries = 0 ; code_block = ForStatement ; int totalDone = 0 ; code_block = ForStatement ; }
@ Override public void run ( ) throws Exception { log . info ( "Server: {}" , getOpts ( ) . getUrl ( ) ) ; log . info ( "Username: {}" , getOpts ( ) . getUsername ( ) ) ; log . info ( "Source language: {}" , DEFAULT_SOURCE_LANG ) ; log . info ( "Translation language: {}" , getOpts ( ) . getTransLang ( ) ) ; code_block = IfStatement ; log . info ( "Glossary file: {}" , getOpts ( ) . getFile ( ) ) ; File glossaryFile = getOpts ( ) . getFile ( ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; String fileExtension = validateFileExtensionWithTransLang ( ) ; String project = getOpts ( ) . getProject ( ) ; String qualifiedName ; log . info ( "Extension [{}]" , fileExtension ) ; code_block = TryStatement ;  AbstractGlossaryPushReader reader = getReader ( fileExtension ) ; log . info ( "Pushing glossary document [{}] to server" , glossaryFile . getName ( ) ) ; Reader inputStreamReader = new InputStreamReader ( new FileInputStream ( glossaryFile ) , "UTF-8" ) ; BufferedReader br = new BufferedReader ( inputStreamReader ) ; Map < LocaleId , List < GlossaryEntry > > glossaries = reader . extractGlossary ( br , qualifiedName ) ; int totalDone = 0 ; code_block = ForStatement ; int totalDone = 0 ; code_block = ForStatement ; }
public void test() { if ( rpe . getResponse ( ) . getStatus ( ) == 404 ) { log . error ( "Project {} not found" , projectId ) ; return ; } else { throw rpe ; } }
@ Override public void run ( ) throws Exception { log . info ( "Server: {}" , getOpts ( ) . getUrl ( ) ) ; log . info ( "Username: {}" , getOpts ( ) . getUsername ( ) ) ; log . info ( "Source language: {}" , DEFAULT_SOURCE_LANG ) ; log . info ( "Translation language: {}" , getOpts ( ) . getTransLang ( ) ) ; code_block = IfStatement ; log . info ( "Glossary file: {}" , getOpts ( ) . getFile ( ) ) ; log . info ( "Batch size: {}" , getOpts ( ) . getBatchSize ( ) ) ; File glossaryFile = getOpts ( ) . getFile ( ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; String fileExtension = validateFileExtensionWithTransLang ( ) ; String project = getOpts ( ) . getProject ( ) ; String qualifiedName ; code_block = TryStatement ;  AbstractGlossaryPushReader reader = getReader ( fileExtension ) ; Reader inputStreamReader = new InputStreamReader ( new FileInputStream ( glossaryFile ) , "UTF-8" ) ; BufferedReader br = new BufferedReader ( inputStreamReader ) ; Map < LocaleId , List < GlossaryEntry > > glossaries = reader . extractGlossary ( br , qualifiedName ) ; log . info ( "Extracted glossary entries: {}" , totalEntries ) ; int totalEntries = 0 ; code_block = ForStatement ; int totalDone = 0 ; code_block = ForStatement ; }
public void testReceiveTwoThenRollback ( ) throws Exception { Message [ ] outbound = new Message [ ] code_block = "" ; ; beginTx ( ) ; code_block = WhileStatement ; commitTx ( ) ; beginTx ( ) ; producer . send ( outbound [ 0 ] ) ; producer . send ( outbound [ 1 ] ) ; commitTx ( ) ; LOG . info ( "Sent 0: " + outbound [ 0 ] ) ; LOG . info ( "Sent 1: " + outbound [ 1 ] ) ; ArrayList < Message > messages = new ArrayList < > ( ) ; beginTx ( ) ; Message message = consumer . receive ( 1000 ) ; assertEquals ( outbound [ 0 ] , message ) ; message = consumer . receive ( 1000 ) ; assertNotNull ( message ) ; assertEquals ( outbound [ 1 ] , message ) ; rollbackTx ( ) ; beginTx ( ) ; message = consumer . receive ( 5000 ) ; assertNotNull ( "Should have re-received the first message again!" , message ) ; messages . add ( message ) ; assertEquals ( outbound [ 0 ] , message ) ; message = consumer . receive ( 5000 ) ; assertNotNull ( "Should have re-received the second message again!" , message ) ; messages . add ( message ) ; assertEquals ( outbound [ 1 ] , message ) ; assertNull ( consumer . receiveNoWait ( ) ) ; commitTx ( ) ; Message inbound [ ] = new Message [ messages . size ( ) ] ; messages . toArray ( inbound ) ; assertTextMessagesEqual ( "Rollback did not work" , outbound , inbound ) ; }
public void testReceiveTwoThenRollback ( ) throws Exception { Message [ ] outbound = new Message [ ] code_block = "" ; ; beginTx ( ) ; code_block = WhileStatement ; commitTx ( ) ; beginTx ( ) ; producer . send ( outbound [ 0 ] ) ; producer . send ( outbound [ 1 ] ) ; commitTx ( ) ; LOG . info ( "Sent 0: " + outbound [ 0 ] ) ; LOG . info ( "Sent 1: " + outbound [ 1 ] ) ; ArrayList < Message > messages = new ArrayList < > ( ) ; beginTx ( ) ; Message message = consumer . receive ( 1000 ) ; assertEquals ( outbound [ 0 ] , message ) ; message = consumer . receive ( 1000 ) ; assertNotNull ( message ) ; assertEquals ( outbound [ 1 ] , message ) ; rollbackTx ( ) ; beginTx ( ) ; message = consumer . receive ( 5000 ) ; assertNotNull ( "Should have re-received the first message again!" , message ) ; messages . add ( message ) ; assertEquals ( outbound [ 0 ] , message ) ; message = consumer . receive ( 5000 ) ; assertNotNull ( "Should have re-received the second message again!" , message ) ; messages . add ( message ) ; assertEquals ( outbound [ 1 ] , message ) ; assertNull ( consumer . receiveNoWait ( ) ) ; commitTx ( ) ; Message inbound [ ] = new Message [ messages . size ( ) ] ; messages . toArray ( inbound ) ; assertTextMessagesEqual ( "Rollback did not work" , outbound , inbound ) ; }
public void test() { try { MAX_MEM_USAGE = Integer . parseInt ( limit ) ; } catch ( NumberFormatException nfe ) { logger . warn ( "Invalid value to " + limit ) ; } }
public void test() { try { List < Integer > ruleIdList = validAlertRuleList . stream ( ) . map ( AlertRule :: getRuleId ) . collect ( Collectors . toList ( ) ) ; alertRuleService . updateAlertRuleLastCheckTime ( ruleIdList ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( result . getHarEntry ( ) != null && result . getHarEntry ( ) . getResponse ( ) != null ) { HarResponse response = result . getHarEntry ( ) . getResponse ( ) ; LOG . debug ( "Missing HAR response for TestStep [" + result . getTestStepName ( ) + "]" ) ; } else { LOG . debug ( "Missing HAR response for TestStep [" + result . getTestStepName ( ) + "]" ) ; } }
public void test() { if ( result . getHarEntry ( ) != null && result . getHarEntry ( ) . getResponse ( ) != null ) { HarResponse response = result . getHarEntry ( ) . getResponse ( ) ; LOG . info ( "TestStep [" + result . getTestStepName ( ) + "] response: " + response . getStatus ( ) + " - " + response . getContent ( ) . getText ( ) ) ; } else { LOG . info ( "TestStep [" + result . getTestStepName ( ) + "] no response found" ) ; } }
@ Override public void fireDisconnected ( ) { LOGGER . warn ( "Failed to fire disconnected channel!" ) ; channelHandlerContext . pipeline ( ) . fireUserEventTriggered ( new DisconnectedEvent ( ) ) ; }
public void test() { if ( StringUtils . isEmpty ( siteName ) && logger . isDebugEnabled ( ) ) { logger . debug ( "Ignoring empty site name" ) ; } }
public void test() { try { verifyProperties ( properties ) ; } catch ( IOException e ) { log . warn ( "Exception while validating properties" , e ) ; StartupListener . properties = null ; } }
public void test() { if ( properties != null ) { code_block = TryStatement ;  StartupListener . properties = properties ; } else { LOG . error ( "Properties was null!" ) ; } }
public void test() { if ( LOGGER . isTraceEnabled ( ) ) { LOGGER . trace ( "Weak listener list status:{}" , System . lineSeparator ( ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( packageParser != null ) { PackageInfo pi = new PackageInfo ( ) ; pi . prefix = packageParser . getPackageName ( ) ; pi . namespace = namespace ; pi . version = extractPackageVersion ( namespace ) ; prefixMap . put ( pi . prefix , pi ) ; namespaceMap . put ( namespace , pi ) ; code_block = IfStatement ; } else { log . warn ( "Possible misconfigured package version (" + packageName + ")" ) ; } }
public void test() { try { Tracker t = new Tracker ( portValue ) ; File parent = new File ( directory ) ; code_block = ForStatement ; logger . info ( "Starting tracker with {} announced torrents..." , t . getTrackedTorrents ( ) . size ( ) ) ; t . start ( true ) ; } catch ( Exception e ) { logger . error ( "Failed to start tracker." , e ) ; System . exit ( 2 ) ; } }
public void test() { try { PoxPayloadOut result = ConceptAuthorityClientUtils . createConceptInstance ( commonPartXML , commonPartName ) ; return result ; } catch ( DocumentException de ) { logger . error ( "Error creating ConceptAuthorityClient" , de ) ; logger . debug ( "commonPartXML: " + commonPartXML ) ; } }
public void test() { try { PoxPayloadOut result = ConceptAuthorityClientUtils . createConceptInstance ( commonPartXML , commonPartName ) ; logger . debug ( "PoxPayloadOut result: " + result ) ; return result ; } catch ( DocumentException de ) { logger . error ( "Problem creating item from XML: " + de . getLocalizedMessage ( ) ) ; } }
public void test() { if ( LOGGER . isLoggable ( Level . INFO ) ) { LOGGER . log ( Level . INFO , e . getMessage ( ) , e ) ; } }
public void test() { if ( LOGGER . isLoggable ( Level . INFO ) ) { LOGGER . log ( Level . INFO , e . getMessage ( ) , e ) ; } }
public void test() { try ( SqlSession sqlSession = MyBatisUtil . getSqlSession ( ) ) { SysUserMapper userMapper = sqlSession . getMapper ( SysUserMapper . class ) ; userMapper . changePassword ( user ) ; sqlSession . commit ( ) ; } catch ( Exception e ) { LOG . error ( e . getMessage ( ) , e ) ; throw new Exception ( e ) ; } }
public void test() { try { task . go ( ) ; transaction . setStatus ( Transaction . SUCCESS ) ; } catch ( Throwable th ) { transaction . setStatus ( th ) ; logger . error ( "post operation failed" , th ) ; } finally { transaction . complete ( ) ; } }
public void test() { if ( e . getErrorType ( ) == READ_ERROR ) { log . error ( "Checksum I/O error during validation on {}" , checksumFile ) ; triggerConsumerError ( CHECKSUM_IO_ERROR , "Checksum I/O error during validation on " + checksumFile ) ; } else-if ( e . getErrorType ( ) == INVALID_FORMAT || e . getErrorType ( ) == DIGEST_ERROR ) { log . error ( "Digester failure during checksum validation on {}" , checksumFile ) ; triggerConsumerError ( CHECKSUM_DIGESTER_FAILURE , "Digester failure during checksum validation on " + checksumFile ) ; } else-if ( e . getErrorType ( ) == FILE_NOT_FOUND ) { log . error ( "File not found during checksum validation: " , e ) ; triggerConsumerError ( CHECKSUM_NOT_FOUND , "File not found during checksum validation: " + e . getMessage ( ) ) ; } }
public void test() { if ( e . getErrorType ( ) == READ_ERROR ) { log . error ( "Checksum read error during validation on {}" , checksumFile ) ; triggerConsumerError ( CHECKSUM_IO_ERROR , "Checksum I/O error during validation on " + checksumFile ) ; } else-if ( e . getErrorType ( ) == INVALID_FORMAT || e . getErrorType ( ) == DIGEST_ERROR ) { log . error ( "Digester failure during checksum validation on {}" , checksumFile ) ; triggerConsumerError ( CHECKSUM_DIGESTER_FAILURE , "Digester failure during checksum validation on " + checksumFile ) ; } else-if ( e . getErrorType ( ) == FILE_NOT_FOUND ) { log . error ( "File not found during checksum validation: " , e ) ; triggerConsumerError ( CHECKSUM_NOT_FOUND , "File not found during checksum validation: " + e . getMessage ( ) ) ; } }
public void test() { if ( e . getErrorType ( ) == READ_ERROR ) { log . error ( "Checksum read error during validation on {}" , checksumFile ) ; triggerConsumerError ( CHECKSUM_IO_ERROR , "Checksum I/O error during validation on " + checksumFile ) ; } else-if ( e . getErrorType ( ) == INVALID_FORMAT || e . getErrorType ( ) == DIGEST_ERROR ) { log . error ( "Digester failure during checksum validation on {}" , checksumFile ) ; triggerConsumerError ( CHECKSUM_DIGESTER_FAILURE , "Digester failure during checksum validation on " + checksumFile ) ; } else-if ( e . getErrorType ( ) == FILE_NOT_FOUND ) { log . debug ( "File not found during checksum validation: {}" , e . getMessage ( ) ) ; triggerConsumerError ( CHECKSUM_NOT_FOUND , "File not found during checksum validation: " + e . getMessage ( ) ) ; } }
public void test() { try { getSubscriptionManager ( ) . unsubscribe ( mailboxSession , mailboxName ) ; unsolicitedResponses ( session , responder , false ) ; okComplete ( request , responder ) ; } catch ( SubscriptionException e ) { logger . error ( e . getMessage ( ) , e ) ; unsolicitedResponses ( session , responder , false ) ; no ( request , responder , HumanReadableText . GENERIC_SUBSCRIPTION_FAILURE ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( ResourceException e ) { log . warn ( e . getMessage ( ) ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( ResourceException e ) { log . warn ( e . getMessage ( ) ) ; } }
private void insertRemoteToDb ( String siteId , String remoteName , String remoteUrl , String authenticationType , String remoteUsername , String remotePassword , String remoteToken , String remotePrivateKey ) throws CryptoException { Map < String , String > params = new HashMap < String , String > ( ) ; params . put ( "siteId" , siteId ) ; params . put ( "remoteName" , remoteName ) ; params . put ( "remoteUrl" , remoteUrl ) ; params . put ( "authenticationType" , authenticationType ) ; params . put ( "remoteUsername" , remoteUsername ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; logger . debug ( "Insert site remote record into database" ) ; remoteRepositoryDAO . insertRemoteRepository ( params ) ; params = new HashMap < String , String > ( ) ; params . put ( "siteId" , siteId ) ; params . put ( "remoteName" , remoteName ) ; RemoteRepository remoteRepository = remoteRepositoryDAO . getRemoteRepository ( params ) ; code_block = IfStatement ; logger . debug ( "Extracted remote record into database" ) ; }
public boolean flowUpdatePreStatusNone ( String networkId , BasicFlow flow ) { log . debug ( "" ) ; List < String > fedFlowIds = conversionTable . getFlow ( networkId , flow . getFlowId ( ) ) ; code_block = IfStatement ; String [ ] fedFlowId = fedFlowIds . get ( 0 ) . split ( "::" ) ; List < String > fedOrgIds = conversionTable . getFlow ( fedFlowId [ 0 ] , fedFlowId [ 1 ] ) ; code_block = ForStatement ; log . debug ( "next federate stauts:: none" ) ; return true ; }
public void test() { if ( fedFlowIds == null || fedFlowIds . size ( ) == 0 ) { log . debug ( "No FederatedFlowFiles defined." ) ; return false ; } }
public void test() { if ( orgFlow == null ) { logger . warn ( "not found Original Flow: {}" , flowId ) ; continue ; } }
public void test() { if ( ! FlowObject . FlowStatus . NONE . toString ( ) . equalsIgnoreCase ( orgFlow . getStatus ( ) ) ) { log . debug ( "not allowed flow" ) ; return false ; } }
public boolean flowUpdatePreStatusNone ( String networkId , BasicFlow flow ) { log . debug ( "" ) ; List < String > fedFlowIds = conversionTable . getFlow ( networkId , flow . getFlowId ( ) ) ; code_block = IfStatement ; String [ ] fedFlowId = fedFlowIds . get ( 0 ) . split ( "::" ) ; List < String > fedOrgIds = conversionTable . getFlow ( fedFlowId [ 0 ] , fedFlowId [ 1 ] ) ; code_block = ForStatement ; log . debug ( "" ) ; return true ; }
public void waitForScanFinishing ( ) { logger . debug ( "Waiting to Homematic device discovery scan" ) ; code_block = TryStatement ;  String gatewayId = bridgeHandler != null && bridgeHandler . getGateway ( ) != null ? bridgeHandler . getGateway ( ) . getId ( ) : "UNKNOWN" ; logger . debug ( "Finished Homematic device discovery scan on gateway '{}'" , gatewayId ) ; }
public void test() { try { waitForInstallModeFinished ( DISCOVER_TIMEOUT_SECONDS * 1000 ) ; waitForLoadDevicesFinished ( ) ; } catch ( ExecutionException | InterruptedException ex ) { } catch ( Exception ex ) { LOGGER . error ( "Error waiting for discovery devices" , ex ) ; } }
public void waitForScanFinishing ( ) { logger . debug ( "Waiting for finishing Homematic device discovery scan" ) ; code_block = TryStatement ;  String gatewayId = bridgeHandler != null && bridgeHandler . getGateway ( ) != null ? bridgeHandler . getGateway ( ) . getId ( ) : "UNKNOWN" ; logger . debug ( "Finished Homematic device discovery scan" ) ; }
public static DataSource initDataSource ( Config config ) throws Exception { log . info ( "Initializing data source" ) ; DruidDataSource dataSource = new DruidDataSource ( ) ; dataSource . setDriverClassName ( "com.mysql.cj.jdbc.Driver" ) ; dataSource . setUrl ( "jdbc:mysql://" + config . getDbUrl ( ) + ":" + config . getDbPort ( ) + "?useSSL=true&verifyServerCertificate=false&serverTimezone=GMT%2B8&characterEncoding=utf8" ) ; dataSource . setUsername ( config . getDbUsername ( ) ) ; dataSource . setPassword ( config . getDbPassword ( ) ) ; dataSource . setInitialSize ( 1 ) ; dataSource . setMaxActive ( 2 ) ; dataSource . setMaxWait ( 60000 ) ; dataSource . setTimeBetweenEvictionRunsMillis ( 60000 ) ; dataSource . setConnectionErrorRetryAttempts ( 2 ) ; dataSource . setBreakAfterAcquireFailure ( true ) ; dataSource . setMinEvictableIdleTimeMillis ( 300000 ) ; dataSource . setValidationQuery ( "SELECT 1 FROM DUAL" ) ; dataSource . setTestWhileIdle ( true ) ; return dataSource ; }
@ GetMapping ( path = "/testvoidInRPC" ) public void testvoidInRPC ( ) { LOG . info ( "testvoidInRPC called" ) ; testvoidInRPCSuccess = true ; }
public void test() { try { final String [ ] wNames = StringUtils . split ( widgetNames , "," ) ; dashboardsService . setWidgetsOrder ( guestId , dashboardId , wNames ) ; return getDashboards ( ) ; } catch ( Exception e ) { StringBuilder sb = new StringBuilder ( "module=API component=dashboardStore action=setWidgetOrder" ) . append ( " guestId=" ) . append ( guestId ) . append ( " stackTrace=<![CDATA[" ) . append ( Utils . stackTrace ( e ) ) . append ( "]]>" ) ; logger . warn ( sb . toString ( ) ) ; return Response . serverError ( ) . entity ( "Failed to set widget order: " + e . getMessage ( ) ) . build ( ) ; } }
private RestResponse < User > create ( ) throws ClientException { logger . debug ( "Creating user" ) ; UserCommandOptions . CreateCommandOptions c = usersCommandOptions . createCommandOptions ; UserCreateParams createParams = new UserCreateParams ( ) . setId ( c . user ) . setName ( c . name ) . setEmail ( c . email ) . setOrganization ( c . organization ) . setPassword ( c . password ) ; return openCGAClient . getUserClient ( ) . create ( createParams ) ; }
@ Post ( subscribeToken = "csrf" ) public Boundary save ( ) { String expires = getParam ( "expires" ) ; code_block = IfStatement ; Date date ; code_block = TryStatement ;  TokensEntity entity = TokensDao . get ( ) . selectOnUserId ( getLoginUserId ( ) ) ; code_block = IfStatement ; entity . setExpires ( new Timestamp ( date . getTime ( ) ) ) ; TokensDao . get ( ) . save ( entity ) ; LOG . info ( "save." ) ; addMsgSuccess ( "message.success.save" ) ; return index ( ) ; }
public void test() { try { ManagementFactory . getPlatformMBeanServer ( ) ; } catch ( Throwable t ) { log . error ( t . getMessage ( ) , t ) ; } }
public void test() { try ( DatagramSocket socket = IdentProtocol . sendRequest ( broadcastAddress ) ) { DatagramPacket incomingPacket ; code_block = WhileStatement ; } catch ( IOException e ) { logger . debug ( "Failed to send request: {}" , e . getMessage ( ) ) ; } }
public int executeUpdate ( String s ) throws SQLException { log . debug ( "executeUpdate [{}]" , s ) ; return getSql ( ) . executeUpdate ( s ) ; }
public void test() { if ( Logger . isDebugEnabled ( this ) ) { Logger . debug ( this , "%s: %s" , getClass ( ) . getName ( ) , Logger . isDebugEnabled ( this ) ) ; } }
public void test() { if ( ! this . configuration . isEventStoreEnabled ( ) || StringUtils . isEmpty ( this . configuration . getEventStore ( ) ) ) { this . logger . info ( "EventStore is disabled" ) ; return ; } }
public void test() { try { this . eventStore = this . componentManager . getInstance ( EventStore . class , this . configuration . getEventStore ( ) ) ; } catch ( ComponentLookupException e ) { this . logger . error ( "Failed to lookup event store" , e ) ; return ; } }
public void test() { if ( getRequest ( ) . isVerbose ( ) ) { this . logger . info ( LOG_DOWNLOADING , "Downloading extension [{}]" , extension . getId ( ) ) ; } }
public void test() { if ( modelDefinition != null && modelDefinition . getExtension ( CompConstants . shortLabel ) != null ) { initSubModels ( ( CompModelPlugin ) modelDefinition . getExtension ( CompConstants . shortLabel ) ) ; } else { LOGGER . error ( "No extension found to " + CompConstants . shortLabel ) ; } }
@ ApiOperation ( value = "Patch entity" ) @ PatchMapping ( CommonConstants . PATH_ID ) @ ResponseStatus ( HttpStatus . OK ) public SecurityProfileDto patch ( final @ PathVariable ( "id" ) String id , @ RequestBody final Map < String , Object > partialDto ) { LOGGER . debug ( "Patch securityProfile with {}" , id ) ; ParameterChecker . checkParameter ( "The Identifier, the partialEntity are mandatory parameters: " , id , partialDto ) ; Assert . isTrue ( StringUtils . equals ( id , ( String ) partialDto . get ( "id" ) ) , "Unable to patch securityProfile : the DTO id must match the path id." ) ; return service . patch ( buildUiHttpContext ( ) , partialDto , id ) ; }
@ Test public void testOneGroupMessage ( ) throws Exception { String message = "8=FIX 4.19=2034=135=049=INVMGR56=BRKR" + "1=BE.CHM.00111=CHM0001-0158=this is a camel - bindy test" + "22=448=BE000124567854=1" + "10=220" + "777=22-06-2013 12:21:11" ; List < String > data = Arrays . asList ( message . split ( "\\u0001" ) ) ; CamelContext camelContext = new DefaultCamelContext ( ) ; LOG . debug ( "About to bind " + data ) ; factory . bind ( camelContext , data , model , counter ) ; assertNotNull ( model ) ; }
public void test() { try { endpointUrl = new URL ( tmp . getProtocol ( ) , hostname , tmp . getPort ( ) , tmp . getFile ( ) ) ; } catch ( MalformedURLException e ) { log . error ( "Could not create server endpoint URL" , e ) ; return tmp ; } }
public void test() { try { this . resendNotifications ( ( short ) notificationsResent , createdBefore ) ; } catch ( final CircuitBreakerOpenException exc ) { LOG . debug ( "CircuitBreaker open exception" , exc ) ; break ; } }
List < T > errorAndReturnEmpty ( ) { log . warn ( "Could not find empty list of arguments" ) ; return emptyList ( ) ; }
public void test() { try { return remoting . doWork ( ) ; } catch ( InterruptedException | CancellationException t ) { } catch ( MalformedURLException ex ) { showMessageToUser ( ex . getMessage ( ) ) ; logger . fatal ( "Connect: MalformedURLException" , ex ) ; } catch ( UndeclaredThrowableException ex ) { String addMessage = "" ; Throwable cause = ex . getCause ( ) ; code_block = IfStatement ; code_block = IfStatement ; showMessageToUser ( addMessage + ( ex . getMessage ( ) != null ? ex . getMessage ( ) : "" ) ) ; } catch ( IOException ex ) { logger . fatal ( "Connect: unknown IO error" , ex ) ; String addMessage = "" ; code_block = IfStatement ; showMessageToUser ( addMessage + ( ex . getMessage ( ) != null ? ex . getMessage ( ) : "" ) ) ; } catch ( MageVersionException ex ) { logger . warn ( "Connect: wrong versions" ) ; disconnect ( false ) ; code_block = IfStatement ; } catch ( CannotConnectException ex ) { code_block = IfStatement ; } catch ( Throwable t ) { logger . fatal ( "Connect: FAIL" , t ) ; disconnect ( false ) ; code_block = IfStatement ; } finally { lastRemotingTask = null ; } }
public void test() { if ( exep . getCause ( ) instanceof IOException ) { code_block = IfStatement ; } else { LOGGER . error ( "Failed to send message" , exep ) ; } }
public void test() { if ( addMessage . isEmpty ( ) ) { LOGGER . warn ( "addMessage is empty" ) ; } }
public void test() { try { return remoting . doWork ( ) ; } catch ( InterruptedException | CancellationException t ) { } catch ( MalformedURLException ex ) { logger . fatal ( "Connect: wrong server address" , ex ) ; showMessageToUser ( ex . getMessage ( ) ) ; } catch ( UndeclaredThrowableException ex ) { String addMessage = "" ; Throwable cause = ex . getCause ( ) ; code_block = IfStatement ; code_block = IfStatement ; showMessageToUser ( addMessage + ( ex . getMessage ( ) != null ? ex . getMessage ( ) : "" ) ) ; } catch ( IOException ex ) { String addMessage = "" ; code_block = IfStatement ; showMessageToUser ( addMessage + ( ex . getMessage ( ) != null ? ex . getMessage ( ) : "" ) ) ; } catch ( MageVersionException ex ) { logger . warn ( "Connect: wrong versions" ) ; disconnect ( false ) ; code_block = IfStatement ; } catch ( CannotConnectException ex ) { code_block = IfStatement ; } catch ( Throwable t ) { logger . fatal ( "Connect: FAIL" , t ) ; disconnect ( false ) ; code_block = IfStatement ; } finally { lastRemotingTask = null ; } }
public void test() { try { return remoting . doWork ( ) ; } catch ( InterruptedException | CancellationException t ) { } catch ( MalformedURLException ex ) { logger . fatal ( "Connect: wrong server address" , ex ) ; showMessageToUser ( ex . getMessage ( ) ) ; } catch ( UndeclaredThrowableException ex ) { String addMessage = "" ; Throwable cause = ex . getCause ( ) ; code_block = IfStatement ; code_block = IfStatement ; showMessageToUser ( addMessage + ( ex . getMessage ( ) != null ? ex . getMessage ( ) : "" ) ) ; } catch ( IOException ex ) { logger . fatal ( "Connect: unknown IO error" , ex ) ; String addMessage = "" ; code_block = IfStatement ; showMessageToUser ( addMessage + ( ex . getMessage ( ) != null ? ex . getMessage ( ) : "" ) ) ; } catch ( MageVersionException ex ) { logger . fatal ( "Connect: " + ex ) ; disconnect ( false ) ; code_block = IfStatement ; } catch ( CannotConnectException ex ) { code_block = IfStatement ; } catch ( Throwable t ) { logger . fatal ( "Connect: FAIL" , t ) ; disconnect ( false ) ; code_block = IfStatement ; } finally { lastRemotingTask = null ; } }
public void test() { try { return remoting . doWork ( ) ; } catch ( InterruptedException | CancellationException t ) { } catch ( MalformedURLException ex ) { logger . fatal ( "Connect: wrong server address" , ex ) ; showMessageToUser ( ex . getMessage ( ) ) ; } catch ( UndeclaredThrowableException ex ) { String addMessage = "" ; Throwable cause = ex . getCause ( ) ; code_block = IfStatement ; code_block = IfStatement ; showMessageToUser ( addMessage + ( ex . getMessage ( ) != null ? ex . getMessage ( ) : "" ) ) ; } catch ( IOException ex ) { logger . fatal ( "Connect: unknown IO error" , ex ) ; String addMessage = "" ; code_block = IfStatement ; showMessageToUser ( addMessage + ( ex . getMessage ( ) != null ? ex . getMessage ( ) : "" ) ) ; } catch ( MageVersionException ex ) { logger . warn ( "Connect: wrong versions" ) ; disconnect ( false ) ; code_block = IfStatement ; } catch ( CannotConnectException ex ) { code_block = IfStatement ; } catch ( Throwable t ) { logger . fatal ( "Connect: " + t . getMessage ( ) , t ) ; disconnect ( false ) ; code_block = IfStatement ; } finally { lastRemotingTask = null ; } }
public void test() { try { Matcher m = PARAM_PATTERN . matcher ( paramNodeValue ) ; code_block = IfStatement ; } catch ( Throwable e ) { logger . warn ( "Unable to parse parameter " + paramNodeValue , e ) ; } }
public void test() { { Path pathInZipFile = zipfs . getPath ( file . toString ( ) . substring ( dest . toString ( ) . length ( ) ) ) ; LOG . debug ( "extract {} to {}" , file , dest ) ; Files . copy ( file , pathInZipFile , StandardCopyOption . REPLACE_EXISTING ) ; return FileVisitResult . CONTINUE ; } }
public void test() { try { JSONArray paths = ( ( JSONArray ) params . get ( "items" ) ) ; String paramDest = ( String ) params . get ( "destination" ) ; final Path dest = Paths . get ( REPOSITORY_BASE_PATH , paramDest ) ; Path zip = dest . resolve ( ( String ) params . get ( "compressedFilename" ) ) ; code_block = IfStatement ; Map < String , String > env = new HashMap < > ( ) ; env . put ( "create" , "true" ) ; boolean zipped = false ; code_block = TryStatement ;  return success ( params ) ; } catch ( IOException e ) { LOG . error ( "Error generating zip" , e ) ; return error ( e . getClass ( ) . getSimpleName ( ) + ":" + e . getMessage ( ) ) ; } }
@ Override public TqlElement visitFieldIsNull ( TqlParser . FieldIsNullContext ctx ) { LOG . debug ( "Visit is field null expression: " + ctx . getText ( ) ) ; TqlElement fieldName = ctx . getChild ( 0 ) . accept ( this ) ; FieldIsNullExpression isNullExpression = new FieldIsNullExpression ( fieldName ) ; LOG . debug ( "End visit is field null expression: " + ctx . getText ( ) ) ; return isNullExpression ; }
@ Override public TqlElement visitFieldIsNull ( TqlParser . FieldIsNullContext ctx ) { LOG . debug ( "Visit is field null expression: " + ctx . getText ( ) ) ; TqlElement fieldName = ctx . getChild ( 0 ) . accept ( this ) ; FieldIsNullExpression isNullExpression = new FieldIsNullExpression ( fieldName ) ; LOG . debug ( "End visit is field null expression: " + ctx . getText ( ) ) ; return isNullExpression ; }
@ Initialize public void init ( ) { final ClassLoader parent = getClass ( ) . getClassLoader ( ) ; _groovyClassLoader = new GroovyClassLoader ( parent ) ; final Class < ? > groovyClass = _groovyClassLoader . parseClass ( code ) ; _log . info ( "groovy class: {}" , code ) ; _groovyObject = ( GroovyObject ) ReflectionUtils . newInstance ( groovyClass ) ; }
@ Override public void onRegistrationSuccess ( ServerIdentity server , RegisterRequest request , String registrationID ) { logger . info ( "Server [" + server . getName ( ) + "] server [" + request . getServerId ( ) + "]" ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { return regionPolicy . selectFromNetworkLocation ( bookieNodeToReplace . getNetworkLocation ( ) , excludeBookies , TruePredicate . INSTANCE , EnsembleForReplacementWithNoConstraints . INSTANCE , true ) ; } catch ( BKException . BKNotEnoughBookiesException e ) { LOG . warn ( "Not enough bookies to match {}, excluding it" , bookieNodeToReplace . getNetworkLocation ( ) ) ; } }
public void update ( UpnpControlBindingConfiguration newConfig ) { String newPath = newConfig . path ; code_block = IfStatement ; logger . debug ( "UpnpControlBindingConfiguration: {}" , path ) ; UpnpControlUtil . bindingConfigurationChanged ( path ) ; }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( LOGGER . isTraceEnabled ( ) ) { LOGGER . trace ( "Weak listener list status:{}" , System . lineSeparator ( ) ) ; } }
public void warn ( Throwable t , String message , Object ... formatArgs ) { log . warn ( t , formatArgs ) ; }
protected void doTestStore ( IPageStore pageStore ) { this . pageStore = new AsynchronousPageStore ( pageStore , 100 ) ; generateSessionsAndPages ( ) ; long start = System . currentTimeMillis ( ) ; code_block = ForStatement ; code_block = ForStatement ; code_block = ForStatement ; code_block = WhileStatement ; code_block = IfStatement ; long duration = System . currentTimeMillis ( ) - start ; log . info ( "Took: " + duration + " ms" ) ; log . info ( "Save: " + saveCount . intValue ( ) + " files, " + bytesWritten . get ( ) + " bytes" ) ; log . info ( "Read: " + ( read1Count . get ( ) + read2Count . get ( ) ) + " files, " + bytesRead . get ( ) + " bytes" ) ; log . info ( "Average save time (ns): " + ( double ) saveTime . get ( ) / ( double ) saveCount . get ( ) ) ; assertEquals ( 0 , failures . get ( ) ) ; code_block = ForStatement ; }
public void test() { try { Thread . sleep ( 50 ) ; } catch ( InterruptedException e ) { logger . error ( "" , e ) ; } }
protected void doTestStore ( IPageStore pageStore ) { this . pageStore = new AsynchronousPageStore ( pageStore , 100 ) ; generateSessionsAndPages ( ) ; log . info ( "Starting..." ) ; long start = System . currentTimeMillis ( ) ; code_block = ForStatement ; code_block = ForStatement ; code_block = ForStatement ; code_block = WhileStatement ; code_block = IfStatement ; long duration = System . currentTimeMillis ( ) - start ; log . info ( "Duration: " + duration ) ; log . info ( "Save: " + saveCount . intValue ( ) + " files, " + bytesWritten . get ( ) + " bytes" ) ; log . info ( "Read: " + ( read1Count . get ( ) + read2Count . get ( ) ) + " files, " + bytesRead . get ( ) + " bytes" ) ; log . info ( "Average save time (ns): " + ( double ) saveTime . get ( ) / ( double ) saveCount . get ( ) ) ; assertEquals ( 0 , failures . get ( ) ) ; code_block = ForStatement ; }
protected void doTestStore ( IPageStore pageStore ) { this . pageStore = new AsynchronousPageStore ( pageStore , 100 ) ; generateSessionsAndPages ( ) ; log . info ( "Starting..." ) ; long start = System . currentTimeMillis ( ) ; code_block = ForStatement ; code_block = ForStatement ; code_block = ForStatement ; code_block = WhileStatement ; code_block = IfStatement ; long duration = System . currentTimeMillis ( ) - start ; log . info ( "Took: " + duration + " ms" ) ; log . info ( "Read: " + ( read1Count . get ( ) + read2Count . get ( ) ) + " files, " + bytesRead . get ( ) + " bytes" ) ; log . info ( "Read: " + ( read2Count . get ( ) + " files, " + bytesRead . get ( ) + " bytes" ) ; log . info ( "Average save time (ns): " + ( double ) saveTime . get ( ) / ( double ) saveCount . get ( ) ) ; assertEquals ( 0 , failures . get ( ) ) ; code_block = ForStatement ; }
protected void doTestStore ( IPageStore pageStore ) { this . pageStore = new AsynchronousPageStore ( pageStore , 100 ) ; generateSessionsAndPages ( ) ; log . info ( "Starting..." ) ; long start = System . currentTimeMillis ( ) ; code_block = ForStatement ; code_block = ForStatement ; code_block = ForStatement ; code_block = WhileStatement ; code_block = IfStatement ; long duration = System . currentTimeMillis ( ) - start ; log . info ( "Took: " + duration + " ms" ) ; log . info ( "Save: " + saveCount . intValue ( ) + " files, " + bytesWritten . get ( ) + " bytes" ) ; log . info ( "Total failures: " + failures . get ( ) ) ; log . info ( "Average save time (ns): " + ( double ) saveTime . get ( ) / ( double ) saveCount . get ( ) ) ; assertEquals ( 0 , failures . get ( ) ) ; code_block = ForStatement ; }
protected void doTestStore ( IPageStore pageStore ) { this . pageStore = new AsynchronousPageStore ( pageStore , 100 ) ; generateSessionsAndPages ( ) ; log . info ( "Starting..." ) ; long start = System . currentTimeMillis ( ) ; code_block = ForStatement ; code_block = ForStatement ; code_block = ForStatement ; code_block = WhileStatement ; code_block = IfStatement ; long duration = System . currentTimeMillis ( ) - start ; log . info ( "Took: " + duration + " ms" ) ; log . info ( "Save: " + saveCount . intValue ( ) + " files, " + bytesWritten . get ( ) + " bytes" ) ; log . info ( "Read: " + ( read1Count . get ( ) + read2Count . get ( ) ) + " files, " + bytesRead . get ( ) + " bytes" ) ; log . info ( "Read: " + ( read2Count . get ( ) + " files, " + bytesRead . get ( ) + " bytes" ) ; assertEquals ( 0 , failures . get ( ) ) ; code_block = ForStatement ; }
public void stopService ( ) { running . set ( false ) ; log . info ( "stopService..." ) ; code_block = TryStatement ;  clientHandler . interrupt ( ) ; log . info ( "stopService, complete." ) ; }
public void test() { try { serverSocket . close ( ) ; } catch ( IOException e ) { logger . error ( "" , e ) ; } }
@ Test public void testCleanupOnDelete ( ) throws Exception { System . setProperty ( EVENTS_DISABLED , "true" ) ; final EntityManager em = app . getEntityManager ( ) ; final int numEntities = 5 ; final int numUpdates = 5 ; final List < Entity > things = new ArrayList < Entity > ( numEntities ) ; code_block = ForStatement ; app . waitForQueueDrainAndRefreshIndex ( ) ; CandidateResults crs = queryCollectionCp ( "things" , "thing" , "select *" ) ; Assert . assertEquals ( "Expect no stale candidates yet" , numEntities , crs . size ( ) ) ; int count = 0 ; List < Entity > maxVersions = new ArrayList < > ( numEntities ) ; code_block = ForStatement ; em . refreshIndex ( ) ; code_block = ForStatement ; System . setProperty ( EVENTS_DISABLED , "false" ) ; Thread . sleep ( 250 ) ; app . waitForQueueDrainAndRefreshIndex ( ) ; Thread . sleep ( 250 ) ; Results results = null ; count = 0 ; do code_block = "" ; while ( crs . size ( ) > 0 && count ++ < 2000 ) ; Assert . assertEquals ( "Expect no candidates" , 0 , crs . size ( ) ) ; log . info ( "Number of objects deleted" ) ; }
public void test() { if ( count % 100 == 0 ) { log . info ( "Processed {} lines" , count ) ; } }
private StringBuilder buildTargetTableArg ( StringBuilder builder , CatalogTable catalog ) throws FalconException { builder . append ( "--skip-dist-cache" ) . append ( ImportExportCommon . ARG_SEPARATOR ) ; Iterator < String > itr = Splitter . on ( "" ) . split ( catalog . getUri ( ) ) . iterator ( ) ; String dbTable = itr . next ( ) ; String partitions = itr . next ( ) ; LOG . debug ( "Target partitions {}" , partitions ) ; Iterator < String > itrDbTable = Splitter . on ( ":" ) . split ( dbTable ) . iterator ( ) ; itrDbTable . next ( ) ; String db = itrDbTable . next ( ) ; String table = itrDbTable . next ( ) ; LOG . debug ( "Target database {}, table {}" , db , table ) ; builder . append ( "--hcatalog-database" ) . append ( ImportExportCommon . ARG_SEPARATOR ) . append ( String . format ( "${coord:databaseOut('%s')}" , FeedImportCoordinatorBuilder . IMPORT_DATAOUT_NAME ) ) . append ( ImportExportCommon . ARG_SEPARATOR ) ; builder . append ( "--hcatalog-table" ) . append ( ImportExportCommon . ARG_SEPARATOR ) . append ( String . format ( "${coord:tableOut('%s')}" , FeedImportCoordinatorBuilder . IMPORT_DATAOUT_NAME ) ) . append ( ImportExportCommon . ARG_SEPARATOR ) ; Map < String , String > partitionsMap = ImportExportCommon . getPartitionKeyValues ( partitions ) ; code_block = IfStatement ; return builder ; }
private StringBuilder buildTargetTableArg ( StringBuilder builder , CatalogTable catalog ) throws FalconException { LOG . info ( "Catalog URI {}" , catalog . getUri ( ) ) ; builder . append ( "--skip-dist-cache" ) . append ( ImportExportCommon . ARG_SEPARATOR ) ; Iterator < String > itr = Splitter . on ( "" ) . split ( catalog . getUri ( ) ) . iterator ( ) ; String dbTable = itr . next ( ) ; String partitions = itr . next ( ) ; Iterator < String > itrDbTable = Splitter . on ( ":" ) . split ( dbTable ) . iterator ( ) ; itrDbTable . next ( ) ; String db = itrDbTable . next ( ) ; String table = itrDbTable . next ( ) ; builder . append ( "--hcatalog-database" ) . append ( ImportExportCommon . ARG_SEPARATOR ) . append ( String . format ( "${coord:databaseOut('%s')}" , FeedImportCoordinatorBuilder . IMPORT_DATAOUT_NAME ) ) . append ( ImportExportCommon . ARG_SEPARATOR ) ; builder . append ( "--hcatalog-table" ) . append ( ImportExportCommon . ARG_SEPARATOR ) . append ( String . format ( "${coord:tableOut('%s')}" , FeedImportCoordinatorBuilder . IMPORT_DATAOUT_NAME ) ) . append ( ImportExportCommon . ARG_SEPARATOR ) ; Map < String , String > partitionsMap = ImportExportCommon . getPartitionKeyValues ( partitions ) ; code_block = IfStatement ; LOG . info ( "Target table {}" , builder . toString ( ) ) ; return builder ; }
public void test() { try { return saxFactory . newTemplates ( stylesheet ) ; } catch ( TransformerConfigurationException tcx ) { LOG . error ( "Could not create stylesheet: {}" , stylesheet , tcx ) ; return null ; } }
@ Test public void test ( ) throws Exception { String publicWebappURL = OneRecordingServer . getPublicWebappUrl ( ) ; File fileToUpload = new File ( "test-files/logo.png" ) ; log . debug ( "Uploading file" ) ; uploadFileWithCURL ( publicWebappURL + "repository_servlet/video-upload" , fileToUpload ) ; log . debug ( "Waiting 10 seconds to auto-termination..." ) ; Thread . sleep ( 10 * 1000 ) ; File downloadedFile = new File ( "test-files/sampleDownload.txt" ) ; log . debug ( "Start downloading file" ) ; downloadFromURL ( publicWebappURL + "repository_servlet/video-download" , downloadedFile ) ; boolean equalFiles = TestUtils . equalFiles ( fileToUpload , downloadedFile ) ; code_block = IfStatement ; assertTrue ( "The uploadad and downloaded files are different" , equalFiles ) ; }
@ Test public void test ( ) throws Exception { String publicWebappURL = OneRecordingServer . getPublicWebappUrl ( ) ; log . debug ( "Start uploading content" ) ; File fileToUpload = new File ( "test-files/logo.png" ) ; uploadFileWithCURL ( publicWebappURL + "repository_servlet/video-upload" , fileToUpload ) ; log . debug ( "Waiting for content" ) ; Thread . sleep ( 10 * 1000 ) ; File downloadedFile = new File ( "test-files/sampleDownload.txt" ) ; log . debug ( "Start downloading file" ) ; downloadFromURL ( publicWebappURL + "repository_servlet/video-download" , downloadedFile ) ; boolean equalFiles = TestUtils . equalFiles ( fileToUpload , downloadedFile ) ; code_block = IfStatement ; assertTrue ( "The uploadad and downloaded files are different" , equalFiles ) ; }
@ Test public void test ( ) throws Exception { String publicWebappURL = OneRecordingServer . getPublicWebappUrl ( ) ; log . debug ( "Start uploading content" ) ; File fileToUpload = new File ( "test-files/logo.png" ) ; uploadFileWithCURL ( publicWebappURL + "repository_servlet/video-upload" , fileToUpload ) ; log . debug ( "Waiting 10 seconds to auto-termination..." ) ; Thread . sleep ( 10 * 1000 ) ; File downloadedFile = new File ( "test-files/sampleDownload.txt" ) ; downloadFromURL ( publicWebappURL + "repository_servlet/video-download" , downloadedFile ) ; log . debug ( "Download uploaded" ) ; boolean equalFiles = TestUtils . equalFiles ( fileToUpload , downloadedFile ) ; code_block = IfStatement ; assertTrue ( "The uploadad and downloaded files are different" , equalFiles ) ; }
public void test() { if ( equalFiles ) { log . debug ( "The uploadad and downloaded files are different" ) ; } else { log . debug ( "The uploadad and downloaded files are different" ) ; } }
public void test() { if ( equalFiles ) { log . debug ( "The uploadad and downloaded files are equal" ) ; } else { log . debug ( "The uploadad and downloaded files are not equal" ) ; } }
public void test() { try { String marshalInfo = SolrConfigDOM . marshalConfig ( solrConfig ) ; this . getConfigManager ( ) . updateConfigItem ( SolrConnectorSystemConstants . JPSOLRCLIENT_SYSTEM_CONFIG_NAME , marshalInfo ) ; } catch ( ApsSystemException e ) { logger . error ( "Error loading schema configuration" , e ) ; throw e ; } }
public void test() { if ( accountAsset . getQuantityATU ( ) > 0 || accountAsset . getUnconfirmedQuantityATU ( ) > 0 ) { accountAssetTable . insert ( accountAsset ) ; log . trace ( "<< update() SULETE, height={}, accountAsset = {}" , accountAsset , accountAsset ) ; } else { int height = blockChainInfoService . getHeight ( ) ; accountAssetTable . deleteAtHeight ( accountAsset , height ) ; log . trace ( "<< update() DELETE, height={}, accountAsset = {}" , height , accountAsset ) ; } }
public void test() { if ( accountAsset . getQuantityATU ( ) > 0 || accountAsset . getUnconfirmedQuantityATU ( ) > 0 ) { accountAssetTable . insert ( accountAsset ) ; log . trace ( "<< update() INSERT accountAsset = {}" , accountAsset ) ; } else { int height = blockChainInfoService . getHeight ( ) ; accountAssetTable . deleteAtHeight ( accountAsset , height ) ; log . trace ( "<< update() DELETE accountAsset = {}" , accountAsset ) ; } }
static ByteBuffer concatBuffers ( List < DataBuffer > buffers ) { log . info ( "[I208] concatBuffers: size={}" , buffers . size ( ) ) ; int partSize = 0 ; code_block = ForStatement ; ByteBuffer partData = ByteBuffer . allocate ( partSize ) ; buffers . forEach ( ( buffer ) code_block = LoopStatement ; ) ; partData . rewind ( ) ; log . info ( "[I208] partData: size={}" , partData . capacity ( ) ) ; return partData ; }
public void test() { try { conn . setAutoCommit ( false ) ; Statement statement = conn . createStatement ( ) ; int counter = 0 ; code_block = ForStatement ; statement . executeBatch ( ) ; conn . commit ( ) ; LOG . info ( new StringBuffer ( "Encrypted " ) . append ( " attributes of table " ) . append ( tableName ) ) ; } catch ( Exception e ) { conn . rollback ( ) ; LOG . error ( new String ( "Encrypted " ) . append ( " attributes of table " ) . append ( tableName ) , e ) ; return false ; } }
public void test() { try { this . controller . update ( update ) ; } catch ( OwsExceptionReport ex ) { logger . error ( "could not update {}" , update , ex ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceInventoryWarehouseServiceUtil . class , "getCommerceInventoryWarehouses" , _getCommerceInventoryWarehousesParameterTypes10 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , companyId , groupId , active ) ; Object returnObj = null ; code_block = TryStatement ;  return ( java . util . List < com . liferay . commerce . inventory . model . CommerceInventoryWarehouse > ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( MembershipRequestServiceUtil . class , "updateStatus" , _updateStatusParameterTypes3 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , membershipRequestId , reviewComments , statusId , serviceContext ) ; code_block = TryStatement ;  } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( hostname == null || hostname . length ( ) == 0 ) { log . error ( "No hostname specified" ) ; } }
public void test() { if ( baseDN == null || baseDN . length ( ) == 0 ) { LOG . error ( "BaseDN not specified" ) ; } }
public void test() { if ( externalIp . isPresent ( ) ) { hostname = externalIp . get ( ) . getHostAddress ( ) ; LOGGER . info ( "Replaced ${myip}-value 'localhost' with '{}'" , hostname ) ; } else { LOGGER . warn ( "Could not find external IP for hostname '{}'" , hostname ) ; } }
public void test() { try { entry . setValue ( entry . getValue ( ) . toString ( ) . replaceAll ( "\\$\\{urlencoded\\:basedn\\}" , URLEncoder . encode ( baseDN , "UTF-8" ) ) ) ; } catch ( UnsupportedEncodingException e ) { LOG . error ( "Error encoding  " , e ) ; } }
public void test() { try { SHUTDOWN_LOGGER . debug ( "Disposing component [{}]..." , instance . getClass ( ) . getName ( ) ) ; ( ( Disposable ) instance ) . dispose ( ) ; SHUTDOWN_LOGGER . debug ( "Component [{}] has been disposed" , instance . getClass ( ) . getName ( ) ) ; } catch ( ComponentLifecycleException e ) { SHUTDOWN_LOGGER . error ( "Component [{}] could not be disposed" , instance . getClass ( ) . getName ( ) , e ) ; } }
public void test() { for ( int i = 0 ; i < 8 ; i ++ ) { SameDiff sd = SameDiff . create ( ) ; SDVariable in3 = sd . var ( "in3" , Nd4j . rand ( new int [ ] code_block = "" ; ) ) ; SDVariable in2 = sd . var ( "in2" , in2Shape ) ; SDVariable bcOp ; String name ; code_block = SwitchStatement ; SDVariable outVar = sd . sum ( bcOp ) ; String msg = "(test " + i + ": " + name + ", dimension=" + dim_sz1 + ")" ; log . info ( msg ) ; INDArray in3Arr = Nd4j . randn ( new int [ ] code_block = "" ; ) . muli ( 100 ) ; INDArray in2Arr = Nd4j . randn ( in2Shape ) . muli ( 100 ) ; sd . associateArrayWithVariable ( in3Arr , in3 ) ; sd . associateArrayWithVariable ( in2Arr , in2 ) ; TestCase tc = new TestCase ( sd ) ; String error = OpValidation . validate ( tc ) ; code_block = IfStatement ; } }
public void test() { try { response = HttpUtil . executeUrl ( "GET" , urlStr , 10000 ) ; result = gson . fromJson ( response , HaasSohnpelletstoveJsonDataDTO . class ) ; resultOk = true ; } catch ( IOException e ) { logger . debug ( "Error processiong Get request {}" , urlStr ) ; statusDescr = "Timeout error with" + config . hostIP + ". Cannot find service on give IP. Please verify the IP-Address!" ; errorDetail = e . getMessage ( ) ; resultOk = false ; } catch ( Exception e ) { logger . debug ( "Unknwon Error: {}" , e . getMessage ( ) ) ; errorDetail = e . getMessage ( ) ; logger . debug ( "Stack trace:" , e ) ; resultOk = false ; } }
public void test() { try { response = HttpUtil . executeUrl ( "GET" , urlStr , 10000 ) ; logger . debug ( "OvenData = {}" , response ) ; result = gson . fromJson ( response , HaasSohnpelletstoveJsonDataDTO . class ) ; resultOk = true ; } catch ( IOException e ) { logger . debug ( "IOException: {}" , e . getMessage ( ) ) ; statusDescr = "Timeout error with" + config . hostIP + ". Cannot find service on give IP. Please verify the IP-Address!" ; errorDetail = e . getMessage ( ) ; resultOk = false ; } catch ( Exception e ) { logger . debug ( "Unknwon Error: {}" , e . getMessage ( ) ) ; errorDetail = e . getMessage ( ) ; resultOk = false ; } }
public void test() { try { response = HttpUtil . executeUrl ( "GET" , urlStr , 10000 ) ; logger . debug ( "OvenData = {}" , response ) ; result = gson . fromJson ( response , HaasSohnpelletstoveJsonDataDTO . class ) ; resultOk = true ; } catch ( IOException e ) { logger . debug ( "Error processiong Get request {}" , urlStr ) ; statusDescr = "Timeout error with" + config . hostIP + ". Cannot find service on give IP. Please verify the IP-Address!" ; errorDetail = e . getMessage ( ) ; resultOk = false ; } catch ( Exception e ) { logger . debug ( "Error processiong Get request {}" , urlStr ) ; errorDetail = e . getMessage ( ) ; resultOk = false ; } }
private static CuratorFramework provideInitializedZookeeperClient ( String zkConnection ) throws Exception { RetryPolicy retryPolicy = new ExponentialBackoffRetry ( 1000 , 3 ) ; CuratorFramework zkClient = CuratorFrameworkFactory . builder ( ) . namespace ( NAMESPACE ) . connectString ( zkConnection ) . retryPolicy ( retryPolicy ) . build ( ) ; LOG . info ( "Connecting to ZK cluster {}" , zkClient . getState ( ) ) ; zkClient . start ( ) ; LOG . info ( "Attempting to ZK cluster {} ..." , zkClient . getState ( ) ) ; zkClient . blockUntilConnected ( ) ; LOG . info ( "Connection to ZK cluster {}" , zkClient . getState ( ) ) ; return zkClient ; }
private static CuratorFramework provideInitializedZookeeperClient ( String zkConnection ) throws Exception { LOG . info ( "Creating Zookeeper Client connecting to {}" , zkConnection ) ; RetryPolicy retryPolicy = new ExponentialBackoffRetry ( 1000 , 3 ) ; CuratorFramework zkClient = CuratorFrameworkFactory . builder ( ) . namespace ( NAMESPACE ) . connectString ( zkConnection ) . retryPolicy ( retryPolicy ) . build ( ) ; zkClient . start ( ) ; zkClient . blockUntilConnected ( ) ; LOG . info ( "Connected to Zookeeper" ) ; LOG . info ( "Connection to ZK cluster {}" , zkClient . getState ( ) ) ; return zkClient ; }
private static CuratorFramework provideInitializedZookeeperClient ( String zkConnection ) throws Exception { LOG . info ( "Creating Zookeeper Client connecting to {}" , zkConnection ) ; RetryPolicy retryPolicy = new ExponentialBackoffRetry ( 1000 , 3 ) ; CuratorFramework zkClient = CuratorFrameworkFactory . builder ( ) . namespace ( NAMESPACE ) . connectString ( zkConnection ) . retryPolicy ( retryPolicy ) . build ( ) ; LOG . info ( "Connecting to ZK cluster {}" , zkClient . getState ( ) ) ; zkClient . start ( ) ; zkClient . blockUntilConnected ( ) ; LOG . info ( "Created Zookeeper Client." ) ; return zkClient ; }
public void test() { if ( regulating && regulatingForcedToOff ) { logger . isDebugEnabled ( ) ) ; regulated = false ; } }
public void startService ( String serviceName ) throws ServiceAdminException , RemoteException { log . info ( "Starting service " + serviceName ) ; serviceAdminStub . startService ( serviceName ) ; }
public void test() { if ( ! shutdown ( 10 , TimeUnit . SECONDS ) ) { log . error ( "Some processors are still active" ) ; } }
public void failJob ( String message , String details ) { LOG . error ( message ) ; throw new JobFailedException ( message , details ) ; }
public void test() { if ( remoteCommit . getServerId ( ) == DomainClassInfo . getServerId ( ) ) { logger . debug ( "Received remote commit message. ServerId={}, tx={}" , remoteCommit . getServerId ( ) , remoteCommit . getTxNumber ( ) ) ; } else { logger . debug ( "Received remote commit message. serverId={}, tx={}" , remoteCommit . getServerId ( ) , remoteCommit . getTxNumber ( ) ) ; REMOTE_COMMITS . offer ( remoteCommit ) ; } }
public void test() { if ( column . getCellEditor ( ) != null && column . getBeanPropertyAccessors ( ) == null ) { LOGGER . warn ( "CellEditor is not enabled for column " + column . getBeanPropertyAccessors ( ) ) ; } }
@ Override public int getProductCount ( ) { logger . debug ( "Getting product count: " , super . getProductCount ( ) ) ; return super . getProductCount ( ) ; }
public void test() { try { select ( ) ; } catch ( RuntimeException e ) { LOG . warn ( "Ignoring unexpected exception" , e ) ; } catch ( Exception e ) { LOG . warn ( "Ignoring unexpected exception" , e ) ; } }
public void test() { try { select ( ) ; } catch ( RuntimeException e ) { LOG . warn ( "Ignoring unexpected runtime exception" , e ) ; } catch ( Exception e ) { LOG . error ( "Ignoring unexpected exception" , e ) ; } }
public void test() { if ( adapter instanceof GeotoolsFeatureDataAdapter ) { gtAdapter = ( GeotoolsFeatureDataAdapter ) adapter ; } else-if ( ( adapter instanceof InternalDataAdapter ) && ( ( ( InternalDataAdapter ) adapter ) . getAdapter ( ) instanceof GeotoolsFeatureDataAdapter ) ) { gtAdapter = ( GeotoolsFeatureDataAdapter ) ( ( InternalDataAdapter ) adapter ) . getAdapter ( ) ; } else { LOGGER . error ( "Unable to get adapter" ) ; return null ; } }
public void test() { try { DriverManager . deregisterDriver ( driver ) ; } catch ( SQLException e ) { logger . error ( "Error while deregistering driver " + driver , e ) ; } }
public void test() { try { code_block = IfStatement ; field . setAccessible ( true ) ; code_block = IfStatement ; } catch ( Exception t ) { logger . error ( t . getMessage ( ) , t ) ; } }
public void test() { try { Field [ ] fields = clazz . getDeclaredFields ( ) ; code_block = ForStatement ; } catch ( Exception t ) { LOG . error ( "Error loading " + clazz , t ) ; } }
public void test() { { String atPort = getAtPort ( ) ; logger . debug ( "sendCommand getSignalStrength :: {}" , Arrays . toString ( reply ) ) ; byte [ ] reply ; CommConnection commAtConnection = openSerialPort ( atPort ) ; code_block = IfStatement ; code_block = TryStatement ;  closeSerialPort ( commAtConnection ) ; code_block = IfStatement ; } }
public void delete ( StgMUmsetzStat persistentInstance ) { log . debug ( "deleting StgMUmsetzStat instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . delete ( persistentInstance ) ; log . debug ( "delete successful" ) ; } catch ( RuntimeException re ) { log . error ( "delete failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . delete ( persistentInstance ) ; log . debug ( "delete successful" ) ; } catch ( RuntimeException re ) { log . error ( "delete failed" , re ) ; throw re ; } }
@ Override public void deleteByQuery ( TypeDescriptor typeDescriptor , Object query ) throws ClientException { JsonNode queryJsonNode = getModelConverter ( ) . convertQuery ( query ) ; LOG . debug ( "Deleting query {}" , queryJsonNode ) ; String json = writeRequestFromJsonNode ( queryJsonNode ) ; Response deleteResponse = restCallTimeoutHandler ( ( ) -> getClient ( ) . performRequest ( ElasticsearchKeywords . ACTION_POST , ElasticsearchResourcePaths . deleteByQuery ( typeDescriptor ) , Collections . emptyMap ( ) , ApplicationJsonEntityBuilder . buildFrom ( json ) , new ContentTypeApplicationJsonHeader ( ) ) , typeDescriptor . getIndex ( ) , "DELETE BY QUERY" ) ; code_block = IfStatement ; }
private static X509Certificate generateCert ( ZonedDateTime now , X509CertificateHolder certHolder ) throws Exception { final X509Certificate certificate = new JcaX509CertificateConverter ( ) . getCertificate ( certHolder ) ; certificate . checkValidity ( Date . from ( now . toInstant ( ) ) ) ; certificate . verify ( certificate . getPublicKey ( ) ) ; final String fingerprint = BaseEncoding . base16 ( ) . withSeparator ( ":" , 2 ) . encode ( MessageDigest . getInstance ( "SHA-256" ) . digest ( certificate . getEncoded ( ) ) ) ; logger . debug ( "{}" , fingerprint ) ; return certificate ; }
public void test() { if ( ! ( abstractParameter instanceof NeptuneValidateParameters ) ) { log . error ( "invalid parameters for Neptune validation " ) ; return false ; } }
public void test() { try { job = this . jobManager . queueJob ( config ) ; } catch ( JobException e ) { String errmsg = this . i18n . tr ( "An unexpected exception occurred " + "while scheduling job \"{0}\"" , config . getJobKey ( ) ) ; log . error ( errmsg , e ) ; throw new IseException ( errmsg , e ) ; } }
public void test() { if ( pair . length != 2 ) { logger . warn ( "Invalid pair: {}" , pair ) ; continue ; } }
public void test() { if ( StringUtils . isNotBlank ( localPropsEnv ) ) { LOGGER . debug ( "Loading local.properties from file/v [" + localPropsEnv + "]" ) ; resourceAsStream = new FileInputStream ( localPropsEnv ) ; } else { LOGGER . debug ( "Loading local.properties from classpath [/local.properties]" ) ; resourceAsStream = Latkes . class . getResourceAsStream ( "/local.properties" ) ; } }
public void test() { if ( StringUtils . isNotBlank ( localPropsEnv ) ) { LOGGER . debug ( "Loading local.properties from env var [$LATKE_LOCAL_PROPS=" + localPropsEnv + "]" ) ; resourceAsStream = new FileInputStream ( localPropsEnv ) ; } else { LOGGER . debug ( "Loading local.properties from /local.properties" ) ; resourceAsStream = Latkes . class . getResourceAsStream ( "/local.properties" ) ; } }
public void test() { try { SLDClassifier c = new SLDClassifier ( credentials , new ClassifierCommand ( getBodyFromRequest ( request ) ) , factory ) ; SLDDocService service = new SLDDocService ( this . docTempDir , this . connectionPool ) ; String fileName = service . saveData ( c . getSLD ( ) , SecurityHeaders . decode ( request . getHeader ( SEC_USERNAME ) ) ) ; PrintWriter out = response . getWriter ( ) ; out . println ( "{\"success\":true,\"" + FILEPATH_VARNAME + "\":\"" + SLD_URL + fileName + "\"}" ) ; } catch ( DocServiceException e ) { LOG . error ( "Cannot do classification" , e ) ; sendErrorToClient ( response , e . getErrorCode ( ) , e . getMessage ( ) ) ; } catch ( IOException e ) { LOG . error ( "I/O exception encountered while doing a classification" , e ) ; } }
public void test() { try { SLDClassifier c = new SLDClassifier ( credentials , new ClassifierCommand ( getBodyFromRequest ( request ) ) , factory ) ; SLDDocService service = new SLDDocService ( this . docTempDir , this . connectionPool ) ; String fileName = service . saveData ( c . getSLD ( ) , SecurityHeaders . decode ( request . getHeader ( SEC_USERNAME ) ) ) ; PrintWriter out = response . getWriter ( ) ; out . println ( "{\"success\":true,\"" + FILEPATH_VARNAME + "\":\"" + SLD_URL + fileName + "\"}" ) ; } catch ( DocServiceException e ) { sendErrorToClient ( response , e . getErrorCode ( ) , e . getMessage ( ) ) ; LOG . error ( "Error occured while doing a classification" , e ) ; } catch ( IOException e ) { LOG . error ( "Error occured while doing a classification" , e ) ; } }
public void test() { if ( CONVERTER_EP . equals ( extensionPoint ) ) { ConverterDescriptor desc = ( ConverterDescriptor ) contribution ; registerConverter ( desc ) ; } else-if ( CONFIG_EP . equals ( extensionPoint ) ) { GlobalConfigDescriptor desc = ( GlobalConfigDescriptor ) contribution ; config . update ( desc ) ; } else { logger . error ( "Unable to update extension point: {}" , extensionPoint ) ; } }
public void test() { try { String response = messageConverter . getResponseTextMessage ( ActiveThreadCountHandler . API_ACTIVE_THREAD_COUNT , resultMap ) ; TextMessage responseTextMessage = new TextMessage ( response ) ; return responseTextMessage ; } catch ( JsonProcessingException e ) { logger . error ( "error processing response text message" ) ; } }
public void test() { if ( meterRemovalPlan . isEmpty ( ) ) { LOG . info ( "No meters to register for nodeId {}" , nodeId . getValue ( ) ) ; return RpcResultBuilder . < Void > success ( ) . buildFuture ( ) ; } }
public void test() { for ( Meter meter : meterRemovalPlan . getItemsToPush ( ) ) { final KeyedInstanceIdentifier < Meter , MeterKey > meterIdent = nodeIdent . child ( Meter . class , meter . key ( ) ) ; LOG . debug ( "Removing meter {}" , meter . getName ( ) ) ; allResults . add ( meterForwarder . remove ( meterIdent , meter , nodeIdent ) ) ; meterCrudCounts . incRemoved ( ) ; } }
public void test() { if ( tableHelper != null ) { tableHelper . configure ( tops ) ; } else { log . info ( "No configuration supplied for table: " + table ) ; } }
public void test() { if ( ActiveMQRALogger . LOGGER . isTraceEnabled ( ) ) { ActiveMQRALogger . LOGGER . trace ( "execute()" ) ; } }
public void test() { if ( host == null ) { return "" ; } }
public void test() { if ( defaultZoneHostnames . size ( ) == 1 && defaultZoneHostnames . contains ( "localhost" ) ) { logger . warn ( "Could not resolve hostname 'localhost'" ) ; return "" ; } }
public void test() { try { List < Person > personenUmsetzungDurch = getPersonsbyProperty ( massnahme ) ; Set < Property > rolesToSearch = findRole ( massnahme ) ; code_block = IfStatement ; } catch ( CommandException ce ) { LOG . error ( "Error while deleting massnahme description." , ce ) ; } }
public void test() { try { Authentication authentication = registery . getByToken ( authToken . get ( ) ) . authenticate ( ( HttpServletRequest ) request , authToken . get ( ) ) ; SecurityContextHolder . getContext ( ) . setAuthentication ( authentication ) ; chain . doFilter ( request , response ) ; SecurityContextHolder . getContext ( ) . setAuthentication ( null ) ; return ; } catch ( OAuthAuthenticationException e ) { LOGGER . warn ( "OAuthAuthenticationException." ) ; ( ( HttpServletResponse ) response ) . sendError ( HttpServletResponse . SC_UNAUTHORIZED , e . getMessage ( ) ) ; return ; } catch ( Exception e ) { LOGGER . warn ( "Server Error." , e ) ; ( ( HttpServletResponse ) response ) . sendError ( HttpServletResponse . SC_INTERNAL_SERVER_ERROR , e . getMessage ( ) ) ; return ; } }
public void test() { try { Authentication authentication = registery . getByToken ( authToken . get ( ) ) . authenticate ( ( HttpServletRequest ) request , authToken . get ( ) ) ; SecurityContextHolder . getContext ( ) . setAuthentication ( authentication ) ; chain . doFilter ( request , response ) ; SecurityContextHolder . getContext ( ) . setAuthentication ( null ) ; return ; } catch ( OAuthAuthenticationException e ) { LOGGER . warn ( "Could not authenticate bearer / jwt token" , e ) ; ( ( HttpServletResponse ) response ) . sendError ( HttpServletResponse . SC_UNAUTHORIZED , e . getMessage ( ) ) ; return ; } catch ( Exception e ) { LOGGER . warn ( "Error occurred during authentication" , e ) ; ( ( HttpServletResponse ) response ) . sendError ( HttpServletResponse . SC_INTERNAL_SERVER_ERROR , e . getMessage ( ) ) ; return ; } }
public void test() { try ( InputStream in = Files . newInputStream ( propertiesFilePath ) ) { logger . info ( "Loading properties {}" , propertiesFilePath ) ; Properties props = new Properties ( ) ; props . load ( in ) ; return props ; } catch ( Exception e ) { logger . error ( "Loading properties {} failed" , propertiesFilePath , e ) ; } }
public void test() { try ( InputStream in = Files . newInputStream ( propertiesFilePath ) ) { Properties props = new Properties ( ) ; props . load ( in ) ; logger . info ( "properties loaded from {}" , propertiesFilePath ) ; return props ; } catch ( Exception e ) { logger . warn ( "Could not load properties from {}" , propertiesFilePath , e ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
private MCRDerivate createDerivate ( MCRObjectID objectID , List < MCRMetaClassification > classifications ) throws MCRPersistenceException , MCRAccessException { MCRObjectID derivateID = getNewCreateDerivateID ( objectID ) ; MCRDerivate derivate = new MCRDerivate ( ) ; derivate . setId ( derivateID ) ; derivate . getDerivate ( ) . getClassifications ( ) . addAll ( classifications ) ; String schema = MCRConfiguration2 . getString ( "MCR.Metadata.Config.derivate" ) . orElse ( "datamodel-derivate.xml" ) . replaceAll ( ".xml" , ".xsd" ) ; derivate . setSchema ( schema ) ; MCRMetaLinkID linkId = new MCRMetaLinkID ( ) ; linkId . setSubTag ( "linkmeta" ) ; linkId . setReference ( objectID , null , null ) ; derivate . getDerivate ( ) . setLinkMeta ( linkId ) ; MCRMetaIFS ifs = new MCRMetaIFS ( ) ; ifs . setSubTag ( "internal" ) ; ifs . setSourcePath ( null ) ; derivate . getDerivate ( ) . setInternals ( ifs ) ; MCRMetadataManager . create ( derivate ) ; setDefaultPermissions ( derivateID ) ; LOGGER . info ( "Created {}" , derivate ) ; return derivate ; }
@ Override @ Transactional public boolean pruneData ( OrganisationUnit organisationUnit ) { User user = currentUserService . getCurrentUser ( ) ; code_block = IfStatement ; dataApprovalService . deleteDataApprovals ( organisationUnit ) ; dataApprovalAuditService . deleteDataApprovalAudits ( organisationUnit ) ; completeRegistrationService . deleteCompleteDataSetRegistrations ( organisationUnit ) ; dataValueAuditService . deleteDataValueAudits ( organisationUnit ) ; dataValueService . deleteDataValues ( organisationUnit ) ; log . info ( "Finished prune data for organisation {}" , organisationUnit ) ; return true ; }
public void test() { try { code_block = ForStatement ; } catch ( EOFException e ) { log . debug ( "EOFException" , e ) ; } }
public void test() { try ( InputStream inputStream = provider . getServer ( url ) . sendGet ( url , response ) ) { code_block = IfStatement ; int in = 0 ; int total = 0 ; DataInputStream dataStream = new DataInputStream ( inputStream ) ; code_block = TryStatement ;  double tolerance = ( 10 - .5 ) / 10 ; return in / ( double ) total > tolerance ; } catch ( IOException | URISyntaxException e ) { logger . error ( "Couldn't contact GeoSearch" , e ) ; } }
public void test() { if ( ! inThread ) { log . warn ( "waitWarrantRunState invoked from invalid context" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( warrant . getRunMode ( ) != Warrant . MODE_RUN ) { LOGGER . log ( Level . SEVERE , "CODE_REQUIRE" ) ; return null ; } }
public void test() { try { logger . debug ( "Upserting a Permission. permission id [{}]" , userLink ) ; RxDocumentServiceRequest request = getPermissionRequest ( userLink , permission , options , OperationType . Upsert ) ; code_block = IfStatement ; return this . upsert ( request ) . map ( response -> toResourceResponse ( response , Permission . class ) ) ; } catch ( Exception e ) { logger . debug ( "Failure in upserting a Permission due to [{}]" , e . getMessage ( ) , e ) ; return Observable . error ( e ) ; } }
public void test() { try { logger . debug ( "Upserting a Permission. userLink [{}], permission id [{}]" , userLink , permission . getId ( ) ) ; RxDocumentServiceRequest request = getPermissionRequest ( userLink , permission , options , OperationType . Upsert ) ; code_block = IfStatement ; return this . upsert ( request ) . map ( response -> toResourceResponse ( response , Permission . class ) ) ; } catch ( Exception e ) { logger . debug ( "Failure in upserting a Permission due to [{}]" , e . getMessage ( ) , e ) ; return Observable . error ( e ) ; } }
public void test() { try { code_block = IfStatement ; long msb = 0 ; long lsb = 0 ; for ( int i = 0 ; i < 8 ; i ++ ) msb = ( msb << 8 ) | ( bytes [ i ] & 0xff ) ; for ( int i = 8 ; i < 16 ; i ++ ) lsb = ( lsb << 8 ) | ( bytes [ i ] & 0xff ) ; return new UUID ( msb , lsb ) ; } catch ( Exception e ) { log . error ( "Unable to create UUID from UUID {}" , msb , e ) ; throw new PropertyAccessException ( e ) ; } }
public void test() { try { row = URLEncoder . encode ( Bytes . toStringBinary ( put . getRow ( ) ) , UTF8 ) ; cf = URLEncoder . encode ( Bytes . toString ( CF ) , UTF8 ) ; } catch ( UnsupportedEncodingException e ) { LOG . error ( "putKey failed" , e ) ; } }
public void test() { try { exist = ( readPubsubNumsubReply ( channel ) > 0 ) ? true : false ; } catch ( ProtocolException e ) { logger . error ( "Error enumerating topic" , e ) ; } }
public void test() { try { return IOUtils . toString ( is ) ; } catch ( IOException e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { metadata = fileDataStore . add ( byteSource , metadata ) ; } catch ( IOException e1 ) { logger . error ( e1 . getMessage ( ) , e1 ) ; throw new BusinessException ( "Can not create a copy of existing document." ) ; } }
public void test() { try { sanRecords = cert . getSubjectAlternativeNames ( ) ; } catch ( CertificateParsingException ex ) { log . error ( "Invalid subject alternative names" ) ; log . debug ( "Exception" , ex ) ; sanRecords = null ; } }
private void log ( final LogMessage . Stream stream , final String containerId , final JobId jobId , final StringBuilder stringBuilder ) { stringBuilder . setLength ( 0 ) ; logger . log ( stringBuilder ) ; }
public void test() { if ( size > rangeFraction . getMaxSize ( ) . getBytes ( ) ) { LOG . info ( "The derived from fraction {} ({}) is greater than its max value {}, max value will be used instead" , memoryDescription , relative . toHumanReadableString ( ) , rangeFraction . getMaxSize ( ) . toHumanReadableString ( ) ) ; size = rangeFraction . getMaxSize ( ) . getBytes ( ) ; } else-if ( size < rangeFraction . getMinSize ( ) . getBytes ( ) ) { LOG . info ( "The derived from fraction {} ({}) is less than its min value {}, min value will be used instead" , memoryDescription , relative . toHumanReadableString ( ) , rangeFraction . getMinSize ( ) . toHumanReadableString ( ) ) ; size = rangeFraction . getMinSize ( ) . getBytes ( ) ; } }
public void test() { if ( size > rangeFraction . getMaxSize ( ) . getBytes ( ) ) { LOG . info ( "The derived from fraction {} ({}) is greater than its max value {}, max value will be used instead" , memoryDescription , relative . toHumanReadableString ( ) , rangeFraction . getMaxSize ( ) . toHumanReadableString ( ) ) ; size = rangeFraction . getMaxSize ( ) . getBytes ( ) ; } else-if ( size < rangeFraction . getMinSize ( ) . getBytes ( ) ) { LOG . info ( "The derived from fraction {} ({}) is smaller than its min value {}, min value will be used instead" , memoryDescription , relative . toHumanReadableString ( ) , rangeFraction . getMinSize ( ) . toHumanReadableString ( ) ) ; size = rangeFraction . getMinSize ( ) . getBytes ( ) ; } }
public void test() { try { code_block = IfStatement ; return _commerceBOMFolderModelResourcePermission . contains ( permissionChecker , commerceBOMFolder , ActionKeys . UPDATE ) ; } catch ( PortalException portalException ) { _log . error ( portalException , portalException ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { listener . tagStyleChanged ( event ) ; } catch ( Exception ex ) { Log . log ( ex ) ; } }
public void test() { try { PluginTagStyleEvent event = createTagStyleEvent ( e ) ; code_block = ForStatement ; } catch ( Exception ex ) { LOG . warning ( ex . getMessage ( ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "JDBC::{}" , sql ) ; } }
public void test() { if ( isEmpty ( ) ) { return true ; } }
private void warnFragmented ( ByteBuffer buffer , int iterations , AtomicInteger counter , String operation ) { int total = counter . incrementAndGet ( ) ; LOGGER . warn ( "Fragmented {} after {} ms" , total , iterations ) ; }
public PreparedCommand buildCommand ( MetaCommand mc , List < ArgumentAssignment > argAssignmentList , String origin , int seq , User user ) throws ErrorInCommand , YamcsException { log . debug ( "buildCommand {}" , meta . getQualifiedName ( ) ) ; CommandBuildResult cbr = metaCommandProcessor . buildCommand ( mc , argAssignmentList ) ; CommandId cmdId = CommandId . newBuilder ( ) . setCommandName ( mc . getQualifiedName ( ) ) . setOrigin ( origin ) . setSequenceNumber ( seq ) . setGenerationTime ( processor . getCurrentTime ( ) ) . build ( ) ; PreparedCommand pc = new PreparedCommand ( cmdId ) ; pc . setMetaCommand ( mc ) ; pc . setBinary ( cbr . getCmdPacket ( ) ) ; pc . setUsername ( user . getName ( ) ) ; Set < String > userAssignedArgumentNames = argAssignmentList . stream ( ) . map ( a -> a . getArgumentName ( ) ) . collect ( Collectors . toSet ( ) ) ; pc . setArgAssignment ( cbr . getArgs ( ) , userAssignedArgumentNames ) ; return pc ; }
public void init ( ) { String methodName = "init" ; LOGGER . trace ( ENTERING , methodName ) ; LOGGER . trace ( EXITING , methodName ) ; }
public void init ( ) { String methodName = "init" ; LOGGER . trace ( ENTERING , methodName ) ; LOGGER . trace ( EXITING , methodName ) ; }
public void test() { try { connector . sendCommand ( NuvoCommand . CFGTIME . getValue ( ) + DATE_FORMAT . format ( new Date ( ) ) ) ; } catch ( NuvoException e ) { logger . warn ( "Nuvo Command: {} failed" , e . getMessage ( ) ) ; } }
public void test() { try { fetchWeatherInfo ( latitude , longitude , closestCity . geo_name , date ) ; } catch ( Exception e ) { logger . warn ( "Exception occurred during weather update: {}" , e . getMessage ( ) , e ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( debugEnabled ) { log . debug ( "destroy({}) Destroy ({})" , this , channel ) ; } }
public void test() { if ( debugEnabled ) { log . debug ( "destroy({}) Destroy ({})" , this , channel ) ; } }
protected Response getPort ( String nodeId , String portId ) { log . debug ( "" ) ; Port port = topology . getPort ( nodeId , portId ) ; code_block = IfStatement ; return new Response ( Response . OK , port ) ; }
public void test() { if ( element == null ) { log . warn ( "null element" ) ; return RequestStatus . FAILURE ; } }
public void test() { try { code_block = IfStatement ; return RequestStatus . SUCCESS ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; return RequestStatus . FAILURE ; } }
public void test() { if ( contextChain . addAuxiliaryConnection ( connectionContext ) ) { LOG . debug ( "Added auxiliary connection {}" , deviceInfo ) ; return ConnectionStatus . MAY_CONTINUE ; } else { LOG . warn ( "Not able to add auxiliary connection to the device {}" , deviceInfo ) ; return ConnectionStatus . REFUSING_AUXILIARY_CONNECTION ; } }
public void test() { if ( contextChain . addAuxiliaryConnection ( connectionContext ) ) { LOG . info ( "An auxiliary connection was added to device: {}" , deviceInfo ) ; return ConnectionStatus . MAY_CONTINUE ; } else { LOG . info ( "Auxiliary connection was not added to the device: {}" , deviceInfo ) ; return ConnectionStatus . REFUSING_AUXILIARY_CONNECTION ; } }
public void test() { try ( RDF4JProtocolSession protocolSession = getSharedHttpClientSessionManager ( ) . createRDF4JProtocolSession ( serverURL ) ) { protocolSession . setUsernameAndPassword ( username , password ) ; protocolSession . deleteRepository ( repositoryID ) ; } catch ( IOException e ) { logger . error ( e . getMessage ( ) ) ; throw new RepositoryConfigException ( e ) ; } }
public void test() { try { return _assetEntryService . getEntries ( assetEntryQuery ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; } }
@ Override public Optional < Page > getGroupPage ( String groupId , String pageUrl ) throws IOException { LOG . debug ( "getting group page " + pageUrl ) ; String encodedUrl = URLEncoder . encode ( pageUrl , CanvasConstants . URLENCODING_TYPE ) ; String url = buildCanvasUrl ( "groups/" + groupId + "/pages/" + encodedUrl , Collections . emptyMap ( ) ) ; Response response = canvasMessenger . getSingleResponseFromCanvas ( oauthToken , url ) ; return responseParser . parseToObject ( Page . class , response ) ; }
public void test() { try { Message msg = currentConnector . getWithoutAck ( batchSize , timeout , unit ) ; logger . info ( "retrying connector for next round" ) ; return msg ; } catch ( Throwable t ) { times ++ ; restart ( ) ; logger . info ( "restart the connector for next round retry." ) ; } }
public void test() { try { Message msg = currentConnector . getWithoutAck ( batchSize , timeout , unit ) ; logger . debug ( "sending message: {}" , msg ) ; return msg ; } catch ( Throwable t ) { logger . warn ( String . format ( "something goes wrong when getWithoutAck data from server:%s" , currentConnector != null ? currentConnector . getAddress ( ) : "null" ) , t ) ; times ++ ; restart ( ) ; } }
public void test() { if ( retryInterval > 0 ) { spoolMessage ( msg ) ; scheduleRetry ( ) ; } else { LOG . error ( "Failed to retry message {}" , msg , e ) ; throw e ; } }
@ Override public synchronized void init ( HiveConf hiveConf ) { log . info ( "Initializing LensML implementation" ) ; ml = new LensMLImpl ( hiveConf ) ; ml . init ( hiveConf ) ; super . init ( hiveConf ) ; serviceProviderFactory = getServiceProviderFactory ( hiveConf ) ; }
@ Override public void onMode ( String string , IRCUser ircUser , IRCModeParser ircModeParser ) { super . onMode ( string , ircUser , ircModeParser ) ; LOG . info ( "onMode.ircUser = " + ircUser ) ; LOG . info ( "onMode.ircModeParser = " + ircModeParser ) ; LOG . info ( "onMode.ircModeParser = " + ircModeParser ) ; }
@ Override public void onMode ( String string , IRCUser ircUser , IRCModeParser ircModeParser ) { super . onMode ( string , ircUser , ircModeParser ) ; LOG . info ( "onMode.string = " + string ) ; LOG . info ( "onMode.ircModeParser = " + ircModeParser ) ; LOG . info ( "onMode.ircUser = " + user ) ; }
@ Override public void onMode ( String string , IRCUser ircUser , IRCModeParser ircModeParser ) { super . onMode ( string , ircUser , ircModeParser ) ; LOG . info ( "onMode.string = " + string ) ; LOG . info ( "onMode.ircUser = " + ircUser ) ; LOG . info ( "onMode.ircUser = " + ircUser ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( session != null ) { session . setAttribute ( "user" , "999998" ) ; } else { log . error ( "Cannot set session" ) ; } }
public void test() { if ( orderType == null ) { log . warn ( "Unable to determine order type to be null" ) ; } }
public void test() { try { spawnMQ . sendControlMessage ( new HostState ( HostMessage . ALL_HOSTS ) ) ; } catch ( Exception e ) { log . error ( "Error sending spawn message" , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( error != null ) { log . error ( "Failed to start connector '" + connectorName + "'" , error ) ; } }
public void test() { try { int returnValue = CommerceNotificationQueueEntryServiceUtil . getCommerceNotificationQueueEntriesCount ( groupId ) ; return returnValue ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( listeners . isEmpty ( ) ) { logger . debug ( "Received AF_INCOMING_MSG from {} and cluster {} to end point {}. Data: {}" , msg . getSrcAddr ( ) , msg . getClusterId ( ) , msg . getDstEndpoint ( ) , msg ) ; } else { logger . trace ( "Received AF_INCOMING_MSG from {} and cluster {} to end point {}. Data: {}" , msg . getSrcAddr ( ) , msg . getClusterId ( ) , msg . getDstEndpoint ( ) , msg ) ; } }
public void test() { if ( listeners . isEmpty ( ) ) { logger . warn ( "Received AF_INCOMING_MSG but no listeners. " + "Message was from {} and cluster {} to end point {}. Data: {}" , msg . getSrcAddr ( ) , msg . getClusterId ( ) , msg . getDstEndpoint ( ) , msg ) ; } else { logger . warn ( "Received AF_INCOMING_MSG, {} was from {} and cluster {} to end point {}. Data: {}" , msg . getSrcAddr ( ) , msg . getClusterId ( ) , msg . getDstEndpoint ( ) , msg ) ; } }
public void test() { try { listener . notify ( msg ) ; } catch ( final Exception e ) { logger . warn ( "Failed to notify listener {}" , listener , e ) ; } }
public void test() { try { List < RadiusClient > radiusclients = null ; code_block = IfStatement ; radiusclients . sort ( Comparator . comparing ( RadiusClient :: getName ) ) ; this . results . clear ( ) ; code_block = ForStatement ; this . oldSearchPattern = this . searchPattern ; this . searchPattern = "" ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; facesMessages . add ( FacesMessage . SEVERITY_ERROR , "{msgs['radius.clients.search.error']}" ) ; conversationService . endConversation ( ) ; return OxTrustConstants . RESULT_FAILURE ; } }
public void test() { try { listener . gotOAuthAccessToken ( token ) ; } catch ( Exception e ) { logger . warn ( "Exception at getOAuthRequestTokenAsync" , e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Failed to clean up." , e ) ; } }
public void test() { try { wifiInterfaceAddressConfig = ( WifiInterfaceAddressConfig ) ( ( AbstractNetInterface < ? > ) wifiInterfaceConfig ) . getNetInterfaceAddressConfig ( ) ; wifiMode = wifiInterfaceAddressConfig . getMode ( ) ; } catch ( KuraException e ) { logger . error ( " wifiInterfaceAddressConfig failed" , e ) ; } }
public void test() { try { edlController . setEdlGlobalConfig ( edlGlobalConfig ) ; edlController . setDefaultDOM ( getDefaultDOM ( edlAssociation ) ) ; loadConfigProcessors ( edlController , edlGlobalConfig ) ; loadPreProcessors ( edlController , edlGlobalConfig ) ; loadPostProcessor ( edlController , edlGlobalConfig ) ; loadStateComponents ( edlController , edlGlobalConfig ) ; loadStyle ( edlController ) ; } catch ( Exception e ) { String edl = null ; code_block = IfStatement ; String message = "Error creating controller for EDL" + ( edl == null ? "" : ": " + edl ) ; LOG . error ( message ) ; throw new WorkflowRuntimeException ( "Problems creating controller for EDL: " + edl , e ) ; } }
public void setPassivator ( final ConnectionPassivator p ) { log . trace ( "settingPassivator: {}" , p ) ; passivator = p ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { IgniteInternalTx tx = ctx . cache ( ) . context ( ) . tm ( ) . tx ( ) ; log . debug ( "Transaction created: " + tx ) ; } }
public void test() { try { log . debug ( "Trying AK 2.2 SslFactory methods." ) ; return ( SSLContext ) SslFactory . class . getDeclaredMethod ( "sslContext" ) . invoke ( sslFactory ) ; } catch ( Exception e ) { log . warn ( "Failed AK 2.2.2 " , e ) ; Object sslEngine ; code_block = TryStatement ;  code_block = TryStatement ;  } }
public void test() { try { sslEngine = SslFactory . class . getDeclaredMethod ( "sslEngineBuilder" ) . invoke ( sslFactory ) ; log . debug ( "Using AK 2.2-2.5 SslFactory methods." ) ; } catch ( Exception ex ) { log . warn ( "Could not use SslFactory: " + ex . getMessage ( ) ) ; code_block = TryStatement ;  } }
public void test() { if ( null != pendingSlotRequest ) { pendingSlotRequest . setRequestFuture ( null ) ; code_block = TryStatement ;  } else { LOG . trace ( "There is no pending slot request." ) ; } }
public void test() { try { synchronized ( runState ) code_block = "" ; code_block = ForStatement ; code_block = ForStatement ; int currentNumOfMessagesInProcess = getNumOfMessagesInProcess ( ) ; code_block = IfStatement ; waitForNoMessagesInProcess ( ) ; log . debug ( "Adapter [" + name + "] is stopping pipeline" ) ; pipeline . stop ( ) ; statsUpSince = 0 ; runState . setRunState ( RunStateEnum . STOPPED ) ; getMessageKeeper ( ) . add ( "Adapter [" + name + "] stopped" ) ; } catch ( Throwable t ) { log . error ( "Error stopping adapter [" + name + "]" , t ) ; addErrorMessageToMessageKeeper ( "got error stopping Adapter" , t ) ; runState . setRunState ( RunStateEnum . ERROR ) ; } finally { configuration . removeStopAdapterThread ( this ) ; } }
public void test() { try { Thread . sleep ( 1000 ) ; } catch ( InterruptedException e ) { logger . error ( "" , e ) ; } }
public void test() { try { synchronized ( runState ) code_block = "" ; log . debug ( "Adapter [" + name + "] is stopping receivers" ) ; code_block = ForStatement ; code_block = ForStatement ; int currentNumOfMessagesInProcess = getNumOfMessagesInProcess ( ) ; code_block = IfStatement ; waitForNoMessagesInProcess ( ) ; pipeline . stop ( ) ; statsUpSince = 0 ; runState . setRunState ( RunStateEnum . STOPPED ) ; getMessageKeeper ( ) . add ( "Adapter [" + name + "] stopped" ) ; } catch ( Throwable t ) { log . error ( "Error stopping adapter [" + name + "]" , t ) ; addErrorMessageToMessageKeeper ( "got error stopping Adapter" , t ) ; runState . setRunState ( RunStateEnum . ERROR ) ; } finally { configuration . removeStopAdapterThread ( this ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { ret = HBaseClient . connectionTest ( serviceName , configs ) ; } catch ( HadoopException e ) { LOG . error ( "connection test failed!" , e ) ; throw e ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( reader . peek ( ) == JsonToken . NULL ) { reader . skipValue ( ) ; } else-if ( name . equals ( VALUE ) ) { scope . setValue ( reader . nextString ( ) ) ; } else-if ( name . equals ( DESCRIPTION ) ) { scope . setDescription ( reader . nextString ( ) ) ; } else-if ( name . equals ( RESTRICTED ) ) { scope . setRestricted ( reader . nextBoolean ( ) ) ; } else-if ( name . equals ( DEFAULT_SCOPE ) ) { scope . setDefaultScope ( reader . nextBoolean ( ) ) ; } else-if ( name . equals ( ICON ) ) { scope . setIcon ( reader . nextString ( ) ) ; } else-if ( name . equals ( STRUCTURED ) ) { logger . warn ( "Found a structured scope, ignoring structure" ) ; } else-if ( name . equals ( STRUCTURED_PARAMETER ) ) { logger . warn ( "Found a structured scope, ignoring structure" ) ; } else { logger . error ( "Found unexpected scope, ignoring structure" ) ; reader . skipValue ( ) ; } }
public void test() { switch ( reader . peek ( ) ) { case END_OBJECT : continue ; case NAME : String name = reader . nextName ( ) ; code_block = IfStatement ; break ; default : logger . debug ( "Found unexpected entry" ) ; reader . skipValue ( ) ; continue ; } }
private void writeCommandToDevice ( RequestMessage requestMessage ) throws IOException { code_block = IfStatement ; byte [ ] deviceCommand = ( requestMessage . getDeviceCommand ( ) + '\r' ) . getBytes ( ) ; logger . debug ( "Writing request to device: {}" , deviceCommand ) ; connectionManager . getCommandOut ( ) . write ( deviceCommand ) ; connectionManager . getCommandOut ( ) . flush ( ) ; }
private void generateCoordinates ( Vector2d firstBondVector , boolean isConnected , boolean isSubLayout ) throws CDKException { if ( firstBondVector == DEFAULT_BOND_VECTOR ) firstBondVector = new Vector2d ( firstBondVector ) ; logger . debug ( "First bond creation detected." ) ; final int numAtoms = molecule . getAtomCount ( ) ; final int numBonds = molecule . getBondCount ( ) ; this . firstBondVector = firstBondVector ; logger . debug ( "We have a molecules with " + numAtoms + " atoms." ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; seedLayout ( ) ; int iter = 0 ; code_block = ForStatement ; if ( iter == numAtoms && ! AtomPlacer . allPlaced ( molecule ) ) throw new CDKException ( "Could not generate layout? If a set of 'fixed' atoms were provided" + " try removing these and regenerating the layout." ) ; code_block = IfStatement ; refinePlacement ( molecule ) ; finalizeLayout ( molecule ) ; if ( ! isSubLayout ) assignStereochem ( molecule ) ; }
private void generateCoordinates ( Vector2d firstBondVector , boolean isConnected , boolean isSubLayout ) throws CDKException { logger . debug ( "Entering generateCoordinates()" ) ; if ( firstBondVector == DEFAULT_BOND_VECTOR ) firstBondVector = new Vector2d ( firstBondVector ) ; final int numAtoms = molecule . getAtomCount ( ) ; final int numBonds = molecule . getBondCount ( ) ; this . firstBondVector = firstBondVector ; logger . debug ( "Entry point of generateCoordinates()" ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; seedLayout ( ) ; int iter = 0 ; code_block = ForStatement ; if ( iter == numAtoms && ! AtomPlacer . allPlaced ( molecule ) ) throw new CDKException ( "Could not generate layout? If a set of 'fixed' atoms were provided" + " try removing these and regenerating the layout." ) ; code_block = IfStatement ; refinePlacement ( molecule ) ; finalizeLayout ( molecule ) ; if ( ! isSubLayout ) assignStereochem ( molecule ) ; }
public void test() { if ( logger . isTraceEnabled ( LogMarker . SERIALIZER_VERBOSE ) ) { logger . trace ( LogMarker . SERIALIZER_VERBOSE , "Serialize {}" , this ) ; } }
public void attachDirty ( MbStatus instance ) { log . debug ( "attaching dirty MbStatus instance" ) ; code_block = TryStatement ;  }
public void test() { try { getSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { getSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { listTB = getDirectConfigProxy ( ) . getTrustBundles ( fetchAnchors ) ; } catch ( Exception ex ) { LOG . error ( "Unable to get trust bundles" , ex ) ; } }
public void test() { try { return CryptoUtils . decrypt ( message , CLOUD_KEY ) ; } catch ( GeneralSecurityException e ) { logger . error ( "Error" , e ) ; return null ; } }
@ NamespacePermission ( fields = "emrClusterDefinitionKey?.namespace" , permissions = NamespacePermissionEnum . WRITE ) @ Override public EmrClusterDefinitionInformation deleteEmrClusterDefinition ( EmrClusterDefinitionKey emrClusterDefinitionKey ) throws Exception { logger . info ( "deleteEmrClusterDefinition {}" , emrClusterDefinitionKey ) ; emrClusterDefinitionHelper . validateEmrClusterDefinitionKey ( emrClusterDefinitionKey ) ; EmrClusterDefinitionEntity emrClusterDefinitionEntity = emrClusterDefinitionDaoHelper . getEmrClusterDefinitionEntity ( emrClusterDefinitionKey ) ; emrClusterDefinitionDao . delete ( emrClusterDefinitionEntity ) ; return createEmrClusterDefinitionFromEntity ( emrClusterDefinitionEntity ) ; }
public void test() { if ( logMessage != null && ! logMessage . isEmpty ( ) ) { log . debug ( "{} socket {}" , reset ? "Resetting" : "Closing" , socket ) ; } else { log . debug ( "{} socket {}" , reset ? "Resetting" : "Closing" , socket ) ; } }
public void test() { if ( logMessage != null && ! logMessage . isEmpty ( ) ) { log . info ( "{} - {} socket {}" , reset ? "Resetting" : "Closing" , logMessage , socket ) ; } else { log . info ( "{} - {} socket {} is empty" , reset ? "Resetting" : "Closing" , socket , socket ) ; } }
@ Test public void testCreateOrder ( ) throws Exception { Order order = new Order ( ) ; order . setAmount ( 1 ) ; order . setPartName ( "motor" ) ; order . setCustomerName ( "honda" ) ; String xml = context . getTypeConverter ( ) . convertTo ( String . class , order ) ; log . info ( "Sending order using xml payload: {}" , xml ) ; String id = template . requestBody ( "http://localhost:8080/orders" , xml , String . class ) ; assertNotNull ( id ) ; log . info ( "Created new order with id " + id ) ; assertEquals ( "3" , id ) ; }
@ Test public void testCreateOrder ( ) throws Exception { Order order = new Order ( ) ; order . setAmount ( 1 ) ; order . setPartName ( "motor" ) ; order . setCustomerName ( "honda" ) ; String xml = context . getTypeConverter ( ) . convertTo ( String . class , order ) ; log . info ( "Sending order using xml payload: {}" , xml ) ; String id = template . requestBody ( "http://localhost:8080/orders" , xml , String . class ) ; assertNotNull ( id ) ; log . info ( "Created new order with id " + id ) ; assertEquals ( "3" , id ) ; }
public void test() { try { Class . forName ( driverClassName ) ; } catch ( ClassNotFoundException e ) { log . debug ( "Class '{}' not found" , driverClassName ) ; } }
public void test() { try { String message ; code_block = IfStatement ; code_block = IfStatement ; Window window = SwingUtilities . getWindowAncestor ( this ) ; code_block = IfStatement ; } catch ( PrivateKeyProviderException e ) { _log . error ( e , e ) ; } }
public void test() { try { constructor = pageClass . getDeclaredConstructor ( new Class [ ] code_block = "" ; ) ; Constructor < C > tmpConstructor = ( Constructor < C > ) constructorForClass . putIfAbsent ( pageClass , constructor ) ; code_block = IfStatement ; log . debug ( "Found constructor for Page of type '{}' and argument of type '{}'." , pageClass , argumentType ) ; } catch ( NoSuchMethodException e ) { log . warn ( "Cannot find constructor for page class {}." , pageClass , e ) ; return null ; } }
public void handleGetLightSensorStatusResponse ( final LightSensorStatusDto lightSensorStatusDto , final CorrelationIds ids , final String messageType , final int messagePriority , final ResponseMessageResultType deviceResult , final OsgpException exception ) { LOGGER . info ( "handleGetLightSensorStatusResponse called with messageType: {}, messagePriority: {}" , messageType , messagePriority ) ; final GetLightSensorStatusResponse response = new GetLightSensorStatusResponse ( ) ; response . setOsgpException ( exception ) ; response . setResult ( deviceResult ) ; code_block = IfStatement ; code_block = IfStatement ; final ResponseMessage responseMessage = ResponseMessage . newResponseMessageBuilder ( ) . withIds ( ids ) . withResult ( response . getResult ( ) ) . withOsgpException ( response . getOsgpException ( ) ) . withDataObject ( response . getLightSensorStatus ( ) ) . withMessagePriority ( messagePriority ) . build ( ) ; code_block = IfStatement ; }
public void test() { if ( deviceResult == ResponseMessageResultType . NOT_OK || exception != null ) { LOGGER . error ( "Device Response not ok." , exception ) ; } }
public void test() { if ( ! OsgpSystemCorrelationUid . CORRELATION_UID . equals ( ids . getCorrelationUid ( ) ) ) { this . webServiceResponseMessageSender . send ( responseMessage ) ; } else { LOGGER . info ( "Unable to send Correlation response message" ) ; } }
public void test() { try { wireGraphService . delete ( ) ; } catch ( KuraException e ) { logger . error ( "Test error" , e ) ; throw e ; } }
public MbB2mDel merge ( MbB2mDel detachedInstance ) { log . debug ( "merging MbB2mDel instance" ) ; code_block = TryStatement ;  }
public void test() { try { MbB2mDel result = ( MbB2mDel ) sessionFactory . getCurrentSession ( ) . merge ( detachedInstance ) ; log . debug ( "merge successful" ) ; return result ; } catch ( RuntimeException re ) { log . error ( "merge failed" , re ) ; throw re ; } }
public void test() { try { MbB2mDel result = ( MbB2mDel ) sessionFactory . getCurrentSession ( ) . merge ( detachedInstance ) ; log . debug ( "merge successful" ) ; return result ; } catch ( RuntimeException re ) { log . error ( "merge failed" , re ) ; throw re ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( LayoutPageTemplateEntryServiceUtil . class , "addLayoutPageTemplateEntry" , _addLayoutPageTemplateEntryParameterTypes0 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , layoutPageTemplateCollectionId , classNameId , classTypeId , name , status , serviceContext ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . layout . page . template . model . LayoutPageTemplateEntry ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( calendar == null ) { LOGGER . warn ( "CalendarDAO is null" ) ; return Collections . emptySet ( ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Refreshing." ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Refreshing." ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try ( ClassicHttpResponse httpResponse = call . execute ( ) ) { code_block = IfStatement ; } catch ( final IOException ex ) { jobFailed ( cacheKey ) ; LOG . error ( "I/O error during asynchronous revalidation" , ex ) ; } catch ( final HttpException ex ) { jobFailed ( cacheKey ) ; LOG . error ( "HTTP protocol exception during asynchronous revalidation" , ex ) ; } catch ( final RuntimeException ex ) { jobFailed ( cacheKey ) ; LOG . error ( "Unexpected runtime exception thrown during asynchronous revalidation" , ex ) ; } }
public void test() { try ( ClassicHttpResponse httpResponse = call . execute ( ) ) { code_block = IfStatement ; } catch ( final IOException ex ) { jobFailed ( cacheKey ) ; LOG . debug ( "Asynchronous revalidation failed due to I/O error" , ex ) ; } catch ( final HttpException ex ) { jobFailed ( cacheKey ) ; LOG . error ( "Http exception thrown during asynchronous revalidation" , ex ) ; } catch ( final RuntimeException ex ) { jobFailed ( cacheKey ) ; LOG . error ( "Unexpected runtime exception thrown during asynchronous revalidation" , ex ) ; } }
public void test() { try ( ClassicHttpResponse httpResponse = call . execute ( ) ) { code_block = IfStatement ; } catch ( final IOException ex ) { jobFailed ( cacheKey ) ; LOG . debug ( "Asynchronous revalidation failed due to I/O error" , ex ) ; } catch ( final HttpException ex ) { jobFailed ( cacheKey ) ; LOG . error ( "HTTP protocol exception during asynchronous revalidation" , ex ) ; } catch ( final RuntimeException ex ) { jobFailed ( cacheKey ) ; LOG . error ( "Runtime error during asynchronous revalidation" , ex ) ; } }
public void test() { try { FutureTask < Void > future = new FutureTask < > ( run , null ) ; Platform . runLater ( future ) ; future . get ( ) ; } catch ( ExecutionException | InterruptedException e ) { LOGGER . error ( "Execution interrupted." , e ) ; } }
public void test() { { String targetPackageId = "target1" ; addTargetPackage ( targetPackageId ) ; ResourceIdentifier id = ResourceIdentifier . create ( ResourceType . PACKAGE , PACKAGE_A ) ; TestProgress progress = new TestProgress ( ) ; copyService . copy ( singletonList ( id ) , targetPackageId , progress ) ; await ( ) . atMost ( 5 , TimeUnit . SECONDS ) . until ( copyJobFinished ( progress ) ) ; waitForWorkToBeFinished ( indexService , LOG ) ; Package targetPackage = metadataService . getPackage ( targetPackageId ) . get ( ) ; List < Package > packages = newArrayList ( targetPackage . getChildren ( ) ) ; assertEquals ( 1 , packages . size ( ) ) ; Package packageACopy = packages . get ( 0 ) ; assertEquals ( "Package A" , packageACopy . getLabel ( ) ) ; List < EntityType > entityTypesInACopy = newArrayList ( packageACopy . getEntityTypes ( ) ) ; List < Package > packagesInACopy = newArrayList ( packageACopy . getChildren ( ) ) ; assertEquals ( 1 , entityTypesInACopy . size ( ) ) ; assertEquals ( 1 , packagesInACopy . size ( ) ) ; Package packageBCopy = packagesInACopy . get ( 0 ) ; assertEquals ( "Package B (child of A)" , packageBCopy . getLabel ( ) ) ; List < EntityType > entityTypesInBCopy = newArrayList ( packageBCopy . getEntityTypes ( ) ) ; List < Package > packagesInBCopy = newArrayList ( packageBCopy . getChildren ( ) ) ; assertEquals ( 1 , entityTypesInBCopy . size ( ) ) ; assertEquals ( 0 , packagesInBCopy . size ( ) ) ; EntityType entityTypeACopy = entityTypesInACopy . get ( 0 ) ; EntityType entityTypeBCopy = entityTypesInBCopy . get ( 0 ) ; assertEquals ( "EntityType A" , entityTypeACopy . getLabel ( ) ) ; assertEquals ( "EntityType B (referenced by A)" , entityTypeBCopy . getLabel ( ) ) ; assertEquals ( ENTITY_TYPE_B , entityTypeA . getAttribute ( "x" , entity
public void test() { try { return RefreshScopeConfigurationScaleTests . this . service . getMessage ( ) ; } finally { latch . countDown ( ) ; LOGGER . debug ( "x" ) ; } }
@ Override public void info ( String string , Object o ) { logger . info ( string , o ) ; }
public void test() { if ( pluginName == null ) { logger . warn ( "Plugin name is null!" ) ; return ; } }
public void setProcessVariable ( String containerId , Number processInstanceId , String varName , String variablePayload , String marshallingType ) { containerId = context . getContainerId ( containerId , new ByProcessInstanceIdContainerLocator ( processInstanceId . longValue ( ) ) ) ; logger . debug ( "About to unmarshal task '{}' from process instance with id '{}'" , varName , processInstanceId ) ; Object variable = marshallerHelper . unmarshal ( containerId , variablePayload , marshallingType , Object . class ) ; logger . debug ( "Setting variable '{}' on process instance with id {} with value {}" , varName , processInstanceId , variable ) ; processService . setProcessVariable ( containerId , processInstanceId . longValue ( ) , varName , variable ) ; }
public void setProcessVariable ( String containerId , Number processInstanceId , String varName , String variablePayload , String marshallingType ) { containerId = context . getContainerId ( containerId , new ByProcessInstanceIdContainerLocator ( processInstanceId . longValue ( ) ) ) ; logger . debug ( "About to unmarshal variable from payload: '{}'" , variablePayload ) ; Object variable = marshallerHelper . unmarshal ( containerId , variablePayload , marshallingType , Object . class ) ; logger . debug ( "Setting process variable '{}' with value '{}'" , varName , variable ) ; processService . setProcessVariable ( containerId , processInstanceId . longValue ( ) , varName , variable ) ; }
@ Test public void testDispatchingJobsHigherMaxLoad ( ) throws Exception { LOG . info ( "Starting testDispatchingJobsHigherMaxLoad" ) ; Job testJob = serviceRegistryJpaImpl . createJob ( TEST_HOST , TEST_SERVICE_FAIRNESS , TEST_OPERATION , null , null , true , null , 10.0f ) ; JobBarrier barrier = new JobBarrier ( null , serviceRegistryJpaImpl , testJob ) ; launchDispatcherOnce ( false ) ; assertThrows ( IllegalStateException . class , ( ) -> barrier . waitForJobs ( JOB_BARRIER_TIMEOUT ) ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { { LOGGER . info ( "Shutting down log manager" ) ; LogManager . shutdown ( ) ; } }
public void test() { try { commands . add ( CommandFactory . create ( initialContext , GtfsDisposeImportCommand . class . getName ( ) ) ) ; } catch ( Exception e ) { log . error ( e , e ) ; throw new RuntimeException ( "unable to call factories" ) ; } }
protected String createWorkDirectory ( String location ) { code_block = IfStatement ; File f = new File ( location ) ; f . mkdirs ( ) ; _log . info ( "Created " + location ) ; return location ; }
@ EventListener public void onApplicationEvent ( ApplicationReadyEvent aEvt ) { log . info ( "Application ready" ) ; log . info ( "Headless: " + ( GraphicsEnvironment . isHeadless ( ) ? "yes" : "no" ) ) ; code_block = IfStatement ; }
public void test() { try { return ResponseUtils . buildSucessResponse ( userRolesService . updateUserRole ( roleDetailsRequest , user . getName ( ) ) ) ; } catch ( Exception exception ) { log . error ( UNEXPECTED_ERROR_OCCURRED , exception ) ; return ResponseUtils . buildFailureResponse ( new Exception ( UNEXPECTED_ERROR_OCCURRED ) , exception . getMessage ( ) ) ; } }
public void test() { if ( device . getConnectionState ( ) != ConnectionState . CONNECTING && ! device . connect ( ) ) { logger . debug ( "Cannot connect!" ) ; return null ; } }
public void test() { if ( ! device . awaitConnection ( 1 , TimeUnit . SECONDS ) ) { logger . debug ( "Device timed out waiting for 1 sec" ) ; return null ; } }
public void test() { if ( ! device . awaitServiceDiscovery ( 10 , TimeUnit . SECONDS ) ) { logger . debug ( "Disconnected from 10 seconds" ) ; return null ; } }
public void test() { try { DiscoveryResult result = participant . createResult ( device ) ; code_block = IfStatement ; } catch ( RuntimeException e ) { logger . error ( "Error creating a result for device: {}" , device , e ) ; } }
public void test() { try { final File dataDir = new File ( nodeInternDir , CFG_DATA_DIR_NAME ) ; loadPieInternals ( dataDir , exec ) ; } catch ( final CanceledExecutionException e ) { throw e ; } catch ( final Exception e ) { LOGGER . warn ( "Error loadingPieInternals" , e ) ; } }
private List < DownloadFile > downloadEnsemblData ( Path geneFolder ) throws IOException , InterruptedException { logger . info ( "Downloading genome information ..." ) ; List < String > downloadedUrls = new ArrayList < > ( 4 ) ; List < DownloadFile > downloadFiles = new ArrayList < > ( ) ; String ensemblHost = ensemblHostUrl + "/" + ensemblRelease ; code_block = IfStatement ; String ensemblCollection = "" ; code_block = IfStatement ; String version = ensemblRelease . split ( "-" ) [ 1 ] ; String url = ensemblHost + "/gtf/" + ensemblCollection + speciesShortName + "/*" + version + ".gtf.gz" ; String fileName = geneFolder . resolve ( speciesShortName + ".gtf.gz" ) . toString ( ) ; downloadFiles . add ( downloadFile ( url , fileName ) ) ; downloadedUrls . add ( url ) ; url = ensemblHost + "/fasta/" + ensemblCollection + speciesShortName + "/pep/*.pep.all.fa.gz" ; fileName = geneFolder . resolve ( speciesShortName + ".pep.all.fa.gz" ) . toString ( ) ; downloadFiles . add ( downloadFile ( url , fileName ) ) ; downloadedUrls . add ( url ) ; url = ensemblHost + "/fasta/" + ensemblCollection + speciesShortName + "/cdna/*.cdna.all.fa.gz" ; fileName = geneFolder . resolve ( speciesShortName + ".cdna.all.fa.gz" ) . toString ( ) ; downloadFiles . add ( downloadFile ( url , fileName ) ) ; downloadedUrls . add ( url ) ; saveVersionData ( EtlCommons . GENE_DATA , ENSEMBL_NAME , ensemblVersion , getTimeStamp ( ) , downloadedUrls , geneFolder . resolve ( "ensemblCoreVersion.json" ) ) ; return downloadFiles ; }
public void test() { if ( null == scannerFactory ) { log . debug ( "Closing ShardQueryLogic scannerFactory: " + System . identityHashCode ( this ) ) ; } else { log . debug ( "Closing ShardQueryLogic scannerFactory: " + System . identityHashCode ( this ) ) ; code_block = TryStatement ;  } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { int nClosed = 0 ; scannerFactory . lockdown ( ) ; code_block = ForStatement ; code_block = IfStatement ; nClosed = 0 ; code_block = ForStatement ; code_block = IfStatement ; } catch ( Exception e ) { LOG . error ( e ) ; } }
public void test() { try { log . debug ( "Closing ShardQueryLogic planner: " + System . identityHashCode ( this ) + '(' + ( this . getSettings ( ) == null ? "empty" : this . getSettings ( ) . getId ( ) ) + ')' ) ; this . planner . close ( getConfig ( ) , this . getSettings ( ) ) ; } catch ( Exception e ) { log . error ( "Error while closing ShardQueryLogic planner: " + System . identityHashCode ( this ) + '(' + ( this . getSettings ( ) == null ? "empty" : this . getSettings ( ) . getId ( ) ) + ']' , e ) ; } }
public void test() { try { log . debug ( "Closing ShardQueryLogic queries: " + System . identityHashCode ( this ) ) ; this . queries . close ( ) ; } catch ( IOException e ) { log . error ( "Error closing ShardQueryLogic queries: " + System . identityHashCode ( this ) , e ) ; } }
public void test() { try { log . debug ( "Closing ShardQueryLogic scheduler: " + System . identityHashCode ( this ) ) ; this . scheduler . close ( ) ; ScanSessionStats stats = this . scheduler . getSchedulerStats ( ) ; code_block = IfStatement ; } catch ( IOException e ) { log . error ( "Error closing ShardQueryLogic scheduler" , e ) ; } }
public org . talend . mdm . webservice . WSTransformerPK putTransformer ( org . talend . mdm . webservice . WSPutTransformer arg0 ) { LOG . info ( "Executing operation putTransformer" ) ; System . out . println ( arg0 ) ; code_block = TryStatement ;  }
public void test() { if ( this . getLogger ( ) . isDebugEnabled ( ) ) { this . getLogger ( ) . debug ( "Extension [" + extension + "] is not valid" ) ; } }
public void test() { if ( this . getLogger ( ) . isDebugEnabled ( ) ) { this . getLogger ( ) . debug ( "Extension [" + extension + "] is not valid" ) ; } }
public void test() { try { code_block = IfStatement ; code_block = ForStatement ; authorizeWithHiveBindings ( context , stmtAuthObject , stmtOperation ) ; } catch ( AuthorizationException e ) { executeOnFailureHooks ( context , stmtOperation , e ) ; StringBuilder permsBuilder = new StringBuilder ( ) ; code_block = ForStatement ; String permsRequired = permsBuilder . toString ( ) ; SessionState . get ( ) . getConf ( ) . set ( HiveAuthzConf . HIVE_SENTRY_AUTH_ERRORS , permsRequired ) ; String msgForLog = HiveAuthzConf . HIVE_SENTRY_PRIVILEGE_ERROR_MESSAGE + "\n Required privileges for this query: " + permsRequired ; String msgForConsole = HiveAuthzConf . HIVE_SENTRY_PRIVILEGE_ERROR_MESSAGE + "\n " + e . getMessage ( ) + "\n The required privileges: " + permsRequired ; log . error ( msgForConsole , e ) ; throw new SemanticException ( msgForConsole , e ) ; } finally { hiveAuthzBinding . close ( ) ; } }
public void test() { try { task . run ( ) ; } catch ( Exception e ) { logger . warn ( "Task [{}] failed" , task . getName ( ) , e ) ; } }
public void test() { try { code_block = IfStatement ; rocketMQConsumer . subscribe ( this . topic , this . filter ) ; rocketMQConsumer . registerMessageListener ( ( MessageListenerOrderly ) ( messageExts , context ) code_block = LoopStatement ; ) ; rocketMQConsumer . start ( ) ; } catch ( MQClientException ex ) { log . error ( ex . getMessage ( ) ) ; } }
@ ApiOperation ( value = "Get access contract by ID" ) @ GetMapping ( path = RestApi . PATH_REFERENTIAL_ID ) public AccessContractDto getById ( final @ PathVariable ( "identifier" ) String identifier ) throws UnsupportedEncodingException { LOGGER . debug ( "getById {}" , identifier ) ; return service . getOne ( buildUiHttpContext ( ) , URLEncoder . encode ( identifier , StandardCharsets . UTF_8 . toString ( ) ) ) ; }
public void test() { try { String clazzName = ( String ) ( args . get ( 0 ) ) ; String methodName = ( String ) ( args . get ( 1 ) ) ; Class [ ] argTypes = new Class [ args . size ( ) - 2 ] ; Object [ ] params = new Object [ args . size ( ) - 2 ] ; code_block = ForStatement ; Class clazz = ClassUtils . getClass ( clazzName ) ; Method method = MethodUtils . getMatchingAccessibleMethod ( clazz , methodName , argTypes ) ; LOGGER . debug ( "Executing method {}" , method ) ; return method . invoke ( null , params ) ; } catch ( Exception ex ) { LOGGER . warn ( "Exception in call to external java method" , ex ) ; return ex . getMessage ( ) ; } }
public void test() { try { String clazzName = ( String ) ( args . get ( 0 ) ) ; String methodName = ( String ) ( args . get ( 1 ) ) ; LOGGER . debug ( "XEditor extension function calling {} {}" , clazzName , methodName ) ; Class [ ] argTypes = new Class [ args . size ( ) - 2 ] ; Object [ ] params = new Object [ args . size ( ) - 2 ] ; code_block = ForStatement ; Class clazz = ClassUtils . getClass ( clazzName ) ; Method method = MethodUtils . getMatchingAccessibleMethod ( clazz , methodName , argTypes ) ; return method . invoke ( null , params ) ; } catch ( Exception ex ) { LOGGER . error ( ex . getMessage ( ) , ex ) ; return ex . getMessage ( ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { try { String result = pipeParser . encode ( message ) ; result = result . replace ( "\r" , "\r\n" ) ; result = result . replace ( "\r\r" , "\r" ) ; logger . debug ( "Parsed pipe {}" , result ) ; } catch ( HL7Exception e ) { logger . error ( "Unexpected error." , e ) ; } }
public void test() { try { String result = pipeParser . encode ( message ) ; result = result . replace ( "\r" , "\r\n" ) ; result = result . replace ( "\r\r" , "\r" ) ; logger . error ( result ) ; } catch ( HL7Exception e ) { logger . error ( e . getMessage ( ) ) ; } }
public void removeTableFromGroup ( TableName groupName , String schemaName , String tableName ) { LOG . debug ( "Removing table {}" , tableName ) ; Group group = ais . getGroup ( groupName ) ; checkFound ( group , "removing join from group" , "group" , groupName ) ; Table table = ais . getTable ( schemaName , tableName ) ; checkFound ( table , "removing join from group" , "table table" , concat ( schemaName , tableName ) ) ; checkInGroup ( group , table , "removing join from group" , "table table" ) ; code_block = IfStatement ; setTablesGroup ( table , null ) ; }
public void test() { if ( ! Boolean . TRUE . equals ( operation ( ) . inNamespace ( namespace ) . withName ( resourceName . toString ( ) ) . withPropagationPolicy ( DeletionPropagation . FOREGROUND ) . delete ( ) ) ) { log . warn ( "resource {} is not deleted" , resourceName ) ; future . complete ( ) ; } else { Util . waitFor ( vertx , "sync resource deletion " + resourceName , "deleted" , 1000 , Long . MAX_VALUE , ( ) code_block = LoopStatement ; ) . onComplete ( future ) ; } }
public void test() { -> { KafkaTopic kafkaTopic = operation ( ) . inNamespace ( namespace ) . withName ( resourceName . toString ( ) ) . get ( ) ; boolean notExists = kafkaTopic == null ; log . info ( "Topic {} doesn't exist" , resourceName ) ; return notExists ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( AssetListEntryServiceUtil . class , "getAssetListEntriesCount" , _getAssetListEntriesCountParameterTypes20 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupIds ) ; Object returnObj = null ; code_block = TryStatement ;  return ( ( Integer ) returnObj ) . intValue ( ) ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( request . getType ( ) . equals ( Type . get ) ) { LOGGER . trace ( "Using search processor: SearchGet" ) ; this . searchGet . process ( request ) ; return ; } else-if ( request . getType ( ) . equals ( Type . set ) ) { LOGGER . trace ( "Using search processor: SearchSet" ) ; this . searchSet . process ( request ) ; return ; } }
public void test() { if ( request . getType ( ) . equals ( Type . get ) ) { LOGGER . trace ( "Using search processor: SearchGet" ) ; this . searchGet . process ( request ) ; return ; } else-if ( request . getType ( ) . equals ( Type . set ) ) { LOGGER . trace ( "Using search processor: SearchSet" ) ; this . searchSet . process ( request ) ; return ; } }
public void test() { try { ret = pagePlugin . render ( this , ret ) ; } catch ( Exception e ) { log . error ( "could be rendered" , e ) ; } }
@ Override public void requestInitialized ( ServletRequestEvent servletRequestEvent ) { Request request = ( ( Request ) servletRequestEvent . getServletRequest ( ) ) ; log . info ( "Initialized request: " + request . toString ( ) ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { fetchedList = criteria . list ( ) ; return fetchedList ; } catch ( Exception e ) { logger . error ( "getByCriteriaWithAliasByOrder failed, criteria = " + criterion . toString ( ) , e ) ; throw new HibernateException ( "getByCriteriaWithAliasByOrder failed, criteria = " + criterion . toString ( ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public List < CosemObject > filterMeterValues ( List < CosemObject > cosemObjects ) { List < CosemObject > filteredValues = cosemObjects . stream ( ) . filter ( cosemObject -> supportedIdentifiers . contains ( cosemObject . getObisIdentifier ( ) . getReducedOBISIdentifier ( ) ) ) . collect ( Collectors . toList ( ) ) ; LOGGER . info ( "Filtering {} values for {}" , filteredValues . size ( ) , cosemObject . getObisIdentifier ( ) ) ; return filteredValues ; }
public void test() { try { final XsLocalNetwork nw = citrixResourceBase . getNetworkByName ( conn , label ) ; code_block = IfStatement ; final PIF pif = nw . getPif ( conn ) ; final PIF . Record pifRec = pif . getRecord ( conn ) ; s_logger . debug ( "PIF object:" + pifRec . uuid + "(" + pifRec . device + ")" ) ; return new OvsFetchInterfaceAnswer ( command , true , "Interface " + pifRec . device + " retrieved successfully" , pifRec . IP , pifRec . netmask , pifRec . MAC ) ; } catch ( final BadServerResponse e ) { s_logger . error ( "An error occurred while fetching the interface to " + label + " on host " + citrixResourceBase . getHost ( ) . getIp ( ) , e ) ; return new OvsFetchInterfaceAnswer ( command , false , "EXCEPTION:" + e . getMessage ( ) ) ; } catch ( final XenAPIException e ) { s_logger . error ( "An error occurred while fetching the interface to " + label + " on host " + citrixResourceBase . getHost ( ) . getIp ( ) , e ) ; return new OvsFetchInterfaceAnswer ( command , false , "EXCEPTION:" + e . getMessage ( ) ) ; } catch ( final XmlRpcException e ) { s_logger . error ( "An error occurred while fetching the interface to " + label + " on host " + citrixResourceBase . getHost ( ) . getIp ( ) , e ) ; return new OvsFetchInterfaceAnswer ( command , false , "EXCEPTION:" + e . getMessage ( ) ) ; } }
public void test() { try { final XsLocalNetwork nw = citrixResourceBase . getNetworkByName ( conn , label ) ; code_block = IfStatement ; s_logger . debug ( "Network object:" + nw . getNetwork ( ) . getUuid ( conn ) ) ; final PIF pif = nw . getPif ( conn ) ; final PIF . Record pifRec = pif . getRecord ( conn ) ; s_logger . debug ( "Interface " + pifRec . device + " retrieved successfully" ) ; return new OvsFetchInterfaceAnswer ( command , true , "Interface " + pifRec . device + " retrieved successfully" , pifRec . IP , pifRec . netmask , pifRec . MAC ) ; } catch ( final BadServerResponse e ) { s_logger . error ( "An error occurred while fetching the interface to " + label + " on host " + citrixResourceBase . getHost ( ) . getIp ( ) , e ) ; return new OvsFetchInterfaceAnswer ( command , false , "EXCEPTION:" + e . getMessage ( ) ) ; } catch ( final XenAPIException e ) { s_logger . error ( "An error occurred while fetching the interface to " + label + " on host " + citrixResourceBase . getHost ( ) . getIp ( ) , e ) ; return new OvsFetchInterfaceAnswer ( command , false , "EXCEPTION:" + e . getMessage ( ) ) ; } catch ( final XmlRpcException e ) { s_logger . error ( "An error occurred while fetching the interface to " + label + " on host " + citrixResourceBase . getHost ( ) . getIp ( ) , e ) ; return new OvsFetchInterfaceAnswer ( command , false , "EXCEPTION:" + e . getMessage ( ) ) ; } }
@ BeforeClass public static void setup ( ) { logger . info ( "========================================================" ) ; logger . info ( "========================================================" ) ; logger . info ( "========================================================" ) ; }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { if ( e instanceof org . apache . accumulo . core . clientImpl . thrift . ThriftSecurityException ) { result . sec = ( org . apache . accumulo . core . clientImpl . thrift . ThriftSecurityException ) e ; result . setSecIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof org . apache . accumulo . core . clientImpl . thrift . ThriftSecurityException ) { result . sec = ( org . apache . accumulo . core . clientImpl . thrift . ThriftSecurityException ) e ; result . setSecIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof org . apache . accumulo . core . clientImpl . thrift . ThriftSecurityException ) { result . sec = ( org . apache . accumulo . core . clientImpl . thrift . ThriftSecurityException ) e ; result . setSecIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { try { fcall . sendResponse ( fb , msg , msgType , seqid ) ; } catch ( java . lang . Exception ex ) { _LOGGER . error ( "Exception writing to internal frame buffer" , ex ) ; fb . close ( ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
@ Override public void doPostRollback ( IndexWriter writer ) throws IOException { Path path = directory . getPath ( ) ; String name = path . getName ( ) ; LOG . debug ( "Deleting Index Writer {}." , path ) ; fileSystem . rename ( path , new Path ( path . getParent ( ) , rename ( name , BADROWIDS ) ) ) ; }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getCapabilities()" ) ; } }
public void test() { try { pingPonger . sendPongMessage ( myChannel , jgAddress , jgmsg . getSrc ( ) ) ; } catch ( Exception e ) { logger . warn ( "sendPongMessage error: {}" , e . getMessage ( ) ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getCapabilities()" ) ; } }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "JDBC::{}" , sql ) ; } }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "JDBC::{}" , sql ) ; } }
public void test() { if ( info . getStatus ( ) == ProtocolStatus . OK ) { invLogger . info ( "response({}) -> bindID ={}, status ={}." , requestID , bindID , info . getReturnData ( ) ) ; local . sendData ( info . getReturnData ( ) ) ; return rsfFuture . completed ( local ) ; } else { invLogger . error ( "response({}) -> statusFailed, bindID ={}, status ={}." , requestID , bindID , local . getStatus ( ) ) ; return rsfFuture . failed ( new RsfException ( local . getStatus ( ) , "status." ) ) ; } }
public void test() { if ( rsfRequest . isMessage ( ) ) { Class < ? > returnType = rsfRequest . getMethod ( ) . getReturnType ( ) ; RsfResultDO returnObject = null ; code_block = IfStatement ; code_block = IfStatement ; String errorInfo = "errorCode = " + returnObject . getErrorCode ( ) + ", errorMessage=" + returnObject . getErrorMessage ( ) ; logger . error ( errorInfo ) ; return rsfFuture . failed ( new RsfException ( local . getStatus ( ) , errorInfo ) ) ; } }
public void test() { try { result = connector . schema ( ) . getObjectClassInfo ( ) ; } catch ( Exception e ) { LOG . error ( "Failed to get objectClassInfo for {}" , name , e ) ; } }
public void test() { try { process = pb . start ( ) ; pid = Platform . getPID ( process ) ; addToGlobalGC ( process , pid ) ; return process ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; throw new ExecutorException ( e . getMessage ( ) , e ) ; } finally { alreadyPerformed = true ; } }
private void parseClientNonce ( ClientEsniInner clientEsniInne ) { byte [ ] clientNonce = parseByteArrayField ( ExtensionByteLength . NONCE ) ; clientEsniInne . setClientNonce ( clientNonce ) ; LOGGER . debug ( "ClientNonce: " + ArrayConverter . bytesToHexString ( clientNonce ) ) ; }
public void test() { try { return SAML2ComponentBuilder . createSubjectConfirmationData ( confirDataBean , keyInforBean ) ; } catch ( SecurityException | WSSecurityException e ) { LOG . error ( e . getLocalizedMessage ( ) , e ) ; throw new SAMLComponentBuilderException ( e . getLocalizedMessage ( ) , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { boolean success = taggerService . deleteAttribute ( id ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Error while deleting attribute " + id , e ) ; return getUIWrapper ( false , e . getMessage ( ) ) ; } }
public void test() { if ( ! cluster . isConnected ( ) || ! cluster . isHealthy ( ) ) { logger . warn ( "Cluster health check not available, skipping health check" ) ; return ; } }
protected RestContainer assertParseRestAsJaxb ( String uri ) throws JAXBException { Object value = parseUri ( uri ) ; RestContainer context = assertIsInstanceOf ( RestContainer . class , value ) ; LOGGER . debug ( "parseRestAsJson: {}" , value ) ; return context ; }
public void test() { if ( ! StringUtils . isNullOrEmpty ( path ) ) { LOG . info ( "HOME: {}" , path ) ; } else { path = System . getProperty ( "DATACLEANER_HOME" ) ; code_block = IfStatement ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { throw new IOException ( "Failed to get number of rows and total size from HiveTable" , e ) ; } finally { LOG . info ( "Done processing" ) ; } }
public void test() { try { process . isAliveOrThrow ( ) ; instructionHandler = clientSource . take ( workerId , Duration . ofSeconds ( 5 ) ) ; } catch ( TimeoutException timeoutEx ) { log . warning ( timeoutEx . getMessage ( ) ) ; } catch ( InterruptedException interruptEx ) { Thread . currentThread ( ) . interrupt ( ) ; throw new RuntimeException ( interruptEx ) ; } }
public void test() { try { Field configListenersField = this . getClass ( ) . getSuperclass ( ) . getDeclaredField ( "configListeners" ) ; configListenersField . setAccessible ( true ) ; List < IConfigurationListener > listeners = ( List < IConfigurationListener > ) configListenersField . get ( this ) ; invokeListeners ( tr , tm , listeners ) ; } catch ( NoSuchFieldException | IllegalAccessException e ) { log . error ( "Unable to invoke listener listeners" , e ) ; } }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { ifaceManager . deleteBridgeDomainFromInterface ( rEp ) . get ( ) ; LOG . debug ( "bridge-domain was deleted from interface for endpoint {}" , rEp ) ; } catch ( InterruptedException | ExecutionException e ) { LOG . warn ( "failed to delete bridge domain {}" , rEp , e ) ; } }
public void test() { if ( ! Strings . isNullOrEmpty ( rEpLoc . getExternalNode ( ) ) ) { code_block = TryStatement ;  } else { LOG . info ( "ExternalNode is null" ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( storeService . exists ( siteContext . getContext ( ) , scriptUrl ) ) { code_block = IfStatement ; return scriptFactory . getScript ( scriptUrl ) ; } else-if ( logger . isDebugEnabled ( ) ) { logger . debug ( "No " + scriptUrl + " found under " + siteContext . getContext ( ) ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( CrafterException e ) { log . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( isValidHttpHeaderName ( headerName ) ) { response . addHeader ( ACCESS_CONTROL_ALLOW_HEADERS , headerName ) ; } else { log . warn ( "Invalid HTTP header name {}" , headerName ) ; } }
public void test() { try { clazz = findClass ( version ) ; } catch ( ClassNotFoundException e ) { LOG . warn ( e . getMessage ( ) ) ; } }
public PinData [ ] publishPinArray ( int [ ] data ) { logger . info ( "publishPinArray called" ) ; int pinDataCnt = data . length / 3 ; PinData [ ] pinArray = new PinData [ pinDataCnt ] ; code_block = ForStatement ; HashMap < String , PinData > pinDataMap = new HashMap < String , PinData > ( ) ; code_block = ForStatement ; code_block = ForStatement ; return pinArray ; }
public void test() { if ( pinDef == null ) { LOG . warn ( "Cannot find PinDef for {}" , pinDef . getName ( ) ) ; continue ; } }
public void disconnect ( ) { log . debug ( "Disconnecting" ) ; JmsUtils . closeQuietly ( connection ) ; }
public void test() { if ( studyId != null ) { code_block = IfStatement ; code_block = ForStatement ; } else { LOGGER . warn ( "Study has no study ID" ) ; } }
public void test() { try { heliumVisualizationFactory . bundle ( helium . getVisualizationPackagesToBundle ( ) ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( subProcess == null ) { logger . warn ( "Failed to find subprocess with ID {}" , pid ) ; return ; } }
public void stop ( ) { code_block = IfStatement ; subProcess . destroy ( ) ; subProcess = null ; afterStop ( ) ; LOG . info ( "Pipeline stopped" ) ; }
public void test() { try { final String url = COMMITTE_PROPOSAL . replace ( ID_KEY , UrlHelper . urlEncode ( id , StandardCharsets . UTF_8 . toString ( ) ) ) ; LOGGER . debug ( "Fetching committee proposal component data request {}" , url ) ; return ( ( JAXBElement < CommitteeProposalComponentData > ) xmlAgent . unmarshallXml ( riksdagenCommitteeProposalMarshaller , url , HTTP_UTSKOTTSFORSLAG_RIKSDAGEN_EXTERNAL_MODEL_CIA_HACK23_COM_IMPL , null , null ) ) . getValue ( ) ; } catch ( final XmlAgentException e ) { throw new DataFailureException ( e ) ; } }
public void test() { try { int returnValue = CommerceOrderItemServiceUtil . getCommerceOrderItemsCount ( groupId , commerceAccountId , orderStatuses ) ; return returnValue ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
private static void logHadoopVersionInfo ( ) { getLogger ( ) . info ( "SYSINFO Hadoop version: " + VersionInfo . getVersion ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop revision: " + VersionInfo . getRevision ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop version: " + VersionInfo . getVersion ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop user: " + VersionInfo . getUser ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop url: " + VersionInfo . getUrl ( ) ) ; }
private static void logHadoopVersionInfo ( ) { getLogger ( ) . info ( "SYSINFO Hadoop version: " + VersionInfo . getVersion ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop version: " + VersionInfo . getVersion ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop version: " + VersionInfo . getVersion ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop user: " + VersionInfo . getUser ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop url: " + VersionInfo . getUrl ( ) ) ; }
private static void logHadoopVersionInfo ( ) { getLogger ( ) . info ( "SYSINFO Hadoop version: " + VersionInfo . getVersion ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop version: " + VersionInfo . getVersion ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop version: " + VersionInfo . getVersion ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop user: " + VersionInfo . getUser ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop url: " + VersionInfo . getUrl ( ) ) ; }
private static void logHadoopVersionInfo ( ) { getLogger ( ) . info ( "SYSINFO Hadoop version: " + VersionInfo . getVersion ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop version: " + VersionInfo . getVersion ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop version: " + VersionInfo . getVersion ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop version: " + VersionInfo . getUrl ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop version: " + VersionInfo . getUrl ( ) ) ; }
private static void logHadoopVersionInfo ( ) { getLogger ( ) . info ( "SYSINFO Hadoop version: " + VersionInfo . getVersion ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop version: " + VersionInfo . getVersion ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop version: " + VersionInfo . getVersion ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop version: " + VersionInfo . getVersion ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop user: " + VersionInfo . getUser ( ) ) ; }
public void addAttributeListener ( ZclAttributeListener listener ) { log . debug ( "addAttributeListener {}" , listener ) ; attributeListeners . add ( listener ) ; }
public void test() { try { tryInstallingFailedArtifacts ( ) ; } catch ( Exception e ) { LOGGER . debug ( "error when trying to instal artifacts" , e ) ; } }
public void test() { try { LogoutRequest logoutRequest = createSignedLogoutRequest ( participant , soapLogoutEndpoint ) ; IClientConfiguration soapClientConfig = createSoapClientConfig ( participant ) ; SAMLLogoutClient client = new SAMLLogoutClient ( soapLogoutEndpoint . getUrl ( ) , soapClientConfig ) ; log . info ( "Logout request: {}" , logoutRequest ) ; LogoutResponseDocument resp = client . logout ( logoutRequest . getXMLBeanDoc ( ) ) ; updateContextAfterParicipantLogout ( ctx , participant , resp ) ; } catch ( Exception e ) { log . warn ( "Logging out the participant " + participant + " via SOAP failed" , e ) ; ctx . getFailed ( ) . add ( participant ) ; } }
public void test() { try { log . info ( "Logging out participant via SOAP: " + participant ) ; LogoutRequest logoutRequest = createSignedLogoutRequest ( participant , soapLogoutEndpoint ) ; IClientConfiguration soapClientConfig = createSoapClientConfig ( participant ) ; SAMLLogoutClient client = new SAMLLogoutClient ( soapLogoutEndpoint . getUrl ( ) , soapClientConfig ) ; LogoutResponseDocument resp = client . logout ( logoutRequest . getXMLBeanDoc ( ) ) ; updateContextAfterParicipantLogout ( ctx , participant , resp ) ; } catch ( Exception e ) { log . error ( "Error occurred while logging out participant: " + participant , e ) ; ctx . getFailed ( ) . add ( participant ) ; } }
@ Override public void bridgeStatusChanged ( ThingStatusInfo bridgeStatusInfo ) { logger . debug ( "bridgeStatusChanged {}" , bridgeStatusInfo ) ; initializeBridge ( ( getBridge ( ) == null ) ? null : getBridge ( ) . getHandler ( ) , bridgeStatusInfo . getStatus ( ) ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( foundField != null ) { final Class < ? > type = foundField . getType ( ) ; final Object value = read ( type , el , key , attrValue ) ; setField ( foundField , obj , value , el , key , attrValue ) ; code_block = IfStatement ; } else { LOG . warn ( "Field not found: " + attrValue ) ; } }
public void test() { if ( ! col . isPresent ( ) ) { LOG . error ( "Column {} does not exist" , columnName ) ; return null ; } }
public void test() { for ( DataFile inFile : inFiles ) { count ++ ; final DataFile outFile = entries . get ( inFile ) ; final Path f = new Path ( tmpInputDir , "distcp-" + nf . format ( count ) + ".cp" ) ; LOGGER . info ( "Distcp file " + f ) ; BufferedWriter bw = new BufferedWriter ( new OutputStreamWriter ( fs . create ( f ) , CHARSET ) ) ; bw . write ( inFile . getSource ( ) + "\t" + outFile . getSource ( ) + "\n" ) ; bw . close ( ) ; } }
@ Test ( groups = "Integration" ) public void testChangeModeFailureStopsTasksButHappyUponResumption ( ) throws Exception { DynamicCluster origServerPool = origApp . createAndManageChild ( EntitySpec . create ( DynamicCluster . class ) . configure ( DynamicCluster . MEMBER_SPEC , EntitySpec . create ( TomcatServer . class ) . configure ( "war" , getTestWar ( ) ) ) . configure ( "initialSize" , 1 ) ) ; NginxController origNginx = origApp . createAndManageChild ( EntitySpec . create ( NginxController . class ) . configure ( "serverPool" , origServerPool ) . configure ( "domain" , "localhost" ) ) ; origApp . start ( ImmutableList . of ( loc ) ) ; Assert . assertTrue ( RecordingSshjTool . connectionCount . get ( ) > 0 ) ; Collection < Feed > origFeeds = ( ( EntityInternal ) origNginx ) . feeds ( ) . getFeeds ( ) ; LOG . info ( "feeds before disabling HA" ) ; Assert . assertTrue ( origFeeds . size ( ) >= 1 ) ; origManagementContext . getRebindManager ( ) . forcePersistNow ( ) ; List < Task < ? > > tasksBefore = ( ( BasicExecutionManager ) origManagementContext . getExecutionManager ( ) ) . getAllTasks ( ) ; LOG . info ( "tasks before disabling HA, " + tasksBefore . size ( ) + ": " + tasksBefore ) ; Assert . assertFalse ( tasksBefore . isEmpty ( ) ) ; origManagementContext . getHighAvailabilityManager ( ) . changeMode ( HighAvailabilityMode . DISABLED ) ; origApp = null ; Repeater . create ( ) . every ( Duration . millis ( 20 ) ) . backoffTo ( Duration . ONE_SECOND ) . limitTimeTo ( Duration . THIRTY_SECONDS ) . until ( new Callable < Boolean > ( ) code_block = "" ; ) . runRequiringTrue ( ) ; RecordingSshjTool . forbidden . set ( true ) ; newManagementContext = createNewManagementContext ( ) ; newApp = ( TestApplication ) RebindTestUtils . rebind ( ( LocalManagementContext ) newManagementContext , classLoader ) ; NginxController newNginx = Iterables
@ Test ( groups = "Integration" ) public void testChangeModeFailureStopsTasksButHappyUponResumption ( ) throws Exception { DynamicCluster origServerPool = origApp . createAndManageChild ( EntitySpec . create ( DynamicCluster . class ) . configure ( DynamicCluster . MEMBER_SPEC , EntitySpec . create ( TomcatServer . class ) . configure ( "war" , getTestWar ( ) ) ) . configure ( "initialSize" , 1 ) ) ; NginxController origNginx = origApp . createAndManageChild ( EntitySpec . create ( NginxController . class ) . configure ( "serverPool" , origServerPool ) . configure ( "domain" , "localhost" ) ) ; origApp . start ( ImmutableList . of ( loc ) ) ; Assert . assertTrue ( RecordingSshjTool . connectionCount . get ( ) > 0 ) ; Collection < Feed > origFeeds = ( ( EntityInternal ) origNginx ) . feeds ( ) . getFeeds ( ) ; LOG . info ( "feeds before rebind are: " + origFeeds ) ; Assert . assertTrue ( origFeeds . size ( ) >= 1 ) ; origManagementContext . getRebindManager ( ) . forcePersistNow ( ) ; List < Task < ? > > tasksBefore = ( ( BasicExecutionManager ) origManagementContext . getExecutionManager ( ) ) . getAllTasks ( ) ; LOG . info ( "tasks before starting: " + tasksBefore ) ; Assert . assertFalse ( tasksBefore . isEmpty ( ) ) ; origManagementContext . getHighAvailabilityManager ( ) . changeMode ( HighAvailabilityMode . DISABLED ) ; origApp = null ; Repeater . create ( ) . every ( Duration . millis ( 20 ) ) . backoffTo ( Duration . ONE_SECOND ) . limitTimeTo ( Duration . THIRTY_SECONDS ) . until ( new Callable < Boolean > ( ) code_block = "" ; ) . runRequiringTrue ( ) ; RecordingSshjTool . forbidden . set ( true ) ; newManagementContext = createNewManagementContext ( ) ; newApp = ( TestApplication ) RebindTestUtils . rebind ( ( LocalManagementContext ) newManagementContext , classLoader ) ; NginxController newNginx = Iterables . getOnlyElement ( Ent
public void test() { { origManagementContext . getGarbageCollector ( ) . gcIteration ( ) ; List < Task < ? > > tasksAfter = ( ( BasicExecutionManager ) origManagementContext . getExecutionManager ( ) ) . getAllTasks ( ) ; log . info ( "tasks after disabling HA, " + tasksAfter . size ( ) + ": " + tasksAfter ) ; return tasksAfter . isEmpty ( ) ; } }
@ Test ( groups = "Integration" ) public void testChangeModeFailureStopsTasksButHappyUponResumption ( ) throws Exception { DynamicCluster origServerPool = origApp . createAndManageChild ( EntitySpec . create ( DynamicCluster . class ) . configure ( DynamicCluster . MEMBER_SPEC , EntitySpec . create ( TomcatServer . class ) . configure ( "war" , getTestWar ( ) ) ) . configure ( "initialSize" , 1 ) ) ; NginxController origNginx = origApp . createAndManageChild ( EntitySpec . create ( NginxController . class ) . configure ( "serverPool" , origServerPool ) . configure ( "domain" , "localhost" ) ) ; origApp . start ( ImmutableList . of ( loc ) ) ; Assert . assertTrue ( RecordingSshjTool . connectionCount . get ( ) > 0 ) ; Collection < Feed > origFeeds = ( ( EntityInternal ) origNginx ) . feeds ( ) . getFeeds ( ) ; LOG . info ( "feeds before rebind are: " + origFeeds ) ; Assert . assertTrue ( origFeeds . size ( ) >= 1 ) ; origManagementContext . getRebindManager ( ) . forcePersistNow ( ) ; List < Task < ? > > tasksBefore = ( ( BasicExecutionManager ) origManagementContext . getExecutionManager ( ) ) . getAllTasks ( ) ; LOG . info ( "tasks before disabling HA, " + tasksBefore . size ( ) + ": " + tasksBefore ) ; Assert . assertFalse ( tasksBefore . isEmpty ( ) ) ; origManagementContext . getHighAvailabilityManager ( ) . changeMode ( HighAvailabilityMode . DISABLED ) ; origApp = null ; Repeater . create ( ) . every ( Duration . millis ( 20 ) ) . backoffTo ( Duration . ONE_SECOND ) . limitTimeTo ( Duration . THIRTY_SECONDS ) . until ( new Callable < Boolean > ( ) code_block = "" ; ) . runRequiringTrue ( ) ; RecordingSshjTool . forbidden . set ( true ) ; newManagementContext = createNewManagementContext ( ) ; newApp = ( TestApplication ) RebindTestUtils . rebind ( ( LocalManagementContext ) newManagementContext , classLoader ) ; } catch (
public List < OWLOntologyChange > getChanges ( ) { logger . info ( "[Deprecate Entity] Deprecating " + info . getEntityToDeprecate ( ) ) ; logger . info ( LogBanner . end ( ) ) ; List < OWLOntologyChange > changes = new ArrayList < > ( ) ; switchUsageOfDeprecatedEntityWithReplacement ( changes ) ; updateDeprecatedEntityLogicalDefinition ( changes ) ; updateDeprecatedEntityAnnotations ( changes ) ; addDeprecatedAnnotationAssertion ( changes ) ; addDeprecationReason ( changes ) ; addDeprecationCode ( changes ) ; relabelDeprecatedEntity ( changes ) ; prefixDeprecatedAnnotationValues ( changes ) ; addReplacedByAnnotation ( changes ) ; addAlternateEntityAnnotations ( changes ) ; reparentDeprecatedEntity ( changes ) ; logger . info ( LogBanner . end ( ) ) ; return changes ; }
public List < OWLOntologyChange > getChanges ( ) { logger . info ( LogBanner . start ( "Deprecating entity" ) ) ; List < OWLOntologyChange > changes = new ArrayList < > ( ) ; switchUsageOfDeprecatedEntityWithReplacement ( changes ) ; updateDeprecatedEntityLogicalDefinition ( changes ) ; updateDeprecatedEntityAnnotations ( changes ) ; addDeprecatedAnnotationAssertion ( changes ) ; addDeprecationReason ( changes ) ; addDeprecationCode ( changes ) ; relabelDeprecatedEntity ( changes ) ; prefixDeprecatedAnnotationValues ( changes ) ; addReplacedByAnnotation ( changes ) ; addAlternateEntityAnnotations ( changes ) ; reparentDeprecatedEntity ( changes ) ; logger . debug ( LogBanner . end ( ) ) ; return changes ; }
public List < OWLOntologyChange > getChanges ( ) { logger . info ( LogBanner . start ( "Deprecating entity" ) ) ; logger . info ( "[Deprecate Entity] Deprecating " + info . getEntityToDeprecate ( ) ) ; List < OWLOntologyChange > changes = new ArrayList < > ( ) ; switchUsageOfDeprecatedEntityWithReplacement ( changes ) ; updateDeprecatedEntityLogicalDefinition ( changes ) ; updateDeprecatedEntityAnnotations ( changes ) ; addDeprecatedAnnotationAssertion ( changes ) ; addDeprecationReason ( changes ) ; addDeprecationCode ( changes ) ; relabelDeprecatedEntity ( changes ) ; prefixDeprecatedAnnotationValues ( changes ) ; addReplacedByAnnotation ( changes ) ; addAlternateEntityAnnotations ( changes ) ; reparentDeprecatedEntity ( changes ) ; logger . info ( LogBanner . end ( "Deprecate entity" ) ) ; return changes ; }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceAccountUserRelServiceUtil . class , "getCommerceAccountUserRels" , _getCommerceAccountUserRelsParameterTypes7 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , commerceAccountId , start , end ) ; Object returnObj = null ; code_block = TryStatement ;  return ( java . util . List < com . liferay . commerce . account . model . CommerceAccountUserRel > ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( lastIndex < index ) { logger . info ( "Resetting index {}" , index ) ; raftLog . reset ( index + 1 ) ; } }
@ Override public void run ( ) { logger . info ( "Try to lock" ) ; connectDistributedSystem ( ) ; DLockService dls = ( DLockService ) DistributedLockService . getServiceNamed ( dlsName ) ; assertThat ( dls . lock ( key1 , - 1 , - 1 ) ) . isTrue ( ) ; assertThat ( dls . getToken ( key1 ) . getUsageCount ( ) ) . isEqualTo ( 1 ) ; assertThat ( dls . lock ( key1 , - 1 , - 1 ) ) . isTrue ( ) ; assertThat ( dls . getToken ( key1 ) . getUsageCount ( ) ) . isEqualTo ( 2 ) ; assertThat ( dls . lock ( key1 , - 1 , - 1 ) ) . isTrue ( ) ; assertThat ( dls . getToken ( key1 ) . getUsageCount ( ) ) . isEqualTo ( 3 ) ; DLockToken token0 = dls . getToken ( key1 ) ; assertThat ( token0 ) . isNotNull ( ) ; Collection tokens = dls . getTokens ( ) ; assertThat ( tokens . contains ( token0 ) ) . isTrue ( ) ; assertThat ( tokens . size ( ) ) . isEqualTo ( 1 ) ; dls . unlock ( key1 ) ; assertThat ( dls . getToken ( key1 ) . getUsageCount ( ) ) . isEqualTo ( 2 ) ; dls . freeResources ( key1 ) ; DLockToken token1 = dls . getToken ( key1 ) ; assertThat ( token1 ) . isNotNull ( ) ; assertThat ( token1 ) . isEqualTo ( token0 ) ; tokens = dls . getTokens ( ) ; assertThat ( tokens . contains ( token1 ) ) . isTrue ( ) ; assertThat ( tokens . size ( ) ) . isEqualTo ( 1 ) ; dls . unlock ( key1 ) ; assertThat ( dls . getToken ( key1 ) . getUsageCount ( ) ) . isEqualTo ( 1 ) ; dls . freeResources ( key1 ) ; assertThat ( dls . getToken ( key1 ) ) . isNotNull ( ) ; DLockToken token2 = dls . getToken ( key1 ) ; assertThat ( token2 )
public void test() { try { messageCount = getQueueNotVisibleMessageCount ( queueUrl , false ) ; } catch ( Exception ex ) { logger . error ( "event=failed_to_get_not_visible_messages queue_url=" + queueUrl ) ; } }
public void test() { try { ret . append ( paramName ) ; ret . append ( "=" ) ; ret . append ( urlCodec . encode ( paramValue ) ) ; } catch ( EncoderException ex ) { LOGGER . error ( "Unable to encode parameter name or value: " + paramName + "=" + paramValue , ex ) ; throw new RuntimeException ( "Unable to encode parameter name or value: " + paramName + "=" + paramValue , ex ) ; } }
public void test() { try { return new BigtableAsyncAdmin ( BigtableAsyncConnection . this ) ; } catch ( IOException e ) { log . error ( "failed to build BigtableAsyncAdmin" , e ) ; throw new UncheckedIOException ( "failed to build BigtableAsyncAdmin" , e ) ; } }
public static void prepareRuntimeContext ( ) { LOGGER . info ( "Preparing WindGate" ) ; RuntimeContext . set ( RuntimeContext . DEFAULT . apply ( System . getenv ( ) ) ) ; RuntimeContext . get ( ) . verifyApplication ( WindGate . class . getClassLoader ( ) ) ; }
public void test() { try { VersioningIterator it = new VersioningIterator ( ) ; IteratorSetting is = new IteratorSetting ( 1 , VersioningIterator . class ) ; VersioningIterator . setMaxVersions ( is , 3 ) ; it . init ( new SortedMapIterator ( tm ) , is . getOptions ( ) , null ) ; it . seek ( new Range ( ) , EMPTY_COL_FAMS , false ) ; TreeMap < Key , Value > tmOut = iteratorOverTestData ( it ) ; code_block = ForStatement ; assertEquals ( "size after keeping 3 versions was " + tmOut . size ( ) , 6 , tmOut . size ( ) ) ; } catch ( IOException e ) { log . error ( "" , e ) ; fail ( ) ; } catch ( Exception e ) { fail ( ) ; } }
@ Deactivate public void stop ( ) { logger . info ( "Shutting down Http Session" ) ; }
ChecksumValue computeLocalFileChecksum ( final File localFile , final ChecksumEncodingEnum overrideChecksumEncoding ) throws JargonException { log . info ( "computeLocalFileChecksum()" ) ; code_block = IfStatement ; log . info ( "localFile:{}" , localFile ) ; code_block = IfStatement ; code_block = IfStatement ; ChecksumEncodingEnum checksumEncoding ; code_block = IfStatement ; log . info ( "using checksum algorithm:{}" , checksumEncoding ) ; AbstractChecksumComputeStrategy strategy = irodsAccessObjectFactory . getIrodsSession ( ) . getLocalChecksumComputerFactory ( ) . instance ( checksumEncoding ) ; code_block = TryStatement ;  }
ChecksumValue computeLocalFileChecksum ( final File localFile , final ChecksumEncodingEnum overrideChecksumEncoding ) throws JargonException { log . info ( "computeLocalFileChecksum()" ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; ChecksumEncodingEnum checksumEncoding ; code_block = IfStatement ; log . info ( "using checksum algorithm:{}" , checksumEncoding ) ; log . info ( "using checksum algorithm:{}" , checksumEncoding ) ; AbstractChecksumComputeStrategy strategy = irodsAccessObjectFactory . getIrodsSession ( ) . getLocalChecksumComputerFactory ( ) . instance ( checksumEncoding ) ; code_block = TryStatement ;  }
ChecksumValue computeLocalFileChecksum ( final File localFile , final ChecksumEncodingEnum overrideChecksumEncoding ) throws JargonException { log . info ( "computeLocalFileChecksum()" ) ; code_block = IfStatement ; log . info ( "localFile:{}" , localFile ) ; log . info ( "checksumEncodingEnumEncoding={}" , overrideChecksumEncoding ) ; code_block = IfStatement ; code_block = IfStatement ; ChecksumEncodingEnum checksumEncoding ; code_block = IfStatement ; AbstractChecksumComputeStrategy strategy = irodsAccessObjectFactory . getIrodsSession ( ) . getLocalChecksumComputerFactory ( ) . instance ( checksumEncoding ) ; code_block = TryStatement ;  }
public void test() { try { return strategy . computeChecksumValueForLocalFile ( localFile . getAbsolutePath ( ) ) ; } catch ( FileNotFoundException e ) { log . error ( "cannot find local file to do the checksum" , e ) ; throw new JargonException ( "cannot find local file to do the checksum" , e ) ; } }
public void test() { try { Map . Entry indexEntry = ( Map . Entry ) it . next ( ) ; PartitionedIndex index = ( PartitionedIndex ) indexEntry . getValue ( ) ; IndexCreationData icd = new IndexCreationData ( index . getName ( ) ) ; new QCompiler ( ) ; String imports = index . getImports ( ) ; icd . setIndexData ( index . getType ( ) , index . getCanonicalizedFromClause ( ) , index . getCanonicalizedIndexedExpression ( ) , index . getImports ( ) ) ; icd . setPartitionedIndex ( index ) ; indexes . add ( icd ) ; } catch ( Exception ignor ) { LOG . warn ( "IGNORED" , ignor ) ; } }
public void test() { try { Thread . sleep ( 30000 ) ; getEndpoint ( ) . createQueue ( getClient ( ) ) ; } catch ( Exception e ) { LOGGER . warn ( "Failed to create queue" , e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( QueueDeletedRecentlyException qdr ) { LOG . debug ( "Queue recently deleted, will retry in 30 seconds." ) ; code_block = TryStatement ;  } catch ( Exception e ) { LOG . error ( "Recieved Message Exception." , e ) ; } }
public void test() { if ( errored . compareAndSet ( false , true ) ) { LOGGER . log ( Level . SEVERE , "Detected error after dispatch" , t ) ; Runtime . getRuntime ( ) . halt ( 1 ) ; } }
public edu . indiana . extreme . wsdl . benchmark1 . ReceiveMeshInterfaceObjectsResponse receiveMeshInterfaceObjects ( edu . indiana . extreme . wsdl . benchmark1 . ReceiveMeshInterfaceObjectsRequest input ) { ReceiveMeshInterfaceObjectsResponse ret = new ReceiveMeshInterfaceObjectsResponse ( ) ; ret . setReceiveMeshInterfaceObjectsReturn ( input . getInput ( ) . getItem ( ) . size ( ) ) ; LOGGER . debug ( "getReceiveMeshInterfaceObjectsReturn received" ) ; return ret ; }
@ Override public void deleteList ( String uuid ) throws BusinessException { ContactList listToDelete = findByUuid ( uuid ) ; logger . debug ( "deleteList() - uuid: {}" , uuid ) ; listRepository . delete ( listToDelete ) ; }
public void test() { try { listener . gotGeoDetails ( place ) ; } catch ( Exception e ) { logger . warn ( "Exception at getGeoDetails" , e ) ; } }
public void test() { for ( Entry < String , Object > e : l ) { logger . info ( e . getMessage ( ) ) ; } }
public void test() { try { ColorScale cs = getColorScaleFromName ( name ) ; return createColorScaleSwatch ( cs , format , width , height ) ; } catch ( Exception e ) { log . error ( "Error creating color scale " + name , e ) ; throw new MrsPyramidServiceException ( "Error creating color scale " + name , e ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { this . groupsFromUGI = ApplicationProperties . get ( ) . getBoolean ( "atlas.authentication.method.pam.ugi-groups" , true ) ; Properties properties = ConfigurationConverter . getProperties ( ApplicationProperties . get ( ) . subset ( "atlas.authentication.method.pam" ) ) ; code_block = ForStatement ; code_block = IfStatement ; code_block = IfStatement ; } catch ( Exception e ) { LOG . error ( "Error occurred: " + e . getMessage ( ) , e ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { return PropertyAccessor . getInstance ( ) . getProperty ( NhincConstants . MESSAGES_PROPERTY_FILE , key ) ; } catch ( PropertyAccessException ex ) { LOG . error ( "Failed to access messages file {}" , key , ex ) ; } }
public void test() { try { byte [ ] d = HexUtils . hexToBytes ( data . toString ( ) ) ; logger . debug ( "hex: {}" , d ) ; return d ; } catch ( IllegalArgumentException e ) { logger . debug ( "Exception occured during data conversion: {}" , e . getMessage ( ) ) ; } }
public void test() { try { byte [ ] d = HexUtils . hexToBytes ( data . toString ( ) ) ; logger . debug ( "File len: {}" , d . length ) ; return d ; } catch ( IllegalArgumentException e ) { logger . debug ( "File error" , e ) ; } }
public void test() { if ( storeFile . exists ( ) ) { LOG . debug ( "Loading store from file" ) ; fin = new FileInputStream ( storeFile ) ; } else { LOG . debug ( "Trying to load store from classpath" ) ; fin = getClass ( ) . getClassLoader ( ) . getResourceAsStream ( storeFile . getPath ( ) ) ; code_block = IfStatement ; } }
public void test() { if ( storeFile . exists ( ) ) { LOG . debug ( "Trying to load store from file" ) ; fin = new FileInputStream ( storeFile ) ; } else { LOG . debug ( "Trying to load store from store" ) ; fin = getClass ( ) . getClassLoader ( ) . getResourceAsStream ( storeFile . getPath ( ) ) ; code_block = IfStatement ; } }
@ Override protected void parseHandshakeMessageContent ( HelloRetryRequestMessage msg ) { LOGGER . debug ( "Parsing HelloRetryRequestMessage" ) ; parseProtocolVersion ( msg ) ; parseSelectedCiphersuite ( msg ) ; code_block = IfStatement ; }
public void test() { if ( commandLineArguments . getLogLevel ( ) == Level . DEBUG ) { LOG . debug ( getMessagesFromException ( e ) ) ; } else { LOG . error ( getMessagesFromException ( e ) ) ; } }
public void test() { if ( commandLineArguments . getLogLevel ( ) == Level . DEBUG ) { LOG . error ( "Unexpected error" , e ) ; } else { LOG . error ( "Unexpected error" , e ) ; } }
public void test() { try { cloudStackContext = new CloudStackSpringContext ( ) ; cloudStackContext . registerShutdownHook ( ) ; event . getServletContext ( ) . setAttribute ( CloudStackSpringContext . CLOUDSTACK_CONTEXT_SERVLET_KEY , cloudStackContext ) ; } catch ( IOException e ) { log . error ( "Failed to initialize CloudStack Spring modules" , e ) ; throw new RuntimeException ( "Failed to initialize CloudStack Spring modules" , e ) ; } }
protected void truncate ( final long length ) throws IOException { LOG . debug ( "Truncating {}" , length ) ; fc . truncate ( length ) ; }
public void test() { if ( this . logInvalidTraces ) { this . logger . error ( "Invalid trace: {}" , invalidTrace ) ; } }
public void test() { try { updaterFuture . get ( ) ; } catch ( ExecutionException ee ) { log . error ( e , "Future for %s could not be scheduled" , this ) ; } catch ( CancellationException ce ) { log . error ( ce , "Future for %s has already been cancelled" , this ) ; } catch ( InterruptedException ie ) { Thread . currentThread ( ) . interrupt ( ) ; throw new RuntimeException ( ie ) ; } }
public void test() { try { updaterFuture . get ( ) ; } catch ( ExecutionException ee ) { log . error ( ee . getCause ( ) , "Error in %s" , this ) ; } catch ( CancellationException ce ) { log . debug ( "Elapsed time in %s" , this ) ; } catch ( InterruptedException ie ) { Thread . currentThread ( ) . interrupt ( ) ; throw new RuntimeException ( ie ) ; } }
@ Override public void init ( final FilterConfig filterConfig ) throws ServletException { LOGGER . info ( "[waffle.servlet.CorsAwareNegotiateSecurityFilter] Starting" ) ; super . init ( filterConfig ) ; CorsAwareNegotiateSecurityFilter . LOGGER . info ( "[waffle.servlet.CorsAwareNegotiateSecurityFilter] Started" ) ; }
@ Override public void init ( final FilterConfig filterConfig ) throws ServletException { CorsAwareNegotiateSecurityFilter . LOGGER . info ( "[waffle.servlet.CorsAwareNegotiateSecurityFilter] Starting" ) ; super . init ( filterConfig ) ; LOGGER . info ( "[waffle.servlet.CorsAwareNegotiateSecurityFilter] Finished" ) ; }
public void test() { if ( configuredLimit > recommendedLimit ) { LOG . info ( "Skipping connection limit: {}" , recommendedLimit ) ; } else { LOG . debug ( "Configured connection limit: {}" , configuredLimit ) ; } }
public void test() { if ( configuredLimit > recommendedLimit ) { LOG . warn ( "Configured connection limit {} is too high: Recommended is maximum {} (based on {})" , configuredLimit , recommendedLimit , strategy . getResourcesDescription ( ) ) ; } else { LOG . info ( "Configured connection limit {} is {}" , configuredLimit , recommendedLimit ) ; } }
@ Override public void run ( ) { logger . info ( "Shutting down management agents" ) ; code_block = TryStatement ;  logger . info ( "Initiating shutdown of management agents" ) ; code_block = ForStatement ; }
@ Override public void run ( ) { logger . info ( "Shutdown of management agent will commence in: " + MANAGEMENT_AGENT_SHUTDOWN_INTERNAL_SECONDS + " seconds" ) ; code_block = TryStatement ;  code_block = ForStatement ; logger . info ( "Shutdown complete." ) ; }
public void test() { try { Message msg = currentConnector . get ( batchSize , timeout , unit ) ; logger . info ( "got the connector: " + msg ) ; return msg ; } catch ( Throwable t ) { times ++ ; restart ( ) ; logger . info ( "restart the connector for next round retry." ) ; } }
public void test() { try { Message msg = currentConnector . get ( batchSize , timeout , unit ) ; logger . debug ( "received: {}" , msg ) ; return msg ; } catch ( Throwable t ) { logger . warn ( String . format ( "something goes wrong when getting data from server:%s" , currentConnector != null ? currentConnector . getAddress ( ) : "null" ) , t ) ; times ++ ; restart ( ) ; } }
public void test() { if ( registration == null ) { localLogger . error ( "FileDAOHibernate updateDetached registration is null" ) ; } else-if ( registration . isLIMSAdmin ( ) || dbObject . givesPermission ( registration ) ) { localLogger . info ( "updateDetached file object" ) ; return updateDetached ( file ) ; } else { localLogger . error ( "FileDAOHibernate updateDetached not authorized" ) ; } }
public void test() { if ( registration == null ) { localLogger . error ( "FileDAOHibernate updateDetached registration is null" ) ; } else-if ( registration . isLIMSAdmin ( ) || dbObject . givesPermission ( registration ) ) { localLogger . info ( "updateDetached file object" ) ; return updateDetached ( file ) ; } else { localLogger . error ( "FileDAOHibernate updateDetached not authorized" ) ; } }
public void test() { if ( registration == null ) { localLogger . error ( "FileDAOHibernate updateDetached registration is null" ) ; } else-if ( registration . isLIMSAdmin ( ) || dbObject . givesPermission ( registration ) ) { localLogger . info ( "updateDetached file object" ) ; return updateDetached ( file ) ; } else { localLogger . error ( "FileDAOHibernate updateDetached not authorized" ) ; } }
public void test() { if ( hasEnvelopeForOffering ( offering ) ) { final ReferencedEnvelope offeringEnvelope = this . envelopeForOfferings . get ( offering ) ; LOG . trace ( "Expanding envelope for offering {} to include {}" , offering , envelope ) ; offeringEnvelope . expandToInclude ( envelope ) ; } else { setEnvelopeForOffering ( offering , new ReferencedEnvelope ( envelope , getDefaultEPSGCode ( ) ) ) ; } }
public void test() { if ( ! future . isSuccess ( ) ) { logger . error ( "Server -> SSL handshake failed. Succeeded: {}" , future . isSuccess ( ) ) ; ctx . close ( ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( PortalException portalException ) { _log . error ( portalException , portalException ) ; return false ; } }
public void test() { try { ResourceHost resourceHost = peerManager . getLocalPeer ( ) . getManagementHost ( ) ; resourceHost . joinP2PSwarmDHCP ( proxyDto . getP2pIfaceName ( ) , proxyDto . getP2pHash ( ) , proxyDto . getP2SecretKey ( ) , proxyDto . getP2pSecretTTL ( ) ) ; code_block = ForStatement ; proxyDto . setState ( ProxyDto . State . READY ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; proxyDto . setState ( ProxyDto . State . FAILED ) ; proxyDto . setLogs ( e . getMessage ( ) ) ; } }
public void test() { if ( message == null ) { LOGGER . info ( "Received message, id = " + message . getJMSMessageID ( ) ) ; } else { LOGGER . info ( "Received message, id = " + message . getJMSMessageID ( ) ) ; receivedMessages . add ( ( MapMessage ) message ) ; } }
public void test() { if ( count >= MAX_MESSAGE_COUNT ) { LOG . error ( "Last message count exceeded. ({})" , count ) ; } }
public void test() { try { LOGGER . info ( "Starting to receive messages." ) ; int count = 0 ; code_block = ForStatement ; LOGGER . info ( "Finished receiving messages." ) ; code_block = IfStatement ; } finally { connection . close ( ) ; } }
public void test() { try { Connection connection = new ActiveMQConnectionFactory ( brokerURL ) . createConnection ( ) ; connection . start ( ) ; Session session = connection . createSession ( false , Session . AUTO_ACKNOWLEDGE ) ; Destination destination = session . createTopic ( topicName ) ; MessageConsumer consumer = session . createConsumer ( destination ) ; code_block = TryStatement ;  } catch ( Exception e ) { LOG . error ( e . getMessage ( ) , e ) ; } }
public void migrateNavigation ( FileSystem sourceFS , FileSystem targetFS ) { LOG . info ( "MigrateNavigation" ) ; migrate ( sourceFS , targetFS , path -> path . getFileName ( ) . toString ( ) . equals ( "navtree.json" ) ) ; }
public void test() { try { Files . walk ( file . toPath ( ) ) . sorted ( Comparator . reverseOrder ( ) ) . map ( Path :: toFile ) . forEach ( File :: delete ) ; } catch ( IOException e ) { log . error ( "Unable to delete files" , e ) ; } }
public synchronized void activate ( ComponentContext context , Map < String , Object > properties ) { logger . info ( "Bundle {} has started!" , this . getClass ( ) . getSimpleName ( ) ) ; this . componentContext = context ; this . keystoreServiceOptions = new KeystoreServiceOptions ( properties , this . cryptoService ) ; this . selfUpdaterExecutor = Executors . newSingleThreadScheduledExecutor ( ) ; code_block = IfStatement ; logger . info ( "Bundle {} has started!" , this . getClass ( ) . getSimpleName ( ) ) ; }
public synchronized void activate ( ComponentContext context , Map < String , Object > properties ) { logger . info ( "Bundle {} is starting!" , this . getClass ( ) . getSimpleName ( ) ) ; this . componentContext = context ; this . keystoreServiceOptions = new KeystoreServiceOptions ( properties , this . cryptoService ) ; this . selfUpdaterExecutor = Executors . newSingleThreadScheduledExecutor ( ) ; code_block = IfStatement ; logger . info ( "Bundle {} is activated" , this . getClass ( ) . getSimpleName ( ) ) ; }
public void test() { try { LoadTestDataSimpleResponseMessageType response = ( LoadTestDataSimpleResponseMessageType ) invokeClientPort ( AdminWSConstants . ADMIN_LTD_SAVEPATIENT , request ) ; logDebug ( AdminWSConstants . ADMIN_LTD_SAVEPATIENT , response . isStatus ( ) , response . getMessage ( ) ) ; code_block = IfStatement ; return response . isStatus ( ) ; } catch ( Exception e ) { LOG . error ( "error during save - {}" , e . getLocalizedMessage ( ) , e ) ; } }
public void test() { if ( ip == null ) { return ; } }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; WatchdogDistributionStatus watchdogDistributionStatus = watchdogDistributionStatusRepository . findById ( iStatus . getDistributionID ( ) ) . orElseGet ( ( ) -> null ) ; code_block = IfStatement ; toscaInstaller . installTheComponentStatus ( iStatus ) ; logger . debug ( "SUCCESSING ASDCStatusCallback" ) ; } catch ( ArtifactInstallerException e ) { logger . error ( "Error in ASDCStatusCallback {}" , e . getMessage ( ) , e ) ; logger . debug ( "Error in ASDCStatusCallback {}" , e . getMessage ( ) ) ; } }
@ Override protected boolean bridgeDirectCommunicate ( BridgeCommunicationProtocol communication , boolean useAuthentication ) { logger . debug ( "bridgeDirectCommunicate({})" , communication ) ; return bridgeDirectCommunicate ( ( JsonBridgeCommunicationProtocol ) communication , useAuthentication ) ; }
@ Override public void generate ( Model model , MolgenisOptions options ) throws Exception { Template template = createTemplate ( "/" + this . getClass ( ) . getSimpleName ( ) + ".psql.ftl" ) ; Map < String , Object > templateArgs = createTemplateArguments ( options ) ; List < Entity > entityList = model . getEntities ( ) ; entityList = MolgenisModel . sortEntitiesByDependency ( entityList , model ) ; File target = new File ( this . getSqlPath ( options ) + "/create_tables.sql" ) ; boolean created = target . getParentFile ( ) . mkdirs ( ) ; code_block = IfStatement ; templateArgs . put ( "model" , model ) ; templateArgs . put ( "entities" , entityList ) ; OutputStream targetOut = new FileOutputStream ( target ) ; template . process ( templateArgs , new OutputStreamWriter ( targetOut , Charset . forName ( "UTF-8" ) ) ) ; targetOut . close ( ) ; logger . info ( "generated " + target ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { in = new BufferedInputStream ( resourceStream . getInputStream ( ) ) ; ValueMap data = loader . loadWicketProperties ( in ) ; code_block = IfStatement ; return data ; } catch ( ResourceStreamNotFoundException | IOException e ) { log . error ( "Failed to load Wicket properties from: " + ExceptionUtils . getRootCauseMessage ( e ) ) ; } finally { IOUtils . closeQuietly ( in ) ; IOUtils . closeQuietly ( resourceStream ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
@ Override public void log ( JobLogPo jobLogPo ) { jobLog . log ( jobLogPo . toString ( ) ) ; }
public void test() { try { DeployedUnit deployedUnit = deploymentService . getDeployedUnit ( id ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . warn ( "Unable to create image reference for container " + id + " by extension " + this + " due to " + e . getMessage ( ) ) ; messages . add ( new Message ( Severity . WARN , "Unable to create image reference for container " + id + " by extension " + this + " due to " + e . getMessage ( ) ) ) ; } }
@ Override public BuildSetTask build ( BuildConfigurationSet buildConfigurationSet , User user , BuildOptions buildOptions ) { log . info ( "Building request from user: {}" , user ) ; return Mockito . mock ( BuildSetTask . class ) ; }
public void test() { try { pt = ( PreparedTickler ) cl . loadClass ( className ) . newInstance ( ) ; } catch ( Exception e ) { log . warn ( "createTickler failed" , e ) ; } }
public void stop ( ) { LOGGER . info ( "Stopping" ) ; running = false ; close ( ) ; }
void unblockTo ( Address address ) { LOGGER . info ( "Unblocked to {}" , address ) ; LockPair lockPair = getLockPair ( address ) ; lockPair . unblockOutgoing ( ) ; }
public void test() { for ( String cache : pools ) { code_block = IfStatement ; } }
public void test() { try { return env . getServerHome ( ) . getCanonicalPath ( ) ; } catch ( IOException e ) { LOGGER . error ( "Unable to get canonical path" , e ) ; return null ; } }
@ Override public void error ( Marker marker , String message , Throwable t ) { getLogger ( ) . error ( marker , message , t ) ; }
@ Test public void shortExample5 ( ) { String sequence = "QIKDLLVSSSTDLDTTLVLVNAIYFKGMWKTAFNAEDTRECMPFHVTKQESKPVQMMCMNNSFNVATLPAE" ; logger . debug ( "Absorbance (Cys Not Reduced): {}" , PeptideProperties . getAbsorbance ( sequence , false ) ) ; logger . debug ( "Extinction Coefficient (Cys Reduced): {}" , PeptideProperties . getExtinctionCoefficient ( sequence , true ) ) ; logger . debug ( "Extinction Coefficient (Cys Not Reduced): {}" , PeptideProperties . getExtinctionCoefficient ( sequence , false ) ) ; logger . debug ( "Instability Index: {}" , PeptideProperties . getInstabilityIndex ( sequence ) ) ; logger . debug ( "Apliphatic Index: {}" , PeptideProperties . getApliphaticIndex ( sequence ) ) ; logger . debug ( "Average Hydropathy Value: {}" , PeptideProperties . getAvgHydropathy ( sequence ) ) ; logger . debug ( "Average Length: {}" , PeptideProperties . getTotalLength ( sequence ) ) ; logger . debug ( "Isoelectric Point: {}" , PeptideProperties . getIsoelectricPoint ( sequence ) ) ; logger . debug ( "Net Charge at pH 7: {}" , PeptideProperties . getNetCharge ( sequence ) ) ; }
@ Test public void shortExample5 ( ) { String sequence = "QIKDLLVSSSTDLDTTLVLVNAIYFKGMWKTAFNAEDTRECMPFHVTKQESKPVQMMCMNNSFNVATLPAE" ; logger . debug ( "Absorbance (Cys Reduced): {}" , PeptideProperties . getAbsorbance ( sequence , true ) ) ; logger . debug ( "Extinction Coefficient (Cys Reduced): {}" , PeptideProperties . getExtinctionCoefficient ( sequence , true ) ) ; logger . debug ( "Extinction Coefficient (Cys Not Reduced): {}" , PeptideProperties . getExtinctionCoefficient ( sequence , false ) ) ; logger . debug ( "Instability Index: {}" , PeptideProperties . getInstabilityIndex ( sequence ) ) ; logger . debug ( "Apliphatic Index: {}" , PeptideProperties . getApliphaticIndex ( sequence ) ) ; logger . debug ( "Average Hydropathy Value: {}" , PeptideProperties . getAvgHydropathy ( sequence ) ) ; logger . debug ( "Average Length: {}" , PeptideProperties . getTotalLength ( sequence ) ) ; logger . debug ( "Isoelectric Point: {}" , PeptideProperties . getIsoelectricPoint ( sequence ) ) ; logger . debug ( "Net Charge at pH 7: {}" , PeptideProperties . getNetCharge ( sequence ) ) ; }
@ Test public void shortExample5 ( ) { String sequence = "QIKDLLVSSSTDLDTTLVLVNAIYFKGMWKTAFNAEDTRECMPFHVTKQESKPVQMMCMNNSFNVATLPAE" ; logger . debug ( "Absorbance (Cys Reduced): {}" , PeptideProperties . getAbsorbance ( sequence , true ) ) ; logger . debug ( "Absorbance (Cys Not Reduced): {}" , PeptideProperties . getAbsorbance ( sequence , false ) ) ; logger . debug ( "Extinction Coefficient (Cys Not Reduced): {}" , PeptideProperties . getExtinctionCoefficient ( sequence , false ) ) ; logger . debug ( "Instability Index: {}" , PeptideProperties . getInstabilityIndex ( sequence ) ) ; logger . debug ( "Apliphatic Index: {}" , PeptideProperties . getApliphaticIndex ( sequence ) ) ; logger . debug ( "Average Id: {}" , PeptideProperties . getId ( sequence ) ) ; logger . debug ( "Average Hydropathy Value: {}" , PeptideProperties . getAvgHydropathy ( sequence ) ) ; logger . debug ( "Isoelectric Point: {}" , PeptideProperties . getIsoelectricPoint ( sequence ) ) ; logger . debug ( "Net Charge at pH 7: {}" , PeptideProperties . getNetCharge ( sequence ) ) ; }
@ Test public void shortExample5 ( ) { String sequence = "QIKDLLVSSSTDLDTTLVLVNAIYFKGMWKTAFNAEDTRECMPFHVTKQESKPVQMMCMNNSFNVATLPAE" ; logger . debug ( "Absorbance (Cys Reduced): {}" , PeptideProperties . getAbsorbance ( sequence , true ) ) ; logger . debug ( "Absorbance (Cys Not Reduced): {}" , PeptideProperties . getAbsorbance ( sequence , false ) ) ; logger . debug ( "Extinction Coefficient (Cys Reduced): {}" , PeptideProperties . getExtinctionCoefficient ( sequence , true ) ) ; logger . debug ( "Instability Index: {}" , PeptideProperties . getInstabilityIndex ( sequence ) ) ; logger . debug ( "Apliphatic Index: {}" , PeptideProperties . getApliphaticIndex ( sequence ) ) ; logger . debug ( "Average Hydropathy Value: {}" , PeptideProperties . getAvgHydropathy ( sequence ) ) ; logger . debug ( "Average Length: {}" , PeptideProperties . getTotalLength ( sequence ) ) ; logger . debug ( "Isoelectric Point: {}" , PeptideProperties . getIsoelectricPoint ( sequence ) ) ; logger . debug ( "Net Charge at pH 7: {}" , PeptideProperties . getNetCharge ( sequence ) ) ; }
@ Test public void shortExample5 ( ) { String sequence = "QIKDLLVSSSTDLDTTLVLVNAIYFKGMWKTAFNAEDTRECMPFHVTKQESKPVQMMCMNNSFNVATLPAE" ; logger . debug ( "Absorbance (Cys Reduced): {}" , PeptideProperties . getAbsorbance ( sequence , true ) ) ; logger . debug ( "Absorbance (Cys Removed): {}" , PeptideProperties . getAbsorbance ( sequence , false ) ) ; logger . debug ( "Extinction Coefficient (Cys Reduced): {}" , PeptideProperties . getExtinctionCoefficient ( sequence , true ) ) ; logger . debug ( "Extinction Coefficient (Cys Not Reduced): {}" , PeptideProperties . getExtinctionCoefficient ( sequence , false ) ) ; logger . debug ( "Apliphatic Index: {}" , PeptideProperties . getApliphaticIndex ( sequence ) ) ; logger . debug ( "Average Hydropathy Value: {}" , PeptideProperties . getAvgHydropathy ( sequence ) ) ; logger . debug ( "Average Hydropathy Value: {}" , PeptideProperties . getAvgHydropathy ( sequence ) ) ; logger . debug ( "Isoelectric Point: {}" , PeptideProperties . getIsoelectricPoint ( sequence ) ) ; logger . debug ( "Net Charge at pH 7: {}" , PeptideProperties . getNetCharge ( sequence ) ) ; }
@ Test public void shortExample5 ( ) { String sequence = "QIKDLLVSSSTDLDTTLVLVNAIYFKGMWKTAFNAEDTRECMPFHVTKQESKPVQMMCMNNSFNVATLPAE" ; logger . debug ( "Absorbance (Cys Reduced): {}" , PeptideProperties . getAbsorbance ( sequence , true ) ) ; logger . debug ( "Absorbance (Cys Removed): {}" , PeptideProperties . getAbsorbance ( sequence , false ) ) ; logger . debug ( "Extinction Coefficient (Cys Reduced): {}" , PeptideProperties . getExtinctionCoefficient ( sequence , true ) ) ; logger . debug ( "Extinction Coefficient (Cys Not Reduced): {}" , PeptideProperties . getExtinctionCoefficient ( sequence , false ) ) ; logger . debug ( "Instability Index: {}" , PeptideProperties . getInstabilityIndex ( sequence ) ) ; logger . debug ( "Average Hydropathy Value: {}" , PeptideProperties . getAvgHydropathy ( sequence ) ) ; logger . debug ( "Total time: {}" , PeptideProperties . getTotalTime ( sequence ) ) ; logger . debug ( "Isoelectric Point: {}" , PeptideProperties . getIsoelectricPoint ( sequence ) ) ; logger . debug ( "Net Charge at pH 7: {}" , PeptideProperties . getNetCharge ( sequence ) ) ; }
@ Test public void shortExample5 ( ) { String sequence = "QIKDLLVSSSTDLDTTLVLVNAIYFKGMWKTAFNAEDTRECMPFHVTKQESKPVQMMCMNNSFNVATLPAE" ; logger . debug ( "Absorbance (Cys Reduced): {}" , PeptideProperties . getAbsorbance ( sequence , true ) ) ; logger . debug ( "Absorbance (Cys Removed): {}" , PeptideProperties . getAbsorbance ( sequence , false ) ) ; logger . debug ( "Extinction Coefficient (Cys Reduced): {}" , PeptideProperties . getExtinctionCoefficient ( sequence , true ) ) ; logger . debug ( "Extinction Coefficient (Cys Not Reduced): {}" , PeptideProperties . getExtinctionCoefficient ( sequence , false ) ) ; logger . debug ( "Instability Index: {}" , PeptideProperties . getInstabilityIndex ( sequence ) ) ; logger . debug ( "Apliphatic Index: {}" , PeptideProperties . getApliphaticIndex ( sequence ) ) ; logger . debug ( "Isoelectric Point: {}" , PeptideProperties . getIsoelectricPoint ( sequence ) ) ; logger . debug ( "Network: {}" , PeptideProperties . getSequence ( sequence ) ) ; logger . debug ( "Net Charge at pH 7: {}" , PeptideProperties . getNetCharge ( sequence ) ) ; }
@ Test public void shortExample5 ( ) { String sequence = "QIKDLLVSSSTDLDTTLVLVNAIYFKGMWKTAFNAEDTRECMPFHVTKQESKPVQMMCMNNSFNVATLPAE" ; logger . debug ( "Absorbance (Cys Reduced): {}" , PeptideProperties . getAbsorbance ( sequence , true ) ) ; logger . debug ( "Absorbance (Cys Removed): {}" , PeptideProperties . getAbsorbance ( sequence , false ) ) ; logger . debug ( "Extinction Coefficient (Cys Reduced): {}" , PeptideProperties . getExtinctionCoefficient ( sequence , true ) ) ; logger . debug ( "Extinction Coefficient (Cys Not Reduced): {}" , PeptideProperties . getExtinctionCoefficient ( sequence , false ) ) ; logger . debug ( "Instability Index: {}" , PeptideProperties . getInstabilityIndex ( sequence ) ) ; logger . debug ( "Apliphatic Index: {}" , PeptideProperties . getApliphaticIndex ( sequence ) ) ; logger . debug ( "Average Id: {}" , PeptideProperties . getAverageHydropathy ( sequence ) ) ; logger . debug ( "Average Hydropathy Value: {}" , PeptideProperties . getAvgHydropathy ( sequence ) ) ; logger . debug ( "Net Charge at pH 7: {}" , PeptideProperties . getNetCharge ( sequence ) ) ; }
@ Test public void shortExample5 ( ) { String sequence = "QIKDLLVSSSTDLDTTLVLVNAIYFKGMWKTAFNAEDTRECMPFHVTKQESKPVQMMCMNNSFNVATLPAE" ; logger . debug ( "Absorbance (Cys Reduced): {}" , PeptideProperties . getAbsorbance ( sequence , true ) ) ; logger . debug ( "Absorbance (Cys Removed): {}" , PeptideProperties . getAbsorbance ( sequence , false ) ) ; logger . debug ( "Extinction Coefficient (Cys Reduced): {}" , PeptideProperties . getExtinctionCoefficient ( sequence , true ) ) ; logger . debug ( "Extinction Coefficient (Cys Not Reduced): {}" , PeptideProperties . getExtinctionCoefficient ( sequence , false ) ) ; logger . debug ( "Instability Index: {}" , PeptideProperties . getInstabilityIndex ( sequence ) ) ; logger . debug ( "Apliphatic Index: {}" , PeptideProperties . getApliphaticIndex ( sequence ) ) ; logger . debug ( "Average Length: {}" , PeptideProperties . getLength ( sequence ) ) ; logger . debug ( "Average Hydropathy Value: {}" , PeptideProperties . getAvgHydropathy ( sequence ) ) ; logger . debug ( "Isoelectric Point: {}" , PeptideProperties . getIsoelectricPoint ( sequence ) ) ; }
public void test() { try { InputStream in = BalancerRunner . class . getResourceAsStream ( "release.properties" ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . warn ( "Could not read release.properties" , e ) ; } }
public void test() { try { checkForToken ( console ) ; setResourceLocation ( tokenFile . getCanonicalPath ( ) ) ; salt = "Ge0W@v3-Ro0t-K3y" . getBytes ( "UTF-8" ) ; generateRootKeyFromToken ( ) ; } catch ( final Throwable t ) { logger . warn ( "" , t ) ; } }
public void test() { if ( CommonUtils . isBlank ( value ) ) { logger . debug ( "Loaded property {} with no value" , configurationKey . getName ( ) ) ; return configurationKey . getDefaultValue ( ) ; } else { logger . trace ( "Loaded property {} with value {}" , configurationKey . getName ( ) , configurationKey . getDefaultValue ( ) ) ; } }
public void test() { if ( CommonUtils . isBlank ( value ) ) { logger . trace ( "No value found for property {}, returning default {}" , configurationKey . getName ( ) , configurationKey . getDefaultValue ( ) ) ; return configurationKey . getDefaultValue ( ) ; } else { logger . trace ( "Value found for {} = {}" , configurationKey . getName ( ) , value ) ; } }
public static void handle ( MessagePublisher publisher , PipelinesBalancerMessage message ) throws IOException { log . info ( "Processing message - {}" , message ) ; ObjectMapper mapper = new ObjectMapper ( ) ; PipelinesIndexedMessage m = mapper . readValue ( message . getPayload ( ) , PipelinesIndexedMessage . class ) ; PipelinesIndexedMessage outputMessage = new PipelinesIndexedMessage ( m . getDatasetUuid ( ) , m . getAttempt ( ) , m . getPipelineSteps ( ) , m . getRunner ( ) , m . getExecutionId ( ) ) ; publisher . send ( outputMessage ) ; log . info ( "The message has been sent - {}" , outputMessage ) ; }
public static void handle ( MessagePublisher publisher , PipelinesBalancerMessage message ) throws IOException { log . info ( "Process PipelinesIndexedMessage - {}" , message ) ; ObjectMapper mapper = new ObjectMapper ( ) ; PipelinesIndexedMessage m = mapper . readValue ( message . getPayload ( ) , PipelinesIndexedMessage . class ) ; PipelinesIndexedMessage outputMessage = new PipelinesIndexedMessage ( m . getDatasetUuid ( ) , m . getAttempt ( ) , m . getPipelineSteps ( ) , m . getRunner ( ) , m . getExecutionId ( ) ) ; publisher . send ( outputMessage ) ; log . info ( "Message - {}" , outputMessage ) ; }
public void test() { if ( workflow == null ) { logger . error ( "Workflow id {} not found" , workflowId ) ; return false ; } }
public void test() { if ( pkg . contains ( OPERATION_REVISON_FILE ) ) { byte [ ] revisionByteArray = pkg . getBytes ( OPERATION_REVISON_FILE ) ; code_block = IfStatement ; } else { LOGGER . info ( "Skip tracking " + pkg ) ; } }
public void test() { try { OdfPackage pkg = mTextDocument . getPackage ( ) ; int revisionNo = 0 ; code_block = IfStatement ; revisionNo ++ ; pkg . insert ( operations . toString ( ) . getBytes ( ) , OPERATION_TEXT_FILE_PREFIX + revisionNo + ".txt" , "text/plain" ) ; pkg . insert ( Integer . toString ( revisionNo ) . getBytes ( ) , OPERATION_REVISON_FILE , "text/plain" ) ; } catch ( Exception ex ) { LOGGER . warn ( "Error while writing content of file '" + mTextDocument . getName ( ) + "': " + ex . getMessage ( ) , ex ) ; } }
public void test() { try { resultSet . close ( ) ; } catch ( final SQLException e ) { LOG . error ( "Unable to close result set." , e ) ; } }
@ Test public void get ( ) { Run run = ESSuiteTest . runDao . get ( ESSuiteTest . RUN_ID_3 ) ; LOG . info ( "Commit id: {}" , run . getCommitId ( ) ) ; assertEquals ( ESSuiteTest . COMMIT_ID_3 , run . getCommitId ( ) ) ; assertEquals ( ESSuiteTest . RUNNER_HOSTNAME_1 , run . getRunner ( ) ) ; assertEquals ( ESSuiteTest . RUN_DURATION , ( Long ) run . getActualTime ( ) ) ; }
public void test() { try { int [ ] data = ( int [ ] ) notification . getUserData ( ) ; int repairNo = data [ 0 ] ; ActiveRepairService . Status status = ActiveRepairService . Status . values ( ) [ data [ 1 ] ] ; String message = notification . getMessage ( ) ; code_block = IfStatement ; } catch ( RuntimeException e ) { log . error ( e . getMessage ( ) , e ) ; } }
public AbstractFeatureEntity getFeature ( String identifier , Session session ) { Criteria criteria = getDefaultCriteria ( session ) . add ( Restrictions . eq ( AbstractFeatureEntity . IDENTIFIER , identifier ) ) ; LOGGER . trace ( "QUERY getFeature with identifier : {}" , HibernateHelper . getSqlString ( criteria ) ) ; return ( AbstractFeatureEntity ) criteria . uniqueResult ( ) ; }
public void test() { try { AddressBookId addressBookId = folder . getTypedBackendId ( ) ; return getBookClient ( ) . getContactFromId ( getAccessToken ( ) , addressBookId . getId ( ) , serverId . getItemId ( ) ) != null ; } catch ( ServerFault | ContactNotFoundException e ) { LOG . error ( "Error getting contact from id" , e ) ; } }
public void test() { if ( logger . isTraceEnabled ( LogMarker . SERIALIZER_VERBOSE ) ) { logger . trace ( LogMarker . SERIALIZER_VERBOSE , "Serialize {}" , this ) ; } }
@ Override protected void startJsonRpcServer ( RomServerJsonRpcHandler jsonRpcHandler ) { handler = jsonRpcHandler ; logger . info ( "startJsonRpcServer" ) ; Properties properties = new Properties ( ) ; String port = getPort ( ) ; properties . put ( "server.port" , port ) ; SpringApplication application = new SpringApplication ( BootTestApplication . class ) ; application . setDefaultProperties ( properties ) ; context = application . run ( ) ; }
public void test() { try { database . storeMapping ( storagePath , objectName , objectVersion ) ; logger . trace ( String . format ( "Stored mapping of object %s in %s" , objectName , objectVersion ) ) ; } catch ( AwsAssetDatabaseException e ) { throw new AssetStoreException ( e ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { if ( ! initializer . isDatabaseStructureCreated ( checkSql ) ) { log . info ( "Creating certificate management repository database" ) ; initializer . createRegistryDatabase ( ) ; } else { log . info ( "Certificate management repository database already exists. Not creating a new database." ) ; } }
public void test() { if ( ! initializer . isDatabaseStructureCreated ( checkSql ) ) { log . info ( "Initializing Certificate management repository database schema" ) ; initializer . createRegistryDatabase ( ) ; } else { log . info ( " Certificate management repository database schema already initialized" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
@ Override protected void doInTransactionWithoutResult ( TransactionStatus status ) { emailDao . populateEmailHash ( email , correctedEmailHash ) ; logger . debug ( "Successfully updated email hash for account {}" , email ) ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { log . info ( "Identified failed message {}" , e . getMessage ( ) ) ; } catch ( JMSException e ) { log . warn ( "Could not identify failed message " , e ) ; } }
public void test() { try { log . trace ( "Failed message " + msg . getJMSMessageID ( ) ) ; } catch ( JMSException e ) { log . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( messages != null ) { code_block = ForStatement ; } else { log . warn ( "No messages set" ) ; } }
public void test() { try { listener . onUpdate ( identifiable , attribute , variantId , oldValue , newValue ) ; } catch ( Exception t ) { logger . warn ( "Could not inform listener" , t ) ; } }
public void test() { try { URLConnection connection = IOUtils . getUrlConnection ( documentURI , CONNECTION_ACCEPT_HTTP_COMPRESSION , CONNECTION_TIMEOUT ) ; code_block = IfStatement ; } catch ( MalformedURLException e ) { logger . info ( "Invalid ontology document {}" , ontologyIRI ) ; } catch ( FileNotFoundException e ) { logger . info ( "Imported ontology document {} does not exist on the Web (File Not Found)." , ontologyIRI ) ; } catch ( UnknownHostException e ) { String host = e . getMessage ( ) ; logger . info ( "Imported ontology document {} could not be retrieved. Cannot connect to {} (Unknown Host)." , ontologyIRI , host ) ; } catch ( IOException e ) { logger . info ( "Imported ontology document {} could not be retrieved: {}" , ontologyIRI , e . getMessage ( ) ) ; } }
public void test() { try { URLConnection connection = IOUtils . getUrlConnection ( documentURI , CONNECTION_ACCEPT_HTTP_COMPRESSION , CONNECTION_TIMEOUT ) ; code_block = IfStatement ; } catch ( MalformedURLException e ) { logger . info ( "Imported ontology document IRI {} is malformed." , ontologyIRI ) ; } catch ( FileNotFoundException e ) { logger . info ( "Imported ontology document {} could not be retrieved. Cannot connect to {} (Unknown Host)." , ontologyIRI , host ) ; } catch ( UnknownHostException e ) { String host = e . getMessage ( ) ; logger . info ( "Imported ontology document {} could not be retrieved. Cannot connect to {} (Unknown Host)." , ontologyIRI , host ) ; } catch ( IOException e ) { logger . info ( "Imported ontology document {} could not be retrieved: {}" , ontologyIRI , e . getMessage ( ) ) ; } }
public void test() { try { URLConnection connection = IOUtils . getUrlConnection ( documentURI , CONNECTION_ACCEPT_HTTP_COMPRESSION , CONNECTION_TIMEOUT ) ; code_block = IfStatement ; } catch ( MalformedURLException e ) { logger . info ( "Imported ontology document IRI {} is malformed." , ontologyIRI ) ; } catch ( FileNotFoundException e ) { logger . info ( "Imported ontology document {} does not exist on the Web (File Not Found)." , ontologyIRI ) ; } catch ( UnknownHostException e ) { String host = e . getMessage ( ) ; logger . error ( "Imported ontology document {} could not be retrieved: {}" , ontologyIRI , e . getMessage ( ) ) ; } catch ( IOException e ) { logger . info ( "Imported ontology document {} could not be retrieved: {}" , ontologyIRI , e . getMessage ( ) ) ; } }
public void test() { try { URLConnection connection = IOUtils . getUrlConnection ( documentURI , CONNECTION_ACCEPT_HTTP_COMPRESSION , CONNECTION_TIMEOUT ) ; code_block = IfStatement ; } catch ( MalformedURLException e ) { logger . info ( "Imported ontology document IRI {} is malformed." , ontologyIRI ) ; } catch ( FileNotFoundException e ) { logger . info ( "Imported ontology document {} does not exist on the Web (File Not Found)." , ontologyIRI ) ; } catch ( UnknownHostException e ) { String host = e . getMessage ( ) ; logger . info ( "Imported ontology document {} could not be retrieved. Cannot connect to {} (Unknown Host)." , ontologyIRI , host ) ; } catch ( IOException e ) { logger . info ( "Imported ontology document {} could not be retrieved. Exception: {}" , ontologyIRI , e . getMessage ( ) ) ; } }
public void test() { try ( AsyncHttpClient client = getAsyncHttpClient ( cg ) ) { String redirectTarget = "bar/test1" ; String destinationUrl = new URI ( getTargetUrl ( ) ) . resolve ( redirectTarget ) . toString ( ) ; Response response = client . prepareGet ( getTargetUrl ( ) ) . setHeader ( "X-redirect" , redirectTarget ) . execute ( ) . get ( ) ; assertNotNull ( response ) ; log . info ( "Request url: " + getTargetUrl ( ) ) ; assertEquals ( response . getStatusCode ( ) , 200 ) ; assertEquals ( response . getUri ( ) . toString ( ) , destinationUrl ) ; } }
public void test() { try { TajoMaster master = new TajoMaster ( ) ; TajoConf conf = new TajoConf ( ) ; master . init ( conf ) ; master . start ( ) ; } catch ( Throwable t ) { LOG . error ( t . getMessage ( ) , t ) ; System . exit ( - 1 ) ; } }
public void test() { try { com . liferay . portal . kernel . repository . model . FileEntry returnValue = DLTrashServiceUtil . moveFileEntryToTrash ( fileEntryId ) ; return com . liferay . portal . kernel . repository . model . FileEntrySoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( ! productToMetacardIdMap . loadAllKeys ( ) . contains ( ref ) ) { LOGGER . debug ( "Could not find reference for key {}" , ref ) ; return null ; } }
public void test() { if ( validPass ) { LOGGER . info ( "Using Fedora Client with valid user" ) ; code_block = IfStatement ; return CLIENT_VALID_USER_VALID_PASS ; } else { LOGGER . info ( "Using Fedora Client with valid user, bogus pass" ) ; code_block = IfStatement ; return CLIENT_VALID_USER_BOGUS_PASS ; } }
public void test() { if ( validPass ) { LOGGER . info ( "Using Fedora Client with valid user, valid pass" ) ; code_block = IfStatement ; return CLIENT_VALID_USER_VALID_PASS ; } else { LOGGER . info ( "Using Fedora Client with no user" ) ; code_block = IfStatement ; return CLIENT_VALID_USER_BOGUS_PASS ; } }
public void test() { if ( args . length != ARGS_LENGTH ) { logger . error ( "Invalid arguments: {}" , Arrays . toString ( args ) ) ; return ; } }
@ Test public void skimPomForExistingRepoAndAddItInGroup ( ) throws Exception { RemoteRepository repo = new RemoteRepository ( REPO , server . formatUrl ( REPO ) ) ; repo . setAllowReleases ( Boolean . TRUE ) ; repo . setAllowSnapshots ( Boolean . FALSE ) ; repo = client . stores ( ) . create ( repo , "Pre stored remote repo" , RemoteRepository . class ) ; final StoreKey remoteRepoKey = repo . getKey ( ) ; final PomRef ref = loadPom ( "one-repo" , Collections . singletonMap ( "one-repo.url" , server . formatUrl ( REPO ) ) ) ; server . expect ( "HEAD" , server . formatUrl ( REPO , "/" ) , 200 , ( String ) null ) ; server . expect ( server . formatUrl ( TEST_REPO , ref . path ) , 200 , ref . pom ) ; final StoreKey pubGroupKey = new StoreKey ( MavenPackageTypeDescriptor . MAVEN_PKG_KEY , StoreType . group , PUBLIC ) ; Group g = client . stores ( ) . load ( pubGroupKey , Group . class ) ; assertThat ( "Group membership should not contain implied before getting pom." , g . getConstituents ( ) . contains ( remoteRepoKey ) , equalTo ( false ) ) ; final InputStream stream = client . content ( ) . get ( pubGroupKey , ref . path ) ; final String downloaded = IOUtils . toString ( stream ) ; IOUtils . closeQuietly ( stream ) ; System . out . println ( "Waiting 5s for events to run." ) ; Thread . sleep ( 5000 ) ; g = client . stores ( ) . load ( pubGroupKey , Group . class ) ; assertThat ( "Group membership does not contain implied repository" , g . getConstituents ( ) . contains ( remoteRepoKey ) , equalTo ( true ) ) ; repo = client . stores ( ) . load ( new StoreKey ( MavenPackageTypeDescriptor . MAVEN_PKG_KEY , StoreType . remote , TEST_REPO ) , RemoteRepository . class ) ; String metadata = repo . getMetadata ( ImpliedRepoMetadataManager . IMPLIED_STORES ) ; LOG . info (
public void test() { if ( Math . abs ( millisNow - millisStored ) > TimeUnit . MILLISECONDS . convert ( 1 , TimeUnit . HOURS ) ) { user . setLastAuthenticationTimestamp ( new java . sql . Timestamp ( millisNow ) ) ; context . commitChanges ( ) ; log . info ( "Transaction successfully changed" ) ; } }
public void test() { try { code_block = IfStatement ; Lang lang = this . extractLang ( pageUrl , reqCtx ) ; IPage destPage = this . extractDestPage ( pageUrl , reqCtx ) ; String friendlyCode = ( ( PageURL ) pageUrl ) . getFriendlyCode ( ) ; code_block = IfStatement ; HttpServletRequest request = ( null != reqCtx ) ? reqCtx . getRequest ( ) : null ; String url = null ; code_block = IfStatement ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Error creating url" , e ) ; throw new RuntimeException ( "Error creating url" , e ) ; } }
public void test() { if ( ! graphiteConfiguration . isEnabled ( ) ) { LOGGER . info ( "Not reporting data points to graphite." ) ; return ; } }
@ Override public void start ( ) throws Exception { code_block = IfStatement ; LOGGER . info ( "Starting Graphite" ) ; final Graphite graphite = new Graphite ( new InetSocketAddress ( graphiteConfiguration . getHostname ( ) , graphiteConfiguration . getPort ( ) ) ) ; final GraphiteReporter . Builder reporterBuilder = GraphiteReporter . forRegistry ( registry ) ; code_block = IfStatement ; code_block = IfStatement ; reporter = Optional . of ( reporterBuilder . build ( graphite ) ) ; reporter . get ( ) . start ( graphiteConfiguration . getPeriodSeconds ( ) , TimeUnit . SECONDS ) ; }
@ Override public List < Page > listPagesInCourse ( String courseId ) throws IOException { LOG . debug ( "Retrieving pages for course " + courseId ) ; String url = buildCanvasUrl ( "courses/" + courseId + "/pages" , Collections . emptyMap ( ) ) ; return getListFromCanvas ( url ) ; }
@ Override public Set < ? extends NodeMetadata > listNodesByIds ( Iterable < String > ids ) { checkNotNull ( ids , "ids" ) ; logger . trace ( ">> list(%s)" , ids ) ; Set < NodeMetadata > set = ImmutableSet . copyOf ( listNodesStrategy . listNodesByIds ( ids ) ) ; logger . trace ( "<< list(%d)" , set . size ( ) ) ; return set ; }
@ Override public Set < ? extends NodeMetadata > listNodesByIds ( Iterable < String > ids ) { checkNotNull ( ids , "ids" ) ; logger . trace ( ">> listing node with ids(%s)" , ids ) ; Set < NodeMetadata > set = ImmutableSet . copyOf ( listNodesStrategy . listNodesByIds ( ids ) ) ; logger . trace ( "<<<%s(%s)" , set , set ) ; return set ; }
public void test() { try { Response response = searchLocation ( request . getRequest ( ) , request . getRequestContext ( ) ) ; sender ( ) . tell ( response , self ( ) ) ; SearchDTO searchDto = Util . createSearchDto ( request . getRequest ( ) ) ; String [ ] types = code_block = "" ; ; generateSearchTelemetryEvent ( searchDto , types , response . getResult ( ) , request . getContext ( ) ) ; } catch ( Exception ex ) { logger . error ( ex . getMessage ( ) , ex ) ; sender ( ) . tell ( ex , self ( ) ) ; } }
public void test() { -> { if ( throwable == null ) return CompletableFuture . completedFuture ( rebalancingStatus != RebalancingStatus . SUSPENDED ) ; code_block = IfStatement ; log . error ( "Failed to fetch latest rebalancing status from coordinator" , throwable ) ; return fetchRebalancingStatusFromCoordinator ( attempts - 1 ) ; } }
public void test() { try ( Tx tx = StructrApp . getInstance ( securityContext ) . tx ( ) ) { org . structr . web . entity . User structrUser = getStructrUser ( userName ) ; tx . success ( ) ; code_block = IfStatement ; } catch ( FrameworkException fex ) { logger . error ( "" , fex ) ; } }
public void test() { try { authenticateProxy ( this . connectMethod ) ; } catch ( AuthenticationException e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { for ( String cache : pools ) { code_block = IfStatement ; } }
@ Override public Response toResponse ( SelfManagedOnlyException exception ) { LOG . error ( "Self managed only has been denied" , exception ) ; return Response . status ( Status . FORBIDDEN ) . entity ( new SelfManagedOnlyExceptionInfo ( Status . FORBIDDEN , exception ) ) . build ( ) ; }
public void test() { if ( portId == null ) { logger . debug ( "Could not resolve port [{}] to identifier [{}]" , portName , portId ) ; } else { logger . debug ( "Resolved port [{}] to identifier [{}]" , portName , portId ) ; this . portIdentifier = portId ; } }
@ Override public void rdbBgsave ( Jedis jedis , String host , String port ) { Long currentTime = System . currentTimeMillis ( ) ; LOG . info ( "RCT generate redis rdb file, port: " + oldLastsave ) ; Long oldLastsave = jedis . lastsave ( ) ; jedis . bgsave ( ) ; boolean isCheck = true ; code_block = WhileStatement ; LOG . info ( "RCT generate redis rdb file success. cost time:{} ms" , ( System . currentTimeMillis ( ) - currentTime ) ) ; }
@ Override public void rdbBgsave ( Jedis jedis , String host , String port ) { Long currentTime = System . currentTimeMillis ( ) ; Long oldLastsave = jedis . lastsave ( ) ; LOG . info ( "RCT start to generate redis rdb file.{}:{}" , host , port ) ; jedis . bgsave ( ) ; boolean isCheck = true ; code_block = WhileStatement ; LOG . info ( "RCT end to generate redis rdb file.{}:{}" , host , port ) ; }
@ Ignore @ Test public void shouldBeFasterWhenRunningProcessingInParallel ( ) throws Exception { testMojoWithConfigurableWroManagerFactoryWithValidConfigFileSet ( ) ; final long begin = System . currentTimeMillis ( ) ; victim . setParallelProcessing ( false ) ; testMojoWithConfigurableWroManagerFactoryWithValidConfigFileSet ( ) ; final long endSerial = System . currentTimeMillis ( ) ; victim . setParallelProcessing ( true ) ; testMojoWithConfigurableWroManagerFactoryWithValidConfigFileSet ( ) ; final long endParallel = System . currentTimeMillis ( ) ; final long serial = endSerial - begin ; final long parallel = endParallel - endSerial ; LOG . info ( "serial took: {}ms" , serial ) ; LOG . info ( "parallel took: {}ms" , parallel ) ; assertTrue ( String . format ( "Serial (%s) > Parallel (%s)" , serial , parallel ) , serial > parallel ) ; }
@ Ignore @ Test public void shouldBeFasterWhenRunningProcessingInParallel ( ) throws Exception { testMojoWithConfigurableWroManagerFactoryWithValidConfigFileSet ( ) ; final long begin = System . currentTimeMillis ( ) ; victim . setParallelProcessing ( false ) ; testMojoWithConfigurableWroManagerFactoryWithValidConfigFileSet ( ) ; final long endSerial = System . currentTimeMillis ( ) ; victim . setParallelProcessing ( true ) ; testMojoWithConfigurableWroManagerFactoryWithValidConfigFileSet ( ) ; final long endParallel = System . currentTimeMillis ( ) ; LOG . info ( " parallel took: {}ms" , endParallel ) ; final long serial = endSerial - begin ; final long parallel = endParallel - endSerial ; LOG . info ( "serial took: {}ms" , serial ) ; assertTrue ( String . format ( "Serial (%s) > Parallel (%s)" , serial , parallel ) , serial > parallel ) ; }
public void test() { try { jsonText = mapper . writerWithDefaultPrettyPrinter ( ) . writeValueAsString ( jsonObj ) ; } catch ( IOException e ) { log . error ( "Unable to convert to json object" , e ) ; } }
public ClusterStatus checkStatus ( ) { log . info ( "Checking cluster status" ) ; List < Result > pollResults = poll ( ) ; Set < String > partitions = new HashSet < String > ( ) ; Set < String > incompleteJoinNodes = new HashSet < String > ( ) ; Set < String > incompleteStateTransferNodes = new HashSet < String > ( ) ; int numAvailableNodes = 0 ; int numMembers = - 1 ; boolean numMembersEqual = true ; code_block = ForStatement ; return new ClusterStatus ( numAvailableNodes , ( numMembersEqual ? numMembers : - 1 ) , partitions , incompleteJoinNodes , incompleteStateTransferNodes ) ; }
public void test() { if ( r . connectError != null ) { logger . error ( "Connect error" , r . connectError ) ; } }
public void test() { if ( r . pollError != null ) { logger . error ( r . pollError ) ; } }
@ Test public void findExperimentRunsNegativeTest ( ) { LOGGER . info ( "FindExperimentRuns Negative test start................................" ) ; FindExperimentRuns findExperimentRuns ; code_block = TryStatement ;  code_block = TryStatement ;  LOGGER . info ( "FindExperimentRuns Negative test stop................................" ) ; }
public void test() { try { List < String > experimentRunIds = new ArrayList < > ( ) ; experimentRunIds . add ( "abc" ) ; experimentRunIds . add ( "xyz" ) ; findExperimentRuns = FindExperimentRuns . newBuilder ( ) . addAllExperimentRunIds ( experimentRunIds ) . build ( ) ; experimentRunServiceStub . findExperimentRuns ( findExperimentRuns ) ; fail ( ) ; } catch ( StatusRuntimeException exc ) { Status status = Status . fromThrowable ( exc ) ; LOGGER . warn ( "Error Code : " + status . getCode ( ) + " Description : " + status . getDescription ( ) ) ; assertEquals ( Status . PERMISSION_DENIED . getCode ( ) , status . getCode ( ) ) ; } }
@ Test public void findExperimentRunsNegativeTest ( ) { LOGGER . info ( "FindExperimentRuns Negative test start................................" ) ; FindExperimentRuns findExperimentRuns ; code_block = TryStatement ;  code_block = TryStatement ;  LOGGER . info ( "FindExperimentRuns Negative test stop................................" ) ; }
public void test() { if ( ! appFolderPath . exists ( ) ) { LOGGER . error ( "Directory path does not exist: " + appFolderPath ) ; return appMap ; } }
public void test() { if ( ! appFolderPath . isDirectory ( ) ) { LOGGER . warn ( "{} is not a directory" , appFolderPath ) ; } else { File [ ] listFiles = appFolderPath . listFiles ( ) ; code_block = IfStatement ; } }
public void test() { if ( listFiles == null ) { LOG . error ( "Cannot find files in directory: " + path ) ; } else { code_block = ForStatement ; } }
public void test() { if ( ! folder . isDirectory ( ) ) { logger . warn ( "Folder {} is not a directory" , folder ) ; } else { File appManifest = new File ( folder , "manifest.webapp" ) ; code_block = IfStatement ; } }
public void test() { if ( ! appManifest . exists ( ) ) { logger . info ( "Manifest {} does not exist, it cannot be read" , appManifestPath ) ; } else { code_block = TryStatement ;  } }
public void test() { try { App app = mapper . readValue ( appManifest , App . class ) ; app . setFolderName ( folder . getName ( ) ) ; app . setAppStorageSource ( AppStorageSource . LOCAL ) ; appList . add ( app ) ; } catch ( IOException ex ) { logger . warn ( "Failed to parse app metadata: " + appManifest , ex ) ; } }
public void test() { if ( appList . isEmpty ( ) ) { logger . debug ( "No apps found" ) ; } }
private OAuthUser getAuthParams ( AuthInfo authInfo , String code , OAuthServer server ) throws IOException , InterruptedException { String requestInfoUrl = prepareUrl ( server . getRequestInfoUrl ( ) , getParams ( server , code , authInfo ) ) ; HttpRequest . Builder builder = setNoCache ( HttpRequest . newBuilder ( ) . uri ( URI . create ( requestInfoUrl ) ) ) ; code_block = IfStatement ; String json = doRequest ( builder . build ( ) ) ; logger . debug ( "RequestInfo: {}" , json ) ; return new OAuthUser ( json , server ) ; }
public void test() { try { logger . debug ( "Start write to HTML" ) ; resultWriter . write ( analysisResult , _configuration , writer ) ; logger . debug ( "End write to HTML" ) ; } finally { FileHelper . safeClose ( writer ) ; } }
public void test() { try { logger . debug ( "Begin write to HTML" ) ; resultWriter . write ( analysisResult , _configuration , writer ) ; logger . debug ( "End write to HTML" ) ; } finally { FileHelper . safeClose ( writer ) ; } }
public void test() { try { get ( ) ; } catch ( final ExecutionException e ) { final Throwable cause = e . getCause ( ) ; logger . warn ( "Error writing result to HTML page" , cause ) ; WidgetUtils . showErrorMessage ( "Error writing result to HTML page" , cause ) ; } catch ( final InterruptedException e ) { logger . warn ( "Unexpected interrupt in done() method!" ) ; } }
public void test() { try { get ( ) ; } catch ( final ExecutionException e ) { logger . error ( "ExecutionException occurred while getting the result of the HTML rendering" , e ) ; final Throwable cause = e . getCause ( ) ; WidgetUtils . showErrorMessage ( "Error writing result to HTML page" , cause ) ; } catch ( final InterruptedException e ) { logger . error ( "ExecutionException occurred while getting the result of the HTML rendering" , e ) ; } }
public void test() { try { AuditLogEventRequest auditRequest = AuditEventMapper . fromHttpServletRequest ( request ) ; code_block = IfStatement ; code_block = IfStatement ; notificationList = notificationService . getNotificationList ( 0 , "" ) ; code_block = ForStatement ; map . addAttribute ( "notificationList" , notificationList ) ; auditLogEventHelper . logEvent ( APP_LEVEL_NOTIFICATION_LIST_VIEWED , auditRequest ) ; mav = new ModelAndView ( "notificationListPage" , map ) ; } catch ( Exception e ) { logger . error ( "Error getting notification list" , e ) ; } }
public void test() { if ( size > 0 ) { code_block = ForStatement ; } else { log . info ( "Skipping table [" + tableName + "]" ) ; } }
@ Override public InstancesInfo getApplicationInstances ( CloudApplication app ) { logger . debug ( Messages . GETTING_INSTANCE_0 , app ) ; return delegate . getApplicationInstances ( app ) ; }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( "Closing connection {}" , connection ) ; } }
public void test() { try { key . setPrivateKey ( privateKey , newCipherFactory ) ; } catch ( final GeneralSecurityException e ) { LOGGER . error ( "Error setting private key." , e ) ; iter . remove ( ) ; continue ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { if ( HttpStatus . getCode ( contentResponse . getStatus ( ) ) . isSuccess ( ) ) { String content = contentResponse . getContentAsString ( ) ; logger . debug ( "Online check success: {}" , content ) ; return true ; } else { logger . debug ( "Online check failed with status code: {}" , contentResponse . getStatus ( ) ) ; return false ; } }
public void test() { if ( HttpStatus . getCode ( contentResponse . getStatus ( ) ) . isSuccess ( ) ) { String content = contentResponse . getContentAsString ( ) ; logger . debug ( "Online check completed with success: {} - status code: {}" , content , contentResponse . getStatus ( ) ) ; return true ; } else { logger . debug ( "Online check failed: {}" , contentResponse . getStatus ( ) ) ; return false ; } }
public void test() { try { String url = this . getPublicInformationUrl ( ) ; Request request = this . createRequest ( url , GET ) ; ContentResponse contentResponse = request . send ( ) ; code_block = IfStatement ; } catch ( TimeoutException | ExecutionException | NullPointerException e ) { logger . error ( "Error occured while querying device" , e ) ; return false ; } }
public void test() { if ( workspacesBackupLocks . putIfAbsent ( workspaceId , lock ) != null ) { String err = "Restore of workspace " + workspaceId + " failed. Another restore process of the same workspace is in progress" ; logger . error ( err ) ; throw new ServerException ( err ) ; } }
public void test() { try { code_block = IfStatement ; Files . createDirectories ( Paths . get ( srcPath ) ) ; CommandLine commandLine = new CommandLine ( restoreScript , srcPath , destinationPath , destAddress , Integer . toString ( destPort ) , destUserId , destGroupId , destUserName ) ; executeCommand ( commandLine . asArray ( ) , restoreDuration , destAddress , workspaceId , RESTORE_SUCCESS_RETURN_CODES ) ; restored = true ; } catch ( TimeoutException e ) { throw new ServerException ( "Restoring of workspace " + workspaceId + " filesystem terminated due to timeout on " + destAddress + " node." ) ; } catch ( InterruptedException e ) { Thread . currentThread ( ) . interrupt ( ) ; throw new ServerException ( "Restoring of workspace " + workspaceId + " filesystem interrupted on " + destAddress + " node." ) ; } catch ( IOException e ) { String error = "Restoring of workspace " + workspaceId + " filesystem terminated on " + destAddress + " node. " + e . getLocalizedMessage ( ) ; LOG . error ( error ) ; throw new ServerException ( error ) ; } finally { lock . unlock ( ) ; code_block = IfStatement ; } }
public static String getTaskManagerShellCommand ( Configuration flinkConfig , ContaineredTaskManagerParameters tmParams , String configDirectory , String logDirectory , boolean hasLogback , boolean hasLog4j , boolean hasKrb5 , Class < ? > mainClass , String mainArgs ) { final Map < String , String > startCommandValues = new HashMap < > ( ) ; startCommandValues . put ( "java" , "$JAVA_HOME/bin/java" ) ; final TaskExecutorProcessSpec taskExecutorProcessSpec = tmParams . getTaskExecutorProcessSpec ( ) ; startCommandValues . put ( "jvmmem" , ProcessMemoryUtils . generateJvmParametersStr ( taskExecutorProcessSpec ) ) ; String javaOpts = flinkConfig . getString ( CoreOptions . FLINK_JVM_OPTIONS ) ; code_block = IfStatement ; code_block = IfStatement ; startCommandValues . put ( "jvmopts" , javaOpts ) ; String logging = "" ; code_block = IfStatement ; startCommandValues . put ( "logging" , logging ) ; startCommandValues . put ( "class" , mainClass . getName ( ) ) ; startCommandValues . put ( "redirects" , "1> " + logDirectory + "/taskmanager.out " + "2> " + logDirectory + "/taskmanager.err" ) ; String argsStr = TaskExecutorProcessUtils . generateDynamicConfigsStr ( taskExecutorProcessSpec ) + " --configDir " + configDirectory ; code_block = IfStatement ; startCommandValues . put ( "args" , argsStr ) ; final String commandTemplate = flinkConfig . getString ( ConfigConstants . YARN_CONTAINER_START_COMMAND_TEMPLATE , ConfigConstants . DEFAULT_YARN_CONTAINER_START_COMMAND_TEMPLATE ) ; String startCommand = getStartCommand ( commandTemplate , startCommandValues ) ; log . info ( "Start Command: " + startCommand ) ; return startCommand ; }
@ Test public void test_12_BRCA_Splice_15_Hgvs ( ) { Log . debug ( "Test" ) ; int spliceSize = 15 ; String genome = "test_BRCA" ; String vcf = path ( "test_BRCA_splice_15.vcf" ) ; String args [ ] = code_block = "" ; ; SnpEffCmdEff snpeff = new SnpEffCmdEff ( ) ; snpeff . parseArgs ( args ) ; snpeff . setDebug ( debug ) ; snpeff . setVerbose ( verbose ) ; snpeff . setSupressOutput ( ! verbose ) ; snpeff . setFormatVersion ( EffFormatVersion . FORMAT_EFF_4 ) ; snpeff . setSpliceSiteSize ( spliceSize ) ; snpeff . setUpDownStreamLength ( 0 ) ; snpeff . setShiftHgvs ( false ) ; List < VcfEntry > results = snpeff . run ( true ) ; VcfEntry ve = results . get ( 0 ) ; boolean ok = false ; code_block = ForStatement ; Assert . assertTrue ( ok ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void persist ( StgMsCmState transientInstance ) { log . debug ( "persisting StgMsCmState instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . persist ( transientInstance ) ; log . debug ( "persist successful" ) ; } catch ( RuntimeException re ) { log . error ( "persist failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . persist ( transientInstance ) ; log . debug ( "persist successful" ) ; } catch ( RuntimeException re ) { log . error ( "persist failed" , re ) ; throw re ; } }
public void addLogln ( String message ) { progressStatus . append ( message ) ; progressStatus . append ( NEWLINE ) ; logger . info ( message ) ; }
public void test() { try { sourceType = PTNetworkSourceTypeEnum . valueOf ( firstLetterUpcase ( netexSourceType ) ) ; } catch ( Exception e ) { log . error ( "unable to translate " + netexSourceType + " as PTNetworkSourceType" ) ; } }
public void test() { if ( Log . isDebugEnabled ( ) ) { Log . debug ( "User does not want to leave unsaved page. Nothing to do." ) ; } }
public void test() { try { responseJson = PacHttpUtils . doHttpPost ( urlToQuery . toString ( ) , requestBody . toString ( ) ) ; } catch ( Exception e ) { LOGGER . error ( ERROR_REQUEST , e ) ; throw e ; } }
private ModulesConfigurationProperties getModulesConfigurationPropertiesFallback ( String serviceName , Throwable e ) { e . printStackTrace ( ) ; logger . error ( "Failed to get modules configuration." ) ; return null ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( ( ! closing ) && ( ! closed ) ) { logger . warn ( toString ( ) ) ; } }
public void test() { try { connection = createPhysicalConnection ( ) ; } catch ( SQLException e ) { LOG . error ( "create connection SQLException" , e ) ; errorCount ++ ; code_block = IfStatement ; } catch ( RuntimeException e ) { LOG . error ( "create connection RuntimeException" , e ) ; setFailContinuous ( true ) ; continue ; } catch ( Error e ) { LOG . error ( "create connection Error" , e ) ; setFailContinuous ( true ) ; break ; } }
public void test() { try { connection = createPhysicalConnection ( ) ; } catch ( SQLException e ) { LOG . error ( "create connection SQLException, url: " + jdbcUrl + ", errorCode " + e . getErrorCode ( ) + ", state " + e . getSQLState ( ) , e ) ; errorCount ++ ; code_block = IfStatement ; } catch ( RuntimeException e ) { LOG . error ( "create connection RuntimeException" , e ) ; setFailContinuous ( true ) ; continue ; } catch ( Error e ) { LOG . error ( "create connection Error" , e ) ; setFailContinuous ( true ) ; break ; } }
public void test() { try { connection = createPhysicalConnection ( ) ; } catch ( SQLException e ) { LOG . error ( "create connection SQLException, url: " + jdbcUrl + ", errorCode " + e . getErrorCode ( ) + ", state " + e . getSQLState ( ) , e ) ; errorCount ++ ; code_block = IfStatement ; } catch ( RuntimeException e ) { LOG . error ( "create connection RuntimeException" , e ) ; setFailContinuous ( true ) ; continue ; } catch ( Error e ) { LOG . error ( "create connection Error" , e ) ; setFailContinuous ( true ) ; break ; } }
public void test() { try { IEntityManager entityManager = this . getEntityManager ( ) ; Map < String , AttributeInterface > attributeTypes = entityManager . getEntityAttributePrototypes ( ) ; Iterator < AttributeInterface > attributeIter = attributeTypes . values ( ) . iterator ( ) ; code_block = WhileStatement ; Collections . sort ( attributes , new BeanComparator ( "type" ) ) ; } catch ( Throwable t ) { _logger . error ( "Error while extracting Allowed Nested Types" , t ) ; throw new RuntimeException ( "Error while extracting Allowed Nested Types" , t ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Refreshing." ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Refreshing." ) ; } }
public void test() { if ( SortOrder . ASCENDING . equals ( sortOrder ) ) { sortPropertyType . setSortOrder ( SortOrderType . ASC ) ; } else-if ( SortOrder . DESCENDING . equals ( sortOrder ) ) { sortPropertyType . setSortOrder ( SortOrderType . DESC ) ; } else { logger . error ( "Invalid sort order: {}" , sortOrder ) ; return null ; } }
public void test() { try ( CancellableInputStream cancellableStream = new CancellableInputStream ( inStream , emptyHandler ) ) { Iterator < VideoDecoder > decodersIterator = ServiceLoader . load ( VideoDecoder . class ) . iterator ( ) ; code_block = IfStatement ; VideoDecoder videoDecoder = decodersIterator . next ( ) ; videoDecoder . setInputStream ( cancellableStream , TimeInstant . get ( startTime ) ) ; videoDecoder . registerVideoContentHandler ( new OSHVideoContentHandler ( dataType , tracker ) , null ) ; videoDecoder . decode ( ) ; } catch ( IOException | VideoDecoderException e ) { logger . warn ( "Failed to decode video" , e ) ; } }
public void test() { if ( signInDetails != null ) { UserDetails user = userDetailsService . loadUserByUsername ( signInDetails . getConnectionData ( ) . getProviderId ( ) + Constants . USER_NAME_SPLITTER + signInDetails . getUserId ( ) ) ; code_block = IfStatement ; updateUserKeys ( signInDetails . getConnectionData ( ) , signInDetails . getUserId ( ) ) ; return authenticationFactory . createAuthenticationFromUserDetails ( user ) ; } else-if ( allowRepeatedAuthenticationAttempts && alreadyAuthenticatedUserId != null ) { LOGGER . debug ( "SpringSocialSecurity sign in configured authenticate" ) ; return SecurityContextHolder . getContext ( ) . getAuthentication ( ) ; } else { throw new InsufficientAuthenticationException ( "SpringSocialSecurity sign in details not found in session" ) ; } }
@ Override protected void before ( ) throws Throwable { log . debug ( "Starting up test server" ) ; server = H2ServerBootstrap . bootstrap ( ) . setLookupRegistry ( new UriPatternMatcher < > ( ) ) . setVersionPolicy ( HttpVersionPolicy . NEGOTIATE ) . setIOReactorConfig ( IOReactorConfig . custom ( ) . setSoTimeout ( TIMEOUT ) . build ( ) ) . setTlsStrategy ( scheme == URIScheme . HTTPS ? new H2ServerTlsStrategy ( SSLTestContexts . createServerSSLContext ( ) ) : null ) . setStreamListener ( LoggingHttp1StreamListener . INSTANCE_SERVER ) . setStreamListener ( LoggingH2StreamListener . INSTANCE ) . setIOSessionDecorator ( LoggingIOSessionDecorator . INSTANCE ) . setExceptionCallback ( LoggingExceptionCallback . INSTANCE ) . setIOSessionListener ( LoggingIOSessionListener . INSTANCE ) . register ( "*" , ( ) -> new EchoHandler ( 2048 ) ) . create ( ) ; }
public void test() { else-if ( candidates . size ( ) > 1 ) { StringBuffer buf = new StringBuffer ( ) ; code_block = ForStatement ; LOG . warn ( buf . toString ( ) ) ; } }
@ Test ( description = "Deploy autoscaling policy" , timeOut = APPLICATION_TEST_TIMEOUT ) public void testAutoscalingPolicy ( ) throws Exception { String policyId = "autoscaling-policy-autoscaling-policy-test" ; boolean added = restClient . addEntity ( RESOURCES_PATH + RestConstants . AUTOSCALING_POLICIES_PATH + "/" + policyId + ".json" , RestConstants . AUTOSCALING_POLICIES , RestConstants . AUTOSCALING_POLICIES_NAME ) ; assertTrue ( String . format ( "Autoscaling policy did not added: [autoscaling-policy-id] %s" , policyId ) , added ) ; AutoscalePolicyBean bean = ( AutoscalePolicyBean ) restClient . getEntity ( RestConstants . AUTOSCALING_POLICIES , policyId , AutoscalePolicyBean . class , RestConstants . AUTOSCALING_POLICIES_NAME ) ; assertEquals ( String . format ( "[autoscaling-policy-id] %s is not correct" , bean . getId ( ) ) , bean . getId ( ) , policyId ) ; assertEquals ( String . format ( "[autoscaling-policy-id] %s RIF is not correct" , policyId ) , bean . getLoadThresholds ( ) . getRequestsInFlight ( ) . getThreshold ( ) , 35.0 , 0.0 ) ; assertEquals ( String . format ( "[autoscaling-policy-id] %s Memory is not correct" , policyId ) , bean . getLoadThresholds ( ) . getMemoryConsumption ( ) . getThreshold ( ) , 45.0 , 0.0 ) ; assertEquals ( String . format ( "[autoscaling-policy-id] %s Load is not correct" , policyId ) , bean . getLoadThresholds ( ) . getLoadAverage ( ) . getThreshold ( ) , 25.0 , 0.0 ) ; boolean updated = restClient . updateEntity ( RESOURCES_PATH + RestConstants . AUTOSCALING_POLICIES_PATH + "/" + policyId + "-v1.json" , RestConstants . AUTOSC
@ Test ( description = "Deploy autoscaling policy" , timeOut = APPLICATION_TEST_TIMEOUT ) public void testAutoscalingPolicy ( ) throws Exception { log . info ( "-------------------------Started autoscaling policy test case-------------------------" ) ; String policyId = "autoscaling-policy-autoscaling-policy-test" ; boolean added = restClient . addEntity ( RESOURCES_PATH + RestConstants . AUTOSCALING_POLICIES_PATH + "/" + policyId + ".json" , RestConstants . AUTOSCALING_POLICIES , RestConstants . AUTOSCALING_POLICIES_NAME ) ; assertTrue ( String . format ( "Autoscaling policy did not added: [autoscaling-policy-id] %s" , policyId ) , added ) ; AutoscalePolicyBean bean = ( AutoscalePolicyBean ) restClient . getEntity ( RestConstants . AUTOSCALING_POLICIES , policyId , AutoscalePolicyBean . class , RestConstants . AUTOSCALING_POLICIES_NAME ) ; assertEquals ( String . format ( "[autoscaling-policy-id] %s is not correct" , bean . getId ( ) ) , bean . getId ( ) , policyId ) ; assertEquals ( String . format ( "[autoscaling-policy-id] %s RIF is not correct" , policyId ) , bean . getLoadThresholds ( ) . getRequestsInFlight ( ) . getThreshold ( ) , 35.0 , 0.0 ) ; assertEquals ( String . format ( "[autoscaling-policy-id] %s Memory is not correct" , policyId ) , bean . getLoadThresholds ( ) . getMemoryConsumption ( ) . getThreshold ( ) , 45.0 , 0.0 ) ; assertEquals ( String . format ( "[autoscaling-policy-id] %s Load is not correct" , policyId ) , bean . getLoadThresholds ( ) . getLoadAverage ( ) . getThreshold ( ) , 25.0 , 0.0 ) ; boolean updated = restClient . updateEntity ( RESOURCES_PATH + RestConstants . AUTOSCALING_POLICIES_PATH + "/
public void test() { if ( state == PlayerState . PREPARED || state == PlayerState . PAUSED || state == PlayerState . STOPPED || state == PlayerState . LAGGING || state == PlayerState . PLAYBACK_COMPLETE ) { code_block = IfStatement ; exoPlayer . setPlayWhenReady ( true ) ; code_block = IfStatement ; state = PlayerState . PLAYING ; logger . debug ( "Playback started" ) ; showController ( ) ; requestAccessibilityFocusPausePlay ( ) ; } else { logger . debug ( "Cannot play any player" ) ; } }
synchronized void start ( ) { if ( ! isEnabled ( ) || started ) return ; pageSize = ctx . igniteConfiguration ( ) . getDataStorageConfiguration ( ) . getPageSize ( ) ; log . info ( "Starting page size" ) ; pageMemoryMock = mockPageMemory ( ) ; GridCacheSharedContext sharedCtx = gridCtx . cache ( ) . context ( ) ; long maxMemorySize = 0 ; code_block = ForStatement ; long [ ] chunks = new long [ ] code_block = "" ; ; memoryProvider = new UnsafeMemoryProvider ( log ) ; memoryProvider . initialize ( chunks ) ; memoryRegion = memoryProvider . nextRegion ( ) ; GridUnsafe . setMemory ( memoryRegion . address ( ) , memoryRegion . size ( ) , ( byte ) 0 ) ; maxPages = ( int ) ( maxMemorySize / pageSize ) ; pageSlots = new DirectMemoryPageSlot [ maxPages ] ; freeSlotsCnt = maxPages ; tmpBuf1 = ByteBuffer . allocateDirect ( pageSize ) ; tmpBuf2 = ByteBuffer . allocateDirect ( pageSize ) ; code_block = IfStatement ; lastPageIdx = 0 ; started = true ; }
@ Incoming ( "my-server" ) public CompletionStage < Void > source ( MqttMessage message ) { LOG . info ( "Received ack message" ) ; return message . ack ( ) ; }
public void test() { if ( ! emitted . contains ( msgId ) ) { log . debug ( "The message id {} is already queued for retry while being acked." , msgId ) ; } else { Validate . isTrue ( ! retryService . isScheduled ( msgId ) , "The message id " + msgId + " is queued for retry while being acked." + " This should never occur barring errors in the RetryService implementation or the spout code." ) ; offsetManagers . get ( msgId . getTopicPartition ( ) ) . addToAckMsgs ( msgId ) ; emitted . remove ( msgId ) ; } }
public void test() { try ( FileOutputStream output = new FileOutputStream ( pipe , true ) ; ) { output . write ( result . getBytes ( ) ) ; output . flush ( ) ; output . close ( ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { return new URI ( string ) ; } catch ( final URISyntaxException e ) { log . error ( e . getMessage ( ) , e ) ; } }
protected void onLinkAddedPost ( final String networkId , final Link link , final HashMap < String , Response > respList ) { log . debug ( "" ) ; }
public void test() { try { FindHuiUrls command = new FindHuiUrls ( allIDs ) ; command = ServiceFactory . lookupCommandService ( ) . executeCommand ( command ) ; result = command . getList ( ) ; } catch ( Exception e ) { logger . error ( "Error while finding hui URLs" , e ) ; } }
public CancellableCompletableFuture < Void > monitor ( Supplier < Boolean > condition , int checkInterval , int timeout , TimeUnit timeUnit ) { LOG . debug ( "Monitoring" ) ; return scheduledExecutor . scheduleWithFixedDelayAndTimeout ( condition , 0L , checkInterval , timeout , timeUnit ) ; }
public void test() { try { Utils . moveToDirectory ( destinationDir , file ) ; } catch ( Exception ex ) { LOGGER . error ( ex . getMessage ( ) , ex ) ; } }
private void parseHeartbeatMode ( HeartbeatExtensionMessage msg ) { msg . setHeartbeatMode ( parseByteArrayField ( ExtensionByteLength . HEARTBEAT_MODE ) ) ; LOGGER . debug ( "HeartbeatMode: " + msg . getHeartbeatMode ( ) . getValue ( ) ) ; }
@ GetMapping ( CommonConstants . PATH_LEVELS ) public List < String > getLevels ( final Optional < String > criteria ) { LOGGER . debug ( "getLevels( criteria={})" , criteria ) ; RestUtils . checkCriteria ( criteria ) ; return internalProfileService . getLevels ( criteria ) ; }
public void test() { try { RoleRestClient . setAnyLayout ( wrapper . getKey ( ) , wrapper . getContent ( ) ) ; SyncopeConsoleSession . get ( ) . success ( getString ( Constants . OPERATION_SUCCEEDED ) ) ; modal . show ( false ) ; modal . close ( target ) ; } catch ( Exception e ) { LOG . error ( "While updating console layout for role {}" , wrapper . getKey ( ) , e ) ; SyncopeConsoleSession . get ( ) . onException ( e ) ; } }
public void test() { try { fileDescriptor = irodsFileSystemAO . createFile ( getAbsolutePath ( ) , DataObjInp . OpenFlags . READ_WRITE , 0600 ) ; } catch ( JargonFileOrCollAlreadyExistsException e ) { return false ; } catch ( ResourceHierarchyException rhe ) { log . warn ( "failed to create file" , rhe ) ; return false ; } catch ( JargonException e ) { String msg = "JargonException caught and rethrown as IOException:" + e . getMessage ( ) ; log . error ( msg , e ) ; throw new IOException ( e ) ; } }
public void test() { try { fileDescriptor = irodsFileSystemAO . createFile ( getAbsolutePath ( ) , DataObjInp . OpenFlags . READ_WRITE , 0600 ) ; log . debug ( "file descriptor from new file create: {}" , fileDescriptor ) ; } catch ( JargonFileOrCollAlreadyExistsException e ) { return false ; } catch ( ResourceHierarchyException rhe ) { return false ; } catch ( JargonException e ) { String msg = "JargonException caught and rethrown as IOException:" + e . getMessage ( ) ; log . error ( msg , e ) ; throw new IOException ( e ) ; } }
public void test() { try { TimeSpan span = TimeSpan . fromISO8601String ( time . getCurrent ( ) ) ; resetPlanIfNecessary ( span ) ; myTimeManager . setPrimaryActiveTimeSpan ( span ) ; } catch ( ParseException e ) { LOGGER . error ( "Failed to parse time span." , e ) ; } }
public void test() { if ( ! actionReturnValue . getSucceeded ( ) ) { _log . error ( "Output" ) ; getReturnValue ( ) . setFault ( actionReturnValue . getFault ( ) ) ; return ; } }
public void test() { try { List < WikiObjectComponentBuilder > componentBuilders = this . contextComponent . getInstanceList ( WikiObjectComponentBuilder . class ) ; code_block = ForStatement ; } catch ( ComponentLookupException e ) { this . logger . error ( "Failed to lookup the wiki component builder" , e ) ; } }
public void test() { try { return mapper . readValue ( jsonstr , type ) ; } catch ( IOException e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { -> { LOGGER . info ( "Setting server to {}" , kafka . getSpec ( ) . toString ( ) ) ; kafka . getSpec ( ) . setCruiseControl ( null ) ; } }
@ ParallelNamespaceTest void testDeployAndUnDeployCruiseControl ( ExtensionContext extensionContext ) throws IOException { final String namespaceName = extensionContext . getStore ( ExtensionContext . Namespace . GLOBAL ) . get ( Constants . NAMESPACE_KEY ) . toString ( ) ; final String clusterName = mapWithClusterNames . get ( extensionContext . getDisplayName ( ) ) ; resourceManager . createResource ( extensionContext , KafkaTemplates . kafkaWithCruiseControl ( clusterName , 3 , 3 ) . build ( ) ) ; Map < String , String > kafkaPods = StatefulSetUtils . ssSnapshot ( namespaceName , KafkaResources . kafkaStatefulSetName ( clusterName ) ) ; KafkaResource . replaceKafkaResourceInSpecificNamespace ( clusterName , kafka code_block = LoopStatement ; , namespaceName ) ; StatefulSetUtils . waitTillSsHasRolled ( namespaceName , kafkaStatefulSetName ( clusterName ) , 3 , kafkaPods ) ; assertThat ( KafkaResource . kafkaClient ( ) . inNamespace ( namespaceName ) . withName ( clusterName ) . get ( ) . getSpec ( ) . getCruiseControl ( ) , nullValue ( ) ) ; LOGGER . info ( "Verifying that {} pod is not present" , clusterName + "-cruise-control-" ) ; PodUtils . waitUntilPodStabilityReplicasCount ( namespaceName , clusterName + "-cruise-control-" , 0 ) ; LOGGER . info ( "Verifying that in Kafka config map there is no configuration to cruise control metric reporter" ) ; assertThrows ( WaitException . class , ( ) -> CruiseControlUtils . verifyCruiseControlMetricReporterConfigurationInKafkaConfigMapIsPresent ( CruiseControlUtils . getKafkaCruiseControlMetricsReporterConfiguration ( namespaceName , clusterName ) ) ) ; LOGGER . info ( "Cruise Control topics will not be deleted and will stay in the Kafka cluster" ) ; CruiseControlUtils . verifyThatCruiseControlTopicsArePresent ( namespaceName ) ; kafkaPods = StatefulSetUtils . ssSnapshot ( namespaceName , KafkaResources . kafkaStatefulSetName ( clusterName ) ) ; KafkaResource . replaceKafkaResource
@ ParallelNamespaceTest void testDeployAndUnDeployCruiseControl ( ExtensionContext extensionContext ) throws IOException { final String namespaceName = extensionContext . getStore ( ExtensionContext . Namespace . GLOBAL ) . get ( Constants . NAMESPACE_KEY ) . toString ( ) ; final String clusterName = mapWithClusterNames . get ( extensionContext . getDisplayName ( ) ) ; resourceManager . createResource ( extensionContext , KafkaTemplates . kafkaWithCruiseControl ( clusterName , 3 , 3 ) . build ( ) ) ; Map < String , String > kafkaPods = StatefulSetUtils . ssSnapshot ( namespaceName , KafkaResources . kafkaStatefulSetName ( clusterName ) ) ; KafkaResource . replaceKafkaResourceInSpecificNamespace ( clusterName , kafka code_block = LoopStatement ; , namespaceName ) ; StatefulSetUtils . waitTillSsHasRolled ( namespaceName , kafkaStatefulSetName ( clusterName ) , 3 , kafkaPods ) ; LOGGER . info ( "Verifying that in {} is not present in the Kafka cluster" , Constants . CRUISE_CONTROL_NAME ) ; assertThat ( KafkaResource . kafkaClient ( ) . inNamespace ( namespaceName ) . withName ( clusterName ) . get ( ) . getSpec ( ) . getCruiseControl ( ) , nullValue ( ) ) ; PodUtils . waitUntilPodStabilityReplicasCount ( namespaceName , clusterName + "-cruise-control-" , 0 ) ; LOGGER . info ( "Verifying that in Kafka config map there is no configuration to cruise control metric reporter" ) ; assertThrows ( WaitException . class , ( ) -> CruiseControlUtils . verifyCruiseControlMetricReporterConfigurationInKafkaConfigMapIsPresent ( CruiseControlUtils . getKafkaCruiseControlMetricsReporterConfiguration ( namespaceName , clusterName ) ) ) ; LOGGER . info ( "Cruise Control topics will not be deleted and will stay in the Kafka cluster" ) ; CruiseControlUtils . verifyThatCruiseControlTopicsArePresent ( namespaceName ) ; kafkaPods = StatefulSetUtils . ssSnapshot ( namespaceName , KafkaResources . kafkaStatefulSetName ( clusterName ) ) ; LOG
@ ParallelNamespaceTest void testDeployAndUnDeployCruiseControl ( ExtensionContext extensionContext ) throws IOException { final String namespaceName = extensionContext . getStore ( ExtensionContext . Namespace . GLOBAL ) . get ( Constants . NAMESPACE_KEY ) . toString ( ) ; final String clusterName = mapWithClusterNames . get ( extensionContext . getDisplayName ( ) ) ; resourceManager . createResource ( extensionContext , KafkaTemplates . kafkaWithCruiseControl ( clusterName , 3 , 3 ) . build ( ) ) ; Map < String , String > kafkaPods = StatefulSetUtils . ssSnapshot ( namespaceName , KafkaResources . kafkaStatefulSetName ( clusterName ) ) ; KafkaResource . replaceKafkaResourceInSpecificNamespace ( clusterName , kafka code_block = LoopStatement ; , namespaceName ) ; StatefulSetUtils . waitTillSsHasRolled ( namespaceName , kafkaStatefulSetName ( clusterName ) , 3 , kafkaPods ) ; LOGGER . info ( "Verifying that in {} is not present in the Kafka cluster" , Constants . CRUISE_CONTROL_NAME ) ; assertThat ( KafkaResource . kafkaClient ( ) . inNamespace ( namespaceName ) . withName ( clusterName ) . get ( ) . getSpec ( ) . getCruiseControl ( ) , nullValue ( ) ) ; LOGGER . info ( "Verifying that {} pod is not present" , clusterName + "-cruise-control-" ) ; PodUtils . waitUntilPodStabilityReplicasCount ( namespaceName , clusterName + "-cruise-control-" , 0 ) ; assertThrows ( WaitException . class , ( ) -> CruiseControlUtils . verifyCruiseControlMetricReporterConfigurationInKafkaConfigMapIsPresent ( CruiseControlUtils . getKafkaCruiseControlMetricsReporterConfiguration ( namespaceName , clusterName ) ) ) ; LOGGER . info ( "Cruise Control topics will not be deleted and will stay in the Kafka cluster" ) ; CruiseControlUtils . verifyThatCruiseControlTopicsArePresent ( namespaceName ) ; kafkaPods = StatefulSetUtils . ssSnapshot ( namespaceName , KafkaResources . kafkaStatefulSetName ( clusterName ) )
@ ParallelNamespaceTest void testDeployAndUnDeployCruiseControl ( ExtensionContext extensionContext ) throws IOException { final String namespaceName = extensionContext . getStore ( ExtensionContext . Namespace . GLOBAL ) . get ( Constants . NAMESPACE_KEY ) . toString ( ) ; final String clusterName = mapWithClusterNames . get ( extensionContext . getDisplayName ( ) ) ; resourceManager . createResource ( extensionContext , KafkaTemplates . kafkaWithCruiseControl ( clusterName , 3 , 3 ) . build ( ) ) ; Map < String , String > kafkaPods = StatefulSetUtils . ssSnapshot ( namespaceName , KafkaResources . kafkaStatefulSetName ( clusterName ) ) ; LOGGER . info ( "Verifying that in the Kafka cluster" ) ; KafkaResource . replaceKafkaResourceInSpecificNamespace ( clusterName , kafka code_block = LoopStatement ; , namespaceName ) ; StatefulSetUtils . waitTillSsHasRolled ( namespaceName , kafkaStatefulSetName ( clusterName ) , 3 , kafkaPods ) ; LOGGER . info ( "Verifying that in {} is not present in the Kafka cluster" , Constants . CRUISE_CONTROL_NAME ) ; assertThat ( KafkaResource . kafkaClient ( ) . inNamespace ( namespaceName ) . withName ( clusterName ) . get ( ) . getSpec ( ) . getCruiseControl ( ) , nullValue ( ) ) ; LOGGER . info ( "Verifying that {} pod is not present" , clusterName + "-cruise-control-" ) ; PodUtils . waitUntilPodStabilityReplicasCount ( namespaceName , clusterName + "-cruise-control-" , 0 ) ; LOGGER . info ( "Verifying that in Kafka config map there is no configuration to cruise control metric reporter" ) ; assertThrows ( WaitException . class , ( ) -> CruiseControlUtils . verifyCruiseControlMetricReporterConfigurationInKafkaConfigMapIsPresent ( CruiseControlUtils . getKafkaCruiseControlMetricsReporterConfiguration ( namespaceName , clusterName ) ) ) ; CruiseControlUtils . verifyThatCruiseControlTopicsArePresent ( namespaceName ) ; kafkaPods = StatefulSetUtils . ssSnapshot ( namespace
public void test() { -> { LOGGER . info ( "Setting up the cruise control" ) ; kafka . getSpec ( ) . setCruiseControl ( new CruiseControlSpec ( ) ) ; } }
@ ParallelNamespaceTest void testDeployAndUnDeployCruiseControl ( ExtensionContext extensionContext ) throws IOException { final String namespaceName = extensionContext . getStore ( ExtensionContext . Namespace . GLOBAL ) . get ( Constants . NAMESPACE_KEY ) . toString ( ) ; final String clusterName = mapWithClusterNames . get ( extensionContext . getDisplayName ( ) ) ; resourceManager . createResource ( extensionContext , KafkaTemplates . kafkaWithCruiseControl ( clusterName , 3 , 3 ) . build ( ) ) ; Map < String , String > kafkaPods = StatefulSetUtils . ssSnapshot ( namespaceName , KafkaResources . kafkaStatefulSetName ( clusterName ) ) ; KafkaResource . replaceKafkaResourceInSpecificNamespace ( clusterName , kafka code_block = LoopStatement ; , namespaceName ) ; StatefulSetUtils . waitTillSsHasRolled ( namespaceName , kafkaStatefulSetName ( clusterName ) , 3 , kafkaPods ) ; LOGGER . info ( "Verifying that in {} is not present in the Kafka cluster" , Constants . CRUISE_CONTROL_NAME ) ; assertThat ( KafkaResource . kafkaClient ( ) . inNamespace ( namespaceName ) . withName ( clusterName ) . get ( ) . getSpec ( ) . getCruiseControl ( ) , nullValue ( ) ) ; LOGGER . info ( "Verifying that {} pod is not present" , clusterName + "-cruise-control-" ) ; PodUtils . waitUntilPodStabilityReplicasCount ( namespaceName , clusterName + "-cruise-control-" , 0 ) ; LOGGER . info ( "Verifying that in Kafka config map there is no configuration to cruise control metric reporter" ) ; assertThrows ( WaitException . class , ( ) -> CruiseControlUtils . verifyCruiseControlMetricReporterConfigurationInKafkaConfigMapIsPresent ( CruiseControlUtils . getKafkaCruiseControlMetricsReporterConfiguration ( namespaceName , clusterName ) ) ) ; LOGGER . info ( "Cruise Control topics will not be deleted and will stay in the Kafka cluster" ) ; CruiseControlUtils . verifyThatCruiseControlTopicsArePresent ( namespaceName ) ; kafkaPods = StatefulSet
@ ParallelNamespaceTest void testDeployAndUnDeployCruiseControl ( ExtensionContext extensionContext ) throws IOException { final String namespaceName = extensionContext . getStore ( ExtensionContext . Namespace . GLOBAL ) . get ( Constants . NAMESPACE_KEY ) . toString ( ) ; final String clusterName = mapWithClusterNames . get ( extensionContext . getDisplayName ( ) ) ; resourceManager . createResource ( extensionContext , KafkaTemplates . kafkaWithCruiseControl ( clusterName , 3 , 3 ) . build ( ) ) ; Map < String , String > kafkaPods = StatefulSetUtils . ssSnapshot ( namespaceName , KafkaResources . kafkaStatefulSetName ( clusterName ) ) ; KafkaResource . replaceKafkaResourceInSpecificNamespace ( clusterName , kafka code_block = LoopStatement ; , namespaceName ) ; StatefulSetUtils . waitTillSsHasRolled ( namespaceName , kafkaStatefulSetName ( clusterName ) , 3 , kafkaPods ) ; LOGGER . info ( "Verifying that in {} is not present in the Kafka cluster" , Constants . CRUISE_CONTROL_NAME ) ; assertThat ( KafkaResource . kafkaClient ( ) . inNamespace ( namespaceName ) . withName ( clusterName ) . get ( ) . getSpec ( ) . getCruiseControl ( ) , nullValue ( ) ) ; LOGGER . info ( "Verifying that {} pod is not present" , clusterName + "-cruise-control-" ) ; PodUtils . waitUntilPodStabilityReplicasCount ( namespaceName , clusterName + "-cruise-control-" , 0 ) ; LOGGER . info ( "Verifying that in Kafka config map there is no configuration to cruise control metric reporter" ) ; assertThrows ( WaitException . class , ( ) -> CruiseControlUtils . verifyCruiseControlMetricReporterConfigurationInKafkaConfigMapIsPresent ( CruiseControlUtils . getKafkaCruiseControlMetricsReporterConfiguration ( namespaceName , clusterName ) ) ) ; LOGGER . info ( "Cruise Control topics will not be deleted and will stay in the Kafka cluster" ) ; CruiseControlUtils . verifyThatCruiseControlTopicsArePresent ( namespaceName ) ; kafkaPods = StatefulSet
public String getCaseFileDataByName ( String containerId , String caseId , String name , String marshallingType ) { verifyContainerId ( containerId , caseId ) ; logger . debug ( "About to get case data by name: '{}'" , caseId ) ; CaseFileInstance caseFileInstance = caseService . getCaseFileInstance ( caseId ) ; Object caseFileData = caseFileInstance . getData ( name ) ; return marshallerHelper . marshal ( containerId , marshallingType , caseFileData , new ByCaseIdContainerLocator ( caseId ) ) ; }
public void test() { try { return resource . delete ( jobId ) . execute ( ) ; } catch ( IOException e ) { log . error ( "Failed to delete jobId" , e ) ; return null ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Refreshing." ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { code_block = IfStatement ; cqlResult = ( CqlResult ) executeCQLQuery ( cqlQuery , true ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; throw new PersistenceException ( e ) ; } }
public void test() { try { return systemMailboxesProvider . findMailbox ( Role . INBOX , addedEvent . getUsername ( ) ) . getId ( ) . equals ( addedEvent . getMailboxId ( ) ) ; } catch ( MailboxException e ) { LOG . warn ( "Unable to find mailbox for {}" , addedEvent . getMailboxId ( ) ) ; return false ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( changedCollections . hasChanges ( ) ) { LOGGER . debug ( "Collections has changed and will not be processed" ) ; } }
@ Override public RetrieveDocumentSetResponseType retrieveDocument ( RetrieveDocumentSetRequestType msg , AssertionType assertion ) { LOG . debug ( "Begin retrieveDocument" ) ; RetrieveDocumentSetResponseType response = null ; code_block = TryStatement ;  LOG . debug ( "End retrieveDocument" ) ; return response ; }
@ Override public RetrieveDocumentSetResponseType retrieveDocument ( RetrieveDocumentSetRequestType msg , AssertionType assertion ) { LOG . debug ( "Begin retrieveDocument" ) ; RetrieveDocumentSetResponseType response = null ; code_block = TryStatement ;  LOG . debug ( "End retrieveDocument" ) ; return response ; }
@ Override public void onFailure ( IMqttToken asyncActionToken , Throwable exception ) { LOGGER . error ( "MQTT connect failed." , exception ) ; Assert . fail ( "MQTT connect failed: " + exception . getMessage ( ) ) ; }
@ Override public void commit ( Xid xid , boolean b ) throws XAException { this . commitStarted = true ; logger . debug ( "Transaction started" ) ; }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( options . isEvictionAllowed ( ) ) { LOG . debug ( "Free space for block expansion: freeing {} bytes on {}. " , options . getSize ( ) , options . getLocation ( ) ) ; freeSpace ( sessionId , options . getSize ( ) , options . getSize ( ) , options . getLocation ( ) ) ; dirView = mAllocator . allocateBlockWithView ( sessionId , options . getSize ( ) , options . getLocation ( ) , allocatorView . refreshView ( ) , true ) ; code_block = IfStatement ; LOG . debug ( "Eviction is disabled." ) ; code_block = IfStatement ; } else { break ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void run ( RegressionEnvironment env ) { String methodName = ".testPerfPropertyAccess" ; String joinStatement = "@name('s0') select * from " + "SupportBeanCombinedPropslength(1)" + " where indexed[0].mapped('a').value = 'dummy'" ; env . compileDeploy ( joinStatement ) . addListener ( "s0" ) ; log . info ( methodName + " Sending events" ) ; SupportBeanCombinedProps theEvent = SupportBeanCombinedProps . makeDefaultBean ( ) ; long startTime = System . currentTimeMillis ( ) ; code_block = ForStatement ; log . info ( methodName + " Done sending events" ) ; long endTime = System . currentTimeMillis ( ) ; log . info ( methodName + " delta=" + ( endTime - startTime ) ) ; assertTrue ( ( endTime - startTime ) < 1000 ) ; env . undeployAll ( ) ; }
public void run ( RegressionEnvironment env ) { String methodName = ".testPerfPropertyAccess" ; String joinStatement = "@name('s0') select * from " + "SupportBeanCombinedPropslength(1)" + " where indexed[0].mapped('a').value = 'dummy'" ; env . compileDeploy ( joinStatement ) . addListener ( "s0" ) ; SupportBeanCombinedProps theEvent = SupportBeanCombinedProps . makeDefaultBean ( ) ; log . info ( methodName + " Sending events" ) ; long startTime = System . currentTimeMillis ( ) ; code_block = ForStatement ; log . info ( methodName + " Sending events done" ) ; long endTime = System . currentTimeMillis ( ) ; log . info ( methodName + " delta=" + ( endTime - startTime ) ) ; assertTrue ( ( endTime - startTime ) < 1000 ) ; env . undeployAll ( ) ; }
public void run ( RegressionEnvironment env ) { String methodName = ".testPerfPropertyAccess" ; String joinStatement = "@name('s0') select * from " + "SupportBeanCombinedPropslength(1)" + " where indexed[0].mapped('a').value = 'dummy'" ; env . compileDeploy ( joinStatement ) . addListener ( "s0" ) ; SupportBeanCombinedProps theEvent = SupportBeanCombinedProps . makeDefaultBean ( ) ; log . info ( methodName + " Sending events" ) ; long startTime = System . currentTimeMillis ( ) ; code_block = ForStatement ; log . info ( methodName + " Done sending events" ) ; log . info ( methodName + " Done sending events" ) ; long endTime = System . currentTimeMillis ( ) ; assertTrue ( ( endTime - startTime ) < 1000 ) ; env . undeployAll ( ) ; }
public void accept ( RoutingContext ctx ) { log . info ( "Accepting" ) ; HttpResponse . responseText ( ctx , 202 ) ; echo ( ctx ) ; }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { try { MinimalDiffBounds geomBuildCommand = context . command ( MinimalDiffBounds . class ) . setOldVersion ( oldCommit . toString ( ) ) . setNewVersion ( newCommit . toString ( ) ) ; geomBuildCommand . setTreeNameFilter ( layerTreeName ) ; minimalBounds = geomBuildCommand . call ( ) ; sw . stop ( ) ; code_block = IfStatement ; code_block = IfStatement ; } catch ( Exception e ) { sw . stop ( ) ; LOGGER . warn ( "Error while performing version cleanup" , e ) ; throw Throwables . propagate ( e ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( InvalidTransportHandlerStateException ex ) { LOG . warn ( ex . toString ( ) ) ; return SocketState . DATA_AVAILABLE ; } }
@ Test public void testQueryList ( ) throws Exception { MvcResult mvcResult = mockMvc . perform ( get ( "/queue/list" ) . header ( SESSION_ID , sessionId ) ) . andExpect ( status ( ) . isOk ( ) ) . andExpect ( content ( ) . contentType ( MediaType . APPLICATION_JSON_UTF8 ) ) . andReturn ( ) ; Result result = JSONUtils . parseObject ( mvcResult . getResponse ( ) . getContentAsString ( ) , Result . class ) ; Assert . assertEquals ( Status . SUCCESS . getCode ( ) , result . getCode ( ) . intValue ( ) ) ; logger . info ( mvcResult . getResponse ( ) . getContentAsString ( ) ) ; }
public void test() { if ( code_block = IfStatement ; }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { if ( ! serviceConfiguration . isPresent ( ) && this . evaluatorConfigurationProviders . isEmpty ( ) ) { LOG . debug ( "No evaluator configuration provided, returning empty" ) ; return Optional . empty ( ) ; } else { final ConfigurationBuilder configurationBuilder = getConfigurationBuilder ( serviceConfiguration ) ; code_block = ForStatement ; return Optional . of ( configurationBuilder . build ( ) ) ; } }
@ Override public void onNext ( T t ) { LOG . log ( Level . INFO , "Sent event: {0}" , t ) ; }
public void test() { if ( getLogger ( ) . isDebugEnabled ( ) ) { getLogger ( ) . debug ( "Shutting down GeoServer" ) ; } }
public void test() { if ( getLogger ( ) . isDebugEnabled ( ) ) { getLogger ( ) . debug ( "Shutting down GeoServer" ) ; } }
public void test() { if ( getLogger ( ) . isDebugEnabled ( ) ) { getLogger ( ) . debug ( "Shutting down GeoServer" ) ; } }
public void test() { if ( getLogger ( ) . isDebugEnabled ( ) ) { getLogger ( ) . debug ( "Shutting down GeoServer" ) ; } }
public void test() { if ( getLogger ( ) . isDebugEnabled ( ) ) { getLogger ( ) . debug ( "Shutting down GeoServer" ) ; } }
public void test() { try { return templateManagerAdminServiceStub . deleteConfiguration ( domainName , configurationName ) ; } catch ( RemoteException e ) { log . error ( "RemoteException" , e ) ; throw new RemoteException ( e . getMessage ( ) , e ) ; } }
public void test() { try { setStatus ( PluginStatus . STOPPING ) ; code_block = TryStatement ;  sensorThread = null ; setStatus ( PluginStatus . STOPPED ) ; PluginHasChanged event = new PluginHasChanged ( this , getName ( ) , PluginHasChanged . PluginActions . STOP ) ; event . getPayload ( ) . addStatement ( "plugin.status" , getStatus ( ) ) ; getBusService ( ) . send ( event ) ; } catch ( Exception e ) { setStatus ( PluginStatus . FAILED ) ; setDescription ( "Plugin stopping FAILED. see logs for details." ) ; LOG . error ( "Plugin \"" + getName ( ) + "\" starting FAILED: " + e . getLocalizedMessage ( ) , e ) ; PluginHasChanged event = new PluginHasChanged ( this , getName ( ) , PluginHasChanged . PluginActions . START ) ; event . getPayload ( ) . addStatement ( "plugin.status" , getStatus ( ) ) ; getBusService ( ) . send ( event ) ; } }
public static void main ( String [ ] args ) { LOG . info ( "Starting pipeline..." ) ; BasePipelineOptions options = PipelinesOptionsFactory . create ( BasePipelineOptions . class , args ) ; String inputPath = options . getInputPath ( ) ; String tmpDir = PathBuilder . getTempDir ( options ) ; String outPath = options . getTargetPath ( ) ; Pipeline p = Pipeline . create ( options ) ; p . apply ( "Read DwCA zip archive" , DwcaIO . Read . fromCompressed ( inputPath , tmpDir ) ) . apply ( "Interpret TemporalRecord" , TemporalTransform . builder ( ) . create ( ) . interpret ( ) ) . apply ( "Interpret ExampleRecord" , ExampleTransform . exampleOne ( ) ) . apply ( "Write as Avro files" , AvroIO . write ( ExampleRecord . class ) . to ( outPath ) . withSuffix ( AVRO_EXTENSION ) ) ; p . run ( ) . waitUntilFinish ( ) ; LOG . info ( "Pipeline has been finished!" ) ; }
public static void main ( String [ ] args ) { BasePipelineOptions options = PipelinesOptionsFactory . create ( BasePipelineOptions . class , args ) ; String inputPath = options . getInputPath ( ) ; String tmpDir = PathBuilder . getTempDir ( options ) ; String outPath = options . getTargetPath ( ) ; Pipeline p = Pipeline . create ( options ) ; p . apply ( "Read DwCA zip archive" , DwcaIO . Read . fromCompressed ( inputPath , tmpDir ) ) . apply ( "Interpret TemporalRecord" , TemporalTransform . builder ( ) . create ( ) . interpret ( ) ) . apply ( "Interpret ExampleRecord" , ExampleTransform . exampleOne ( ) ) . apply ( "Write as Avro files" , AvroIO . write ( ExampleRecord . class ) . to ( outPath ) . withSuffix ( AVRO_EXTENSION ) ) ; LOG . info ( "Running the pipeline" ) ; p . run ( ) . waitUntilFinish ( ) ; LOG . info ( "Done." ) ; }
public void test() { try { log . debug ( "kurentoTest." + jsFunction + "('" + peerConnectionId + "');" ) ; browser . executeScript ( "kurentoTest." + jsFunction + "('" + peerConnectionId + "');" ) ; } catch ( WebDriverException we ) { we . printStackTrace ( ) ; log . warn ( we . getMessage ( ) ) ; } }
public void test() { if ( fs . isFile ( jarPath ) && jarPath . getName ( ) . endsWith ( ".jar" ) ) { LOG . info ( "Copying jar {} to {}" , jarPath , localPath ) ; fs . copyToLocalFile ( jarPath , localPath ) ; } }
public List < Product > getProducts ( ) { log . info ( "getProducts()" ) ; return Arrays . asList ( new Product ( "Laptop" , 31000.00 ) , new Product ( "Mobile" , 16000.00 ) , new Product ( "Tablet" , 15000.00 ) , new Product ( "Camera" , 23000.00 ) ) ; }
public void test() { try { code_block = IfStatement ; } catch ( Exception ex ) { LOGGER . error ( "Failed to clean up." , ex ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( EntryPersistenceException e ) { code_block = TryStatement ;  } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceWishListItemServiceUtil . class , "getCommerceWishListItemByContainsCProductCount" , _getCommerceWishListItemByContainsCProductCountParameterTypes5 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , commerceWishListId , cProductId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( ( Integer ) returnObj ) . intValue ( ) ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { cipher = createDecryptCipherInternal ( ) ; } catch ( Exception e ) { logger . error ( "Unable to create Cipher" , e ) ; cipher = null ; } }
public void test() { if ( failedNodes . size ( ) > 0 ) { Set < String > lostIds = Sets . newLinkedHashSet ( ) ; code_block = ForStatement ; int numberOfNodesToDelete = lostIds . size ( ) ; Set < ? extends NodeMetadata > destroyedNodes = novaContext . getComputeService ( ) . destroyNodesMatching ( Predicates . in ( failedNodes . keySet ( ) ) ) ; lostIds . clear ( ) ; code_block = ForStatement ; logger . info ( "Failed nodes destroyed ;)" ) ; int numberOfNodesSuccesfullyDeleted = lostIds . size ( ) ; success = numberOfNodesSuccesfullyDeleted == numberOfNodesToDelete ; logger . info ( "Successfully deleted " + numberOfNodes ) ; } else { success = true ; } }
public void test() { if ( failedNodes . size ( ) > 0 ) { Set < String > lostIds = Sets . newLinkedHashSet ( ) ; code_block = ForStatement ; int numberOfNodesToDelete = lostIds . size ( ) ; logger . info ( String . format ( "Destroying failed nodes with ids: %s" , lostIds . toString ( ) ) ) ; Set < ? extends NodeMetadata > destroyedNodes = novaContext . getComputeService ( ) . destroyNodesMatching ( Predicates . in ( failedNodes . keySet ( ) ) ) ; logger . info ( String . format ( "Destroying failed nodes with ids: %s" , destroyedNodes . toString ( ) ) ) ; lostIds . clear ( ) ; code_block = ForStatement ; int numberOfNodesSuccesfullyDeleted = lostIds . size ( ) ; success = numberOfNodesSuccesfullyDeleted == numberOfNodesToDelete ; } else { success = true ; } }
public void test() { if ( LOGGER . isInfoEnabled ( ) ) { LOGGER . info ( "Success!" ) ; } }
public void test() { try { process . stop ( ) ; } catch ( Throwable t ) { LOG . warn ( "Error stopping process" , t ) ; } }
public void test() { try { cli . init ( ) ; } catch ( Exception e ) { log . error ( "Could not initialize Cloud Controller" , e ) ; System . exit ( 0 ) ; } }
public void test() { try { _log . info ( sql ) ; return facade . selectStringList ( sql , columnList ) ; } catch ( RuntimeException continued ) { _log . info ( "*Failed to select string list of datasource" ) ; return DfCollectionUtil . newArrayList ( ) ; } }
public void test() { if ( tcpServer == null ) { logger . info ( "begin to start h2 database tcp server..." ) ; tcpServer = Server . createTcpServer ( new String [ ] code_block = "" ; ) . start ( ) ; logger . info ( String . format ( "h2 database tcp server is started on port %s" , TCP_PORT ) ) ; } else-if ( ! tcpServer . isRunning ( true ) ) { logger . info ( "begin to start h2 database tcp server..." ) ; tcpServer . start ( ) ; logger . info ( String . format ( "h2 database is started on port %s" , TCP_PORT ) ) ; } else { logger . info ( "h2 database tcp server is already running" ) ; } }
public void test() { if ( tcpServer == null ) { logger . info ( "begin to create h2 database tcp server..." ) ; tcpServer = Server . createTcpServer ( new String [ ] code_block = "" ; ) . start ( ) ; logger . info ( String . format ( "h2 database tcp server is started on port %s" , TCP_PORT ) ) ; } else-if ( ! tcpServer . isRunning ( true ) ) { logger . info ( "begin to create h2 database tcp server..." ) ; tcpServer . start ( ) ; logger . info ( String . format ( "h2 database tcp server is started on port %s" , TCP_PORT ) ) ; } else { logger . info ( "h2 database tcp server is already running" ) ; } }
public void test() { if ( tcpServer == null ) { logger . info ( "begin to create h2 database tcp server..." ) ; tcpServer = Server . createTcpServer ( new String [ ] code_block = "" ; ) . start ( ) ; logger . info ( String . format ( "h2 database tcp server is started on port %s" , TCP_PORT ) ) ; } else-if ( ! tcpServer . isRunning ( true ) ) { logger . info ( "begin to start h2 database tcp server..." ) ; tcpServer . start ( ) ; logger . info ( String . format ( "h2 database tcp server is started on port %s" , TCP_PORT ) ) ; } else { logger . info ( "h2 database tcp server is already running" ) ; } }
public void test() { if ( webServer == null ) { logger . info ( "begin to start h2 database web server..." ) ; webServer = Server . createWebServer ( new String [ ] code_block = "" ; ) . start ( ) ; logger . info ( String . format ( "h2 database web server is started on port %s" , WEB_PORT ) ) ; } else-if ( ! webServer . isRunning ( true ) ) { logger . info ( "begin to start h2 database web server..." ) ; webServer . start ( ) ; logger . info ( String . format ( "h2 database web server is started on port %s" , WEB_PORT ) ) ; } else { logger . info ( "h2 database web server is already running." ) ; } }
public void test() { if ( webServer == null ) { logger . info ( "begin to create h2 database web server..." ) ; webServer = Server . createWebServer ( new String [ ] code_block = "" ; ) . start ( ) ; logger . info ( String . format ( "h2 database web server is started on port %s" , WEB_PORT ) ) ; } else-if ( ! webServer . isRunning ( true ) ) { logger . info ( "start to create h2 database web server..." ) ; webServer . start ( ) ; logger . info ( String . format ( "h2 database web server is started on port %s" , WEB_PORT ) ) ; } else { logger . info ( "h2 database web server is already running." ) ; } }
public void test() { if ( webServer == null ) { logger . info ( "begin to create h2 database web server..." ) ; webServer = Server . createWebServer ( new String [ ] code_block = "" ; ) . start ( ) ; logger . info ( String . format ( "h2 database web server is started on port %s" , WEB_PORT ) ) ; } else-if ( ! webServer . isRunning ( true ) ) { logger . info ( "begin to start h2 database web server..." ) ; webServer . start ( ) ; logger . info ( String . format ( "h2 database web server is started on port %s" , WEB_PORT ) ) ; } else { logger . info ( "h2 database web server is already running" ) ; } }
public void test() { try { String baseDir = getBaseDir ( url ) ; code_block = IfStatement ; code_block = IfStatement ; } catch ( SQLException e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( s3ObjectSummary . getSize ( ) == 0 ) { log . info ( "Found empty S3 file \"%s\" in \"%s\" storage. Business object data {%s}" , s3ObjectSummary . getKey ( ) , storageName , businessObjectDataHelper . businessObjectDataKeyToString ( businessObjectDataKey ) ) ) ; } else { throw new IllegalStateException ( String . format ( "Found unregistered non-empty S3 file \"%s\" in \"%s\" storage. Business object data {%s}" , s3ObjectSummary . getKey ( ) , storageName , businessObjectDataHelper . businessObjectDataKeyToString ( businessObjectDataKey ) ) ) ; } }
public void test() { try { userSettings = userSettingsService . findUserSettings ( limit , offset ) ; } catch ( UserSettingsServiceException e ) { log . error ( e . getMessage ( ) , e ) ; return ( Response . serverError ( ) . build ( ) ) ; } }
public void test() { -> { Registration registration = new Registration ( WORKSPACE_FOLDERS_CAPABILITY_ID , WORKSPACE_FOLDERS_CAPABILITY_NAME , null ) ; RegistrationParams registrationParams = new RegistrationParams ( Collections . singletonList ( registration ) ) ; LOG . debug ( "RegisterCapability: " + registrationParams ) ; getClient ( ) . registerCapability ( registrationParams ) ; this . initialized . complete ( null ) ; } }
public void test() { if ( loggedMBeanGauges . add ( mbeanObjectName ) ) { LOGGER . info ( "MBean gauge[" + mbeanObjectName + "] = " + mbeanObjectName ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
@ Test void testUpgradeAcrossVersionsWithNoKafkaVersion ( ExtensionContext extensionContext ) throws IOException { JsonObject acrossUpgradeData = buildDataForUpgradeAcrossVersions ( ) ; JsonObject conversionTool = getConversionToolDataFromUpgradeJSON ( ) ; String continuousTopicName = "continuous-topic" ; String producerName = "hello-world-producer" ; String consumerName = "hello-world-consumer" ; String continuousConsumerGroup = "continuous-consumer-group" ; setupEnvAndUpgradeClusterOperator ( extensionContext , acrossUpgradeData , producerName , consumerName , continuousTopicName , continuousConsumerGroup , null , NAMESPACE ) ; convertCRDs ( conversionTool , NAMESPACE ) ; changeClusterOperator ( acrossUpgradeData , NAMESPACE ) ; zkPods = StatefulSetUtils . waitTillSsHasRolled ( KafkaResources . zookeeperStatefulSetName ( clusterName ) , 3 , zkPods ) ; kafkaPods = StatefulSetUtils . waitTillSsHasRolled ( KafkaResources . kafkaStatefulSetName ( clusterName ) , 3 , kafkaPods ) ; eoPods = DeploymentUtils . waitTillDepHasRolled ( KafkaResources . entityOperatorDeploymentName ( clusterName ) , 1 , eoPods ) ; logPodImages ( clusterName ) ; changeKafkaAndLogFormatVersion ( acrossUpgradeData . getJsonObject ( "proceduresAfterOperatorUpgrade" ) , acrossUpgradeData , clusterName , extensionContext ) ; logPodImages ( clusterName ) ; checkAllImages ( acrossUpgradeData . getJsonObject ( "imagesAfterKafkaUpgrade" ) ) ; PodUtils . verifyThatRunningPodsAreStable ( clusterName ) ; verifyProcedure ( acrossUpgradeData , producerName , consumerName , NAMESPACE ) ; assertNoCoErrorsLogged ( 0 ) ; LOGGER . info ( "Upgrade complete" ) ; }
@ Override public void init ( ) throws Exception { String xmlConfig = this . getConfigManager ( ) . getConfigItem ( SystemConstants . CONFIG_ITEM_LANGS ) ; this . getCacheWrapper ( ) . initCache ( xmlConfig ) ; _logger . debug ( "Initialized" ) ; }
public void test() { switch ( channelUID . getId ( ) ) { case PLAYURL : playMedia ( channelUID , command ) ; break ; case STOP : stopMedia ( channelUID , command ) ; break ; default : logger . debug ( "Channel {} not supported" , channelUID ) ; break ; } }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "JDBC::{}" , sql ) ; } }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "JDBC::{}" , sql ) ; } }
public void test() { try { unregister ( url ) ; code_block = IfStatement ; } catch ( Throwable t ) { LOGGER . warn ( "Failed to unregister {}" , url , t ) ; } }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "JDBC::{}" , sql ) ; } }
public void test() { try { unsubscribe ( url , listener ) ; code_block = IfStatement ; } catch ( Throwable t ) { logger . error ( "Failed to unsubscribe" , t ) ; } }
public void test() { try { addMemberToRoleOrGroup ( session , groupDN , memberDN , UNIQUE_MEMBER ) ; } catch ( final LdapNoSuchObjectException e ) { log . debug ( "LDAP lookup failed: {}" , e . getMessage ( ) ) ; } }
public void test() { try { String xml = new AvatarConfigDOM ( ) . createConfigXml ( config ) ; this . getConfigManager ( ) . updateConfigItem ( JpAvatarSystemConstants . CONFIG_ITEM , xml ) ; this . setConfig ( config ) ; } catch ( Throwable t ) { _logger . error ( "Error updating jpavatar config" , t ) ; throw new ApsSystemException ( "Error updating jpavatar config" , t ) ; } }
@ Test public void testMarkStartTime ( ) { final DefaultTraceId traceId = new DefaultTraceId ( "agentId" , 0 , 0 ) ; TraceRoot traceRoot = new DefaultTraceRoot ( traceId , "agentId" , System . currentTimeMillis ( ) , 0 ) ; Span span = new Span ( traceRoot ) ; span . markBeforeTime ( ) ; span . setElapsedTime ( ( int ) ( span . getStartTime ( ) + 10 ) ) ; final SpanEvent spanEvent = new SpanEvent ( ) ; long currentTime = System . currentTimeMillis ( ) ; logger . debug ( "startTime:{}" , currentTime ) ; spanEvent . setStartTime ( currentTime ) ; spanEvent . setElapsedTime ( 10 ) ; logger . debug ( "spanEvent:{}" , spanEvent ) ; span . setSpanEventList ( Arrays . asList ( spanEvent ) ) ; TSpan tSpan = new TSpan ( ) ; TSpanEvent tSpanEvent = new TSpanEvent ( ) ; tSpan . addToSpanEventList ( tSpanEvent ) ; compressorV1 . postProcess ( span , tSpan ) ; Assert . assertEquals ( "startTime" , span . getStartTime ( ) + tSpanEvent . getStartElapsed ( ) , spanEvent . getStartTime ( ) ) ; Assert . assertEquals ( "endTime" , span . getStartTime ( ) + tSpanEvent . getStartElapsed ( ) + spanEvent . getElapsedTime ( ) , spanEvent . getAfterTime ( ) ) ; }
@ Test public void testMarkStartTime ( ) { final DefaultTraceId traceId = new DefaultTraceId ( "agentId" , 0 , 0 ) ; TraceRoot traceRoot = new DefaultTraceRoot ( traceId , "agentId" , System . currentTimeMillis ( ) , 0 ) ; Span span = new Span ( traceRoot ) ; span . markBeforeTime ( ) ; span . setElapsedTime ( ( int ) ( span . getStartTime ( ) + 10 ) ) ; logger . debug ( "span:{}" , span ) ; final SpanEvent spanEvent = new SpanEvent ( ) ; long currentTime = System . currentTimeMillis ( ) ; spanEvent . setStartTime ( currentTime ) ; spanEvent . setElapsedTime ( 10 ) ; span . setSpanEventList ( Arrays . asList ( spanEvent ) ) ; TSpan tSpan = new TSpan ( ) ; TSpanEvent tSpanEvent = new TSpanEvent ( ) ; tSpan . addToSpanEventList ( tSpanEvent ) ; logger . debug ( "startTime:{}" , spanEvent . getStartTime ( ) ) ; compressorV1 . preProcess ( span , tSpan ) ; compressorV1 . postProcess ( span , tSpan ) ; Assert . assertEquals ( "startTime" , span . getStartTime ( ) + tSpanEvent . getStartElapsed ( ) , spanEvent . getStartTime ( ) ) ; Assert . assertEquals ( "endTime" , span . getStartTime ( ) + tSpanEvent . getStartElapsed ( ) + spanEvent . getElapsedTime ( ) , spanEvent . getAfterTime ( ) ) ; }
@ Override public void setValency ( Integer valency ) { logger . debug ( "Setting valency: " , valency ) ; super . setValency ( valency ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
@ Override public Message receive ( TestContext context , long timeout ) { String endpointUri = context . replaceDynamicContentInString ( endpointConfiguration . getEndpointUri ( ) ) ; code_block = IfStatement ; Exchange exchange = getConsumerTemplate ( ) . receive ( endpointUri , timeout ) ; code_block = IfStatement ; Message message = endpointConfiguration . getMessageConverter ( ) . convertInbound ( exchange , endpointConfiguration , context ) ; LOG . debug ( "Received message {}" , message ) ; context . onInboundMessage ( message ) ; String correlationKeyName = endpointConfiguration . getCorrelator ( ) . getCorrelationKeyName ( getName ( ) ) ; String correlationKey = endpointConfiguration . getCorrelator ( ) . getCorrelationKey ( message ) ; correlationManager . saveCorrelationKey ( correlationKeyName , correlationKey , context ) ; correlationManager . store ( correlationKey , exchange ) ; return message ; }
public void test() { try { MethodKey methodKey = new MethodKey ( CalendarBookingServiceUtil . class , "getCalendarBookings" , _getCalendarBookingsParameterTypes12 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , calendarId , startTime , endTime ) ; Object returnObj = null ; code_block = TryStatement ;  return ( java . util . List < com . liferay . calendar . model . CalendarBooking > ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( MBBanServiceUtil . class , "addBan" , _addBanParameterTypes0 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , banUserId , serviceContext ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . message . boards . model . MBBan ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void registerUuid ( String key ) throws DaikinCommunicationException { Map < String , String > params = new HashMap < > ( ) ; params . put ( "key" , key ) ; String response = invoke ( registerUuidUri , params ) ; logger . debug ( "request returned to client: {}" , response ) ; }
public boolean isFinancialYearActiveForPosting ( Date fromDate , Date toDate ) { String result = "" ; Query query = getCurrentSession ( ) . createQuery ( "" + " from CFinancialYear cfinancialyear where   cfinancialyear.isActiveForPosting=false and cfinancialyear.startingDate <=:sDate and cfinancialyear.endingDate >=:eDate  " ) ; query . setDate ( "sDate" , fromDate ) ; query . setDate ( "eDate" , toDate ) ; ArrayList list = ( ArrayList ) query . list ( ) ; if ( list . size ( ) > 0 ) return false ; logger . debug ( "Campaignyear active for posting: " + list ) ; else return true ; }
public void test() { try { String data = ( String ) s . get ( JsonKey . FEED_DATA ) ; code_block = IfStatement ; feedList . add ( mapper . convertValue ( s , Feed . class ) ) ; } catch ( Exception ex ) { log . error ( "Failed to get feed" , ex ) ; } }
public void test() { try { jsonObject . put ( "testsList" , array ) ; response . setContentType ( "application/json" ) ; response . getWriter ( ) . print ( jsonObject . toString ( ) ) ; } catch ( JSONException exception ) { LOG . error ( exception . toString ( ) ) ; } }
public void test() { if ( innerRank != size ) { log . error ( "invalidatorName=" + innerRank ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { if ( LOGGER . isWarnEnabled ( ) ) { LOGGER . warn ( "Unable to find parameter {}" , key ) ; } }
private List < BundleItem > getBundleListFromTDM ( ) throws Exception { _log . info ( "Retrieving bundle list from TDM..." ) ; ArrayList < BundleItem > output = new ArrayList < BundleItem > ( ) ; List < JsonObject > bundles = _apiLibrary . getItemsForRequest ( "bundle" , "list" ) ; code_block = ForStatement ; _log . info ( "Found " + output . size ( ) + " bundle(s) available from the TDM." ) ; return output ; }
private List < BundleItem > getBundleListFromTDM ( ) throws Exception { ArrayList < BundleItem > output = new ArrayList < BundleItem > ( ) ; _log . info ( "Getting current bundle list from TDM..." ) ; List < JsonObject > bundles = _apiLibrary . getItemsForRequest ( "bundle" , "list" ) ; code_block = ForStatement ; _log . info ( "Returning current bundle list from TDM..." ) ; return output ; }
public void test() { if ( level . intValue ( ) >= Level . SEVERE . intValue ( ) ) { slf4jLogger . error ( msg ) ; } else-if ( level . intValue ( ) >= Level . WARNING . intValue ( ) ) { slf4jLogger . warn ( msg ) ; } else-if ( level . intValue ( ) >= Level . CONFIG . intValue ( ) ) { slf4jLogger . info ( msg ) ; } else-if ( level . intValue ( ) >= Level . FINE . intValue ( ) ) { slf4jLogger . debug ( msg ) ; } else { slf4jLogger . trace ( msg ) ; } }
public void test() { if ( level . intValue ( ) >= Level . SEVERE . intValue ( ) ) { slf4jLogger . error ( msg ) ; } else-if ( level . intValue ( ) >= Level . WARNING . intValue ( ) ) { slf4jLogger . warn ( msg ) ; } else-if ( level . intValue ( ) >= Level . CONFIG . intValue ( ) ) { slf4jLogger . info ( msg ) ; } else-if ( level . intValue ( ) >= Level . FINE . intValue ( ) ) { slf4jLogger . debug ( msg ) ; } else { slf4jLogger . trace ( msg ) ; } }
public void test() { if ( level . intValue ( ) >= Level . SEVERE . intValue ( ) ) { slf4jLogger . error ( msg ) ; } else-if ( level . intValue ( ) >= Level . WARNING . intValue ( ) ) { slf4jLogger . warn ( msg ) ; } else-if ( level . intValue ( ) >= Level . CONFIG . intValue ( ) ) { slf4jLogger . info ( msg ) ; } else-if ( level . intValue ( ) >= Level . FINE . intValue ( ) ) { slf4jLogger . debug ( msg ) ; } else { slf4jLogger . trace ( msg ) ; } }
public void test() { if ( level . intValue ( ) >= Level . SEVERE . intValue ( ) ) { slf4jLogger . error ( msg ) ; } else-if ( level . intValue ( ) >= Level . WARNING . intValue ( ) ) { slf4jLogger . warn ( msg ) ; } else-if ( level . intValue ( ) >= Level . CONFIG . intValue ( ) ) { slf4jLogger . info ( msg ) ; } else-if ( level . intValue ( ) >= Level . FINE . intValue ( ) ) { slf4jLogger . debug ( msg ) ; } else { slf4jLogger . trace ( msg ) ; } }
public void test() { if ( level . intValue ( ) >= Level . SEVERE . intValue ( ) ) { slf4jLogger . error ( msg ) ; } else-if ( level . intValue ( ) >= Level . WARNING . intValue ( ) ) { slf4jLogger . warn ( msg ) ; } else-if ( level . intValue ( ) >= Level . CONFIG . intValue ( ) ) { slf4jLogger . info ( msg ) ; } else-if ( level . intValue ( ) >= Level . FINE . intValue ( ) ) { slf4jLogger . debug ( msg ) ; } else { slf4jLogger . log ( msg ) ; } }
public void test() { try { cancelJob ( datasetName , bqDataset ) ; } catch ( BigQueryException e ) { LOG . error ( "Exception when executing cleanup for stage '%s'" , datasetName ) ; ex = new SQLEngineException ( String . format ( "Exception when executing cleanup for stage '%s'" , datasetName ) , e ) ; } }
public void test() { try { deleteTable ( datasetName , bqDataset ) ; } catch ( BigQueryException e ) { LOGGER . info ( "Unable to delete the BigQuery" , e ) ; code_block = IfStatement ; } }
public void test() { try { pool . setDriverClass ( driver ) ; } catch ( PropertyVetoException ex ) { log . error ( "PropertyVetoException" , ex ) ; } }
public void test() { try { return Double . parseDouble ( provider . getAsString ( new OID ( ".1.3.6.1.4.1.2021.11.11.0" ) ) ) / 100.0 ; } catch ( Exception e ) { log . error ( "Unable to retrieve provider value" , e ) ; } }
private void killBookie ( ArrayList < BookieSocketAddress > firstEnsemble , BookieSocketAddress ensemble ) throws InterruptedException { logger . info ( "Killing bookie" ) ; killBookie ( ensemble ) ; }
public void test() { try { FileUtils . saveCommandHistoryString ( history , new File ( Constants . CMD_HISTORY_FILE ) ) ; } catch ( Throwable e ) { logger . error ( "cannot save command history" , e ) ; } }
@ Test public void testGetRootFolder ( ) throws Exception { final com . box . sdk . BoxFolder result = requestBody ( "direct://GETROOTFOLDER" , null ) ; assertNotNull ( result , "getRootFolder result" ) ; LOG . debug ( "getRootFolder: " + result ) ; }
private synchronized void stopping ( Bundle bundle ) { ModuleState . State moduleState = getModuleState ( bundle ) . getState ( ) ; code_block = IfStatement ; code_block = IfStatement ; logger . info ( "--- Starting DX OSGi bundle {}..." , getDisplayName ( bundle ) ) ; long startTime = System . currentTimeMillis ( ) ; JahiaTemplatesPackage jahiaTemplatesPackage = templatePackageRegistry . lookupByBundle ( bundle ) ; code_block = IfStatement ; code_block = IfStatement ; long totalTime = System . currentTimeMillis ( ) - startTime ; logger . info ( "--- Finished stopping DX OSGi bundle {} in {}ms --" , getDisplayName ( bundle ) , totalTime ) ; }
private synchronized void stopping ( Bundle bundle ) { ModuleState . State moduleState = getModuleState ( bundle ) . getState ( ) ; code_block = IfStatement ; logger . info ( "--- Stopping DX OSGi bundle {} --" , getDisplayName ( bundle ) ) ; code_block = IfStatement ; long startTime = System . currentTimeMillis ( ) ; JahiaTemplatesPackage jahiaTemplatesPackage = templatePackageRegistry . lookupByBundle ( bundle ) ; code_block = IfStatement ; code_block = IfStatement ; long totalTime = System . currentTimeMillis ( ) - startTime ; logger . info ( "--- Finished processing DX OSGi bundle {} --" , getDisplayName ( bundle ) ) ; }
public void test() { try { infoListBasicListTag . doTag ( infoListRendererContext . getHttpServletRequest ( ) , infoListRendererContext . getHttpServletResponse ( ) ) ; } catch ( Exception exception ) { _log . error ( "Unable to render journal articles list" , exception ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( isSpoolingEnabled ( conf ) && StringUtils . isNotEmpty ( spoolDir ) ) { LOG . info ( "Setting notification spool to {}" , spoolDir ) ; conf . setProperty ( CONF_ATLAS_HOOK_SPOOL_DIR , spoolDir ) ; notificationProvider = new AtlasFileSpool ( conf , kafka ) ; } else { LOG . info ( "Notification spooling is not enabled" ) ; notificationProvider = kafka ; } }
public void test() { if ( isSpoolingEnabled ( conf ) && StringUtils . isNotEmpty ( spoolDir ) ) { LOG . info ( "Notification spooling is enabled: spool directory={}" , spoolDir ) ; conf . setProperty ( CONF_ATLAS_HOOK_SPOOL_DIR , spoolDir ) ; notificationProvider = new AtlasFileSpool ( conf , kafka ) ; } else { LOG . info ( "Notification spooling is disabled" ) ; notificationProvider = kafka ; } }
public void test() { try { return findChannel ( channelName ) == null ; } catch ( IOException e ) { LOG . warn ( "Failed to open channel: " + channelName , e ) ; return false ; } }
public void test() { try { Object obj = compile ( expression , context ) ; return MVEL . executeExpression ( obj , vars ) ; } catch ( Throwable e ) { log . error ( e . getMessage ( ) , e ) ; return null ; } }
public void test() { if ( name != null ) { log . warn ( "Cannot close. Reason: {}" , name , e ) ; } else { log . warn ( "Cannot close. Reason: {}" , e . getMessage ( ) , e ) ; } }
public String registerBlockListener ( BlockingQueue < QueuedBlockEvent > blockEventQueue , long timeout , TimeUnit timeUnit ) throws InvalidArgumentException { code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; String handle = new BL ( blockEventQueue , timeout , timeUnit ) . getHandle ( ) ; logger . debug ( "Registering block listener {}" , handle ) ; return handle ; }
public void test() { try { logger . debug ( "Firing BuildResult: {}" , results ) ; final BuildResults results = buildService . build ( module ) ; buildResultsEvent . fire ( results ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { logger . info ( "Incremental build request being processed: " + module . getRootPath ( ) + " (updated)." ) ; final BuildResults results = buildService . build ( module ) ; buildResultsEvent . fire ( results ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { service . createVcVms ( spec , vNodes , null , false , null ) ; Assert . assertTrue ( false , "should throw exception but not." ) ; } catch ( Exception e ) { logger . info ( e . getMessage ( ) , e ) ; Assert . assertTrue ( true , "got expected exception." ) ; } }
@ Override publicCoachShuttleGatheringSolution readSolution ( ) throws IOException { solution = new CoachShuttleGatheringSolution ( ) ; solution . setId ( 0L ) ; readLocationList ( ) ; busOrStopOrHubId = 0L ; readBusList ( ) ; readBusStopList ( ) ; int busListSize = solution . getCoachList ( ) . size ( ) + solution . getShuttleList ( ) . size ( ) ; int base = solution . getStopList ( ) . size ( ) + solution . getShuttleList ( ) . size ( ) ; BigInteger a = factorial ( base + busListSize - 1 ) ; BigInteger b = factorial ( busListSize - 1 ) ; BigInteger possibleSolutionSize = ( a == null || b == null ) ? null : a . divide ( b ) ; log . info ( "Snapshots, possible solution size: " + possibleSolutionSize ) ; return solution ; }
public void test() { try { return binder . getApplicationTO ( applicationDAO . find ( key ) ) ; } catch ( Throwable ignore ) { LOG . debug ( "Unresolved reference" , ignore ) ; throw new UnresolvedReferenceException ( ignore ) ; } }
@ Test public void testGrantResource ( ) throws Exception { MultiValueMap < String , String > paramsMap = new LinkedMultiValueMap < > ( ) ; paramsMap . add ( "userId" , "32" ) ; paramsMap . add ( "resourceIds" , "5" ) ; MvcResult mvcResult = mockMvc . perform ( post ( "/users/grant-file" ) . header ( SESSION_ID , sessionId ) . params ( paramsMap ) ) . andExpect ( status ( ) . isOk ( ) ) . andExpect ( content ( ) . contentType ( MediaType . APPLICATION_JSON_UTF8 ) ) . andReturn ( ) ; Result result = JSONUtils . parseObject ( mvcResult . getResponse ( ) . getContentAsString ( ) , Result . class ) ; Assert . assertEquals ( Status . SUCCESS . getCode ( ) , result . getCode ( ) . intValue ( ) ) ; logger . info ( mvcResult . getResponse ( ) . getContentAsString ( ) ) ; }
public Module find ( final Long id ) { log . debug ( "find() - id: {}" , id ) ; return moduleRepository . findOne ( id ) ; }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Failed to clean up." , e ) ; } }
public void test() { if ( ! incomingFile . delete ( ) && incomingFile . exists ( ) ) { LOG . warn ( "Could not delete the staging file {}" , incomingFile ) ; } }
public void test() { try { Thread . sleep ( initialSleep ) ; } catch ( InterruptedException e ) { logger . error ( e ) ; } }
public void test() { if ( debug ) { logger . debug ( "Expected error: " + e . getMessage ( ) ) ; } }
public void test() { try { return context . getJCRPath ( path ) ; } catch ( NamespaceException e ) { LOGGER . log ( Level . WARNING , e . getMessage ( ) , e ) ; return path . toString ( ) ; } }
static void addRegistrationDateColumn ( Statement st , String tableName , Columns col ) throws SQLException { st . executeUpdate ( "ALTER TABLE " + tableName + " ADD COLUMN " + col . REGISTRATION_DATE + " BIGINT NOT NULL DEFAULT 0;" ) ; long currentTimestamp = System . currentTimeMillis ( ) ; int updatedRows = st . executeUpdate ( String . format ( "UPDATE %s SET %s = %d;" , tableName , col . REGISTRATION_DATE , currentTimestamp ) ) ; logger . info ( "Created column '" + col . REGISTRATION_DATE + "'" ) ; }
public void test() { try { resp = httpclient . execute ( httpget ) ; code_block = IfStatement ; } catch ( Exception e ) { log . error ( "HttpGet request failed" , e ) ; } }
public void test() { if ( ! directory . exists ( ) ) { logger . warn ( "Directory doesn't exist: " + directory ) ; return ; } }
public void test() { if ( ! moveSuccess ) { logger . warn ( Messages . getInstance ( ) . getErrorString ( "FileOutputHandler.ERROR_0003_FAILED" ) ) ; } }
public void test() { try { File directory = new File ( this . dirName ) ; code_block = IfStatement ; File [ ] listOfFiles = directory . listFiles ( new HL7A04FileFilter ( ) ) ; code_block = IfStatement ; String sendAddr = oscarProperties . getEmeraldHL7A04TransportAddr ( ) ; int sendPort = oscarProperties . getEmeraldHL7A04TransportPort ( ) ; Socket client = new Socket ( sendAddr , sendPort ) ; PrintStream out = new PrintStream ( client . getOutputStream ( ) ) ; BufferedReader in = new BufferedReader ( new InputStreamReader ( client . getInputStream ( ) ) ) ; logger . info ( "Sending file(s) to '" + sendAddr + ":" + sendPort + "'" ) ; int numErrors = 0 ; code_block = ForStatement ; in . close ( ) ; out . close ( ) ; if ( ! client . isClosed ( ) ) client . shutdownOutput ( ) ; if ( ! client . isClosed ( ) ) client . close ( ) ; if ( numErrors == 0 ) logger . info ( "Successfully sent HL7 A04 file(s)!" ) ; code_block = "" ; } catch ( Exception e ) { logger . error ( "Exception:" , e ) ; } }
public void test() { if ( null == group ) { logger . warn ( "No group in the server." ) ; } else { final String application = PreferencesFactory . get ( ) . getProperty ( "application.datafolder.name" ) ; final Local folder = new FinderLocal ( String . format ( "%s/Library/Application Support" , group . path ( ) ) , application ) ; final Local previous = new ApplicationSupportDirectoryFinder ( ) . find ( ) ; code_block = IfStatement ; return folder ; } }
public void test() { try { final Trash trash = LocalTrashFactory . get ( ) ; trash . trash ( previous ) ; final Symlink symlink = LocalSymlinkFactory . get ( ) ; symlink . symlink ( previous , folder . getAbsolute ( ) ) ; } catch ( AccessDeniedException e ) { log . warn ( String . format ( "Failure releasing symbolic link. %s" , e . getMessage ( ) ) ) ; } }
public void test() { try { FileUtils . copyDirectory ( new File ( previous . getAbsolute ( ) ) , new File ( folder . getAbsolute ( ) ) ) ; log . warn ( String . format ( "Move application support folder %s to Trash" , previous ) ) ; code_block = TryStatement ;  } catch ( IOException e ) { log . warn ( String . format ( "Failed to move application support folder %s" , folder ) , e ) ; } }
public void test() { if ( previous . exists ( ) && ! previous . isSymbolicLink ( ) ) { log . warn ( String . format ( "Migrate application support folder from %s to %s" , previous , folder ) ) ; code_block = TryStatement ;  } else { log . warn ( String . format ( "Skip migrate application support folder %s" , folder ) ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceShipmentServiceUtil . class , "getCommerceShipment" , _getCommerceShipmentParameterTypes4 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , commerceShipmentId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . commerce . model . CommerceShipment ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
@ Test public void testGetPaginatedGroup ( ) { logger . debug ( "testGetPaginatedEntities" ) ; super . testGetPaginatedEntities ( ) ; }
public void test() { try { Map < String , Object > dynamicMap = new HashMap < String , Object > ( ) ; TrasformazioniUtils . fillDynamicMapRispostaTracciatoCSV ( log , dynamicMap , ContextThreadLocal . get ( ) , headerRisposta , jsonEsito , codDominio , codTipoVersamento , dominio , applicazione , versamento , documento , esitoOperazione , descrizioneEsitoOperazione , tipoOperazione ) ; TrasformazioniUtils . convertFreeMarkerTemplate ( name , template , dynamicMap , bw ) ; log . debug ( "Trasformazione esito caricamento pendenza JSON -> CSV tramite template freemarker completata con successo." ) ; } catch ( TrasformazioneException e ) { log . error ( "Trasformazione esito caricamento pendenza JSON -> CSV tramite template freemarker completata con errore: " + e . getMessage ( ) , e ) ; throw new GovPayException ( e . getMessage ( ) , EsitoOperazione . TRASFORMAZIONE , e , e . getMessage ( ) ) ; } catch ( UnprocessableEntityException e ) { log . error ( "Trasformazione esito caricamento pendenza JSON -> CSV tramite template freemarker completata con errore: " + e . getMessage ( ) , e ) ; } }
public void test() { if ( reader . next ( key , value ) ) { LOG . info ( "Output File: " + status . getPath ( ) ) ; LOG . info ( "Output File: " + status . getPath ( ) ) ; assertEquals ( "Expected value: '" + expectedResult + "' != '" + value + "'" , expectedResult , value . get ( ) , delta ) ; } }
public void test() { if ( id == null ) { LOG . warn ( "Ignoring invalid request to " + cont ) ; } else { assignedContainerToIDMap . remove ( cont . getContainerId ( ) ) ; idToContainerMap . remove ( id ) ; String diagnostics = StringInterner . weakIntern ( cont . getDiagnostics ( ) ) ; code_block = IfStatement ; } }
public void test() { try { sendHeartbeat ( false ) ; } catch ( Exception e ) { logger . warn ( "Failed to send heart beat" , e ) ; } }
public void test() { try { log . info ( "Deleting secret {}." , secret . getAccountId ( ) ) ; secretsService . delete ( secret . getAccountId ( ) , secret . getValue ( ) ) ; } catch ( IOException e ) { throw new IllegalStateException ( e ) ; } }
private void processCurrentTagAgainstDesiredTags ( final IRODSTagValue currentTag , final String [ ] userTags , final UserAnnotatedCatalogItem irodsTagGrouping ) throws JargonException { log . info ( "processCurrentTagAgainstDesiredTags()" ) ; boolean isDesired = false ; code_block = ForStatement ; code_block = IfStatement ; }
public void test() { try { listener . updatedProfileImage ( user ) ; } catch ( Exception e ) { logger . warn ( "Exception at updateProfileImage" , e ) ; } }
public String getRemoteBssWsUrl ( ) { String servletAddress = "https://" + host + ":" + port + "/oscm/" ; code_block = IfStatement ; servletAddress += serviceName + "/" + servicePort . name ( ) ; String url = servletAddress + "?wsdl" ; code_block = IfStatement ; logger . debug ( "URL is: " + url ) ; verifyAttributes ( url ) ; return url ; }
@ Override public void removeStoreCommand ( final org . locationtech . geowave . service . grpc . protobuf . RemoveStoreCommandParametersProtos request , final StreamObserver < org . locationtech . geowave . service . grpc . protobuf . GeoWaveReturnTypesProtos . StringResponseProtos > responseObserver ) { final RemoveStoreCommand cmd = new RemoveStoreCommand ( ) ; final Map < FieldDescriptor , Object > m = request . getAllFields ( ) ; GeoWaveGrpcServiceCommandUtil . setGrpcToCommandFields ( m , cmd ) ; final File configFile = GeoWaveGrpcServiceOptions . geowaveConfigFile ; final OperationParams params = new ManualOperationParams ( ) ; params . getContext ( ) . put ( ConfigOptions . PROPERTIES_FILE_CONTEXT , configFile ) ; cmd . prepare ( params ) ; LOGGER . info ( "Executing RemoveStoreCommand..." ) ; code_block = TryStatement ;  }
public void test() { try { final String result = cmd . computeResults ( params ) ; final StringResponseProtos resp = StringResponseProtos . newBuilder ( ) . setResponseValue ( result ) . build ( ) ; responseObserver . onNext ( resp ) ; responseObserver . onCompleted ( ) ; } catch ( final Exception e ) { LOGGER . error ( "Exception encountered executing command" , e ) ; responseObserver . onError ( e ) ; } }
public void test() { if ( result . size ( ) == 0 ) { LOGGER . warn ( "Could not delete project {" + project . getName ( ) + "}" ) ; } else { commit . addDelete ( project ) ; } }
public MGsiegel findById ( java . lang . Short id ) { log . debug ( "getting MGsiegel instance with id: " + id ) ; code_block = TryStatement ;  }
public void test() { try { MGsiegel instance = ( MGsiegel ) getSession ( ) . get ( "sernet.gs.reveng.MGsiegel" , id ) ; return instance ; } catch ( RuntimeException re ) { log . error ( "get failed" , re ) ; throw re ; } }
private void changeInputValueForm2 ( int index , String keyword ) throws Exception { logger . info ( "Change configuration button" ) ; WebElement input = getSettingWebElement ( index ) ; input . clear ( ) ; input . sendKeys ( keyword ) ; driver . findElement ( By . xpath ( "//form[@id='" + AppHtmlElements . APP_CONFIG_FORM2 + "']" + "//input[@class='" + AppHtmlElements . APP_CONFIG_FORM_BUTTON_CLASS + "']" ) ) . click ( ) ; logger . info ( "Clicked save configuration button in APP settings" ) ; if ( ! getExecutionResult ( ) ) throw new Exception ( ) ; }
private void changeInputValueForm2 ( int index , String keyword ) throws Exception { WebElement input = getSettingWebElement ( index ) ; input . clear ( ) ; input . sendKeys ( keyword ) ; logger . info ( String . format ( "Wrote value: %s to element with id %s" , keyword , index ) ) ; driver . findElement ( By . xpath ( "//form[@id='" + AppHtmlElements . APP_CONFIG_FORM2 + "']" + "//input[@class='" + AppHtmlElements . APP_CONFIG_FORM_BUTTON_CLASS + "']" ) ) . click ( ) ; logger . info ( "Clicked element" ) ; if ( ! getExecutionResult ( ) ) throw new Exception ( ) ; }
public void test() { try { setupCurrentEntry ( ) ; hll . addRaw ( id . hashCode ( ) ) ; } catch ( Exception e ) { LOG . error ( "Error adding entry" , e ) ; } }
public void test() { try { return Integer . parseInt ( matcher . group ( 1 ) ) ; } catch ( NumberFormatException ex ) { logger . debug ( "failed to parse ip address: {}" , matcher . group ( 1 ) , ex ) ; } }
@ Test public void testLanguageJsonSchema ( ) throws Exception { CamelContext context = new DefaultCamelContext ( ) ; String json = context . getLanguageParameterJsonSchema ( "groovy" ) ; LOG . info ( "Using JSON: {}" , json ) ; assertNotNull ( "Should have found some auto-generated JSON" , json ) ; assertTrue ( json . contains ( "\"name\": \"groovy\"" ) ) ; assertTrue ( json . contains ( "\"modelName\": \"groovy\"" ) ) ; }
public void test() { try { Map < String , Object > changes = new HashMap < > ( ) ; changes . put ( "encodingType" , "SQUARE" ) ; changes . put ( "feature" , new JSONObject ( "{ \"type\": \"Point\", \"coordinates\": [-114.05, 51.05] }" ) ) ; changes . put ( "name" , "POIUYTREW" ) ; changes . put ( "description" , "POIUYTREW" ) ; return changes ; } catch ( JSONException ex ) { LOGGER . error ( "Exception:" , ex ) ; Assert . fail ( "Generating FeatureOfInterest changes failed: " + ex . getMessage ( ) ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( JournalArticleServiceUtil . class , "getGroupArticlesCount" , _getGroupArticlesCountParameterTypes43 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , userId , rootFolderId , status ) ; Object returnObj = null ; code_block = TryStatement ;  return ( ( Integer ) returnObj ) . intValue ( ) ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { LOG . debug ( String . format ( "Preparing to start process %s" , commandLine . toString ( ) ) ) ; printResult = executeProgram ( commandLine , workingDirectory , printJobTimeout , executeInBackground , successExitValue , streamHandler ) ; LOG . debug ( String . format ( "Successfully start process %s" , commandLine . toString ( ) ) ) ; } catch ( Exception ex ) { ex . printStackTrace ( ) ; LOG . error ( String . format ( "Failed to execute process %s" , commandLine . toString ( ) ) , ex ) ; return false ; } }
public static boolean executeProgram ( CommandLine commandLine , String workingDirectory , boolean executeInBackground , int successExitValue , OutputStream outputStream ) { LOG . debug ( String . format ( "Executing command %s" , commandLine . toString ( ) ) ) ; long printJobTimeout = PRINT_JOB_TIMEOUT ; ExecuteStreamHandler streamHandler = null ; code_block = IfStatement ; PrintResultHandler printResult = null ; code_block = TryStatement ;  code_block = TryStatement ;  LOG . debug ( String . format ( "Process %s has finished" , commandLine . toString ( ) ) ) ; return true ; }
public void test() { try { code_block = IfStatement ; printResult . waitFor ( ) ; } catch ( InterruptedException ex ) { logger . warn ( "" , ex ) ; } }
public void test() { try { producer = initTransactionalProducer ( transaction . transactionalId , false ) ; producer . resumeTransaction ( transaction . producerId , transaction . epoch ) ; producer . commitTransaction ( ) ; } catch ( InvalidTxnStateException | ProducerFencedException ex ) { LOG . warn ( "Got invalid transaction: {}" , ex . getLocalizedMessage ( ) ) ; } finally { code_block = IfStatement ; } }
public void test() { switch ( role ) { case VARIABLE_NAME : return DISTINGUISH_VARIABLES ; case FIELD_NAME : return DISTINGUISH_VARIABLES ; case FUNCTION_NAME : return DISTINGUISH_FUNCTIONS ; default : LOG . info ( "Unknown role: {}" , role ) ; return true ; } }
public void test() { if ( ! dir . mkdirs ( ) ) { log . warn ( "Unable to create {}" , dir ) ; } }
@ AdapterDelegationEvent ( beforeBuilder = PRPAIN201305UV02EventDescriptionBuilder . class , afterReturningBuilder = PRPAIN201306UV02EventDescriptionBuilder . class , serviceType = "Patient Discovery" , version = "LEVEL_a0" ) @ Override public PRPAIN201306UV02 respondingGatewayPRPAIN201305UV02 ( PRPAIN201305UV02 body , AssertionType assertion ) { LOG . trace ( "Using HPAIN201306UV02" ) ; return new PRPAIN201306UV02 ( ) ; }
public void test() { if ( LOG . isInfoEnabled ( ) ) { LOG . info ( String . format ( "Call to '%s' on file '%s'" , uri . toString ( ) , file ) ) ; } }
public void test() { if ( LOG . isInfoEnabled ( ) ) { LOG . info ( String . format ( "Call to '%s' on file '%s'" , uri . toString ( ) , file ) ) ; } }
public void test() { if ( LOG . isInfoEnabled ( ) ) { LOG . info ( String . format ( "Call to '%s' on file '%s'" , uri . toString ( ) , file ) ) ; } }
@ Test public void testTimePositionSubset ( ) { String xml = "<wfs:GetFeature " + "service=\"WFS\" " + "version=\"1.1.0\" " + "outputFormat=\"gml32\" " + "xmlns:cdf=\"http://www.opengis.net/cite/data\" " + "xmlns:ogc=\"http://www.opengis.net/ogc\" " + "xmlns:wfs=\"http://www.opengis.net/wfs\" " + "xmlns:gml=\"http://www.opengis.net/gml/3.2\" " + "xmlns:csml=\"" + TimeSeriesMockData . CSML_URI + "\" " + "xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" " + "xsi:schemaLocation=\"" + "http://www.opengis.net/wfs http://schemas.opengis.net/wfs/1.1.0/wfs.xsd" + "\"" + ">" + "<wfs:Query typeName=\"csml:PointSeriesFeature\">" + "    <ogc:Filter>" + "        <ogc:PropertyIsBetween>" + "             <ogc:PropertyName>csml:PointSeriesFeature/csml:value/csml:PointSeriesCoverage/csml:pointSeriesDomain/csml:TimeSeries/csml:timePositionList</ogc:PropertyName>" + "            <ogc:LowerBoundary><ogc:Literal>1949-05-01</ogc:Literal></ogc:LowerBoundary>" + "            <ogc:UpperBoundary><ogc:Literal>1949-09-01</ogc:Literal></ogc:UpperBoundary>" + "        </ogc:PropertyIsBetween>" + "    </ogc:Filter>" + "</wfs:Query> " + "</wfs:Query
public void test() { for ( ActiveQuerySnapshot q : sublist ) { LOGGER . debug ( String . format ( "Snapshot: %s" , q . getName ( ) ) ) ; } }
public boolean cancel ( final AccountDTO accountDTO ) { logger . debug ( "Cancelling account {}" , accountDTO ) ; return accountMapper . cancel ( accountDTO ) > 0 ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { for ( String cache : pools ) { code_block = IfStatement ; } }
public String getProcessInstanceVariable ( String containerId , Number processInstanceId , String varName , String marshallingType ) { containerId = context . getContainerId ( containerId , new ByProcessInstanceIdContainerLocator ( processInstanceId . longValue ( ) ) ) ; Object variable = processService . getProcessInstanceVariable ( containerId , processInstanceId . longValue ( ) , varName ) ; code_block = IfStatement ; String response = marshallerHelper . marshal ( containerId , marshallingType , variable ) ; logger . debug ( "Return '{}' from process instance '{}'" , response , containerId ) ; return response ; }
@ PostMapping public ContextDto create ( @ Valid @ RequestBody ContextDto contextDto ) { LOGGER . debug ( "Create ContextDto: {}" , contextDto ) ; final VitamContext vitamContext = securityService . buildVitamContext ( securityService . getTenantIdentifier ( ) ) ; return contextInternalService . create ( vitamContext , contextDto ) ; }
public void test() { if ( ! sensorState . equals ( DeviceTypeConstants . STATE_ON ) && ! sensorState . equals ( DeviceTypeConstants . STATE_OFF ) ) { log . debug ( "Device type {} not supported" , sensorState ) ; return Response . status ( Response . Status . BAD_REQUEST . getStatusCode ( ) ) . build ( ) ; } }
public void test() { try { code_block = IfStatement ; String sensorState = state . toUpperCase ( ) ; code_block = IfStatement ; Map < String , String > dynamicProperties = new HashMap < > ( ) ; String publishTopic = APIUtil . getAuthenticatedUserTenantDomain ( ) + "/" + DeviceTypeConstants . DEVICE_TYPE + "/" + deviceId + "/command" ; dynamicProperties . put ( DeviceTypeConstants . ADAPTER_TOPIC_PROPERTY , publishTopic ) ; APIUtil . getOutputEventAdapterService ( ) . publish ( DeviceTypeConstants . MQTT_ADAPTER_NAME , dynamicProperties , state ) ; return Response . ok ( ) . build ( ) ; } catch ( DeviceAccessAuthorizationException e ) { log . error ( e . getMessage ( ) , e ) ; return Response . status ( Response . Status . INTERNAL_SERVER_ERROR ) . build ( ) ; } }
public void test() { try { String rootKeyHex = CryptoUtils . extractKeyFromBootstrapFile ( ) ; LOG . info ( "Extracting Key from Bootstrap" ) ; return new SecretKeySpec ( Hex . decodeHex ( rootKeyHex . toCharArray ( ) ) , "AES" ) ; } catch ( IOException | DecoderException e ) { throw new KeyManagementException ( e ) ; } }
public void test() { if ( ! running . compareAndSet ( false , true ) ) { logger . info ( "[doRun][already shutdown]" ) ; return ; } }
public void test() { try { this . mongoClient = replicaSetsContext . createMongoClient ( replicaSetConfig ) ; this . checkReplicaMongo ( ) ; executorService . submit ( new ReplicatorTask ( this , mongoClient , replicaSetConfig , replicaSetsContext ) ) ; } catch ( Exception e ) { LOG . error ( "Replicator task failed." , e ) ; shutdown ( ) ; } }
public void test() { if ( ! autoStartScheduler ) { log . warn ( "Auto-startScheduler cannot be paused" ) ; } else { code_block = IfStatement ; } }
public void test() { if ( scheduler . isStarted ( ) ) { LOG . debug ( "Not starting scheduler with startDelayedSeconds={}" , startDelayedSeconds ) ; } else { LOG . info ( "Starting scheduler with startDelayedSeconds={}" , startDelayedSeconds ) ; scheduler . startDelayed ( startDelayedSeconds ) ; } }
public void test() { if ( scheduler . isStarted ( ) ) { LOG . info ( "Already started scheduler." ) ; } else { LOG . info ( "Starting scheduler." ) ; scheduler . start ( ) ; } }
public void test() { if ( scheduler . isStarted ( ) ) { LOG . info ( "The scheduler has already been started." ) ; } else { LOG . info ( "The scheduler has started." ) ; scheduler . start ( ) ; } }
public void run ( ) { log . info ( "Started event send for read" ) ; code_block = TryStatement ;  log . info ( "Completed event send for read" ) ; }
public void test() { try { code_block = WhileStatement ; } catch ( RuntimeException ex ) { log . error ( "Exception encountered: " + ex . getMessage ( ) , ex ) ; exception = ex ; } }
public void run ( ) { log . info ( "Started event send for read" ) ; code_block = TryStatement ;  log . info ( "Completed event send for read" ) ; }
@ Override public Response getLatestBundle ( ) { LOG . info ( "Get latest bundle" ) ; Bundle latestBundle = getLatestBundleAsBundle ( ) ; code_block = IfStatement ; return Response . ok ( "Error: No bundles deployed." ) . build ( ) ; }
public void test() { try { JSONObject response = new JSONObject ( ) ; response . put ( "id" , latestBundle . getId ( ) ) ; response . put ( "dataset" , latestBundle . getDataset ( ) ) ; response . put ( "name" , latestBundle . getName ( ) ) ; return Response . ok ( response . toString ( ) ) . build ( ) ; } catch ( Exception e ) { log . error ( "Error getting latest bundle" , e ) ; } }
public void test() { try { dataIn . reset ( ) ; } catch ( IOException e ) { logger . error ( "Failed to reset data in" , e ) ; } }
public void test() { try { SerialPortIdentifier portIdentifier = serialPortManager . getIdentifier ( serialPortName ) ; code_block = IfStatement ; SerialPort commPort = portIdentifier . open ( this . getClass ( ) . getName ( ) , 2000 ) ; commPort . setSerialPortParams ( 19200 , SerialPort . DATABITS_8 , SerialPort . STOPBITS_1 , SerialPort . PARITY_NONE ) ; commPort . enableReceiveThreshold ( 1 ) ; commPort . enableReceiveTimeout ( 100 ) ; commPort . setFlowControlMode ( SerialPort . FLOWCONTROL_NONE ) ; InputStream dataIn = commPort . getInputStream ( ) ; OutputStream dataOut = commPort . getOutputStream ( ) ; code_block = IfStatement ; code_block = IfStatement ; Thread thread = new KaleidescapeReaderThread ( this , this . uid , this . serialPortName ) ; setReaderThread ( thread ) ; thread . start ( ) ; this . serialPort = commPort ; this . dataIn = dataIn ; this . dataOut = dataOut ; setConnected ( true ) ; } catch ( PortInUseException e ) { setConnected ( false ) ; throw new KaleidescapeException ( "Opening serial connection failed: Port in Use Exception" , e ) ; } catch ( UnsupportedCommOperationException e ) { setConnected ( false ) ; throw new KaleidescapeException ( "Opening serial connection failed: Unsupported Comm Operation Exception" , e ) ; } catch ( UnsupportedEncodingException e ) { setConnected ( false ) ; throw new KaleidescapeException ( "Opening serial connection failed: Unsupported Encoding Exception" , e ) ; } catch ( IOException e ) { log . error ( "Opening serial connection failed: IO Exception" , e ) ; setConnected ( false ) ; throw new KaleidescapeException ( "Opening serial connection failed: IO Exception" , e ) ; } }
@ VisibleForTesting void logFailure ( AttachmentJobStatusResponse response ) throws InternalServerErrorException { CloudStackErrorResponse errorResponse = response . getErrorResponse ( ) ; String errorText = String . format ( FAILED_ATTACH_ERROR_MESSAGE , errorResponse . getErrorCode ( ) , errorResponse . getErrorText ( ) ) ; LOGGER . error ( errorText ) ; }
public void test() { if ( _startTime == null ) { _log . error ( _startTime + " not found" ) ; return false ; } }
public void test() { if ( ( _recurUnit == null && _recurInterval != null ) || ( _recurUnit != null && _recurInterval == null ) ) { _log . error ( "Locator " + _recurUnit + " is not a valid interval" ) ; return false ; } }
public void test() { if ( _recurInterval != null && _recurInterval <= 0 ) { _log . error ( "no record found to " + _recurId ) ; return false ; } }
public void test() { if ( converted < MIN_RECURRENCE_MILLIS ) { logger . warn ( "The value of {} cannot be converted from {} to {}" , value , converted , MIN_RECURRENCE_MILLIS ) ; return false ; } }
public void test() { try { Document doc = createDocument ( docketData , true ) ; XMLOutputter outp = new XMLOutputter ( ) ; outp . setFormat ( Format . getPrettyFormat ( ) ) ; outp . output ( doc , os ) ; os . close ( ) ; } catch ( RuntimeException e ) { LOG . error ( "Error connecting to Docket: {}" , e . getMessage ( ) ) ; throw new IOException ( e ) ; } }
public void test() { try ( MetaDataClient metaDataClient = metaDataClientFactory . getClient ( ) ) { SanityChecker . checkJsonAll ( jsonQuery ) ; LOGGER . debug ( "{}" , jsonNode ) ; final RequestParserMultiple parser = RequestParserHelper . getParser ( jsonQuery . deepCopy ( ) ) ; parser . getRequest ( ) . reset ( ) ; code_block = IfStatement ; jsonNode = metaDataClient . selectObjectGroups ( jsonQuery ) ; LOGGER . debug ( "DEBUG {}" , jsonNode ) ; } catch ( final InvalidParseOperationException e ) { LOGGER . error ( PARSING_ERROR , e ) ; throw e ; } catch ( final IllegalArgumentException e ) { LOGGER . error ( ILLEGAL_ARGUMENT , e ) ; throw e ; } catch ( final Exception e ) { LOGGER . error ( "exeption thrown" , e ) ; throw new AccessInternalExecutionException ( e ) ; } }
public void test() { try ( MetaDataClient metaDataClient = metaDataClientFactory . getClient ( ) ) { SanityChecker . checkJsonAll ( jsonQuery ) ; LOGGER . debug ( "{}" , jsonNode ) ; final RequestParserMultiple parser = RequestParserHelper . getParser ( jsonQuery . deepCopy ( ) ) ; parser . getRequest ( ) . reset ( ) ; code_block = IfStatement ; jsonNode = metaDataClient . selectObjectGroups ( jsonQuery ) ; LOGGER . debug ( " {}" , jsonNode ) ; } catch ( final InvalidParseOperationException e ) { LOGGER . error ( PARSING_ERROR , e ) ; throw e ; } catch ( final IllegalArgumentException e ) { LOGGER . error ( ILLEGAL_ARGUMENT , e ) ; throw e ; } catch ( final Exception e ) { LOGGER . error ( "exeption thrown" , e ) ; throw new AccessInternalExecutionException ( e ) ; } }
public void test() { try ( MetaDataClient metaDataClient = metaDataClientFactory . getClient ( ) ) { SanityChecker . checkJsonAll ( jsonQuery ) ; LOGGER . debug ( "{}" , jsonNode ) ; final RequestParserMultiple parser = RequestParserHelper . getParser ( jsonQuery . deepCopy ( ) ) ; parser . getRequest ( ) . reset ( ) ; code_block = IfStatement ; jsonNode = metaDataClient . selectObjectGroups ( jsonQuery ) ; LOGGER . debug ( "DEBUG {}" , jsonNode ) ; } catch ( final InvalidParseOperationException e ) { LOGGER . error ( PARSING_ERROR , e ) ; throw e ; } catch ( final IllegalArgumentException e ) { LOGGER . error ( ILLEGAL_ARGUMENT , e ) ; throw e ; } catch ( final Exception e ) { LOGGER . error ( "exeption thrown" , e ) ; throw new AccessInternalExecutionException ( e ) ; } }
public void test() { try ( MetaDataClient metaDataClient = metaDataClientFactory . getClient ( ) ) { SanityChecker . checkJsonAll ( jsonQuery ) ; LOGGER . debug ( "{}" , jsonNode ) ; final RequestParserMultiple parser = RequestParserHelper . getParser ( jsonQuery . deepCopy ( ) ) ; parser . getRequest ( ) . reset ( ) ; code_block = IfStatement ; jsonNode = metaDataClient . selectObjectGroups ( jsonQuery ) ; LOGGER . debug ( "DEBUG {}" , jsonNode ) ; } catch ( final InvalidParseOperationException e ) { LOGGER . error ( PARSING_ERROR , e ) ; throw e ; } catch ( final IllegalArgumentException e ) { LOGGER . error ( ILLEGAL_ARGUMENT , e ) ; throw e ; } catch ( final Exception e ) { LOGGER . error ( "exeption thrown" , e ) ; throw new AccessInternalExecutionException ( e ) ; } }
public void test() { try ( MetaDataClient metaDataClient = metaDataClientFactory . getClient ( ) ) { SanityChecker . checkJsonAll ( jsonQuery ) ; LOGGER . debug ( "{}" , jsonNode ) ; final RequestParserMultiple parser = RequestParserHelper . getParser ( jsonQuery . deepCopy ( ) ) ; parser . getRequest ( ) . reset ( ) ; code_block = IfStatement ; jsonNode = metaDataClient . selectObjectGroups ( jsonQuery ) ; LOGGER . debug ( "DEBUG {}" , jsonNode ) ; } catch ( final InvalidParseOperationException e ) { LOGGER . error ( PARSING_ERROR , e ) ; throw e ; } catch ( final IllegalArgumentException e ) { LOGGER . error ( ILLEGAL_ARGUMENT , e ) ; throw e ; } catch ( final Exception e ) { LOGGER . error ( "exeption thrown" , e ) ; throw new AccessInternalExecutionException ( e ) ; } }
public int read ( final String context ) throws IOException { int b = read ( ) ; logger . trace ( "read(context): {}" , context ) ; return b ; }
public void test() { try { dnsResponse = dnsClient . reverseLookup ( key . toString ( ) ) ; } catch ( Exception e ) { LOG . error ( e . getMessage ( ) , e ) ; errorCounter . inc ( ) ; return getErrorResult ( ) ; } }
public void test() { try { return Integer . parseInt ( sPoolSize ) ; } catch ( NumberFormatException exc ) { _log . error ( "Error in Setting the pool size to " + sPoolSize , exc ) ; } }
@ Override public void rip ( ) throws IOException { LOGGER . info ( "  Retrieving " + url ) ; Document doc = Http . url ( url ) . get ( ) ; List < String > mp4s = Utils . between ( doc . html ( ) , "file:\"" , "\"" ) ; code_block = IfStatement ; String vidUrl = mp4s . get ( 0 ) ; addURLToDownload ( new URL ( vidUrl ) , HOST + "_" + getGID ( this . url ) ) ; waitForThreads ( ) ; }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Refreshing." ) ; } }
@ Override public void init ( ) throws Exception { this . getCacheWrapper ( ) . initCache ( this . getWidgetTypeDAO ( ) ) ; logger . debug ( "{} initialized" , this . getClass ( ) . getName ( ) ) ; }
@ UserAggregationUpdate public void logAround ( ) { log . debug ( "1" ) ; log . info ( "2" ) ; log . warn ( "3" ) ; log . error ( "4" ) ; }
@ UserAggregationUpdate public void logAround ( ) { log . debug ( "1" ) ; log . info ( "2" ) ; log . warn ( "3" ) ; log . error ( "4" ) ; }
@ UserAggregationUpdate public void logAround ( ) { log . debug ( "1" ) ; log . info ( "2" ) ; log . warn ( "3" ) ; log . error ( "4" ) ; }
@ UserAggregationUpdate public void logAround ( ) { log . debug ( "1" ) ; log . info ( "2" ) ; log . warn ( "3" ) ; log . error ( "4" ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
protected void initParams ( final Request request ) { log . debug ( "Enter init..." ) ; uuid = ( String ) request . getAttributes ( ) . get ( "uuid" ) ; log . trace ( "Exit init" ) ; }
protected void initParams ( final Request request ) { log . trace ( "Entered init" ) ; uuid = ( String ) request . getAttributes ( ) . get ( "uuid" ) ; log . trace ( "Exiting init" ) ; }
public void test() { if ( deref == null ) { LOG . warn ( "Node {} was not found" , node ) ; } else { transformNode ( deref , matrix , results ) ; } }
public ResponseEntity < ResultsDto > findObjectById ( String id , ExternalHttpContext context ) { LOGGER . info ( "getObjectById started ..." ) ; return archiveSearchExternalRestClient . findObjectById ( id , context ) ; }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { atlasPluginClassLoader = AtlasPluginClassLoader . getInstance ( ATLAS_PLUGIN_TYPE , this . getClass ( ) ) ; @ SuppressWarnings ( "unchecked" ) Class < ExecuteWithHookContext > cls = ( Class < ExecuteWithHookContext > ) Class . forName ( ATLAS_HIVE_HOOK_IMPL_CLASSNAME , true , atlasPluginClassLoader ) ; activatePluginClassLoader ( ) ; hiveHookImpl = cls . newInstance ( ) ; } catch ( Exception excp ) { LOG . error ( "failed to instantiate hive hook" , excp ) ; } finally { deactivatePluginClassLoader ( ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( variable == null ) { logger . warn ( "Variable '" + name + "' not found" ) ; return ; } }
public void test() { try { Response < JsonElement > response = execJavaMethod ( transaction . getSession ( ) , object , m , transaction , request ) ; code_block = IfStatement ; } catch ( InvocationTargetException e ) { code_block = IfStatement ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; transaction . sendError ( e ) ; } }
public void test() { if ( published ) { LOG . info ( "Published segments: {}" , segments ) ; } else { throw new ISE ( "Failed to publish segments" ) ; } }
public void test() { if ( getCreated ( ) != null && getCreated ( ) . after ( invocationDate ) ) { _log . info ( "Calling " + getCreated ( ) + " since " + invocationDate ) ; return false ; } else { return true ; } }
public void test() { if ( alertConfigSpec == null ) { logger . error ( "Invalid alert configuration: {}" , scheduledJobKey ) ; stopJob ( scheduledJobKey ) ; } }
@ Override public void start ( Collection < ? extends Location > locs ) { callHistory . add ( "start" ) ; ServiceStateLogic . setExpectedState ( this , Lifecycle . STARTING ) ; counter . incrementAndGet ( ) ; LOG . info ( "Started " + locs . size ( ) + " messages" ) ; addLocations ( locs ) ; sensors ( ) . set ( SERVICE_UP , true ) ; ServiceStateLogic . setExpectedState ( this , Lifecycle . RUNNING ) ; }
public void test() { try { transport . disconnect ( true ) ; } catch ( IOException e ) { log . warn ( "Error on closing transport connection" , e ) ; se . addSuppressed ( e ) ; } }
public void test() { try { getPage ( i ) . subscribeEvents ( "playing" ) ; getPage ( i ) . initWebRtc ( webRtcReceiver [ i - 1 ] , WebRtcChannel . AUDIO_AND_VIDEO , WebRtcMode . RCV_ONLY ) ; Assert . assertTrue ( "Not received media in receiver " + i , getPage ( i ) . waitForEvent ( "playing" ) ) ; recorder [ i - 1 ] . record ( ) ; Thread . sleep ( PLAYTIME_MS ) ; recorder [ i - 1 ] . stopAndWait ( ) ; Thread . sleep ( 4000 ) ; } catch ( InterruptedException e ) { logger . error ( "" , e ) ; } }
public void test() { if ( ! service . getClusterVersion ( ) . equals ( request . getClusterVersion ( ) ) ) { logger . info ( "Ignoring cluster version change request for {} " , service ) ; return false ; } }
public void test() { if ( validFields . size ( ) != 1 ) { log . info ( "Ignoring field '" + field . getName ( ) + "' on " + field . getName ( ) ) ; return ; } }
public void test() { try { List < String > validFields = dataSourceProperties . getValidField ( ) ; code_block = IfStatement ; AbstractDataSourceProperties abstractDataSourceProperties = dataSourceProperties . getValidDataSourceProperties ( ) ; abstractDataSourceProperties . setEnv ( env ) ; abstractDataSourceProperties . preCheck ( dataSourceName ) ; registerBean ( abstractDataSourceProperties , dataSourceName + "-sentinel-" + validFields . get ( 0 ) + "-datasource" ) ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; } }
public void test() { while ( true ) { Query query = new QueryImpl ( filter , 1 , BATCH_SIZE , SortBy . NATURAL_ORDER , false , TimeUnit . SECONDS . toMillis ( 90 ) ) ; QueryRequest queryRequest = new QueryRequestImpl ( query ) ; LOGGER . trace ( "Removing existing geonames data with filter: {}" , filter ) ; QueryResponse response = catalogFramework . query ( queryRequest ) ; LOGGER . trace ( "Response: {}" , response ) ; List < Serializable > metacardsToDelete = response . getResults ( ) . stream ( ) . map ( Result :: getMetacard ) . map ( Metacard :: getId ) . collect ( Collectors . toList ( ) ) ; code_block = IfStatement ; removeMetacards ( catalogProvider , extractionCallback , metacardsToDelete ) ; } }
public void test() { if ( hasDiff ) { logger . warn ( "Some of errors in the same result file: " + result ) ; } }
@ Test public void testDeleted ( ) { SentinelHello normalHello = new SentinelHello ( sentinels . iterator ( ) . next ( ) , masterAddr , sentinelMonitorName ) ; HostPort remoteDcMAster = new HostPort ( "127.0.0.1" , 7379 ) ; Mockito . when ( metaCache . getDc ( remoteDcMAster ) ) . thenReturn ( "remote-dc" ) ; Set < SentinelHello > hellos = new HashSet < > ( ) ; hellos . add ( normalHello ) ; hellos . add ( new SentinelHello ( new HostPort ( "11.0.0.1" , 5000 ) , masterAddr , sentinelMonitorName ) ) ; hellos . add ( new SentinelHello ( sentinels . iterator ( ) . next ( ) , remoteDcMAster , sentinelMonitorName ) ) ; hellos . add ( new SentinelHello ( sentinels . iterator ( ) . next ( ) , masterAddr , "error-monitor-name" ) ) ; Set < SentinelHello > needDeletedHello = collector . checkAndDelete ( sentinelMonitorName , sentinels , hellos , quorumConfig , masterAddr ) ; LOG . info ( "{}" , needDeletedHello ) ; Assert . assertEquals ( 3 , needDeletedHello . size ( ) ) ; Assert . assertFalse ( needDeletedHello . contains ( normalHello ) ) ; Assert . assertEquals ( 1 , hellos . size ( ) ) ; Assert . assertTrue ( hellos . contains ( normalHello ) ) ; }
public void test() { for ( CacheConfiguration < Object , Object > ccfg : cacheCfgs ) { ccfg . setCacheStoreFactory ( new TestStoreFactory ( ) ) ; ccfg . setReadThrough ( false ) ; boolean near = ( ccfg . getNearConfiguration ( ) != null ) ; log . info ( "Test cache [mode=" + ccfg . getCacheMode ( ) + ", atomicity=" + ccfg . getAtomicityMode ( ) + ", backups=" + ccfg . getBackups ( ) + ", near=" + near + "]" ) ; ignite ( 0 ) . createCache ( ccfg ) ; awaitPartitionMapExchange ( ) ; code_block = TryStatement ;  } }
@ Before public void setup ( ) throws Exception { log . debug ( "Setting up testcase" ) ; policyFile = super . setupPolicy ( ) ; super . setup ( ) ; }
public void test() { if ( imageURL == null ) { logger . error ( "Failed to find image at " + url ) ; return null ; } else { return ( new ImageIcon ( imageURL , description ) ) . getImage ( ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception exc ) { log . error ( exc . toString ( ) , exc ) ; } }
private Application create ( AssemblyTemplate template , CampPlatform platform ) { ManagementContext mgmt = getManagementContext ( platform ) ; BrooklynClassLoadingContext loader = JavaBrooklynClassLoadingContext . create ( mgmt ) ; EntitySpec < ? extends Application > spec = createApplicationSpec ( template , platform , loader , MutableSet . < String > of ( ) ) ; Application instance = mgmt . getEntityManager ( ) . createEntity ( spec ) ; log . debug ( "Created " + instance ) ; return instance ; }
public void test() { if ( assignmentErrorCode . get ( ) == AssignorError . SHUTDOWN_REQUESTED . code ( ) ) { LOG . info ( "Assigned rebalance for {}" , assignmentId ) ; mainConsumer . enforceRebalance ( ) ; } }
public void test() { if ( reason != null ) { reason . addSuppressed ( e ) ; } else { LOGGER . debug ( null , e ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { if ( getParent ( ) == null ) { log . warn ( "Cannot save parent because no parent" ) ; return ; } }
public void test() { try { ItemPath virtualItemPath = getVirtualItemPath ( virtualItem ) ; ItemWrapper itemWrapper = objectWrapper . findItem ( virtualItemPath , ItemWrapper . class ) ; code_block = IfStatement ; code_block = IfStatement ; nonContainers . add ( itemWrapper ) ; } catch ( SchemaException e ) { logger . error ( e . getMessage ( ) , e ) ; } }
@ Override public void initializeUI ( UIBuilder builder ) throws Exception { StopWatch watch = new StopWatch ( ) ; final UIContext context = builder . getUIContext ( ) ; letsChat = ( LetsChatClient ) builder . getUIContext ( ) . getAttributeMap ( ) . get ( "letsChatClient" ) ; taigaClient = ( TaigaClient ) builder . getUIContext ( ) . getAttributeMap ( ) . get ( "taigaClient" ) ; ProjectConfig config = ( ProjectConfig ) context . getAttributeMap ( ) . get ( "projectConfig" ) ; code_block = IfStatement ; builder . add ( chatRoom ) ; builder . add ( issueProjectName ) ; builder . add ( codeReview ) ; log . info ( "API : " + watch . taken ( ) ) ; }
public List findByExample ( FilterResZob instance ) { log . debug ( "finding FilterResZob instance by example" ) ; code_block = TryStatement ;  }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.FilterResZob" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.FilterResZob" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
public void test() { for ( IOException e ) { LOG . warn ( "Failed to connect to {}" , DOCKER_FOR_LINUX_STATIC_IP ) ; return DOCKER_FOR_LINUX_STATIC_IP ; } }
@ Test public void test_05 ( ) { Log . debug ( "Test" ) ; double pvals [ ] = code_block = "" ; ; ScoreList gpl = new ScoreList ( ) ; for ( double pval : pvals ) gpl . add ( pval ) ; double pvalue = gpl . score ( ScoreSummary . FISHER_CHI_SQUARE ) ; Assert . assertEquals ( 0.021561751324834642 , pvalue , EPSILON ) ; }
public void test() { if ( this . bluetoothAdapter != null ) { logger . info ( "Bluetooth adapter address => {}" , this . bluetoothAdapter . getAddress ( ) ) ; logger . info ( "Bluetooth adapter address => {}" , this . bluetoothAdapter . getAddress ( ) ) ; logger . info ( "Bluetooth adapter le enabled => {}" , this . bluetoothAdapter . isLeReady ( ) ) ; code_block = IfStatement ; configureBeacon ( ) ; } else { logger . warn ( "No Bluetooth adapter found ..." ) ; } }
public void test() { if ( this . bluetoothAdapter != null ) { logger . info ( "Bluetooth adapter interface => {}" , this . name ) ; logger . info ( "Bluetooth adapter address => {}" , this . bluetoothAdapter . getAddress ( ) ) ; logger . info ( "Bluetooth adapter address => {}" , this . bluetoothAdapter . getAddress ( ) ) ; code_block = IfStatement ; configureBeacon ( ) ; } else { logger . warn ( "No Bluetooth adapter found ..." ) ; } }
public void test() { if ( ! this . bluetoothAdapter . isEnabled ( ) ) { logger . info ( "Enabling bluetooth adapter..." ) ; this . bluetoothAdapter . enable ( ) ; logger . info ( "Bluetooth adapter address => {}" , this . bluetoothAdapter . getAddress ( ) ) ; } }
public void test() { if ( this . bluetoothAdapter != null ) { logger . info ( "Bluetooth adapter interface => {}" , this . name ) ; logger . info ( "Bluetooth adapter address => {}" , this . bluetoothAdapter . getAddress ( ) ) ; logger . info ( "Bluetooth adapter le enabled => {}" , this . bluetoothAdapter . isLeReady ( ) ) ; code_block = IfStatement ; configureBeacon ( ) ; } else { logger . info ( "No Bluetooth adapter found ... {}" , this . name ) ; } }
protected void updated ( Map < String , Object > properties ) { logger . debug ( "Updating bluetooth adapter..." ) ; doUpdate ( properties ) ; this . bluetoothAdapter . stopBeaconAdvertising ( ) ; this . bluetoothAdapter = null ; this . bluetoothAdapter = this . bluetoothService . getBluetoothAdapter ( this . name , this ) ; code_block = IfStatement ; }
public void test() { try { Integer version = processDefinitionLogMapper . queryMaxVersionForDefinition ( processDefinition . getCode ( ) ) ; ProcessDefinitionLog processDefinitionLog = new ProcessDefinitionLog ( processDefinition ) ; processDefinitionLog . setVersion ( version == null || version == 0 ? 1 : version + 1 ) ; processDefinitionLog . setProjectCode ( targetProjectCode ) ; processDefinitionLog . setOperator ( loginUser . getId ( ) ) ; Date now = new Date ( ) ; processDefinitionLog . setOperateTime ( now ) ; processDefinitionLog . setUpdateTime ( now ) ; processDefinitionLog . setCreateTime ( now ) ; int update = processDefinitionMapper . updateById ( processDefinitionLog ) ; int insertLog = processDefinitionLogMapper . insert ( processDefinitionLog ) ; code_block = IfStatement ; return processDefinitionLog ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; putMsg ( result , Status . UPDATE_PROCESS_DEFINITION_ERROR ) ; failedProcessList . add ( processDefinition . getId ( ) + "[" + processDefinition . getName ( ) + "]" ) ; } }
private int runCommand ( boolean json ) throws InterruptedException , ExecutionException , IOException { final int ret = command . run ( options , client , out , json , null ) ; Logger . debug ( this , "Run %s" , ret ) ; return ret ; }
private BrooklynNode setUpBrooklynNodeWithApp ( ) throws InterruptedException , ExecutionException { BrooklynNode brooklynNode = app . createAndManageChild ( EntitySpec . create ( BrooklynNode . class ) . configure ( BrooklynNode . NO_WEB_CONSOLE_AUTHENTICATION , Boolean . TRUE ) ) ; app . start ( locs ) ; EntityAsserts . assertAttributeEqualsEventually ( brooklynNode , BrooklynNode . SERVICE_UP , true ) ; String baseUrl = brooklynNode . getAttribute ( BrooklynNode . WEB_CONSOLE_URI ) . toString ( ) ; waitForApps ( baseUrl ) ; final String id = brooklynNode . invoke ( BrooklynNode . DEPLOY_BLUEPRINT , ConfigBag . newInstance ( ) . configure ( DeployBlueprintEffector . BLUEPRINT_TYPE , BasicApplication . class . getName ( ) ) . getAllConfig ( ) ) . get ( ) ; String entityUrl = Urls . mergePaths ( baseUrl , "v1/applications/" , id , "entities" , id ) ; Entity mirror = brooklynNode . addChild ( EntitySpec . create ( BrooklynEntityMirror . class ) . configure ( BrooklynEntityMirror . MIRRORED_ENTITY_URL , entityUrl ) . configure ( BrooklynEntityMirror . MIRRORED_ENTITY_ID , id ) ) ; LOG . info ( "Created {}" , mirror ) ; assertEquals ( brooklynNode . getChildren ( ) . size ( ) , 1 ) ; return brooklynNode ; }
public void test() { try { MethodKey methodKey = new MethodKey ( SocialActivityServiceUtil . class , "getUserGroupsAndOrganizationsActivitiesCount" , _getUserGroupsAndOrganizationsActivitiesCountParameterTypes28 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , userId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( ( Integer ) returnObj ) . intValue ( ) ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( exported ) { _log . info ( "Json exported" ) ; } }
public void test() { try { int [ ] domains = conn . listDomains ( ) ; s_logger . debug ( String . format ( "found %d domains" , domains . length ) ) ; code_block = ForStatement ; } catch ( LibvirtException e ) { s_logger . debug ( "Failed to find domains" , e ) ; } }
public static void dumpToLog ( final AtlasGraph < ? , ? > graph ) { LOG . debug ( "*******************Graph Dump****************************" ) ; code_block = ForStatement ; LOG . debug ( "All edges of {}" , graph ) ; LOG . debug ( "Edges of {}" , graph ) ; code_block = ForStatement ; LOG . debug ( "*******************Graph Dump****************************" ) ; }
public void test() { for ( AtlasVertex vertex : graph . getVertices ( ) ) { LOG . info ( vertex . getId ( ) ) ; } }
public static void dumpToLog ( final AtlasGraph < ? , ? > graph ) { LOG . debug ( "*******************Graph Dump****************************" ) ; LOG . debug ( "Vertices of {}" , graph ) ; LOG . debug ( "Vertices of {}" , graph ) ; code_block = ForStatement ; code_block = ForStatement ; LOG . debug ( "*******************Graph Dump****************************" ) ; }
public void test() { for ( AtlasEdge edge : graph . getEdges ( ) ) { LOG . debug ( "found edge: {}" , edge ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { Object in = object ; code_block = IfStatement ; LOG . debug ( "Converted object to {}" , object ) ; } }
public void test() { if ( response != null ) { LOG . debug ( "Writing response to Mina" ) ; MinaHelper . writeBody ( session , response , exchange , configuration . getWriteTimeout ( ) ) ; } else { LOG . debug ( "Writing no response" ) ; disconnect = Boolean . TRUE ; } }
public void test() { if ( response != null ) { LOG . debug ( "Writing body: {}" , response ) ; MinaHelper . writeBody ( session , response , exchange , configuration . getWriteTimeout ( ) ) ; } else { LOG . debug ( "No response to send." ) ; disconnect = Boolean . TRUE ; } }
public void test() { try { resetSearchers ( ) ; code_block = ForStatement ; _indexManager . enqueue ( mutations ) ; } catch ( Exception e ) { LOG . error ( "Unknown error while trying to reindex [{0}]" , e ) ; code_block = IfStatement ; throw new BException ( e . getMessage ( ) , e ) ; } }
public void test() { try { return connectedUser . equals ( fromUser ) || emailIsAnAliasOfTheConnectedUser ( connectedUser , fromUser ) ; } catch ( RecipientRewriteTableException | RecipientRewriteTable . ErrorMappingException e ) { logger . error ( e . getMessage ( ) , e ) ; return false ; } }
@ NwhinInvocationEvent ( beforeBuilder = DeferredResponseDescriptionBuilder . class , afterReturningBuilder = DeferredResponseDescriptionBuilder . class , serviceType = "Document Submission Deferred Response" , version = "1.1" ) @ Override public XDRAcknowledgementType provideAndRegisterDocumentSetBDeferredResponse11 ( RegistryResponseType request , AssertionType assertion , NhinTargetSystemType target ) { LOG . debug ( "Begin provideAndRegisterDocumentSetBDeferredResponse" ) ; XDRAcknowledgementType response = null ; code_block = TryStatement ;  LOG . debug ( "End provideAndRegisterDocumentSetBDeferredResponse" ) ; return response ; }
@ NwhinInvocationEvent ( beforeBuilder = DeferredResponseDescriptionBuilder . class , afterReturningBuilder = DeferredResponseDescriptionBuilder . class , serviceType = "Document Submission Deferred Response" , version = "1.1" ) @ Override public XDRAcknowledgementType provideAndRegisterDocumentSetBDeferredResponse11 ( RegistryResponseType request , AssertionType assertion , NhinTargetSystemType target ) { LOG . debug ( "Begin provideAndRegisterDocumentSetBDeferredResponse" ) ; XDRAcknowledgementType response = null ; code_block = TryStatement ;  LOG . debug ( "End provideAndRegisterDocumentSetBDeferredResponse" ) ; return response ; }
public void test() { try { s = c . prepareStatement ( "SELECT template_id, orderxml, isActive FROM ordertemplates WHERE name = ?" ) ; s . setString ( 1 , orderXmlName ) ; ResultSet res = s . executeQuery ( ) ; code_block = IfStatement ; Reader orderTemplateReader = null ; long template_id = res . getLong ( 1 ) ; code_block = IfStatement ; HeritrixTemplate heritrixTemplate = HeritrixTemplate . read ( template_id , orderTemplateReader ) ; heritrixTemplate . setIsActive ( res . getBoolean ( 3 ) ) ; return heritrixTemplate ; } catch ( SQLException e ) { final String message = "SQL error finding order.xml to " + orderXmlName + "\n" + ExceptionUtils . getSQLExceptionCause ( e ) ; log . warn ( message , e ) ; throw new IOFailure ( message , e ) ; } finally { DBUtils . closeStatementIfOpen ( s ) ; HarvestDBConnection . release ( c ) ; } }
public void test() { if ( checkName instanceof Text ) { return checkName . toString ( ) ; } else { LOGGER . warn ( MessageFormat . format ( this . getActionExecution ( ) . getAction ( ) . getType ( ) + " does not accept {0} as type for checkName" , checkName . getClass ( ) ) ) ; return checkName . toString ( ) ; } }
public static void main ( final String [ ] args ) { final long start = System . currentTimeMillis ( ) ; final String inputFilePath = args [ 0 ] ; final Integer numFeatures = Integer . parseInt ( args [ 1 ] ) ; final Integer numClasses = Integer . parseInt ( args [ 2 ] ) ; final Integer numItr = Integer . parseInt ( args [ 3 ] ) ; final List < Integer > initialModelKeys = new ArrayList < > ( numClasses ) ; code_block = ForStatement ; final PipelineOptions options = NemoPipelineOptionsFactory . create ( ) ; options . setJobName ( "MLR" ) ; options . setStableUniqueNames ( PipelineOptions . CheckEnabled . OFF ) ; final Pipeline p = Pipeline . create ( options ) ; PCollection < KV < Integer , List < Double > > > model = p . apply ( Create . of ( initialModelKeys ) ) . apply ( ParDo . of ( new DoFn < Integer , KV < Integer , List < Double > > > ( ) code_block = "" ; ) ) ; LOG . info ( "Reading " + inputFilePath ) ; final PCollection < String > readInput = GenericSourceSink . read ( p , inputFilePath ) ; code_block = ForStatement ; p . run ( ) . waitUntilFinish ( ) ; LOG . info ( "JCT " + ( System . currentTimeMillis ( ) - start ) ) ; }
public static void main ( final String [ ] args ) { final long start = System . currentTimeMillis ( ) ; LOG . info ( Arrays . toString ( args ) ) ; final String inputFilePath = args [ 0 ] ; final Integer numFeatures = Integer . parseInt ( args [ 1 ] ) ; final Integer numClasses = Integer . parseInt ( args [ 2 ] ) ; final Integer numItr = Integer . parseInt ( args [ 3 ] ) ; final List < Integer > initialModelKeys = new ArrayList < > ( numClasses ) ; code_block = ForStatement ; final PipelineOptions options = NemoPipelineOptionsFactory . create ( ) ; options . setJobName ( "MLR" ) ; options . setStableUniqueNames ( PipelineOptions . CheckEnabled . OFF ) ; final Pipeline p = Pipeline . create ( options ) ; PCollection < KV < Integer , List < Double > > > model = p . apply ( Create . of ( initialModelKeys ) ) . apply ( ParDo . of ( new DoFn < Integer , KV < Integer , List < Double > > > ( ) code_block = "" ; ) ) ; final PCollection < String > readInput = GenericSourceSink . read ( p , inputFilePath ) ; code_block = ForStatement ; p . run ( ) . waitUntilFinish ( ) ; LOG . info ( Arrays . toString ( System . currentTimeMillis ( ) - start ) ) ; }
public void test() { try { pulsar ( ) . getBrokerService ( ) . deleteTopic ( topicName . toString ( ) , false , deleteSchema ) . get ( ) ; log . info ( "[{}] Successfully removed topic {}" , clientAppId ( ) , topicName ) ; } catch ( Exception e ) { log . error ( "[{}] Failed to remove topic {}" , clientAppId ( ) , topicName , e ) ; Throwable t = e . getCause ( ) ; code_block = IfStatement ; } }
public void test() { if ( shouldLogAsWarning ( msg . getSeverity ( ) ) ) { log . warn ( msg . getSeverity ( ) ) ; } }
public void test() { for ( String s : result ) { logger . info ( s ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { logger . info ( context . getResultSender ( ) , e ) ; context . getResultSender ( ) . sendException ( e ) ; } }
@ Override public Object call ( ) { Thread . currentThread ( ) . setName ( THREAD_NAME ) ; log . trace ( "Amqp reactor thread {} started" , THREAD_NAME ) ; code_block = TryStatement ;  log . trace ( "Amqp reactor thread {} has finished" , THREAD_NAME ) ; return null ; }
public void test() { try { amqpReactor . run ( ) ; } catch ( HandlerException e ) { log . warn ( e . getMessage ( ) , e ) ; throw e ; } }
public void test() { if ( notification . getDeviceToken ( ) != null && notification . getDeviceToken ( ) . length ( ) > 0 && appPropertiesDetails != null ) { authKey = appPropertiesDetails . getAndroidServerKey ( ) ; URL url = new URL ( ( String ) applicationPropertyConfiguration . getApiUrlFcm ( ) ) ; HttpURLConnection conn = ( HttpURLConnection ) url . openConnection ( ) ; conn . setUseCaches ( false ) ; conn . setDoInput ( true ) ; conn . setDoOutput ( true ) ; conn . setRequestMethod ( "POST" ) ; conn . setRequestProperty ( "Authorization" , "key=" + authKey ) ; conn . setRequestProperty ( "Content-Type" , "application/json" ) ; JSONObject json = new JSONObject ( ) ; json . put ( "registration_ids" , notification . getDeviceToken ( ) ) ; json . put ( "priority" , "high" ) ; JSONObject dataInfo = new JSONObject ( ) ; dataInfo . put ( "subtype" , notification . getNotificationSubType ( ) ) ; dataInfo . put ( "type" , notification . getNotificationType ( ) ) ; dataInfo . put ( "title" , notification . getNotificationTitle ( ) ) ; dataInfo . put ( "message" , notification . getNotificationText ( ) ) ; code_block = IfStatement ; json . put ( "data" , dataInfo ) ; OutputStreamWriter wr = new OutputStreamWriter ( conn . getOutputStream ( ) ) ; wr . write ( json . toString ( ) ) ; wr . flush ( ) ; String response = IOUtils . toString ( conn . getInputStream ( ) , StandardCharsets . UTF_8 ) ; JsonNode responseJson = new ObjectMapper ( ) . readTree ( response ) ; FcmPushNotificationResponse fcmNotificationResponse = new FcmPushNotificationResponse ( responseJson , conn . getResponseCode ( ) , conn . getResponseMessage ( ) ) ; logger . info ( "FcmPush notification response: {}" , fcmNotificationResponse ) ; return fcmNotificationResponse ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( DLAppServiceUtil . class , "unsubscribeFolder" , _unsubscribeFolderParameterTypes97 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , folderId ) ; code_block = TryStatement ;  } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
@ Test public void testCreateAndRemoveElement ( ) throws Exception { uuidList = new LinkedList < String > ( ) ; Organization organization = createOrganization ( ) ; uuidList . add ( organization . getUuid ( ) ) ; checkOrganization ( organization ) ; uuidList . addAll ( createElementsInGroups ( organization , NUMBER_PER_GROUP ) ) ; logger . debug ( "Number of elements: " + uuidList . size ( ) ) ; RemoveElement < CnATreeElement > removeCommand = new RemoveElement < CnATreeElement > ( organization ) ; commandService . executeCommand ( removeCommand ) ; code_block = ForStatement ; }
public void test() { try ( BufferedReader r = new BufferedReader ( new InputStreamReader ( response . getEntity ( ) . getContent ( ) ) ) ) { content = r . readLine ( ) ; } catch ( Exception e ) { LOGGER . warn ( "Failed to close connection" , e ) ; } finally { closeConnection ( response ) ; } }
public void test() { try { getModelRepository ( ModelId . fromPrettyFormat ( modelId ) ) . removeModel ( ModelId . fromPrettyFormat ( modelId ) ) ; return new ResponseEntity < > ( false , HttpStatus . OK ) ; } catch ( FatalModelRepositoryException | NullPointerException e ) { log . error ( e . getMessage ( ) , e ) ; return new ResponseEntity < > ( false , HttpStatus . NOT_FOUND ) ; } }
public void test() { try { Method method = dsClass . getMethod ( methodName ) ; method . invoke ( ds ) ; } catch ( NoSuchMethodException e ) { LOG . warn ( "Unable to register {} access for dataset {}" , accessType , referenceName ) ; } catch ( Exception e ) { LOG . warn ( "Unable to register {} access for dataset {}" , accessType , referenceName ) ; } }
public void test() { try { Method method = dsClass . getMethod ( methodName ) ; method . invoke ( ds ) ; } catch ( NoSuchMethodException e ) { LOG . warn ( "ExternalDataset '{}' does not have method '{}'. " + "Can't register {} lineage for this dataset" , referenceName , methodName , accessType ) ; } catch ( Exception e ) { LOG . error ( "Unable to invoke {} lineage for {}" , referenceName , methodName , e ) ; } }
public void test() { try { NodeId nodeId = activity . getId ( ) ; code_block = IfStatement ; NodeStateEx act = getNodeStateEx ( nodeId ) ; NodeId parentId = act . getParentId ( ) ; Name name = act . getName ( ) ; code_block = WhileStatement ; operation . save ( ) ; } catch ( ItemStateException e ) { log . error ( e . getMessage ( ) ) ; } finally { operation . close ( ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { try { setterName = getSetterName ( propertyName ) ; setterMethod = step . getClass ( ) . getMethod ( setterName , expectedType ) ; } catch ( Exception e1 ) { logger . info ( e1 . getMessage ( ) ) ; } }
public void test() { try { sendStatusMessage ( new StatusTaskEnd ( uuid , kick . getJobUuid ( ) , kick . getNodeID ( ) , 0 , 0 , 0 ) ) ; } catch ( Exception ex ) { logger . warn ( "Error sending status message: {}" , ex . getMessage ( ) ) ; } }
public void test() { try { Class . forName ( "org.apache.derby.jdbc.EmbeddedDriver" ) ; DriverManager . getConnection ( "jdbc:derby:memory:argus;create=true" ) . close ( ) ; } catch ( Exception ex ) { logger . error ( "Exception during database startup." , ex ) ; fail ( "Exception during database startup." ) ; } }
public void test() { try { String tokenStr = token . encodeToUrlString ( ) ; HiveConf hcatConf = createHiveConf ( metaStoreUri , hiveMetaStorePrincipal ) ; hcatClient = HCatClient . create ( hcatConf ) ; Long expiryTime = hcatClient . renewDelegationToken ( tokenStr ) ; LOG . info ( "Renewed delegation token. new expiryTime={}" , expiryTime ) ; return expiryTime ; } catch ( Exception ex ) { LOG . error ( "Failed to renew delegation tokens." , ex ) ; throw new RuntimeException ( "Failed to renew delegation tokens." , ex ) ; } finally { code_block = IfStatement ; } }
public void test() { try { String tokenStr = token . encodeToUrlString ( ) ; HiveConf hcatConf = createHiveConf ( metaStoreUri , hiveMetaStorePrincipal ) ; LOG . debug ( "renewing delegation tokens for principal={}" , hiveMetaStorePrincipal ) ; hcatClient = HCatClient . create ( hcatConf ) ; Long expiryTime = hcatClient . renewDelegationToken ( tokenStr ) ; LOG . debug ( " renewed token {}" , expiryTime ) ; return expiryTime ; } catch ( Exception ex ) { throw new RuntimeException ( "Failed to renew delegation tokens." , ex ) ; } finally { code_block = IfStatement ; } }
public void test() { try { hcatClient . close ( ) ; } catch ( HCatException e ) { LOG . warn ( "error closing hcat client" , e ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( JMSException e ) { LOG . error ( "Got a JMSException {}" , e . toString ( ) , e ) ; failed ( "Got a JMSException " + e . toString ( ) ) ; latch . countDown ( ) ; } }
public void test() { try { result = persistenceEntryManager . contains ( GluuCustomPerson . class ) ; } catch ( Exception e ) { log . debug ( e . getMessage ( ) , e ) ; } }
@ Ignore @ Test public void findAllUsers ( ) { LOGGER . debug ( "findAllUsers" ) ; UserSearchForm searchForm = new UserSearchForm ( ) ; searchForm . setSelectedLanguage ( UserLanguage . ENGLISH ) ; Pageable pageable = new PageRequest ( 0 , 10 , Sort . Direction . ASC , "firstName" ) ; Page < User > usersPage = userService . findAll ( pageable , searchForm ) ; assertEquals ( 5 , usersPage . getTotalElements ( ) ) ; }
@ Test public void test_binSeq_03 ( ) { Log . debug ( "Test" ) ; checkOverlap ( "acgtacgtacgtacgtacgtacgtacgtacgtacgtacgtacgtacgtacgtacgtacgtacgtacgtacgt" , "acgt" , 0 , 0 ) ; }
@ Override public void onError ( Throwable t , Invoker < ? > invoker , Invocation invocation ) { LOGGER . log ( Level . SEVERE , "Error executing method" , t ) ; }
public void test() { if ( cycleYear == null || cycleYear . isEmpty ( ) ) { logger . error ( new Exception ( ) ) ; } else { p = cb . and ( p , cb . equal ( root . get ( AttendanceDetail_ . cycleYear ) , cycleYear ) ) ; } }
public void test() { if ( cycleMonth == null || cycleMonth . isEmpty ( ) ) { logger . error ( new PeriodicalMonthEmptyException ( ) ) ; } else { p = cb . and ( p , cb . equal ( root . get ( AttendanceDetail_ . cycleMonth ) , cycleMonth ) ) ; } }
public void test() { try { serviceRegistration . unregister ( ) ; } catch ( final IllegalStateException ex ) { logger . warn ( "Problem unregistering service registration" , ex ) ; } }
@ Override public List < Project > getProjects ( String key , String value , UserInfo userInfo ) throws InvalidProtocolBufferException , ExecutionException , InterruptedException { logger . debug ( "getProjects started..." ) ; FindProjects findProjects = FindProjects . newBuilder ( ) . addPredicates ( KeyValueQuery . newBuilder ( ) . setKey ( key ) . setValue ( Value . newBuilder ( ) . setStringValue ( value ) . build ( ) ) . setOperator ( OperatorEnum . Operator . EQ ) . setValueType ( ValueTypeEnum . ValueType . STRING ) . build ( ) ) . build ( ) ; ProjectPaginationDTO projectPaginationDTO = findProjects ( findProjects , null , userInfo , ResourceVisibility . PRIVATE ) ; return projectPaginationDTO . getProjects ( ) ; }
@ Override public QanaryMessage process ( QanaryMessage myQanaryMessage ) throws Exception { long startTime = System . currentTimeMillis ( ) ; QanaryUtils myQanaryUtils = this . getUtils ( myQanaryMessage ) ; QanaryQuestion < String > myQanaryQuestion = this . getQanaryQuestion ( myQanaryMessage ) ; String myQuestion = myQanaryQuestion . getTextualRepresentation ( ) ; logger . info ( "Question: {}" , myQuestion ) ; logger . info ( "Question: {}" , myQuestion ) ; Detector detector = DetectorFactory . create ( ) ; detector . append ( myQuestion ) ; String lang = detector . detect ( ) ; logger . info ( "Language: {}" , lang ) ; logger . info ( "store data in graph {}" , myQanaryMessage . getEndpoint ( ) ) ; myQanaryQuestion . setLanguageText ( lang ) ; return myQanaryMessage ; }
@ Override public QanaryMessage process ( QanaryMessage myQanaryMessage ) throws Exception { long startTime = System . currentTimeMillis ( ) ; logger . info ( "Qanary Message: {}" , myQanaryMessage ) ; QanaryUtils myQanaryUtils = this . getUtils ( myQanaryMessage ) ; QanaryQuestion < String > myQanaryQuestion = this . getQanaryQuestion ( myQanaryMessage ) ; String myQuestion = myQanaryQuestion . getTextualRepresentation ( ) ; Detector detector = DetectorFactory . create ( ) ; detector . append ( myQuestion ) ; String lang = detector . detect ( ) ; logger . info ( "Message: {}" , lang ) ; logger . info ( "Language: {}" , lang ) ; logger . info ( "store data in graph {}" , myQanaryMessage . getEndpoint ( ) ) ; myQanaryQuestion . setLanguageText ( lang ) ; return myQanaryMessage ; }
@ Override public QanaryMessage process ( QanaryMessage myQanaryMessage ) throws Exception { long startTime = System . currentTimeMillis ( ) ; logger . info ( "Qanary Message: {}" , myQanaryMessage ) ; QanaryUtils myQanaryUtils = this . getUtils ( myQanaryMessage ) ; QanaryQuestion < String > myQanaryQuestion = this . getQanaryQuestion ( myQanaryMessage ) ; String myQuestion = myQanaryQuestion . getTextualRepresentation ( ) ; logger . info ( "Question: {}" , myQuestion ) ; Detector detector = DetectorFactory . create ( ) ; detector . append ( myQuestion ) ; String lang = detector . detect ( ) ; logger . info ( "lang: {}" , lang ) ; logger . info ( "store data in graph {}" , myQanaryMessage . getEndpoint ( ) ) ; myQanaryQuestion . setLanguageText ( lang ) ; return myQanaryMessage ; }
@ Override public QanaryMessage process ( QanaryMessage myQanaryMessage ) throws Exception { long startTime = System . currentTimeMillis ( ) ; logger . info ( "Qanary Message: {}" , myQanaryMessage ) ; QanaryUtils myQanaryUtils = this . getUtils ( myQanaryMessage ) ; QanaryQuestion < String > myQanaryQuestion = this . getQanaryQuestion ( myQanaryMessage ) ; String myQuestion = myQanaryQuestion . getTextualRepresentation ( ) ; logger . info ( "Question: {}" , myQuestion ) ; Detector detector = DetectorFactory . create ( ) ; detector . append ( myQuestion ) ; logger . info ( "Question: {}" , myQuestion ) ; String lang = detector . detect ( ) ; logger . info ( "Language: {}" , lang ) ; myQanaryQuestion . setLanguageText ( lang ) ; return myQanaryMessage ; }
public void test() { try { URI uri = new URI ( url ) ; desktop . browse ( uri ) ; } catch ( IOException | URISyntaxException ex ) { LOG . error ( ex . getLocalizedMessage ( ) ) ; } }
public void test() { try { handles = getAllPreparedQueries ( sessionid , user , queryName , fromDate , toDate ) ; code_block = ForStatement ; } catch ( Exception e ) { log . error ( "Failed to get allPrepared queries" , e ) ; failed = true ; } }
public void test() { try ( Tx tx = StructrApp . getInstance ( securityContext ) . tx ( ) ) { final Date date = file . getLastModifiedDate ( ) ; code_block = IfStatement ; tx . success ( ) ; } catch ( FrameworkException fex ) { logger . error ( "" , fex ) ; } }
public void test() { try { final SuggestionEntry entry = new SuggestionEntry ( this . server , factory , application , search , result , start , end , result . getQueryTime ( ) , result . getElapsedTime ( ) , session ) ; final HashMap < String , Object > mergedMetadata = new HashMap < > ( ) ; mergedMetadata . putAll ( this . monitoringMetadata ) ; mergedMetadata . putAll ( metadata ) ; entry . setMetadata ( mergedMetadata ) ; log . debug ( "Monitoring is adding a Suggestion entry" ) ; logger . log ( entry ) ; } catch ( Exception e ) { log . error ( "Suggestion monitoring error: {}" , e . getMessage ( ) , e ) ; code_block = IfStatement ; } }
public void test() { if ( logger . isTraceEnabled ( ) && valid && ( b < - 128 || b > 127 ) ) { logger . trace ( "Channel " + channel + ": invalid" ) ; } }
public void test() { if ( ! checkCompatibility ( pluginInfo ) ) { final String msg = "Plugin '" + pluginInfo . getName ( ) + "' not compatible with this version of JSPWiki" ; m_logger . warn ( msg ) ; } else { plugin = pluginInfo . newPluginInstance ( m_searchPath , m_externalJars ) ; } }
void process ( Text key , Reducer < Text , BytesWritable , Text , Text > . Context context , Iterable < BytesWritable > documents , int maxNumberOfDocuments ) throws IOException , InterruptedException { log . info ( "-- start process, key: {}" , key ) ; Iterator < BytesWritable > docsIterator = documents . iterator ( ) ; int partNb = 1 ; code_block = WhileStatement ; log . info ( "-- end process, key: {}" , key ) ; }
void process ( Text key , Reducer < Text , BytesWritable , Text , Text > . Context context , Iterable < BytesWritable > documents , int maxNumberOfDocuments ) throws IOException , InterruptedException { Iterator < BytesWritable > docsIterator = documents . iterator ( ) ; log . info ( "-- start process, key: {}" , key . toString ( ) ) ; int partNb = 1 ; code_block = WhileStatement ; log . info ( "-- end process, key: {}" , key . toString ( ) ) ; }
public void test() { try { code_block = IfStatement ; authorizationDriver . publishAuthUpdate ( systemId ) ; } catch ( final Throwable ex ) { logger . warn ( ex . getMessage ( ) ) ; logger . debug ( "Exception:" , ex . getMessage ( ) ) ; } }
public void test() { if ( Thread . currentThread ( ) . isInterrupted ( ) ) { LOG . warn ( "Thread {} is interrupted." , Thread . currentThread ( ) . getName ( ) ) ; return ; } }
public void test() { try { logger . debug ( "PublishAuthUpdateTask.run started..." ) ; code_block = IfStatement ; authorizationDriver . publishAuthUpdate ( systemId ) ; } catch ( final Throwable ex ) { logger . error ( "PublishAuthUpdateTask failed..." , ex ) ; } }
public StgSysExportZos merge ( StgSysExportZos detachedInstance ) { log . debug ( "merging StgSysExportZos instance" ) ; code_block = TryStatement ;  }
public void test() { try { StgSysExportZos result = ( StgSysExportZos ) sessionFactory . getCurrentSession ( ) . merge ( detachedInstance ) ; log . debug ( "merge successful" ) ; return result ; } catch ( RuntimeException re ) { log . error ( "merge failed" , re ) ; throw re ; } }
public void test() { try { StgSysExportZos result = ( StgSysExportZos ) sessionFactory . getCurrentSession ( ) . merge ( detachedInstance ) ; log . debug ( "merge successful" ) ; return result ; } catch ( RuntimeException re ) { log . error ( "merge failed" , re ) ; throw re ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
@ PatchMapping ( CommonConstants . PATH_ID ) @ Secured ( ServicesData . ROLE_UPDATE_SECURITY_PROFILES ) public SecurityProfileDto patch ( final @ PathVariable ( "id" ) String id , @ RequestBody final Map < String , Object > partialDto ) { LOGGER . debug ( "Patch {} with {}" , id , partialDto ) ; ParameterChecker . checkParameter ( "The Identifier is a mandatory parameter: " , id ) ; Assert . isTrue ( StringUtils . equals ( id , ( String ) partialDto . get ( "id" ) ) , "The DTO identifier must match the path identifier for update." ) ; return securityProfileExternalService . patch ( partialDto ) ; }
@ Override public void onDeviceAdded ( Bridge bridge , Device device ) { logger . debug ( "onDeviceAdded(): {}" , device ) ; ThingUID thingUID = null ; code_block = SwitchStatement ; code_block = IfStatement ; }
public void test() { if ( thingUID != null ) { String name = device . getName ( ) ; code_block = IfStatement ; DiscoveryResult discoveryResult = DiscoveryResultBuilder . create ( thingUID ) . withProperty ( Thing . PROPERTY_SERIAL_NUMBER , device . getSerialNumber ( ) ) . withBridge ( bridge . getUID ( ) ) . withLabel ( device . getType ( ) + ": " + name ) . withRepresentationProperty ( Thing . PROPERTY_SERIAL_NUMBER ) . build ( ) ; thingDiscovered ( discoveryResult ) ; } else { logger . debug ( "discovered unsupported device {}, type: {}" , device . getType ( ) , device . getType ( ) ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Refreshing." ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Refreshing." ) ; } }
public void test() { try { Analyzer analyzer = createAnalyzer ( ) ; QueryParser queryParser1 = new QueryParser ( "text" , analyzer ) ; Query termQuery1 = queryParser1 . parse ( keyword ) ; BooleanClause booleanClause1 = new BooleanClause ( termQuery1 , BooleanClause . Occur . SHOULD ) ; QueryParser queryParser2 = new QueryParser ( "title" , analyzer ) ; Query termQuery2 = queryParser2 . parse ( keyword ) ; BooleanClause booleanClause2 = new BooleanClause ( termQuery2 , BooleanClause . Occur . SHOULD ) ; BooleanQuery . Builder builder = new BooleanQuery . Builder ( ) ; builder . add ( booleanClause1 ) . add ( booleanClause2 ) ; return builder . build ( ) ; } catch ( ParseException e ) { LOGGER . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { if ( ! dependencySet . isUseTransitiveDependencies ( ) && dependencySet . isUseTransitiveFiltering ( ) ) { logger . debug ( "Transitive removal enabled for {}" , dependencySet ) ; } }
public void test() { try { ProjectBuildingResult build = projectBuilder1 . build ( depArtifact , pbr ) ; depProject = build . getProject ( ) ; } catch ( final ProjectBuildingException e ) { LOGGER . error ( "Unable to build project" , e ) ; depProject = buildProjectStub ( depArtifact ) ; } }
public void test() { try { return doSearch ( dataRequest , cube , tupleConverter , recordsSerializer , queryReceiver , tupleInfo ) ; } catch ( IOException e ) { exception = e ; logger . error ( "Error in search" , e ) ; failedReceivers . put ( queryReceiver , System . currentTimeMillis ( ) ) ; } }
public void test() { try { return doSearch ( dataRequest , cube , tupleConverter , recordsSerializer , receiver , tupleInfo ) ; } catch ( IOException e ) { logger . error ( "Exception occurred during search" , e ) ; exception = e ; failedReceivers . put ( receiver , System . currentTimeMillis ( ) ) ; } }
@ Override public void connectionLost ( SocketAddress sa ) { String msg = "lost memcached connection [" + sa + "] reconnecting..." ; log . info ( msg ) ; }
public void test() { if ( to . getScheduleSnapshots ( ) != from . getScheduleSnapshots ( ) || to . getReplicationSupported ( ) != from . getFileReplicationSupported ( ) || to . getAllowFilePolicyAtProjectLevel ( ) != from . getAllowFilePolicyAtProjectLevel ( ) || to . getAllowFilePolicyAtFSLevel ( ) != from . getAllowFilePolicyAtFSLevel ( ) ) { LOG . debug ( "Partition {} already exists" , to . getFullPath ( ) ) ; return true ; } }
public void test() { if ( to . getMinRpoType ( ) != from . getFrRpoType ( ) || to . getMinRpoValue ( ) != from . getFrRpoValue ( ) ) { log . info ( "Skipping range between minRpo: " + to . getFrRpoType ( ) + ", to: " + from . getFrRpoValue ( ) ) ; return true ; } }
public void test() { try { Content publishingContent = this . getContentManager ( ) . loadContent ( this . getContentId ( ) , true ) ; code_block = IfStatement ; } catch ( Throwable t ) { _logger . error ( "error in validazione contenuto con id {}" , this . getContentId ( ) , t ) ; throw new RuntimeException ( "Errore in validazione contenuto con id " + this . getContentId ( ) , t ) ; } }
public void test() { try { this . createValuedShowlet ( ) ; } catch ( Throwable t ) { _logger . error ( "error in createValued showlet" , t ) ; throw new RuntimeException ( "Errore in creazione widget valorizzato" , t ) ; } }
private synchronized void discover ( ) { logger . debug ( "Discovery job is starting" ) ; MulticastListener epsonMulticastListener ; String local = "127.0.0.1" ; code_block = TryStatement ;  code_block = WhileStatement ; epsonMulticastListener . shutdown ( ) ; logger . debug ( "Discovery job is exiting" ) ; }
public void test() { try { String ip = networkAddressService . getPrimaryIpv4HostAddress ( ) ; epsonMulticastListener = new MulticastListener ( ( ip != null ? ip : local ) ) ; } catch ( SocketException se ) { logger . debug ( "Discovery job got Socket exception creating multicast socket: {}" , se . getMessage ( ) ) ; return ; } catch ( IOException ioe ) { logger . debug ( "Discovery job got IO exception creating multicast socket: {}" , ioe . getMessage ( ) ) ; return ; } }
public void test() { try { String ip = networkAddressService . getPrimaryIpv4HostAddress ( ) ; epsonMulticastListener = new MulticastListener ( ( ip != null ? ip : local ) ) ; } catch ( SocketException se ) { logger . debug ( "Discovery job got Socket exception creating multicast socket: {}" , se . getMessage ( ) ) ; return ; } catch ( IOException ioe ) { logger . debug ( "Discovery job got IO exception creating multicast socket: {}" , ioe . getMessage ( ) ) ; return ; } }
public void test() { try { beaconReceived = epsonMulticastListener . waitForBeacon ( ) ; } catch ( IOException ioe ) { beaconReceived = false ; logger . error ( "Couldn't send beacon to beacon" , ioe ) ; } }
private synchronized void discover ( ) { logger . debug ( "Discovery job is running" ) ; MulticastListener epsonMulticastListener ; String local = "127.0.0.1" ; code_block = TryStatement ;  code_block = WhileStatement ; epsonMulticastListener . shutdown ( ) ; logger . debug ( "Discovery stopped" ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { publishAppCertificate ( ) ; } catch ( KuraException e ) { logger . warn ( "Can't publish app certificate" ) ; } }
public void test() { if ( taggerAllCrisesResponse . getCrisises ( ) != null ) { logger . info ( "getCrisesResponse.getCrisises: " + taggerAllCrisesResponse . getCrisises ( ) ) ; } }
public void test() { try { WebTarget webResource = client . target ( taggerMainUrl + "/crisis?userID=" + userId ) ; ObjectMapper objectMapper = JacksonWrapper . getObjectMapper ( ) ; objectMapper . configure ( DeserializationConfig . Feature . FAIL_ON_UNKNOWN_PROPERTIES , false ) ; Response clientResponse = webResource . request ( MediaType . APPLICATION_JSON ) . get ( ) ; String jsonResponse = clientResponse . readEntity ( String . class ) ; TaggerAllCrisesResponse taggerAllCrisesResponse = objectMapper . readValue ( jsonResponse , TaggerAllCrisesResponse . class ) ; code_block = IfStatement ; return taggerAllCrisesResponse . getCrisises ( ) ; } catch ( Exception e ) { logger . error ( "Error while getting tagged collection" , e ) ; throw new AidrException ( "No collection is enabled for Tagger. Please enable tagger for one of your collections." , e ) ; } }
public void test() { try { docs = serializer . toDocuments ( index , value ) ; } catch ( Exception e ) { exceptionHappened = true ; stats . incFailedEntries ( ) ; logger . info ( "Failed to update index to " + value + " due to " + e . getMessage ( ) ) ; } }
public void test() { if ( ! rack . isPresent ( ) ) { LOG . log ( Level . WARNING , "Cannot find rack {0}" , rack ) ; } else { code_block = TryStatement ;  } }
public void test() { try { handleExpiringObject ( expiringObject , rack . get ( ) , getMessage ( expiringObject ) ) ; } catch ( Exception e ) { log . error ( "Error occurred during expiration of " + expiringObject , e ) ; } }
@ AfterClass public static void reportTest ( ) { LOGGER . warn ( "-----------------------------------------" ) ; LOGGER . warn ( "*                                      *" ) ; LOGGER . warn ( "*      FINISHED CustomCRSLandsatIT               *" ) ; LOGGER . warn ( "*       FINISHED CustomCRSLandsatIT          *" ) ; LOGGER . warn ( "*                          *" ) ; LOGGER . warn ( "-----------------------------------------" ) ; }
public void test() { try { service . executeTransaction ( interfaceName , txId , arguments , callerServiceId , context ) ; } catch ( Exception e ) { logger . warn ( "Could not execute transaction" , e ) ; throw e ; } }
public void test() { if ( ! differences . isEmpty ( ) ) { logger . info ( "{}" , differences ) ; } }
public void test() { try { sourceContext . setCurrent ( masterSource ) ; differences = loadObjects ( proxy , loadedObjects ) ; code_block = IfStatement ; } catch ( IllegalSourceException e ) { logger . warn ( e . getMessage ( ) , e ) ; } finally { sourceContext . setCurrent ( originalSource ) ; } }
public void test() { try { code_block = IfStatement ; upgradeLogPath . createNewFile ( ) ; upgradeLogWriter = new BufferedWriter ( new FileWriter ( getUpgradeLogPath ( ) , true ) ) ; return true ; } catch ( IOException e ) { LOG . warn ( "Unable to open upgrade file " + getUpgradeLogPath ( ) , e ) ; return false ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getCapabilities()" ) ; } }
public org . talend . mdm . webservice . WSString getTransformerPluginV2Configuration ( org . talend . mdm . webservice . WSTransformerPluginV2GetConfiguration arg0 ) { LOG . info ( "Executing operation getTransformerPluginV2Configuration" ) ; System . out . println ( arg0 ) ; code_block = TryStatement ;  }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { newToken . setOpenidToken ( createIdToken ( now , newToken , Arrays . asList ( new Audience ( clientUserName ) ) , userInfoClaimSet ) ) ; } catch ( Exception e ) { logger . error ( "Unable to set user id token" , e ) ; throw new OAuthErrorException ( makeError ( OAuth2Error . SERVER_ERROR , e . getMessage ( ) ) ) ; } }
public void test() { try { targetType = arg1 . getRequiredType ( ) ; Converter conv = defaultConv . lookupConverterForType ( targetType ) ; result = conv . unmarshal ( arg0 , arg1 ) ; } catch ( final Exception ex ) { LOGGER . log ( Level . WARNING , "Can't unmarshal" , ex ) ; return null ; } }
public void test() { try { code_block = IfStatement ; } catch ( final HibernateException | NullPointerException ex ) { Log . error ( ex ) ; } }
public void test() { -> { LOGGER . info ( "Loaded {} results" , result . size ( ) ) ; } }
public void test() { for ( Logger logger : this . loggers ) { logger . info ( message , e ) ; } }
@ Override public Optional < MigrationIssue > getCourseMigrationIssue ( String courseId , Integer migrationId , Integer issueId ) throws IOException { LOG . debug ( "listing a migration issue for course " + courseId + " and migrationId " + migrationId ) ; String url = buildCanvasUrl ( "courses/" + courseId + "/content_migrations/" + migrationId . toString ( ) + "/migration_issues/" + issueId . toString ( ) , Collections . emptyMap ( ) ) ; Response response = canvasMessenger . getSingleResponseFromCanvas ( oauthToken , url ) ; return responseParser . parseToObject ( MigrationIssue . class , response ) ; }
public void test() { if ( givenWorkflowIndex . isEmpty ( ) ) { logger . error ( "Given workflow index is empty!" ) ; } }
public void test() { if ( machine . isEmpty ( ) || workflowIndex < 0 || machine . get ( ) . getWorkflows ( ) . size ( ) <= workflowIndex ) { LOG . debug ( "The workflow index is empty." ) ; return createEmptyRoot ( ) ; } }
public void test() { try { sendErrorStatus ( ) ; } catch ( IOException ioe ) { logger . warn ( "Failed to send error status: {}" , ioe . getMessage ( ) ) ; } }
public void test() { if ( notification == null ) { return ; } }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; } catch ( Throwable e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { switch ( result ) { case SUCCESS : LOG . debug ( "Deleting {} submitted open keys." , numSubmittedOpenKeys ) ; break ; case FAILURE : omMetrics . incNumOpenKeyDeleteRequestFails ( ) ; LOG . error ( "Failure occurred while trying to delete {} submitted open " + "keys." , numSubmittedOpenKeys ) ; break ; default : LOG . error ( "Unrecognized result for OMOpenKeysDeleteRequest: {}" , request ) ; } }
public void test() { switch ( result ) { case SUCCESS : LOG . debug ( "Deleted {} open keys out of {} submitted keys." , numDeletedOpenKeys , numSubmittedOpenKeys ) ; break ; case FAILURE : omMetrics . incNumOpenKeyDeleteRequestFails ( ) ; LOG . error ( "Remove {} open keys out of {} submitted keys failed." , numDeletedOpenKeys , numSubmittedOpenKeys ) ; break ; default : LOG . error ( "Unrecognized result for OMOpenKeysDeleteRequest: {}" , request ) ; } }
public void test() { switch ( result ) { case SUCCESS : LOG . debug ( "Deleted {} open keys out of {} submitted keys." , numDeletedOpenKeys , numSubmittedOpenKeys ) ; break ; case FAILURE : omMetrics . incNumOpenKeyDeleteRequestFails ( ) ; LOG . error ( "Failure occurred while trying to delete {} submitted open " + "keys." , numSubmittedOpenKeys ) ; break ; default : LOG . warn ( "Unable to delete operation {}" , result ) ; } }
public void test() { if ( ! ObjectUtils . isNull ( category ) ) { final KualiDecimal oneCent = new KualiDecimal ( 0.01 ) ; int size = contractsGrantsInvoiceDocument . getAccountDetails ( ) . size ( ) ; KualiDecimal amount = new KualiDecimal ( invoiceDetail . getInvoiceAmount ( ) . bigDecimalValue ( ) . divide ( new BigDecimal ( size ) , 2 , RoundingMode . HALF_UP ) ) ; KualiDecimal remainder = invoiceDetail . getInvoiceAmount ( ) . subtract ( amount . multiply ( new KualiDecimal ( size ) ) ) ; code_block = ForStatement ; } else { log . warn ( "Unable to find contract details for: " + contractsGrantsInvoiceDocument . getAccountDetails ( ) ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Refreshing." ) ; } }
private void interruptBackup ( ) { LOG . info ( "Interrupting backup" ) ; code_block = IfStatement ; boolean shouldResume = true ; code_block = IfStatement ; code_block = IfStatement ; LOG . warn ( "Backup interrupted successfully." ) ; }
public void test() { if ( mBackupFuture != null && ! mBackupFuture . isDone ( ) ) { LOG . debug ( "BackupFuture is cancelled: {}" , mBackupFuture . isDone ( ) ) ; mBackupFuture . cancel ( true ) ; } }
public void test() { try { LOG . info ( "Attempt to resume journal application." ) ; mJournalSystem . resume ( ) ; } catch ( Exception e ) { LOG . error ( "Failed to resume journal application." , e ) ; } }
public void test() { try { LOGGER . info ( "Processing: {}" , processed ) ; } catch ( ProcessingException ignore ) { } }
public void test() { try { String clusterName = ( String ) getRequest ( ) . getAttributes ( ) . get ( "clusterName" ) ; JsonParameters jsonParameters = new JsonParameters ( entity ) ; String command = jsonParameters . getCommand ( ) ; ZkClient zkClient = ( ZkClient ) getContext ( ) . getAttributes ( ) . get ( RestAdminApplication . ZKCLIENT ) ; ClusterSetup setupTool = new ClusterSetup ( zkClient ) ; code_block = IfStatement ; getResponse ( ) . setEntity ( getInstancesRepresentation ( clusterName ) ) ; getResponse ( ) . setStatus ( Status . SUCCESS_OK ) ; } catch ( Exception e ) { getResponse ( ) . setEntity ( ClusterRepresentationUtil . getErrorAsJsonStringFromException ( e ) , MediaType . APPLICATION_JSON ) ; getResponse ( ) . setStatus ( Status . SUCCESS_OK ) ; LOG . error ( "Error in getInstances" , e ) ; } }
public void test() { try { final GUIDImpl parsed1 = new GUIDImpl ( properties . getProperty ( FIELDS . BASE32 . name ( ) ) ) ; final GUIDImpl parsed2 = new GUIDImpl ( properties . getProperty ( FIELDS . BASE64 . name ( ) ) ) ; final GUIDImpl parsed0 = new GUIDImpl ( properties . getProperty ( FIELDS . BASE16 . name ( ) ) ) ; final GUIDImpl parsed8 = new GUIDImpl ( properties . getProperty ( FIELDS . BASEARK . name ( ) ) ) ; final byte [ ] bytes = StringUtils . getBytesFromArraysToString ( properties . getProperty ( FIELDS . BYTES . name ( ) ) ) ; final GUIDImpl parsed9 = new GUIDImpl ( bytes ) ; assertTrue ( parsed1 . equals ( parsed2 ) ) ; assertTrue ( parsed1 . equals ( parsed0 ) ) ; assertTrue ( parsed1 . equals ( parsed8 ) ) ; assertTrue ( parsed1 . equals ( parsed9 ) ) ; final GUIDImpl parsed3 = new GUIDImpl ( parsed9 . getBytes ( ) ) ; final GUIDImpl parsed4 = new GUIDImpl ( parsed9 . toBase32 ( ) ) ; final GUIDImpl parsed5 = new GUIDImpl ( parsed9 . toHex ( ) ) ; final GUIDImpl parsed6 = new GUIDImpl ( parsed9 . toString ( ) ) ; final GUIDImpl parsed7 = new GUIDImpl ( parsed9 . toBase64 ( ) ) ; assertTrue ( parsed9 . equals ( parsed3 ) ) ; assertTrue ( parsed9 . equals ( parsed4 ) ) ; assertTrue ( parsed9 . equals ( parsed5 ) ) ; assertTrue ( parsed9 . equals ( parsed6 ) ) ; assertTrue ( parsed9 . equals ( parsed7 ) ) ; final GUIDImpl generated = new GUIDImpl ( ) ; assertTrue ( generated . getVersion ( ) == 0 ) ; } catch ( final InvalidGuidOperationException e ) { LOGGER . error ( e . getMessage ( ) , e ) ; fail ( e . getMessage ( ) ) ; } }
public void test() { try { mp = MP_FACTORY . newMediaPackageBuilder ( ) . loadFromXml ( mediaPackageXml ) ; code_block = IfStatement ; } catch ( MediaPackageException e ) { logger . info ( "Unable to create media package '{}'" , mediaPackageXml ) ; return Response . status ( Status . BAD_REQUEST ) . build ( ) ; } }
public void test() { if ( mediaPackageElements . length != 1 ) { logger . warn ( "Media package '{}' is not a valid media package" , mediaPackageElementId ) ; return Response . status ( Status . BAD_REQUEST ) . build ( ) ; } }
public void test() { if ( conditionHandlers . isEmpty ( ) ) { String message = "Condition encountered processing deployment id '" + deplomentId + "' statement '" + statementName + "'" ; code_block = IfStatement ; message += " :" + condition . toString ( ) ; LOG . warn ( message ) ; return ; } }
public void test() { if ( isoFile == null || ! isoFile . exists ( ) ) { String svm = "client/target/generated-webapp/WEB-INF/classes/vms/" + iso ; isoFile = new File ( svm ) ; logger . info ( "vm file: " + svm ) ; } }
public void test() { if ( ! isoFile . exists ( ) ) { LOG . warn ( "The iso file {} does not exist" , isoFile ) ; } }
public SecurityEvent createSecurityEvent ( String loggingClass , URI requestUri , String slMessage , Entity explicitRealmEntity , String entityType , Set < Entity > entities ) { Set < String > targetEdOrgs = getTargetEdOrgStateIds ( entityType , entities ) ; LOG . debug ( "Creating security event for {}" , requestUri ) ; return createSecurityEvent ( loggingClass , requestUri , slMessage , null , null , explicitRealmEntity , targetEdOrgs , false ) ; }
public void test() { if ( bridge != null ) { ObjectStatus objStatus = bridge . requestObjectStatus ( Message . OBJ_TYPE_AREA , thingID , thingID , true ) ; return Optional . of ( ( ExtendedAreaStatus ) objStatus . getStatuses ( ) [ 0 ] ) ; } else { logger . debug ( "Bridge is null" ) ; return Optional . empty ( ) ; } }
public void test() { try { final OmnilinkBridgeHandler bridge = getOmnilinkBridgeHandler ( ) ; code_block = IfStatement ; } catch ( OmniInvalidResponseException | OmniUnknownMessageTypeException | BridgeOfflineException e ) { logger . debug ( "Status: {}" , e . getMessage ( ) ) ; return Optional . empty ( ) ; } }
public void test() { try { primaryClient = establishConnectionToPrimary ( replicaSet ) ; code_block = IfStatement ; } catch ( Throwable t ) { errorHandler . setProducerThrowable ( t ) ; logger . error ( "Producer failure" , t ) ; } finally { code_block = IfStatement ; } }
public void test() { if ( LOG . isInfoEnabled ( ) ) { LOG . info ( String . format ( "Call to '%s' on file '%s'" , uri . toString ( ) , file ) ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
@ Override public SegmentMetadata load ( File segmentDir ) throws Exception { final SegmentMetadata segmentMetadata = new SegmentMetadataImpl ( segmentDir ) ; logger . info ( "Load segment metadata segment:{}" , segmentDir ) ; return segmentMetadata ; }
public void test() { try { LOG . debug ( "HTTP server started." ) ; ServerLauncher . setSetUpSuite ( true ) ; SUPPORT = new DropwizardTestSupport < OxdServerConfiguration > ( OxdServerApplication . class , ResourceHelpers . resourceFilePath ( "oxd-server-jenkins.yml" ) , ConfigOverride . config ( "server.applicationConnectors[0].port" , "0" ) ) ; SUPPORT . before ( ) ; LOG . debug ( "HTTP server started." ) ; removeExistingRps ( ) ; LOG . debug ( "Existing RPs are removed." ) ; RegisterSiteResponse setupClient = SetupClientTest . setupClient ( Tester . newClient ( host ) , opHost , redirectUrls ) ; Tester . setSetupClient ( setupClient , host , opHost ) ; LOG . debug ( "SETUP_CLIENT is set in Tester." ) ; Preconditions . checkNotNull ( Tester . getAuthorization ( ) ) ; LOG . debug ( "Tester's authorization is set." ) ; setupSwaggerSuite ( Tester . getTargetHost ( host ) , opHost , redirectUrls ) ; LOG . debug ( "Finished beforeSuite!" ) ; } catch ( Exception e ) { LOG . error ( "Failed to start suite." , e ) ; throw new AssertionError ( "Failed to start suite." ) ; } }
public void test() { try { LOG . debug ( "Running beforeSuite ..." ) ; ServerLauncher . setSetUpSuite ( true ) ; SUPPORT = new DropwizardTestSupport < OxdServerConfiguration > ( OxdServerApplication . class , ResourceHelpers . resourceFilePath ( "oxd-server-jenkins.yml" ) , ConfigOverride . config ( "server.applicationConnectors[0].port" , "0" ) ) ; SUPPORT . before ( ) ; removeExistingRps ( ) ; LOG . debug ( "Existing RPs are removed." ) ; RegisterSiteResponse setupClient = SetupClientTest . setupClient ( Tester . newClient ( host ) , opHost , redirectUrls ) ; LOG . debug ( "SETUP_CLIENT is set in Tester." ) ; Tester . setSetupClient ( setupClient , host , opHost ) ; LOG . debug ( "SETUP_CLIENT is set in Tester." ) ; Preconditions . checkNotNull ( Tester . getAuthorization ( ) ) ; LOG . debug ( "Tester's authorization is set." ) ; setupSwaggerSuite ( Tester . getTargetHost ( host ) , opHost , redirectUrls ) ; LOG . debug ( "Finished beforeSuite!" ) ; } catch ( Exception e ) { LOG . error ( "Failed to start suite." , e ) ; throw new AssertionError ( "Failed to start suite." ) ; } }
public void test() { try { LOG . debug ( "Running beforeSuite ..." ) ; ServerLauncher . setSetUpSuite ( true ) ; LOG . debug ( "HTTP server started." ) ; SUPPORT = new DropwizardTestSupport < OxdServerConfiguration > ( OxdServerApplication . class , ResourceHelpers . resourceFilePath ( "oxd-server-jenkins.yml" ) , ConfigOverride . config ( "server.applicationConnectors[0].port" , "0" ) ) ; SUPPORT . before ( ) ; LOG . debug ( "HTTP server started." ) ; removeExistingRps ( ) ; RegisterSiteResponse setupClient = SetupClientTest . setupClient ( Tester . newClient ( host ) , opHost , redirectUrls ) ; Tester . setSetupClient ( setupClient , host , opHost ) ; LOG . debug ( "SETUP_CLIENT is set in Tester." ) ; Preconditions . checkNotNull ( Tester . getAuthorization ( ) ) ; LOG . debug ( "Tester's authorization is set." ) ; setupSwaggerSuite ( Tester . getTargetHost ( host ) , opHost , redirectUrls ) ; LOG . debug ( "Finished beforeSuite!" ) ; } catch ( Exception e ) { LOG . error ( "Failed to start suite." , e ) ; throw new AssertionError ( "Failed to start suite." ) ; } }
public void test() { try { LOG . debug ( "Running beforeSuite ..." ) ; ServerLauncher . setSetUpSuite ( true ) ; SUPPORT = new DropwizardTestSupport < OxdServerConfiguration > ( OxdServerApplication . class , ResourceHelpers . resourceFilePath ( "oxd-server-jenkins.yml" ) , ConfigOverride . config ( "server.applicationConnectors[0].port" , "0" ) ) ; SUPPORT . before ( ) ; LOG . debug ( "HTTP server started." ) ; removeExistingRps ( ) ; LOG . debug ( "Existing RPs are removed." ) ; RegisterSiteResponse setupClient = SetupClientTest . setupClient ( Tester . newClient ( host ) , opHost , redirectUrls ) ; LOG . debug ( "Setup client for host {}." , host ) ; Tester . setSetupClient ( setupClient , host , opHost ) ; Preconditions . checkNotNull ( Tester . getAuthorization ( ) ) ; LOG . debug ( "Tester's authorization is set." ) ; setupSwaggerSuite ( Tester . getTargetHost ( host ) , opHost , redirectUrls ) ; LOG . debug ( "Finished beforeSuite!" ) ; } catch ( Exception e ) { LOG . error ( "Failed to start suite." , e ) ; throw new AssertionError ( "Failed to start suite." ) ; } }
public void test() { try { LOG . debug ( "Running beforeSuite ..." ) ; ServerLauncher . setSetUpSuite ( true ) ; LOG . debug ( "HTTP server is running." ) ; SUPPORT = new DropwizardTestSupport < OxdServerConfiguration > ( OxdServerApplication . class , ResourceHelpers . resourceFilePath ( "oxd-server-jenkins.yml" ) , ConfigOverride . config ( "server.applicationConnectors[0].port" , "0" ) ) ; SUPPORT . before ( ) ; LOG . debug ( "HTTP server started." ) ; removeExistingRps ( ) ; LOG . debug ( "Existing RPs are removed." ) ; RegisterSiteResponse setupClient = SetupClientTest . setupClient ( Tester . newClient ( host ) , opHost , redirectUrls ) ; Tester . setSetupClient ( setupClient , host , opHost ) ; LOG . debug ( "SETUP_CLIENT is set in Tester." ) ; Preconditions . checkNotNull ( Tester . getAuthorization ( ) ) ; setupSwaggerSuite ( Tester . getTargetHost ( host ) , opHost , redirectUrls ) ; LOG . debug ( "Finished beforeSuite!" ) ; } catch ( Exception e ) { LOG . error ( "Failed to start suite." , e ) ; throw new AssertionError ( "Failed to start suite." ) ; } }
public void test() { try { LOG . debug ( "Running beforeSuite ..." ) ; ServerLauncher . setSetUpSuite ( true ) ; LOG . debug ( "HTTP server is running." ) ; SUPPORT = new DropwizardTestSupport < OxdServerConfiguration > ( OxdServerApplication . class , ResourceHelpers . resourceFilePath ( "oxd-server-jenkins.yml" ) , ConfigOverride . config ( "server.applicationConnectors[0].port" , "0" ) ) ; SUPPORT . before ( ) ; LOG . debug ( "HTTP server started." ) ; removeExistingRps ( ) ; LOG . debug ( "Existing RPs are removed." ) ; RegisterSiteResponse setupClient = SetupClientTest . setupClient ( Tester . newClient ( host ) , opHost , redirectUrls ) ; Tester . setSetupClient ( setupClient , host , opHost ) ; LOG . debug ( "SETUP_CLIENT is set in Tester." ) ; Preconditions . checkNotNull ( Tester . getAuthorization ( ) ) ; LOG . debug ( "Tester's authorization is set." ) ; setupSwaggerSuite ( Tester . getTargetHost ( host ) , opHost , redirectUrls ) ; } catch ( Exception e ) { LOG . error ( "Failed to start suite." , e ) ; throw new AssertionError ( "Failed to start suite." ) ; } }
public void test() { try { LOG . debug ( "Running beforeSuite ..." ) ; ServerLauncher . setSetUpSuite ( true ) ; SUPPORT = new DropwizardTestSupport < OxdServerConfiguration > ( OxdServerApplication . class , ResourceHelpers . resourceFilePath ( "oxd-server-jenkins.yml" ) , ConfigOverride . config ( "server.applicationConnectors[0].port" , "0" ) ) ; SUPPORT . before ( ) ; LOG . debug ( "HTTP server started." ) ; removeExistingRps ( ) ; LOG . debug ( "Existing RPs are removed." ) ; RegisterSiteResponse setupClient = SetupClientTest . setupClient ( Tester . newClient ( host ) , opHost , redirectUrls ) ; Tester . setSetupClient ( setupClient , host , opHost ) ; LOG . debug ( "SETUP_CLIENT is set in Tester." ) ; Preconditions . checkNotNull ( Tester . getAuthorization ( ) ) ; LOG . debug ( "Tester's authorization is set." ) ; setupSwaggerSuite ( Tester . getTargetHost ( host ) , opHost , redirectUrls ) ; LOG . debug ( "Finished beforeSuite!" ) ; } catch ( Exception e ) { LOG . error ( "Failed to start suite." , e ) ; throw new AssertionError ( "Failed to start suite." ) ; } }
public void test() { try { plugin = formatPlugin . createIngestFromHdfsPlugin ( formatOptions ) ; code_block = IfStatement ; } catch ( final UnsupportedOperationException e ) { LOGGER . debug ( "Failed to create ingest from HDFS: {}" , formatOptions , e ) ; continue ; } }
public void test() { if ( UserGroupInformation . isLoginKeytabBased ( ) ) { log . info ( "Performing key-cache-based re-login" ) ; loginUser . reloginFromKeytab ( ) ; } else { log . info ( "Performing ticket-cache-based Kerberos re-login" ) ; loginUser . reloginFromTicketCache ( ) ; } }
public void test() { if ( UserGroupInformation . isLoginKeytabBased ( ) ) { log . info ( "Performing keytab-based onKeytab ( ) ; loginUser . reloginFromKeytab ( ) ; } else { log . info ( "Performing ticket-based re-login" ) ; loginUser . reloginFromTicketCache ( ) ; } }
public void test() { if ( loginUser . equals ( currentUser ) || loginUser . equals ( realUser ) ) { code_block = IfStatement ; code_block = TryStatement ;  } else { log . info ( "User [" + currentUser + "] does not match." ) ; } }
public org . talend . mdm . webservice . WSTransformerV2PK putTransformerV2 ( org . talend . mdm . webservice . WSPutTransformerV2 arg0 ) { LOG . info ( "Executing operation putTransformerV2" ) ; System . out . println ( arg0 ) ; code_block = TryStatement ;  }
public void test() { if ( _logger . isDebugEnabled ( ) ) { String localAddress = "not connected" ; code_block = IfStatement ; _logger . debug ( "Local address: " + localAddress ) ; } }
public void test() { if ( _logger . isTraceEnabled ( ) ) { _logger . trace ( "Exhausted " + toString ( ) ) ; } }
@ Override public void onError ( Throwable t ) { LOG . debug ( "Error in ping request." , t ) ; PingPongImpl . this . streamRequests . add ( streamRequests ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { entry . getValue ( ) . onEvent ( target , ctx . getEvent ( ) , node , ctx ) ; } catch ( final RuntimeException ex ) { log . error ( "Exception executing event." , ex ) ; } }
private void saveTranslationsInBatches ( List < HTextFlow > textFlows , MTDocument transDoc , HLocale targetLocale , ContentState saveState ) { log . debug ( "[PERF] save translations to {}" , textFlows . size ( ) ) ; int batchStart = 0 ; Stopwatch saveAllTimer = Stopwatch . createStarted ( ) ; code_block = WhileStatement ; log . debug ( "[PERF] Batches complete ({}ms)" , saveAllTimer ) ; }
public void test() { -> { List < TransUnitUpdateRequest > updateRequests = makeUpdateRequestsForBatch ( targetLocale , transDoc . getBackendId ( ) , saveState , sourceBatch , transContentBatch ) ; Stopwatch storeTranslations = Stopwatch . createStarted ( ) ; translationService . translate ( targetLocale . getLocaleId ( ) , updateRequests ) ; log . info ( "Translation: {}" , storeTranslations ) ; } }
public void test() { try { transactionUtil . run ( ( ) code_block = LoopStatement ; ) ; } catch ( Exception e ) { LOGGER . error ( "remove work item failed." , e ) ; } }
public void test() { while ( batchStart < textFlows . size ( ) ) { int batchEnd = Math . min ( batchStart + SAVE_BATCH_SIZE , textFlows . size ( ) ) ; List < HTextFlow > sourceBatch = textFlows . subList ( batchStart , batchEnd ) ; List < TypeString > transContentBatch = transDoc . getContents ( ) . subList ( batchStart , batchEnd ) ; log . debug ( "[PERF] Batch save transaction {} - {}" , batchStart , batchEnd ) ; Stopwatch transactionTime = Stopwatch . createStarted ( ) ; code_block = TryStatement ;  entityManager . clear ( ) ; batchStart = batchEnd ; log . debug ( "[PERF] Text Query Time to save: {}" , transactionTime ) ; } }
private void saveTranslationsInBatches ( List < HTextFlow > textFlows , MTDocument transDoc , HLocale targetLocale , ContentState saveState ) { int batchStart = 0 ; log . debug ( "[PERF] Saving {} translations in batches" , textFlows . size ( ) ) ; Stopwatch saveAllTimer = Stopwatch . createStarted ( ) ; code_block = WhileStatement ; log . debug ( "[PERF] Finished saving {} messages in {}ms" , textFlows . size ( ) , saveAllTimer ) ; }
public void test() { if ( this . currentAttemptCount < this . maxAttemptCount && ( retryDelay = checkIfRetryNeeded ( exception ) ) != null ) { logger . debug ( "Operation will be retried. Current attempt {}" , this . currentAttemptCount ) ; this . currentAttemptCount ++ ; return Single . just ( ShouldRetryResult . retryAfter ( retryDelay ) ) ; } else { logger . debug ( "Operation will NOT be retried. Current attempt {}" , this . currentAttemptCount , exception ) ; return Single . just ( ShouldRetryResult . noRetry ( ) ) ; } }
public void test() { if ( this . currentAttemptCount < this . maxAttemptCount && ( retryDelay = checkIfRetryNeeded ( exception ) ) != null ) { this . currentAttemptCount ++ ; logger . warn ( "Operation will be retried after {} milliseconds. Current attempt {}, Cumulative delay {}" , retryDelay . toMillis ( ) , this . currentAttemptCount , this . cumulativeRetryDelay , exception ) ; return Single . just ( ShouldRetryResult . retryAfter ( retryDelay ) ) ; } else { logger . warn ( "Operation will be retried. No retry operation" ) ; return Single . just ( ShouldRetryResult . noRetry ( ) ) ; } }
public void test() { try { InetAddress inetAddress = InetAddress . getByName ( ipAddress . getHostAddress ( ) ) ; ret = inetAddress . isReachable ( tout ) ; } catch ( Exception e ) { logger . error ( "A network interface failed: {}" , e . getMessage ( ) ) ; throw new KuraException ( KuraErrorCode . INTERNAL_ERROR , e ) ; } }
public void test() { if ( wsSession . isOpen ( ) ) { wsSession . sendMessage ( new TextMessage ( jsonMessage ) ) ; } else { log . error ( "Selected websocket session is not open: " + jsonMessage ) ; } }
public void test() { try { String messageJson = message . getPayload ( ) ; ServerSessionFactory factory = new ServerSessionFactory ( ) code_block = "" ; ; protocolManager . processMessage ( messageJson , factory , new ResponseSender ( ) code_block = "" ; , wsSession . getId ( ) ) ; } catch ( Throwable t ) { logger . error ( "Error processing HTTP request" , t ) ; } }
private void initializeFileSystem ( final IndexedDiskCacheAttributes cattr ) { this . rafDir = cattr . getDiskPath ( ) ; log . info ( "FileSystem initialized with path: {}" , this . rafDir ) ; }
public void test() { if ( hasSampling ) { String samplingPath = String . join ( "/" , samplingDir , "*.avro" ) ; LOGGER . debug ( "Skipping sampling for {}" , samplingPath ) ; return p . apply ( AvroIO . read ( SampleRecord . class ) . from ( samplingPath ) ) ; } else { return p . apply ( Create . empty ( AvroCoder . of ( SampleRecord . class ) ) ) ; } }
public void test() { try { final IpAddressTO [ ] ips = cmd . getIpAddresses ( ) ; code_block = ForStatement ; } catch ( final InternalErrorException e ) { return new ExecutionResult ( false , e . getMessage ( ) ) ; } catch ( final Exception e ) { s_logger . error ( "Internal error" , e ) ; return new ExecutionResult ( false , e . getMessage ( ) ) ; } }
private void assertEqualPositions ( SourceFile [ ] sourceFiles , SourceFile sourceFile , String javaCodeSnippet , String tsCodeSnippet ) { SourcePosition tsPosition = getPosition ( sourceFile . getTsFile ( ) , tsCodeSnippet ) ; SourcePosition javaPosition = SourceFile . findOriginPosition ( tsPosition , Arrays . asList ( sourceFiles ) ) ; logger . info ( "srcPosition: " + javaPosition + " --> " + tsPosition ) ; logger . info ( "org: " + javaPosition + " --> " + tsPosition ) ; assertEquals ( getPosition ( sourceFile . getJavaFile ( ) , javaCodeSnippet ) . getStartLine ( ) , javaPosition . getStartLine ( ) ) ; }
private void assertEqualPositions ( SourceFile [ ] sourceFiles , SourceFile sourceFile , String javaCodeSnippet , String tsCodeSnippet ) { logger . info ( "assert equal positions to '" + javaCodeSnippet + "' -> '" + tsCodeSnippet + "'" ) ; SourcePosition tsPosition = getPosition ( sourceFile . getTsFile ( ) , tsCodeSnippet ) ; SourcePosition javaPosition = SourceFile . findOriginPosition ( tsPosition , Arrays . asList ( sourceFiles ) ) ; assertEquals ( getPosition ( sourceFile . getJavaFile ( ) , javaCodeSnippet ) . getStartLine ( ) , javaPosition . getStartLine ( ) ) ; logger . info ( "assert equal position to '" + javaCodeSnippet + "' -> '" + javaPosition . getStartLine ( ) + "'" ) ; }
@ Override public void onCommunicationLost ( Association association ) { logger . info ( "onCommunicationLost" ) ; this . handleCommDown ( ) ; }
public void test() { try { eventProcessorAdminServiceStub . setTracingEnabled ( executionPlanName , isEnabled ) ; } catch ( RemoteException e ) { log . error ( "RemoteException" , e ) ; throw e ; } }
public void test() { try { sleep ( flushInterval ) ; bookie . getSyncThread ( ) . checkpoint ( Checkpoint . MAX ) ; ledgerCache . flushLedger ( true ) ; } catch ( InterruptedException ie ) { Thread . currentThread ( ) . interrupt ( ) ; return ; } catch ( Exception e ) { LOG . error ( "Exception flushing ledgers" , e ) ; } }
public void test() { if ( this . properties . isVerbose ( ) ) { this . properties . put ( this . properties . isVerbose ( ) ) ; this . logger . info ( "Gene '" + this . getName ( ) + "'\n\n\tNew gene            :\n\tTranscript: " + this . properties . toString ( ) + "\n\tGene               :\n\tTranscript           :\n\n\tTranscript                                      :\n\n\tTranscript
public void test() { if ( this . properties . isVerbose ( ) ) { this . properties . put ( this . properties . isVerbose ( ) ) ; this . logger . info ( "Gene '" + this . getName ( ) + "'\n\n\tNew gene            :\n\tTranscript: " + this . properties . toString ( ) + "\n\tGene               :\n\tTranscript           :\n\n\tTranscript                                      :\n\n\tTranscript
public void test() { if ( this . properties . isVerbose ( ) ) { this . properties . put ( this . properties . isVerbose ( ) ) ; this . logger . info ( "Gene '" + this . getName ( ) + "'\n\n\tNew gene            :\n\tTranscript: " + this . properties . toString ( ) + "\n\tGene               :\n\tTranscript           :\n\n\tTranscript                                      :\n\n\tTranscript
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { insertTestCaseCountryProperties ( tccp ) ; } catch ( CerberusException ex ) { LOG . warn ( ex . toString ( ) ) ; return false ; } }
public void test() { if ( ip . getFailCount ( ) . incrementAndGet ( ) >= switchDomain . getCheckTimes ( ) ) { code_block = IfStatement ; } else { LOGGER . info ( "IP " + ip . getPort ( ) + " not matched, skip" ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Throwable t ) { logger . error ( t . getMessage ( ) , t ) ; } }
public void test() { -> { List < String > current = TestUtils . listReadyPods ( Kubernetes . getInstance ( ) , SystemtestsKubernetesApps . SELENIUM_PROJECT ) . stream ( ) . map ( pod -> pod . getMetadata ( ) . getName ( ) ) . collect ( Collectors . toList ( ) ) ; current . removeAll ( beforeRestart ) ; LOGGER . info ( "Exiting" ) ; return current . size ( ) == beforeRestart . size ( ) ; } }
public void test() { try { TestUtils . waitUntilCondition ( "Selenium pods ready" , ( phase ) code_block = LoopStatement ; , new TimeoutBudget ( 1 , TimeUnit . MINUTES ) ) ; break ; } catch ( Exception ex ) { LOGGER . log ( Level . WARNING , "Pod pod skip" , ex ) ; } }
public void test() { if ( LOGGER . isTraceEnabled ( ) ) { LOGGER . trace ( "Weak listener list status:{}" , System . lineSeparator ( ) ) ; } }
public void test() { try { final String payload = String . format ( "grant_type=client_credentials&client_id=%s&client_secret=%s&resource=%s" , principal . getValue ( ) , URLEncoder . encode ( secret . getValue ( ) , "UTF-8" ) , URLEncoder . encode ( "https://graph.windows.net" , "UTF-8" ) ) ; final URL url = new URL ( String . format ( "https://login.microsoftonline.com/%s/oauth2/token" , tenant . getName ( ) ) ) ; final HttpsURLConnection connection = ( HttpsURLConnection ) url . openConnection ( ) ; connection . setRequestMethod ( "POST" ) ; connection . setRequestProperty ( "Host" , "login.microsoftonline.com" ) ; connection . setRequestProperty ( "Content-Type" , "application/x-www-form-urlencoded" ) ; connection . setRequestProperty ( "Accept" , "application/json" ) ; connection . setDoOutput ( true ) ; connection . getOutputStream ( ) . write ( payload . getBytes ( ) ) ; connection . getOutputStream ( ) . flush ( ) ; final StringBuilder result = new StringBuilder ( ) ; code_block = TryStatement ;  final ObjectMapper mapper = new ObjectMapper ( ) ; final JsonNode node = mapper . readValue ( result . toString ( ) . getBytes ( ) , JsonNode . class ) ; return node . get ( "access_token" ) . asText ( ) ; } catch ( IOException e ) { LOGGER . error ( e . getMessage ( ) , e ) ; return null ; } catch ( RuntimeException e ) { LOGGER . error ( e . getMessage ( ) , e ) ; return null ; } }
public void test() { try { final String payload = String . format ( "grant_type=client_credentials&client_id=%s&client_secret=%s&resource=%s" , principal . getValue ( ) , URLEncoder . encode ( secret . getValue ( ) , "UTF-8" ) , URLEncoder . encode ( "https://graph.windows.net" , "UTF-8" ) ) ; final URL url = new URL ( String . format ( "https://login.microsoftonline.com/%s/oauth2/token" , tenant . getName ( ) ) ) ; final HttpsURLConnection connection = ( HttpsURLConnection ) url . openConnection ( ) ; connection . setRequestMethod ( "POST" ) ; connection . setRequestProperty ( "Host" , "login.microsoftonline.com" ) ; connection . setRequestProperty ( "Content-Type" , "application/x-www-form-urlencoded" ) ; connection . setRequestProperty ( "Accept" , "application/json" ) ; connection . setDoOutput ( true ) ; connection . getOutputStream ( ) . write ( payload . getBytes ( ) ) ; connection . getOutputStream ( ) . flush ( ) ; final StringBuilder result = new StringBuilder ( ) ; code_block = TryStatement ;  final ObjectMapper mapper = new ObjectMapper ( ) ; final JsonNode node = mapper . readValue ( result . toString ( ) . getBytes ( ) , JsonNode . class ) ; return node . get ( "access_token" ) . asText ( ) ; } catch ( IOException e ) { LOGGER . error ( "IO Exception" , e ) ; return null ; } catch ( RuntimeException e ) { LOGGER . error ( "Runtime Exception" , e ) ; return null ; } }
public void subscribeEventChannel ( ) throws MagentaTVException { String sid = "" ; String subscribe = MessageFormat . format ( PAIRING_SUBSCRIBE , config . getIpAddress ( ) , config . getPort ( ) , network . getLocalIP ( ) , network . getLocalPort ( ) , PAIRING_NOTIFY_URI , PAIRING_TIMEOUT_SEC ) ; logger . debug ( "subscribe event channel '{}'" , subscribe ) ; String response = http . sendData ( config . getIpAddress ( ) , config . getPort ( ) , subscribe ) ; code_block = IfStatement ; code_block = IfStatement ; StringTokenizer tokenizer = new StringTokenizer ( response , "\r\n" ) ; code_block = WhileStatement ; }
public void test() { if ( ! notifiedRecords ) { logger . warn ( "Unable to record {}" , record ) ; } }
public void test() { if ( keyspaces . isEmpty ( ) ) { String message = format ( "No keyspace found in cluster %s" , cluster . getName ( ) ) ; logger . error ( message ) ; throw new IllegalArgumentException ( message ) ; } }
@ Override public void onResponse ( Result appResponse , Invoker < ? > invoker , Invocation invocation ) { LOGGER . log ( Level . SEVERE , "Error onResponse" , appResponse ) ; }
public void test() { else { LOGGER . info ( "Caught nested callback" ) ; } }
public void test() { if ( requestedHost . getStatus ( ) == ResourceHostRegistrationStatus . REQUESTED && System . currentTimeMillis ( ) - ( requestedHost . getDateUpdated ( ) == null ? 0L : requestedHost . getDateUpdated ( ) ) > TimeUnit . MINUTES . toMillis ( 60 ) ) { requestDataService . remove ( requestedHost . getId ( ) ) ; LOGGER . info ( "Removed host registration status of resource host: {}" , requestedHost . getId ( ) ) ; } }
public void test() { try { AccountEntryUserRelServiceUtil . deleteAccountEntryUserRelByEmailAddress ( accountEntryId , emailAddress ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( logger . isTraceEnabled ( LogMarker . VERSIONED_OBJECT_LIST_VERBOSE ) ) { logger . trace ( LogMarker . VERSIONED_OBJECT_LIST_VERBOSE , "getObject()" ) ; } }
private void preLoadClass ( String agentId , long agentStartTimestamp , AgentStatMetricCollector < AgentStatMetricSnapshot > agentStatCollector ) { logger . debug ( "pre-load class start" ) ; CollectJob collectJob = new CollectJob ( EmptyDataSender . INSTANCE , agentId , agentStartTimestamp , agentStatCollector , 1 ) ; collectJob . run ( ) ; logger . debug ( "pre-load class finished" ) ; collectJob . run ( ) ; }
public String computeDiffWithDiffCommand ( File directoryVersionOne , File directoryVersionTwo ) { LOGGER . info ( "------ -------------" ) ; LOGGER . info ( "The diff will be computed between:" ) ; LOGGER . info ( directoryVersionOne . getAbsolutePath ( ) + " and " ) ; LOGGER . info ( directoryVersionTwo . getAbsolutePath ( ) ) ; final String command = String . join ( " " , new String [ ] code_block = "" ; ) ; return this . executeCommand ( command , directoryVersionOne ) ; }
public String computeDiffWithDiffCommand ( File directoryVersionOne , File directoryVersionTwo ) { LOGGER . info ( "Computing the diff with diff commnd line" ) ; LOGGER . info ( directoryVersionOne . getAbsolutePath ( ) + " and " ) ; LOGGER . info ( directoryVersionTwo . getAbsolutePath ( ) ) ; LOGGER . info ( "command" ) ; final String command = String . join ( " " , new String [ ] code_block = "" ; ) ; return this . executeCommand ( command , directoryVersionOne ) ; }
public String computeDiffWithDiffCommand ( File directoryVersionOne , File directoryVersionTwo ) { LOGGER . info ( "Computing the diff with diff commnd line" ) ; LOGGER . info ( "The diff will be computed between:" ) ; LOGGER . info ( directoryVersionOne . getAbsolutePath ( ) ) ; LOGGER . info ( directoryVersionOne . getAbsolutePath ( ) ) ; final String command = String . join ( " " , new String [ ] code_block = "" ; ) ; return this . executeCommand ( command , directoryVersionOne ) ; }
public String computeDiffWithDiffCommand ( File directoryVersionOne , File directoryVersionTwo ) { LOGGER . info ( "Computing the diff with diff commnd line" ) ; LOGGER . info ( "The diff will be computed between:" ) ; LOGGER . info ( directoryVersionOne . getAbsolutePath ( ) + " and " ) ; LOGGER . info ( directoryVersionOne . getAbsolutePath ( ) + " and " ) ; final String command = String . join ( " " , new String [ ] code_block = "" ; ) ; return this . executeCommand ( command , directoryVersionOne ) ; }
@ Override public boolean create ( Personname personnameRecord ) { log . debug ( "Creating: " + personnameRecord ) ; return personnameRecord != null ? super . create ( personnameRecord ) : true ; }
public void test() { try ( EntityManagerContainer emc = EntityManagerContainerFactory . instance ( ) . create ( ) ) { _bbsConfigSetting = emc . find ( bbsConfigSetting . getId ( ) , BBSConfigSetting . class ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . warn ( "system get bbsConfigSetting got an exception!" ) ; throw e ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( bom != null ) { logger . log ( TreeLogger . ERROR , "Ignoring: " + bom . toString ( ) ) ; } }
public void test() { try { wsSession . close ( new CloseStatus ( CloseStatus . NORMAL . getCode ( ) , reason ) ) ; } catch ( IOException e ) { LOGGER . error ( "Error closing websocket session" , e ) ; } }
public void test() { try { connection2 . connect ( ) ; fail ( "Should not be able to connect with same container Id." ) ; } catch ( Exception ex ) { LOG . debug ( "Caught expected exception" , ex ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getCapabilities()" ) ; } }
public void test() { try { code_block = IfStatement ; hrq . removeDispatchedEvents ( id ) ; } catch ( RegionDestroyedException ignore ) { logger . info ( "Queue found destroyed while processing the last dispatched sequence ID for a HARegionQueue's DACE. The event ID is {} code_block = ForStatement ; ) ; } catch ( CancelException ignore ) { return false ; } catch ( CacheException e ) { logger . warn ( "Cache could not be deleted" , e ) ; } catch ( InterruptedException ignore ) { Thread . currentThread ( ) . interrupt ( ) ; return false ; } catch ( RejectedExecutionException ignore ) { interrupted = true ; } finally { code_block = IfStatement ; } }
public void test() { if ( sessionMd != null ) { sessionMd . sendPing ( currentTime ) ; } else { log . warn ( "Session md . getId ( ) + " not found in session: " + sessionId ) ; } }
public void test() { if ( internalId != null ) { SessionMetaData sessionMd = internalSessionMap . get ( internalId ) ; code_block = IfStatement ; } else { log . warn ( "Session " + sessionId + " not found" ) ; } }
public void test() { try { preDestroy . invoke ( obj ) ; } catch ( Exception e ) { log . error ( "Error while calling pre destroy method" , e ) ; } }
public void test() { try { code_block = WhileStatement ; } catch ( final IOException e ) { LOGGER . error ( "Cannot read the configuration file" , e ) ; } }
public void test() { try { setFailedStatus ( query , "Launching query failed" , e ) ; } catch ( LensException e1 ) { log . warn ( "Query failed" , e1 ) ; } }
public void test() { try { release ( query . getLensSessionIdentifier ( ) ) ; } catch ( LensException e ) { log . warn ( "Error releasing session identifier : {}" , query . getLensSessionIdentifier ( ) , e ) ; } }
public void test() { if ( showFull && logger . isDebugEnabled ( ) ) { logger . debug ( "to be created, contact common" ) ; logger . debug ( objectAsXmlString ( contact , ContactsCommon . class ) ) ; } }
public void test() { -> { LOGGER . info ( "[{}][doShutdown] Shutdown" , id ) ; return Mono . whenDelayError ( processBeforeDestroy ( ) , compositeDiscovery . shutdown ( ) , gatewayBootstrap . shutdown ( ) , transportBootstrap . shutdown ( ) ) . doOnSuccess ( s -> LOGGER . info ( "[{}][doShutdown] Shutdown" , id ) ) ; } }
public void test() { try { cf = readChemFile ( cf ) ; } catch ( IOException exception ) { String error = "Input/Output error while reading from input: " + exception . getMessage ( ) ; logger . error ( error ) ; logger . debug ( exception ) ; throw new CDKException ( error , exception ) ; } }
public void test() { if ( newest . size ( ) > 0 ) { ret = newest . get ( 0 ) . date ; logger . debug ( "Moved: guestId=" + updateInfo . getGuestId ( ) + ", maxDateInDB: " + ret ) ; } else { logger . info ( "Moves: guestId=" + updateInfo . getGuestId ( ) + ", maxDateInDB=null" ) ; } }
public void test() { if ( newest . size ( ) > 0 ) { ret = newest . get ( 0 ) . date ; logger . info ( "Moves: guestId=" + updateInfo . getGuestId ( ) + ", maxDateInDB=" + ret ) ; } else { logger . info ( "Could not find newestId=" + updateInfo . getGuestId ( ) ) ; } }
public void test() { switch ( fieldSchema . getType ( ) ) { case MAP : result = convertDocFieldToAvroMap ( docf , fieldSchema , obj , field , storeType ) ; break ; case ARRAY : result = convertDocFieldToAvroList ( docf , fieldSchema , obj , field , storeType ) ; break ; case RECORD : ODocument record = obj . field ( docf ) ; code_block = IfStatement ; result = convertAvroBeanToOrientDoc ( fieldSchema , record ) ; break ; case BOOLEAN : result = OType . convert ( obj . field ( docf ) , Boolean . class ) ; break ; case DOUBLE : result = OType . convert ( obj . field ( docf ) , Double . class ) ; break ; case FLOAT : result = OType . convert ( obj . field ( docf ) , Float . class ) ; break ; case INT : result = OType . convert ( obj . field ( docf ) , Integer . class ) ; break ; case LONG : result = OType . convert ( obj . field ( docf ) , Long . class ) ; break ; case STRING : result = convertDocFieldToAvroString ( storeType , docf , obj ) ; break ; case ENUM : result = AvroUtils . getEnumValue ( fieldSchema , obj . field ( docf ) ) ; break ; case BYTES : case FIXED : code_block = IfStatement ; result = ByteBuffer . wrap ( ( byte [ ] ) obj . field ( docf ) ) ; break ; case NULL : result = null ; break ; case UNION : result = convertDocFieldToAvroUnion ( fieldSchema , storeType , field , docf , obj ) ; break ; default : LOG . error ( "Unsupported schema type {}" , fieldSchema . getType ( ) ) ; break ; } }
public void test() { try { code_block = ForStatement ; } catch ( Exception e ) { logger . error ( Messages . getInstance ( ) . getErrorString ( "Workspace.ERROR_0002_PROPS_EXCEPTION" ) , e ) ; } }
public synchronized void deleteUser ( User user ) throws IOException { String username = user . getName ( ) ; code_block = ForStatement ; users . remove ( username ) ; LOG . info ( "Removed user {}" , username ) ; persistChanges ( ) ; }
public void test() { try { ctx . close ( ) ; } catch ( Exception ex1 ) { LOGGER . log ( Level . SEVERE , null , ex1 ) ; } }
public void test() { try { PersistMode persistMode = PersistMode . REBIND ; HighAvailabilityMode highAvailabilityMode = HighAvailabilityMode . DISABLED ; launcher = BrooklynLauncher . newInstance ( ) . localBrooklynPropertiesFile ( localBrooklynProperties ) . brooklynProperties ( OsgiManager . USE_OSGI , false ) . persistMode ( persistMode ) . persistenceDir ( persistenceDir ) . persistenceLocation ( persistenceLocation ) . highAvailabilityMode ( highAvailabilityMode ) ; log . info ( "Brooklyn: " + launcher . getAbsolutePath ( ) ) ; } catch ( FatalConfigurationRuntimeException e ) { throw e ; } catch ( Exception e ) { throw new FatalConfigurationRuntimeException ( "Fatal error configuring Brooklyn launch: " + e . getMessage ( ) , e ) ; } }
public void test() { try { launcher . cleanOrphanedState ( destinationDir , destinationLocation ) ; } catch ( FatalRuntimeException e ) { throw e ; } catch ( Exception e ) { Exceptions . propagateIfFatal ( e ) ; log . warn ( "Error removing orphaned state from " + destinationLocation , e ) ; Exceptions . propagate ( e ) ; } finally { code_block = TryStatement ;  } }
public void test() { try { log . info ( "Terminating launcher with name: " + name ) ; launcher . terminate ( ) ; } catch ( Exception e2 ) { log . debug ( "Details of subsequent error during termination: " + e2 , e2 ) ; } }
@ Override public void start ( ) { logger . info ( "Starting RollingFileSink {}." , getName ( ) ) ; sinkCounter . start ( ) ; super . start ( ) ; pathController . setBaseDirectory ( directory ) ; code_block = IfStatement ; logger . info ( "RollingFileSink {} started." , getName ( ) ) ; }
public void test() { if ( rollInterval > 0 ) { rollService = Executors . newScheduledThreadPool ( 1 , new ThreadFactoryBuilder ( ) . setNameFormat ( "rollingFileSink-roller-" + Thread . currentThread ( ) . getId ( ) + "-%d" ) . build ( ) ) ; rollService . scheduleAtFixedRate ( new Runnable ( ) code_block = "" ; , rollInterval , rollInterval , TimeUnit . SECONDS ) ; } else { log . warn ( String . format ( "Skipping message rolling interval %d" , rollInterval ) ) ; } }
@ Override public void start ( ) { logger . info ( "Starting {}..." , this ) ; sinkCounter . start ( ) ; super . start ( ) ; pathController . setBaseDirectory ( directory ) ; code_block = IfStatement ; logger . info ( "Finished {}." , this ) ; }
public void test() { try { return Optional . of ( user . asMailAddress ( ) ) ; } catch ( AddressException e ) { logger . warn ( "Can not validate mail address" , e ) ; return Optional . empty ( ) ; } }
public void test() { try { Callback [ ] callbacks = new Callback [ 1 ] ; callbacks [ 0 ] = callback ; handler . handle ( callbacks ) ; } catch ( UnsupportedCallbackException e ) { LOGGER . debug ( "Certificate verification failed." ) ; throw new ClientException ( e ) ; } catch ( IOException e ) { LOGGER . error ( "Unable to handle connection" , e ) ; throw new ClientException ( e ) ; } }
public void test() { if ( ! callback . isVerified ( ) ) { LOGGER . debug ( "Certificate verification failed." ) ; throw new ClientException ( "CA certificate fingerprint could not be verified." ) ; } else { LOGGER . debug ( "Certificate verified." ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( SharingEntryServiceUtil . class , "deleteSharingEntry" , _deleteSharingEntryParameterTypes2 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , sharingEntryId , serviceContext ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . sharing . model . SharingEntry ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
@ Secured ( ServicesData . ROLE_GET_INGEST ) @ GetMapping ( CommonConstants . PATH_ID ) public LogbookOperationDto getOne ( @ PathVariable ( "id" ) final String id ) { LOGGER . debug ( "get ingest: {}" , id ) ; ParameterChecker . checkParameter ( "The Identifier is a mandatory parameter: " , id ) ; return ingestExternalService . getOne ( id ) ; }
@ Test @ Order ( 1 ) void sampleBook_shouldHave_injectionPointsResolved ( ) { log . debug ( "TEST 1 ENTERING" ) ; val book = getSampleBook ( ) ; assertTrue ( book . hasInjectionPointsResolved ( ) ) ; log . debug ( "TEST 1 EXITING" ) ; }
@ Test @ Order ( 1 ) void sampleBook_shouldHave_injectionPointsResolved ( ) { log . debug ( "TEST 1 ENTERING" ) ; val book = getSampleBook ( ) ; assertTrue ( book . hasInjectionPointsResolved ( ) ) ; log . debug ( "TEST 1 EXITING" ) ; }
public void test() { try { code_block = WhileStatement ; } catch ( InterruptedException ex ) { LOG . error ( ex ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { status . compareAndSet ( Status . RUNNING , Status . FAILED ) ; logger . log ( Level . WARNING , "Failed to invoke service: " + service , e ) ; } finally { status . compareAndSet ( Status . RUNNING , Status . COMPLETE ) ; } }
protected InputStream getConfigurationInputStream ( String resource ) throws HibernateException { log . info ( "Reading configuration input from " + resource ) ; return ConfigHelper . getResourceAsStream ( resource ) ; }
@ Test public void testBadPassword_PreHashedServer ( ) throws Exception { Map < String , Object > serverProps = new HashMap < String , Object > ( ) ; serverProps . put ( REALM_PROPERTY , "TestRealm" ) ; serverProps . put ( PRE_DIGESTED_PROPERTY , "true" ) ; SaslServer server = new SaslServerBuilder ( DigestServerFactory . class , DIGEST ) . setUserName ( "George" ) . setPassword ( DigestPassword . ALGORITHM_DIGEST_MD5 , getDigestKeySpec ( "George" , "gpwd" , "TestRealm" ) ) . setProperties ( serverProps ) . setProtocol ( "TestProtocol" ) . setServerName ( "TestServer" ) . build ( ) ; CallbackHandler clientCallback = createClearPwdClientCallbackHandler ( "George" , "bad" , null ) ; SaslClient client = Sasl . createSaslClient ( new String [ ] code_block = "" ; , "George" , "TestProtocol" , "TestServer" , Collections . < String , Object > emptyMap ( ) , clientCallback ) ; assertFalse ( client . hasInitialResponse ( ) ) ; byte [ ] message = server . evaluateResponse ( new byte [ 0 ] ) ; log . debug ( "Challenge:" + new String ( message , StandardCharsets . ISO_8859_1 ) ) ; message = client . evaluateChallenge ( message ) ; log . debug ( "Client response:" + new String ( message , StandardCharsets . ISO_8859_1 ) ) ; code_block = TryStatement ;  }
@ Test public void testBadPassword_PreHashedServer ( ) throws Exception { Map < String , Object > serverProps = new HashMap < String , Object > ( ) ; serverProps . put ( REALM_PROPERTY , "TestRealm" ) ; serverProps . put ( PRE_DIGESTED_PROPERTY , "true" ) ; SaslServer server = new SaslServerBuilder ( DigestServerFactory . class , DIGEST ) . setUserName ( "George" ) . setPassword ( DigestPassword . ALGORITHM_DIGEST_MD5 , getDigestKeySpec ( "George" , "gpwd" , "TestRealm" ) ) . setProperties ( serverProps ) . setProtocol ( "TestProtocol" ) . setServerName ( "TestServer" ) . build ( ) ; CallbackHandler clientCallback = createClearPwdClientCallbackHandler ( "George" , "bad" , null ) ; SaslClient client = Sasl . createSaslClient ( new String [ ] code_block = "" ; , "George" , "TestProtocol" , "TestServer" , Collections . < String , Object > emptyMap ( ) , clientCallback ) ; assertFalse ( client . hasInitialResponse ( ) ) ; byte [ ] message = server . evaluateResponse ( new byte [ 0 ] ) ; log . debug ( "Challenge:" + new String ( message , StandardCharsets . ISO_8859_1 ) ) ; message = client . evaluateChallenge ( message ) ; log . debug ( "Response:" + new String ( message , StandardCharsets . ISO_8859_1 ) ) ; code_block = TryStatement ;  }
@ Override public int indexOf ( ISingleElectron electron ) { logger . debug ( "Getting electron: " , electron ) ; return super . indexOf ( electron ) ; }
public void test() { try { StorageUtils . writeInOut ( fis , fos , true ) ; return tmp ; } catch ( Exception e ) { LOGGER . error ( "write tmp file failed." , e ) ; return null ; } finally { fos . close ( ) ; fis . close ( ) ; } }
public static LdapContext getWiredContext ( LdapServer ldapServer , String principalDn , String password ) throws NamingException { Hashtable < String , String > env = new Hashtable < > ( ) ; env . put ( Context . INITIAL_CONTEXT_FACTORY , CTX_FACTORY ) ; env . put ( Context . PROVIDER_URL , Network . ldapLoopbackUrl ( ldapServer . getPort ( ) ) ) ; env . put ( Context . SECURITY_PRINCIPAL , principalDn ) ; env . put ( Context . SECURITY_CREDENTIALS , password ) ; env . put ( Context . SECURITY_AUTHENTICATION , "simple" ) ; LOGGER . info ( "Created ldap server: " + ldapServer . getPort ( ) ) ; return new InitialLdapContext ( env , null ) ; }
public void test() { if ( i % 1000 == 0 ) { logger . info ( "sent: " + i ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( AssetListEntryServiceUtil . class , "getAssetListEntriesCount" , _getAssetListEntriesCountParameterTypes18 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( ( Integer ) returnObj ) . intValue ( ) ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
@ GET @ Timed @ Compress @ Produces ( APPLICATION_JSON_WITH_CHARSET ) public String list ( @ Context GraphManager manager , @ PathParam ( "graph" ) String graph , @ QueryParam ( "ids" ) List < String > stringIds ) { LOG . debug ( "Graph [{}] list: {}" , graph , stringIds ) ; E . checkArgument ( stringIds != null && ! stringIds . isEmpty ( ) , "The ids parameter can't be null or empty" ) ; Object [ ] ids = new Id [ stringIds . size ( ) ] ; code_block = ForStatement ; HugeGraph g = graph ( manager , graph ) ; Iterator < Edge > edges = g . edges ( ids ) ; return manager . serializer ( g ) . writeEdges ( edges , false ) ; }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { try { String json = _OBJECT_MAPPER . writeValueAsString ( regionModels ) ; return Response . ok ( json , MediaType . APPLICATION_JSON ) . build ( ) ; } catch ( JsonProcessingException jsonProcessingException ) { _log . error ( jsonProcessingException , jsonProcessingException ) ; } }
public void test() { try { FileStatus fileStatus = fileSystem . getFileStatus ( path ) ; code_block = IfStatement ; } catch ( FileNotFoundException e ) { Path parent = path . getParent ( ) ; code_block = IfStatement ; } catch ( InterruptedIOException ioe ) { throw ioe ; } catch ( Exception e ) { log . warn ( "Unable to clean up temporary file: " + path , e ) ; } }
public void test() { if ( StringUtils . isEmpty ( System . getenv ( "DART_POST_PROCESS_FILE" ) ) ) { LOGGER . info ( "Environment variable DART_POST_PROCESS_FILE not defined so the Dart code may not be properly formatted. To define it, try 'export DART_POST_PROCESS_FILE=\"/usr/local/bin/dartfmt -i\"' (Linux/Mac)" ) ; LOGGER . info ( "NOTE: To enable file post-processing, 'enablePostProcessFile' must be set to `true` (--enable-post-process-file for CLI)." ) ; } }
public void test() { if ( ! additionalProperties . containsKey ( CLIENT_NAME ) ) { final String name = org . openapitools . codegen . utils . StringUtils . camelize ( pubName ) ; additionalProperties . put ( CLIENT_NAME , name ) ; LOG . debug ( "Cluster name set to {}" , name ) ; } }
public void test() { try { synchronized ( solrServer ) code_block = "" ; } catch ( Exception e ) { logger . warn ( "Unable to index Solr: {}" , e . getMessage ( ) ) ; } }
public void test() { try { addEntryEventSubscription ( LINK_CHANGED , layerizedId ) ; addEntryEventSubscription ( FLOW_CHANGED , layerizedId ) ; addEntryEventSubscription ( OUT_PACKET_ADDED , layerizedId ) ; String attrBase = AttrElements . ATTRIBUTES + SEPARATOR + "%s" ; ArrayList < String > portAttributes = new ArrayList < String > ( Arrays . asList ( String . format ( attrBase , AttrElements . UNRESERVED_BANDWIDTH ) , String . format ( attrBase , AttrElements . IS_BOUNDARY ) ) ) ; updateEntryEventSubscription ( PORT_CHANGED , layerizedId , portAttributes ) ; ArrayList < String > linkAttributes = new ArrayList < String > ( Arrays . asList ( String . format ( attrBase , AttrElements . COST ) , String . format ( attrBase , AttrElements . REQ_LATENCY ) , String . format ( attrBase , AttrElements . UNRESERVED_BANDWIDTH ) , String . format ( attrBase , AttrElements . REQ_BANDWIDTH ) ) ) ; updateEntryEventSubscription ( LINK_CHANGED , layerizedId , linkAttributes ) ; ArrayList < String > flowAttributes = new ArrayList < String > ( Arrays . asList ( NetworkElements . OWNER , NetworkElements . ENABLED , NetworkElements . PRIORITY , String . format ( attrBase , AttrElements . BANDWIDTH ) , String . format ( attrBase , AttrElements . LATENCY ) ) ) ; updateEntryEventSubscription ( FLOW_CHANGED , layerizedId , flowAttributes ) ; applyEventSubscription ( ) ; } catch ( Exception ex ) { LOGGER . error ( ex . getMessage ( ) ) ; } }
public void test() { if ( ! c . equals ( b ) ) { LOGGER . warning ( "Subject " + c + " does not equal " + b ) ; } }
public void test() { if ( this . locationCache . shouldRefreshEndpoints ( canRefreshInBackground ) ) { code_block = IfStatement ; code_block = IfStatement ; logger . debug ( "shouldRefreshEndpoints: false, nothing to do." ) ; this . isRefreshing . set ( false ) ; return Completable . complete ( ) ; } else { logger . debug ( "shouldRefreshEndpoints: false, nothing to do." ) ; this . isRefreshing . set ( false ) ; return Completable . complete ( ) ; } }
public void test() { if ( this . locationCache . shouldRefreshEndpoints ( canRefreshInBackground ) ) { logger . debug ( "shouldRefreshEndpoints: true" ) ; code_block = IfStatement ; code_block = IfStatement ; this . isRefreshing . set ( false ) ; return Completable . complete ( ) ; } else { logger . debug ( "Should not refresh, returning false" ) ; this . isRefreshing . set ( false ) ; return Completable . complete ( ) ; } }
public void test() { try { logger . debug ( "deleting temp file : " + tempFile . getName ( ) ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "can not delete temp file : " + e . getMessage ( ) ) ; } }
public void setScannerFactory ( ScannerFactory scannerFactory ) { LOG . info ( "Using ScannerFactory: {}" , scannerFactory ) ; this . scannerFactory = scannerFactory ; }
public void test() { if ( bridge == null ) { updateStatus ( ThingStatus . OFFLINE , ThingStatusDetail . CONFIGURATION_ERROR , "No bridge configured" ) ; logger . debug ( "No bridge configuration found for {}" , bridge . getStatus ( ) ) ; } else-if ( bridge . getStatus ( ) == ThingStatus . ONLINE ) { updateStatus ( ThingStatus . ONLINE ) ; logger . debug ( "Initialized device state for shade {}" , ThingStatus . ONLINE ) ; } else { updateStatus ( ThingStatus . OFFLINE , ThingStatusDetail . BRIDGE_OFFLINE ) ; logger . debug ( "Initialized device state for shade {} {}" , ThingStatus . OFFLINE , ThingStatusDetail . BRIDGE_OFFLINE ) ; } }
public void test() { if ( bridge == null ) { updateStatus ( ThingStatus . OFFLINE , ThingStatusDetail . CONFIGURATION_ERROR , "No bridge configured" ) ; logger . debug ( "Initialized device state for shade {} {}" , ThingStatus . OFFLINE , ThingStatusDetail . CONFIGURATION_ERROR ) ; } else-if ( bridge . getStatus ( ) == ThingStatus . ONLINE ) { updateStatus ( ThingStatus . ONLINE ) ; logger . debug ( "Initializing device state for shade {} {}" , ThingStatus . ONLINE , ThingStatus . ONLINE ) ; } else { updateStatus ( ThingStatus . OFFLINE , ThingStatusDetail . BRIDGE_OFFLINE ) ; logger . debug ( "Initialized device state for shade {} {}" , ThingStatus . OFFLINE , ThingStatusDetail . BRIDGE_OFFLINE ) ; } }
public void test() { if ( bridge == null ) { updateStatus ( ThingStatus . OFFLINE , ThingStatusDetail . CONFIGURATION_ERROR , "No bridge configured" ) ; logger . debug ( "Initialized device state for shade {} {}" , ThingStatus . OFFLINE , ThingStatusDetail . CONFIGURATION_ERROR ) ; } else-if ( bridge . getStatus ( ) == ThingStatus . ONLINE ) { updateStatus ( ThingStatus . ONLINE ) ; logger . debug ( "Initialized device state for shade {}" , ThingStatus . ONLINE ) ; } else { updateStatus ( ThingStatus . OFFLINE , ThingStatusDetail . BRIDGE_OFFLINE ) ; logger . debug ( "Initializing device state for shade {} failed" , bridge . getStatus ( ) ) ; } }
public static void main ( String [ ] args ) { code_block = IfStatement ; log . info ( "Running Spring application" ) ; SpringApplication . run ( LambdaApplication . class , args ) ; }
public void test() { if ( ! ObjectUtils . isEmpty ( args ) ) { logger . info ( args ) ; } }
public void test() { try { final JwtParser jwtParser = Jwts . parser ( ) . setSigningKey ( key ) ; final Jws < Claims > claims = jwtParser . parseClaimsJws ( token ) ; this . subject = claims . getBody ( ) . getSubject ( ) ; this . expiration = claims . getBody ( ) . getExpiration ( ) ; this . identityProvider = IdentityProvider . forName ( claims . getBody ( ) . get ( IDENTITY_PROVIDER_CLAIM , String . class ) ) ; return true ; } catch ( SignatureException e ) { LOGGER . debug ( "Signature exception" , e ) ; } catch ( ExpiredJwtException e ) { LOGGER . debug ( SecurityMarkers . SECURITY_FAILURE , "Received expired token" ) ; } catch ( MalformedJwtException e ) { LOGGER . debug ( SecurityMarkers . SECURITY_FAILURE , "Received malformed token" ) ; LOGGER . debug ( SecurityMarkers . SECURITY_FAILURE , e . getMessage ( ) ) ; } catch ( UnsupportedJwtException | IllegalArgumentException e ) { LOGGER . error ( SecurityMarkers . SECURITY_FAILURE , e . getMessage ( ) ) ; } }
public void test() { try { final JwtParser jwtParser = Jwts . parser ( ) . setSigningKey ( key ) ; final Jws < Claims > claims = jwtParser . parseClaimsJws ( token ) ; this . subject = claims . getBody ( ) . getSubject ( ) ; this . expiration = claims . getBody ( ) . getExpiration ( ) ; this . identityProvider = IdentityProvider . forName ( claims . getBody ( ) . get ( IDENTITY_PROVIDER_CLAIM , String . class ) ) ; return true ; } catch ( SignatureException e ) { LOGGER . info ( SecurityMarkers . SECURITY_FAILURE , "Received token that did not pass signature verification" ) ; } catch ( ExpiredJwtException e ) { LOGGER . debug ( SecurityMarkers . SECURITY_FAILURE , "Received expired token" ) ; } catch ( MalformedJwtException e ) { LOGGER . debug ( SecurityMarkers . SECURITY_FAILURE , "Received malformed token" ) ; LOGGER . debug ( SecurityMarkers . SECURITY_FAILURE , e . getMessage ( ) ) ; } catch ( UnsupportedJwtException | IllegalArgumentException e ) { LOGGER . error ( SecurityMarkers . SECURITY_FAILURE , e . getMessage ( ) ) ; } }
public void test() { try { final JwtParser jwtParser = Jwts . parser ( ) . setSigningKey ( key ) ; final Jws < Claims > claims = jwtParser . parseClaimsJws ( token ) ; this . subject = claims . getBody ( ) . getSubject ( ) ; this . expiration = claims . getBody ( ) . getExpiration ( ) ; this . identityProvider = IdentityProvider . forName ( claims . getBody ( ) . get ( IDENTITY_PROVIDER_CLAIM , String . class ) ) ; LOGGER . info ( SecurityMarkers . SECURITY_FAILURE , "Received token" ) ; return true ; } catch ( SignatureException e ) { LOGGER . info ( SecurityMarkers . SECURITY_FAILURE , "Received token that did not pass signature verification" ) ; } catch ( ExpiredJwtException e ) { LOGGER . debug ( SecurityMarkers . SECURITY_FAILURE , "Received expired token" ) ; } catch ( MalformedJwtException e ) { LOGGER . debug ( SecurityMarkers . SECURITY_FAILURE , e . getMessage ( ) ) ; } catch ( UnsupportedJwtException | IllegalArgumentException e ) { LOGGER . error ( SecurityMarkers . SECURITY_FAILURE , e . getMessage ( ) ) ; } }
public void test() { try { final JwtParser jwtParser = Jwts . parser ( ) . setSigningKey ( key ) ; final Jws < Claims > claims = jwtParser . parseClaimsJws ( token ) ; this . subject = claims . getBody ( ) . getSubject ( ) ; this . expiration = claims . getBody ( ) . getExpiration ( ) ; this . identityProvider = IdentityProvider . forName ( claims . getBody ( ) . get ( IDENTITY_PROVIDER_CLAIM , String . class ) ) ; LOGGER . info ( SecurityMarkers . SECURITY_FAILURE , "Received token" ) ; return true ; } catch ( SignatureException e ) { LOGGER . info ( SecurityMarkers . SECURITY_FAILURE , "Received token that did not pass signature verification" ) ; } catch ( ExpiredJwtException e ) { LOGGER . debug ( SecurityMarkers . SECURITY_FAILURE , "Received expired token" ) ; } catch ( MalformedJwtException e ) { LOGGER . debug ( SecurityMarkers . SECURITY_FAILURE , "Received malformed token" ) ; } catch ( UnsupportedJwtException | IllegalArgumentException e ) { LOGGER . error ( SecurityMarkers . SECURITY_FAILURE , e . getMessage ( ) ) ; } }
public void test() { try { final JwtParser jwtParser = Jwts . parser ( ) . setSigningKey ( key ) ; final Jws < Claims > claims = jwtParser . parseClaimsJws ( token ) ; this . subject = claims . getBody ( ) . getSubject ( ) ; this . expiration = claims . getBody ( ) . getExpiration ( ) ; this . identityProvider = IdentityProvider . forName ( claims . getBody ( ) . get ( IDENTITY_PROVIDER_CLAIM , String . class ) ) ; return true ; } catch ( SignatureException e ) { LOGGER . info ( SecurityMarkers . SECURITY_FAILURE , "Received token that did not pass signature verification" ) ; } catch ( ExpiredJwtException e ) { LOGGER . debug ( SecurityMarkers . SECURITY_FAILURE , "Received expired token" ) ; } catch ( MalformedJwtException e ) { LOGGER . debug ( SecurityMarkers . SECURITY_FAILURE , "Received malformed token" ) ; LOGGER . debug ( SecurityMarkers . SECURITY_FAILURE , e . getMessage ( ) ) ; } catch ( UnsupportedJwtException | IllegalArgumentException e ) { LOGGER . warn ( "Unsupported JWT token" ) ; } }
public void test() { for ( AugmentedRow ar : extractedAugmentedRows ) { ar . setCommitTimestamp ( commitTimestamp ) ; ar . setTransactionSequenceNumber ( transactionSequenceNumber ) ; Long microsOverride = commitTimestamp * 1000 + ar . getTransactionSequenceNumber ( ) ; ar . setRowMicrosecondTimestamp ( microsOverride ) ; LOGGER . debug ( "commitTimestamp = {}" , ar . getTransactionTimestamp ( ) ) ; } }
public void test() { for ( int i = 0 ; i < this . runs ; i ++ ) { logger . info ( "Run " + i ) ; } }
public void test() { for ( Pair < AnnotationKey > annotationKey : annotationKeys ) { LOGGER . debug ( "\t{}" , annotationKey . getId ( ) ) ; } }
public void delete ( StgMUmsetzStatTxt persistentInstance ) { log . debug ( "deleting StgMUmsetzStatTxt instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . delete ( persistentInstance ) ; log . debug ( "delete successful" ) ; } catch ( RuntimeException re ) { log . error ( "delete failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . delete ( persistentInstance ) ; log . debug ( "delete successful" ) ; } catch ( RuntimeException re ) { log . error ( "delete failed" , re ) ; throw re ; } }
public void stop ( ) { clientController . stopClient ( false ) ; if ( executor == null ) return ; serverChannel . unbind ( ) ; serverSecureChannel . unbind ( ) ; serverChannel . close ( ) ; serverSecureChannel . close ( ) ; serverChannel . getCloseFuture ( ) . awaitUninterruptibly ( ) ; serverSecureChannel . getCloseFuture ( ) . awaitUninterruptibly ( ) ; executor . shutdownNow ( ) ; executor = null ; serverBootstrap . shutdown ( ) ; serverSecureBootstrap . shutdown ( ) ; nioServerSocketChannelFactory . shutdown ( ) ; log . info ( "Cluster Server stopped" ) ; }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Subscribing to {}" , uri ) ; } }
public void onReleased ( String serviceName ) { log . info ( "Released: " + serviceName ) ; }
public void test() { if ( distanceTolerance < MIN_DISTANCE_TOLERANCE_IN_METERS ) { LOGGER . debug ( "DistanceTolerance set to: {}" , distanceTolerance ) ; this . distanceTolerance = distanceTolerance ; } else { this . distanceTolerance = distanceTolerance ; } }
public void test() { try { attMap . put ( "id.documentId" , documentNominalLabel . getIdDTO ( ) . getDocumentId ( ) ) ; attMap . put ( "id.nominalLabelId" , documentNominalLabel . getIdDTO ( ) . getNominalLabelId ( ) ) ; } catch ( PropertyNotSetException e ) { logger . error ( "PropertyNotSetException while getting id.documentId: " + e . getMessage ( ) ) ; } }
public void test() { if ( JoalAudioFactory . checkALError ( ) ) { log . warn ( "Error creating JoalAudioListener ({})" , this . getSystemName ( ) ) ; } }
public void test() { try { termsPolicyResponse = appMetaDataOrchestration . termsPolicy ( ) ; code_block = IfStatement ; } catch ( Exception e ) { LOGGER . error ( "StudyMetaDataOrchestration - termsPolicy() :: ERROR" , e ) ; StudyMetaDataUtil . getFailureResponse ( ErrorCodes . STATUS_104 , ErrorCodes . UNKNOWN , StudyMetaDataConstants . FAILURE , response ) ; return Response . status ( Response . Status . INTERNAL_SERVER_ERROR ) . entity ( StudyMetaDataConstants . FAILURE ) . build ( ) ; } }
public void test() { try { Files . copy ( file , dest , StandardCopyOption . REPLACE_EXISTING ) ; } catch ( Exception ex ) { LOGGER . error ( ex . getMessage ( ) , ex ) ; } }
public void test() { try { future . get ( ) ; } catch ( InterruptedException | ExecutionException ex ) { Log . error ( ex ) ; } }
public void test() { try { sink . process ( ) ; failed = false ; } catch ( EventDeliveryException ex ) { log . error ( "Failed to publish events" , ex ) ; failed = true ; } }
@ Before public void createClient ( ) throws IOException { final SshClient client = SshClient . setUpDefaultClient ( ) ; client . setForwardingFilter ( AcceptAllForwardingFilter . INSTANCE ) ; client . start ( ) ; LOG . debug ( "Using client: {}" , test ) ; session = client . connect ( "user" , TEST_LOCALHOST , sshServerPort ) . verify ( CONNECT_TIMEOUT ) . getSession ( ) ; LOG . debug ( "Authenticating..." ) ; session . addPasswordIdentity ( "foo" ) ; session . auth ( ) . verify ( AUTH_TIMEOUT ) ; LOG . debug ( "Authenticated" ) ; }
@ Before public void createClient ( ) throws IOException { final SshClient client = SshClient . setUpDefaultClient ( ) ; client . setForwardingFilter ( AcceptAllForwardingFilter . INSTANCE ) ; client . start ( ) ; LOG . debug ( "Connecting..." ) ; session = client . connect ( "user" , TEST_LOCALHOST , sshServerPort ) . verify ( CONNECT_TIMEOUT ) . getSession ( ) ; session . addPasswordIdentity ( "foo" ) ; LOG . debug ( "Session created" ) ; session . auth ( ) . verify ( AUTH_TIMEOUT ) ; LOG . debug ( "Authenticated" ) ; }
@ Before public void createClient ( ) throws IOException { final SshClient client = SshClient . setUpDefaultClient ( ) ; client . setForwardingFilter ( AcceptAllForwardingFilter . INSTANCE ) ; client . start ( ) ; LOG . debug ( "Connecting..." ) ; session = client . connect ( "user" , TEST_LOCALHOST , sshServerPort ) . verify ( CONNECT_TIMEOUT ) . getSession ( ) ; LOG . debug ( "Authenticating..." ) ; session . addPasswordIdentity ( "foo" ) ; session . auth ( ) . verify ( AUTH_TIMEOUT ) ; LOG . debug ( "Client created" ) ; }
public void test() { if ( ActiveMQRALogger . LOGGER . isTraceEnabled ( ) ) { ActiveMQRALogger . LOGGER . trace ( "execute()" ) ; } }
@ Override public void onClose ( ) { logger . info ( "onClose" ) ; doWakeup ( ) ; }
private Response notifyInPacketAdded ( Packet inpacket ) throws Exception { log . debug ( "" ) ; InPacketAdded msg = new InPacketAdded ( inpacket ) ; return postEvent ( InPacketAdded . TYPE , msg ) ; }
public void test() { try { VerbRef verbRef = _mohoRemote . getRayoClient ( ) . output ( text , this . getId ( ) ) ; output = new OutputImpl < T > ( verbRef , this , ( T ) this ) ; } catch ( XmppException e ) { LOG . error ( "" , e ) ; throw new MediaException ( e ) ; } finally { getComponentstLock ( ) . unlock ( ) ; } }
@ Test public void testProcessFlagsOrder ( ) throws Exception { setUpFlagDir ( ) ; createTestFiles ( 2 , 5 ) ; final List < Collection < InputFile > > flagFileLists = new ArrayList < > ( ) ; FlagMaker instance = new TestWrappedFlagMaker ( fmc ) code_block = "" ; ; instance . processFlags ( ) ; assertEquals ( 2 , flagFileLists . size ( ) ) ; long lastTime = 0 ; code_block = ForStatement ; LOG . info ( "Flags = " + lastTime ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isErrorEnabled ( ) ) { log . error ( throwable . getMessage ( ) ) ; } }
@ Override public void putCollectionReference ( String ref , CollectionReference col ) { LOGGER . debug ( "Clearing collection '" + ref + "' from cache" ) ; code_block = TryStatement ;  LOGGER . debug ( "Clearing field types of collection '" + ref + "' from cache" ) ; this . instance . getReplicatedMap ( ref ) . clear ( ) ; }
public void test() { if ( logger . isTraceEnabled ( LogMarker . SERIALIZER_VERBOSE ) ) { logger . trace ( LogMarker . SERIALIZER_VERBOSE , "Serialize {}" , this ) ; } }
public void test() { try { FileWriter fw = new FileWriter ( urlFile , true ) ; fw . write ( url . toExternalForm ( ) ) ; fw . write ( "\n" ) ; fw . close ( ) ; RipStatusMessage msg = new RipStatusMessage ( STATUS . DOWNLOAD_COMPLETE , urlFile ) ; observer . update ( this , msg ) ; } catch ( IOException e ) { logger . error ( "Failed to download URL" , e ) ; return false ; } }
@ Override public List < String > getListInDirWithFilter ( final IRODSFile irodsFile , final FilenameFilter fileNameFilter ) throws JargonException , DataNotFoundException { log . info ( "getListInDirWithFilter()" ) ; code_block = IfStatement ; code_block = IfStatement ; List < String > subdirs = new ArrayList < > ( ) ; String path = irodsFile . getAbsolutePath ( ) ; log . debug ( "path for query:{}" , path ) ; IRODSGenQueryBuilder builder = new IRODSGenQueryBuilder ( true , null ) ; IRODSQueryResultSet resultSet = null ; code_block = TryStatement ;  resultSet = null ; builder = new IRODSGenQueryBuilder ( true , null ) ; IRODSFileSystemAOHelper . buildQueryListAllFiles ( path , builder ) ; code_block = TryStatement ;  return subdirs ; }
@ Override public List < String > getListInDirWithFilter ( final IRODSFile irodsFile , final FilenameFilter fileNameFilter ) throws JargonException , DataNotFoundException { log . info ( "getListInDirWithFilter(final IRODSFile irodsFile,final FilenameFilter fileNameFilter) " ) ; code_block = IfStatement ; code_block = IfStatement ; List < String > subdirs = new ArrayList < > ( ) ; String path = irodsFile . getAbsolutePath ( ) ; log . info ( "query subdirs: {}" , path ) ; IRODSGenQueryBuilder builder = new IRODSGenQueryBuilder ( true , null ) ; IRODSQueryResultSet resultSet = null ; code_block = TryStatement ;  resultSet = null ; builder = new IRODSGenQueryBuilder ( true , null ) ; IRODSFileSystemAOHelper . buildQueryListAllFiles ( path , builder ) ; code_block = TryStatement ;  return subdirs ; }
public void test() { try { email_ Intent . addFlags ( Intent . FLAG_ACTIVITY_NEW_TASK ) ; code_block = IfStatement ; } catch ( android . content . ActivityNotFoundException ex ) { Toast . makeText ( activityContext , activityContext . getString ( R . string . email_client_not_present ) , Toast . LENGTH_SHORT ) . show ( ) ; Logger . log ( ex ) ; } }
public void test() { try { ListTypeServiceUtil . validate ( listTypeId , classNameId , type ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
protected void processProcedure ( Connection conn , DatabaseMetaData metaData ) throws SQLException { _log . info ( "...Extracting procedures" ) ; final Map < String , DfProcedureMeta > procedureMap = extractProcedureMap ( ) ; code_block = IfStatement ; _log . info ( "...Extracting procedure" ) ; final Element procedureGroupElement = _doc . createElement ( "procedureGroup" ) ; code_block = ForStatement ; _databaseNode . appendChild ( procedureGroupElement ) ; }
@ Test public void restartBothBundles ( ) throws BundleException { Logger log = LoggerFactory . getLogger ( this . getClass ( ) ) ; Bundle paxLoggingApi = Helpers . paxLoggingApi ( context ) ; Bundle paxLoggingService = Helpers . paxLoggingLog4j1 ( context ) ; log . info ( "Before restarting" ) ; osgiLog . log ( LogService . LOG_INFO , "Before restarting" ) ; paxLoggingApi . stop ( Bundle . STOP_TRANSIENT ) ; paxLoggingService . stop ( Bundle . STOP_TRANSIENT ) ; log . info ( "When bundles are stopped" ) ; osgiLog . log ( LogService . LOG_INFO , "When bundles are stopped (log service)" ) ; assertNull ( context . getServiceReference ( LogService . class ) ) ; Logger log1 = LoggerFactory . getLogger ( this . getClass ( ) ) ; log1 . info ( "When bundles are stopped (log1)" ) ; paxLoggingService . start ( Bundle . START_TRANSIENT ) ; paxLoggingApi . start ( Bundle . START_TRANSIENT ) ; log . info ( "After restarting bundles" ) ; log1 . info ( "After restarting bundles (log1)" ) ; osgiLog . log ( LogService . LOG_INFO , "After restarting bundles (log service old ref)" ) ; ServiceReference < LogService > ref = context . getServiceReference ( LogService . class ) ; assertNotNull ( ref ) ; context . getService ( ref ) . log ( LogService . LOG_INFO , "After restarting bundles (log service new ref)" ) ; Logger log3 = LoggerFactory . getLogger ( this . getClass ( ) ) ; log3 . info ( "After restarting bundles (log3)" ) ; List < String > lines = readLines ( ) ; assertTrue ( "TTCCLayout" , lines . contains ( "[main] INFO org.ops4j.pax.logging.it.Log4J1RestartBothPaxLoggingBundlesIntegrationTest - Before restarting" ) ) ; assertTrue ( "TTCCLayout" , lines . contains ( "[main] INFO PaxExam-Pro
@ Test public void restartBothBundles ( ) throws BundleException { Logger log = LoggerFactory . getLogger ( this . getClass ( ) ) ; Bundle paxLoggingApi = Helpers . paxLoggingApi ( context ) ; Bundle paxLoggingService = Helpers . paxLoggingLog4j1 ( context ) ; log . info ( "Before restarting" ) ; osgiLog . log ( LogService . LOG_INFO , "Before restarting" ) ; paxLoggingApi . stop ( Bundle . STOP_TRANSIENT ) ; paxLoggingService . stop ( Bundle . STOP_TRANSIENT ) ; log . info ( "When bundles are stopped" ) ; osgiLog . log ( LogService . LOG_INFO , "When bundles are stopped (log service)" ) ; assertNull ( context . getServiceReference ( LogService . class ) ) ; Logger log1 = LoggerFactory . getLogger ( this . getClass ( ) ) ; log1 . info ( "When bundles are stopped (log1)" ) ; paxLoggingService . start ( Bundle . START_TRANSIENT ) ; paxLoggingApi . start ( Bundle . START_TRANSIENT ) ; log . info ( "After restarting bundles" ) ; log1 . info ( "After restarting bundles (log1)" ) ; osgiLog . log ( LogService . LOG_INFO , "After restarting bundles (log service old ref)" ) ; ServiceReference < LogService > ref = context . getServiceReference ( LogService . class ) ; assertNotNull ( ref ) ; context . getService ( ref ) . log ( LogService . LOG_INFO , "After restarting bundles (log service new ref)" ) ; Logger log3 = LoggerFactory . getLogger ( this . getClass ( ) ) ; log3 . info ( "After restarting bundles (log3)" ) ; List < String > lines = readLines ( ) ; assertTrue ( "TTCCLayout" , lines . contains ( "[main] INFO org.ops4j.pax.logging.it.Log4J1RestartBothPaxLoggingBundlesIntegrationTest - Before restarting" ) ) ; assertTrue ( "TTCCLayout" , lines . contains ( "[main] INFO PaxExam-Pro
@ Test public void restartBothBundles ( ) throws BundleException { Logger log = LoggerFactory . getLogger ( this . getClass ( ) ) ; Bundle paxLoggingApi = Helpers . paxLoggingApi ( context ) ; Bundle paxLoggingService = Helpers . paxLoggingLog4j1 ( context ) ; log . info ( "Before restarting" ) ; osgiLog . log ( LogService . LOG_INFO , "Before restarting" ) ; paxLoggingApi . stop ( Bundle . STOP_TRANSIENT ) ; paxLoggingService . stop ( Bundle . STOP_TRANSIENT ) ; log . info ( "When bundles are stopped" ) ; osgiLog . log ( LogService . LOG_INFO , "When bundles are stopped (log service)" ) ; assertNull ( context . getServiceReference ( LogService . class ) ) ; Logger log1 = LoggerFactory . getLogger ( this . getClass ( ) ) ; log1 . info ( "When bundles are stopped (log1)" ) ; paxLoggingService . start ( Bundle . START_TRANSIENT ) ; paxLoggingApi . start ( Bundle . START_TRANSIENT ) ; log1 . info ( "After restarting bundles (log1)" ) ; osgiLog . log ( LogService . LOG_INFO , "After restarting bundles (log service old ref)" ) ; ServiceReference < LogService > ref = context . getServiceReference ( LogService . class ) ; assertNotNull ( ref ) ; context . getService ( ref ) . log ( LogService . LOG_INFO , "After restarting bundles (log service new ref)" ) ; Logger log3 = LoggerFactory . getLogger ( this . getClass ( ) ) ; log3 . info ( "After restarting bundles (log3)" ) ; List < String > lines = readLines ( ) ; assertTrue ( "TTCCLayout" , lines . contains ( "[main] INFO org.ops4j.pax.logging.it.Log4J1RestartBothPaxLoggingBundlesIntegrationTest - Before restarting" ) ) ; assertTrue ( "TTCCLayout" , lines . contains ( "[main] INFO PaxExam-Probe - Before restarting" ) ) ; osgiLog
public void test() { if ( DependencyUtil . containsLibraryDependency ( deps , nested_lib ) ) { logger . warn ( "Deprecated use '" + nested_lib + "'" ) ; } else { final Dependency dep = new Dependency ( ) ; dep . setLib ( nested_lib ) ; dep . setApp ( this . getApplication ( ) ) ; final Path archive_path = paa . getArchivePath ( ) ; code_block = IfStatement ; dep . setDeclared ( false ) ; dep . setScope ( Scope . RUNTIME ) ; dep . setTransitive ( true ) ; deps . add ( dep ) ; } }
public void test() { if ( nested_fa instanceof PythonArchiveAnalyzer ) { code_block = TryStatement ;  } else { logger . log ( Level . SEVERE , "Analyzer does not support nested analyzer" ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception ex ) { LOGGER . error ( "Failed to clean up." , ex ) ; } }
@ Override public void onClick ( AjaxRequestTarget target ) { String cacheInformation = getCacheInformation ( ) ; MidPointApplication . get ( ) . getCacheRegistry ( ) . dumpContent ( ) ; getSession ( ) . success ( getPageBase ( ) . getString ( "InternalsCachePanel.result.dumped" ) ) ; target . add ( getPageBase ( ) ) ; LOGGER . info ( "Cache for cache " + cacheInformation ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void foo ( @ Simple ( "${header.foo}" ) String bar ) { LOG . info ( "foo {}" , bar ) ; this . bar = bar ; }
public void test() { try { getCoordinator ( ) . makeCubeImmutableForReceiver ( node , cubeName ) ; } catch ( IOException ioe ) { logger . error ( ioe . getMessage ( ) , ioe ) ; failedNodes . add ( node ) ; } }
public void test() { try { } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { eventExecutorGroup . shutdownGracefully ( ) . sync ( ) ; } catch ( InterruptedException e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( ! StringUtil . isEmpty ( dbDir ) ) { logger . info ( "kylin.source.hive.warehouse-dir is {}" , dbDir ) ; } else { logger . warn ( "kylin.source.hive.warehouse-dir is null" ) ; } }
public void test() { if ( ! StringUtil . isEmpty ( dbDir ) ) { logger . info ( "kylin.source.hive.warehouse-dir is {}" , dbDir ) ; } else { logger . info ( "kylin.source.hive.warehouse-dir is empty" ) ; } }
public void test() { if ( LOGGER . isInfoEnabled ( ) ) { LOGGER . info ( "Success!" ) ; } }
public void test() { if ( caught instanceof AuthenticationError ) { Log . error ( "Authentication error (" + sessionId + ")" , caught ) ; Application . redirectToLogin ( ) ; } else-if ( caught instanceof InvalidTokenError ) { Log . error ( "Invalid Token error (" + sessionId + ")" , caught ) ; Application . redirectToLogin ( ) ; } else-if ( caught instanceof AuthorizationError ) { Log . info ( "RCP Authorization Error calling " + action . getClass ( ) + ": " + caught . getMessage ( ) ) ; callback . onFailure ( caught ) ; } else { callback . onFailure ( caught ) ; } }
public void test() { if ( caught instanceof AuthenticationError ) { Log . error ( "Authentication error." , caught ) ; Application . redirectToLogin ( ) ; } else-if ( caught instanceof InvalidTokenError ) { Log . error ( "Invalid token error." , caught ) ; Application . redirectToLogin ( ) ; } else-if ( caught instanceof AuthorizationError ) { Log . info ( "RCP Authorization Error calling " + action . getClass ( ) + ": " + caught . getMessage ( ) ) ; callback . onFailure ( caught ) ; } else { callback . onFailure ( caught ) ; } }
public void test() { if ( caught instanceof AuthenticationError ) { Log . error ( "Authentication error." , caught ) ; Application . redirectToLogin ( ) ; } else-if ( caught instanceof InvalidTokenError ) { Log . error ( "Invalid Token error (" + sessionId + ")" , caught ) ; Application . redirectToLogin ( ) ; } else-if ( caught instanceof AuthorizationError ) { Log . info ( "Authorization error [" + sessionId + "]: " + caught . getMessage ( ) ) ; callback . onFailure ( caught ) ; } else { callback . onFailure ( caught ) ; } }
@ Override public void deleteStaticRoute ( String netIdIpAdress , String nextHopIpAddress ) throws CapabilityException { log . info ( "Start of deleteStaticRoute call" ) ; String [ ] aParams = new String [ 2 ] ; aParams [ 0 ] = netIdIpAdress ; aParams [ 1 ] = nextHopIpAddress ; IAction action = createActionAndCheckParams ( StaticRouteActionSet . STATIC_ROUTE_DELETE , aParams ) ; queueAction ( action ) ; log . info ( "End of deleteStaticRoute call" ) ; }
@ Override public void deleteStaticRoute ( String netIdIpAdress , String nextHopIpAddress ) throws CapabilityException { log . info ( "Start of deleteStaticRoute call" ) ; String [ ] aParams = new String [ 2 ] ; aParams [ 0 ] = netIdIpAdress ; aParams [ 1 ] = nextHopIpAddress ; IAction action = createActionAndCheckParams ( StaticRouteActionSet . STATIC_ROUTE_DELETE , aParams ) ; queueAction ( action ) ; log . info ( "End of deleteStaticRoute call" ) ; }
@ Override public int read ( final String context ) throws IOException { int b = read ( ) ; logger . trace ( "read({})" , context ) ; return b ; }
public void test() { if ( logScore ) { logger . debug ( String . format ( Locale . ROOT , "Score %s" , score ) ) ; } }
@ BeforeRun public void beforeRun ( ThreadState state ) { state . threadId = ( int ) threadIdGenerator . getAndIncrement ( ) ; state . totalThreadCount = ( int ) totalThreadCount . get ( ) ; logger . info ( "threadId: " + state . threadId ) ; logger . info ( "totalThreadCount: " + totalThreadCount ) ; }
@ BeforeRun public void beforeRun ( ThreadState state ) { state . threadId = ( int ) threadIdGenerator . getAndIncrement ( ) ; logger . info ( "threadId: " + threadIdGenerator . getAndIncrement ( ) ) ; state . totalThreadCount = ( int ) totalThreadCount . get ( ) ; logger . info ( "totalThreadCount: " + state . totalThreadCount ) ; }
public void test() { -> { caseManagementServiceBase . reopenCase ( caseId , containerId , caseDefId , payload , type ) ; logger . debug ( "Opening case {} with payload {}" , caseId , payload ) ; return createResponse ( "" , v , Response . Status . CREATED , customHeaders ) ; } }
public void test() { try { URL serviceRoot = new URL ( serviceRootUrl ) ; List < String > genEndpoints = new ArrayList < > ( ) ; genEndpoints . add ( "mqtt://" + serviceRoot . getHost ( ) + ":" + getPort ( ) ) ; endpoints = Collections . unmodifiableList ( genEndpoints ) ; LOGGER . info ( endpoints ) ; LOGGER . info ( "Please set " + PREFIX_MQTT + TAG_EXPOSED_MQTT_ENDPOINTS + " to set the correct MQTT end points." ) ; } catch ( MalformedURLException ex ) { LOGGER . error ( "Failed to create MQTT urls." , ex ) ; } }
public void test() { try { LOGGER . info ( "Generating MQTT endpoint list" ) ; URL serviceRoot = new URL ( serviceRootUrl ) ; List < String > genEndpoints = new ArrayList < > ( ) ; genEndpoints . add ( "mqtt://" + serviceRoot . getHost ( ) + ":" + getPort ( ) ) ; endpoints = Collections . unmodifiableList ( genEndpoints ) ; LOGGER . info ( "Generated MQTT endpoint list: {}" , endpoints ) ; } catch ( MalformedURLException ex ) { LOGGER . error ( "Failed to create MQTT urls." , ex ) ; } }
public void test() { try { URL serviceRoot = new URL ( serviceRootUrl ) ; List < String > genEndpoints = new ArrayList < > ( ) ; genEndpoints . add ( "mqtt://" + serviceRoot . getHost ( ) + ":" + getPort ( ) ) ; endpoints = Collections . unmodifiableList ( genEndpoints ) ; LOGGER . info ( "Generated MQTT endpoint list: {}" , endpoints ) ; LOGGER . info ( "Please set " + PREFIX_MQTT + TAG_EXPOSED_MQTT_ENDPOINTS + " to set the correct MQTT end points." ) ; } catch ( MalformedURLException ex ) { LOGGER . error ( "Failed to parse service root URL: {}" , ex . getMessage ( ) ) ; } }
public void test() { try { java . util . List < com . liferay . document . library . kernel . model . DLFileEntry > returnValue = DLFileEntryServiceUtil . getFileEntries ( groupId , folderId , mimeTypes , status , start , end , orderByComparator ) ; return com . liferay . document . library . kernel . model . DLFileEntrySoap . toSoapModels ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( ! accepted ) { _log . error ( "Media package " + mediaPackage . getId ( ) + " for media package " + mediaPackage . getId ( ) ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getCapabilities()" ) ; } }
public void test() { try { wsl . onWebsocketHandshakeSentAsClient ( this , this . handshakerequest ) ; } catch ( InvalidDataException e ) { LOG . debug ( "Handshake invalid by client." , e ) ; throw new InvalidHandshakeException ( "Handshake data rejected by client." ) ; } catch ( RuntimeException e ) { wsl . onWebsocketError ( this , e ) ; throw new InvalidHandshakeException ( "rejected because of " + e ) ; } }
public void test() { if ( session != null ) { session . removeAttribute ( ActionContext . SESSION_ATTRIBUTE_PREFIX . concat ( sources [ 0 ] . toString ( ) ) ) ; } else { LOG . warn ( "Unable to remove Session" ) ; } }
public void test() { try { accountServlet = new AccountServlet ( httpService , this . getThing ( ) . getUID ( ) . getId ( ) , this , gson ) ; } catch ( IllegalStateException e ) { logger . warn ( "Could not create account servlet" ) ; } }
public void test() { if ( dto != null ) { logger . info ( "Try to add crisisType " + dto . getCrisisTypeId ( ) + ":" + dto . getCrisisTypeId ( ) ) ; Integer ret = remoteCrisisTypeEJB . deleteCrisisType ( dto . getCrisisTypeId ( ) ) ; if ( ret != null && ret . intValue ( ) == 1 ) return Response . ok ( "CrisisType Add-Delete test successful" + dto ) . build ( ) ; } }
public void test() { try { CrisisTypeDTO crisisType = new CrisisTypeDTO ( "test_db-manager_crisisType" ) ; CrisisTypeDTO dto = remoteCrisisTypeEJB . addCrisisType ( crisisType ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Warning: " + e . getMessage ( ) ) ; return Response . ok ( "Exception: " + e ) . build ( ) ; } }
public void test() { if ( oslpDevice == null ) { logger . info ( "Device not configured. Skipping OSLP device." ) ; return ; } }
public void addSession ( Session session ) { LOGGER . debug ( "Adding session {}" , session . getId ( ) ) ; this . availableSessionsById . put ( session . getId ( ) , session ) ; final KieServerMessageHandler messageHandler = new KieServerMessageHandler ( session ) ; this . handlersPerSession . put ( session . getId ( ) , messageHandler ) ; session . addMessageHandler ( messageHandler ) ; }
public void test() { try { commentDocModel = ( DocumentModel ) relationManager . getResourceRepresentation ( config . commentNamespace , subject , ctxMap ) ; } catch ( Exception e ) { logger . error ( "" , e ) ; } }
public void test() { while ( startBatch < textFlowsToTranslate . size ( ) ) { int batchEnd = Math . min ( startBatch + REQUEST_BATCH_SIZE , textFlowsToTranslate . size ( ) ) ; List < HTextFlow > textFlowBatch = textFlowsToTranslate . subList ( startBatch , batchEnd ) ; log . debug ( "[PERF] Sending textFlow {}" , textFlowBatch ) ; MTDocument mtDocument = textFlowsToMTDoc . fromTextFlows ( projectSlug , versionSlug , doc . getDocId ( ) , doc . getSourceLocaleId ( ) , textFlowBatch , TextFlowsToMTDoc :: extractPluralIfPresent , backendId ) ; log . debug ( "[PERF] Sending batch {} - {}" , startBatch , batchEnd ) ; Stopwatch mtProviderStopwatch = Stopwatch . createStarted ( ) ; MTDocument result = getTranslationFromMT ( mtDocument , targetLocale . getLocaleId ( ) ) ; log . debug ( "[PERF] Received response [{} contents] ({}ms)" , result . getContents ( ) . size ( ) , mtProviderStopwatch ) ; saveTranslationsInBatches ( textFlowBatch , result , targetLocale , saveState ) ; backendIdConfirmation = result . getBackendId ( ) ; startBatch = batchEnd ; taskHandle . increaseProgress ( textFlowBatch . size ( ) ) ; } }
public void test() { while ( startBatch < textFlowsToTranslate . size ( ) ) { int batchEnd = Math . min ( startBatch + REQUEST_BATCH_SIZE , textFlowsToTranslate . size ( ) ) ; log . debug ( "[PERF] Starting batch {} - {}" , startBatch , batchEnd ) ; List < HTextFlow > textFlowBatch = textFlowsToTranslate . subList ( startBatch , batchEnd ) ; MTDocument mtDocument = textFlowsToMTDoc . fromTextFlows ( projectSlug , versionSlug , doc . getDocId ( ) , doc . getSourceLocaleId ( ) , textFlowBatch , TextFlowsToMTDoc :: extractPluralIfPresent , backendId ) ; log . debug ( "[PERF] Sending batch {} - {}" , startBatch , batchEnd ) ; Stopwatch mtProviderStopwatch = Stopwatch . createStarted ( ) ; MTDocument result = getTranslationFromMT ( mtDocument , targetLocale . getLocaleId ( ) ) ; saveTranslationsInBatches ( textFlowBatch , result , targetLocale , saveState ) ; backendIdConfirmation = result . getBackendId ( ) ; startBatch = batchEnd ; taskHandle . increaseProgress ( textFlowBatch . size ( ) ) ; log . debug ( "[PERF] TextFlowBatch {} took {} millis" , startBatch , textFlowBatch . size ( ) ) ; } }
public void test() { if ( backendIdConfirmation == null ) { getLogger ( ) . warn ( "The backend ID confirmation will not be used." ) ; } }
public void test() { try { int aEventId = eventManager . createAsync ( 60 ) ; LOGGER . info ( "Async Event [{}] has been started." , aEventId ) ; eventManager . start ( aEventId ) ; LOGGER . info ( "Sync Event [{}] has been started." , sEventId ) ; int sEventId = eventManager . create ( 60 ) ; LOGGER . info ( "Sync Event [{}] has been created." , sEventId ) ; eventManager . start ( sEventId ) ; LOGGER . info ( "Sync Event [{}] has been started." , sEventId ) ; eventManager . status ( aEventId ) ; eventManager . status ( sEventId ) ; eventManager . cancel ( aEventId ) ; LOGGER . info ( "Async Event [{}] has been stopped." , aEventId ) ; eventManager . cancel ( sEventId ) ; LOGGER . info ( "Sync Event [{}] has been stopped." , sEventId ) ; } catch ( MaxNumOfEventsAllowedException | LongRunningEventException | EventDoesNotExistException | InvalidOperationException e ) { LOGGER . error ( e . getMessage ( ) ) ; } }
public void test() { try { int aEventId = eventManager . createAsync ( 60 ) ; LOGGER . info ( "Async Event [{}] has been created." , aEventId ) ; eventManager . start ( aEventId ) ; int sEventId = eventManager . create ( 60 ) ; LOGGER . info ( "Sync Event [{}] has been created." , sEventId ) ; eventManager . start ( sEventId ) ; LOGGER . info ( "Sync Event [{}] has been started." , sEventId ) ; eventManager . status ( aEventId ) ; eventManager . status ( sEventId ) ; LOGGER . info ( "Cance Event [{}] has been started." , sEventId ) ; eventManager . cancel ( aEventId ) ; LOGGER . info ( "Async Event [{}] has been stopped." , aEventId ) ; eventManager . cancel ( sEventId ) ; LOGGER . info ( "Sync Event [{}] has been stopped." , sEventId ) ; } catch ( MaxNumOfEventsAllowedException | LongRunningEventException | EventDoesNotExistException | InvalidOperationException e ) { LOGGER . error ( e . getMessage ( ) ) ; } }
public void test() { try { int aEventId = eventManager . createAsync ( 60 ) ; LOGGER . info ( "Async Event [{}] has been created." , aEventId ) ; eventManager . start ( aEventId ) ; LOGGER . info ( "Async Event [{}] has been started." , aEventId ) ; int sEventId = eventManager . create ( 60 ) ; eventManager . start ( sEventId ) ; LOGGER . info ( "Sync Event [{}] has been started." , sEventId ) ; eventManager . status ( aEventId ) ; eventManager . status ( sEventId ) ; LOGGER . info ( "Async Event [{}] has been cancelled." , sEventId ) ; eventManager . cancel ( aEventId ) ; LOGGER . info ( "Async Event [{}] has been stopped." , aEventId ) ; eventManager . cancel ( sEventId ) ; LOGGER . info ( "Sync Event [{}] has been stopped." , sEventId ) ; } catch ( MaxNumOfEventsAllowedException | LongRunningEventException | EventDoesNotExistException | InvalidOperationException e ) { LOGGER . error ( e . getMessage ( ) ) ; } }
public void test() { try { int aEventId = eventManager . createAsync ( 60 ) ; LOGGER . info ( "Async Event [{}] has been created." , aEventId ) ; eventManager . start ( aEventId ) ; LOGGER . info ( "Async Event [{}] has been started." , aEventId ) ; int sEventId = eventManager . create ( 60 ) ; LOGGER . info ( "Sync Event [{}] has been created." , sEventId ) ; eventManager . start ( sEventId ) ; eventManager . status ( aEventId ) ; eventManager . status ( sEventId ) ; LOGGER . info ( "Async Event [{}] has been stopped." , aEventId ) ; eventManager . cancel ( aEventId ) ; LOGGER . info ( "Async Event [{}] has been stopped." , aEventId ) ; eventManager . cancel ( sEventId ) ; LOGGER . info ( "Sync Event [{}] has been stopped." , sEventId ) ; } catch ( MaxNumOfEventsAllowedException | LongRunningEventException | EventDoesNotExistException | InvalidOperationException e ) { LOGGER . error ( e . getMessage ( ) ) ; } }
public void test() { try { int aEventId = eventManager . createAsync ( 60 ) ; LOGGER . info ( "Async Event [{}] has been created." , aEventId ) ; eventManager . start ( aEventId ) ; LOGGER . info ( "Async Event [{}] has been started." , aEventId ) ; int sEventId = eventManager . create ( 60 ) ; LOGGER . info ( "Sync Event [{}] has been created." , sEventId ) ; eventManager . start ( sEventId ) ; LOGGER . info ( "Sync Event [{}] has been started." , sEventId ) ; eventManager . status ( aEventId ) ; eventManager . status ( sEventId ) ; LOGGER . info ( "Sync Event [{}] has been stopped." , aEventId ) ; eventManager . cancel ( aEventId ) ; eventManager . cancel ( sEventId ) ; LOGGER . info ( "Sync Event [{}] has been stopped." , sEventId ) ; } catch ( MaxNumOfEventsAllowedException | LongRunningEventException | EventDoesNotExistException | InvalidOperationException e ) { LOGGER . error ( e . getMessage ( ) ) ; } }
public void test() { try { int aEventId = eventManager . createAsync ( 60 ) ; LOGGER . info ( "Async Event [{}] has been created." , aEventId ) ; eventManager . start ( aEventId ) ; LOGGER . info ( "Async Event [{}] has been started." , aEventId ) ; int sEventId = eventManager . create ( 60 ) ; LOGGER . info ( "Sync Event [{}] has been created." , sEventId ) ; eventManager . start ( sEventId ) ; LOGGER . info ( "Sync Event [{}] has been started." , sEventId ) ; eventManager . status ( sEventId ) ; eventManager . status ( sEventId ) ; LOGGER . info ( "Async Event [{}] has been stopped." , aEventId ) ; eventManager . cancel ( aEventId ) ; LOGGER . info ( "Async Event [{}] has been stopped." , aEventId ) ; eventManager . cancel ( sEventId ) ; } catch ( MaxNumOfEventsAllowedException | LongRunningEventException | EventDoesNotExistException | InvalidOperationException e ) { LOGGER . error ( e . getMessage ( ) ) ; } }
public void test() { try { int aEventId = eventManager . createAsync ( 60 ) ; LOGGER . info ( "Async Event [{}] has been created." , aEventId ) ; eventManager . start ( aEventId ) ; LOGGER . info ( "Async Event [{}] has been started." , aEventId ) ; int sEventId = eventManager . create ( 60 ) ; LOGGER . info ( "Sync Event [{}] has been created." , sEventId ) ; eventManager . start ( sEventId ) ; LOGGER . info ( "Sync Event [{}] has been started." , sEventId ) ; eventManager . status ( aEventId ) ; eventManager . status ( sEventId ) ; eventManager . cancel ( aEventId ) ; LOGGER . info ( "Async Event [{}] has been stopped." , aEventId ) ; eventManager . cancel ( sEventId ) ; LOGGER . info ( "Sync Event [{}] has been stopped." , sEventId ) ; } catch ( MaxNumOfEventsAllowedException | LongRunningEventException | EventDoesNotExistException | InvalidOperationException e ) { LOGGER . warn ( "MaxNumOfEventsAllowedException." , e ) ; } }
public void test() { try { return getRedisKeyValueState ( namespace , topoConf , context , getStateConfig ( topoConf ) ) ; } catch ( Exception ex ) { LOG . error ( "load redis key value failed" , ex ) ; throw new RuntimeException ( ex ) ; } }
public void test() { try { String res = results . get ( Replica . getReplicaFromId ( replicaId ) ) ; KeyValuePair < String , String > kvp = ChecksumJob . parseLine ( res ) ; code_block = IfStatement ; log . warn ( "Found unexpected file '" + kvp . getKey ( ) + "' while asking replica '" + Replica . getReplicaFromId ( replicaId ) + "' for file '" + filename + "'" ) ; } catch ( ArgumentNotValid e ) { log . warn ( "Argument not valid" , e ) ; } }
public void test() { try { Object o = PentahoSystem . get ( ISolutionEngine . class ) ; code_block = IfStatement ; } catch ( Throwable e ) { Logger . log ( e ) ; } }
public void test() { if ( runtime > 2000 ) { LOGGER . warn ( "Runtime info: {}" , runtime ) ; } }
@ Override public void kill ( ) { getLogger ( ) . info ( "Attempting to kill Hadoop job " + this . jobId ) ; final Configuration conf = createConfiguration ( ) ; code_block = TryStatement ;  getLogger ( ) . info ( "Hadoop job " + this . jobId + " killed" ) ; }
public void test() { try { @ SuppressWarnings ( "unchecked" ) List < UserDefinedCalendarConfiguration > configurations = ( List < UserDefinedCalendarConfiguration > ) getHibernateTemplate ( ) . find ( "from CalendarConfiguration config where " + "subscribeId = ? and displayed = true " + "order by calendarDefinition.name" , subscribeId ) ; return configurations ; } catch ( HibernateException ex ) { logger . error ( ex . getMessage ( ) , ex ) ; throw convertHibernateAccessException ( ex ) ; } }
public void test() { try { blurIndexes = _indexServer . getIndexes ( table ) ; } catch ( IOException e ) { LOG . error ( "Unknown error while trying to get indexes for table [" + table + "]" , e ) ; throw new BException ( e . getMessage ( ) , e ) ; } }
public CreateProjectPage enterDescription ( String projectDescription ) { log . info ( "Enter description {}" , projectDescription ) ; enterText ( readyElement ( descriptionField ) , projectDescription ) ; return new CreateProjectPage ( getDriver ( ) ) ; }
public void test() { if ( parentSysmlId == null || childSysmlId == null || parentSysmlId . isEmpty ( ) || childSysmlId . isEmpty ( ) ) { logger . warn ( "parentSysmlId: " + parentSysmlId ) ; logger . warn ( "parentSysmlId: " + childSysmlId ) ; logger . warn ( "childSysmlId: " + childSysmlId ) ; return ; } }
public void test() { if ( parentSysmlId == null || childSysmlId == null || parentSysmlId . isEmpty ( ) || childSysmlId . isEmpty ( ) ) { logger . warn ( "Parent or child not found" ) ; logger . warn ( "parentSysmlId: " + parentSysmlId ) ; logger . warn ( "parentSysmlId: " + parentSysmlId ) ; return ; } }
public void test() { if ( parentSysmlId == null || childSysmlId == null || parentSysmlId . isEmpty ( ) || childSysmlId . isEmpty ( ) ) { logger . warn ( "Parent or child not found" ) ; logger . warn ( "parentSysmlId: " + childSysmlId ) ; logger . warn ( "childSysmlId: " + childSysmlId ) ; return ; } }
public void test() { if ( e . getMessage ( ) . contains ( "duplicate key" ) ) { logger . warn ( String . format ( "%s already loaded %s" , LogUtil . getStackTrace ( e ) ) ) ; } else { logger . warn ( String . format ( "%s" , LogUtil . getStackTrace ( e ) ) ) ; } }
public void test() { if ( e . getMessage ( ) . contains ( "duplicate key" ) ) { logger . info ( String . format ( "%s" , LogUtil . getStackTrace ( e ) ) ) ; } else { logger . error ( String . format ( "%s: %s" , LogUtil . getStackTrace ( e ) ) ) ; } }
public void test() { try { File dataExample = new File ( Info . PATHS . PATH_WORKDIR . getParentFile ( ) . getParentFile ( ) + "/data-example" ) ; copy ( dataExample . getAbsolutePath ( ) , Info . PATHS . PATH_DATA_FOLDER . getAbsolutePath ( ) ) ; canPerformTest = true ; } catch ( IOException e ) { LOG . warn ( "Could not copy data example: " + e . getMessage ( ) ) ; canPerformTest = false ; } }
@ Test ( enabled = false ) public void testPresenceSolutionBlind ( ) { log . info ( "=== TEST for SOLUTION GENERATION of BLIND optimizer STARTED ===" ) ; optimizer = new Optimizer ( NUM_PLANS_TO_GENERATE , SearchMethodName . BLINDSEARCH ) ; String [ ] arrayDam = optimizer . optimize ( appModel , suitableCloudOffer ) ; code_block = ForStatement ; log . info ( "=== TEST for SOLUTION GENERATION of BLIND optimizer FINISEHD ===" ) ; }
public void test() { try { checkCorrectness ( arrayDam [ damnum ] ) ; } catch ( Exception e ) { log . error ( "There was an error in the check of correctness. Solution was: " + arrayDam [ damnum ] ) ; throw e ; } }
@ Test ( enabled = false ) public void testPresenceSolutionBlind ( ) { log . info ( "=== TEST for SOLUTION GENERATION of BLIND optimizer STARTED (syntax July 2015)===" ) ; optimizer = new Optimizer ( NUM_PLANS_TO_GENERATE , SearchMethodName . BLINDSEARCH ) ; String [ ] arrayDam = optimizer . optimize ( appModel , suitableCloudOffer ) ; code_block = ForStatement ; log . info ( "=== TEST for SOLUTION GENERATION of BLIND optimizer FINISEHD ====" ) ; }
public void test() { try { String value = this . extractValue ( ) ; code_block = IfStatement ; IParameterParentTag parentTag = ( IParameterParentTag ) findAncestorWithClass ( this , IParameterParentTag . class ) ; parentTag . addParameter ( this . getName ( ) , value ) ; } catch ( Throwable t ) { _logger . error ( "Error closing tag" , t ) ; throw new JspException ( "Error closing tag " , t ) ; } }
public void test() { try { validator . validate ( context , reader ) ; } catch ( ObjectValidityException e ) { LOG . error ( "Invalid validation" , e ) ; throw e ; } }
public void test() { try { fileSystem = dir . getFileSystem ( hadoopConf ) ; res = fileSystem . exists ( dir ) ; } catch ( IOException e ) { LOG . warn ( "IOException checking path: {}" , dir , e ) ; } }
public void test() { try { GridCompoundFuture < SchemaIndexCacheStat , SchemaIndexCacheStat > compoundFut = ( GridCompoundFuture < SchemaIndexCacheStat , SchemaIndexCacheStat > ) fut ; SchemaIndexCacheStat resStat = new SchemaIndexCacheStat ( ) ; compoundFut . futures ( ) . stream ( ) . map ( IgniteInternalFuture :: result ) . filter ( Objects :: nonNull ) . forEach ( resStat :: accumulate ) ; log . info ( indexStatStr ( resStat ) ) ; } catch ( Exception e ) { log . error ( "" , e ) ; } }
public void test() { try { doOperations ( ) ; periodicRemoveObsoletes ( ) ; code_block = TryStatement ;  } catch ( Exception e ) { LOGGER . warn ( "Unable to remove obsolete files" , e ) ; } }
public void debugFreeList ( ) { logger . debug ( "Free list: " + freeList ) ; }
public void test() { try { Event event = serializer . deserialize ( message ) ; return Optional . of ( event ) ; } catch ( Exception e ) { LOG . warn ( "Could not deserialize event {}" , message , e ) ; return Optional . empty ( ) ; } }
public void test() { try { Class . forName ( getChosenSQLDriver ( ) ) ; } catch ( ClassNotFoundException e ) { log . error ( "Selected SQL driver does not exist: " + getChosenSQLDriver ( ) ) ; } }
@ Override public void startFolderEvent ( Session session , FileOperation op , Path file , Set < PosixFilePermission > perms ) throws IOException { log . debug ( "startFolderEvent()" ) ; }
public void test() { try { ProcessBuilder processBuilder = new ProcessBuilder ( command ) . directory ( appCDSDir . toFile ( ) ) ; code_block = IfStatement ; exitCode = processBuilder . start ( ) . waitFor ( ) ; } catch ( Exception e ) { LOGGER . error ( "Failed to run {}" , command , e ) ; return null ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
private RestResponse < CustomGroup > groups ( ) throws CatalogException , ClientException { log . debug ( "Getting groups" ) ; studiesCommandOptions . groupsCommandOptions . study = getSingleValidStudy ( studiesCommandOptions . groupsCommandOptions . study ) ; ObjectMap params = new ObjectMap ( ) ; params . putIfNotNull ( "id" , studiesCommandOptions . groupsCommandOptions . group ) ; return openCGAClient . getStudyClient ( ) . groups ( studiesCommandOptions . groupsCommandOptions . study , params ) ; }
protected void deactivate ( ComponentContext componentContext ) { logger . info ( "Deactivating {}" , this . getClass ( ) . getSimpleName ( ) ) ; code_block = IfStatement ; this . dataService . removeDataServiceListener ( this ) ; this . cloudClients . clear ( ) ; this . dataService = null ; this . systemAdminService = null ; this . networkService = null ; this . positionService = null ; this . eventAdmin = null ; this . certificatesService = null ; this . cloudServiceRegistration . unregister ( ) ; this . notificationPublisherRegistration . unregister ( ) ; }
public void test() { try { publishDisconnectCertificate ( ) ; } catch ( KuraException e ) { logger . warn ( "Failed to publish DisconnectCertificate" , e ) ; } }
public void test() { if ( include . isResource ( ) ) { LOG . info ( "Loading includes from resource: {}" , include . getFile ( ) ) ; includeTopologyDef = parseResource ( include . getFile ( ) , true , false , properties , envSub ) ; } else { LOG . info ( "Loading includes from file: {}" , include . getFile ( ) ) ; includeTopologyDef = parseFile ( include . getFile ( ) , true , false , properties , envSub ) ; } }
public void test() { if ( include . isResource ( ) ) { LOG . info ( "Loading includes from resource: {}" , include . getFile ( ) ) ; includeTopologyDef = parseResource ( include . getFile ( ) , true , false , properties , envSub ) ; } else { LOG . info ( "Loading includes from file: {}" , include . getFile ( ) ) ; includeTopologyDef = parseFile ( include . getFile ( ) , true , false , properties , envSub ) ; } }
public void test() { if ( config . containsKey ( key ) ) { LOGGER . warn ( "Duplicate config found: " + key ) ; } else { config . put ( key , includeConfig . get ( key ) ) ; } }
@ Override public void start ( final Map < String , String > props ) { config = new JdbcSinkConfig ( props ) ; initWriter ( ) ; remainingRetries = config . maxRetries ; code_block = TryStatement ;  LOG . info ( "JdbcSinkConfig initialized successfully." ) ; }
public void test() { if ( LOG . isInfoEnabled ( ) ) { StringBuilder entityInfo = new StringBuilder ( "Entity[" ) ; entityInfo . append ( dataDefinition . getPluginIdentifier ( ) ) . append ( '.' ) . append ( dataDefinition . getName ( ) ) ; entityInfo . append ( "][id=" ) . append ( entityId ) . append ( "] " ) ; entityInfo . append ( message ) ; LOG . info ( entityInfo . toString ( ) ) ; } }
public void test() { if ( verboseLogs ) { log . info ( "Waiting for contexts to complete" ) ; } }
public boolean evaluateState ( VeluxBridge bridge ) { logger . trace ( "evaluateState() called." ) ; boolean success = false ; GetHouseStatus bcp = bridge . bridgeAPI ( ) . getHouseStatus ( ) ; code_block = IfStatement ; logger . debug ( "evaluateState() finished {}." , ( success ? "successfully" : "with failure" ) ) ; return success ; }
public void test() { try { final CertPath certChain = message . getCertificateChain ( ) ; code_block = IfStatement ; final List < ? extends Certificate > list = certChain . getCertificates ( ) ; final int pathSize = list . size ( ) ; code_block = IfStatement ; code_block = IfStatement ; adapter . runOnContext ( ( v ) -> validateCertificateAndLoadDevice ( cid , certChain , session ) ) ; return null ; } catch ( HandshakeException e ) { log . error ( "Certificate validation failed" , e ) ; return new CertificateVerificationResult ( cid , e , null ) ; } }
public void test() { if ( res . succeeded ( ) && ( res . result ( ) . statusCode ( ) == 200 || res . result ( ) . statusCode ( ) == 404 ) ) { LOGGER . debug ( "Canceling existing job {}" , id ) ; } else { LOGGER . error ( "Canceling of job {} failed with response code {}" , id , res . result ( ) . statusCode ( ) , res . cause ( ) ) ; } }
public void test() { if ( res . succeeded ( ) && ( res . result ( ) . statusCode ( ) == 200 || res . result ( ) . statusCode ( ) == 404 ) ) { LOGGER . debug ( "Canceling of the job {} done with status code {} " , id , res . result ( ) . statusCode ( ) ) ; } else { LOGGER . warn ( "Cannot canceling of the job {} due to {}" , id , res . result ( ) . statusCode ( ) ) ; } }
public void test() { for ( Entry < Path , String > outputDataEntry : outputDataByPath . entrySet ( ) ) { Path outputPath = outputDataEntry . getKey ( ) ; outputPath . getParent ( ) . toFile ( ) . mkdirs ( ) ; MoreFiles . asCharSink ( outputPath , UTF_8 ) . write ( outputDataEntry . getValue ( ) ) ; logger . info ( "Wrote data to {}" , outputPath ) ; } }
@ Override public void transportInterupted ( ) { log . info ( "Transport interrupted" ) ; interruptedCount . incrementAndGet ( ) ; }
public void test() { try { ObjectMapper objectMapper = new ObjectMapper ( ) ; fieldValues = objectMapper . readValue ( partitionfieldValuesStr , Map . class ) ; code_block = ForStatement ; return fieldValues ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; throw new RuntimeException ( e ) ; } }
public void test() { if ( ! elementFile . delete ( ) ) { log . warn ( "Unable to delete temporary file: " + elementFile ) ; } }
public void test() { if ( elementFile . exists ( ) ) { code_block = IfStatement ; } else { LOGGER . error ( "File " + elementName + " does not exist." ) ; } }
public void test() { if ( ! elementDir . delete ( ) ) { LOG . warn ( "Unable to delete temporary directory " + elementDir ) ; } }
public void test() { if ( elementDir . list ( ) . length == 0 ) { code_block = IfStatement ; } else { LOGGER . info ( "Directory doesn't exist " + elementDir . getAbsolutePath ( ) ) ; } }
public void test() { if ( elementDir != null && elementDir . exists ( ) && elementDir . list ( ) != null ) { code_block = IfStatement ; } else { LOG . warn ( "Element dir {} does not exist" , elementDir . getAbsolutePath ( ) ) ; } }
public void test() { if ( ! mediapackageDir . delete ( ) ) { logger . warn ( "Unable to delete mediapackage directory" ) ; } }
public void test() { if ( mediapackageDir . list ( ) . length == 0 ) { code_block = IfStatement ; } else { logger . info ( "No media package found in mediapackage directory" ) ; } }
public void test() { if ( mediapackageDir != null && mediapackageDir . exists ( ) ) { code_block = IfStatement ; } else { logger . debug ( "No mediapackage directory found" ) ; } }
public StepPhase stageApp ( CloudApplication app ) { logger . info ( "Build cloud " + app . getName ( ) ) ; CloudPackage cloudPackage = context . getVariable ( Variables . CLOUD_PACKAGE ) ; code_block = IfStatement ; return createBuild ( cloudPackage . getGuid ( ) ) ; }
public void test() { if ( encryptedData . isIntegrityProtected ( ) ) { code_block = IfStatement ; } else { LOGGER . debug ( "{} is not the encrypted data" , logPrefix ( ) ) ; } }
public void test() { if ( ! isAttached ( device ) ) { log . warn ( "Device {} is not attached" , device ) ; return ; } }
public void test() { if ( message == null ) { LOG . info ( "message is null - no patient" ) ; return ; } }
public void test() { try { Files . delete ( file ) ; } catch ( Exception e ) { log . error ( e , e ) ; } }
public void test() { try { jid = JavaId . getJavaId ( type , qname ) ; def_ctx = JavaId . getCompilationUnit ( jid ) ; log . debug ( "Determined compilation unit " + def_ctx + " for qname [" + qname + "]" ) ; file = ClassDownloader . getInstance ( ) . getClass ( mvnGroup , artifact , version , def_ctx . getQualifiedName ( ) , ClassDownloader . Format . JAVA ) ; code_block = IfStatement ; } catch ( IllegalArgumentException iae ) { log . error ( iae . getMessage ( ) , iae ) ; throw new RuntimeException ( iae . getMessage ( ) ) ; } catch ( FileNotFoundException e ) { log . error ( "Error: " + e . getMessage ( ) , e ) ; throw new RuntimeException ( "Cannot read file [" + file + "]" ) ; } catch ( IOException e ) { log . error ( "Error: " + e . getMessage ( ) , e ) ; throw new RuntimeException ( "IO exception when reading file [" + file + "]" ) ; } catch ( Exception e ) { log . error ( "Error: " + e . getMessage ( ) , e ) ; throw new RuntimeException ( e . getClass ( ) . getSimpleName ( ) + " when writing file to output stream: " + e . getMessage ( ) ) ; } }
public void test() { try ( Connection con = getDatasource ( ) . getConnection ( ) ; PreparedStatement stmt = con . prepareStatement ( query ) ; ) { ResultSet rs = stmt . executeQuery ( ) ; code_block = WhileStatement ; } catch ( SQLException e ) { String message = Messages . get ( locale , "error_db_seq" , seqName ) ; logger . error ( message , e ) ; throw new Exception ( message ) ; } }
public void test() { if ( srcContentSummary . getFileCount ( ) > conf . getLongVar ( HiveConf . ConfVars . HIVE_EXEC_COPYFILE_MAXNUMFILES ) && srcContentSummary . getLength ( ) > conf . getLongVar ( HiveConf . ConfVars . HIVE_EXEC_COPYFILE_MAXSIZE ) ) { LOG . info ( "Skipping " + srcContentSummary . getFileCount ( ) + " bytes." ) ; LOG . info ( "Source is " + srcContentSummary . getLength ( ) + " bytes. (MAX: " + conf . getLongVar ( HiveConf . ConfVars . HIVE_EXEC_COPYFILE_MAXSIZE ) + ")" ) ; LOG . info ( "Source is " + srcContentSummary . getFileCount ( ) + " files. (MAX: " + conf . getLongVar ( HiveConf . ConfVars . HIVE_EXEC_COPYFILE_MAXNUMFILES ) + ")" ) ; triedDistcp = true ; copied = distCp ( srcFS , Collections . singletonList ( src ) , dst , deleteSource , null , conf , shims ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Failed to clean up." , e ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( SQLException ex ) { LOGGER . error ( ex . toString ( ) ) ; } }
public void test() { if ( trimmedPassword != null && ! trimmedPassword . equals ( trimmedPassword2 ) ) { addFieldError ( "password2" , getText ( "validation.password2.wrong" ) ) ; password2 = null ; LOG . error ( "Password 2 not changed" ) ; } else-if ( trimmedPassword == null ) { addFieldError ( "user.password" , getText ( "validation.password.reentered" ) ) ; LOG . error ( "The primary password entered is empty" ) ; } else { user . setPassword ( trimmedPassword ) ; LOG . error ( "The password has been reset" ) ; } }
public void test() { if ( trimmedPassword != null && ! trimmedPassword . equals ( trimmedPassword2 ) ) { addFieldError ( "password2" , getText ( "validation.password2.wrong" ) ) ; LOG . error ( "The passwords entered do not match" ) ; password2 = null ; } else-if ( trimmedPassword == null ) { addFieldError ( "user.password" , getText ( "validation.password.reentered" ) ) ; LOG . error ( "The password was null" ) ; } else { user . setPassword ( trimmedPassword ) ; LOG . error ( "The password has been reset" ) ; } }
public void test() { if ( WARN_ON_ERROR ) { log . warn ( getMarker ( ) + ": numErrors=" + e . getMessage ( ) ) ; numErrors ++ ; } else { throw e ; } }
public void test() { try { this . saveLevelDat ( ) ; } catch ( IOException cause ) { log . error ( "Error saving level dat" , cause ) ; } }
public void test() { if ( tp == null ) { continue ; } }
public final Response putConnection ( final ComponentConnection body ) { log . debug ( "" ) ; code_block = IfStatement ; String path = String . format ( CONNECTION_PATH , body . getObjectId ( ) ) ; return putObjectToSystemMng ( path , body ) ; }
public void test() { try { code_block = IfStatement ; } catch ( IllegalArgumentException e ) { logger . log ( Level . INFO , "Invalid value for channel: {0}" , channelUID ) ; } }
public void test() { try { instants = metaClient . scanHoodieInstantsFromFileSystem ( new Path ( metaClient . getMetaAuxiliaryPath ( ) ) , HoodieActiveTimeline . VALID_EXTENSIONS_IN_ACTIVE_TIMELINE , false ) ; } catch ( FileNotFoundException e ) { LOG . info ( e . getMessage ( ) ) ; return success ; } }
public void test() { try { result = read ( attribute ) . get ( ) ; } catch ( InterruptedException e ) { logger . debug ( "readSync interrupted" ) ; return null ; } catch ( ExecutionException e ) { logger . debug ( "readSync exception " , e ) ; return null ; } }
public void test() { try { result = read ( attribute ) . get ( ) ; } catch ( InterruptedException e ) { logger . debug ( "readSync interrupted" ) ; return null ; } catch ( ExecutionException e ) { logger . debug ( "readSync exception" , e ) ; return null ; } }
@ EventListener ( ApplicationReadyEvent . class ) @ Order ( value = 2 ) public void onApplicationEvent ( ApplicationReadyEvent event ) { LOG . info ( "Received ApplicationReadyEvent" ) ; this . nfConsumer . subscribe ( ) ; launchNotificationsConsumer ( ) ; launchMainConsumers ( ) ; }
public void test() { if ( ! subscriptionStates . containsKey ( subscriptionId ) || ! broadcastSubscriptionListenerDirectory . containsKey ( subscriptionId ) ) { log . warn ( "No subscription state found for subscriptionId: {}" , subscriptionId ) ; } }
public void test() { try { dispatchMessageQueue . put ( new ServerEvent ( EventType . SERVER_FAILED , server . psLoc ) ) ; } catch ( Exception e ) { LOGGER . error ( e . getMessage ( ) , e ) ; } }
private boolean updateTextSections ( ) { XTextSectionsSupplier supp = UNO . XTextSectionsSupplier ( doc ) ; code_block = IfStatement ; long startTime = System . currentTimeMillis ( ) ; HashSet < String > knownTextSections = new HashSet < > ( ) ; HashSet < TextSection > invalidTextSections = new HashSet < > ( ) ; code_block = ForStatement ; HashSet < TextSection > newTextSections = new HashSet < > ( ) ; String [ ] textSectionNames = supp . getTextSections ( ) . getElementNames ( ) ; code_block = ForStatement ; removeInvalidTextSections ( invalidTextSections ) ; addNewTextSections ( newTextSections ) ; LOGGER . info ( "Updated " + ( System . currentTimeMillis ( ) - startTime ) + " known text sections in " + ( System . currentTimeMillis ( ) - startTime ) + "ms" ) ; return ! invalidTextSections . isEmpty ( ) || ! newTextSections . isEmpty ( ) ; }
public void test() { try { final List < Elem > choices = new ArrayList < Elem > ( ) ; getContainer ( ) . run ( true , true , new IRunnableWithProgress ( ) code_block = "" ; ) ; if ( ! choices . isEmpty ( ) ) return choices ; MessageDialog . openConfirm ( getShell ( ) , getLocalString ( "_UI_No_modules_found" ) , getLocalString ( "_UI_No_module_matching_keyword" , keyword ) ) ; } catch ( InvocationTargetException e ) { Throwable t = e . getTargetException ( ) ; StringBuilder builder = new StringBuilder ( ) ; builder . append ( t . getClass ( ) . getName ( ) ) ; builder . append ( "\n" ) ; builder . append ( t . getMessage ( ) ) ; builder . append ( "\n\n(See the log view for technical details)." ) ; MessageDialog . openError ( getShell ( ) , "Error while communicating with the ForgeAPI." , builder . toString ( ) ) ; LOGGER . error ( t , "Error while communicating with the ForgeAPI" ) ; } catch ( InterruptedException e ) { } }
public void test() { try { Map < String , String > params = new LinkedHashMap < String , String > ( ) ; code_block = IfStatement ; params . put ( OUTPUT_FORMAT_PARAM , DEFAULT_OUTPUT_FORMAT ) ; HttpResponse response = httpGet ( fullUrl , params ) ; LOGGER . debug ( "Response code: " + response . getResponseCode ( ) + "\n\t" + response . getBody ( ) ) ; checkResponse ( response ) ; code_block = IfStatement ; } catch ( ArcgisException e ) { throw e ; } catch ( Exception e ) { LOGGER . error ( e . getMessage ( ) , e ) ; throw new ArcgisException ( "getTableAttributesInfo, Unexpected Exception " + e . toString ( ) ) ; } }
public void test() { try { Map < String , String > params = new LinkedHashMap < String , String > ( ) ; code_block = IfStatement ; params . put ( OUTPUT_FORMAT_PARAM , DEFAULT_OUTPUT_FORMAT ) ; LOGGER . debug ( "HttpGet " + fullUrl . toString ( ) + " number of params: " + params . size ( ) ) ; HttpResponse response = httpGet ( fullUrl , params ) ; checkResponse ( response ) ; code_block = IfStatement ; } catch ( ArcgisException e ) { throw e ; } catch ( Exception e ) { LOGGER . error ( e . getMessage ( ) , e ) ; throw new ArcgisException ( "getTableAttributesInfo, Unexpected Exception " + e . toString ( ) ) ; } }
public void test() { if ( response . isSuccessful ( ) ) { responseJSON = response . getBody ( ) ; LOGGER . debug ( "    tokenJSON: " + responseJSON ) ; getUniqueIdFieldFromJson ( responseJSON ) ; getAttributeInfoFromJson ( responseJSON ) ; getAttributeIndexFromJson ( responseJSON ) ; } else { String errorMsg = "getTableAttributesInfo: Unexpected server response, Error: " + response . getErrorCode ( ) + "\n" + response . getErrorMessage ( ) ; errorMsg += " \n\t token: " + token ; LOGGER . error ( errorMsg ) ; throw new ArcgisException ( errorMsg ) ; } }
@ PayloadRoot ( localPart = "SetDeviceVerificationKeyRequest" , namespace = DEVICE_MANAGEMENT_NAMESPACE ) @ ResponsePayload public SetDeviceVerificationKeyAsyncResponse setDeviceVerificationKey ( @ OrganisationIdentification final String organisationIdentification , @ RequestPayload final SetDeviceVerificationKeyRequest request , @ MessagePriority final String messagePriority ) throws OsgpException { LOGGER . info ( "Set device verification key for organisation: {} and device: {}." , organisationIdentification , request . getDeviceIdentification ( ) ) ; final SetDeviceVerificationKeyAsyncResponse response = new SetDeviceVerificationKeyAsyncResponse ( ) ; code_block = TryStatement ;  return response ; }
protected Scroll buildScroll ( BulkCommand command ) { ScrollRequest request ; String query = command . getQuery ( ) ; log . debug ( "Build query: {}" , query ) ; code_block = IfStatement ; ScrollService service = Framework . getService ( ScrollService . class ) ; return service . scroll ( request ) ; }
private void executeImpl ( ) { boolean purgePoints = SystemSettingsDao . instance . getBooleanValue ( ENABLE_POINT_DATA_PURGE ) ; this . countPointValues = SystemSettingsDao . instance . getBooleanValue ( SystemSettingsDao . POINT_DATA_PURGE_COUNT ) ; code_block = IfStatement ; filedataPurge ( ) ; log . info ( "Filedata purge started" ) ; if ( deletedFiles > 0 ) log . info ( "Filedata purge ended, " + deletedFiles + " files deleted" ) ; eventPurge ( ) ; for ( PurgeDefinition def : ModuleRegistry . getDefinitions ( PurgeDefinition . class ) ) def . execute ( runtime ) ; }
public void test() { if ( countPointValues ) { log . info ( "Data purge ended, number of point values were deleted" ) ; } else-if ( anyDeletedSamples ) { log . info ( "Data purge ended, unknown number of point values were deleted" ) ; } else { log . info ( "Data purge ended, no point values were deleted" ) ; } }
public void test() { if ( countPointValues ) { log . info ( "Data purge ended, " + deletedSamples + " point values were deleted" ) ; } else-if ( anyDeletedSamples ) { log . info ( "Data purge ended" ) ; } else { log . info ( "Data purge ended, no point values were deleted" ) ; } }
public void test() { if ( countPointValues ) { log . info ( "Data purge ended, " + deletedSamples + " point values were deleted" ) ; } else-if ( anyDeletedSamples ) { log . info ( "Data purge ended, unknown number of point values were deleted" ) ; } else { log . info ( "Data purge ended" ) ; } }
public void test() { if ( purgePoints ) { List < PurgeFilter > purgeFilters = new ArrayList < PurgeFilter > ( ) ; for ( PurgeFilterDefinition pfd : ModuleRegistry . getDefinitions ( PurgeFilterDefinition . class ) ) purgeFilters . add ( pfd . getPurgeFilter ( ) ) ; List < DataPointVO > dataPoints = dataPointDao . getAll ( ) ; for ( DataPointVO dataPoint : dataPoints ) purgePoint ( dataPoint , countPointValues , purgeFilters ) ; code_block = IfStatement ; pointValueDao . deleteOrphanedPointValueAnnotations ( ) ; code_block = IfStatement ; } else { LOGGER . debug ( "Purgepoints disabled" ) ; } }
public void test() { { OuterJoinOperator oper = new OuterJoinOperator ( ) ; oper . setFullJoin ( true ) ; CollectorTestSink sink = new CollectorTestSink ( ) ; oper . outport . setSink ( sink ) ; Condition cond = new JoinColumnEqualCondition ( "a" , "a" ) ; oper . setJoinCondition ( cond ) ; oper . selectTable1Column ( new ColumnIndex ( "b" , null ) ) ; oper . selectTable2Column ( new ColumnIndex ( "c" , null ) ) ; oper . setup ( null ) ; oper . beginWindow ( 1 ) ; HashMap < String , Object > tuple = new HashMap < String , Object > ( ) ; tuple . put ( "a" , 0 ) ; tuple . put ( "b" , 1 ) ; tuple . put ( "c" , 2 ) ; oper . inport1 . process ( tuple ) ; tuple = new HashMap < String , Object > ( ) ; tuple . put ( "a" , 1 ) ; tuple . put ( "b" , 3 ) ; tuple . put ( "c" , 4 ) ; oper . inport1 . process ( tuple ) ; tuple = new HashMap < String , Object > ( ) ; tuple . put ( "a" , 2 ) ; tuple . put ( "b" , 11 ) ; tuple . put ( "c" , 12 ) ; oper . inport1 . process ( tuple ) ; tuple = new HashMap < String , Object > ( ) ; tuple . put ( "a" , 0 ) ; tuple . put ( "b" , 7 ) ; tuple . put ( "c" , 8 ) ; oper . inport2 . process ( tuple ) ; tuple = new HashMap < String , Object > ( ) ; tuple . put ( "a" , 1 ) ; tuple . put ( "b" , 5 ) ; tuple . put ( "c" , 6 ) ; oper . inport2 . process ( tuple ) ; oper . endWindow ( ) ; oper . teardown ( ) ; LOG . debug ( "{}" , sink . collectedTuples ) ; } }
private void severeCannotLoad ( AbstractResource menubarsLayoutXmlResource , Exception cause ) { logger . error ( "error!" , cause ) ; }
public void test() { if ( u == null ) { logger . warn ( "No uuid found!" ) ; return ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( StringUtil . isNotBlank ( u ) ) { code_block = IfStatement ; urlList . add ( u ) ; } else-if ( logger . isDebugEnabled ( ) ) { logger . debug ( "url is null" ) ; } }
public void test() { try { byte [ ] keySrcBytes = encodeUTF8 ( keyString ) ; byte [ ] newKey1 = Base64 . decodeBase64 ( keySrcBytes ) ; Key newKey2 = toKey ( newKey1 ) ; byte [ ] srcBytes = encodeUTF8 ( password ) ; byte [ ] desBytes = decrypt ( Base64 . decodeBase64 ( srcBytes ) , newKey2 ) ; String tempdecodeUTF8 = decodeUTF8 ( desBytes ) ; code_block = IfStatement ; return password ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; return password ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { if ( command instanceof RefreshType ) { VelbusStatusRequestPacket packet = new VelbusStatusRequestPacket ( getModuleAddress ( ) . getChannelIdentifier ( channelUID ) ) ; byte [ ] packetBytes = packet . getBytes ( ) ; velbusBridgeHandler . sendPacket ( packetBytes ) ; } else-if ( command instanceof UpDownType ) { UpDownType s = ( UpDownType ) command ; code_block = IfStatement ; } else-if ( command instanceof StopMoveType ) { StopMoveType s = ( StopMoveType ) command ; code_block = IfStatement ; } else-if ( command instanceof PercentType ) { VelbusBlindPositionPacket packet = new VelbusBlindPositionPacket ( getModuleAddress ( ) . getChannelIdentifier ( channelUID ) , ( ( PercentType ) command ) . byteValue ( ) ) ; byte [ ] packetBytes = packet . getBytes ( ) ; velbusBridgeHandler . sendPacket ( packetBytes ) ; } else { logger . debug ( "Unknown command {}" , command ) ; } }
public void test() { try { URI uri = new URI ( homepageUrl ) ; setHomepage ( uri ) ; } catch ( URISyntaxException e ) { logger . error ( "An error occured while parsing the homepage URL '" + homepageUrl + "'." ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( KBArticleServiceUtil . class , "getKBArticleRSS" , _getKBArticleRSSParameterTypes17 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , resourcePrimKey , status , rssDelta , rssDisplayStyle , rssFormat , themeDisplay ) ; Object returnObj = null ; code_block = TryStatement ;  return ( String ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( isDebugEnabled_SERIALIZER ) { logger . trace ( LogMarker . DISTRIBUTION_SERIALIZER ) ; } }
public void test() { if ( isDebugEnabled_SERIALIZER ) { logger . trace ( LogMarker . DISTRIBUTION_SERIALIZER ) ; } }
public void test() { if ( logger . isTraceEnabled ( LogMarker . SERIALIZER_ANNOUNCE_TYPE_WRITTEN_VERBOSE ) ) { logger . trace ( LogMarker . SERIALIZER_ANNOUNCE_TYPE_WRITTEN_VERBOSE , msg ) ; } }
public void test() { -> { LOGGER . info ( "Response code from the Bridge" ) ; assertThat ( ar . succeeded ( ) , is ( true ) ) ; HttpResponse < JsonObject > response = ar . result ( ) ; LOGGER . info ( "Response code from the Bridge is " + response . statusCode ( ) ) ; assertThat ( response . statusCode ( ) , is ( HttpResponseStatus . OK . code ( ) ) ) ; JsonObject bridgeResponse = response . body ( ) ; consumerInstanceId = bridgeResponse . getString ( "instance_id" ) ; LOGGER . info ( "Consumer instance of the consumer is " + consumerInstanceId ) ; assertThat ( consumerInstanceId . startsWith ( "kafka-bridge-consumer-" ) , is ( true ) ) ; create . complete ( true ) ; } }
public void test() { -> { LOGGER . info ( "Verifying that consumer name is created with 'kafka-bridge-consumer-' plus random hashcode" ) ; assertThat ( ar . succeeded ( ) , is ( true ) ) ; HttpResponse < JsonObject > response = ar . result ( ) ; LOGGER . info ( "Response code from the Bridge is " + response . statusCode ( ) ) ; assertThat ( response . statusCode ( ) , is ( HttpResponseStatus . OK . code ( ) ) ) ; JsonObject bridgeResponse = response . body ( ) ; consumerInstanceId = bridgeResponse . getString ( "instance_id" ) ; LOGGER . info ( "Instance id is {}" , consumerInstanceId ) ; assertThat ( consumerInstanceId . startsWith ( "kafka-bridge-consumer-" ) , is ( true ) ) ; create . complete ( true ) ; } }
public void test() { if ( ( condition . success == null || condition . success == succeeded ) && condition . predicate . test ( rCtx , rCommand ) ) { assert condition . action != null ; log . trace ( "Condition succeeded" ) ; toExecute . add ( condition . action ) ; code_block = IfStatement ; } else { log . trace ( "Condition did not succeed" ) ; } }
public void test() { try { output = mapper . readValue ( json , clazz ) ; } catch ( Exception e ) { logger . error ( "Exception in deserializing " + json , e ) ; } }
public void test() { if ( ! NotificationService . class . isAssignableFrom ( clazz ) ) { LOG . warn ( "Notification service does not implement {}" , clazz ) ; return null ; } }
public void test() { try { test = getNameFromIp ( host ) ; } catch ( UnknownHostException e1 ) { LOG . warn ( "Unknown host: {}" , host ) ; return Collections . singletonList ( DEFAULT_POOL ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { String tokenText = Utils . obfuscateToken ( Utils . getHex ( token ) ) ; logger . debug ( "Generated token: " + tokenText ) ; } }
public void test() { try { JsonObject fullCommand = new JsonObject ( ) ; int cmdId = id . incrementAndGet ( ) ; code_block = IfStatement ; fullCommand . addProperty ( "id" , cmdId ) ; fullCommand . addProperty ( "method" , command ) ; fullCommand . add ( "params" , JsonParser . parseString ( params ) ) ; MiIoSendCommand sendCmd = new MiIoSendCommand ( cmdId , MiIoCommand . getCommand ( command ) , fullCommand , cloudServer ) ; concurrentLinkedQueue . add ( sendCmd ) ; code_block = IfStatement ; code_block = IfStatement ; return cmdId ; } catch ( JsonSyntaxException e ) { logger . debug ( "jsonSyntaxException {}" , e . getMessage ( ) ) ; throw e ; } }
public void test() { try { FileDTO dto = new FileDTO ( ) ; dto . setFileName ( pckg . getName ( ) . replaceAll ( "\\s" , "_" ) + ".zip" ) ; tempZipFile = File . createTempFile ( "experimentDownload_" , ".zip" ) ; fileOutputStream = new FileOutputStream ( tempZipFile ) ; zipOutputStream = new ZipOutputStream ( fileOutputStream ) ; code_block = ForStatement ; dto . setFile ( tempZipFile ) ; FileUtils . deleteOnExitQuietly ( tempZipFile ) ; IOUtils . closeQuietly ( zipOutputStream ) ; IOUtils . closeQuietly ( fileOutputStream ) ; return dto ; } catch ( Exception e ) { IOUtils . closeQuietly ( zipOutputStream ) ; IOUtils . closeQuietly ( fileOutputStream ) ; FileUtils . deleteOnExitQuietly ( tempZipFile ) ; FileUtils . deleteOnExitQuietly ( file ) ; FileUtils . deleteQuietly ( tempZipFile ) ; FileUtils . deleteQuietly ( file ) ; logger . error ( e . getLocalizedMessage ( ) , e ) ; return null ; } }
public void test() { try { XMLConfiguration . validate ( XMLConfiguration . loadDocument ( new FileInputStream ( configFile ) ) ) ; } catch ( SAXParseException e ) { log . error ( e . getMessage ( ) ) ; fail ( e . getMessage ( ) ) ; } }
@ GET public Object getCountries ( @ QueryParam ( "fields" ) String fields ) { LOG . debug ( "Retrieving countries" ) ; List < Country > countries = CountryService . getInstance ( ) . getAll ( ) ; return parsedCountries ( countries , fields ) ; }
public void test() { { log . info ( Color . GREEN + "Stop_3_1 : missing column stop_type" + Color . NORMAL ) ; Context context = new Context ( ) ; CheckPointReport result = verifyValidation ( log , context , "stop_3_1" , GTFS_1_GTFS_Common_12 , SEVERITY . ERROR , RESULT . NOK , true ) ; Assert . assertEquals ( result . getCheckPointErrorCount ( ) , 1 , "detail count" ) ; code_block = ForStatement ; } }
public void test() { if ( input . length ( ) == 0 ) { LOG . warn ( "Command line is empty." ) ; } }
public void test() { try { return _resourceTypeHandler . parsePath ( path ) ; } catch ( final Exception e ) { LOG . warn ( "Failed to parse path: {}" , path , e ) ; return null ; } }
public void test() { if ( command instanceof PercentType ) { PercentType percent = ( PercentType ) command ; Transform transform = new Transform ( ) ; transform . setLuminanceGain ( percent . doubleValue ( ) / 100 ) ; TransformCommand transformCommand = new TransformCommand ( transform ) ; sendCommand ( transformCommand ) ; } else { logger . warn ( "Unknown command '{}'" , command ) ; } }
public void test() { try { sessions = new ArrayList < > ( getSocketAcceptor ( transport ) . getManagedSessions ( ) . values ( ) ) ; } catch ( IllegalArgumentException e ) { log . debug ( "Unable to get managed sessions" , e ) ; return ; } }
public void test() { try { code_block = ForStatement ; stopConsumers ( ) ; } catch ( Exception e ) { LOG . warn ( "Failed to stop messages." , e ) ; } }
@ Override public void stop ( ) { LOGGER . info ( "Stopping Jetty server" ) ; code_block = TryStatement ;  started = false ; }
public void test() { if ( message instanceof ObjectMessage ) { log . debug ( "message received is of ObjectMessage type." ) ; ObjectMessage objectMessage = ( ObjectMessage ) message ; UserActionRequest userActionRequest = ( UserActionRequest ) objectMessage . getObject ( ) ; log . debug ( "found action " + userActionRequest . getActionKey ( ) ) ; taskHandler . handleTask ( userActionRequest ) ; } else { log . error ( "message received is not a valid object." ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Failed to clean up." , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { { log . info ( Color . GREEN + "Stop_2_3 : missing column stop_type" + Color . NORMAL ) ; Context context = new Context ( ) ; CheckPointReport result = verifyValidation ( log , context , "stop_2_3" , GTFS_1_GTFS_Common_9 , SEVERITY . ERROR , RESULT . NOK , true ) ; Assert . assertEquals ( result . getCheckPointErrorCount ( ) , 1 , "detail count" ) ; code_block = ForStatement ; } }
public void test() { if ( structure instanceof DestinationInfo ) { DestinationInfo destinationInfo = ( DestinationInfo ) structure ; LOG . debug ( "Destination: {}" , destinationInfo ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log != null && log . isDebugEnabled ( ) ) { log . debug ( "<" + id + ">" ) ; } }
public void test() { if ( log != null && log . isErrorEnabled ( ) ) { log . error ( "Exception:" , e ) ; } }
public void test() { if ( ( errorCounter . getAndIncrement ( ) % 10 ) == 0 ) { log . error ( "Error updating aggregate host stats for SpawnBalancer: {}" , e . getMessage ( ) , e ) ; } else { log . error ( "Error updating aggregate host stats for SpawnBalancer: {}" , e . getMessage ( ) ) ; } }
public void test() { if ( ( errorCounter . getAndIncrement ( ) % 10 ) == 0 ) { log . error ( "Error updating aggregate host stats for SpawnBalancer: {}" , e . getMessage ( ) , e ) ; } else { log . error ( "Error updating aggregate host stats for SpawnBalancer: {}" , e . getMessage ( ) , e ) ; } }
@ Override public String getPassword ( ) { logger . trace ( "getPassword()" ) ; return config . getPassword ( ) ; }
public void test() { try { sourceHolder . close ( ) ; } catch ( Throwable e ) { LOG . warn ( e . getMessage ( ) , e ) ; } }
public void test() { if ( ActiveMQRALogger . LOGGER . isTraceEnabled ( ) ) { ActiveMQRALogger . LOGGER . trace ( "execute()" ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void attachClean ( StgNZielobjekt instance ) { log . debug ( "attaching clean StgNZielobjekt instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . lock ( instance , LockMode . NONE ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { code_block = WhileStatement ; } catch ( SQLException e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( ReflectiveOperationException e ) { log . error ( e , e ) ; } }
public void test() { if ( isAccepted ) { item . setState ( newState ) ; } else { LOG . warn ( "Item {} is rejected for item {}: {}" , item . getName ( ) , item . getState ( ) , item . getId ( ) ) ; } }
public void test() { try { GenericItem item = ( GenericItem ) itemRegistry . getItem ( itemName ) ; boolean isAccepted = false ; code_block = IfStatement ; code_block = IfStatement ; } catch ( ItemNotFoundException e ) { logger . warn ( "Item '{}' doesn't exist." , itemName ) ; } }
public void test() { if ( channel == null ) { logger . debug ( "Sending Command: '{}'" , command ) ; } else { logger . debug ( "Sending Command: '{}'" , command ) ; channel . write ( toSend ) ; } }
public void test() { { log . info ( Color . GREEN + "Route_8 : invalid route_name" + Color . NORMAL ) ; Context context = new Context ( ) ; CheckPointReport result = verifyValidation ( log , context , "route_8" , GTFS_2_GTFS_Route_2 , SEVERITY . WARNING , RESULT . NOK , true ) ; Assert . assertEquals ( result . getCheckPointErrorCount ( ) , 1 , "detail count" ) ; code_block = ForStatement ; } }
public void test() { try { fs . delete ( basePath , true ) ; } catch ( IOException e ) { LOG . warn ( "Failed to delete file {}" , basePath , e ) ; } }
@ PostConstruct public void load ( ) { this . loadLoginModuleNames ( ) ; this . loadJaasConfig ( ) ; authenticatedSessionTimeoutMinutes = sysPropConfigStore . get ( "authenticatedSessionTimeoutMinutes" , 180 ) ; enforceMatchingUsernames = Boolean . parseBoolean ( sysPropConfigStore . get ( "zanata.enforce.matchingusernames" ) ) ; tokenExpiresInSeconds = sysPropConfigStore . getLong ( ACCESS_TOKEN_EXPIRES_IN_SECONDS , 3600 ) ; logger . info ( "Token expiration: {}" , tokenExpiresInSeconds ) ; }
@ Transactional ( rollbackFor = ArrowheadException . class ) public void deleteSubscription ( final String eventType , final SystemRequestDTO subscriberSystem ) { logger . debug ( "deleteSubscription started..." ) ; final EventType validEventType = validateEventTypeIsInDB ( eventType ) ; final System validSubscriber = validateSystemRequestDTO ( subscriberSystem ) ; code_block = TryStatement ;  }
public void test() { try { final Optional < Subscription > subcriptionOptional = subscriptionRepository . findByEventTypeAndSubscriberSystem ( validEventType , validSubscriber ) ; code_block = IfStatement ; } catch ( final Exception ex ) { logger . debug ( ex . getMessage ( ) , ex ) ; throw new ArrowheadException ( CoreCommonConstants . DATABASE_OPERATION_EXCEPTION_MSG ) ; } }
public void test() { try { bis = new ByteArrayInputStream ( script . getBytes ( "UTF-8" ) ) ; externalType = pythonService . loadPythonScript ( bis , scriptName , customScriptType . getPythonClass ( ) , customScriptType . getCustomScriptType ( ) , new PyObject [ ] code_block = "" ; ) ; } catch ( UnsupportedEncodingException e ) { logger . error ( "can't load python script" , e ) ; } finally { IOUtils . closeQuietly ( bis ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception ex ) { LOGGER . error ( "Failed to clean up." , ex ) ; } }
public void test() { try { bratRenderCommand ( getCasProvider ( ) . get ( ) ) . ifPresent ( cmd code_block = LoopStatement ; ) ; } catch ( IOException e ) { LOG . error ( "Unable to load data." , e ) ; error ( "Unable to load data: " + ExceptionUtils . getRootCauseMessage ( e ) ) ; aTarget . addChildren ( getPage ( ) , IFeedback . class ) ; } }
public void test() { try { return io . gomint . server . player . PlayerSkin . fromInputStream ( inputStream ) ; } catch ( IOException e ) { log . error ( "Unable to load PlayerSkin from: " + e . getMessage ( ) ) ; return null ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { String q = "key=" + CouchDBUtils . appendQuotes ( pKeyColumnValue ) ; uri = new URI ( CouchDBConstants . PROTOCOL , null , httpHost . getHostName ( ) , httpHost . getPort ( ) , CouchDBConstants . URL_SEPARATOR + schemaName . toLowerCase ( ) + CouchDBConstants . URL_SEPARATOR + CouchDBConstants . DESIGN + tableName + CouchDBConstants . VIEW + pKeyColumnName , q , null ) ; HttpGet get = new HttpGet ( uri ) ; get . addHeader ( "Accept" , "application/json" ) ; response = httpClient . execute ( get ) ; InputStream content = response . getEntity ( ) . getContent ( ) ; Reader reader = new InputStreamReader ( content ) ; JsonObject json = gson . fromJson ( reader , JsonObject . class ) ; JsonElement jsonElement = json . get ( "rows" ) ; code_block = IfStatement ; JsonArray array = jsonElement . getAsJsonArray ( ) ; code_block = ForStatement ; } catch ( Exception e ) { logger . error ( "Error while deleting object, Caused by: ." , e ) ; throw new KunderaException ( e ) ; } finally { closeContent ( response ) ; } }
@ Override public Void run ( ) { log . info ( "Starting Jetty server" ) ; startServer ( server ) ; return null ; }
public void test() { if ( testProcess == null ) { log . warn ( "Can't find test process with id {}" , jobID ) ; continue ; } }
public void test() { try { CommerceInventoryWarehouseItemServiceUtil . moveQuantitiesBetweenWarehouses ( fromCommerceInventoryWarehouseId , toCommerceInventoryWarehouseId , sku , quantity ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try { btree = new BTreeStore ( pool , STRUCTURAL_INDEX_ID , FILE_FORMAT_VERSION_ID , false , file , pool . getCacheManager ( ) ) ; } catch ( final DBException e ) { LOG . error ( "Problem creating BTree store" , e ) ; throw new DatabaseConfigurationException ( e . getMessage ( ) , e ) ; } }
public void test() { try { String [ ] keys = key . split ( KEY_SEPARATOR ) ; int dpc = Integer . parseInt ( keys [ 0 ] ) ; code_block = ForStatement ; } catch ( Exception ex ) { LOG . warn ( "key:" + key + " Unknown error." , ex ) ; } }
public void test() { try { method = Utils . findSetter ( field . getName ( ) , classEntity , field . getType ( ) ) ; currentBin = bins . get ( AnnotationUtils . deepFieldName ( field ) ) ; code_block = IfStatement ; } catch ( IllegalAccessException | InvocationTargetException | IllegalArgumentException e ) { LOGGER . debug ( "Could not invoke '{}' on {}. Trying next()." , classEntity . getName ( ) , e ) ; method . invoke ( t , Utils . castNumberType ( insert , classField ) ) ; } }
public void test() { if ( ! expired . isEmpty ( ) ) { logger . debug ( "Deleting {} expired sessions" , expired . size ( ) ) ; deletedCount = mBlobRepository . deleteByIds ( expired ) ; } else { logger . debug ( "Nothing to delete" ) ; deletedCount = 0 ; } }
public void test() { if ( ! expired . isEmpty ( ) ) { deletedCount = mBlobRepository . deleteByIds ( expired ) ; logger . debug ( "Number of Mbob deleted: {}" , deletedCount ) ; } else { logger . debug ( "No Mbob deleted" ) ; deletedCount = 0 ; } }
private void runJoinSample ( EPRuntime runtime ) { String epl = "select sw.* " + "from SampleJoinEventlastevent sje, MySampleWindow sw " + "where sw.key1 = sje.propOne and sw.key2 = sje.propTwo" ; EPStatement stmt = compileDeploy ( epl , runtime ) ; SampleUpdateListener sampleListener = new SampleUpdateListener ( ) ; stmt . addListener ( sampleListener ) ; runtime . getEventService ( ) . sendEventBean ( new SampleJoinEvent ( "sample1" , "sample2" ) , "SampleJoinEvent" ) ; log . info ( "Generated join sample." ) ; }
public void test() { if ( filter . matches ( map ) ) { LOGGER . trace ( "filter = {}" , importPAXWicketAPI ) ; return true ; } else { LOGGER . trace ( "filter = {} not matches {}" , importPAXWicketAPI ) ; } }
public void test() { if ( filter . matches ( map ) ) { LOGGER . trace ( "filter = {} matches {}" , importPAXWicketAPI ) ; return true ; } else { LOGGER . trace ( "filter = {} does not match {}" , importPAXWicketAPI , import ) ; } }
public void test() { try { Filter filter = paxBundleContext . createFilter ( filterString ) ; code_block = IfStatement ; } catch ( InvalidSyntaxException e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( ClassNameServiceUtil . class , "fetchByClassNameId" , _fetchByClassNameIdParameterTypes0 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , classNameId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . portal . kernel . model . ClassName ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void setPrimaryFieldPos ( int p ) { LOGGER . trace ( "setPrimaryFieldPos {}" , p ) ; primaryFieldPos = p ; }
public void test() { if ( IS_TIMER_COMPSS_ENABLED ) { final long timeAssignResourcesEnd = System . nanoTime ( ) ; final float timeAssignResourcesElapsed = ( timeAssignResourcesEnd - timeAssignResourcesStart ) / ( float ) NANO_TO_MS ; LOG . info ( "Resource duration is " + timeAssignResourcesElapsed + " milliseconds" ) ; } }
public void test() { try { encodeSensorDataToNetcdf ( netcdfFile , sensorDataset , version ) ; return new BinaryAttachmentResponse ( Files . toByteArray ( netcdfFile ) , getContentType ( ) , String . format ( filename , makeDateSafe ( new DateTime ( DateTimeZone . UTC ) ) ) ) ; } catch ( IOException e ) { throw new EncodingException ( "Couldn't create netCDF file" , e ) ; } finally { logger . debug ( "encode : {}" , json ) ; } }
public void test() { try { usersFileLoader . getProperties ( ) . remove ( username ) ; usersFileLoader . persistProperties ( ) ; getGroupsPropertiesManager ( ) . removeEntry ( username ) ; } catch ( IOException e ) { LOG . error ( "Error removing user " + username , e ) ; throw new SecurityManagementException ( e ) ; } }
public void test() { if ( failIfLatencyProblem ) { thread . interrupt ( ) ; throw latencyException ; } else { log . warn ( "Latency failed" , e ) ; } }
public void test() { try { final Thread waitingThread = Thread . currentThread ( ) ; Thread thread ; code_block = IfStatement ; do code_block = "" ; code_block = WhileStatement ; while ( ! similarColor ( lastRemoteColor , Color . GREEN ) ) ; code_block = WhileStatement ; } catch ( InterruptedException e ) { logger . warn ( e . getMessage ( ) , e ) ; } }
public static void schedule ( ) { LOGGER . debug ( "Trying to schedule data cache..." ) ; service . scheduleWithFixedDelay ( new DataCacheHandler ( ) , 0 , TTL , TimeUnit . SECONDS ) ; }
public void test() { if ( content != null ) { final Document change = DocumentReader . floatNumbersAsTextReader ( ) . read ( content ) ; LOGGER . trace ( "Wrote change {}" , change ) ; processor . process ( new Wal2JsonReplicationMessage ( txId , commitTime , change , containsMetadata , lastMessage , typeRegistry ) ) ; } else { LOGGER . trace ( "Empty change arrived" ) ; processor . process ( new NoopMessage ( txId , commitTime ) ) ; } }
public void test() { if ( content != null ) { final Document change = DocumentReader . floatNumbersAsTextReader ( ) . read ( content ) ; LOGGER . trace ( "Change arrived for decoding {}" , change ) ; processor . process ( new Wal2JsonReplicationMessage ( txId , commitTime , change , containsMetadata , lastMessage , typeRegistry ) ) ; } else { LOGGER . trace ( "No change received for decoding {}" , txId ) ; processor . process ( new NoopMessage ( txId , commitTime ) ) ; } }
@ Test public void callerData ( ) { assertEquals ( 0 , listAppender . list . size ( ) ) ; PatternLayout pl = new PatternLayout ( ) ; pl . setPattern ( "%-5level [%class] %logger - %msg" ) ; pl . setContext ( lc ) ; pl . start ( ) ; listAppender . layout = pl ; Logger logger = Logger . getLogger ( "basic-test" ) ; assertEquals ( 0 , listAppender . list . size ( ) ) ; rootLogger . setLevel ( Level . TRACE ) ; logger . trace ( HELLO ) ; logger . trace ( event ) ; assertEquals ( 1 , listAppender . list . size ( ) ) ; ILoggingEvent event = ( ILoggingEvent ) listAppender . list . get ( 0 ) ; assertEquals ( HELLO , event . getMessage ( ) ) ; assertEquals ( 1 , listAppender . stringList . size ( ) ) ; assertEquals ( "TRACE [" + Log4jInvocation . class . getName ( ) + "] basic-test - Hello" , listAppender . stringList . get ( 0 ) ) ; }
@ Test public void callerData ( ) { assertEquals ( 0 , listAppender . list . size ( ) ) ; PatternLayout pl = new PatternLayout ( ) ; pl . setPattern ( "%-5level [%class] %logger - %msg" ) ; pl . setContext ( lc ) ; pl . start ( ) ; listAppender . layout = pl ; Logger logger = Logger . getLogger ( "basic-test" ) ; logger . debug ( "none" ) ; assertEquals ( 0 , listAppender . list . size ( ) ) ; rootLogger . setLevel ( Level . TRACE ) ; assertEquals ( 1 , listAppender . list . size ( ) ) ; ILoggingEvent event = ( ILoggingEvent ) listAppender . list . get ( 0 ) ; assertEquals ( HELLO , event . getMessage ( ) ) ; assertEquals ( 1 , listAppender . stringList . size ( ) ) ; assertEquals ( "TRACE [" + Log4jInvocation . class . getName ( ) + "] basic-test - Hello" , listAppender . stringList . get ( 0 ) ) ; }
public void test() { if ( _log . isWarnEnabled ( ) ) { StringBundler sb = new StringBundler ( 4 ) ; sb . append ( "Unable to export small image " ) ; sb . append ( entry . getSmallImageId ( ) ) ; sb . append ( " to blogs entry " ) ; sb . append ( entry . getEntryId ( ) ) ; _log . warn ( sb . toString ( ) , exception ) ; } }
@ Override protected void supplyActiveThreads ( ) { code_block = IfStatement ; startTime = System . currentTimeMillis ( ) ; boolean isDebugEnabled = log . isDebugEnabled ( ) ; log . debug ( "Preparing threads..." ) ; code_block = WhileStatement ; log . info ( "Done supplying threads" ) ; }
public void test() { if ( isDebugEnabled ) { logger . debug ( "Closing client with key {}" , key ) ; } }
@ Override protected void supplyActiveThreads ( ) { log . info ( "Start supplying threads" ) ; code_block = IfStatement ; startTime = System . currentTimeMillis ( ) ; boolean isDebugEnabled = log . isDebugEnabled ( ) ; code_block = WhileStatement ; log . info ( "Finished supplying threads" ) ; }
public void test() { try { return ( ContentRecordVO ) this . getContentDAO ( ) . loadEntityRecord ( id ) ; } catch ( Throwable t ) { _logger . error ( "Error while loading content vo : id {}" , id , t ) ; throw new ApsSystemException ( "Error while loading content vo : id " + id , t ) ; } }
public void test() { try { URL dataSourcesUrl = thirdeyeConfig . getDataSourcesAsUrl ( ) ; DataSources dataSources = DataSourcesLoader . fromDataSourcesUrl ( dataSourcesUrl ) ; code_block = IfStatement ; Map < String , ThirdEyeDataSource > thirdEyeDataSourcesMap = DataSourcesLoader . getDataSourceMap ( dataSources ) ; QueryCache queryCache = new QueryCache ( thirdEyeDataSourcesMap , Executors . newCachedThreadPool ( ) ) ; ThirdEyeCacheRegistry . getInstance ( ) . registerQueryCache ( queryCache ) ; } catch ( Exception e ) { LOGGER . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( heartbeatTime < oldTime ) { logger . info ( "{} ignoring heartbeat request" , heartbeatTime ) ; return ; } }
public void test() { if ( ! resultOptional . isPresent ( ) ) { LOG . debug ( "Cancelling scheduled task with id {}" , id ) ; trigger . cancel ( ) ; return null ; } }
public void test() { if ( ! triggerResult . isSuccessful ( ) ) { logger . error ( "Failed to cancel trigger {}" , triggerResult . getErrorMessage ( ) ) ; trigger . cancel ( ) ; failureYieldExpiration = System . currentTimeMillis ( ) + 1000L ; return null ; } }
@ Override public List < SourceRecord > poll ( ) throws InterruptedException { final long yieldExpiration = Math . max ( failureYieldExpiration , dataflow . getSourceYieldExpiration ( ) ) ; final long now = System . currentTimeMillis ( ) ; final long yieldMillis = yieldExpiration - now ; code_block = IfStatement ; code_block = IfStatement ; logger . debug ( "Triggering dataflow" ) ; final long start = System . nanoTime ( ) ; final DataflowTrigger trigger = dataflow . trigger ( ) ; final Optional < TriggerResult > resultOptional = trigger . getResult ( timeoutMillis , TimeUnit . MILLISECONDS ) ; code_block = IfStatement ; triggerResult = resultOptional . get ( ) ; code_block = IfStatement ; verifyFlowFilesTransferredToProperPort ( triggerResult , outputPortName , trigger ) ; final long nanos = System . nanoTime ( ) - start ; logger . debug ( "Skipping dataflow due to {} nanos" , nanos ) ; final List < FlowFile > outputFlowFiles = triggerResult . getOutputFlowFiles ( outputPortName ) ; final List < SourceRecord > sourceRecords = new ArrayList < > ( outputFlowFiles . size ( ) ) ; Map < String , ? > componentState = dataflow . getComponentStates ( Scope . CLUSTER ) ; final Map < String , ? > partitionMap ; code_block = IfStatement ; code_block = ForStatement ; code_block = IfStatement ; return sourceRecords ; }
public void test() { if ( opentsdbEvent != null ) { sender . enqueue ( opentsdbEvent ) ; } else { LOG . warn ( "opentsdbEvent is null" ) ; } }
public void test() { try { Runtime . getRuntime ( ) . removeShutdownHook ( shutdownHook ) ; } catch ( IllegalStateException e ) { logger . debug ( "IllegalStateException while un-registering {}'s shutdown hook." , serviceName , e ) ; } catch ( Throwable t ) { logger . warn ( "Exception while un-registering {}'s shutdown hook." , serviceName , t ) ; } }
public void test() { try { Runtime . getRuntime ( ) . removeShutdownHook ( shutdownHook ) ; } catch ( IllegalStateException e ) { logger . debug ( "Unable to remove shutdown hook for {}, shutdown already in progress" , serviceName , e ) ; } catch ( Throwable t ) { logger . warn ( "Unable to remove shutdown hook for {}" , serviceName , t ) ; } }
public void test() { if ( TwoFactorWebAuthenticationDetails . class . isAssignableFrom ( details . getClass ( ) ) ) { TwoFactorWebAuthenticationDetails authDetails = ( TwoFactorWebAuthenticationDetails ) details ; log . debug ( "{}" , details ) ; } }
public void test() { if ( OAuth2LoginAuthenticationToken . class . isAssignableFrom ( auth . getClass ( ) ) ) { OAuth2LoginAuthenticationToken authenticationToken = ( OAuth2LoginAuthenticationToken ) auth ; DhisOidcUser principal = ( DhisOidcUser ) authenticationToken . getPrincipal ( ) ; UserCredentials userCredentials = principal . getUserCredentials ( ) ; username = userCredentials . getUsername ( ) ; WebAuthenticationDetails tokenDetails = ( WebAuthenticationDetails ) authenticationToken . getDetails ( ) ; String remoteAddress = tokenDetails . getRemoteAddress ( ) ; log . info ( "Authentication with remote address: " + remoteAddress ) ; } }
public void test() { try { preWalk ( ) ; specWalker . walk ( profileInstance . getProfileSpec ( ) , walkState ) ; } catch ( InterruptedException e ) { log . error ( e . getMessage ( ) , e ) ; } catch ( IOException e ) { inError = true ; log . error ( e . getMessage ( ) , e ) ; throw new ProfileException ( e ) ; } finally { postWalk ( ) ; counter . cancel ( ) ; countFuture . cancel ( false ) ; code_block = IfStatement ; submissionGateway . save ( ) ; profileWalkerDao . delete ( ) ; } }
public void test() { try { preWalk ( ) ; specWalker . walk ( profileInstance . getProfileSpec ( ) , walkState ) ; } catch ( InterruptedException e ) { log . debug ( e . getMessage ( ) , e ) ; } catch ( IOException e ) { log . error ( e . getMessage ( ) , e ) ; inError = true ; throw new ProfileException ( e ) ; } finally { postWalk ( ) ; counter . cancel ( ) ; countFuture . cancel ( false ) ; code_block = IfStatement ; submissionGateway . save ( ) ; profileWalkerDao . delete ( ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
@ Test public void testPrepare ( ) { TimeHelper . setProvider ( new FixedTimeProvider ( 12345l ) ) ; List < CipherSuite > suiteList = new LinkedList < > ( ) ; context . getConfig ( ) . setHighestProtocolVersion ( ProtocolVersion . TLS12 ) ; suiteList . add ( CipherSuite . TLS_DHE_DSS_WITH_AES_256_CBC_SHA256 ) ; suiteList . add ( CipherSuite . TLS_DHE_RSA_WITH_CHACHA20_POLY1305_SHA256 ) ; suiteList . add ( CipherSuite . TLS_DHE_PSK_WITH_AES_128_GCM_SHA256 ) ; context . setClientSupportedCiphersuites ( suiteList ) ; List < CipherSuite > ourSuiteList = new LinkedList < > ( ) ; ourSuiteList . add ( CipherSuite . TLS_DHE_RSA_WITH_CHACHA20_POLY1305_SHA256 ) ; List < CompressionMethod > ourCompressionList = new LinkedList < > ( ) ; ourCompressionList . add ( CompressionMethod . LZS ) ; context . getConfig ( ) . setDefaultClientSupportedCiphersuites ( ourSuiteList ) ; context . getConfig ( ) . setDefaultServerSupportedCompressionMethods ( ourCompressionList ) ; context . setHighestClientProtocolVersion ( ProtocolVersion . TLS11 ) ; List < CompressionMethod > compressionList = new LinkedList < > ( ) ; compressionList . add ( CompressionMethod . NULL ) ; compressionList . add ( CompressionMethod . LZS ) ; context . setClientSupportedCompressions ( compressionList ) ; context . getConfig ( ) . setDefaultServerSessionId ( new byte [ ] code_block = "" ; ) ; preparator . prepare ( ) ; assertArrayEquals ( ProtocolVersion . TLS11 . getValue ( ) , message . getProtocolVersion ( ) . getValue ( ) ) ; assertArrayEquals ( ArrayConverter . longToUint32Bytes ( 12345l ) , message . getUnixTime ( ) . getValue ( ) ) ; assertArrayEquals ( ArrayConverter . concatenate ( ArrayConverter . longToUint32Bytes
public void test() { try { code_block = IfStatement ; } catch ( final InterruptedException e ) { LOGGER . error ( "Interrupted while waiting for selection." , e ) ; } }
private void updateRack ( ) { Rack rack = this . rackClient . getByName ( RACK_NAME ) . get ( 0 ) ; String resourceId = rack . getResourceId ( ) ; rack . setThermalLimit ( Integer . valueOf ( 1000 ) ) ; Rack updatedRack = this . rackClient . update ( resourceId , rack ) ; LOGGER . info ( "Rack returned to client : " + updatedRack . toJsonString ( ) ) ; }
public void test() { if ( entry == null ) { recordCacheMiss ( ) ; logger . info ( "Cache miss" ) ; return null ; } }
public void test() { if ( loc == null ) { recordCacheMiss ( ) ; logger . info ( recordCacheMiss ( ) ) ; return null ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( "Closing connection {}" , connection ) ; } }
public void test() { if ( jvmargs != null ) { } else { jvmargs = env . get ( "JAVA_OPTS" ) ; code_block = IfStatement ; log . info ( "Using " + jvmargs ) ; } }
public void test() { if ( jvmargs != null ) { LOGGER . info ( "Using default JAVA_OPTS" ) ; } else { LOGGER . info ( "Using default JAVA_OPTS" ) ; jvmargs = "" ; } }
public void test() { if ( jvmargs != null ) { LOGGER . info ( "Using JAVA_OPTS from environment" ) ; } else { LOGGER . info ( "Using JAVA_OPTS from environment" ) ; jvmargs = "" ; } }
public void test() { if ( ( ExecutionPathDebugLog . isDebugEnabled ) && ( log . isDebugEnabled ( ) ) ) { log . debug ( ".pause" ) ; } }
public void afterPropertiesSet ( ) throws Exception { JdbcTemplate jdbcTemplate = new JdbcTemplate ( dataSource ) ; jdbcTemplate . execute ( CREATE_TABLES ) ; String query = "insert into users values ('user', '" + new Sha256Hash ( "user" ) . toBase64 ( ) + "' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created dashboard." ) ; query = "insert into users values ( 'admin', '" + new Sha256Hash ( "admin" ) . toBase64 ( ) + "' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created admin." ) ; query = "insert into roles values ( 'user' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created user" ) ; query = "insert into roles values ( 'admin' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created admin" ) ; query = "insert into roles_permissions values ( 'user', 'view')" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created permission view for role user" ) ; query = "insert into roles_permissions values ( 'admin', 'user:*')" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created permission user:* for role admin" ) ; query = "insert into user_roles values ( 'user', 'user' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Assigned user role user" ) ; query = "insert into user_roles values ( 'admin', 'admin' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Assigned admin role admin" ) ; }
public void afterPropertiesSet ( ) throws Exception { JdbcTemplate jdbcTemplate = new JdbcTemplate ( dataSource ) ; jdbcTemplate . execute ( CREATE_TABLES ) ; String query = "insert into users values ('user', '" + new Sha256Hash ( "user" ) . toBase64 ( ) + "' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created user." ) ; query = "insert into users values ( 'admin', '" + new Sha256Hash ( "admin" ) . toBase64 ( ) + "' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created query." ) ; query = "insert into roles values ( 'user' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created user" ) ; query = "insert into roles values ( 'admin' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created admin" ) ; query = "insert into roles_permissions values ( 'user', 'view')" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created permission view for role user" ) ; query = "insert into roles_permissions values ( 'admin', 'user:*')" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created permission user:* for role admin" ) ; query = "insert into user_roles values ( 'user', 'user' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Assigned user role user" ) ; query = "insert into user_roles values ( 'admin', 'admin' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Assigned admin role admin" ) ; }
public void afterPropertiesSet ( ) throws Exception { JdbcTemplate jdbcTemplate = new JdbcTemplate ( dataSource ) ; jdbcTemplate . execute ( CREATE_TABLES ) ; String query = "insert into users values ('user', '" + new Sha256Hash ( "user" ) . toBase64 ( ) + "' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created user." ) ; query = "insert into users values ( 'admin', '" + new Sha256Hash ( "admin" ) . toBase64 ( ) + "' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created admin." ) ; query = "insert into roles values ( 'user' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created permission view for role {}" , query ) ; query = "insert into roles values ( 'admin' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created admin" ) ; query = "insert into roles_permissions values ( 'user', 'view')" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created permission view for role user" ) ; query = "insert into roles_permissions values ( 'admin', 'user:*')" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created permission user:* for role admin" ) ; query = "insert into user_roles values ( 'user', 'user' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Assigned user role user" ) ; query = "insert into user_roles values ( 'admin', 'admin' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Assigned admin role admin" ) ; }
public void afterPropertiesSet ( ) throws Exception { JdbcTemplate jdbcTemplate = new JdbcTemplate ( dataSource ) ; jdbcTemplate . execute ( CREATE_TABLES ) ; String query = "insert into users values ('user', '" + new Sha256Hash ( "user" ) . toBase64 ( ) + "' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created user." ) ; query = "insert into users values ( 'admin', '" + new Sha256Hash ( "admin" ) . toBase64 ( ) + "' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created admin." ) ; query = "insert into roles values ( 'user' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created user" ) ; query = "insert into roles values ( 'admin' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created permission view for role view" ) ; query = "insert into roles_permissions values ( 'user', 'view')" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created permission view for role user" ) ; query = "insert into roles_permissions values ( 'admin', 'user:*')" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created permission user:* for role admin" ) ; query = "insert into user_roles values ( 'user', 'user' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Assigned user role user" ) ; query = "insert into user_roles values ( 'admin', 'admin' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Assigned admin role admin" ) ; }
public void afterPropertiesSet ( ) throws Exception { JdbcTemplate jdbcTemplate = new JdbcTemplate ( dataSource ) ; jdbcTemplate . execute ( CREATE_TABLES ) ; String query = "insert into users values ('user', '" + new Sha256Hash ( "user" ) . toBase64 ( ) + "' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created user." ) ; query = "insert into users values ( 'admin', '" + new Sha256Hash ( "admin" ) . toBase64 ( ) + "' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created admin." ) ; query = "insert into roles values ( 'user' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created user" ) ; query = "insert into roles values ( 'admin' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created admin" ) ; query = "insert into roles_permissions values ( 'user', 'view')" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created permission user:* for role admin" ) ; query = "insert into roles_permissions values ( 'user', 'user:*')" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created permission user:* for role admin" ) ; query = "insert into user_roles values ( 'user', 'user' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Assigned user role user" ) ; query = "insert into user_roles values ( 'admin', 'admin' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Assigned admin role admin" ) ; }
public void afterPropertiesSet ( ) throws Exception { JdbcTemplate jdbcTemplate = new JdbcTemplate ( dataSource ) ; jdbcTemplate . execute ( CREATE_TABLES ) ; String query = "insert into users values ('user', '" + new Sha256Hash ( "user" ) . toBase64 ( ) + "' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created user." ) ; query = "insert into users values ( 'admin', '" + new Sha256Hash ( "admin" ) . toBase64 ( ) + "' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created admin." ) ; query = "insert into roles values ( 'user' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created user" ) ; query = "insert into roles values ( 'admin' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created admin" ) ; query = "insert into roles_permissions values ( 'user', 'view')" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created permission view for role user" ) ; query = "insert into roles_permissions values ( 'admin', 'user:*')" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created permission view for role user" ) ; query = "insert into user_roles values ( 'user', 'user' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Assigned user role user" ) ; query = "insert into user_roles values ( 'admin', 'admin' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Assigned admin role admin" ) ; }
public void afterPropertiesSet ( ) throws Exception { JdbcTemplate jdbcTemplate = new JdbcTemplate ( dataSource ) ; jdbcTemplate . execute ( CREATE_TABLES ) ; String query = "insert into users values ('user', '" + new Sha256Hash ( "user" ) . toBase64 ( ) + "' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created user." ) ; query = "insert into users values ( 'admin', '" + new Sha256Hash ( "admin" ) . toBase64 ( ) + "' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created admin." ) ; query = "insert into roles values ( 'user' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created user" ) ; query = "insert into roles values ( 'admin' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created admin" ) ; query = "insert into roles_permissions values ( 'user', 'view')" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created permission view for role user" ) ; query = "insert into roles_permissions values ( 'admin', 'user:*')" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created permission user:* for role admin" ) ; query = "insert into user_roles values ( 'user', 'user' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Assigned admin role admin" ) ; query = "insert into user_roles values ( 'admin', 'admin' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Assigned admin role admin" ) ; }
public void afterPropertiesSet ( ) throws Exception { JdbcTemplate jdbcTemplate = new JdbcTemplate ( dataSource ) ; jdbcTemplate . execute ( CREATE_TABLES ) ; String query = "insert into users values ('user', '" + new Sha256Hash ( "user" ) . toBase64 ( ) + "' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created user." ) ; query = "insert into users values ( 'admin', '" + new Sha256Hash ( "admin" ) . toBase64 ( ) + "' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created admin." ) ; query = "insert into roles values ( 'user' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created user" ) ; query = "insert into roles values ( 'admin' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created admin" ) ; query = "insert into roles_permissions values ( 'user', 'view')" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created permission view for role user" ) ; query = "insert into roles_permissions values ( 'admin', 'user:*')" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created permission user:* for role admin" ) ; query = "insert into user_roles values ( 'user', 'user' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Assigned user role user" ) ; query = "insert into user_roles values ( 'admin', 'admin' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Assigned table role user" ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { t0 = System . currentTimeMillis ( ) ; log . debug ( "storing blob " + blobId + " to Azure" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { long dtms = System . currentTimeMillis ( ) - t0 ; log . debug ( "stored blob " + digest + " to Azure in " + dtms + "ms" ) ; } }
@ Override public byte [ ] serializeHandshakeMessageContent ( ) { LOGGER . debug ( "Serializing PublicKeyMessage" ) ; code_block = IfStatement ; writeSerializedPublickey ( msg ) ; return getAlreadySerialized ( ) ; }
public void test() { try { logger . error ( t , "Error while processing message" , t ) ; } catch ( Exception e ) { t . addSuppressed ( e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceTaxMethodServiceUtil . class , "getCommerceTaxMethods" , _getCommerceTaxMethodsParameterTypes6 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( java . util . List < com . liferay . commerce . tax . model . CommerceTaxMethod > ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
@ Test public void test_00 ( ) { Log . debug ( "Test" ) ; initRand ( ) ; for ( int len = 1 ; len < 1000 ; len ++ ) DnaSequenceBaseAt ( len ) ; }
public void terminated ( TaskAttemptID taskid ) { TaskStatus status = taskStatuses . get ( taskid ) ; log . info ( "Firing task %s" , taskid ) ; status . setRunState ( TaskStatus . State . FAILED ) ; activeTasks . remove ( taskid ) ; }
public void test() { try { java . util . List < com . liferay . oauth2 . provider . model . OAuth2Authorization > returnValue = OAuth2AuthorizationServiceUtil . getApplicationOAuth2Authorizations ( oAuth2ApplicationId , start , end , orderByComparator ) ; return com . liferay . oauth2 . provider . model . OAuth2AuthorizationSoap . toSoapModels ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( ! txStillOpen && janusGraphManagerIsInBadState ) { break ; } else-if ( ! txStillOpen ) { DataOutput out = graph . getDataSerializer ( ) . getDataOutput ( 64 ) ; out . writeObjectNotNull ( MgmtLogType . CACHED_TYPE_EVICTION_ACK ) ; out . writeObjectNotNull ( originId ) ; VariableLong . writePositive ( out , evictionId ) ; code_block = IfStatement ; code_block = TryStatement ;  LOG . info ( "Acquired transaction {} for eviction {}" , evictionId , evictionId ) ; break ; } }
public void test() { try { sysLog . add ( out . getStaticBuffer ( ) ) ; log . debug ( "Sent {}: evictionID={} originID={}" , MgmtLogType . CACHED_TYPE_EVICTION_ACK , evictionId , originId ) ; } catch ( ResourceUnavailableException e ) { log . warn ( "Failed to send {}: evictionID={} originID={}" , MgmtLogType . CACHED_TYPE_EVICTION_ACK , evictionId , originId , e ) ; } }
public void test() { try { times . sleepPast ( times . getTime ( ) . plus ( SLEEP_INTERVAL ) ) ; } catch ( InterruptedException e ) { LOGGER . info ( "block is interrupted" , e ) ; break ; } }
public void test() { if ( validationResult . isPresent ( ) ) { LOG . debug ( validationResult . get ( ) ) ; return null ; } }
public void test() { try { isSystemUpdatingWebView = false ; return inflater . inflate ( R . layout . fragment_authenticated_webview , container , false ) ; } catch ( InflateException e ) { logger . error ( e . getMessage ( ) , e ) ; isSystemUpdatingWebView = true ; return inflater . inflate ( R . layout . content_error , container , false ) ; } }
public void test() { try { zkClient . newNamespaceAwareEnsurePath ( path ) . ensure ( zkClient . getZookeeperClient ( ) ) ; zkClient . setData ( ) . forPath ( path , Bytes . toBytes ( layoutID ) ) ; } catch ( Exception e ) { log . warn ( "Set data failure." , e ) ; wrapAndRethrow ( e ) ; } }
public void test() { try { logger . debug ( "Reliability loss with policy of reconnect and membership thread doing reconnect" ) ; initializationLatchAfterMemberTimeout . await ( ) ; getSystem ( ) . tryReconnect ( false , "Role Loss" , getCache ( ) ) ; synchronized ( missingRequiredRoles ) code_block = "" ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( CancelException ignor ) { throw ignor ; } catch ( Exception e ) { log . warn ( "Ignored exception." , e ) ; } }
public void test() { if ( "true" . equals ( OscarProperties . getInstance ( ) . getProperty ( "integrator.send.documents.disabled" , "false" ) ) ) { logger . warn ( "Can't send command.send.documents.disabled" ) ; return ; } }
public void test() { try { is = evidence . getBufferedStream ( ) ; extractFile ( is , evidence , null ) ; } catch ( IOException e ) { logger . warn ( evidence . toString ( ) , e ) ; } finally { IOUtil . closeQuietly ( is ) ; } }
public void test() { if ( externalInitializer != null ) { InetSocketAddress externalLocalAddress = configureInitializer ( externalInitializer ) ; portRegister . register ( BoltConnector . NAME , externalLocalAddress ) ; var host = externalInitializer . address ( ) . getHostname ( ) ; var port = externalLocalAddress . getPort ( ) ; log . info ( "Attempted to connect to {}:{}" , host , port ) ; } }
public void test() { if ( internalInitializer != null ) { var internalLocalAddress = configureInitializer ( internalInitializer ) ; portRegister . register ( BoltConnector . INTERNAL_NAME , internalLocalAddress ) ; var host = internalInitializer . address ( ) . getHostname ( ) ; var port = internalLocalAddress . getPort ( ) ; log . info ( "Created Bolt connector with id {} and port {}" , host , port ) ; } }
public void test() { try { return authorizationIntraCloudRepository . findAll ( PageRequest . of ( validatedPage , validatedSize , validatedDirection , validatedSortField ) ) ; } catch ( final Exception ex ) { logger . debug ( ex . getMessage ( ) , ex ) ; throw new ArrowheadException ( CoreCommonConstants . DATABASE_OPERATION_EXCEPTION_MSG ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; } catch ( final ProviderException e ) { LOG . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { onFinish ( ) ; } catch ( final Exception e ) { logger . warn ( "Failed to perform tasks when enumerator was finished" , e ) ; } }
public void test() { if ( child instanceof UIPortlet < ? , ? > ) { return targetContainer . hasMoveAppsPermission ( ) ; } else-if ( child instanceof org . exoplatform . portal . webui . container . UIContainer ) { return targetContainer . hasMoveContainersPermission ( ) ; } else-if ( child instanceof org . exoplatform . portal . webui . page . UIPageBody ) { return true ; } else { log . warn ( "Unexpected content type: " + child . getClass ( ) . getName ( ) ) ; return false ; } }
public void test() { -> { log . error ( "Forced failover by {}" , user ) ; abort . abort ( AbortReason . MANUAL , Optional . of ( new RuntimeException ( String . format ( "Forced failover by %s" , user . getId ( ) ) ) ) ) ; } }
@ Test public void createDatasetVersionTest ( ) { CreateDatasetVersion createDatasetVersionRequest = getDatasetVersionRequest ( dataset . getId ( ) ) ; CreateDatasetVersion . Response createDatasetVersionResponse = datasetVersionServiceStub . createDatasetVersion ( createDatasetVersionRequest ) ; DatasetVersion datasetVersion2 = createDatasetVersionResponse . getDatasetVersion ( ) ; LOGGER . info ( "CreateDatasetVersion created successfully" ) ; assertEquals ( "DatasetVersion datsetId not match with expected DatasetVersion datsetId" , dataset . getId ( ) , datasetVersion2 . getDatasetId ( ) ) ; DeleteDatasetVersion deleteDatasetVersionRequest = DeleteDatasetVersion . newBuilder ( ) . setId ( datasetVersion2 . getId ( ) ) . build ( ) ; DeleteDatasetVersion . Response deleteDatasetVersionResponse = datasetVersionServiceStub . deleteDatasetVersion ( deleteDatasetVersionRequest ) ; LOGGER . info ( "DeleteDatasetVersion deleted successfully" ) ; LOGGER . info ( deleteDatasetVersionResponse . toString ( ) ) ; createDatasetVersionRequest = getDatasetVersionRequest ( dataset . getId ( ) ) ; createDatasetVersionRequest = createDatasetVersionRequest . toBuilder ( ) . setDatasetBlob ( CommitTest . getDatasetBlobFromPath ( "datasetVersion" ) . getDataset ( ) ) . build ( ) ; createDatasetVersionResponse = datasetVersionServiceStub . createDatasetVersion ( createDatasetVersionRequest ) ; DatasetVersion datasetVersion = createDatasetVersionResponse . getDatasetVersion ( ) ; LOGGER . info ( "CreateDatasetVersion Response : \n" + datasetVersion ) ; assertEquals ( "DatasetVersion datsetId not match with expected DatasetVersion datsetId" , createDatasetVersionRequest . getDatasetBlob ( ) , datasetVersion . getDatasetBlob ( ) ) ; deleteDatasetVersionRequest = DeleteDatasetVersion . newBuilder ( ) . setId ( datasetVersion . getId ( ) ) . build ( ) ; deleteDat
@ Test public void createDatasetVersionTest ( ) { CreateDatasetVersion createDatasetVersionRequest = getDatasetVersionRequest ( dataset . getId ( ) ) ; CreateDatasetVersion . Response createDatasetVersionResponse = datasetVersionServiceStub . createDatasetVersion ( createDatasetVersionRequest ) ; DatasetVersion datasetVersion2 = createDatasetVersionResponse . getDatasetVersion ( ) ; LOGGER . info ( "CreateDatasetVersion Response : \n" + datasetVersion2 ) ; assertEquals ( "DatasetVersion datsetId not match with expected DatasetVersion datsetId" , dataset . getId ( ) , datasetVersion2 . getDatasetId ( ) ) ; DeleteDatasetVersion deleteDatasetVersionRequest = DeleteDatasetVersion . newBuilder ( ) . setId ( datasetVersion2 . getId ( ) ) . build ( ) ; DeleteDatasetVersion . Response deleteDatasetVersionResponse = datasetVersionServiceStub . deleteDatasetVersion ( deleteDatasetVersionRequest ) ; LOGGER . info ( "DeleteDatasetVersion deleted successfully" ) ; LOGGER . info ( deleteDatasetVersionResponse . toString ( ) ) ; createDatasetVersionRequest = getDatasetVersionRequest ( dataset . getId ( ) ) ; createDatasetVersionRequest = createDatasetVersionRequest . toBuilder ( ) . setDatasetBlob ( CommitTest . getDatasetBlobFromPath ( "datasetVersion" ) . getDataset ( ) ) . build ( ) ; createDatasetVersionResponse = datasetVersionServiceStub . createDatasetVersion ( createDatasetVersionRequest ) ; DatasetVersion datasetVersion = createDatasetVersionResponse . getDatasetVersion ( ) ; LOGGER . info ( "CreateDatasetVersion Response : \n" + datasetVersion . toString ( ) ) ; assertEquals ( "DatasetVersion datsetId not match with expected DatasetVersion datsetId" , createDatasetVersionRequest . getDatasetBlob ( ) , datasetVersion . getDatasetBlob ( ) ) ; deleteDatasetVersionRequest = DeleteDatasetVersion . newBuilder ( ) . setId ( datasetVersion . get
public void test() { try { ManagementFactory . getPlatformMBeanServer ( ) . unregisterMBean ( MoodleUserProviderFactory . getObjectName ( pid ) ) ; } catch ( Exception e ) { logger . warn ( "Unable to unregister mbean for pid='{}': {}" , pid , e . getMessage ( ) ) ; } }
public void test() { try { sleeper . sleepFor ( retryIntervalMs , TimeUnit . MILLISECONDS ) ; } catch ( InterruptedException e ) { logger . debug ( "Sleep interrupted" , e ) ; Thread . currentThread ( ) . interrupt ( ) ; return false ; } }
public void test() { if ( sourceFileName instanceof Text ) { return sourceFileName . toString ( ) ; } else { LOGGER . warn ( MessageFormat . format ( this . getActionExecution ( ) . getAction ( ) . getType ( ) + " does not accept {0} as type for sourceFileName" , sourceFileName . getClass ( ) ) ) ; return sourceFileName . toString ( ) ; } }
public void test() { try { resolved = identityResolver . resolveSubject ( subject , IDENTITY_TYPES , credentialName ) ; } catch ( Exception e ) { log . info ( "Authentication failed: " + e . getMessage ( ) ) ; return new AuthenticationResult ( Status . deny , null ) ; } }
public void test() { if ( ! valid ) { log . debug ( "Authentication denied" ) ; return new AuthenticationResult ( Status . deny , null ) ; } }
public void test() { try { String dbCredential = resolved . getCredentialValue ( ) ; OTPCredentialDBState credState = JsonUtil . parse ( dbCredential , OTPCredentialDBState . class ) ; boolean valid = TOTPCodeVerificator . verifyCode ( code , credState . secret , credState . otpParams , credentialConfig . allowedTimeDriftSteps ) ; code_block = IfStatement ; AuthenticatedEntity ae = new AuthenticatedEntity ( resolved . getEntityId ( ) , subject , credState . outdated ? resolved . getCredentialName ( ) : null ) ; return new AuthenticationResult ( Status . success , ae ) ; } catch ( Exception e ) { log . debug ( "Error validating authentication" , e ) ; return new AuthenticationResult ( Status . deny , null ) ; } }
public void test() { try { ldapService . stop ( ) ; service . shutdown ( ) ; code_block = IfStatement ; } catch ( Exception e ) { LOG . error ( "Failed to close LDAP service" , e ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { String unitId = O2SDKManager . companion . instance ( ) . prefs ( ) . getString ( O2 . INSTANCE . getPRE_BIND_UNIT_ID_KEY ( ) , "" ) ; XLog . debug ( vo . toString ( ) ) ; BBSCollectionRealmObject object = new BBSCollectionRealmObject ( ) ; object . setId ( vo . getId ( ) ) ; object . setSectionName ( vo . getSectionName ( ) ) ; object . setSectionIcon ( vo . getSectionIcon ( ) ) ; object . setCreateTime ( vo . getCreateTime ( ) ) ; object . setUnitId ( unitId ) ; realm . copyToRealmOrUpdate ( object ) ; return true ; } catch ( Exception e ) { XLog . error ( "" , e ) ; return false ; } }
public void test() { try { response = getResponse ( urlString , timeoutSeconds ) ; HttpEntity entity = getEntity ( response ) ; InputStream content = entity . getContent ( ) ; code_block = TryStatement ;  } catch ( Exception e ) { _log . error ( "Error handling url: " + urlString , e ) ; throw e ; } finally { code_block = IfStatement ; } }
public static List < String > getVMCommand ( Configuration conf , ApplicationId appid , PSAttemptId psAttemptId ) { Vector < String > vargs = new Vector < String > ( 8 ) ; vargs . add ( Environment . JAVA_HOME . $ ( ) + "/bin/java" ) ; String javaOpts = getChildJavaOpts ( conf , appid , psAttemptId ) ; LOG . info ( "javaOpts=" + javaOpts ) ; String [ ] javaOptsSplit = javaOpts . split ( " " ) ; code_block = ForStatement ; Path childTmpDir = new Path ( Environment . PWD . $ ( ) , YarnConfiguration . DEFAULT_CONTAINER_TEMP_DIR ) ; vargs . add ( "-Djava.io.tmpdir=" + childTmpDir ) ; long logSize = 0 ; setupLog4jProperties ( conf , vargs , logSize ) ; String psClassName = conf . get ( AngelConf . ANGEL_PS_CLASS , AngelConf . DEFAULT_ANGEL_PS_CLASS ) ; vargs . add ( psClassName ) ; vargs . add ( "1>" + getTaskLogFile ( TaskLog . LogName . STDOUT ) ) ; vargs . add ( "2>" + getTaskLogFile ( TaskLog . LogName . STDERR ) ) ; StringBuilder mergedCommand = new StringBuilder ( ) ; code_block = ForStatement ; Vector < String > vargsFinal = new Vector < String > ( 1 ) ; vargsFinal . add ( mergedCommand . toString ( ) ) ; return vargsFinal ; }
public void test() { { LOG . error ( "submitAndSchedule is not supported on Server.Please run your operation on Prism " ) ; throw FalconWebException . newAPIException ( "submitAndSchedule is not supported on Server. Please run your " + "operation on Prism." ) ; } }
public void persist ( StgG20Scl transientInstance ) { log . debug ( "persisting StgG20Scl instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . persist ( transientInstance ) ; log . debug ( "persist successful" ) ; } catch ( RuntimeException re ) { log . error ( "persist failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . persist ( transientInstance ) ; log . debug ( "persist successful" ) ; } catch ( RuntimeException re ) { log . error ( "persist failed" , re ) ; throw re ; } }
public void test() { try { Mockito . when ( callback . awaitCompletion ( ) ) . thenReturn ( callback ) ; } catch ( InterruptedException e ) { LOG . warn ( "Interrupted while setting up mocks" , e ) ; } }
public void test() { try { CnATreeElement element = getElement ( uuid ) ; code_block = IfStatement ; } catch ( Exception t ) { LOG . error ( "Error while loading comment" , t ) ; } }
public void test() { try { gitPath = Paths . get ( FILE_SEPARATOR ) . relativize ( gitPath ) ; } catch ( IllegalArgumentException e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { Response response = service . generate ( size , algorithm ) ; jwksTO = response . readEntity ( OIDCJWKSTO . class ) ; } catch ( Exception ge ) { LOGGER . log ( ge ) ; } }
public void test() { if ( e . getType ( ) == ClientExceptionType . NotFound ) { code_block = TryStatement ;  } else { LOGGER . error ( "Could not delete {}" , objectName , e ) ; } }
@ After public void tearDown ( ) { log . info ( "Tearing down..." ) ; engine . freeTableId ( ) ; engine . clear ( ) ; TestUtils . removeTestPath ( root ) ; configOverrideMaxUncommittedRows = - 1 ; configOverrideCommitLag = - 1 ; currentMicros = - 1 ; }
public void test() { if ( logger . isTraceEnabled ( LogMarker . BRIDGE_SERVER_VERBOSE ) ) { logger . trace ( LogMarker . BRIDGE_SERVER_VERBOSE , "oplog magic code_block = ForStatement ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( logger . isTraceEnabled ( LogMarker . BRIDGE_SERVER_VERBOSE ) ) { logger . trace ( LogMarker . BRIDGE_SERVER_VERBOSE , "oplog magic code_block = ForStatement ; } }
@ Override public void initialize ( ) { LOG . info ( "Trying to start the Application Server" ) ; container . start ( ) ; registerProperties ( ) ; LOG . info ( "FHIR instance running at {}" , getServiceBaseURL ( ) ) ; }
@ Override public void initialize ( ) { LOG . info ( "Trying to start the FHIR container" ) ; container . start ( ) ; registerProperties ( ) ; LOG . info ( "Trying to start the FHIR container" ) ; }
public void test() { try { String recipientUser = recipientMailAddress . getLocalPart ( ) . toLowerCase ( Locale . US ) ; String recipientHost = recipientMailAddress . getDomain ( ) . asString ( ) ; code_block = IfStatement ; List < String > nets = new ArrayList < > ( ) ; code_block = TryStatement ;  NetMatcher matcher = new NetMatcher ( nets , dns ) ; boolean matched = matcher . matchInetNetwork ( mail . getRemoteAddr ( ) ) ; code_block = IfStatement ; return matched ; } catch ( SQLException sqle ) { LOGGER . error ( "Exception thrown" , sqle ) ; throw new MessagingException ( "Exception thrown" , sqle ) ; } finally { theJDBCUtil . closeJDBCConnection ( conn ) ; } }
public void test() { try { final HttpResponse response = post ( String . format ( "/jobs/%s?action=start" , jobId ) , null ) ; return checkTaskStatus ( response ) ; } catch ( final IOException e ) { LOG . error ( "Failed to start task {}" , jobId , e ) ; checkResponseTimeOut ( e ) ; } }
public void test() { try { List < String > rsList = client . getChildren ( ) . forPath ( replicaSetRoot ) ; List < Integer > rsIDList = Lists . transform ( rsList , new Function < String , Integer > ( ) code_block = "" ; ) ; int currMaxID = - 1 ; code_block = IfStatement ; int newReplicaSetID = currMaxID + 1 ; rs . setReplicaSetID ( newReplicaSetID ) ; String replicaSetPath = ZKPaths . makePath ( replicaSetRoot , String . valueOf ( newReplicaSetID ) ) ; client . create ( ) . creatingParentsIfNeeded ( ) . forPath ( replicaSetPath , serializeReplicaSet ( rs ) ) ; writeSuccess . getAndIncrement ( ) ; logger . info ( "Set the replicaSet id {} to {}" , rs , newReplicaSetID ) ; return newReplicaSetID ; } catch ( Exception e ) { writeFail . getAndIncrement ( ) ; logger . error ( "Error when create replicaSet " + rs , e ) ; throw new StoreException ( e ) ; } }
public void test() { try { List < String > rsList = client . getChildren ( ) . forPath ( replicaSetRoot ) ; List < Integer > rsIDList = Lists . transform ( rsList , new Function < String , Integer > ( ) code_block = "" ; ) ; int currMaxID = - 1 ; code_block = IfStatement ; int newReplicaSetID = currMaxID + 1 ; logger . trace ( "Id of new replica set {} is {}." , rs , newReplicaSetID ) ; rs . setReplicaSetID ( newReplicaSetID ) ; String replicaSetPath = ZKPaths . makePath ( replicaSetRoot , String . valueOf ( newReplicaSetID ) ) ; client . create ( ) . creatingParentsIfNeeded ( ) . forPath ( replicaSetPath , serializeReplicaSet ( rs ) ) ; writeSuccess . getAndIncrement ( ) ; return newReplicaSetID ; } catch ( Exception e ) { writeFail . getAndIncrement ( ) ; logger . error ( "write replicaSet:{} error:" , replicaSetRoot , e ) ; throw new StoreException ( e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
@ Override public void handleError ( @ Nullable Throwable cause ) { Closeables . closeQuietly ( responseInfo ) ; LOG . error ( "Response with error {}" , cause . getMessage ( ) ) ; }
@ Override public void stop ( Platform platform ) throws Exception { code_block = IfStatement ; doneFuture . complete ( "" ) ; executor . shutdownNow ( ) ; executor . awaitTermination ( 1 , TimeUnit . DAYS ) ; LOG . info ( "Stopping Kafka consumer" ) ; consumer . close ( ) ; this . consumer = null ; this . executor = null ; this . statusUpdater = null ; this . statusUpdaterFuture = null ; this . workerStatus = null ; this . doneFuture = null ; }
public void test() { try { opeList . addAll ( opeManager . getOperationAdm ( ) ) ; } catch ( OHServiceException ex ) { LOG . error ( "Unable to get operation adm" , ex ) ; } }
@ Override public void start ( ) { int numCores = Runtime . getRuntime ( ) . availableProcessors ( ) ; int numWorkerThreads = Integer . getInteger ( ZOOKEEPER_COMMIT_PROC_NUM_WORKER_THREADS , numCores ) ; workerShutdownTimeoutMS = Long . getLong ( ZOOKEEPER_COMMIT_PROC_SHUTDOWN_TIMEOUT , 5000 ) ; initBatchSizes ( ) ; code_block = IfStatement ; LOG . info ( "Cookies are started." ) ; stopped = false ; stoppedMainLoop = false ; super . start ( ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { @ SuppressWarnings ( "rawtypes" ) final GenericDao dao = GenericDaoBase . getDao ( job . getClass ( ) ) ; code_block = IfStatement ; publishOnEventBus ( job , "submit" ) ; code_block = IfStatement ; code_block = TryStatement ;  } catch ( Exception e ) { String errMsg = "Unable to schedule async job for command " + job . getCmd ( ) + ", unexpected exception." ; s_logger . error ( errMsg , e ) ; throw new CloudRuntimeException ( errMsg ) ; } }
public void test() { for ( MetricSlice slice : slices ) { logger . info ( "skipping metric {}" , slice ) ; } }
public void test() { try { log . debug ( "Executing python 'logoutExternalUrl' method" ) ; PersonAuthenticationType externalAuthenticator = ( PersonAuthenticationType ) customScriptConfiguration . getExternalType ( ) ; Map < String , SimpleCustomProperty > configurationAttributes = customScriptConfiguration . getConfigurationAttributes ( ) ; return externalAuthenticator . getLogoutExternalUrl ( configurationAttributes , requestParameters ) ; } catch ( Exception ex ) { log . error ( ex . getMessage ( ) , ex ) ; saveScriptError ( customScriptConfiguration . getCustomScript ( ) , ex ) ; } }
public void test() { try { log . trace ( "Executing python 'getLogouExternalUrl' authenticator method" ) ; PersonAuthenticationType externalAuthenticator = ( PersonAuthenticationType ) customScriptConfiguration . getExternalType ( ) ; Map < String , SimpleCustomProperty > configurationAttributes = customScriptConfiguration . getConfigurationAttributes ( ) ; return externalAuthenticator . getLogoutExternalUrl ( configurationAttributes , requestParameters ) ; } catch ( Exception ex ) { log . error ( ex . getMessage ( ) , ex ) ; saveScriptError ( customScriptConfiguration . getCustomScript ( ) , ex ) ; } }
public void test() { try { userGroups . add ( ( UserGroup ) READER . read ( uri , user , new UserGroup ( ) ) ) ; } catch ( ImejiException e ) { LOGGER . error ( "Error reading user group" , e ) ; } }
public void test() { try { sourceInfo . getCurAsyncClient ( RaftServer . getReadOperationTimeoutMS ( ) ) . fetchSingleSeries ( sourceInfo . getHeader ( ) , sourceInfo . getReaderId ( ) , handler ) ; fetchResult . wait ( RaftServer . getReadOperationTimeoutMS ( ) ) ; } catch ( TException e ) { code_block = IfStatement ; return fetchResultAsync ( ) ; } catch ( InterruptedException e ) { Thread . currentThread ( ) . interrupt ( ) ; logger . warn ( "Query {} interrupted" , sourceInfo ) ; return null ; } }
@ Test public void testGenerateGroupIdWithEntity ( ) { app = ApplicationBuilder . newManagedApp ( EntitySpec . create ( TestApplication . class ) . displayName ( "TistApp" ) , LocalManagementContextForTests . newInstance ( ) ) ; TestEntity child = app . createAndManageChild ( EntitySpec . create ( TestEntity . class ) . displayName ( "TestEnt" ) ) ; ConfigBag cfg = new ConfigBag ( ) . configure ( CloudLocationConfig . CALLER_CONTEXT , child ) ; String result = new BasicCloudMachineNamer ( ) . generateNewGroupId ( cfg ) ; log . info ( result ) ; Assert . assertTrue ( result . length ( ) <= 60 ) ; String user = Strings . maxlen ( System . getProperty ( "user.name" ) , 4 ) . toLowerCase ( ) ; Assert . assertTrue ( result . indexOf ( user ) >= 0 ) ; Assert . assertTrue ( result . indexOf ( "-tistapp-" ) >= 0 ) ; Assert . assertTrue ( result . indexOf ( "-testent-" ) >= 0 ) ; Assert . assertTrue ( result . indexOf ( "-" + Strings . maxlen ( app . getId ( ) , 4 ) . toLowerCase ( ) ) >= 0 ) ; Assert . assertTrue ( result . indexOf ( "-" + Strings . maxlen ( child . getId ( ) , 4 ) . toLowerCase ( ) ) >= 0 ) ; }
public void test() { if ( _logger . isWarnEnabled ( ) ) { _logger . warn ( ex . toString ( ) , ex ) ; } }
public boolean dropRole ( String roleName ) throws SentryClientException { String dropRoleStmt = "drop role " + roleName ; logger . info ( "Dropping role " + roleName ) ; this . sentryJdbcTemplate . execute ( dropRoleStmt ) ; return true ; }
public void test() { try { code_block = ForStatement ; code_block = ForStatement ; code_block = ForStatement ; code_block = ForStatement ; code_block = ForStatement ; } catch ( RuntimeException ex ) { log . error ( "Exception:" , ex ) ; } }
@ Scheduled ( fixedDelayString = "${statistics.key.interval.delay:3600000}" ) private void setLatestStatisticsTimeline ( ) { LOG . info ( "Setting the statistics timeline" ) ; Map < StatisticsEnum , StatisticsTimeline > latestStatisticsTimelineMap = new HashMap < StatisticsEnum , StatisticsTimeline > ( ) ; code_block = ForStatement ; code_block = IfStatement ; LOG . info ( "Latest statistics timeline map is set" ) ; }
@ Override public String getLogoutRedirectionUrl ( WebContext context ) { init ( ) ; final String state = RandomStringUtils . randomAlphanumeric ( 10 ) ; final String postLogoutRedirectUri = this . appConfiguration . getOpenIdPostLogoutRedirectUri ( ) ; String idToken = ( String ) context . getSessionAttribute ( getName ( ) + SESSION_ID_TOKEN_PARAMETER ) ; code_block = IfStatement ; final EndSessionRequest endSessionRequest = new EndSessionRequest ( idToken , postLogoutRedirectUri , state ) ; final String redirectionUrl = this . openIdConfiguration . getEndSessionEndpoint ( ) + "?" + endSessionRequest . getQueryString ( ) ; logger . debug ( "Redirection URL: {}" , redirectionUrl ) ; return redirectionUrl ; }
public void test() { try { PersistentBundle persistentBundle = ModuleUtils . persist ( bundle ) ; String newLocation = persistentBundle . getLocation ( ) ; logger . info ( "Persisting " + newLocation ) ; return newLocation ; } catch ( ModuleManagementException e ) { throw e ; } catch ( Exception e ) { String msg = "Unable to transform bundle " + bundle + ". Cause: " + e . getMessage ( ) ; logger . error ( msg , e ) ; throw new ModuleManagementException ( msg , e ) ; } }
public void test() { try { PersistentBundle persistentBundle = ModuleUtils . persist ( bundle ) ; String newLocation = persistentBundle . getLocation ( ) ; logger . info ( "Transformed bundle {} with location {} to be handled by the DX protocol handler under new location {}" , getDisplayName ( bundle ) , bundle . getLocation ( ) , newLocation ) ; return newLocation ; } catch ( ModuleManagementException e ) { throw e ; } catch ( Exception e ) { String msg = "Unable to transform bundle " + bundle + ". Cause: " + e . getMessage ( ) ; logger . error ( msg , e ) ; throw new ModuleManagementException ( msg , e ) ; } }
public void test() { if ( getLogger ( ) . isDebugEnabled ( ) ) { getLogger ( ) . debug ( "Shutting down GeoServer" ) ; } }
public void test() { try { List < ResultsGroup > queryResults = searchService . query ( user , project , query , document , annotationLayer , annotationFeature , first , count ) . entrySet ( ) . stream ( ) . map ( e -> new ResultsGroup ( e . getKey ( ) , e . getValue ( ) ) ) . collect ( Collectors . toList ( ) ) ; pagesCacheModel . getObject ( ) . putPage ( first , count , queryResults ) ; return queryResults . iterator ( ) ; } catch ( IOException | ExecutionException e ) { logger . error ( "Exception while executing query" , e ) ; return emptyIterator ( ) ; } }
public void test() { try { return JAXBContext . newInstance ( TemplateListModel . class ) ; } catch ( JAXBException e ) { logger . error ( "Unable to create template list model" , e ) ; return null ; } }
public void test() { try { atomUri = new URI ( uri ) ; code_block = ForStatement ; } catch ( URISyntaxException e ) { logger . debug ( "Could not parse atom URI: {}" , uri ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( ParseException e ) { Logger . log ( e ) ; } }
public void test() { if ( duration < 0 || duration >= 255 ) { logger . debug ( "Invalid duration {}" , duration ) ; return ZigBeeStatus . INVALID_ARGUMENTS ; } }
@ Test public void testInsufficientArgsLogsErrorForWarn ( ) { String format = "some message: %s, %d" ; String param = "blah" ; logger . error ( format , param ) ; assertLogLike ( Level . SEVERE , ImmutableList . of ( "Invalid format" , "WARN" , format , param ) , IllegalArgumentException . class ) ; assertLog ( Level . WARNING , String . format ( "'%s' [%s]" , format , param ) ) ; }
public void test() { try { user = userManager . saveUser ( user ) ; } catch ( AccessDeniedException ade ) { getResponse ( ) . sendError ( HttpServletResponse . SC_FORBIDDEN ) ; LOG . info ( ade . getMessage ( ) ) ; return null ; } catch ( UserExistsException e ) { addMessage ( "errors.existing.user" , new Object [ ] code_block = "" ; ) ; user . setPassword ( user . getConfirmPassword ( ) ) ; return null ; } }
public void test() { catch ( SQLException | RuntimeException e ) { LOGGER . error ( "Unable to connect to hostname:" , e ) ; hostnameValue . addErrorMessage ( "Unable to connect: " + e . getMessage ( ) ) ; } }
public void test() { if ( nativeDecoding ) { LOG . info ( "Native decoding is enabled to " + inboundName ) ; } else { LOG . info ( "Native decoding is disabled to " + inboundName + ". Inbound message conversion done by Spring Cloud Stream." ) ; } }
public void test() { if ( nativeDecoding ) { LOG . info ( "Native decoding is enabled to " + inboundName + ". Inbound deserialization done at the broker." ) ; } else { LOG . info ( "Native decoding is disabled to " + inboundName + ". Inbound deserialization done at the broker." ) ; } }
public void test() { try { LOG . info ( "connection invalidated" ) ; m_impl . close ( ) ; LOG . info ( "connection invalidated" ) ; } finally { m_impl = null ; } }
public void test() { if ( _log . isWarnEnabled ( ) ) { _log . warn ( StringBundler . concat ( "Unable to find a JMX bean with id " , serviceId , " in " , contextId ) ) ; } }
public void test() { if ( r != null ) { LOGGER . info ( "remove pool success" ) ; } }
public void test() { try { connectManager ( ) ; ManagerResponse r = con . sendAction ( action ) ; code_block = IfStatement ; return ( r instanceof ManagerError ) ? null : r ; } catch ( Exception e ) { log . error ( "Error sending action:" + action , e ) ; } }
public void test() { try { responseData = post ( url , headers , requestData ) ; } catch ( Exception e ) { logger . error ( "Http request failed" , e ) ; } }
public void test() { if ( error . isJsonPrimitive ( ) ) { logger . debug ( "Remote exception occurred: '{}':'{}': '{}'" , error . toString ( ) , error . toString ( ) , error . toString ( ) ) ; } else-if ( error . isJsonObject ( ) ) { JsonObject o = error . getAsJsonObject ( ) ; Integer code = ( o . has ( "code" ) ? o . get ( "code" ) . getAsInt ( ) : null ) ; String message = ( o . has ( "message" ) ? o . get ( "message" ) . getAsString ( ) : null ) ; String data = ( o . has ( "data" ) ? ( o . get ( "data" ) instanceof JsonObject ? o . get ( "data" ) . toString ( ) : o . get ( "data" ) . getAsString ( ) ) : null ) ; logger . debug ( "A remote exception occurred: '{}':'{}':'{}'" , code , message , data ) ; } else { logger . debug ( "An unknown remote exception occurred: '{}'" , error . toString ( ) ) ; } }
public void test() { if ( error . isJsonPrimitive ( ) ) { logger . debug ( "A remote exception occurred: '{}'" , error . getAsString ( ) ) ; } else-if ( error . isJsonObject ( ) ) { JsonObject o = error . getAsJsonObject ( ) ; Integer code = ( o . has ( "code" ) ? o . get ( "code" ) . getAsInt ( ) : null ) ; String message = ( o . has ( "message" ) ? o . get ( "message" ) . getAsString ( ) : null ) ; String data = ( o . has ( "data" ) ? ( o . get ( "data" ) instanceof JsonObject ? o . get ( "data" ) . toString ( ) : o . get ( "data" ) . getAsString ( ) ) : null ) ; logger . debug ( "An unexpected exception occurred: '{}'" , error . getAsString ( ) ) ; } else { logger . debug ( "An unknown remote exception occurred: '{}'" , error . toString ( ) ) ; } }
public void test() { if ( error . isJsonPrimitive ( ) ) { logger . debug ( "A remote exception occurred: '{}'" , error . getAsString ( ) ) ; } else-if ( error . isJsonObject ( ) ) { JsonObject o = error . getAsJsonObject ( ) ; Integer code = ( o . has ( "code" ) ? o . get ( "code" ) . getAsInt ( ) : null ) ; String message = ( o . has ( "message" ) ? o . get ( "message" ) . getAsString ( ) : null ) ; String data = ( o . has ( "data" ) ? ( o . get ( "data" ) instanceof JsonObject ? o . get ( "data" ) . toString ( ) : o . get ( "data" ) . getAsString ( ) ) : null ) ; logger . debug ( "A remote exception occurred: '{}':'{}':'{}'" , code , message , data ) ; } else { logger . error ( "Unexpected exception" , error ) ; } }
public void test() { if ( ! wonMessage . getForwardedMessageURIs ( ) . isEmpty ( ) ) { logger . debug ( "getForwardedMessageURIs:{}" , wonMessage . getForwardedMessageURIs ( ) ) ; } }
public void test() { try { successConsumer . accept ( event . getMessage ( ) ) ; } catch ( Exception e ) { LOG . error ( e . getMessage ( ) ) ; } }
public void test() { try { errorConsumer . accept ( exception ) ; } catch ( Exception e ) { log . error ( String . format ( "%s" , LogUtil . getStackTrace ( e ) ) ) ; } }
public void test() { try { payload = message . getObject ( ) ; code_block = IfStatement ; } catch ( JMSException ex ) { LOG . error ( ex . getMessage ( ) , ex ) ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( "Closing connection {}" , connection ) ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( "Closing connection {}" , connection ) ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( "Closing connection {}" , connection ) ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( "Closing connection {}" , connection ) ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( "Closing connection {}" , connection ) ; } }
public void test() { try { Class < ? > aClass = Class . forName ( className , false , TaskExecutors . class . getClassLoader ( ) ) ; Method method = aClass . getDeclaredMethod ( methodName , parameterTypes ) ; method . setAccessible ( true ) ; return method ; } catch ( Exception e ) { LOG . warn ( "Failed to look up task executor for {}" , className , e ) ; return null ; } }
@ Activate public void activate ( ComponentContext cc ) throws Exception { logger . info ( "Activating " + SecurityConstants . GLOBAL_ADMIN_USER_PROPERTY ) ; BundleContext bundleCtx = cc . getBundleContext ( ) ; adminUserName = StringUtils . trimToNull ( bundleCtx . getProperty ( SecurityConstants . GLOBAL_ADMIN_USER_PROPERTY ) ) ; adminPassword = StringUtils . trimToNull ( bundleCtx . getProperty ( OPT_ADMIN_PASSWORD ) ) ; adminEmail = StringUtils . trimToNull ( bundleCtx . getProperty ( OPT_ADMIN_EMAIL ) ) ; adminRoles = StringUtils . trimToNull ( bundleCtx . getProperty ( OPT_ADMIN_ROLES ) ) ; code_block = IfStatement ; componentCtx = cc ; code_block = ForStatement ; }
public void test() { if ( DEFAULT_ADMIN_PASSWORD_CONFIGURATION . equals ( adminPassword ) ) { LOGGER . info ( "Using default admin password." ) ; } }
public void test() { try { result = SchemaMethodResource . wrapInResult ( typedIdResource . getEntity ( ) . invokeMethod ( securityContext , methodName , propertySet , true , new EvaluationHints ( ) ) ) ; } catch ( Throwable t ) { logger . warn ( "" , t ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
protected void init ( StartupStrategy startupStrategy ) { code_block = ForStatement ; logger . info ( "Configured '{}' server state repository" , this . repository . getClass ( ) . getSimpleName ( ) ) ; this . context = new KieServerRegistryImpl ( ) ; this . context . registerIdentityProvider ( new JACCIdentityProvider ( ) ) ; this . context . registerStateRepository ( repository ) ; ContainerLocatorProvider . get ( ) ; ContainerManager containerManager = getContainerManager ( ) ; KieServerState currentState = repository . load ( KieServerEnvironment . getServerId ( ) ) ; List < KieServerExtension > extensions = sortKnownExtensions ( ) ; code_block = ForStatement ; policyManager = new PolicyManager ( ) ; policyManager . start ( this , context ) ; kieServerActive . set ( true ) ; eventSupport . fireBeforeServerStarted ( this ) ; startTimestamp = System . currentTimeMillis ( ) ; startupStrategy . startup ( this , containerManager , currentState , kieServerActive ) ; eventSupport . fireAfterServerStarted ( this ) ; logger . info ( "Started {} server state in {} ms" , System . currentTimeMillis ( ) - startTimestamp , System . currentTimeMillis ( ) - startTimestamp ) ; }
protected void init ( StartupStrategy startupStrategy ) { logger . info ( "Selected startup strategy {}" , startupStrategy ) ; code_block = ForStatement ; this . context = new KieServerRegistryImpl ( ) ; this . context . registerIdentityProvider ( new JACCIdentityProvider ( ) ) ; this . context . registerStateRepository ( repository ) ; ContainerLocatorProvider . get ( ) ; ContainerManager containerManager = getContainerManager ( ) ; KieServerState currentState = repository . load ( KieServerEnvironment . getServerId ( ) ) ; List < KieServerExtension > extensions = sortKnownExtensions ( ) ; code_block = ForStatement ; policyManager = new PolicyManager ( ) ; policyManager . start ( this , context ) ; kieServerActive . set ( true ) ; eventSupport . fireBeforeServerStarted ( this ) ; startTimestamp = System . currentTimeMillis ( ) ; startupStrategy . startup ( this , containerManager , currentState , kieServerActive ) ; eventSupport . fireAfterServerStarted ( this ) ; logger . info ( "KieServer started: {}" , System . currentTimeMillis ( ) - startTimestamp ) ; }
public void test() { if ( extension . isInitialized ( ) ) { logger . debug ( "{} has been registered as server extension" , extension ) ; } else { logger . warn ( "{} has not been registered as server extension" , extension ) ; } }
public void test() { if ( extension . isInitialized ( ) ) { logger . info ( "{} has been successfully registered as server extension" , extension ) ; } else { logger . warn ( "{} does not exist as server extension" , extension ) ; } }
public void test() { try { extension . init ( this , this . context ) ; this . context . registerServerExtension ( extension ) ; code_block = IfStatement ; } catch ( Exception e ) { serverMessages . add ( new Message ( Severity . ERROR , "Error when initializing server extension of type " + extension + " due to " + e . getMessage ( ) ) ) ; logger . error ( "Error when initializing server extension of type " + extension , e ) ; } }
public void test() { try { BooleanQuery . setMaxClauseCount ( propertyService . get ( LUCENE_BOOLEAN_QUERY_MAX_CLAUSE_COUNT ) ) ; } catch ( PropertyServiceIncompleteRegistrationException e ) { LOGGER . error ( String . format ( "Property service error: %s" , e . getMessage ( ) ) ) ; } catch ( NullPointerException e ) { LOGGER . warn ( String . format ( "Property service is null, boolean query max clause count value is set to %d" , BooleanQuery . getMaxClauseCount ( ) ) ) ; } }
public void test() { try { BooleanQuery . setMaxClauseCount ( propertyService . get ( LUCENE_BOOLEAN_QUERY_MAX_CLAUSE_COUNT ) ) ; } catch ( PropertyServiceIncompleteRegistrationException e ) { LOGGER . warn ( String . format ( "Property boolean query max clause count doesn't exists, value is set to %d" , BooleanQuery . getMaxClauseCount ( ) ) ) ; } catch ( NullPointerException e ) { LOGGER . warn ( "" , e ) ; } }
private JSONObject getTweets ( ) throws IOException { currentRequest ++ ; String url = getApiURL ( lastMaxID - 1 ) ; Document doc = Http . url ( url ) . ignoreContentType ( ) . header ( "Authorization" , "Bearer " + accessToken ) . header ( "Content-Type" , "application/x-www-form-urlencoded;charset=UTF-8" ) . header ( "User-agent" , "ripe and zipe" ) . get ( ) ; String body = doc . body ( ) . html ( ) . replaceAll ( "&quot;" , "\"" ) ; Object jsonObj = new JSONTokener ( body ) . nextValue ( ) ; JSONArray statuses ; code_block = IfStatement ; logger . debug ( "tweets: {}" , jsonObj ) ; JSONObject r = new JSONObject ( ) ; r . put ( "tweets" , statuses ) ; return r ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { java . util . List < com . liferay . portal . reports . engine . console . model . Entry > returnValue = EntryServiceUtil . getEntries ( groupId , definitionName , userName , createDateGT , createDateLT , andSearch , start , end , orderByComparator ) ; return com . liferay . portal . reports . engine . console . model . EntrySoap . toSoapModels ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
@ Test public void intervalTest2 ( ) throws BackendException { String [ ] [ ] values = generateValues ( ) ; log . info ( "Loading values..." ) ; loadValues ( values ) ; newTx ( ) ; Set < KeyColumn > deleted = deleteValues ( 7 ) ; clopen ( ) ; checkRandomSlices ( values , deleted ) ; }
public void test() { try { bindingPOA . deactivate_object ( objectId ) ; } catch ( ObjectNotActive ona ) { logger . info ( objectId + " object not active" ) ; } catch ( Exception ex ) { throw new CorbaBindingException ( "Unable to deactivate CORBA servant" , ex ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getCapabilities()" ) ; } }
public void test() { try { addEids ( patientsBuilder , eids ) ; addIds ( patientsBuilder , ids ) ; final String json = this . objectMapper . writeValueAsString ( patientsBuilder . build ( ) ) ; return Response . ok ( json , MediaType . APPLICATION_JSON_TYPE ) . build ( ) ; } catch ( final JsonProcessingException ex ) { this . slf4Jlogger . error ( "Failed to process patients with external ids [{}]: {}" , ex . getMessage ( ) , ex ) ; return Response . status ( Response . Status . INTERNAL_SERVER_ERROR ) . build ( ) ; } catch ( final QueryException ex ) { this . slf4Jlogger . error ( "Failed to retrieve patients with external ids [{}]: {}" , eids , ex . getMessage ( ) ) ; return Response . status ( Response . Status . INTERNAL_SERVER_ERROR ) . build ( ) ; } }
public void test() { try { addEids ( patientsBuilder , eids ) ; addIds ( patientsBuilder , ids ) ; final String json = this . objectMapper . writeValueAsString ( patientsBuilder . build ( ) ) ; return Response . ok ( json , MediaType . APPLICATION_JSON_TYPE ) . build ( ) ; } catch ( final JsonProcessingException ex ) { this . slf4Jlogger . error ( "Failed to serialize patients [{}] to JSON: {}" , eids , ex . getMessage ( ) ) ; return Response . status ( Response . Status . INTERNAL_SERVER_ERROR ) . build ( ) ; } catch ( final QueryException ex ) { this . slf4Jlogger . error ( "Failed to query patients: {}" , ex . getMessage ( ) ) ; return Response . status ( Response . Status . INTERNAL_SERVER_ERROR ) . build ( ) ; } }
public void test() { if ( isDebug ) { log . error ( basicMsg + ": " + msg ) ; } else { log . error ( basicMsg + ": " + msg ) ; code_block = IfStatement ; } }
public void test() { if ( printStack ) { getLogger ( ) . log ( Level . SEVERE , "Stack trace" , new Exception ( "Stacktrace" ) ) ; } }
public void test() { if ( _log . isInfoEnabled ( ) ) { _log . info ( StringBundler . concat ( "Removing " , companyId , " from company " , companyId , " to " , company . getCompanyId ( ) , " using " , company . getCompanyId ( ) ) ) ; } }
@ Override public void initialize ( IAsyncResultHandler < Void > startupHandler ) { log . info ( "Initializing EBRegistryProxy" ) ; this . proxy = new EBRegistryProxy ( vertx , address ( ) , registryUuid ) ; listenProxyHandler ( startupHandler ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( AssetTagServiceUtil . class , "mergeTags" , _mergeTagsParameterTypes21 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , fromTagIds , toTagId ) ; code_block = TryStatement ;  } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
@ Override public ODistributedTxContext popTxContext ( final ODistributedRequestId requestId ) { final ODistributedTxContext ctx = activeTxContexts . remove ( requestId ) ; LOG . trace ( "popTxContext {}" , ctx ) ; return ctx ; }
public void trace ( final String message , final Throwable error ) { log . trace ( message , error ) ; }
public void test() { if ( getLogger ( ) . isDebugEnabled ( ) ) { getLogger ( ) . debug ( "Shutting down GeoServer" ) ; } }
public void test() { if ( getLogger ( ) . isDebugEnabled ( ) ) { getLogger ( ) . debug ( "Shutting down GeoServer" ) ; } }
public void test() { if ( getLogger ( ) . isDebugEnabled ( ) ) { getLogger ( ) . debug ( "Shutting down GeoServer" ) ; } }
public void test() { try { updatedVariant . merge ( webPushVariant ) ; validateModelClass ( webPushVariant ) ; } catch ( ConstraintViolationException cve ) { logger . debug ( "Unable to update WebPush variant" ) ; logger . debug ( "Details: {}" , cve ) ; Response . ResponseBuilder builder = createBadRequestResponse ( cve . getConstraintViolations ( ) ) ; return builder . build ( ) ; } }
@ Override public void doExecute ( TestContext context ) { log . info ( "Done" ) ; }
public void test() { try { querier . maybeStartQuery ( cachedConnectionProvider . getConnection ( ) ) ; int batchMaxRows = config . getInt ( JdbcSourceTaskConfig . BATCH_MAX_ROWS_CONFIG ) ; boolean hadNext = true ; code_block = WhileStatement ; code_block = IfStatement ; code_block = IfStatement ; log . debug ( "Returning {} records for {}" , results . size ( ) , querier . toString ( ) ) ; return results ; } catch ( SQLException sqle ) { log . error ( "Failed to run query for table {}: {}" , querier . toString ( ) , sqle ) ; resetAndRequeueHead ( querier ) ; return null ; } catch ( Throwable t ) { resetAndRequeueHead ( querier ) ; closeResources ( ) ; log . warn ( "Query failed" , t ) ; throw t ; } }
public void test() { if ( Collections . min ( consecutiveEmptyResults . values ( ) ) >= CONSECUTIVE_EMPTY_RESULTS_BEFORE_RETURN ) { logger . debug ( "Returning empty collection for query {}" , query ) ; return null ; } else { continue ; } }
public void test() { try { log . debug ( "Checking for next block of results from {}" , querier . toString ( ) ) ; querier . maybeStartQuery ( cachedConnectionProvider . getConnection ( ) ) ; int batchMaxRows = config . getInt ( JdbcSourceTaskConfig . BATCH_MAX_ROWS_CONFIG ) ; boolean hadNext = true ; code_block = WhileStatement ; code_block = IfStatement ; code_block = IfStatement ; return results ; } catch ( SQLException sqle ) { log . error ( "Failed to run query for table {}: {}" , querier . toString ( ) , sqle ) ; resetAndRequeueHead ( querier ) ; return null ; } catch ( Throwable t ) { resetAndRequeueHead ( querier ) ; log . warn ( "Query failed" , t ) ; closeResources ( ) ; throw t ; } }
public void test() { try { log . debug ( "Checking for next block of results from {}" , querier . toString ( ) ) ; querier . maybeStartQuery ( cachedConnectionProvider . getConnection ( ) ) ; int batchMaxRows = config . getInt ( JdbcSourceTaskConfig . BATCH_MAX_ROWS_CONFIG ) ; boolean hadNext = true ; code_block = WhileStatement ; code_block = IfStatement ; code_block = IfStatement ; log . debug ( "Returning {} records for {}" , results . size ( ) , querier . toString ( ) ) ; return results ; } catch ( SQLException sqle ) { resetAndRequeueHead ( querier ) ; return null ; } catch ( Throwable t ) { resetAndRequeueHead ( querier ) ; log . warn ( "Error while executing fetching records" , t ) ; closeResources ( ) ; throw t ; } }
@ Override public void delete ( ScriptKey scriptKey ) { LOGGER . trace ( MessageFormat . format ( "Deleting Script {0}." , scriptKey . toString ( ) ) ) ; ScriptVersionKey scriptVersionKey = new ScriptVersionKey ( new ScriptKey ( scriptKey . getScriptId ( ) , scriptKey . getScriptVersion ( ) ) ) ; ScriptVersionConfiguration . getInstance ( ) . delete ( scriptVersionKey ) ; ActionConfiguration . getInstance ( ) . deleteByScript ( scriptKey ) ; ScriptParameterConfiguration . getInstance ( ) . deleteByScript ( scriptKey ) ; ScriptLabelConfiguration . getInstance ( ) . deleteByScript ( scriptKey ) ; getDeleteStatement ( scriptKey ) . ifPresent ( getMetadataRepository ( ) :: executeUpdate ) ; }
public void test() { try { capabilities = liveJvmService . getCapabilities ( agentId ) ; } catch ( AgentNotConnectedException e ) { logger . debug ( e . getMessage ( ) , e ) ; return "{\"agentNotConnected\":true}" ; } }
public void test() { try { CloudFileDirectory dir = rootDir . getDirectoryReference ( note . getId ( ) ) ; dir . createIfNotExists ( ) ; CloudFile cloudFile = dir . getFileReference ( "note.json" ) ; cloudFile . uploadFromByteArray ( buffer , 0 , buffer . length ) ; } catch ( URISyntaxException | StorageException e ) { String msg = String . format ( "Error saving notebook %s to Azure storage" , note . getId ( ) ) ; LOGGER . error ( msg , e ) ; throw new IOException ( msg , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { for ( AuthorityRefList . AuthorityRefItem item : items ) { logger . debug ( testName + ": list-item[" + i + "] URI=" + item . getUri ( ) ) ; logger . debug ( testName + ": list-item[" + i + "] refName=" + item . getRefName ( ) ) ; logger . debug ( testName + ": list-item[" + i + "] URI=" + item . getUri ( ) ) ; i ++ ; } }
public void test() { for ( AuthorityRefList . AuthorityRefItem item : items ) { logger . debug ( testName + ": list-item[" + i + "] Field:" + item . getSourceField ( ) + " =" + " item display name = " + item . getAuthDisplayName ( ) + " auth display name = " + item . getItemDisplayName ( ) ) ; logger . debug ( testName + ": list-item[" + i + "] refName=" + item . getRefName ( ) ) ; logger . debug ( testName + ": list-item[" + i + "] URI=" + item . getUri ( ) ) ; i ++ ; } }
public void test() { for ( AuthorityRefList . AuthorityRefItem item : items ) { logger . debug ( testName + ": list-item[" + i + "] Field:" + item . getSourceField ( ) + " =" + " item display name = " + item . getAuthDisplayName ( ) + " auth display name = " + item . getItemDisplayName ( ) ) ; logger . debug ( testName + ": list-item[" + i + "] refName=" + item . getRefName ( ) ) ; logger . debug ( testName + ": list-item[" + i + "] refName=" + item . getRefName ( ) ) ; i ++ ; } }
public void test() { for ( String transportName : crucialExceptions . keySet ( ) ) { JournalException e = crucialExceptions . get ( transportName ) ; logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { if ( aEvt . getSeverityLevel ( ) . equals ( SeverityLevel . WARNING ) ) { log . warn ( sb . toString ( ) ) ; } else-if ( aEvt . getSeverityLevel ( ) . equals ( SeverityLevel . INFO ) ) { log . info ( sb . toString ( ) ) ; } else { log . error ( sb . toString ( ) ) ; } }
public void test() { if ( aEvt . getSeverityLevel ( ) . equals ( SeverityLevel . WARNING ) ) { log . warn ( sb . toString ( ) ) ; } else-if ( aEvt . getSeverityLevel ( ) . equals ( SeverityLevel . INFO ) ) { log . info ( sb . toString ( ) ) ; } else { log . error ( sb . toString ( ) ) ; } }
public void test() { if ( aEvt . getSeverityLevel ( ) . equals ( SeverityLevel . WARNING ) ) { log . warn ( sb . toString ( ) ) ; } else-if ( aEvt . getSeverityLevel ( ) . equals ( SeverityLevel . INFO ) ) { log . info ( sb . toString ( ) ) ; } else { log . error ( sb . toString ( ) ) ; } }
public void test() { try { value = URLDecoder . decode ( elements [ 1 ] , "UTF8" ) ; } catch ( UnsupportedEncodingException e ) { logger . error ( e . getMessage ( ) , e ) ; } }
@ Test @ Ignore public void ensureInternalEndpointIsSecured ( ) throws Throwable { final String connectorTasksEndpoint = connect . endpointForResource ( String . format ( "connectors/%s/tasks" , CONNECTOR_NAME ) ) ; final Map < String , String > emptyHeaders = new HashMap < > ( ) ; final Map < String , String > invalidSignatureHeaders = new HashMap < > ( ) ; invalidSignatureHeaders . put ( SIGNATURE_HEADER , "S2Fma2Flc3F1ZQ==" ) ; invalidSignatureHeaders . put ( SIGNATURE_ALGORITHM_HEADER , "HmacSHA256" ) ; assertEquals ( BAD_REQUEST . getStatusCode ( ) , connect . requestPost ( connectorTasksEndpoint , "[]" , emptyHeaders ) . getStatus ( ) ) ; log . info ( "Making a POST request to the {} endpoint started and an invalid signature header; " + "expecting 403 error response" , connectorTasksEndpoint ) ; assertEquals ( FORBIDDEN . getStatusCode ( ) , connect . requestPost ( connectorTasksEndpoint , "[]" , invalidSignatureHeaders ) . getStatus ( ) ) ; Map < String , String > connectorProps = new HashMap < > ( ) ; connectorProps . put ( CONNECTOR_CLASS_CONFIG , MonitorableSinkConnector . class . getSimpleName ( ) ) ; connectorProps . put ( TASKS_MAX_CONFIG , String . valueOf ( 1 ) ) ; connectorProps . put ( TOPICS_CONFIG , "test-topic" ) ; connectorProps . put ( KEY_CONVERTER_CLASS_CONFIG , StringConverter . class . getName ( ) ) ; connectorProps . put ( VALUE_CONVERTER_CLASS_CONFIG , StringConverter . class . getName ( ) ) ; log . info ( "Starting the {} connector" , CONNECTOR_NAME ) ; StartAndStopLatch startLatch = connectorHandle . expectedStarts ( 1 ) ; connect . configureConnector ( CONNECTOR_NAME , connectorProps ) ; log . info ( "{} connector started" , CONNECTOR_SETUP_DURATION_MS ) ; startLatch . await ( CON
@ Test @ Ignore public void ensureInternalEndpointIsSecured ( ) throws Throwable { final String connectorTasksEndpoint = connect . endpointForResource ( String . format ( "connectors/%s/tasks" , CONNECTOR_NAME ) ) ; final Map < String , String > emptyHeaders = new HashMap < > ( ) ; final Map < String , String > invalidSignatureHeaders = new HashMap < > ( ) ; invalidSignatureHeaders . put ( SIGNATURE_HEADER , "S2Fma2Flc3F1ZQ==" ) ; invalidSignatureHeaders . put ( SIGNATURE_ALGORITHM_HEADER , "HmacSHA256" ) ; log . info ( "Making a POST request to the {} endpoint started and no signature header; " + "expecting 400 error response" , connectorTasksEndpoint ) ; assertEquals ( BAD_REQUEST . getStatusCode ( ) , connect . requestPost ( connectorTasksEndpoint , "[]" , emptyHeaders ) . getStatus ( ) ) ; assertEquals ( FORBIDDEN . getStatusCode ( ) , connect . requestPost ( connectorTasksEndpoint , "[]" , invalidSignatureHeaders ) . getStatus ( ) ) ; Map < String , String > connectorProps = new HashMap < > ( ) ; connectorProps . put ( CONNECTOR_CLASS_CONFIG , MonitorableSinkConnector . class . getSimpleName ( ) ) ; connectorProps . put ( TASKS_MAX_CONFIG , String . valueOf ( 1 ) ) ; connectorProps . put ( TOPICS_CONFIG , "test-topic" ) ; connectorProps . put ( KEY_CONVERTER_CLASS_CONFIG , StringConverter . class . getName ( ) ) ; connectorProps . put ( VALUE_CONVERTER_CLASS_CONFIG , StringConverter . class . getName ( ) ) ; log . info ( "Starting the {} connector" , CONNECTOR_NAME ) ; StartAndStopLatch startLatch = connectorHandle . expectedStarts ( 1 ) ; connect . configureConnector ( CONNECTOR_NAME , connectorProps ) ; startLatch . await ( CONNECTOR_SETUP_DURATION_MS , TimeUnit . MILLISECONDS ) ; startLatch
@ Test @ Ignore public void ensureInternalEndpointIsSecured ( ) throws Throwable { final String connectorTasksEndpoint = connect . endpointForResource ( String . format ( "connectors/%s/tasks" , CONNECTOR_NAME ) ) ; final Map < String , String > emptyHeaders = new HashMap < > ( ) ; final Map < String , String > invalidSignatureHeaders = new HashMap < > ( ) ; invalidSignatureHeaders . put ( SIGNATURE_HEADER , "S2Fma2Flc3F1ZQ==" ) ; invalidSignatureHeaders . put ( SIGNATURE_ALGORITHM_HEADER , "HmacSHA256" ) ; log . info ( "Making a POST request to the {} endpoint with no connector started and no signature header; " + "expecting 400 error response" , connectorTasksEndpoint ) ; assertEquals ( BAD_REQUEST . getStatusCode ( ) , connect . requestPost ( connectorTasksEndpoint , "[]" , emptyHeaders ) . getStatus ( ) ) ; log . info ( "Making a POST request to the {} endpoint with no connector started and an invalid signature header; " + "expecting 403 error response" , connectorTasksEndpoint ) ; assertEquals ( FORBIDDEN . getStatusCode ( ) , connect . requestPost ( connectorTasksEndpoint , "[]" , invalidSignatureHeaders ) . getStatus ( ) ) ; Map < String , String > connectorProps = new HashMap < > ( ) ; connectorProps . put ( CONNECTOR_CLASS_CONFIG , MonitorableSinkConnector . class . getSimpleName ( ) ) ; connectorProps . put ( TASKS_MAX_CONFIG , String . valueOf ( 1 ) ) ; connectorProps . put ( TOPICS_CONFIG , "test-topic" ) ; connectorProps . put ( KEY_CONVERTER_CLASS_CONFIG , StringConverter . class . getName ( ) ) ; connectorProps . put ( VALUE_CONVERTER_CLASS_CONFIG , StringConverter . class . getName ( ) ) ; StartAndStopLatch startLatch = connectorHandle . expectedStarts ( 1 ) ; connect . configureConnector ( CONNECTOR_NAME , connectorProps ) ; startLatch . put ( connector
@ Test @ Ignore public void ensureInternalEndpointIsSecured ( ) throws Throwable { final String connectorTasksEndpoint = connect . endpointForResource ( String . format ( "connectors/%s/tasks" , CONNECTOR_NAME ) ) ; final Map < String , String > emptyHeaders = new HashMap < > ( ) ; final Map < String , String > invalidSignatureHeaders = new HashMap < > ( ) ; invalidSignatureHeaders . put ( SIGNATURE_HEADER , "S2Fma2Flc3F1ZQ==" ) ; invalidSignatureHeaders . put ( SIGNATURE_ALGORITHM_HEADER , "HmacSHA256" ) ; log . info ( "Making a POST request to the {} endpoint with no connector started and no signature header; " + "expecting 400 error response" , connectorTasksEndpoint ) ; assertEquals ( BAD_REQUEST . getStatusCode ( ) , connect . requestPost ( connectorTasksEndpoint , "[]" , emptyHeaders ) . getStatus ( ) ) ; log . info ( "Making a POST request to the {} endpoint with no connector started and an invalid signature header; " + "expecting 403 error response" , connectorTasksEndpoint ) ; assertEquals ( FORBIDDEN . getStatusCode ( ) , connect . requestPost ( connectorTasksEndpoint , "[]" , invalidSignatureHeaders ) . getStatus ( ) ) ; Map < String , String > connectorProps = new HashMap < > ( ) ; connectorProps . put ( CONNECTOR_CLASS_CONFIG , MonitorableSinkConnector . class . getSimpleName ( ) ) ; connectorProps . put ( TASKS_MAX_CONFIG , String . valueOf ( 1 ) ) ; connectorProps . put ( TOPICS_CONFIG , "test-topic" ) ; connectorProps . put ( KEY_CONVERTER_CLASS_CONFIG , StringConverter . class . getName ( ) ) ; connectorProps . put ( VALUE_CONVERTER_CLASS_CONFIG , StringConverter . class . getName ( ) ) ; log . info ( "Starting the {} connector" , CONNECTOR_NAME ) ; StartAndStopLatch startLatch = connectorHandle . expectedStarts ( 1 ) ; connect . stop ( )
@ Test @ Ignore public void ensureInternalEndpointIsSecured ( ) throws Throwable { final String connectorTasksEndpoint = connect . endpointForResource ( String . format ( "connectors/%s/tasks" , CONNECTOR_NAME ) ) ; final Map < String , String > emptyHeaders = new HashMap < > ( ) ; final Map < String , String > invalidSignatureHeaders = new HashMap < > ( ) ; invalidSignatureHeaders . put ( SIGNATURE_HEADER , "S2Fma2Flc3F1ZQ==" ) ; invalidSignatureHeaders . put ( SIGNATURE_ALGORITHM_HEADER , "HmacSHA256" ) ; log . info ( "Making a POST request to the {} endpoint with no connector started and no signature header; " + "expecting 400 error response" , connectorTasksEndpoint ) ; assertEquals ( BAD_REQUEST . getStatusCode ( ) , connect . requestPost ( connectorTasksEndpoint , "[]" , emptyHeaders ) . getStatus ( ) ) ; log . info ( "Making a POST request to the {} endpoint with no connector started and an invalid signature header; " + "expecting 403 error response" , connectorTasksEndpoint ) ; assertEquals ( FORBIDDEN . getStatusCode ( ) , connect . requestPost ( connectorTasksEndpoint , "[]" , invalidSignatureHeaders ) . getStatus ( ) ) ; Map < String , String > connectorProps = new HashMap < > ( ) ; connectorProps . put ( CONNECTOR_CLASS_CONFIG , MonitorableSinkConnector . class . getSimpleName ( ) ) ; connectorProps . put ( TASKS_MAX_CONFIG , String . valueOf ( 1 ) ) ; connectorProps . put ( TOPICS_CONFIG , "test-topic" ) ; connectorProps . put ( KEY_CONVERTER_CLASS_CONFIG , StringConverter . class . getName ( ) ) ; connectorProps . put ( VALUE_CONVERTER_CLASS_CONFIG , StringConverter . class . getName ( ) ) ; log . info ( "Starting the {} connector" , CONNECTOR_NAME ) ; StartAndStopLatch startLatch = connectorHandle . expectedStarts ( 1 ) ; connect . stop ( )
public void test() { if ( failureCount > MAXIMUM_FAILURE_COUNT ) { LOGGER . error ( "Failed {} processing {} times. Mail is requeued with increased failure count." , mail . getName ( ) , failureCount ) ; storeInErrorRepository ( queueItem ) ; } else { LOGGER . error ( "Failed {} processing {} consecutive times. Mail is requeued with increased failure count." , mail . getName ( ) , failureCount , processingException ) ; reEnqueue ( queueItem , failureCount ) ; } }
public void test() { if ( failureCount > MAXIMUM_FAILURE_COUNT ) { LOGGER . error ( "Failed {} processing {} consecutive times. Abort. Mail is saved in {}" , mail . getName ( ) , failureCount , configuration . getErrorRepositoryURL ( ) . asString ( ) ) ; storeInErrorRepository ( queueItem ) ; } else { LOGGER . warn ( "Failed {} processing {} consecutive times. Abort. Mail is saved in {}" , mail . getName ( ) , failureCount , configuration . getErrorRepositoryURL ( ) . asString ( ) ) ; reEnqueue ( queueItem , failureCount ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception nestedE ) { LOGGER . error ( nestedE . getMessage ( ) , nestedE ) ; nack ( queueItem , processingException ) ; } }
public void test() { if ( ! isPortForwarded ( vmName , port ) ) { LOGGER . info ( "Executing VBoxManage controlvm" ) ; commandExecutor . exec ( String . format ( "VBoxManage controlvm " + vmName + " natpf1 %d,tcp,127.0.0.1,%d,,%d" , port , port , port ) ) ; } }
public void test() { try { Set < IPAddress > servers = getConfiguredDnsServers ( netInterfaceConfig ) ; logger . trace ( "{} is WAN, adding its dns servers: {}" , netInterfaceConfig . getName ( ) , servers ) ; serverList . addAll ( servers ) ; } catch ( KuraException e ) { logger . error ( "Cannot get configured DNS Servers" , e ) ; } }
public void test() { if ( testDriver != null && ! setUpClassCalled ) { LOGGER . info ( "Setting up class called for TestDriver" ) ; testDriver . setUpClass ( ) ; setUpClassCalled = true ; } }
public void test() { try { dos = new DataOutputStream ( new FileOutputStream ( f ) ) ; dos . writeBytes ( OperationResultPanel . this . getModel ( ) . getObject ( ) . getXml ( ) ) ; } catch ( IOException e ) { LOGGER . error ( "Unable to write file" , e ) ; } finally { IOUtils . closeQuietly ( dos ) ; } }
public void test() { if ( _logger . isDebugEnabled ( ) ) { _logger . debug ( "[" + _handler . getClass ( ) . getName ( ) + "] DirectPersistencyListHandler[" + toString ( ) + "]" ) ; } }
public void test() { try { String encryptedData = encryptionService . encryptData ( decryptedData , null ) ; return encryptedData ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; throw new ProjectCommonException ( ResponseCode . SERVER_ERROR . getErrorCode ( ) , ResponseCode . userDataEncryptionError . getErrorMessage ( ) , ResponseCode . userDataEncryptionError . getResponseCode ( ) ) ; } }
@ Override public void afterConnectionEstablished ( Session session ) throws Exception { LOGGER . info ( "AfterConnectionEstablished" ) ; afterConnectionEstablishedLatch . countDown ( ) ; }
public void test() { try { BlogsPortletInstanceConfiguration blogsPortletInstanceConfiguration = portletDisplay . getPortletInstanceConfiguration ( BlogsPortletInstanceConfiguration . class ) ; code_block = IfStatement ; } catch ( ConfigurationException configurationException ) { _log . error ( "Unable to get BlogsPortletInstanceConfiguration blogs portlet instance" , configurationException ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "SQL : " + query ) ; LOG . debug ( "SQL.param.system : " + system ) ; LOG . debug ( "SQL.param.test : " + test ) ; LOG . debug ( "SQL.param.testcase : " + testcase ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "SQL : " + query . toString ( ) ) ; LOG . debug ( "SQL.param.test : " + test ) ; LOG . debug ( "SQL.param.country : " + country ) ; LOG . debug ( "SQL.param.testcase : " + testcase ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "SQL : " + query . toString ( ) ) ; LOG . debug ( "SQL.param.system : " + system ) ; LOG . debug ( "SQL.param.testcase : " + testcase ) ; LOG . debug ( "SQL.param.country : " + country ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "SQL : " + query . toString ( ) ) ; LOG . debug ( "SQL.param.system : " + system ) ; LOG . debug ( "SQL.param.test : " + test ) ; LOG . debug ( "SQL.param.country : " + country ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( SQLException e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try { Transaction transaction = tracer . currentTransaction ( ) ; code_block = IfStatement ; } catch ( Exception e ) { LOGGER . log ( Level . FINE , e . getMessage ( ) , e ) ; } }
public void test() { try { code_block = WhileStatement ; buffer . mark ( ) ; buffer . compact ( ) ; } catch ( final BufferUnderflowException ex ) { this . logger . warn ( "Unexpected buffer underflow. Resetting and compacting buffer." , ex ) ; buffer . reset ( ) ; buffer . compact ( ) ; } }
public void test() { try { doProcess ( auditMessage ) ; } catch ( CTTransactionException ctTransactionException ) { throw ctTransactionException ; } catch ( Exception exception ) { _log . error ( "Error processing audit message" , exception ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { try { ConfigurationParameters config = securityProvider . getConfiguration ( AuthorizationConfiguration . class ) . getParameters ( ) ; supportedPaths = CugUtil . getSupportedPaths ( config , mountInfoProvider ) ; importBehavior = CugUtil . getImportBehavior ( config ) ; code_block = IfStatement ; initialized = true ; } catch ( RepositoryException e ) { log . error ( "Error while initialization" , e ) ; } }
protected void informAdministrator ( String errorMessage ) { code_block = IfStatement ; _informAdministrator = false ; StringBundler sb = new StringBundler ( 7 ) ; sb . append ( "Liferay does not have the Xuggler native libraries " ) ; sb . append ( "installed. In order to generate video and audio previews, " ) ; sb . append ( "please follow the instructions for Xuggler in the Server " ) ; sb . append ( "Administration section of the Control Panel at: " ) ; sb . append ( "http://<server>/group/control_panel/manage/-/server" ) ; sb . append ( "/external-services. Warning: " ) ; sb . append ( errorMessage ) ; _log . warn ( sb . toString ( ) ) ; }
public void test() { try { final ServerSocket socket = new ServerSocket ( configuration . getEmbeddedLdapPort ( ) ) ; socket . close ( ) ; LOG . debug ( "Closed ldap server socket" ) ; return ; } catch ( Exception e ) { LOG . debug ( "Exception trying to acquire the server socket" , e ) ; } }
public void test() { try { final ServerSocket socket = new ServerSocket ( configuration . getEmbeddedLdapPort ( ) ) ; socket . close ( ) ; LOG . info ( "Directory service stopped" ) ; return ; } catch ( Exception e ) { LOG . info ( "Directory service still alive. Waiting for shutdown..." ) ; } }
public void test() { if ( destroyVirtualMachine != null ) { logger . debug ( ">> destroying virtualMachine(%s) job(%s)" , virtualMachineId , destroyVirtualMachine ) ; awaitCompletion ( destroyVirtualMachine ) ; } else { logger . trace ( "<< virtualMachine(%s) not found" , virtualMachineId ) ; } }
public void test() { if ( destroyVirtualMachine != null ) { logger . debug ( ">> destroying virtualMachine(%s) job(%s)" , virtualMachineId , destroyVirtualMachine ) ; awaitCompletion ( destroyVirtualMachine ) ; } else { logger . trace ( "<< virtualMachine(%s) not found" , virtualMachineId ) ; } }
public void test() { try { fileURL = new URL ( file ) ; } catch ( MalformedURLException e ) { log . warn ( "Illegal URL for file: {}" , file ) ; return false ; } }
@ Transactional ( rollbackFor = ArrowheadException . class ) public ServiceDefinitionResponseDTO createServiceDefinitionResponse ( final String serviceDefinition ) { logger . debug ( "createServiceDefinitionResponse started..." ) ; final ServiceDefinition serviceDefinitionEntry = createServiceDefinition ( serviceDefinition ) ; return DTOConverter . convertServiceDefinitionToServiceDefinitionResponseDTO ( serviceDefinitionEntry ) ; }
public void test() { if ( processed % logInterval == 0 ) { LOGGER . info ( "Processed {} nodes" , processed ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getCapabilities()" ) ; } }
public void test() { try { code_block = ForStatement ; } catch ( Exception e ) { String errMsg = "Error encountered in " + CLASSNAME + ": " + e . getLocalizedMessage ( ) + " " ; errMsg = errMsg + "Successfully updated " + numUpdated + " CollectionObject record(s) prior to error." ; LOGGER . error ( errMsg , e ) ; setErrorResult ( errMsg ) ; getResults ( ) . setNumAffected ( numUpdated ) ; return getResults ( ) ; } }
public void test() { try { MimeMessage message = mail . getMessage ( ) ; message . writeTo ( System . err ) ; } catch ( IOException ioe ) { LOGGER . error ( "Error while trying to send message" , ioe ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
static void banner ( String text ) { log . info ( "" ) ; log . info ( repeat ( "" , 100 ) ) ; log . info ( repeat ( "" , 100 ) ) ; log . info ( repeat ( "" , 100 ) ) ; log . info ( "" ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( request . isEnded ( ) ) { log . debug ( "Forwarding request to Mesh" ) ; proxyEndHandler ( forwardRequest , rc . getBody ( ) ) ; } else { request . exceptionHandler ( e -> log . error ( "Could not forward request to Mesh: {}" , e , e . getMessage ( ) ) ) . endHandler ( v -> proxyEndHandler ( forwardRequest , null ) ) ; Pump . pump ( request , forwardRequest ) . setWriteQueueMaxSize ( 8192 ) . start ( ) ; } }
public void test() { try ( final DefaultClient client = factory . getClient ( ) ) { MultivaluedHashMap < String , Object > headers = new MultivaluedHashMap < > ( ) ; List < Object > objectList = new ArrayList < > ( ) ; objectList . add ( pem ) ; headers . put ( GlobalDataRest . X_SSL_CLIENT_CERT , objectList ) ; client . checkStatus ( headers ) ; } catch ( final VitamException e ) { LOGGER . error ( "Error" , e ) ; fail ( "THIS SHOULD NOT RAIZED AN EXCEPTION" ) ; } finally { code_block = TryStatement ;  } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( offer != null ) { code_block = IfStatement ; listOffers . add ( offer ) ; } else { LOG . warn ( "Received offer with nullid: {}" , id ) ; } }
public void test() { try { log . debug ( "Calling transform()" ) ; String url = h2oEndpoint + H2ORestApiUrls . TRANSFORM_WORD2VEC ; JsonObject queryParams = new JsonObject ( ) ; queryParams . addProperty ( "model" , model ) ; queryParams . addProperty ( "words_frame" , inputFrame + ".hex" ) ; queryParams . addProperty ( "aggregate_method" , "AVERAGE" ) ; String response = RestApiHandler . httpQueryParamRequest ( url , queryParams , null , "Get" ) ; JsonObject payload = new Gson ( ) . fromJson ( response , JsonObject . class ) ; JsonObject vectors = payload . get ( "vectors_frame" ) . getAsJsonObject ( ) ; String frameUrl = vectors . get ( "URL" ) . getAsString ( ) ; url = h2oEndpoint + frameUrl ; queryParams = new JsonObject ( ) ; queryParams . addProperty ( "row_count" , rowCount ) ; response = RestApiHandler . httpQueryParamRequest ( url , queryParams , null , "Get" ) ; return getVectorColumns ( response ) ; } catch ( Exception e ) { log . error ( e . getMessage ( ) ) ; throw new InsightsCustomException ( e . getMessage ( ) ) ; } }
public void test() { try { log . debug ( "Transforming frame using Word2Vec algorithm: {} " , model ) ; String url = h2oEndpoint + H2ORestApiUrls . TRANSFORM_WORD2VEC ; JsonObject queryParams = new JsonObject ( ) ; queryParams . addProperty ( "model" , model ) ; queryParams . addProperty ( "words_frame" , inputFrame + ".hex" ) ; queryParams . addProperty ( "aggregate_method" , "AVERAGE" ) ; String response = RestApiHandler . httpQueryParamRequest ( url , queryParams , null , "Get" ) ; JsonObject payload = new Gson ( ) . fromJson ( response , JsonObject . class ) ; JsonObject vectors = payload . get ( "vectors_frame" ) . getAsJsonObject ( ) ; String frameUrl = vectors . get ( "URL" ) . getAsString ( ) ; url = h2oEndpoint + frameUrl ; queryParams = new JsonObject ( ) ; queryParams . addProperty ( "row_count" , rowCount ) ; response = RestApiHandler . httpQueryParamRequest ( url , queryParams , null , "Get" ) ; return getVectorColumns ( response ) ; } catch ( Exception e ) { log . error ( e . getMessage ( ) ) ; throw new InsightsCustomException ( e . getMessage ( ) ) ; } }
public void test() { try { userService . save ( user ) ; } catch ( ValidationException e ) { log . error ( "Unable to update user" , e ) ; } }
public void test() { if ( grnTypeCapability != null ) { final Capability capability = grnTypeCapability . capability ; GRN targetGRN ; if ( permissions . stream ( ) . anyMatch ( p code_block = LoopStatement ; code_block = "" ; code_block = IfStatement ; final List < String > updatedPermissions = user . getPermissions ( ) ; updatedPermissions . removeAll ( permissions . stream ( ) . map ( p -> p + ":" + entityID ) . collect ( Collectors . toSet ( ) ) ) ; user . setPermissions ( updatedPermissions ) ; code_block = TryStatement ;  LOG . info ( "Migrating entity <{}> permissions <{}> to <{}> grant for user <{}>" , targetGRN , permissions , capability , user . getName ( ) ) ; } else { LOG . warn ( "Cannot migrate permissions for user <{}>" , user . getName ( ) ) ; } }
private void reportMessageSizes ( final Map < URI , URI > msgUris , String name ) { int [ ] counter = new int [ 4 ] ; Set < URI > keys = msgUris . keySet ( ) ; code_block = ForStatement ; String sizeInfo = "\nSIZES to " + name + ":\n" + "messages=" + counter [ 0 ] + ", named-graphs=" + counter [ 1 ] + ", " + "quads=" + counter [ 2 ] + ", bytes-in-Trig-UTF8=" + counter [ 3 ] ; LOGGER . info ( sizeInfo ) ; }
public void test() { if ( optimizer != null ) { final boolean success = optimizer . optimizeTable ( ) ; log . info ( "OptimizerRunner: The optimizer is '{}'." , success ) ; } else { log . warn ( "OptimizerRunner: The optimizer is null. Could not optimize table." ) ; } }
public void test() { if ( optimizer != null ) { final boolean success = optimizer . optimizeTable ( ) ; log . info ( "Optimization success status [{0}]" , success ) ; } else { log . warn ( "Optimization not found" ) ; } }
public void test() { try { orderedFields = SchemaHelper . getOrderedFieldsByMetaField ( schema , "dbFieldPosition" , new Comparator < String > ( ) code_block = "" ; ) ; int cnt = 0 ; code_block = ForStatement ; return ( T ) new SourceEvent ( keyValuePairs , getPkListFromSchema ( schema ) , schema . getName ( ) , schema . getNamespace ( ) , eventType ) ; } catch ( Exception e ) { log . error ( "Error while getting the source event type" , e ) ; } }
public void test() { try { WikiPageServiceUtil . deletePageAttachments ( nodeId , title ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( ignoredEntities > 0 || prunedEntities > 0 ) { LOG . debug ( "Ignoring {}/{}" , ignoredEntities , prunedEntities ) ; } }
public void test() { if ( StringUtil . isNullOrEmpty ( endpointUri ) ) { LOG . error ( "failed to get endpoint, skipping" ) ; return ; } }
@ Override public StreamDiscoverListsDTO execute ( final Serializable inRequest ) { log . info ( "Regenerating weekday count temp data to " + numberOfDaysOfWeekdayCountDataToGenerate + " days" ) ; repopulateTempWeekdaysSinceDateStrategy . execute ( numberOfDaysOfWeekdayCountDataToGenerate ) ; log . info ( "Finished generating Stream Discovery lists for all users." ) ; StreamDiscoverListsDTO result = new StreamDiscoverListsDTO ( ) ; log . info ( "Generating the list of featured streams" ) ; result . setFeaturedStreams ( featuredStreamDTOMapper . execute ( null ) ) ; log . info ( "Generating the list of most active streams" ) ; result . setMostActiveStreams ( mostActiveStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most viewed streams." ) ; result . setMostViewedStreams ( mostViewedStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most followed streams." ) ; result . setMostFollowedStreams ( mostFollowedStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most recent streams." ) ; result . setMostRecentStreams ( mostRecentStreamsMapper . execute ( null ) ) ; log . info ( "Finished generating Stream Discovery lists for all users." ) ; return result ; }
@ Override public StreamDiscoverListsDTO execute ( final Serializable inRequest ) { log . info ( "Beginning to generate Stream Discovery lists for all users." ) ; repopulateTempWeekdaysSinceDateStrategy . execute ( numberOfDaysOfWeekdayCountDataToGenerate ) ; log . info ( "Generating Stream Discovery lists" ) ; StreamDiscoverListsDTO result = new StreamDiscoverListsDTO ( ) ; log . info ( "Generating the list of featured streams" ) ; result . setFeaturedStreams ( featuredStreamDTOMapper . execute ( null ) ) ; log . info ( "Generating the list of most active streams" ) ; result . setMostActiveStreams ( mostActiveStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most viewed streams." ) ; result . setMostViewedStreams ( mostViewedStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most followed streams." ) ; result . setMostFollowedStreams ( mostFollowedStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most recent streams." ) ; result . setMostRecentStreams ( mostRecentStreamsMapper . execute ( null ) ) ; log . info ( "Finished generating Stream Discovery lists for all users." ) ; return result ; }
@ Override public StreamDiscoverListsDTO execute ( final Serializable inRequest ) { log . info ( "Beginning to generate Stream Discovery lists for all users." ) ; log . info ( "Regenerating weekday count temp data to " + numberOfDaysOfWeekdayCountDataToGenerate + " days" ) ; repopulateTempWeekdaysSinceDateStrategy . execute ( numberOfDaysOfWeekdayCountDataToGenerate ) ; StreamDiscoverListsDTO result = new StreamDiscoverListsDTO ( ) ; log . info ( "Generating Stream Discovery lists" ) ; result . setFeaturedStreams ( featuredStreamDTOMapper . execute ( null ) ) ; log . info ( "Generating the list of most active streams" ) ; result . setMostActiveStreams ( mostActiveStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most viewed streams." ) ; result . setMostViewedStreams ( mostViewedStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most followed streams." ) ; result . setMostFollowedStreams ( mostFollowedStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most recent streams." ) ; result . setMostRecentStreams ( mostRecentStreamsMapper . execute ( null ) ) ; log . info ( "Finished generating Stream Discovery lists for all users." ) ; return result ; }
@ Override public StreamDiscoverListsDTO execute ( final Serializable inRequest ) { log . info ( "Beginning to generate Stream Discovery lists for all users." ) ; log . info ( "Regenerating weekday count temp data to " + numberOfDaysOfWeekdayCountDataToGenerate + " days" ) ; repopulateTempWeekdaysSinceDateStrategy . execute ( numberOfDaysOfWeekdayCountDataToGenerate ) ; StreamDiscoverListsDTO result = new StreamDiscoverListsDTO ( ) ; log . info ( "Generating the list of featured streams" ) ; result . setFeaturedStreams ( featuredStreamDTOMapper . execute ( null ) ) ; log . info ( "Generating the list of most active streams." ) ; result . setMostActiveStreams ( mostActiveStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most viewed streams." ) ; result . setMostViewedStreams ( mostViewedStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most followed streams." ) ; result . setMostFollowedStreams ( mostFollowedStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most recent streams." ) ; result . setMostRecentStreams ( mostRecentStreamsMapper . execute ( null ) ) ; log . info ( "Finished generating Stream Discovery lists for all users." ) ; return result ; }
@ Override public StreamDiscoverListsDTO execute ( final Serializable inRequest ) { log . info ( "Beginning to generate Stream Discovery lists for all users." ) ; log . info ( "Regenerating weekday count temp data to " + numberOfDaysOfWeekdayCountDataToGenerate + " days" ) ; repopulateTempWeekdaysSinceDateStrategy . execute ( numberOfDaysOfWeekdayCountDataToGenerate ) ; log . info ( "Generating Stream Discovery lists" ) ; StreamDiscoverListsDTO result = new StreamDiscoverListsDTO ( ) ; log . info ( "Generating the list of featured streams" ) ; result . setFeaturedStreams ( featuredStreamDTOMapper . execute ( null ) ) ; log . info ( "Generating the list of most active streams" ) ; result . setMostActiveStreams ( mostActiveStreamsMapper . execute ( null ) ) ; result . setMostViewedStreams ( mostViewedStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most followed streams." ) ; result . setMostFollowedStreams ( mostFollowedStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most recent streams." ) ; result . setMostRecentStreams ( mostRecentStreamsMapper . execute ( null ) ) ; log . info ( "Finished generating Stream Discovery lists for all users." ) ; return result ; }
@ Override public StreamDiscoverListsDTO execute ( final Serializable inRequest ) { log . info ( "Beginning to generate Stream Discovery lists for all users." ) ; log . info ( "Regenerating weekday count temp data to " + numberOfDaysOfWeekdayCountDataToGenerate + " days" ) ; repopulateTempWeekdaysSinceDateStrategy . execute ( numberOfDaysOfWeekdayCountDataToGenerate ) ; StreamDiscoverListsDTO result = new StreamDiscoverListsDTO ( ) ; log . info ( "Generating the list of featured streams" ) ; result . setFeaturedStreams ( featuredStreamDTOMapper . execute ( null ) ) ; log . info ( "Generating the list of most active streams" ) ; result . setMostActiveStreams ( mostActiveStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most viewed streams." ) ; result . setMostViewedStreams ( mostViewedStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of mostFollow streams." ) ; result . setMostFollowedStreams ( mostFollowedStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most recent streams." ) ; result . setMostRecentStreams ( mostRecentStreamsMapper . execute ( null ) ) ; log . info ( "Finished generating Stream Discovery lists for all users." ) ; return result ; }
@ Override public StreamDiscoverListsDTO execute ( final Serializable inRequest ) { log . info ( "Beginning to generate Stream Discovery lists for all users." ) ; log . info ( "Regenerating weekday count temp data to " + numberOfDaysOfWeekdayCountDataToGenerate + " days" ) ; repopulateTempWeekdaysSinceDateStrategy . execute ( numberOfDaysOfWeekdayCountDataToGenerate ) ; log . info ( "Generating StreamDiscoverListsDTO..." ) ; StreamDiscoverListsDTO result = new StreamDiscoverListsDTO ( ) ; log . info ( "Generating the list of featured streams" ) ; result . setFeaturedStreams ( featuredStreamDTOMapper . execute ( null ) ) ; log . info ( "Generating the list of most active streams" ) ; result . setMostActiveStreams ( mostActiveStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most viewed streams." ) ; result . setMostViewedStreams ( mostViewedStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most followed streams." ) ; result . setMostFollowedStreams ( mostFollowedStreamsMapper . execute ( null ) ) ; result . setMostRecentStreams ( mostRecentStreamsMapper . execute ( null ) ) ; log . info ( "Finished generating Stream Discovery lists for all users." ) ; return result ; }
@ Override public StreamDiscoverListsDTO execute ( final Serializable inRequest ) { log . info ( "Beginning to generate Stream Discovery lists for all users." ) ; log . info ( "Regenerating weekday count temp data to " + numberOfDaysOfWeekdayCountDataToGenerate + " days" ) ; repopulateTempWeekdaysSinceDateStrategy . execute ( numberOfDaysOfWeekdayCountDataToGenerate ) ; log . info ( "Generating Stream Discovery lists" ) ; StreamDiscoverListsDTO result = new StreamDiscoverListsDTO ( ) ; log . info ( "Generating the list of featured streams" ) ; result . setFeaturedStreams ( featuredStreamDTOMapper . execute ( null ) ) ; log . info ( "Generating the list of most active streams" ) ; result . setMostActiveStreams ( mostActiveStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most viewed streams." ) ; result . setMostViewedStreams ( mostViewedStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most followed streams." ) ; result . setMostFollowedStreams ( mostFollowedStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most recent streams." ) ; result . setMostRecentStreams ( mostRecentStreamsMapper . execute ( null ) ) ; return result ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "User " , user . getUsername ( ) , " already exists" ) ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( KBArticleServiceUtil . class , "getLatestKBArticle" , _getLatestKBArticleParameterTypes26 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , resourcePrimKey , status ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . knowledge . base . model . KBArticle ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceBOMFolderServiceUtil . class , "deleteCommerceBOMFolder" , _deleteCommerceBOMFolderParameterTypes1 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , commerceBOMFolderId ) ; code_block = TryStatement ;  } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
@ Test public void mysqlParse6 ( ) { final String jdbcUrl = CONNECTION_STRING + "?useUnicode=true&characterEncoding=UTF-8&noAccessToProcedureBodies=true&autoDeserialize=true&elideSetAutoCommits=true&sessionVariables=time_zone='%2B09:00',tx_isolation='READ-COMMITTED'" ; DatabaseInfo dbInfo = jdbcUrlParser . parse ( jdbcUrl ) ; Assert . assertTrue ( dbInfo . isParsingComplete ( ) ) ; log . info ( "URL: " + dbInfo ) ; Assert . assertEquals ( dbInfo . getType ( ) , SERVICE_TYPE ) ; Assert . assertEquals ( dbInfo . getHost ( ) . get ( 0 ) , IP_PORT ) ; Assert . assertEquals ( dbInfo . getDatabaseId ( ) , DATABASE_ID ) ; Assert . assertEquals ( dbInfo . getUrl ( ) , CONNECTION_STRING ) ; }
private void logException ( IOException e ) { code_block = IfStatement ; exceptionLogged = true ; LOGGER . log ( Level . SEVERE , e . getMessage ( ) , e ) ; }
public void test() { try { HeartbeatBean bean = heartbeatBeanRef . get ( ) ; code_block = IfStatement ; final HeartbeatPayload hbPayload = new HeartbeatPayload ( ) ; hbPayload . setSystemStartTime ( systemStartTime ) ; hbPayload . setActiveThreadCount ( getActiveThreadCount ( ) ) ; hbPayload . setRevisionUpdateCount ( revisionManager . getRevisionUpdateCount ( ) ) ; final QueueSize queueSize = getTotalFlowFileCount ( bean . getRootGroup ( ) ) ; hbPayload . setTotalFlowFileCount ( queueSize . getObjectCount ( ) ) ; hbPayload . setTotalFlowFileBytes ( queueSize . getByteCount ( ) ) ; hbPayload . setClusterStatus ( clusterCoordinator . getConnectionStatuses ( ) ) ; final NodeIdentifier nodeId = getNodeId ( ) ; code_block = IfStatement ; final Heartbeat heartbeat = new Heartbeat ( nodeId , connectionStatus , hbPayload . marshal ( ) ) ; final HeartbeatMessage message = new HeartbeatMessage ( ) ; message . setHeartbeat ( heartbeat ) ; LOG . debug ( "Generated heartbeat" ) ; return message ; } catch ( final Throwable ex ) { LOG . error ( "Unable to generate Heartbeat request" , ex ) ; return null ; } }
public void persist ( TmpItvSel transientInstance ) { log . debug ( "persisting TmpItvSel instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . persist ( transientInstance ) ; log . debug ( "persist successful" ) ; } catch ( RuntimeException re ) { log . error ( "persist failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . persist ( transientInstance ) ; log . debug ( "persist successful" ) ; } catch ( RuntimeException re ) { log . error ( "persist failed" , re ) ; throw re ; } }
public void test() { try { assertSshable ( machineConfig ) ; Assert . fail ( "ssh should not have succeeded " + machineConfig ) ; } catch ( Exception e ) { log . debug ( "ssh" , e ) ; } }
public void test() { if ( LOGGER . isTraceEnabled ( ) ) { LOGGER . trace ( "Weak listener list status:{}" , System . lineSeparator ( ) ) ; } }
public void test() { try { long requestFlow = ( long ) ( ( ByteBuf ) msg ) . readableBytes ( ) ; counterContainer . addRequestFlow ( requestFlow ) ; } catch ( Throwable e ) { LOGGER . warn ( "Error processing request flow." , e ) ; } finally { ctx . fireChannelRead ( msg ) ; } }
public void test() { { final Integer amount = Optional . ofNullable ( result ) . map ( Collection :: size ) . orElse ( 0 ) ; LOG . debug ( "[{}] Successfully fetched [{}]" , name , amount ) ; } }
public void test() { try { logger . trace ( "Liste Filme lesen von: {}" , source ) ; listeFilme . clear ( ) ; notifyStart ( source ) ; checkDays ( days ) ; code_block = IfStatement ; else processFromFile ( source , listeFilme ) ; } catch ( MalformedURLException ex ) { logger . error ( ex . getMessage ( ) , ex ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { try ( AccessInternalClient client = accessInternalClientFactory . getClient ( ) ) { return client . startTransferReplyWorkflow ( transferReply ) . toResponse ( ) ; } catch ( Exception e ) { LOG . error ( "Error starting transfer reply" , e ) ; return Response . status ( Status . INTERNAL_SERVER_ERROR ) . entity ( getErrorEntity ( Status . INTERNAL_SERVER_ERROR , e . getLocalizedMessage ( ) ) ) . build ( ) ; } }
public void test() { try { return retrieve ( URI . create ( id ) , Imeji . adminUser ) ; } catch ( ImejiException e ) { LOGGER . error ( "Could not create URI" , e ) ; } }
public void test() { try { HiveConf hiveConf = new HiveConf ( ) ; hiveConf . set ( "hive.metastore.uris" , IMConfig . getProperty ( "etl.hive-metastore-uris" ) ) ; hiveConf . set ( "hive.exec.dynamic.partition.mode" , "nonstrict" ) ; hiveConf . set ( "hive.exec.dynamic.partition" , "true" ) ; hiveConf . set ( "hive.exec.max.dynamic.partitions.pernode" , "1000" ) ; hiveClient = new HiveMetaStoreClient ( hiveConf ) ; } catch ( MetaException e ) { LOG . error ( e . getMessage ( ) , e ) ; throw new ETLException ( e ) ; } }
public void test() { if ( DEBUG_ENABLED ) { LOG . debug ( "==> GlossaryService.createBundle()" ) ; } }
public void test() { if ( DEBUG_ENABLED ) { LOG . debug ( "==> GlossaryService.createBundle()" ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { if ( DEBUG_ENABLED ) { LOG . debug ( "==> GlossaryService.createBundle()" ) ; } }
public void test() { if ( ++ windowCount == logWindows ) { long endTime = System . currentTimeMillis ( ) ; totalCount += tupleCount ; windowCount = 0 ; beginTime = System . currentTimeMillis ( ) ; tupleCount = 0 ; log . info ( String . format ( "Time to " + ( endTime - beginTime ) + " ms" ) ) ; } }
public void test() { if ( cache == null ) { return false ; } }
public void test() { try { result . put ( "id" , this . getId ( ) ) ; result . put ( "level" , this . getLevel ( ) ) ; result . put ( "fileDesc" , this . getFileDesc ( ) ) ; result . put ( "fileName" , this . getFileName ( ) ) ; result . put ( "fileType" , this . getFileType ( ) ) ; } catch ( JSONException ex ) { LOG . error ( ex ) ; } }
public void test() { try { return ( NextState ) scannerState . get ( instance ) ; } catch ( final Exception e ) { LOG . error ( "Failed to get next state." , e ) ; exception = true ; } }
private DataObjectAttrExecutor getDayProfileTablePassiveExecutor ( final Set < DayProfileDto > dayProfileSet ) { LOGGER . info ( "Creating day profile table passive" ) ; final AttributeAddress dayProfileTablePassive = new AttributeAddress ( CLASS_ID , OBIS_CODE , ATTRIBUTE_ID_DAY_PROFILE_TABLE_PASSIVE ) ; final DataObject dayArray = DataObject . newArrayData ( this . configurationMapper . mapAsList ( dayProfileSet , DataObject . class ) ) ; return new DataObjectAttrExecutor ( "DAYS" , dayProfileTablePassive , dayArray , CLASS_ID , OBIS_CODE , ATTRIBUTE_ID_DAY_PROFILE_TABLE_PASSIVE ) ; }
public void test() { if ( terminationPointIdInTopology != null ) { InstanceIdentifier < TerminationPoint > iiToTopologyTerminationPoint = provideIIToTopologyTerminationPoint ( terminationPointIdInTopology , iiToNodeInventory ) ; TerminationPoint point = prepareTopologyTerminationPoint ( terminationPointIdInTopology , iiToNodeInventory ) ; sendToTransactionChain ( point , iiToTopologyTerminationPoint ) ; removeLinks ( modification . getRootNode ( ) . getDataAfter ( ) , point ) ; } else { LOG . warn ( "Termination point {} is not present in topology" , ii ) ; } }
public void test() { if ( ! ( nci instanceof NormalizedFieldAndValue ) ) { logger . warn ( "Unexpected NormalizedFieldAndValue type: {}" , nci . getClass ( ) . getName ( ) ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { try { daoManager . startTransaction ( em ) ; result = em . find ( UserTokenEntity . class , token ) ; daoManager . commitTransaction ( em ) ; } catch ( Exception e ) { daoManager . rollBackTransaction ( em ) ; logger . error ( "**** Error in UserTokenDAO:" , e ) ; } finally { daoManager . closeEntityManager ( em ) ; } }
public void test() { for ( String cache : pools ) { code_block = IfStatement ; } }
public void test() { if ( liveMsgContent . equals ( textMessage . getText ( ) ) ) { success . countDown ( ) ; } else { log . info ( "Match " + textMessage . getText ( ) ) ; listenerFailure . set ( true ) ; } }
public void test() { try { TextMessage textMessage = ( TextMessage ) incoming ; code_block = IfStatement ; } catch ( Exception e ) { log . error ( "Caught exception in listener" , e ) ; listenerFailure . set ( true ) ; } }
public void test() { if ( updatedController . canHandleRequest ( request ) ) { return updatedController . processRequest ( request , nextRequestMono ) . doOnError ( throwable -> this . handleException ( throwable ) ) ; } else { log . debug ( "Unable to process request {}" , request ) ; return nextRequestMono ; } }
public void test() { try { Collection collectionToUpdate = collectionService . findByCode ( code ) ; code_block = IfStatement ; return getUIWrapper ( null , true ) ; } catch ( Exception e ) { logger . error ( "Unable to update micromapperEnabled for collection, code = " + code , e ) ; return getUIWrapper ( false , "Unable to update micromapperEnabled for collection, code = " + code ) ; } }
public void test() { try { checkInitialized ( events . get ( 0 ) . getRegion ( ) ) ; } catch ( RuntimeException ex ) { changeFailedEvents ( events . size ( ) ) ; logger . warn ( "Exception occurred while processing events" , ex ) ; return true ; } }
public void test() { if ( isInstall ) { installServerAssembly ( ) ; } else { log . info ( MessageFormat . format ( messages . getString ( "info.install.type.preexisting" ) , "" ) ) ; checkServerHomeExists ( ) ; } }
public void test() { if ( createServer ) { log . info ( MessageFormat . format ( messages . getString ( "info.server.start.create" ) , serverName ) ) ; ServerTask serverTask = initializeJava ( ) ; serverTask . setOperation ( "create" ) ; serverTask . setTemplate ( template ) ; serverTask . setNoPassword ( noPassword ) ; serverTask . execute ( ) ; log . info ( MessageFormat . format ( messages . getString ( "info.server.execute" ) , serverName ) ) ; } }
public void test() { try { item = handler . getPresetContainer ( ) . get ( command . intValue ( ) ) ; postContentItem ( item ) ; } catch ( NoPresetFoundException e ) { logger . debug ( "No preset for {}" , command . intValue ( ) ) ; } }
@ Test public void test_02_long ( ) { Log . debug ( "Test" ) ; long seed = 20100614 ; int lenMask = 0xffff ; int numTests = 10 ; randDnaSeqTest ( numTests , lenMask , seed ) ; }
@ Override public void setFirewallPortForwardingConfiguration ( List < FirewallPortForwardConfigIP < ? extends IPAddress > > firewallConfiguration ) throws KuraException { logger . debug ( "setFirewallPortForwardingConfiguration()" ) ; deleteAllPortForwardRules ( ) ; ArrayList < PortForwardRule > portForwardRules = new ArrayList < > ( ) ; code_block = ForStatement ; addPortForwardRules ( portForwardRules ) ; }
public void test() { try { portForwardEntry . setPermittedNetwork ( getNetworkPair00 ( ) ) ; } catch ( UnknownHostException e ) { logger . warn ( "Failed to configure port forward entry" , e ) ; } }
@ Override public void memberRemoved ( MembershipEvent membershipEvent ) { LOGGER . info ( "list of members now :" + membershipEvent . getMembers ( ) ) ; LOGGER . info ( "list of members now :" + membershipEvent . getMembers ( ) ) ; }
@ Override public void memberRemoved ( MembershipEvent membershipEvent ) { LOGGER . info ( "Member removed: " + membershipEvent . getMember ( ) ) ; LOGGER . info ( "Member removed: " + membershipEvent . getMember ( ) ) ; }
public void test() { if ( metaClient . getFs ( ) . exists ( indexPath ) ) { LOG . warn ( "Index <{}> is deleted." , indexPath ) ; metaClient . getFs ( ) . delete ( indexPath ) ; } }
public void test() { try { String json = dbManager . selectClonedProject ( token ) ; code_block = IfStatement ; code_block = IfStatement ; } catch ( Exception e ) { log . error ( "Error getting cloned project." , e ) ; ctx . writeAndFlush ( serverError ( "Error getting cloned project." ) , ctx . voidPromise ( ) ) ; } }
public void test() { if ( properties . getConnectionProperties ( ) . authenticationType . getValue ( ) != AuthType . ACTIVE_DIRECTORY_CLIENT_CREDENTIAL ) { log . error ( "Authentication denied" , e ) ; throw new ComponentException ( e ) ; } else { return Collections . emptyList ( ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Deleting entity {}" , entity ) ; } }
public void test() { try { PluginLoader loader = PluginLoader . create ( jarPath , _loader ) ; PluginLoader . setClassLoader ( loader ) ; pluginClass = loader . loadPlugin ( ) ; handler = ( AgentServerHandler ) pluginClass . newInstance ( ) ; } catch ( Exception e ) { _log . error ( "Unable to load server handler " + "jar" , e ) ; throw new AgentLoaderException ( "Unable to load server handler " + "jar: " + e . getMessage ( ) ) ; } finally { code_block = IfStatement ; } }
public void test() { try { checkThreads ( ) ; } catch ( Throwable t ) { LOG . warn ( "Error checking threads" , t ) ; } finally { LOG . trace ( "Finished checking threads after {}" , JavaUtils . duration ( start ) ) ; } }
public void test() { try { checkThreads ( ) ; } catch ( Throwable t ) { LOG . error ( "While checking threads" , t ) ; } finally { LOG . debug ( "Exiting" ) ; } }
private void benchmarkAddExtendedDataRows ( Random random , int vertexCount , int extendedDataRowCount ) { double startTime = System . currentTimeMillis ( ) ; code_block = ForStatement ; graph . flush ( ) ; double endTime = System . currentTimeMillis ( ) ; LOGGER . info ( "Add extended data rows in " + ( endTime - startTime ) + " ms" ) ; }
public void test() { try { GSSManager manager = GSSManager . getInstance ( ) ; GSSName peerName = manager . createName ( servicePrincipalName , GSSName . NT_HOSTBASED_SERVICE ) ; GSSContext context = manager . createContext ( peerName , null , null , GSSContext . DEFAULT_LIFETIME ) ; code_block = WhileStatement ; return context ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; return null ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cluster already exists" ) ; } }
public void test() { for ( String body : bodies ) { log . debug ( "Body: " + body ) ; } }
public static void createIndices ( ) { logger . trace ( "Start creating SQL indices" ) ; code_block = TryStatement ;  logger . trace ( "Finished creating SQL indices" ) ; }
public void test() { try ( var connection = PooledDatabaseConnection . INSTANCE . getDataSource ( ) . getConnection ( ) ; var ignored = new SqlAutoSetAutoCommit ( connection , false ) ; var tm = new SqlAutoRollback ( connection ) ; var statement = connection . createStatement ( ) ) { statement . executeUpdate ( "CREATE INDEX IF NOT EXISTS IDX_DESC_ID ON mediathekview.description (id)" ) ; statement . executeUpdate ( "CREATE INDEX IF NOT EXISTS IDX_WEBSITE_LINKS_ID ON mediathekview.website_links (id)" ) ; tm . commit ( ) ; } catch ( SQLException ex ) { log . warn ( ex . getMessage ( ) ) ; } }
public void test() { try { byName = InetAddress . getByName ( substringBeforeSlash ( rendEp . getAddress ( ) ) ) ; } catch ( UnknownHostException e ) { LOG . warn ( "Cannot resolve address: {}" , e . getMessage ( ) ) ; return null ; } }
public void test() { try { return new URI ( this . getURIPart ( ) ) ; } catch ( final URISyntaxException e ) { LOG . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { CombineService . mergeTwoBlocks ( _brokerRequest , mergedBlock , blockToMerge ) ; LOGGER . debug ( "Merged response from operator {} after: {}" , mergedBlocksNumber , ( System . currentTimeMillis ( ) - startTime ) ) ; } catch ( Exception e ) { LOGGER . error ( "Merge response from operator {} failed after: {}" , mergedBlocksNumber , e ) ; mergedBlock . getExceptions ( ) . add ( QueryException . getException ( QueryException . MERGE_RESPONSE_ERROR , e ) ) ; } }
public void test() { try { LOGGER . debug ( "Got response from operator {} after: {}" , mergedBlocksNumber , ( System . currentTimeMillis ( ) - startTime ) ) ; CombineService . mergeTwoBlocks ( _brokerRequest , mergedBlock , blockToMerge ) ; } catch ( Exception e ) { LOGGER . error ( "Exception while merging merged block" , e ) ; mergedBlock . getExceptions ( ) . add ( QueryException . getException ( QueryException . MERGE_RESPONSE_ERROR , e ) ) ; } }
public void test() { try { sendAndStateLock . lock ( ) ; handleEvent ( new Event ( Event . Type . FAILED_SEND_RECORD , createAccountRequest ( request ) ) ) ; } catch ( Exception e ) { logger . warn ( "Failed to send record" , e ) ; } finally { sendAndStateLock . unlock ( ) ; } }
public void test() { if ( inputStream != null ) { code_block = TryStatement ;  } }
public void test() { try { ctor = ezspClass . getConstructor ( int [ ] . class ) ; EzspFrameResponse ezspFrame = ( EzspFrameResponse ) ctor . newInstance ( data ) ; return ezspFrame ; } catch ( SecurityException | NoSuchMethodException | IllegalArgumentException | InstantiationException | IllegalAccessException | InvocationTargetException e ) { logger . error ( "Error instantiating EZSP: {}" , e . getMessage ( ) ) ; } }
public void test() { try { return resourceBundleLocator . getBundle ( INTRIGUE_BASE_NAME ) . getString ( "validation.attribute.unsupported" ) ; } catch ( IOException e ) { logger . error ( "Unable to load " + INTRIGUE_BASE_NAME + ": " + DEFAULT_MESSAGE_FORMAT , e ) ; return DEFAULT_MESSAGE_FORMAT ; } }
public void test() { try { Configuration [ ] configurations = configAdmin . listConfigurations ( "(service.factoryPid=" + factoryPid + ")" ) ; code_block = IfStatement ; } catch ( InvalidSyntaxException e ) { logger . error ( "" , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Deleting session {}" , session ) ; } }
public void test() { if ( localeResolver != null ) { logger . info ( "Using locale resolver" ) ; } }
public void checkPolicy ( ) { log . warn ( "Policy is not implemented yet" ) ; }
public void test() { try { HuiRelation relation = HUITypeFactory . getInstance ( ) . getRelation ( linkId ) ; String hql ; code_block = IfStatement ; IBaseDao < Configuration , Serializable > dao = getDaoFactory ( ) . getDAO ( Configuration . class ) ; List < ? > result = dao . findByQuery ( hql , new String [ ] code_block = "" ; ) ; code_block = IfStatement ; } catch ( Exception t ) { logger . error ( "error" , t ) ; } }
@ Override public Void call ( ) throws Exception { SchedulableProgramType programType = node . getProgram ( ) . getProgramType ( ) ; String programName = node . getProgram ( ) . getProgramName ( ) ; String prettyProgramType = ProgramType . valueOf ( programType . name ( ) ) . getPrettyName ( ) ; ProgramWorkflowRunner programWorkflowRunner = workflowProgramRunnerFactory . getProgramWorkflowRunner ( programType , token , node . getNodeId ( ) , nodeStates ) ; code_block = IfStatement ; Runnable programRunner = programWorkflowRunner . create ( programName ) ; LOG . info ( "Starting {} Program '{}' in workflow" , prettyProgramType , programName ) ; programRunner . run ( ) ; LOG . info ( "{} Program '{}' in workflow completed" , prettyProgramType , programName ) ; return null ; }
@ Override public Void call ( ) throws Exception { SchedulableProgramType programType = node . getProgram ( ) . getProgramType ( ) ; String programName = node . getProgram ( ) . getProgramName ( ) ; String prettyProgramType = ProgramType . valueOf ( programType . name ( ) ) . getPrettyName ( ) ; ProgramWorkflowRunner programWorkflowRunner = workflowProgramRunnerFactory . getProgramWorkflowRunner ( programType , token , node . getNodeId ( ) , nodeStates ) ; code_block = IfStatement ; Runnable programRunner = programWorkflowRunner . create ( programName ) ; LOG . info ( "Starting {} Program '{}' in workflow" , prettyProgramType , programName ) ; programRunner . run ( ) ; LOG . info ( "{} Program '{}' in workflow completed" , prettyProgramType , programName ) ; return null ; }
public void test() { try { index ( upperSession , entity ( upperSession , tuple ) ) ; } catch ( Throwable e ) { errorHandler . handleException ( log . massIndexerUnexpectedErrorMessage ( ) , e ) ; } finally { log . info ( "Unexpected: " + upperSession ) ; } }
public void test() { switch ( status ) { case FAULT : evt . setAspect ( ProcessMessageExchangeEvent . PARTNER_FAULT ) ; responseChannel . onFault ( ) ; break ; case RESPONSE : evt . setAspect ( ProcessMessageExchangeEvent . PARTNER_OUTPUT ) ; responseChannel . onResponse ( ) ; break ; case FAILURE : evt . setAspect ( ProcessMessageExchangeEvent . PARTNER_FAILURE ) ; responseChannel . onFailure ( ) ; break ; default : log . warn ( "unknown status: " + status ) ; } }