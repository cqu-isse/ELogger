public void test() { try { LOG . debug ( "Initializing XRay tracer" ) ; camelContext . addService ( this , true , true ) ; } catch ( Exception e ) { throw RuntimeCamelException . wrapRuntimeCamelException ( e ) ; } }
public void test() { try { return openIterator ( getQueryConstraints ( ) ) ; } catch ( TransformException | FactoryException e ) { LOGGER . warn ( "Unable to transform geometry" , e ) ; } }
protected void handleRemoteDisconnect ( final ProtonConnection con ) { log . debug ( "client [container: {}] disconnected" , con . getRemoteContainer ( ) ) ; con . disconnect ( ) ; publishConnectionClosedEvent ( con ) ; }
@ Override public Map < K , ICacheElement < K , V > > getMultiple ( final Set < K > keys ) { log . info ( "get [" + keys + "]" ) ; return getMultipleSetupMap . get ( keys ) ; }
public void test() { if ( applicationContext . getResource ( resolvedLocation ) . isReadable ( ) ) { LOGGER . debug ( "Configuration {} detected and added" , location ) ; return applicationContext . getResource ( resolvedLocation ) ; } else-if ( throwErrorIfNotReadable ) { throw new IllegalStateException ( String . format ( "Configuration %s does not exist or is not readable" , location ) ) ; } else { LOGGER . warn ( "Configuration {} not readable; ignored" , location ) ; return null ; } }
public void test() { if ( applicationContext . getResource ( resolvedLocation ) . isReadable ( ) ) { LOGGER . debug ( "Configuration {} detected and added" , location ) ; return applicationContext . getResource ( resolvedLocation ) ; } else-if ( throwErrorIfNotReadable ) { throw new IllegalStateException ( String . format ( "Configuration %s does not exist or is not readable" , location ) ) ; } else { LOGGER . warn ( "Configuration {} not readable; ignored" , location ) ; return null ; } }
public void test() { if ( _log . isWarnEnabled ( ) ) { _log . warn ( "Missing />" ) ; } }
public void testForMemoryLeaks ( ) throws Exception { final long differenceMemoryCache = thrashCache ( ) ; LOG . info ( "Memory Difference is: " + differenceMemoryCache ) ; assertTrue ( differenceMemoryCache < 500000 ) ; }
public void test() { if ( code == HttpStatus . SC_OK ) { String content = post . getResponseBodyAsString ( ) ; Map < String , Object > resp = gson . fromJson ( content , new TypeToken < Map < String , Object > > ( ) code_block = "" ; . getType ( ) ) ; LOG . info ( "Received from Zeppelin LoginRestApi : " + content ) ; return ( Map < String , String > ) resp . get ( "body" ) ; } else { LOG . error ( "Failed Zeppelin login {}, status code {}" , endpoint , code ) ; return Collections . emptyMap ( ) ; } }
public void test() { try { int code = client . executeMethod ( post ) ; code_block = IfStatement ; } catch ( IOException e ) { LOG . error ( "Cannot login into Zeppelin" , e ) ; return Collections . emptyMap ( ) ; } }
@ Test ( timeout = 60000 ) public void testPresettledReceiverReadsAllMessages ( ) throws Exception { final int MSG_COUNT = 100 ; sendMessages ( getQueueName ( ) , MSG_COUNT ) ; AmqpClient client = createAmqpClient ( ) ; AmqpConnection connection = addConnection ( client . connect ( ) ) ; AmqpSession session = connection . createSession ( ) ; AmqpReceiver receiver = session . createReceiver ( getQueueName ( ) , null , false , true ) ; final Queue queueView = getProxyToQueue ( getQueueName ( ) ) ; assertEquals ( MSG_COUNT , queueView . getMessageCount ( ) ) ; receiver . flow ( MSG_COUNT ) ; code_block = ForStatement ; receiver . close ( ) ; instanceLog . debug ( "Message Count after all consumed: " + queueView . getMessageCount ( ) ) ; receiver = session . createReceiver ( getQueueName ( ) ) ; receiver . flow ( 1 ) ; AmqpMessage received = receiver . receive ( 5 , TimeUnit . SECONDS ) ; code_block = IfStatement ; assertNull ( received ) ; assertEquals ( 0 , queueView . getMessageCount ( ) ) ; connection . close ( ) ; }
public void test() { if ( received != null ) { instanceLog . debug ( "Message read: " + received . getMessageId ( ) ) ; } }
public void test() { if ( seenModules != null ) { log . trace ( "seenModules.size: " + seenModules . size ( ) ) ; } }
public void test() { for ( URL url : getURLs ( ) ) { log . trace ( "url: " + url ) ; } }
public void test() { if ( log != null && log . isDebugEnabled ( ) ) { log . debug ( "Exception in closing " + c , e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( DLFileEntryTypeServiceUtil . class , "updateFileEntryType" , _updateFileEntryTypeParameterTypes16 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , fileEntryTypeId , nameMap , descriptionMap , ddmStructureIds , serviceContext ) ; code_block = TryStatement ;  } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { this . isLocalVariable = false ; this . visibility = VisibilitySet . PUBLIC . toString ( ) ; this . isFinal = true ; this . hasClassScope = true ; dispatchAnnotationsOfMember ( modifierList , belongsToClass ) ; this . declareType = determineTypeOfTypeType ( typeType , belongsToClass ) ; determine_name ( variableDeclarators ) ; } catch ( Exception e ) { logger . warn ( " Exception while processing: " + belongsToClass + " Line: " + typeType . start . getLine ( ) + " " + e . getMessage ( ) ) ; } }
private void infolog ( String msg ) { logger . info ( msg ) ; processingLog += msg + ".\n" ; }
public void test() { try { restTemplate = getRestTemplateForReadingLinkedData ( requesterWebID ) ; } catch ( Exception e ) { logger . error ( "Failed to create ssl tofu rest template" , e ) ; throw new RuntimeException ( e ) ; } }
public void test() { try { JsonNode jsonNode = MAPPER . readTree ( IOUtils . toString ( getClass ( ) . getResourceAsStream ( "/MIMETypes.json" ) ) ) ; code_block = ForStatement ; mimeTypesMap = Collections . unmodifiableMap ( mimeTypesMap ) ; LOG . debug ( "MIME types loaded: {}" , mimeTypesMap ) ; mimeTypes = new ArrayList < > ( mimeTypesMap . keySet ( ) ) ; Collections . sort ( mimeTypes ) ; mimeTypes = Collections . unmodifiableList ( mimeTypes ) ; } catch ( Exception e ) { LOG . error ( "Error reading file MIMETypes from resources" , e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( FragmentCompositionServiceUtil . class , "deleteFragmentComposition" , _deleteFragmentCompositionParameterTypes1 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , fragmentCompositionId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . fragment . model . FragmentComposition ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( rv != EXIT_SUCCESS ) { LOGGER . info ( "========= Workflow did not execute sucessfully ============" ) ; retVal = rv ; code_block = IfStatement ; } else { LOGGER . info ( "============= Workflow executed sucessfully ===============" ) ; } }
public void test() { if ( rv != EXIT_SUCCESS ) { LOGGER . info ( "========= Workflow did not execute sucessfully ============" ) ; retVal = rv ; code_block = IfStatement ; } else { LOGGER . info ( "============= Workflow executed sucessfully ===============" ) ; } }
@ Override public void process ( final FilterChain chain , final Request request , final Response response ) { long tm = System . currentTimeMillis ( ) ; code_block = TryStatement ;  tm = System . currentTimeMillis ( ) - tm ; log . info ( "Finished request: " + tm + "ms  to " + request . getAbsolutePath ( ) + " method=" + request . getMethod ( ) ) ; }
@ Override public void doStopSelfTest ( final OslpEnvelope oslpRequest , final DeviceRequest deviceRequest , final DeviceResponseHandler deviceResponseHandler , final String ipAddress ) throws IOException { LOGGER . info ( "doStopSelfTest() code_block = ForStatement ; ; this . sendMessage ( ipAddress , oslpRequest , oslpResponseHandler , deviceRequest ) ; }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( format ( "Generating schema for udt of type %s" , udtClassProperty . udtClass . getCanonicalName ( ) ) ) ; } }
public void test() { if ( ACHILLES_DML_LOGGER . isDebugEnabled ( ) ) { ACHILLES_DML_LOGGER . debug ( udtSchema + "\n" ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( sb . toString ( ) ) ; } }
public void test() { try { return createProjectVersion ( designRepository . check ( designFolderName ) ) ; } catch ( IOException e ) { log . error ( e . getMessage ( ) , e ) ; } }
public void runRepeatedly ( int numIterations ) { logger . info ( "~~~~ Initial State ~~~~~" ) ; RebalanceStrategy strategy = new AutoRebalanceStrategy ( RESOURCE_NAME , _partitions , _states , _maxPerNode ) ; ZNRecord initialResult = strategy . computePartitionAssignment ( _allNodes , _liveNodes , _currentMapping , null ) ; _currentMapping = getMapping ( initialResult . getListFields ( ) ) ; logger . info ( _currentMapping . toString ( ) ) ; getRunResult ( _currentMapping , initialResult . getListFields ( ) ) ; code_block = ForStatement ; }
public void runRepeatedly ( int numIterations ) { logger . info ( "~~~~ Initial State ~~~~~" ) ; RebalanceStrategy strategy = new AutoRebalanceStrategy ( RESOURCE_NAME , _partitions , _states , _maxPerNode ) ; ZNRecord initialResult = strategy . computePartitionAssignment ( _allNodes , _liveNodes , _currentMapping , null ) ; _currentMapping = getMapping ( initialResult . getListFields ( ) ) ; logger . info ( _currentMapping . toString ( ) ) ; getRunResult ( _currentMapping , initialResult . getListFields ( ) ) ; code_block = ForStatement ; }
public void test() { if ( znRecord != null ) { final Map < String , List < String > > listResult = znRecord . getListFields ( ) ; final Map < String , Map < String , String > > mapResult = getMapping ( listResult ) ; logger . info ( mapResult . toString ( ) ) ; logger . info ( listResult . toString ( ) ) ; getRunResult ( mapResult , listResult ) ; _currentMapping = mapResult ; } }
public List < Asset > findAll ( Long repositoryId , String path , Boolean deleted , Boolean virtual , Long branchId ) { logger . debug ( "Find all assets for repositoryId: {}, path: {}, deleted: {}, virtual: {}, branchId: {}" , repositoryId , path , deleted , virtual , branchId ) ; Specification < Asset > assetSpecifications = distinct ( ifParamNotNull ( repositoryIdEquals ( repositoryId ) ) ) . and ( ifParamNotNull ( pathEquals ( path ) ) ) . and ( ifParamNotNull ( deletedEquals ( deleted ) ) ) . and ( ifParamNotNull ( virtualEquals ( virtual ) ) ) . and ( ifParamNotNull ( branchId ( branchId , deleted ) ) ) ; List < Asset > all = assetRepository . findAll ( assetSpecifications ) ; return all ; }
public void test() { -> { code_block = IfStatement ; RemoteSessionObject remoteSessionObject = new RemoteSessionObject ( user ) ; log . debug ( remoteSessionObject . toString ( ) ) ; String xmlString = remoteSessionObject . toString ( ) ; log . debug ( "jsonString {}" , xmlString ) ; String hash = soapDao . addSOAPLogin ( sid , options ) ; code_block = IfStatement ; return UNKNOWN ; } }
public void test() { try ( Response response = make ( request ) ) { check ( response ) ; return RequestResponse . parseFromResponse ( response ) ; } catch ( VitamClientInternalException e ) { throw new AccessExternalClientException ( e ) ; } catch ( AdminExternalClientException e ) { LOGGER . error ( e ) ; return e . getVitamError ( ) ; } }
public void test() { try { code_block = ForStatement ; } catch ( Exception e ) { log . info ( "exceotuib reseting stats" , e ) ; } }
private void writeExtensions ( CertificatePair pair ) { appendBytes ( pair . getExtensions ( ) . getValue ( ) ) ; LOGGER . debug ( "Extensions: " + ArrayConverter . bytesToHexString ( pair . getExtensions ( ) . getValue ( ) ) ) ; }
public void test() { try { GeneralBuildingBlock gBBInput = execution . getGeneralBuildingBlock ( ) ; RequestContext requestContext = gBBInput . getRequestContext ( ) ; ServiceInstance serviceInstance = extractPojosForBB . extractByKey ( execution , ResourceKey . SERVICE_INSTANCE_ID ) ; GenericVnf vnf = extractPojosForBB . extractByKey ( execution , ResourceKey . GENERIC_VNF_ID ) ; VfModule vfModule = extractPojosForBB . extractByKey ( execution , ResourceKey . VF_MODULE_ID ) ; Customer customer = gBBInput . getCustomer ( ) ; CloudRegion cloudRegion = gBBInput . getCloudRegion ( ) ; SDNCRequest sdncRequest = new SDNCRequest ( ) ; GenericResourceApiVfModuleOperationInformation req = sdncVfModuleResources . deactivateVfModule ( vfModule , vnf , serviceInstance , customer , cloudRegion , requestContext , buildCallbackURI ( sdncRequest ) ) ; sdncRequest . setSDNCPayload ( req ) ; sdncRequest . setTopology ( SDNCTopology . VFMODULE ) ; execution . setVariable ( SDNC_REQUEST , sdncRequest ) ; } catch ( Exception ex ) { logger . error ( "Exception occurred in SDNCDeactivateTasks deactivateVfModule" , ex ) ; exceptionUtil . buildAndThrowWorkflowException ( execution , 7000 , ex ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "==> reorderEvaluators()" ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "<== reorderEvaluators()" ) ; } }
public void test() { try { deSerializedValue = elementSerializer . deSerialize ( serialized . getSerializedValue ( ) , null ) ; } catch ( final ClassNotFoundException | IOException e ) { log . error ( "Problem de-serializing object." , e ) ; throw e ; } }
public void test() { try { Attribute at = m . getAttribute ( ) ; code_block = IfStatement ; } catch ( LdapInvalidAttributeValueException e ) { PROVIDER_LOG . warn ( "Invalid attribute type" , e ) ; } }
public void close ( ) { LOG . debug ( "thread {}: connection stop" , Thread . currentThread ( ) . getId ( ) ) ; running . set ( false ) ; receiver . interrupt ( ) ; connection . close ( ) ; }
@ Test public void testDb2TableCreation ( ) { logger . info ( "Testing DB2 schema creation" ) ; FhirSchemaGenerator gen = new FhirSchemaGenerator ( ADMIN_SCHEMA_NAME , SCHEMA_NAME , false ) ; PhysicalDataModel model = new PhysicalDataModel ( ) ; gen . buildSchema ( model ) ; PrintTarget tgt = new PrintTarget ( null , logger . isLoggable ( Level . FINE ) ) ; Db2Adapter adapter = new Db2Adapter ( tgt ) ; model . apply ( adapter ) ; }
@ Override public void startApplication ( String applicationName ) { logger . debug ( Messages . STARTING_APPLICATION_0 , applicationName ) ; delegate . startApplication ( applicationName ) ; }
@ Path ( "/getAllShouldSucceed" ) @ POST public void getAllShouldSucceed ( ) { LOG . debug ( "Calling OpenstackNeutronNetworkResource.getAllShouldSucceed()" ) ; String uri = String . format ( URI_FORMAT , OpenstackConstants . GET_ALL ) ; Network [ ] networks = template . requestBody ( uri , null , Network [ ] . class ) ; assertNotNull ( networks ) ; assertEquals ( 1 , networks . length ) ; assertEquals ( NETWORK_NAME , networks [ 0 ] . getName ( ) ) ; assertNotNull ( networks [ 0 ] . getSubnets ( ) ) ; assertEquals ( 1 , networks [ 0 ] . getSubnets ( ) . size ( ) ) ; assertEquals ( "0c4faf33-8c23-4dc9-8bf5-30dd1ab452f9" , networks [ 0 ] . getSubnets ( ) . get ( 0 ) ) ; assertEquals ( "73f6f1ac-5e58-4801-88c3-7e12c6ddfb39" , networks [ 0 ] . getId ( ) ) ; assertEquals ( NetworkType . VXLAN , networks [ 0 ] . getNetworkType ( ) ) ; }
public void test() { if ( System . currentTimeMillis ( ) - lastLaunch < RELAUNCH_EXCLUSION_WINDOW_MILLIS ) { LOGGER . info ( "Encountered a subsequent failure. Giving up." ) ; break ; } else-if ( code == 2 ) { LOGGER . info ( "User requested restart." ) ; } else { code_block = IfStatement ; code_block = IfStatement ; } }
public void test() { if ( System . currentTimeMillis ( ) - lastLaunch < RELAUNCH_EXCLUSION_WINDOW_MILLIS ) { LOGGER . info ( "Encountered a subsequent failure. Giving up." ) ; break ; } else-if ( code == 2 ) { LOGGER . info ( "User requested restart." ) ; } else { code_block = IfStatement ; code_block = IfStatement ; } }
public void test() { if ( code != 0 && ourRestart ) { LOGGER . info ( "Unexpected exit. Restarting..." ) ; } else { LOGGER . info ( "Exiting with status " + code ) ; break ; } }
public void test() { try { ( new ResponseContentHandle ( ) ) . parse ( data , msg ) ; } catch ( IOException e ) { logger . error ( "Decode RQNT Response failed" , e ) ; } }
public void kinit ( String user ) throws Exception { UserGroupInformation . loginUserFromKeytab ( user , KEYTAB_LOCATION + "/" + user + ".keytab" ) ; LOGGER . info ( "Kinited user: " + user + " keytab: " + KEYTAB_LOCATION + "/" + user + ".keytab" ) ; }
@ Override public void onError ( SubscriptionException error ) { logger . info ( name . getMethodName ( ) + " - callback - error" ) ; subscribeBroadcastWithSingleStructParameterCallbackResult = false ; subscribeBroadcastWithSingleStructParameterCallbackDone = true ; }
private void changeTrackPosition ( long positionOffsetInMs ) throws SpeakerException { long currentPosition = speaker . getPlayState ( ) . getPositionInMs ( ) ; logger . debug ( "Jumping from old track position {} ms to new position {} ms" , currentPosition , currentPosition + positionOffsetInMs ) ; speaker . setPosition ( currentPosition + positionOffsetInMs ) ; }
public void test() { try { master . getValue ( ) . dispose ( ) ; } catch ( java . lang . Exception e ) { LOGGER . error ( "" , e ) ; } }
public void test() { try { CarbonAppPersistenceManager capm = new CarbonAppPersistenceManager ( getAxisConfig ( ) ) ; regConfig = capm . loadRegistryConfig ( AppDeployerConstants . APPLICATIONS + parentAppName + AppDeployerConstants . APP_DEPENDENCIES + artifactName ) ; } catch ( Exception e ) { log . error ( "Error while trying to load registry config for C-App : " + parentAppName + " artifact : " + artifactName , e ) ; } }
public void test() { if ( s_logger . isDebugEnabled ( ) ) { s_logger . debug ( "Ping command port, " + privateIp + ":" + cmdPort ) ; } }
public void test() { try { String result = connect ( cmd . getName ( ) , privateIp , cmdPort ) ; code_block = IfStatement ; } catch ( Exception e ) { s_logger . error ( "Can not ping System vm " + vmName + "due to exception" ) ; return new CheckSshAnswer ( cmd , e ) ; } }
public void test() { if ( result != null ) { s_logger . error ( "Can not ping System vm " + vmName + "due to:" + result ) ; return new CheckSshAnswer ( cmd , "Can not ping System vm " + vmName + "due to:" + result ) ; } }
public void test() { if ( s_logger . isDebugEnabled ( ) ) { s_logger . debug ( "Execute network usage setup command on " + vmName ) ; } }
public void test() { if ( o == objectFilename ) { String val = ( ( JTextField ) o ) . getText ( ) ; sf . loadObjectImageFilename ( val ) ; } else { log . warn ( "Inknown object invoked in surf filter ui" ) ; } }
public void test() { try { data . close ( ) ; } catch ( Exception e ) { LOG . error ( "closing piped input stream failed" , e ) ; } }
public void test() { try { writeTask . cancel ( true ) ; } catch ( Exception e ) { LOG . error ( "cancelling write task failed" , e ) ; } }
public void test() { if ( ! snapThreadMutex . tryAcquire ( ) ) { LOG . warn ( "Too busy to snap, skipping" ) ; } else { new ZooKeeperThread ( "Snapshot Thread" ) code_block = "" ; . start ( ) ; } }
public void test() { try { zks . takeSnapshot ( ) ; } catch ( Exception e ) { LOG . warn ( "Unexpected exception" , e ) ; } finally { snapThreadMutex . release ( ) ; } }
public void test() { if ( IS_DEBUG ) { LOG . debug ( "Authenticator created" ) ; } }
public void test() { if ( virtualDatapointHandler != null ) { logger . debug ( "Handling virtual datapoint '{}' on gateway with id '{}'" , dp . getName ( ) , id ) ; virtualDatapointHandler . handleCommand ( gateway , dp , dpConfig , newValue ) ; } else-if ( dp . isScript ( ) ) { code_block = IfStatement ; } else-if ( dp . isVariable ( ) ) { logger . debug ( "Sending variable '{}' with value '{}' to gateway with id '{}'" , dp . getInfo ( ) , newValue , id ) ; setVariable ( dp , newValue ) ; } else { logger . debug ( "Sending datapoint '{}' with value '{}' to gateway with id '{}' using rxMode '{}'" , dpInfo , newValue , id , rxMode == null ? "DEFAULT" : rxMode ) ; getRpcClient ( dp . getChannel ( ) . getDevice ( ) . getHmInterface ( ) ) . setDatapointValue ( dp , newValue , rxMode ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "got " + messages . size ( ) + " message(s): " + messages ) ; } }
public void test() { try { SharedData shared = ( SharedData ) context . get ( SHARED_DATA ) ; code_block = IfStatement ; saveData ( context ) ; reporter . addObjectReport ( context , "merged" , OBJECT_TYPE . CONNECTION_LINK , "connection links" , OBJECT_STATE . OK , IO_TYPE . OUTPUT ) ; reporter . addObjectReport ( context , "merged" , OBJECT_TYPE . ACCESS_POINT , "access points" , OBJECT_STATE . OK , IO_TYPE . OUTPUT ) ; reporter . addObjectReport ( context , "merged" , OBJECT_TYPE . STOP_AREA , "stop areas" , OBJECT_STATE . OK , IO_TYPE . OUTPUT ) ; reporter . setStatToObjectReport ( context , "merged" , OBJECT_TYPE . CONNECTION_LINK , OBJECT_TYPE . CONNECTION_LINK , shared . connectionLinks . getItems ( ) . size ( ) ) ; reporter . setStatToObjectReport ( context , "merged" , OBJECT_TYPE . ACCESS_POINT , OBJECT_TYPE . ACCESS_POINT , shared . accessPoints . getItems ( ) . size ( ) ) ; reporter . setStatToObjectReport ( context , "merged" , OBJECT_TYPE . STOP_AREA , OBJECT_TYPE . STOP_AREA , shared . physicalStops . getItems ( ) . size ( ) + shared . commercialStops . getItems ( ) . size ( ) + shared . getStopPlaces ( ) . getItems ( ) . size ( ) ) ; result = SUCCESS ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; } finally { log . info ( Color . MAGENTA + monitor . stop ( ) + Color . NORMAL ) ; } }
public void test() { try { SharedData shared = ( SharedData ) context . get ( SHARED_DATA ) ; code_block = IfStatement ; saveData ( context ) ; reporter . addObjectReport ( context , "merged" , OBJECT_TYPE . CONNECTION_LINK , "connection links" , OBJECT_STATE . OK , IO_TYPE . OUTPUT ) ; reporter . addObjectReport ( context , "merged" , OBJECT_TYPE . ACCESS_POINT , "access points" , OBJECT_STATE . OK , IO_TYPE . OUTPUT ) ; reporter . addObjectReport ( context , "merged" , OBJECT_TYPE . STOP_AREA , "stop areas" , OBJECT_STATE . OK , IO_TYPE . OUTPUT ) ; reporter . setStatToObjectReport ( context , "merged" , OBJECT_TYPE . CONNECTION_LINK , OBJECT_TYPE . CONNECTION_LINK , shared . connectionLinks . getItems ( ) . size ( ) ) ; reporter . setStatToObjectReport ( context , "merged" , OBJECT_TYPE . ACCESS_POINT , OBJECT_TYPE . ACCESS_POINT , shared . accessPoints . getItems ( ) . size ( ) ) ; reporter . setStatToObjectReport ( context , "merged" , OBJECT_TYPE . STOP_AREA , OBJECT_TYPE . STOP_AREA , shared . physicalStops . getItems ( ) . size ( ) + shared . commercialStops . getItems ( ) . size ( ) + shared . getStopPlaces ( ) . getItems ( ) . size ( ) ) ; result = SUCCESS ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; } finally { log . info ( Color . MAGENTA + monitor . stop ( ) + Color . NORMAL ) ; } }
public void test() { if ( annotationType == null ) { logger . debug ( "Couldn't resolve " + an . desc + " annotation type whilst searching for hints on " + getName ( ) ) ; } else { Stack < Type > s = new Stack < > ( ) ; annotationType . collectHints ( an , hints , new HashSet < > ( ) , s ) ; } }
public static void main ( String [ ] args ) { logger . info ( "Before Starting application" ) ; SpringApplication . run ( SpringLoggerApplication . class , args ) ; logger . debug ( "Starting my application in debug with {} arguments" , args . length ) ; logger . info ( "Starting my application with {} arguments." , args . length ) ; }
public static void main ( String [ ] args ) { logger . info ( "Before Starting application" ) ; SpringApplication . run ( SpringLoggerApplication . class , args ) ; logger . debug ( "Starting my application in debug with {} arguments" , args . length ) ; logger . info ( "Starting my application with {} arguments." , args . length ) ; }
public static void main ( String [ ] args ) { logger . info ( "Before Starting application" ) ; SpringApplication . run ( SpringLoggerApplication . class , args ) ; logger . debug ( "Starting my application in debug with {} arguments" , args . length ) ; logger . info ( "Starting my application with {} arguments." , args . length ) ; }
@ Override public int ignite ( String host , int port , SslStores sslStores , int maxThreads , int minThreads , int threadIdleTimeoutMillis ) throws ContainerInitializationException { Timer . start ( "SPARK_EMBEDDED_IGNITE" ) ; log . info ( "Starting Spark server, ignoring port and host" ) ; code_block = IfStatement ; sparkFilter . init ( null ) ; Timer . stop ( "SPARK_EMBEDDED_IGNITE" ) ; return port ; }
public void test() { try { com . liferay . commerce . price . list . model . CommercePriceList returnValue = CommercePriceListServiceUtil . fetchCatalogBaseCommercePriceListByType ( groupId , type ) ; return com . liferay . commerce . price . list . model . CommercePriceListSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
@ Override public Object visit ( Before filter , Object data ) { LOGGER . trace ( "ENTERING: Before filter" ) ; buildTemporalSearch ( filter , data , literal code_block = LoopStatement ; ) ; LOGGER . trace ( "EXITING: Before filter" ) ; return super . visit ( filter , data ) ; }
public void test() { if ( date != null ) { Date start = new Date ( 0L ) ; return new DateRange ( start , date ) ; } else { LOGGER . debug ( "Unable to extract date from Before filter {}. Ignoring filter." , literal ) ; return null ; } }
@ Override public Object visit ( Before filter , Object data ) { LOGGER . trace ( "ENTERING: Before filter" ) ; buildTemporalSearch ( filter , data , literal code_block = LoopStatement ; ) ; LOGGER . trace ( "EXITING: Before filter" ) ; return super . visit ( filter , data ) ; }
public void test() { try { dBuilder = dbFactory . newDocumentBuilder ( ) ; } catch ( ParserConfigurationException ex ) { LOG . error ( ex . getLocalizedMessage ( ) ) ; } }
public void test() { try { statusFileURL = "http://" + board . getIpAddress ( ) + ":" + Integer . toString ( board . getPort ( ) ) + "/" + GET_SENSORS_URL ; LOG . info ( "Zibase gets relay status from file " + statusFileURL ) ; doc = dBuilder . parse ( new URL ( statusFileURL ) . openStream ( ) ) ; doc . getDocumentElement ( ) . normalize ( ) ; } catch ( ConnectException connEx ) { disconnect ( ) ; this . stop ( ) ; this . setDescription ( "Connection timed out, no reply from the board at " + statusFileURL ) ; } catch ( SAXException ex ) { disconnect ( ) ; this . stop ( ) ; LOG . error ( Freedomotic . getStackTraceInfo ( ex ) ) ; } catch ( Exception ex ) { disconnect ( ) ; this . stop ( ) ; setDescription ( "Unable to connect to " + statusFileURL ) ; LOG . error ( Freedomotic . getStackTraceInfo ( ex ) ) ; } }
public void flush ( ) { release ( true ) ; resetHitCounter ( ) ; log . debug ( this . getName ( ) + " flushed." ) ; }
public void test() { try { ( ( ApplicationContextImpl ) _remoteParticipant . getApplicationContext ( ) ) . getRemoteCommunication ( ) . joinAnswer ( _remoteParticipant . getId ( ) , _localParticipant . getRemoteAddress ( ) , sdp ) ; } catch ( final Exception e ) { LOG . error ( "" , e ) ; notifyRemote = false ; done ( Cause . ERROR , e ) ; } }
public void test() { try ( final Socket socket = new Socket ( ) ) { logger . debug ( "Connecting to NiFi instance" ) ; socket . setSoTimeout ( 10000 ) ; socket . connect ( new InetSocketAddress ( "localhost" , port ) ) ; logger . debug ( "Established connection to NiFi instance." ) ; socket . setSoTimeout ( 0 ) ; logger . debug ( "Sending DECOMMISSION Command to port {}" , port ) ; final OutputStream out = socket . getOutputStream ( ) ; out . write ( ( DECOMMISSION_CMD + " " + secretKey + "\n" ) . getBytes ( StandardCharsets . UTF_8 ) ) ; out . flush ( ) ; socket . shutdownOutput ( ) ; final String response = readResponse ( socket . getInputStream ( ) ) ; code_block = IfStatement ; } finally { code_block = IfStatement ; } }
public void test() { try ( final Socket socket = new Socket ( ) ) { logger . debug ( "Connecting to NiFi instance" ) ; socket . setSoTimeout ( 10000 ) ; socket . connect ( new InetSocketAddress ( "localhost" , port ) ) ; logger . debug ( "Established connection to NiFi instance." ) ; socket . setSoTimeout ( 0 ) ; logger . debug ( "Sending DECOMMISSION Command to port {}" , port ) ; final OutputStream out = socket . getOutputStream ( ) ; out . write ( ( DECOMMISSION_CMD + " " + secretKey + "\n" ) . getBytes ( StandardCharsets . UTF_8 ) ) ; out . flush ( ) ; socket . shutdownOutput ( ) ; final String response = readResponse ( socket . getInputStream ( ) ) ; code_block = IfStatement ; } finally { code_block = IfStatement ; } }
public void test() { if ( DECOMMISSION_CMD . equals ( response ) ) { logger . debug ( "Received response to DECOMMISSION command: {}" , response ) ; code_block = IfStatement ; return null ; } else { logger . error ( "When sending DECOMMISSION command to NiFi, got unexpected response {}" , response ) ; return 18 ; } }
public void test() { if ( lockFile . exists ( ) && ! lockFile . delete ( ) ) { logger . error ( "Failed to delete lock file {}; this file should be cleaned up manually" , lockFile ) ; } }
@ Override public List < ReceiptDetail > reconstructReceiptDetail ( final String billReferenceNumber , final BigDecimal actualAmountPaid , final List < ReceiptDetail > receiptDetailList ) { final Long billID = Long . valueOf ( billReferenceNumber ) ; final List < EgBillDetails > billDetails = new ArrayList < > ( 0 ) ; final EgBill bill = applicationBpaBillService . updateBillWithLatest ( billID ) ; LOGGER . debug ( "Reconstruct consumer code :" + bill . getConsumerId ( ) + ", with bill reference number: " + billReferenceNumber + ", for Amount Paid :" + actualAmountPaid ) ; final CollectionApportioner apportioner = new CollectionApportioner ( ) ; billDetails . addAll ( bill . getEgBillDetails ( ) ) ; return apportioner . reConstruct ( actualAmountPaid , billDetails , functionDAO , chartOfAccountsDAO ) ; }
public void test() { try { code_block = IfStatement ; } catch ( IOException e ) { LOG . info ( "Can't create admin log and detail for logo replacement." , e ) ; } }
public void test() { if ( status == 1 ) { LOGGER . error ( "[loadConfig][{}] Exception occurred, cause: {}" , sourceName , throwable . toString ( ) ) ; } else { LOGGER . debug ( "[loadConfig][{}] Loaded config properties" , sourceName ) ; } }
public void test() { if ( status == 1 ) { LOGGER . error ( "[loadConfig][{}] Exception occurred, cause: {}" , sourceName , throwable . toString ( ) ) ; } else { LOGGER . debug ( "[loadConfig][{}] Loaded config properties" , sourceName ) ; } }
public void test() { try { dialPlan = new DialPlanExtension ( extension ) ; } catch ( final IllegalArgumentException e ) { logger . error ( e , e ) ; } }
public void test() { try { write ( network , pos ) ; } catch ( Exception t ) { LOGGER . error ( t . toString ( ) , t ) ; } finally { code_block = TryStatement ;  } }
public void test() { try { pos . close ( ) ; } catch ( IOException e ) { LOGGER . error ( e . toString ( ) , e ) ; } }
public boolean clientExists ( Integer demographicNo ) { boolean exists = getHibernateTemplate ( ) . get ( Demographic . class , demographicNo ) != null ; log . debug ( "exists: " + exists ) ; return exists ; }
protected void onPageBiggerThanMaxSize ( String urlStr , long pageSize ) { logger . warn ( "Skipping a URL: {} which was bigger ( {} ) than max allowed size" , urlStr , pageSize ) ; }
public void test() { try { code_block = IfStatement ; checkDirectoriesContainSameContent ( getExpectedResourcesTestDir ( ) , targetTestDir ) ; } catch ( DifferentDirectoryContentException e ) { String msg = "Generated resources do not match the expected resource (Use -DoverrideExpectedTestFiles=true " + "to run test and override expected test files instead of doing this check)" ; logger . debug ( msg , e ) ; Assert . fail ( msg + "\n" + e . getMessage ( ) ) ; } }
public void test() { try { Iterator < Row > resultIterator = result . iterator ( ) ; code_block = WhileStatement ; logger . info ( context , "ShadowUserProcessor:getSyncCallback:SUCCESS:SYNC CALLBACK SUCCESSFULLY MIGRATED  ALL Shadow user" ) ; } catch ( Exception e ) { logger . error ( context , "ShadowUserProcessor:getSyncCallback:SUCCESS:ERROR OCCURRED WHILE GETTING SYNC CALLBACKS" , e ) ; } }
@ Override public void beginWindow ( long windowId ) { currentWindowId = windowId ; store . beginTransaction ( ) ; logger . debug ( "Transaction started for window {}" , windowId ) ; }
public void test() { try { Method m = wrapee . getMethod ( method . getName ( ) , method . getParameterTypes ( ) ) ; this . methods . put ( m , method ) ; } catch ( NoSuchMethodException e ) { log . error ( e , e ) ; } }
@ Path ( "/entity/{entityId}/groups" ) @ GET public String getGroups ( @ PathParam ( "entityId" ) String entityId , @ QueryParam ( "identityType" ) String idType ) throws EngineException , JsonProcessingException { log . debug ( "getGroups query to " + entityId ) ; Map < String , GroupMembership > groups = identitiesMan . getGroups ( getEP ( entityId , idType ) ) ; return mapper . writeValueAsString ( groups . keySet ( ) ) ; }
public void deregisterStreamConsumer ( final String stream ) throws InterruptedException , ExecutionException { LOG . debug ( "Deregistering stream consumer - {}" , stream ) ; int attempt = 1 ; String streamConsumerArn = getStreamConsumerArn ( stream ) ; deregistrationBackoff ( configuration , backoff , attempt ++ ) ; Optional < DescribeStreamConsumerResponse > response = describeStreamConsumer ( streamConsumerArn ) ; code_block = IfStatement ; waitForConsumerToDeregister ( response . orElse ( null ) , streamConsumerArn , attempt ) ; LOG . debug ( "Deregistered stream consumer - {}" , streamConsumerArn ) ; }
public void test() { try { List < TopicPartitionGroup > topicPartitionGroups = partitionGroupServerService . findByTopic ( topic , namespace ) ; code_block = IfStatement ; code_block = IfStatement ; return findPartitionGroupLeaderBroker ( topicPartitionGroups ) ; } catch ( Exception e ) { logger . error ( "" , e ) ; throw new ServiceException ( ServiceException . INTERNAL_SERVER_ERROR , e . getMessage ( ) , e ) ; } }
public void test() { if ( this . task . getStatus ( ) == TaskState . CANCELED ) { JOB_LOGGER . debug ( "Ignoring notification for cancelled job " + job . getJobId ( ) ) ; } else { code_block = IfStatement ; } }
public void test() { if ( this . isCancelling ( ) ) { JOB_LOGGER . debug ( "Received a notification for cancelled job " + job . getJobId ( ) ) ; doOutputTransfers ( job ) ; notifyError ( ) ; } else { int jobId = job . getJobId ( ) ; JOB_LOGGER . error ( "Received a notification for job " + jobId + " with state FAILED" ) ; JOB_LOGGER . error ( "Job " + job . getJobId ( ) + ", running Task " + this . task . getId ( ) + " on worker " + this . getAssignedResource ( ) . getName ( ) + ", has failed." ) ; ErrorManager . warn ( "Job " + job . getJobId ( ) + ", running Task " + this . task . getId ( ) + " on worker " + this . getAssignedResource ( ) . getName ( ) + ", has failed." ) ; ++ this . executionErrors ; code_block = IfStatement ; } }
public ExpirationConfiguration loadExpirationConfiguration ( ) { ExpirationConfiguration result ; LOG . debug ( "begin loadExpirationConfiguration()" ) ; final String propertyDir = PropertyAccessor . getInstance ( ) . getPropertyFileLocation ( ) ; LOG . debug ( "Property Directory: " + propertyDir ) ; result = loadExpirationConfiguration ( propertyDir , FTA_CONFIG_FILE ) ; return result ; }
public ExpirationConfiguration loadExpirationConfiguration ( ) { ExpirationConfiguration result ; LOG . debug ( "begin loadExpirationConfiguration()" ) ; final String propertyDir = PropertyAccessor . getInstance ( ) . getPropertyFileLocation ( ) ; LOG . debug ( "Property Directory: " + propertyDir ) ; result = loadExpirationConfiguration ( propertyDir , FTA_CONFIG_FILE ) ; return result ; }
public void test() { try { Document document = null ; code_block = IfStatement ; StringWriter stringOut = new StringWriter ( ) ; DOMImplementationLS domImpl = ( DOMImplementationLS ) document . getImplementation ( ) ; LSSerializer lsSerializer = domImpl . createLSSerializer ( ) ; lsSerializer . getDomConfig ( ) . setParameter ( XML_DECLARATION , false ) ; LSOutput lsout = domImpl . createLSOutput ( ) ; lsout . setEncoding ( encoding ) ; lsout . setCharacterStream ( stringOut ) ; code_block = IfStatement ; lsSerializer . write ( n , lsout ) ; return stringOut . toString ( ) ; } catch ( DOMException | LSException e ) { LOGGER . debug ( e . getMessage ( ) , e ) ; } }
@ Test public void testCreateGRETunnelTemplate ( ) { template = "/VM_files/createTunnel.vm" ; String message = callGRETunnelVelocity ( template , newParamsGRETunnelService ( ) ) ; Assert . assertNotNull ( message ) ; log . info ( message ) ; }
public void sendFeatureList ( List < Feature > featureList , String action ) throws ArcgisException { LOGGER . debug ( action + " feature list(" + featureList . size ( ) + "), into Feature table: " + serviceUrl ) ; Map < String , String > params = new LinkedHashMap < String , String > ( ) ; params . put ( OUT_SPATIAL_REFERENCE_PARAM , DEFAULT_SPATIAL_REFERENCE ) ; code_block = IfStatement ; params . put ( ROLLBACK_ON_FAILURE_PARAM , "true" ) ; Map < String , String > bodyParams = new LinkedHashMap < String , String > ( ) ; bodyParams . put ( FEATURES_PARAM , featureListToStrArray ( featureList ) ) ; bodyParams . put ( OUTPUT_FORMAT_PARAM , DEFAULT_OUTPUT_FORMAT ) ; String fullUrl = serviceUrl . toString ( ) ; code_block = IfStatement ; HttpResponse response = httpPost ( fullUrl , params , bodyParams ) ; LOGGER . debug ( "Response code: " + response . getResponseCode ( ) + "\n\t" + response . getBody ( ) ) ; checkResponse ( response ) ; }
public void sendFeatureList ( List < Feature > featureList , String action ) throws ArcgisException { LOGGER . debug ( action + " feature list(" + featureList . size ( ) + "), into Feature table: " + serviceUrl ) ; Map < String , String > params = new LinkedHashMap < String , String > ( ) ; params . put ( OUT_SPATIAL_REFERENCE_PARAM , DEFAULT_SPATIAL_REFERENCE ) ; code_block = IfStatement ; params . put ( ROLLBACK_ON_FAILURE_PARAM , "true" ) ; Map < String , String > bodyParams = new LinkedHashMap < String , String > ( ) ; bodyParams . put ( FEATURES_PARAM , featureListToStrArray ( featureList ) ) ; bodyParams . put ( OUTPUT_FORMAT_PARAM , DEFAULT_OUTPUT_FORMAT ) ; String fullUrl = serviceUrl . toString ( ) ; code_block = IfStatement ; HttpResponse response = httpPost ( fullUrl , params , bodyParams ) ; LOGGER . debug ( "Response code: " + response . getResponseCode ( ) + "\n\t" + response . getBody ( ) ) ; checkResponse ( response ) ; }
public void test() { switch ( columnIndex ) { case 0 : return Formatter . formatString ( sru . getName ( ) ) ; case 1 : return sru . getType ( ) ; case 2 : return calculate . get ( rowIndex ) ; default : LOG . error ( "Unknown column " + columnIndex ) ; return new String ( "" ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Successfully connected to " + context . getDescription ( ) ) ; } }
@ Override public void run ( ) { LOG . info ( "doing async send..." ) ; code_block = TryStatement ;  sendDoneLatch . countDown ( ) ; LOG . info ( "done async send" ) ; }
public void test() { try { produceMessage ( session , destination ) ; } catch ( JMSException e ) { LOG . info ( "got send exception: " , e ) ; } }
@ Override public void run ( ) { LOG . info ( "doing async send..." ) ; code_block = TryStatement ;  sendDoneLatch . countDown ( ) ; LOG . info ( "done async send" ) ; }
public User login ( String userOrEmail , String userpass ) throws OmException { List < User > users = em . createNamedQuery ( "getUserByLoginOrEmail" , User . class ) . setParameter ( "userOrEmail" , userOrEmail ) . setParameter ( "type" , Type . USER ) . getResultList ( ) ; log . debug ( "login:: {} users were found" , users . size ( ) ) ; code_block = IfStatement ; User u = users . get ( 0 ) ; code_block = IfStatement ; code_block = IfStatement ; log . debug ( "login user groups {}" , u . getGroupUsers ( ) ) ; code_block = IfStatement ; u . setLastlogin ( new Date ( ) ) ; return update ( u , u . getId ( ) ) ; }
public User login ( String userOrEmail , String userpass ) throws OmException { List < User > users = em . createNamedQuery ( "getUserByLoginOrEmail" , User . class ) . setParameter ( "userOrEmail" , userOrEmail ) . setParameter ( "type" , Type . USER ) . getResultList ( ) ; log . debug ( "login:: {} users were found" , users . size ( ) ) ; code_block = IfStatement ; User u = users . get ( 0 ) ; code_block = IfStatement ; code_block = IfStatement ; log . debug ( "login user groups {}" , u . getGroupUsers ( ) ) ; code_block = IfStatement ; u . setLastlogin ( new Date ( ) ) ; return update ( u , u . getId ( ) ) ; }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Resource has no File reference: {}" , resource ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Resource has no URI reference: {}" , resource ) ; } }
public void test() { try { exec ( stmt , "DROP ROLE " + roleName ) ; } catch ( Exception ex ) { LOGGER . info ( "test role doesn't exist, but it's ok" ) ; } finally { exec ( stmt , "CREATE ROLE " + roleName ) ; } }
@ SuppressWarnings ( "ThrowableResultOfMethodCallIgnored" ) @ Override public void exceptionCaught ( ChannelHandlerContext ctx , Throwable cause ) { code_block = IfStatement ; LOG . warn ( "Unexpected exception. Closing channel {}" , ctx . channel ( ) , cause ) ; ctx . channel ( ) . close ( ) ; }
public void test() { try ( ProcessReader processReader = new ProcessReader ( process ) ) { processReader . readAll ( ) ; final String errOutput = processReader . getError ( ) ; code_block = IfStatement ; final String verboseJson = Arrays . stream ( processReader . getOutput ( ) . split ( "\n" ) ) . filter ( line -> line . contains ( "Audit Request" ) ) . findFirst ( ) . get ( ) ; String auditRequest ; code_block = TryStatement ;  LOGGER . debug ( "Audit Request: {}" , auditRequest ) ; return Json . createReader ( IOUtils . toInputStream ( auditRequest , StandardCharsets . UTF_8 ) ) . readObject ( ) ; } catch ( InterruptedException ex ) { Thread . currentThread ( ) . interrupt ( ) ; throw new AnalysisException ( "Yarn audit process was interrupted." , ex ) ; } }
public void test() { try { fileSystem . createTags ( createTagsRequest ) ; return Boolean . TRUE ; } catch ( AmazonServiceException ase ) { logger . error ( "error tagging efs - > " + resourceId , ase ) ; throw ase ; } }
public void test() { try { ProjectFile file = new ProjectFile ( event . getUploadedFile ( ) ) ; uploadedFiles . add ( file ) ; String fileName = file . getName ( ) ; setFileName ( fileName ) ; code_block = IfStatement ; } catch ( IOException e ) { log . error ( e . getMessage ( ) , e ) ; WebStudioUtils . addErrorMessage ( "Error occurred during uploading file." , e . getMessage ( ) ) ; } }
public void start ( String ip , int port , String keyspace , int dataTableDaysTimeArea , int slotSecondsTimeArea ) throws Exception { code_block = IfStatement ; this . dataTableDaysTimeArea = dataTableDaysTimeArea ; this . slotSecondsTimeArea = slotSecondsTimeArea ; Builder builder = Cluster . builder ( ) ; builder . withPort ( port ) ; builder . addContactPoint ( ip ) ; this . cluster = builder . build ( ) ; Metadata metadata = cluster . getMetadata ( ) ; logger . info ( String . format ( "Connected to cluster: %s\n" , metadata . getClusterName ( ) ) ) ; code_block = ForStatement ; session = cluster . connect ( ) ; session . execute ( "USE \"" + keyspace + "\"" ) ; this . started = true ; }
public void test() { for ( Host host : metadata . getAllHosts ( ) ) { logger . info ( String . format ( "Datacenter: %s; Host: %s; Rack: %s\n" , host . getDatacenter ( ) , host . getAddress ( ) , host . getRack ( ) ) ) ; } }
public void test() { if ( ! pd . validate ( ) ) { LOGGER . debug ( "ProcessDescription is not valid. Removing " + processID + " from Repository." ) ; this . registeredProcessDescriptions . remove ( processID ) ; this . registeredAlgorithmParameters . remove ( processID ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( String . format ( "Use default network interface to bind %s" , address ) ) ; } }
public void test() { if ( null == network ) { return delegate . createSocket ( address , port , localAddr , localPort ) ; } }
public void test() { try { listener . onRemoval ( identifiable ) ; } catch ( Throwable t ) { LOGGER . error ( t . toString ( ) , t ) ; } }
public void test() { if ( pool == null || pool . isTerminating ( ) || pool . isShutdown ( ) ) { pool = getDefaultExecutor ( builder ) ; LOG . info ( "Creating new pool to " + builder . getName ( ) ) ; poolCache . put ( builder . getName ( ) , pool ) ; } }
public void test() { if ( ! zippedFile . delete ( ) ) { LOG . warn ( "Failed to delete temp file[%s]" , zippedFile ) ; } }
public static File extractFile ( String fileName , String targetInfix , TokenResolver tokenResolver ) throws IOException { InputStream in = Externalization . class . getClassLoader ( ) . getResourceAsStream ( fileName ) ; if ( in == null ) return null ; File target = getTempFile ( fileName . replace ( "." , "_" + targetInfix + "." ) ) ; log . info ( "Extracting " + fileName + " to " + target ) ; Reader reader = new TokenReplacingReader ( new InputStreamReader ( in ) , tokenResolver ) ; FileWriter writer = new FileWriter ( target ) ; copyAndClose ( reader , writer ) ; return target ; }
public void test() { if ( null == configCenterConfigurationSource ) { LOGGER . info ( "none of config center source enabled." ) ; return null ; } }
@ Override public void addOrder ( Transaction transaction , ColoredCoinsBidOrderPlacement attachment ) { final BidOrder order = new BidOrder ( transaction , attachment , blockchain . getHeight ( ) ) ; log . trace ( ">> addOrder() bidOrder={}" , order ) ; bidOrderTable . insert ( order ) ; }
public void test() { if ( StringUtils . hasText ( generatorBeanId ) ) { orderNumberGenerator = Context . getRegisteredComponent ( generatorBeanId , OrderNumberGenerator . class ) ; log . info ( "Successfully set the configured order number generator" ) ; } else { orderNumberGenerator = this ; log . info ( "Setting default order number generator" ) ; } }
public void test() { if ( StringUtils . hasText ( generatorBeanId ) ) { orderNumberGenerator = Context . getRegisteredComponent ( generatorBeanId , OrderNumberGenerator . class ) ; log . info ( "Successfully set the configured order number generator" ) ; } else { orderNumberGenerator = this ; log . info ( "Setting default order number generator" ) ; } }
public void test() { if ( ! ( t instanceof OperationFailureException ) ) { _logger . warn ( String . format ( "unhandled exception happened when calling %s" , task . getClass ( ) . getName ( ) ) , t ) ; } }
public void test() { { int c = count . incrementAndGet ( ) ; LOG . info ( "asyn call time " + c ) ; command . run ( ) ; } }
public void test() { if ( isValid ( map ) ) { final String name = ( String ) map . get ( StructrLicenseManager . NameKey ) ; final byte [ ] response = name . getBytes ( "utf-8" ) ; socket . getOutputStream ( ) . write ( sign ( response ) ) ; socket . getOutputStream ( ) . flush ( ) ; } else { logger . info ( "License verification failed." ) ; } }
public void test() { if ( isDebug ) { LOG . info ( "BOLT ack TASK: {} TIME: {} TUPLE: {}" , taskId , delta , input ) ; } }
public void test() { if ( log . isTraceEnable ( ) ) { log . info ( this , "HttpServiceComponent[" + this . cName + "] for feature[" + this . feature + "] started SUCCESS: port=" + this . port ) ; } }
public void test() { try { return new Pair < > ( ttk . getServer ( ) . toString ( ) , createNewTransport ( ttk ) ) ; } catch ( TTransportException tte ) { log . debug ( "Failed to connect to {}" , servers . get ( index ) , tte ) ; servers . remove ( index ) ; retryCount ++ ; } }
private void flushBuffer ( final boolean scheduled ) { code_block = IfStatement ; code_block = IfStatement ; final List < BatchEntry < ? , O > > entries = new ArrayList < > ( _maxBatchSize ) ; final int batchSize = _queue . drainTo ( entries ) ; code_block = IfStatement ; final int batchNumber = _batchNo . incrementAndGet ( ) ; logger . info ( "Batch {} - Preparing {} entries, scheduled={}" , batchNumber , batchSize , scheduled ) ; final Object [ ] input = new Object [ batchSize ] ; code_block = ForStatement ; final BatchSource < I > source = new ArrayBatchSource < > ( input ) ; final BatchEntryBatchSink < O > sink = new BatchEntryBatchSink < > ( entries ) ; _transformation . map ( source , sink ) ; logger . info ( "Batch {} - Finished" , batchNumber , batchSize ) ; }
public void test() { for ( CamelEndpointDetails detail : endpoints ) { LOG . info ( detail . getEndpointUri ( ) ) ; } }
public void test() { try { XcesBodyBasic parasBasic = ( XcesBodyBasic ) unmarshallerBasic . unmarshal ( xmlEventReaderBasic , XcesBodyBasic . class ) . getValue ( ) ; readPara ( jb , parasBasic ) ; } catch ( RuntimeException ex ) { getLogger ( ) . warn ( "Input is not in basic xces format." ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( LayoutPageTemplateEntryServiceUtil . class , "getLayoutPageTemplateEntries" , _getLayoutPageTemplateEntriesParameterTypes12 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , type , status , start , end , orderByComparator ) ; Object returnObj = null ; code_block = TryStatement ;  return ( java . util . List < com . liferay . layout . page . template . model . LayoutPageTemplateEntry > ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { SessionObject sesObj = ( SessionObject ) request . getSession ( ) . getAttribute ( FdahpStudyDesignerConstants . SESSION_OBJECT ) ; code_block = IfStatement ; jsonobject . put ( FdahpStudyDesignerConstants . MESSAGE , message ) ; response . setContentType ( FdahpStudyDesignerConstants . APPLICATION_JSON ) ; out = response . getWriter ( ) ; out . print ( jsonobject ) ; } catch ( Exception e ) { logger . error ( "StudyController - deleteComprehensionTestQuestion - ERROR" , e ) ; } }
public void test() { try { bufferedReader = new BufferedReader ( new FileReader ( notebookJson ) ) ; String line ; code_block = WhileStatement ; } catch ( Exception e ) { LOG . error ( "Exception in NotebookRestApi while get notebook info" , e ) ; return new JsonResponse < > ( Response . Status . INTERNAL_SERVER_ERROR , e . getMessage ( ) , ExceptionUtils . getStackTrace ( e ) ) . build ( ) ; } }
public void test() { for ( PluginWrapper plugin : getStartedPlugins ( ) ) { Class pluginClass = plugin . getPlugin ( ) . getClass ( ) ; LOG . info ( "Found plugin: {}" , plugin . getDescriptor ( ) . getPluginId ( ) ) ; GenericApplicationContext pluginContext = ( GenericApplicationContext ) ( ( Plugin ) plugin . getPlugin ( ) ) . getApplicationContext ( ) ; pluginContext . setParent ( applicationContext ) ; } }
public void test() { if ( ( ( EmptyDeviceResponse ) deviceResponse ) . getStatus ( ) . equals ( DeviceMessageStatus . OK ) ) { LOGGER . info ( "setLight() successful for device : {}" , deviceResponse . getDeviceIdentification ( ) ) ; } else { PublicLightingSetLightRequestMessageProcessor . this . handleEmptyDeviceResponse ( deviceResponse , PublicLightingSetLightRequestMessageProcessor . this . responseMessageSender , domain , domainVersion , messageType , retryCount ) ; } }
public void test() { if ( header != null && header . getIndex ( ) < 0 ) { log . trace ( "Message " + msg . getSrc ( ) + " -> " + msg . getDest ( ) + " with workerIndex -1" ) ; } else-if ( header != null && allowedWorkers != null ) { code_block = IfStatement ; } }
public void test() { if ( ! allowedWorkers . contains ( header . getIndex ( ) ) ) { log . trace ( "Discarding message " + msg . getSrc ( ) + " -> " + msg . getDest ( ) + " with workerIndex " + header . getIndex ( ) ) ; return null ; } }
public void test() { if ( pluginInfo == null ) { logger . error ( "Upon plug-in service registration - " + " can not create a PluginInfo object!" ) ; } else { registeredPlugins . put ( pluginInfo . getHashcode ( ) , pluginInfo ) ; logger . info ( "Plug-in service (" + pluginInfo . getPluginName ( ) + ")" + " was registered." ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "acceptReducedValues: Accepted one set with " + numReducers + " aggregated values" ) ; } }
public void test() { try { return nw . getMatching ( pattern ) ; } catch ( final IOException ex ) { log . debug ( "Failed to getMatching." ) ; } }
public void test() { try { mailer . sendTaskCompletedMail ( taskHistory . get ( ) , requestWithState . get ( ) . getRequest ( ) ) ; } catch ( Throwable t ) { LOG . error ( "While trying to send task completed mail for {}" , taskId , t ) ; } finally { SingularityDeleteResult result = taskManager . deleteFinishedTaskMailQueue ( taskId ) ; LOG . debug ( "Task {} mail sent with status {} (delete result {})" , taskId , shouldSendState , result ) ; } }
public void test() { try { mailer . sendTaskCompletedMail ( taskHistory . get ( ) , requestWithState . get ( ) . getRequest ( ) ) ; } catch ( Throwable t ) { LOG . error ( "While trying to send task completed mail for {}" , taskId , t ) ; } finally { SingularityDeleteResult result = taskManager . deleteFinishedTaskMailQueue ( taskId ) ; LOG . debug ( "Task {} mail sent with status {} (delete result {})" , taskId , shouldSendState , result ) ; } }
public void test() { try { logger . debug ( "New deployment {} has been discovered and will be deployed" , name ) ; DeploymentConfig deployment = deploymentFactory . newDeployment ( deploymentConfig ) ; addedDeploymentEvent . fire ( new DeploymentConfigChangedEvent ( deployment . getDeploymentUnit ( ) ) ) ; registeredDeployments . put ( deployment . getIdentifier ( ) , deployment ) ; logger . debug ( "Deployment {} deployed successfully" , name ) ; } catch ( RuntimeException e ) { logger . warn ( "Deployment {} failed to deploy due to {}" , name , e . getMessage ( ) , e ) ; } }
public void test() { try { logger . debug ( "New deployment {} has been discovered and will be deployed" , identifier ) ; DeploymentConfig deployment = registeredDeployments . remove ( identifier ) ; removedDeploymentEvent . fire ( new DeploymentConfigChangedEvent ( deployment . getDeploymentUnit ( ) ) ) ; logger . debug ( "Deployment {} undeployed successfully" , identifier ) ; } catch ( RuntimeException e ) { logger . warn ( "Undeployment {} failed to deploy due to {}" , identifier , e . getMessage ( ) , e ) ; } }
public void stopCounting ( ) { runCounting = false ; LOG . debug ( "Stop counting..." ) ; stopLogReport ( ) ; }
public void addHandler ( JettyHandler handler ) { LOGGER . info ( "Bind handler {} into jetty server {}:{}" , handler . getClass ( ) . getSimpleName ( ) , jettyServerConfig . getHost ( ) , jettyServerConfig . getPort ( ) ) ; ServletHolder servletHolder = new ServletHolder ( ) ; servletHolder . setServlet ( handler ) ; servletContextHandler . addServlet ( servletHolder , handler . pathSpec ( ) ) ; }
@ Test public void testManyFiles ( ) throws Exception { Session s = conn . createSession ( true , Session . SESSION_TRANSACTED ) ; Queue jmsQueue = s . createQueue ( address . toString ( ) ) ; MessageProducer p = s . createProducer ( jmsQueue ) ; p . setDeliveryMode ( DeliveryMode . PERSISTENT ) ; conn . start ( ) ; code_block = ForStatement ; s . commit ( ) ; Assert . assertTrue ( server . getStorageManager ( ) . getJournalSequentialFileFactory ( ) . getCriticalAnalyzer ( ) . getNumberOfComponents ( ) < 10 ) ; log . debug ( "Number of components:" + server . getStorageManager ( ) . getJournalSequentialFileFactory ( ) . getCriticalAnalyzer ( ) . getNumberOfComponents ( ) ) ; }
public void test() { if ( numRows != m . numRows || numCols != m . numCols ) { log . info ( "dimensions bad in dot()" ) ; return 0.0 ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "AncestorQueryIterator init()" ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Token was successfully renewed (new TTL = {} seconds), response: {}" , ttl , bodyAsString ( response . getRestResponse ( ) ) ) ; } }
public void test() { if ( ttl > 1 ) { long delay = TimeUnit . SECONDS . toMillis ( suggestedRefreshInterval ( ttl ) ) ; timer . schedule ( new RenewTokenTask ( ) , delay ) ; } else { LOGGER . warn ( "Token TTL ({}) is not enough for scheduling" , ttl ) ; vault = recreateVault ( vault ) ; } }
public void test() { if ( response . isAuthRenewable ( ) ) { code_block = IfStatement ; } else { LOGGER . warn ( "Vault token is not renewable now" ) ; } }
public void test() { if ( e . getHttpStatusCode ( ) == STATUS_CODE_FORBIDDEN ) { LOGGER . warn ( "Could not renew the Vault token" , e ) ; vault = recreateVault ( vault ) ; } }
public void test() { try { lookup . close ( ) ; } catch ( Exception e ) { LOG . error ( "Unable to cleanup access tracker" , e ) ; } }
public void test() { if ( actualInput != null ) { final InputStream entityStream = new ByteArrayInputStream ( actualInput . getBytes ( StandardCharsets . UTF_8 ) ) ; final NormalizedNodeContext inputContext = JsonNormalizedNodeBodyReader . readFrom ( uriPath , entityStream , true , controllerContext ) ; LOG . debug ( "Parsed YangInstanceIdentifier: {}" , inputContext . getInstanceIdentifierContext ( ) . getInstanceIdentifier ( ) ) ; LOG . debug ( "Parsed NormalizedNode: {}" , inputContext . getData ( ) ) ; outputContext = restconfService . invokeRpc ( uriPath , inputContext , null ) ; } else { outputContext = restconfService . invokeRpc ( uriPath , null , null ) ; } }
public void test() { if ( actualInput != null ) { final InputStream entityStream = new ByteArrayInputStream ( actualInput . getBytes ( StandardCharsets . UTF_8 ) ) ; final NormalizedNodeContext inputContext = JsonNormalizedNodeBodyReader . readFrom ( uriPath , entityStream , true , controllerContext ) ; LOG . debug ( "Parsed YangInstanceIdentifier: {}" , inputContext . getInstanceIdentifierContext ( ) . getInstanceIdentifier ( ) ) ; LOG . debug ( "Parsed NormalizedNode: {}" , inputContext . getData ( ) ) ; outputContext = restconfService . invokeRpc ( uriPath , inputContext , null ) ; } else { outputContext = restconfService . invokeRpc ( uriPath , null , null ) ; } }
public void test() { try { threadPool . shutdown ( ) ; code_block = WhileStatement ; } catch ( InterruptedException e ) { LOG . error ( "Insert obervations thread was interrupted!" , e ) ; } }
public void test() { try { code_block = IfStatement ; result = git . clean ( ) . setCleanDirectories ( true ) . call ( ) ; } catch ( Exception e ) { LOG . error ( "There was an error in Git {} operation" , operation ) ; throw e ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( invalidFileVersionException , invalidFileVersionException ) ; } }
public void test() { try { tm . checkServerTrusted ( chain , authType ) ; logger . debug ( "checkServerTrusted for {} succeeded" , tm ) ; code_block = IfStatement ; } catch ( CertificateException e ) { logger . debug ( "checkServerTrusted for {} failed" , tm ) ; code_block = IfStatement ; code_block = IfStatement ; } }
public void test() { if ( ! Objects . equals ( e , DEFAULT_PUBLIC_EXPONENT ) ) { log . warn ( "decodePrivateKey({}) non-standard RSA exponent found: {}" , keyType , e ) ; } }
public void test() { if ( ! Objects . equals ( n , modulus ) ) { log . warn ( "decodePrivateKey({}) mismatched modulus values: encoded={}, calculated={}" , keyType , n , modulus ) ; } }
public void test() { try { Tasks . setBlockingDetails ( "Installing " + urlToInstall + " at " + machine ) ; return machine . installTo ( resolver , props , urlToInstall , target ) ; } catch ( Exception e ) { Exceptions . propagateIfFatal ( e ) ; lastError = e ; String stack = StackTraceSimplifier . toString ( e ) ; code_block = IfStatement ; log . warn ( "Failed to transfer " + urlToInstall + " to " + machine + ", not a retryable error so failing: " + e ) ; throw Exceptions . propagate ( e ) ; } finally { Tasks . resetBlockingDetails ( ) ; } }
public void test() { try { DecodedJwt decodedJwt = response . getDecodedAccessToken ( ) ; LOGGER . debug ( "Access token: {}" , decodedJwt ) ; } catch ( IllegalArgumentException e ) { LOGGER . debug ( "Access token can not be logged. {}" , e . getMessage ( ) ) ; } }
public void test() { try { DecodedJwt decodedJwt = response . getDecodedAccessToken ( ) ; LOGGER . debug ( "Access token: {}" , decodedJwt ) ; } catch ( IllegalArgumentException e ) { LOGGER . debug ( "Access token can not be logged. {}" , e . getMessage ( ) ) ; } }
public void test() { try { byte [ ] p = queue . poll ( 1 , TimeUnit . SECONDS ) ; code_block = IfStatement ; } catch ( IOException e1 ) { log . error ( "Error while sending " + name + " packet" , e1 ) ; connect ( ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( IOException e1 ) { log . error ( "Error while receiving packet on " + name + " socket" , e1 ) ; connect ( ) ; } }
public void test() { try { owner . connection . setAutoCommit ( true ) ; } catch ( SQLException e ) { log . error ( "Cannot set autoCommit=true" , e ) ; } }
public void test() { try { XStringSubstitution xSS = UNO . XStringSubstitution ( UnoComponent . createComponentWithContext ( UnoComponent . CSS_UTIL_PATH_SUBSTITUTION ) ) ; String myPath = xSS . substituteVariables ( "$(user)/uno_packages/cache/uno_packages/" , true ) ; String oooPath = xSS . substituteVariables ( "$(inst)/share/uno_packages/cache/uno_packages/" , true ) ; String oooPathNew = xSS . substituteVariables ( "$(brandbaseurl)/share/uno_packages/cache/uno_packages/" , true ) ; code_block = IfStatement ; findWollMuxInstallations ( wmInstallations , myPath , false ) ; findWollMuxInstallations ( wmInstallations , oooPath , true ) ; code_block = IfStatement ; } catch ( NoSuchElementException e ) { LOGGER . error ( "" , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "to be created, intake common" ) ; logger . debug ( objectAsXmlString ( intake , IntakesCommon . class ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "to be created, intake common" ) ; logger . debug ( objectAsXmlString ( intake , IntakesCommon . class ) ) ; } }
public MbZielobjTypTxt findById ( sernet . gs . reveng . MbZielobjTypTxtId id ) { log . debug ( "getting MbZielobjTypTxt instance with id: " + id ) ; code_block = TryStatement ;  }
public void test() { if ( instance == null ) { log . debug ( "get successful, no instance found" ) ; } else { log . debug ( "get successful, instance found" ) ; } }
public void test() { if ( instance == null ) { log . debug ( "get successful, no instance found" ) ; } else { log . debug ( "get successful, instance found" ) ; } }
public void test() { try { MbZielobjTypTxt instance = ( MbZielobjTypTxt ) sessionFactory . getCurrentSession ( ) . get ( "sernet.gs.reveng.MbZielobjTypTxt" , id ) ; code_block = IfStatement ; return instance ; } catch ( RuntimeException re ) { log . error ( "get failed" , re ) ; throw re ; } }
public void test() { try { var reader = new StringReader ( publicPart ) ; var readerPem = new PemReader ( reader ) ; var obj = readerPem . readPemObject ( ) ; readerPem . close ( ) ; return KeyFactory . getInstance ( algorithm ) . generatePublic ( new X509EncodedKeySpec ( obj . getContent ( ) ) ) ; } catch ( InvalidKeySpecException | NoSuchAlgorithmException | IOException e ) { logger . warn ( "Exception loading public key from PEM" , e ) ; return null ; } }
@ Path ( "/entity/{entityId}/credential/{credential}/status/{status}" ) @ PUT public void setCredentialStatus ( @ PathParam ( "entityId" ) String entityId , @ PathParam ( "credential" ) String credential , @ QueryParam ( "identityType" ) String idType , @ PathParam ( "status" ) String status ) throws EngineException , JsonProcessingException { log . info ( "setCredential {} status for {} to {}" , credential , entityId , status ) ; LocalCredentialState desiredCredentialState = LocalCredentialState . valueOf ( status ) ; entityCredMan . setEntityCredentialStatus ( getEP ( entityId , idType ) , credential , desiredCredentialState ) ; }
@ Test public void testBbox1 ( ) { Document doc = getAsDOM ( "wfs?request=GetFeature&version=1.1.0&typename=gsml:MappedFeature&srsName=EPSG:4979&bbox=-200,-200,0,200,200,50" ) ; LOGGER . info ( "WFS GetFeature&typename=gsml:MappedFeature response:\n" + prettyString ( doc ) ) ; assertXpathCount ( 0 , "//gsml:MappedFeature[@gml:id='gsml.mappedfeature.mf1']" , doc ) ; assertXpathCount ( 1 , "//gsml:MappedFeature[@gml:id='gsml.mappedfeature.mf2']" , doc ) ; assertXpathCount ( 1 , "//gsml:MappedFeature[@gml:id='gsml.mappedfeature.mf3']" , doc ) ; assertXpathCount ( 0 , "//gsml:MappedFeature[@gml:id='gsml.mappedfeature.mf4']" , doc ) ; assertXpathEvaluatesTo ( "167.9388 -29.0434 7" , "//gsml:MappedFeature[@gml:id='gsml.mappedfeature.mf2']/gsml:shape/gml:Point/gml:pos" , doc ) ; assertXpathEvaluatesTo ( "3" , "//gsml:MappedFeature[@gml:id='gsml.mappedfeature.mf2']/gsml:shape/gml:Point/@srsDimension" , doc ) ; assertXpathEvaluatesTo ( "http://www.opengis.net/gml/srs/epsg.xml4979" , "//gsml:MappedFeature[@gml:id='gsml.mappedfeature.mf2']/gsml:shape/gml:Point/@srsName" , doc ) ; }
private ServerConnector createHttpConnector ( Config config ) { LOG . info ( "Setting up HTTP connector for web server" ) ; final ServerConnector httpConnector = new ServerConnector ( jettyServer , new HttpConnectionFactory ( baseHttpConfig ( ) ) ) ; httpConnector . setPort ( config . getInt ( DrillOnYarnConfig . HTTP_PORT ) ) ; return httpConnector ; }
@ Override public FileStatus [ ] listStatus ( Path f ) throws FileNotFoundException , IOException { LOG . debug ( "List status of {}" , f . toString ( ) ) ; return listStatus ( f , null ) ; }
public void test() { if ( e instanceof org . apache . airavata . model . error . InvalidRequestException ) { result . ire = ( org . apache . airavata . model . error . InvalidRequestException ) e ; result . setIreIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AiravataClientException ) { result . ace = ( org . apache . airavata . model . error . AiravataClientException ) e ; result . setAceIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AiravataSystemException ) { result . ase = ( org . apache . airavata . model . error . AiravataSystemException ) e ; result . setAseIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AuthorizationException ) { result . ae = ( org . apache . airavata . model . error . AuthorizationException ) e ; result . setAeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof org . apache . airavata . model . error . InvalidRequestException ) { result . ire = ( org . apache . airavata . model . error . InvalidRequestException ) e ; result . setIreIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AiravataClientException ) { result . ace = ( org . apache . airavata . model . error . AiravataClientException ) e ; result . setAceIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AiravataSystemException ) { result . ase = ( org . apache . airavata . model . error . AiravataSystemException ) e ; result . setAseIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AuthorizationException ) { result . ae = ( org . apache . airavata . model . error . AuthorizationException ) e ; result . setAeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof org . apache . airavata . model . error . InvalidRequestException ) { result . ire = ( org . apache . airavata . model . error . InvalidRequestException ) e ; result . setIreIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AiravataClientException ) { result . ace = ( org . apache . airavata . model . error . AiravataClientException ) e ; result . setAceIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AiravataSystemException ) { result . ase = ( org . apache . airavata . model . error . AiravataSystemException ) e ; result . setAseIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AuthorizationException ) { result . ae = ( org . apache . airavata . model . error . AuthorizationException ) e ; result . setAeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { try { fcall . sendResponse ( fb , msg , msgType , seqid ) ; } catch ( java . lang . Exception ex ) { _LOGGER . error ( "Exception writing to internal frame buffer" , ex ) ; fb . close ( ) ; } }
public void test() { try { JsonNode location = jsonResults . get ( j ) . get ( "locations" ) . get ( 0 ) ; geocodedAddresses . add ( getGeocodedAddressFromLocationNode ( location ) ) ; } catch ( Exception ex ) { logger . warn ( "Error retrieving GeocodedAddress from MapQuest response " + json , ex ) ; geocodedAddresses . add ( new GeocodedAddress ( ) ) ; } }
public void test() { try { json = UrlRequest . getResponseFromUrl ( url ) ; code_block = IfStatement ; return geocodedAddresses ; } catch ( MalformedURLException ex ) { logger . error ( "Malformed MapQuest url!" , ex ) ; } catch ( UnsupportedEncodingException ex ) { logger . error ( "UTF-8 Unsupported?!" , ex ) ; } catch ( IOException ex ) { logger . error ( "Error opening API resource! " + ex . toString ( ) + " Response: " + json ) ; } catch ( NullPointerException ex ) { logger . error ( "MapQuest response was not formatted correctly. Response: " + json , ex ) ; } catch ( Exception ex ) { logger . error ( "" + ex ) ; } }
public void test() { try { json = UrlRequest . getResponseFromUrl ( url ) ; code_block = IfStatement ; return geocodedAddresses ; } catch ( MalformedURLException ex ) { logger . error ( "Malformed MapQuest url!" , ex ) ; } catch ( UnsupportedEncodingException ex ) { logger . error ( "UTF-8 Unsupported?!" , ex ) ; } catch ( IOException ex ) { logger . error ( "Error opening API resource! " + ex . toString ( ) + " Response: " + json ) ; } catch ( NullPointerException ex ) { logger . error ( "MapQuest response was not formatted correctly. Response: " + json , ex ) ; } catch ( Exception ex ) { logger . error ( "" + ex ) ; } }
public void test() { try { json = UrlRequest . getResponseFromUrl ( url ) ; code_block = IfStatement ; return geocodedAddresses ; } catch ( MalformedURLException ex ) { logger . error ( "Malformed MapQuest url!" , ex ) ; } catch ( UnsupportedEncodingException ex ) { logger . error ( "UTF-8 Unsupported?!" , ex ) ; } catch ( IOException ex ) { logger . error ( "Error opening API resource! " + ex . toString ( ) + " Response: " + json ) ; } catch ( NullPointerException ex ) { logger . error ( "MapQuest response was not formatted correctly. Response: " + json , ex ) ; } catch ( Exception ex ) { logger . error ( "" + ex ) ; } }
public void test() { try { json = UrlRequest . getResponseFromUrl ( url ) ; code_block = IfStatement ; return geocodedAddresses ; } catch ( MalformedURLException ex ) { logger . error ( "Malformed MapQuest url!" , ex ) ; } catch ( UnsupportedEncodingException ex ) { logger . error ( "UTF-8 Unsupported?!" , ex ) ; } catch ( IOException ex ) { logger . error ( "Error opening API resource! " + ex . toString ( ) + " Response: " + json ) ; } catch ( NullPointerException ex ) { logger . error ( "MapQuest response was not formatted correctly. Response: " + json , ex ) ; } catch ( Exception ex ) { logger . error ( "" + ex ) ; } }
public void test() { try { json = UrlRequest . getResponseFromUrl ( url ) ; code_block = IfStatement ; return geocodedAddresses ; } catch ( MalformedURLException ex ) { logger . error ( "Malformed MapQuest url!" , ex ) ; } catch ( UnsupportedEncodingException ex ) { logger . error ( "UTF-8 Unsupported?!" , ex ) ; } catch ( IOException ex ) { logger . error ( "Error opening API resource! " + ex . toString ( ) + " Response: " + json ) ; } catch ( NullPointerException ex ) { logger . error ( "MapQuest response was not formatted correctly. Response: " + json , ex ) ; } catch ( Exception ex ) { logger . error ( "" + ex ) ; } }
public void test() { if ( conflictExists ) { LOGGER . warn ( "You can use @Routeorder() to ensure the routes are not executed in random order" ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Error validating standard format DMS string." , e ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Error validating standard format DMS pattern." , e ) ; } }
public void test() { try { BufferedImage i = ImageIO . read ( new ByteArrayInputStream ( image . getBytes ( ) ) ) ; images . add ( i ) ; } catch ( IOException e ) { logger . debug ( "IOException creating BufferedImage from downloaded image: {}" , e . getMessage ( ) ) ; } }
public void test() { if ( images . size ( ) < numberOfImages ) { logger . debug ( "Some images could not be downloaded: wanted={}, actual={}" , numberOfImages , images . size ( ) ) ; } }
@ Override public boolean isEmpty ( ) { logger . debug ( "Checking if the atom container set empty: " , atomContainerCount == 0 ) ; return atomContainerCount == 0 ; }
public void test() { if ( sessionThread != null ) { sessionThread . interrupt ( ) ; } else { logger . debug ( "{} is a {} which is not interruptable as the thread running the session has not " + "been set - please check the implementation if this is not desirable" , sessionId , this . getClass ( ) . getSimpleName ( ) ) ; } }
@ Override protected void onStop ( ) { LOG . info ( "Twilight plugin stopped " ) ; setPollingWait ( - 1 ) ; }
public void test() { try { logger . info ( "Processing schedule MatchManagerTask..." + matchManager . processEvent ( null , IMatchManager . Event . SCHEDULED_EVENT ) ) ; } catch ( MatchManagerException e ) { logger . error ( "Error while processing scheduled MatchManager scheduled task.." ) ; logger . error ( "error" , e ) ; } }
public void test() { try { RemediationRestClient . delete ( model . getObject ( ) . getKey ( ) ) ; SyncopeConsoleSession . get ( ) . success ( getString ( Constants . OPERATION_SUCCEEDED ) ) ; target . add ( container ) ; } catch ( SyncopeClientException e ) { LOG . error ( "While deleting {}" , model . getObject ( ) . getKey ( ) , e ) ; SyncopeConsoleSession . get ( ) . onException ( e ) ; } }
public void runIteration ( Configuration conf , Path corpusInput , Path modelInput , Path modelOutput , int iterationNumber , int maxIterations , int numReduceTasks ) throws IOException , ClassNotFoundException , InterruptedException { String jobName = String . format ( "Iteration %d of %d, input path: %s" , iterationNumber , maxIterations , modelInput ) ; log . info ( "About to run: {}" , jobName ) ; Job job = prepareJob ( corpusInput , modelOutput , CachingCVB0Mapper . class , IntWritable . class , VectorWritable . class , VectorSumReducer . class , IntWritable . class , VectorWritable . class ) ; job . setCombinerClass ( VectorSumReducer . class ) ; job . setNumReduceTasks ( numReduceTasks ) ; job . setJobName ( jobName ) ; setModelPaths ( job , modelInput ) ; HadoopUtil . delete ( conf , modelOutput ) ; code_block = IfStatement ; }
@ Before public void setUp ( ) throws Exception { log . debug ( "" ) ; log . debug ( "Starting wiser on port " + smtpPort ) ; wiser = startWiser ( smtpPort ) ; setDriver ( new ChromeDriver ( ) ) ; getDriver ( ) . manage ( ) . timeouts ( ) . implicitlyWait ( 30 , TimeUnit . SECONDS ) ; getDriver ( ) . manage ( ) . window ( ) . setSize ( new Dimension ( 1024 , 900 ) ) ; }
@ Before public void setUp ( ) throws Exception { log . debug ( "" ) ; log . debug ( "Starting wiser on port " + smtpPort ) ; wiser = startWiser ( smtpPort ) ; setDriver ( new ChromeDriver ( ) ) ; getDriver ( ) . manage ( ) . timeouts ( ) . implicitlyWait ( 30 , TimeUnit . SECONDS ) ; getDriver ( ) . manage ( ) . window ( ) . setSize ( new Dimension ( 1024 , 900 ) ) ; }
public void test() { try { return qoSInterDirectMeasurementPingRepository . findByMeasurement ( measurement ) ; } catch ( final Exception ex ) { logger . debug ( ex . getMessage ( ) , ex ) ; throw new ArrowheadException ( CoreCommonConstants . DATABASE_OPERATION_EXCEPTION_MSG ) ; } }
public void test() { try { LOG . debug ( "release of transaction member '{}'. " , mem ) ; mem . release ( ) ; } catch ( Throwable t ) { LOG . error ( "release member {}" , mem , t ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Configured Hibernate Dialect:\t" + getHibernateDialect ( ) ) ; } }
public void test() { try { retVal = session . createSQLQuery ( "select dbid, dbms_lob.getlength(obj) from " + ORACLE_TEMP_TABLE_NAME ) . list ( ) ; } catch ( Exception e ) { log . warn ( "table not created yet, trying again" , e ) ; } }
public void test() { switch ( fieldType ) { case STRING : sql += fieldName + " = " + "'" + fieldValue + "'" ; break ; case DATETIME : sql += fieldName + " = " + "'" + new DateTimeColumnParser ( ) . getValue ( fieldValue ) + "'" ; break ; case INT32 : case INT64 : case FLOAT32 : case FLOAT64 : case BIG_INTEGER : sql += fieldName + " = " + fieldValue ; break ; default : log . error ( "fieldType {} is illegal." , fieldType . toString ( ) ) ; } }
protected void writeCommand ( byte [ ] message ) throws SonyProjectorException { logger . debug ( "writeCommand: {}" , HexUtils . bytesToHex ( message ) ) ; code_block = IfStatement ; OutputStream dataOut = this . dataOut ; code_block = IfStatement ; code_block = TryStatement ;  }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Failed to send nameservice request to " + name . name , ioe ) ; } }
public void test() { try { send ( request , response , config . getNetbiosRetryTimeout ( ) ) ; } catch ( InterruptedIOException ioe ) { code_block = IfStatement ; throw new UnknownHostException ( name . name ) ; } catch ( IOException ioe ) { log . info ( "Failed to send nameservice request to " + name . name , ioe ) ; throw new UnknownHostException ( name . name ) ; } }
public void test() { if ( Log . isTraceEnabled ( ) ) { Log . trace ( "Session {} Request ID {}, event complete: {}" , streamID , rid , asyncEvent ) ; } }
@ Override public void process ( ClusterEvent event ) { LOG . info ( "START TaskPersistDataStage.process()" ) ; long startTime = System . currentTimeMillis ( ) ; WorkflowControllerDataProvider cache = event . getAttribute ( AttributeName . ControllerDataProvider . name ( ) ) ; HelixManager manager = event . getAttribute ( AttributeName . helixmanager . name ( ) ) ; cache . getTaskDataCache ( ) . persistDataChanges ( manager . getHelixDataAccessor ( ) ) ; long endTime = System . currentTimeMillis ( ) ; LOG . info ( "END TaskPersistDataStage.process() for cluster {} took {} ms" , cache . getClusterName ( ) , ( endTime - startTime ) ) ; }
@ Override public void process ( ClusterEvent event ) { LOG . info ( "START TaskPersistDataStage.process()" ) ; long startTime = System . currentTimeMillis ( ) ; WorkflowControllerDataProvider cache = event . getAttribute ( AttributeName . ControllerDataProvider . name ( ) ) ; HelixManager manager = event . getAttribute ( AttributeName . helixmanager . name ( ) ) ; cache . getTaskDataCache ( ) . persistDataChanges ( manager . getHelixDataAccessor ( ) ) ; long endTime = System . currentTimeMillis ( ) ; LOG . info ( "END TaskPersistDataStage.process() for cluster {} took {} ms" , cache . getClusterName ( ) , ( endTime - startTime ) ) ; }
public void test() { if ( ProfileManager . getDefault ( ) . migrateToProfiles ( getConfigFileName ( ) ) ) { log . info ( Bundle . getMessage ( "ConfigMigratedToProfile" ) ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( IOException | IllegalArgumentException ex ) { log . error ( "Profiles not configurable. Using fallback per-application configuration. Error: {}" , ex . getMessage ( ) ) ; } }
public void test() { if ( profile != null ) { log . info ( "Starting with profile {}" , profile . getId ( ) ) ; } else { log . info ( "Starting without a profile" ) ; } }
public void test() { if ( profile != null ) { log . info ( "Starting with profile {}" , profile . getId ( ) ) ; } else { log . info ( "Starting without a profile" ) ; } }
public void test() { if ( ProfileManager . getStartingProfile ( ) != null ) { System . setProperty ( "org.jmri.Apps.configFilename" , Profile . CONFIG_FILENAME ) ; Profile profile = ProfileManager . getDefault ( ) . getActiveProfile ( ) ; code_block = IfStatement ; } else { log . error ( "Specify profile to use as command line argument." ) ; log . error ( "If starting with saved profile configuration, ensure the autoStart property is set to \"true\"" ) ; log . error ( "Profiles not configurable. Using fallback per-application configuration." ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( IOException ex ) { log . info ( "Profiles not configurable. Using fallback per-application configuration. Error: {}" , ex . getMessage ( ) ) ; } }
private void log ( String string ) { Log . info ( string ) ; System . out . println ( string ) ; }
public void attachClean ( StgMBstnStatus instance ) { log . debug ( "attaching clean StgMBstnStatus instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . lock ( instance , LockMode . NONE ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { logger . trace ( "Opening port {}" , serialPortName ) ; SerialPort oldSerialPort = serialPortReference . get ( ) ; SerialPort serialPort = portIdentifier . open ( DSMRBindingConstants . DSMR_PORT_NAME , SERIAL_PORT_READ_TIMEOUT_MILLISECONDS ) ; logger . trace ( "Configure serial port parameters: {}" , portSettings ) ; serialPort . setSerialPortParams ( portSettings . getBaudrate ( ) , portSettings . getDataBits ( ) , portSettings . getStopbits ( ) , portSettings . getParity ( ) ) ; logger . trace ( "SerialPort opened successful on {}" , serialPortName ) ; open ( serialPort . getInputStream ( ) ) ; serialPort . addEventListener ( this ) ; serialPort . notifyOnDataAvailable ( true ) ; serialPort . notifyOnBreakInterrupt ( true ) ; serialPort . notifyOnFramingError ( true ) ; serialPort . notifyOnOverrunError ( true ) ; serialPort . notifyOnParityError ( true ) ; code_block = TryStatement ;  code_block = TryStatement ;  serialPort . setRTS ( true ) ; code_block = IfStatement ; } catch ( IOException ioe ) { logger . debug ( "Failed to get inputstream for serialPort" , ioe ) ; errorEvent = DSMRConnectorErrorEvent . READ_ERROR ; } catch ( TooManyListenersException tmle ) { logger . warn ( "Possible bug because a listener was added while one already set." , tmle ) ; errorEvent = DSMRConnectorErrorEvent . INTERNAL_ERROR ; } catch ( PortInUseException piue ) { logger . debug ( "Port already in use: {}" , serialPortName , piue ) ; errorEvent = DSMRConnectorErrorEvent . IN_USE ; } catch ( UnsupportedCommOperationException ucoe ) { logger . debug ( "Port does not support requested port settings (invalid dsmr:portsettings parameter?): {}" , serialPortName , ucoe ) ; errorEvent = DSMRConnectorErrorEvent . NOT_COMPATIBLE ; } }
public void test() { try { logger . trace ( "Opening port {}" , serialPortName ) ; SerialPort oldSerialPort = serialPortReference . get ( ) ; SerialPort serialPort = portIdentifier . open ( DSMRBindingConstants . DSMR_PORT_NAME , SERIAL_PORT_READ_TIMEOUT_MILLISECONDS ) ; logger . trace ( "Configure serial port parameters: {}" , portSettings ) ; serialPort . setSerialPortParams ( portSettings . getBaudrate ( ) , portSettings . getDataBits ( ) , portSettings . getStopbits ( ) , portSettings . getParity ( ) ) ; logger . trace ( "SerialPort opened successful on {}" , serialPortName ) ; open ( serialPort . getInputStream ( ) ) ; serialPort . addEventListener ( this ) ; serialPort . notifyOnDataAvailable ( true ) ; serialPort . notifyOnBreakInterrupt ( true ) ; serialPort . notifyOnFramingError ( true ) ; serialPort . notifyOnOverrunError ( true ) ; serialPort . notifyOnParityError ( true ) ; code_block = TryStatement ;  code_block = TryStatement ;  serialPort . setRTS ( true ) ; code_block = IfStatement ; } catch ( IOException ioe ) { logger . debug ( "Failed to get inputstream for serialPort" , ioe ) ; errorEvent = DSMRConnectorErrorEvent . READ_ERROR ; } catch ( TooManyListenersException tmle ) { logger . warn ( "Possible bug because a listener was added while one already set." , tmle ) ; errorEvent = DSMRConnectorErrorEvent . INTERNAL_ERROR ; } catch ( PortInUseException piue ) { logger . debug ( "Port already in use: {}" , serialPortName , piue ) ; errorEvent = DSMRConnectorErrorEvent . IN_USE ; } catch ( UnsupportedCommOperationException ucoe ) { logger . debug ( "Port does not support requested port settings (invalid dsmr:portsettings parameter?): {}" , serialPortName , ucoe ) ; errorEvent = DSMRConnectorErrorEvent . NOT_COMPATIBLE ; } }
public void test() { try { serialPort . enableReceiveThreshold ( SERIAL_TIMEOUT_MILLISECONDS ) ; } catch ( UnsupportedCommOperationException e ) { logger . debug ( "Enable receive threshold is unsupported" ) ; } }
public void test() { try { serialPort . enableReceiveTimeout ( SERIAL_TIMEOUT_MILLISECONDS ) ; } catch ( UnsupportedCommOperationException e ) { logger . debug ( "Enable receive timeout is unsupported" ) ; } }
public void test() { if ( ! serialPortReference . compareAndSet ( oldSerialPort , serialPort ) ) { logger . warn ( "Possible bug because a new serial port value was set during opening new port." ) ; errorEvent = DSMRConnectorErrorEvent . INTERNAL_ERROR ; } }
public void test() { try { logger . trace ( "Opening port {}" , serialPortName ) ; SerialPort oldSerialPort = serialPortReference . get ( ) ; SerialPort serialPort = portIdentifier . open ( DSMRBindingConstants . DSMR_PORT_NAME , SERIAL_PORT_READ_TIMEOUT_MILLISECONDS ) ; logger . trace ( "Configure serial port parameters: {}" , portSettings ) ; serialPort . setSerialPortParams ( portSettings . getBaudrate ( ) , portSettings . getDataBits ( ) , portSettings . getStopbits ( ) , portSettings . getParity ( ) ) ; logger . trace ( "SerialPort opened successful on {}" , serialPortName ) ; open ( serialPort . getInputStream ( ) ) ; serialPort . addEventListener ( this ) ; serialPort . notifyOnDataAvailable ( true ) ; serialPort . notifyOnBreakInterrupt ( true ) ; serialPort . notifyOnFramingError ( true ) ; serialPort . notifyOnOverrunError ( true ) ; serialPort . notifyOnParityError ( true ) ; code_block = TryStatement ;  code_block = TryStatement ;  serialPort . setRTS ( true ) ; code_block = IfStatement ; } catch ( IOException ioe ) { logger . debug ( "Failed to get inputstream for serialPort" , ioe ) ; errorEvent = DSMRConnectorErrorEvent . READ_ERROR ; } catch ( TooManyListenersException tmle ) { logger . warn ( "Possible bug because a listener was added while one already set." , tmle ) ; errorEvent = DSMRConnectorErrorEvent . INTERNAL_ERROR ; } catch ( PortInUseException piue ) { logger . debug ( "Port already in use: {}" , serialPortName , piue ) ; errorEvent = DSMRConnectorErrorEvent . IN_USE ; } catch ( UnsupportedCommOperationException ucoe ) { logger . debug ( "Port does not support requested port settings (invalid dsmr:portsettings parameter?): {}" , serialPortName , ucoe ) ; errorEvent = DSMRConnectorErrorEvent . NOT_COMPATIBLE ; } }
public void test() { try { logger . trace ( "Opening port {}" , serialPortName ) ; SerialPort oldSerialPort = serialPortReference . get ( ) ; SerialPort serialPort = portIdentifier . open ( DSMRBindingConstants . DSMR_PORT_NAME , SERIAL_PORT_READ_TIMEOUT_MILLISECONDS ) ; logger . trace ( "Configure serial port parameters: {}" , portSettings ) ; serialPort . setSerialPortParams ( portSettings . getBaudrate ( ) , portSettings . getDataBits ( ) , portSettings . getStopbits ( ) , portSettings . getParity ( ) ) ; logger . trace ( "SerialPort opened successful on {}" , serialPortName ) ; open ( serialPort . getInputStream ( ) ) ; serialPort . addEventListener ( this ) ; serialPort . notifyOnDataAvailable ( true ) ; serialPort . notifyOnBreakInterrupt ( true ) ; serialPort . notifyOnFramingError ( true ) ; serialPort . notifyOnOverrunError ( true ) ; serialPort . notifyOnParityError ( true ) ; code_block = TryStatement ;  code_block = TryStatement ;  serialPort . setRTS ( true ) ; code_block = IfStatement ; } catch ( IOException ioe ) { logger . debug ( "Failed to get inputstream for serialPort" , ioe ) ; errorEvent = DSMRConnectorErrorEvent . READ_ERROR ; } catch ( TooManyListenersException tmle ) { logger . warn ( "Possible bug because a listener was added while one already set." , tmle ) ; errorEvent = DSMRConnectorErrorEvent . INTERNAL_ERROR ; } catch ( PortInUseException piue ) { logger . debug ( "Port already in use: {}" , serialPortName , piue ) ; errorEvent = DSMRConnectorErrorEvent . IN_USE ; } catch ( UnsupportedCommOperationException ucoe ) { logger . debug ( "Port does not support requested port settings (invalid dsmr:portsettings parameter?): {}" , serialPortName , ucoe ) ; errorEvent = DSMRConnectorErrorEvent . NOT_COMPATIBLE ; } }
public void test() { try { logger . trace ( "Opening port {}" , serialPortName ) ; SerialPort oldSerialPort = serialPortReference . get ( ) ; SerialPort serialPort = portIdentifier . open ( DSMRBindingConstants . DSMR_PORT_NAME , SERIAL_PORT_READ_TIMEOUT_MILLISECONDS ) ; logger . trace ( "Configure serial port parameters: {}" , portSettings ) ; serialPort . setSerialPortParams ( portSettings . getBaudrate ( ) , portSettings . getDataBits ( ) , portSettings . getStopbits ( ) , portSettings . getParity ( ) ) ; logger . trace ( "SerialPort opened successful on {}" , serialPortName ) ; open ( serialPort . getInputStream ( ) ) ; serialPort . addEventListener ( this ) ; serialPort . notifyOnDataAvailable ( true ) ; serialPort . notifyOnBreakInterrupt ( true ) ; serialPort . notifyOnFramingError ( true ) ; serialPort . notifyOnOverrunError ( true ) ; serialPort . notifyOnParityError ( true ) ; code_block = TryStatement ;  code_block = TryStatement ;  serialPort . setRTS ( true ) ; code_block = IfStatement ; } catch ( IOException ioe ) { logger . debug ( "Failed to get inputstream for serialPort" , ioe ) ; errorEvent = DSMRConnectorErrorEvent . READ_ERROR ; } catch ( TooManyListenersException tmle ) { logger . warn ( "Possible bug because a listener was added while one already set." , tmle ) ; errorEvent = DSMRConnectorErrorEvent . INTERNAL_ERROR ; } catch ( PortInUseException piue ) { logger . debug ( "Port already in use: {}" , serialPortName , piue ) ; errorEvent = DSMRConnectorErrorEvent . IN_USE ; } catch ( UnsupportedCommOperationException ucoe ) { logger . debug ( "Port does not support requested port settings (invalid dsmr:portsettings parameter?): {}" , serialPortName , ucoe ) ; errorEvent = DSMRConnectorErrorEvent . NOT_COMPATIBLE ; } }
public void test() { try { logger . trace ( "Opening port {}" , serialPortName ) ; SerialPort oldSerialPort = serialPortReference . get ( ) ; SerialPort serialPort = portIdentifier . open ( DSMRBindingConstants . DSMR_PORT_NAME , SERIAL_PORT_READ_TIMEOUT_MILLISECONDS ) ; logger . trace ( "Configure serial port parameters: {}" , portSettings ) ; serialPort . setSerialPortParams ( portSettings . getBaudrate ( ) , portSettings . getDataBits ( ) , portSettings . getStopbits ( ) , portSettings . getParity ( ) ) ; logger . trace ( "SerialPort opened successful on {}" , serialPortName ) ; open ( serialPort . getInputStream ( ) ) ; serialPort . addEventListener ( this ) ; serialPort . notifyOnDataAvailable ( true ) ; serialPort . notifyOnBreakInterrupt ( true ) ; serialPort . notifyOnFramingError ( true ) ; serialPort . notifyOnOverrunError ( true ) ; serialPort . notifyOnParityError ( true ) ; code_block = TryStatement ;  code_block = TryStatement ;  serialPort . setRTS ( true ) ; code_block = IfStatement ; } catch ( IOException ioe ) { logger . debug ( "Failed to get inputstream for serialPort" , ioe ) ; errorEvent = DSMRConnectorErrorEvent . READ_ERROR ; } catch ( TooManyListenersException tmle ) { logger . warn ( "Possible bug because a listener was added while one already set." , tmle ) ; errorEvent = DSMRConnectorErrorEvent . INTERNAL_ERROR ; } catch ( PortInUseException piue ) { logger . debug ( "Port already in use: {}" , serialPortName , piue ) ; errorEvent = DSMRConnectorErrorEvent . IN_USE ; } catch ( UnsupportedCommOperationException ucoe ) { logger . debug ( "Port does not support requested port settings (invalid dsmr:portsettings parameter?): {}" , serialPortName , ucoe ) ; errorEvent = DSMRConnectorErrorEvent . NOT_COMPATIBLE ; } }
public void test() { if ( elapsedNS >= threshold ) { logger . warn ( "Slow request: {} {} ({}ms)" , req . getMethod ( ) , getFullUrl ( req ) , elapsedMS ) ; } }
public void test() { try { logger . info ( "Will parse {}" , serializedData ) ; template = mapper . readValue ( serializedData , Template . class ) ; logger . debug ( "Template parsed {}" , template ) ; } catch ( JsonProcessingException e ) { throw new ParserException ( e ) ; } catch ( Throwable e ) { throw new ParserException ( e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( SourceServiceUtil . class , "addSource" , _addSourceParameterTypes0 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , nameMap , driverClassName , driverUrl , driverUserName , driverPassword , serviceContext ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . portal . reports . engine . console . model . Source ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( replyTo != null ) { LOG . debug ( "Using JMSReplyTo destination: {}" , replyTo ) ; JmsMessageHelper . setJMSReplyTo ( answer , replyTo ) ; } else { LOG . trace ( "Not using JMSReplyTo" ) ; JmsMessageHelper . setJMSReplyTo ( answer , null ) ; } }
public void test() { if ( replyTo != null ) { LOG . debug ( "Using JMSReplyTo destination: {}" , replyTo ) ; JmsMessageHelper . setJMSReplyTo ( answer , replyTo ) ; } else { LOG . trace ( "Not using JMSReplyTo" ) ; JmsMessageHelper . setJMSReplyTo ( answer , null ) ; } }
public void test() { try { code_block = IfStatement ; SpatialFilter spatialFilter = new SpatialFilter ( geometryWkt ) ; code_block = IfStatement ; } catch ( IllegalArgumentException e ) { LOGGER . debug ( "Invalid spatial query type specified.  Will not apply spatial filter." ) ; return ; } }
public void test() { try { return _assetEntryService . getEntries ( assetEntryQuery ) ; } catch ( Exception exception ) { _log . error ( "Unable to get asset entries" , exception ) ; } }
public void test() { try { return MCRConfiguration2 . < MCRDefaultLogicalStructMapTypeProvider > getClass ( "MCR.Component.MetsMods.LogicalStructMapTypeProvider" ) . orElse ( MCRDefaultLogicalStructMapTypeProvider . class ) . getDeclaredConstructor ( ) . newInstance ( ) ; } catch ( Exception e ) { LOGGER . warn ( "Could not load class" , e ) ; return new MCRDefaultLogicalStructMapTypeProvider ( ) ; } }
@ Override public OutputStream append ( String path ) throws IOException { log . debug ( "append [{}]" , path ) ; return updateFile ( path , true ) ; }
@ Override public void sendMessage ( Message message , Role role ) throws MessageException { log . debug ( "Sending message to role " + role ) ; log . debug ( "User Service : " + Context . getUserService ( ) ) ; List < Role > roles = new ArrayList < > ( ) ; roles . add ( role ) ; Collection < User > users = Context . getUserService ( ) . getUsers ( null , roles , false ) ; log . debug ( "Sending message " + message + " to " + users ) ; Context . getMessageService ( ) . sendMessage ( message , users ) ; }
@ Override public void sendMessage ( Message message , Role role ) throws MessageException { log . debug ( "Sending message to role " + role ) ; log . debug ( "User Service : " + Context . getUserService ( ) ) ; List < Role > roles = new ArrayList < > ( ) ; roles . add ( role ) ; Collection < User > users = Context . getUserService ( ) . getUsers ( null , roles , false ) ; log . debug ( "Sending message " + message + " to " + users ) ; Context . getMessageService ( ) . sendMessage ( message , users ) ; }
@ Override public void sendMessage ( Message message , Role role ) throws MessageException { log . debug ( "Sending message to role " + role ) ; log . debug ( "User Service : " + Context . getUserService ( ) ) ; List < Role > roles = new ArrayList < > ( ) ; roles . add ( role ) ; Collection < User > users = Context . getUserService ( ) . getUsers ( null , roles , false ) ; log . debug ( "Sending message " + message + " to " + users ) ; Context . getMessageService ( ) . sendMessage ( message , users ) ; }
public void test() { { log . info ( "=======================Testing the order: ExP, EP, ER, ES-1, ES-2======================= " ) ; eventStreamCount = eventStreamManagerAdminServiceClient . getEventStreamCount ( ) ; eventReceiverCount = eventReceiverAdminServiceClient . getActiveEventReceiverCount ( ) ; eventPublisherCount = eventPublisherAdminServiceClient . getActiveEventPublisherCount ( ) ; executionPlanCount = eventProcessorAdminServiceClient . getActiveExecutionPlanConfigurationCount ( ) ; log . info ( "=======================Adding an execution plan ======================= " ) ; String executionPlan = getExecutionPlanFromFile ( "DeployArtifactsTestCase" , "testPlan.siddhiql" ) ; eventProcessorAdminServiceClient . addExecutionPlan ( executionPlan ) ; Assert . assertEquals ( eventProcessorAdminServiceClient . getActiveExecutionPlanConfigurationCount ( ) , executionPlanCount ) ; log . info ( "=======================Adding an event receiver ======================= " ) ; String eventReceiverConfig = getXMLArtifactConfiguration ( "DeployArtifactsTestCase" , "PizzaOrder.xml" ) ; eventReceiverAdminServiceClient . addEventReceiverConfiguration ( eventReceiverConfig ) ; Assert . assertEquals ( eventReceiverAdminServiceClient . getActiveEventReceiverCount ( ) , eventReceiverCount ) ; log . info ( "=======================Adding an event publisher ======================= " ) ; String eventPublisherConfig = getXMLArtifactConfiguration ( "DeployArtifactsTestCase" , "PizzaDeliveryNotification.xml" ) ; eventPublisherAdminServiceClient . addEventPublisherConfiguration ( eventPublisherConfig ) ; Assert . assertEquals ( eventPublisherAdminServiceClient . getActiveEventPublisherCount ( ) , eventPublisherCount ) ; log . info ( "=======================Adding a stream definition====================" ) ; String pizzaStreamDefinition = getJSONArtifactConfiguration ( "DeployArtifactsTestCase" , "org.wso2.sample.pizza.order_1.0.0.json" ) ; eventStreamManagerAdminServiceClient . addEventStreamAsString ( pizzaStreamDefinition ) ; Assert . assertEquals ( eventStreamManagerAdminServiceClient . getEventStreamCount ( ) , ++ eventStreamCount ) ; log . info ( "=======================Adding another stream definition====================" ) ; String outStreamDefinition = getJSONArtifactConfiguration ( "DeployArtifactsTestCase" , "outStream_1.0.0.json" ) ; eventStreamManagerAdminServiceClient . addEventStreamAsString ( outStreamDefinition ) ; Assert . assertEquals ( eventStreamManagerAdminServiceClient . getEventStreamCount ( ) , ++ eventStreamCount ) ; Thread . sleep ( 1000 ) ; Assert . assertEquals ( eventProcessorAdminServiceClient . getActiveExecutionPlanConfigurationCount ( ) , ++ executionPlanCount ) ; Assert . assertEquals ( eventReceiverAdminServiceClient . getActiveEventReceiverCount ( ) , ++ eventReceiverCount ) ; Assert . assertEquals ( eventPublisherAdminServiceClient . getActiveEventPublisherCount ( ) , ++ eventPublisherCount ) ; } }
public void test() { { log . info ( "=======================Testing the order: ExP, EP, ER, ES-1, ES-2======================= " ) ; eventStreamCount = eventStreamManagerAdminServiceClient . getEventStreamCount ( ) ; eventReceiverCount = eventReceiverAdminServiceClient . getActiveEventReceiverCount ( ) ; eventPublisherCount = eventPublisherAdminServiceClient . getActiveEventPublisherCount ( ) ; executionPlanCount = eventProcessorAdminServiceClient . getActiveExecutionPlanConfigurationCount ( ) ; log . info ( "=======================Adding an execution plan ======================= " ) ; String executionPlan = getExecutionPlanFromFile ( "DeployArtifactsTestCase" , "testPlan.siddhiql" ) ; eventProcessorAdminServiceClient . addExecutionPlan ( executionPlan ) ; Assert . assertEquals ( eventProcessorAdminServiceClient . getActiveExecutionPlanConfigurationCount ( ) , executionPlanCount ) ; log . info ( "=======================Adding an event receiver ======================= " ) ; String eventReceiverConfig = getXMLArtifactConfiguration ( "DeployArtifactsTestCase" , "PizzaOrder.xml" ) ; eventReceiverAdminServiceClient . addEventReceiverConfiguration ( eventReceiverConfig ) ; Assert . assertEquals ( eventReceiverAdminServiceClient . getActiveEventReceiverCount ( ) , eventReceiverCount ) ; log . info ( "=======================Adding an event publisher ======================= " ) ; String eventPublisherConfig = getXMLArtifactConfiguration ( "DeployArtifactsTestCase" , "PizzaDeliveryNotification.xml" ) ; eventPublisherAdminServiceClient . addEventPublisherConfiguration ( eventPublisherConfig ) ; Assert . assertEquals ( eventPublisherAdminServiceClient . getActiveEventPublisherCount ( ) , eventPublisherCount ) ; log . info ( "=======================Adding a stream definition====================" ) ; String pizzaStreamDefinition = getJSONArtifactConfiguration ( "DeployArtifactsTestCase" , "org.wso2.sample.pizza.order_1.0.0.json" ) ; eventStreamManagerAdminServiceClient . addEventStreamAsString ( pizzaStreamDefinition ) ; Assert . assertEquals ( eventStreamManagerAdminServiceClient . getEventStreamCount ( ) , ++ eventStreamCount ) ; log . info ( "=======================Adding another stream definition====================" ) ; String outStreamDefinition = getJSONArtifactConfiguration ( "DeployArtifactsTestCase" , "outStream_1.0.0.json" ) ; eventStreamManagerAdminServiceClient . addEventStreamAsString ( outStreamDefinition ) ; Assert . assertEquals ( eventStreamManagerAdminServiceClient . getEventStreamCount ( ) , ++ eventStreamCount ) ; Thread . sleep ( 1000 ) ; Assert . assertEquals ( eventProcessorAdminServiceClient . getActiveExecutionPlanConfigurationCount ( ) , ++ executionPlanCount ) ; Assert . assertEquals ( eventReceiverAdminServiceClient . getActiveEventReceiverCount ( ) , ++ eventReceiverCount ) ; Assert . assertEquals ( eventPublisherAdminServiceClient . getActiveEventPublisherCount ( ) , ++ eventPublisherCount ) ; } }
public void test() { { log . info ( "=======================Testing the order: ExP, EP, ER, ES-1, ES-2======================= " ) ; eventStreamCount = eventStreamManagerAdminServiceClient . getEventStreamCount ( ) ; eventReceiverCount = eventReceiverAdminServiceClient . getActiveEventReceiverCount ( ) ; eventPublisherCount = eventPublisherAdminServiceClient . getActiveEventPublisherCount ( ) ; executionPlanCount = eventProcessorAdminServiceClient . getActiveExecutionPlanConfigurationCount ( ) ; log . info ( "=======================Adding an execution plan ======================= " ) ; String executionPlan = getExecutionPlanFromFile ( "DeployArtifactsTestCase" , "testPlan.siddhiql" ) ; eventProcessorAdminServiceClient . addExecutionPlan ( executionPlan ) ; Assert . assertEquals ( eventProcessorAdminServiceClient . getActiveExecutionPlanConfigurationCount ( ) , executionPlanCount ) ; log . info ( "=======================Adding an event receiver ======================= " ) ; String eventReceiverConfig = getXMLArtifactConfiguration ( "DeployArtifactsTestCase" , "PizzaOrder.xml" ) ; eventReceiverAdminServiceClient . addEventReceiverConfiguration ( eventReceiverConfig ) ; Assert . assertEquals ( eventReceiverAdminServiceClient . getActiveEventReceiverCount ( ) , eventReceiverCount ) ; log . info ( "=======================Adding an event publisher ======================= " ) ; String eventPublisherConfig = getXMLArtifactConfiguration ( "DeployArtifactsTestCase" , "PizzaDeliveryNotification.xml" ) ; eventPublisherAdminServiceClient . addEventPublisherConfiguration ( eventPublisherConfig ) ; Assert . assertEquals ( eventPublisherAdminServiceClient . getActiveEventPublisherCount ( ) , eventPublisherCount ) ; log . info ( "=======================Adding a stream definition====================" ) ; String pizzaStreamDefinition = getJSONArtifactConfiguration ( "DeployArtifactsTestCase" , "org.wso2.sample.pizza.order_1.0.0.json" ) ; eventStreamManagerAdminServiceClient . addEventStreamAsString ( pizzaStreamDefinition ) ; Assert . assertEquals ( eventStreamManagerAdminServiceClient . getEventStreamCount ( ) , ++ eventStreamCount ) ; log . info ( "=======================Adding another stream definition====================" ) ; String outStreamDefinition = getJSONArtifactConfiguration ( "DeployArtifactsTestCase" , "outStream_1.0.0.json" ) ; eventStreamManagerAdminServiceClient . addEventStreamAsString ( outStreamDefinition ) ; Assert . assertEquals ( eventStreamManagerAdminServiceClient . getEventStreamCount ( ) , ++ eventStreamCount ) ; Thread . sleep ( 1000 ) ; Assert . assertEquals ( eventProcessorAdminServiceClient . getActiveExecutionPlanConfigurationCount ( ) , ++ executionPlanCount ) ; Assert . assertEquals ( eventReceiverAdminServiceClient . getActiveEventReceiverCount ( ) , ++ eventReceiverCount ) ; Assert . assertEquals ( eventPublisherAdminServiceClient . getActiveEventPublisherCount ( ) , ++ eventPublisherCount ) ; } }
public void test() { { log . info ( "=======================Testing the order: ExP, EP, ER, ES-1, ES-2======================= " ) ; eventStreamCount = eventStreamManagerAdminServiceClient . getEventStreamCount ( ) ; eventReceiverCount = eventReceiverAdminServiceClient . getActiveEventReceiverCount ( ) ; eventPublisherCount = eventPublisherAdminServiceClient . getActiveEventPublisherCount ( ) ; executionPlanCount = eventProcessorAdminServiceClient . getActiveExecutionPlanConfigurationCount ( ) ; log . info ( "=======================Adding an execution plan ======================= " ) ; String executionPlan = getExecutionPlanFromFile ( "DeployArtifactsTestCase" , "testPlan.siddhiql" ) ; eventProcessorAdminServiceClient . addExecutionPlan ( executionPlan ) ; Assert . assertEquals ( eventProcessorAdminServiceClient . getActiveExecutionPlanConfigurationCount ( ) , executionPlanCount ) ; log . info ( "=======================Adding an event receiver ======================= " ) ; String eventReceiverConfig = getXMLArtifactConfiguration ( "DeployArtifactsTestCase" , "PizzaOrder.xml" ) ; eventReceiverAdminServiceClient . addEventReceiverConfiguration ( eventReceiverConfig ) ; Assert . assertEquals ( eventReceiverAdminServiceClient . getActiveEventReceiverCount ( ) , eventReceiverCount ) ; log . info ( "=======================Adding an event publisher ======================= " ) ; String eventPublisherConfig = getXMLArtifactConfiguration ( "DeployArtifactsTestCase" , "PizzaDeliveryNotification.xml" ) ; eventPublisherAdminServiceClient . addEventPublisherConfiguration ( eventPublisherConfig ) ; Assert . assertEquals ( eventPublisherAdminServiceClient . getActiveEventPublisherCount ( ) , eventPublisherCount ) ; log . info ( "=======================Adding a stream definition====================" ) ; String pizzaStreamDefinition = getJSONArtifactConfiguration ( "DeployArtifactsTestCase" , "org.wso2.sample.pizza.order_1.0.0.json" ) ; eventStreamManagerAdminServiceClient . addEventStreamAsString ( pizzaStreamDefinition ) ; Assert . assertEquals ( eventStreamManagerAdminServiceClient . getEventStreamCount ( ) , ++ eventStreamCount ) ; log . info ( "=======================Adding another stream definition====================" ) ; String outStreamDefinition = getJSONArtifactConfiguration ( "DeployArtifactsTestCase" , "outStream_1.0.0.json" ) ; eventStreamManagerAdminServiceClient . addEventStreamAsString ( outStreamDefinition ) ; Assert . assertEquals ( eventStreamManagerAdminServiceClient . getEventStreamCount ( ) , ++ eventStreamCount ) ; Thread . sleep ( 1000 ) ; Assert . assertEquals ( eventProcessorAdminServiceClient . getActiveExecutionPlanConfigurationCount ( ) , ++ executionPlanCount ) ; Assert . assertEquals ( eventReceiverAdminServiceClient . getActiveEventReceiverCount ( ) , ++ eventReceiverCount ) ; Assert . assertEquals ( eventPublisherAdminServiceClient . getActiveEventPublisherCount ( ) , ++ eventPublisherCount ) ; } }
public void test() { { log . info ( "=======================Testing the order: ExP, EP, ER, ES-1, ES-2======================= " ) ; eventStreamCount = eventStreamManagerAdminServiceClient . getEventStreamCount ( ) ; eventReceiverCount = eventReceiverAdminServiceClient . getActiveEventReceiverCount ( ) ; eventPublisherCount = eventPublisherAdminServiceClient . getActiveEventPublisherCount ( ) ; executionPlanCount = eventProcessorAdminServiceClient . getActiveExecutionPlanConfigurationCount ( ) ; log . info ( "=======================Adding an execution plan ======================= " ) ; String executionPlan = getExecutionPlanFromFile ( "DeployArtifactsTestCase" , "testPlan.siddhiql" ) ; eventProcessorAdminServiceClient . addExecutionPlan ( executionPlan ) ; Assert . assertEquals ( eventProcessorAdminServiceClient . getActiveExecutionPlanConfigurationCount ( ) , executionPlanCount ) ; log . info ( "=======================Adding an event receiver ======================= " ) ; String eventReceiverConfig = getXMLArtifactConfiguration ( "DeployArtifactsTestCase" , "PizzaOrder.xml" ) ; eventReceiverAdminServiceClient . addEventReceiverConfiguration ( eventReceiverConfig ) ; Assert . assertEquals ( eventReceiverAdminServiceClient . getActiveEventReceiverCount ( ) , eventReceiverCount ) ; log . info ( "=======================Adding an event publisher ======================= " ) ; String eventPublisherConfig = getXMLArtifactConfiguration ( "DeployArtifactsTestCase" , "PizzaDeliveryNotification.xml" ) ; eventPublisherAdminServiceClient . addEventPublisherConfiguration ( eventPublisherConfig ) ; Assert . assertEquals ( eventPublisherAdminServiceClient . getActiveEventPublisherCount ( ) , eventPublisherCount ) ; log . info ( "=======================Adding a stream definition====================" ) ; String pizzaStreamDefinition = getJSONArtifactConfiguration ( "DeployArtifactsTestCase" , "org.wso2.sample.pizza.order_1.0.0.json" ) ; eventStreamManagerAdminServiceClient . addEventStreamAsString ( pizzaStreamDefinition ) ; Assert . assertEquals ( eventStreamManagerAdminServiceClient . getEventStreamCount ( ) , ++ eventStreamCount ) ; log . info ( "=======================Adding another stream definition====================" ) ; String outStreamDefinition = getJSONArtifactConfiguration ( "DeployArtifactsTestCase" , "outStream_1.0.0.json" ) ; eventStreamManagerAdminServiceClient . addEventStreamAsString ( outStreamDefinition ) ; Assert . assertEquals ( eventStreamManagerAdminServiceClient . getEventStreamCount ( ) , ++ eventStreamCount ) ; Thread . sleep ( 1000 ) ; Assert . assertEquals ( eventProcessorAdminServiceClient . getActiveExecutionPlanConfigurationCount ( ) , ++ executionPlanCount ) ; Assert . assertEquals ( eventReceiverAdminServiceClient . getActiveEventReceiverCount ( ) , ++ eventReceiverCount ) ; Assert . assertEquals ( eventPublisherAdminServiceClient . getActiveEventPublisherCount ( ) , ++ eventPublisherCount ) ; } }
public void test() { { log . info ( "=======================Testing the order: ExP, EP, ER, ES-1, ES-2======================= " ) ; eventStreamCount = eventStreamManagerAdminServiceClient . getEventStreamCount ( ) ; eventReceiverCount = eventReceiverAdminServiceClient . getActiveEventReceiverCount ( ) ; eventPublisherCount = eventPublisherAdminServiceClient . getActiveEventPublisherCount ( ) ; executionPlanCount = eventProcessorAdminServiceClient . getActiveExecutionPlanConfigurationCount ( ) ; log . info ( "=======================Adding an execution plan ======================= " ) ; String executionPlan = getExecutionPlanFromFile ( "DeployArtifactsTestCase" , "testPlan.siddhiql" ) ; eventProcessorAdminServiceClient . addExecutionPlan ( executionPlan ) ; Assert . assertEquals ( eventProcessorAdminServiceClient . getActiveExecutionPlanConfigurationCount ( ) , executionPlanCount ) ; log . info ( "=======================Adding an event receiver ======================= " ) ; String eventReceiverConfig = getXMLArtifactConfiguration ( "DeployArtifactsTestCase" , "PizzaOrder.xml" ) ; eventReceiverAdminServiceClient . addEventReceiverConfiguration ( eventReceiverConfig ) ; Assert . assertEquals ( eventReceiverAdminServiceClient . getActiveEventReceiverCount ( ) , eventReceiverCount ) ; log . info ( "=======================Adding an event publisher ======================= " ) ; String eventPublisherConfig = getXMLArtifactConfiguration ( "DeployArtifactsTestCase" , "PizzaDeliveryNotification.xml" ) ; eventPublisherAdminServiceClient . addEventPublisherConfiguration ( eventPublisherConfig ) ; Assert . assertEquals ( eventPublisherAdminServiceClient . getActiveEventPublisherCount ( ) , eventPublisherCount ) ; log . info ( "=======================Adding a stream definition====================" ) ; String pizzaStreamDefinition = getJSONArtifactConfiguration ( "DeployArtifactsTestCase" , "org.wso2.sample.pizza.order_1.0.0.json" ) ; eventStreamManagerAdminServiceClient . addEventStreamAsString ( pizzaStreamDefinition ) ; Assert . assertEquals ( eventStreamManagerAdminServiceClient . getEventStreamCount ( ) , ++ eventStreamCount ) ; log . info ( "=======================Adding another stream definition====================" ) ; String outStreamDefinition = getJSONArtifactConfiguration ( "DeployArtifactsTestCase" , "outStream_1.0.0.json" ) ; eventStreamManagerAdminServiceClient . addEventStreamAsString ( outStreamDefinition ) ; Assert . assertEquals ( eventStreamManagerAdminServiceClient . getEventStreamCount ( ) , ++ eventStreamCount ) ; Thread . sleep ( 1000 ) ; Assert . assertEquals ( eventProcessorAdminServiceClient . getActiveExecutionPlanConfigurationCount ( ) , ++ executionPlanCount ) ; Assert . assertEquals ( eventReceiverAdminServiceClient . getActiveEventReceiverCount ( ) , ++ eventReceiverCount ) ; Assert . assertEquals ( eventPublisherAdminServiceClient . getActiveEventPublisherCount ( ) , ++ eventPublisherCount ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( IOException e ) { log . error ( "Can't watch env.properties file for changes" , e ) ; } }
public void test() { try { future . get ( ) ; break ; } catch ( final ExecutionException e ) { final Throwable cause = e . getCause ( ) ; logger . error ( "Failed when attempting to disconnect node from cluster" , cause ) ; } }
public void test() { if ( handler != null ) { return handler . actionGetRingTimeLimit ( ) ; } else { logger . info ( "Doorbird Action service ThingHandler is null!" ) ; return "" ; } }
public void test() { if ( size >= 10 ) { log . info ( "Re-indexing of " + size + " objects done after updating " + obj . getClass ( ) . getName ( ) + ":" + obj . getId ( ) ) ; } }
public void test() { try { printAST ( getHiveTokenMapping ( ) , node , 0 , 0 ) ; } catch ( Exception e ) { log . error ( "Error in printing AST." , e ) ; } }
@ BeforeMethod ( alwaysRun = true ) public void setUp ( ) throws Exception { startTime = TimeUtil . getTimeWrtSystemTime ( 0 ) ; endTime = TimeUtil . addMinsToTime ( startTime , 20 ) ; LOGGER . info ( "Time range between : " + startTime + " and " + endTime ) ; bundles [ 0 ] = BundleUtil . readELBundle ( ) ; bundles [ 0 ] = new Bundle ( bundles [ 0 ] , cluster ) ; bundles [ 0 ] . generateUniqueBundle ( this ) ; bundles [ 0 ] . setProcessWorkflow ( aggregateWorkflowDir ) ; bundles [ 0 ] . setProcessValidity ( startTime , endTime ) ; bundles [ 0 ] . setProcessPeriodicity ( 5 , Frequency . TimeUnit . minutes ) ; bundles [ 0 ] . setOutputFeedPeriodicity ( 5 , Frequency . TimeUnit . minutes ) ; clusterName = Util . readEntityName ( bundles [ 0 ] . getDataSets ( ) . get ( 0 ) ) ; }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "ElasticsearchPlugin config:{}" , elasticsearchPluginConfig ) ; } }
public void test() { if ( LOGGER . isErrorEnabled ( ) ) { LOGGER . error ( "Number of IO operations cannot be negative for dataset: " + this ) ; } }
public void test() { try { long lastActivityTime = getEntryValue ( data , LAST_ACTIVITY_TIME , 0L ) ; long inactivityAlarmTime = getEntryValue ( data , INACTIVITY_ALARM_TIME , 0L ) ; long inactivityTimeout = getEntryValue ( data , INACTIVITY_TIMEOUT , TimeUnit . SECONDS . toMillis ( defaultInactivityTimeoutInSec ) ) ; boolean active = System . currentTimeMillis ( ) < lastActivityTime + inactivityTimeout ; DeviceState deviceState = DeviceState . builder ( ) . active ( active ) . lastConnectTime ( getEntryValue ( data , LAST_CONNECT_TIME , 0L ) ) . lastDisconnectTime ( getEntryValue ( data , LAST_DISCONNECT_TIME , 0L ) ) . lastActivityTime ( lastActivityTime ) . lastInactivityAlarmTime ( inactivityAlarmTime ) . inactivityTimeout ( inactivityTimeout ) . build ( ) ; TbMsgMetaData md = new TbMsgMetaData ( ) ; md . putValue ( "deviceName" , device . getName ( ) ) ; md . putValue ( "deviceType" , device . getType ( ) ) ; return DeviceStateData . builder ( ) . customerId ( device . getCustomerId ( ) ) . tenantId ( device . getTenantId ( ) ) . deviceId ( device . getId ( ) ) . deviceCreationTime ( device . getCreatedTime ( ) ) . metaData ( md ) . state ( deviceState ) . build ( ) ; } catch ( Exception e ) { log . warn ( "[{}] Failed to fetch device state data" , device . getId ( ) , e ) ; throw new RuntimeException ( e ) ; } }
public void test() { if ( logDetails ) { logger . info ( "creating schema '{}'" , schemaName ) ; } }
public void test() { if ( LOGGER . isInfoEnabled ( ) ) { LOGGER . info ( "RTree " + getTestOpName ( ) + " Test With Two Dimensions With Integer Keys." ) ; } }
public void test() { try { zooKeeper . close ( ) ; } catch ( InterruptedException e ) { LOGGER . warn ( "could not close ZooKeeper client due to interrupt" , e ) ; Thread . currentThread ( ) . interrupt ( ) ; } }
public void test() { if ( ! pluginDir . exists ( ) && ! pluginDir . isDirectory ( ) ) { LOGGER . info ( "UDFs can't be loaded as as dir {} doesn't exist or is not a directory" , pluginDir ) ; return ; } }
public void test() { try { code_block = IfStatement ; Files . find ( pluginDir . toPath ( ) , 1 , ( path , attributes ) -> path . toString ( ) . endsWith ( ".jar" ) ) . map ( path -> UdfClassLoader . newClassLoader ( path , parentClassLoader , blacklist ) ) . forEach ( classLoader -> loadFunctions ( classLoader , Optional . of ( classLoader . getJarPath ( ) ) ) ) ; } catch ( final IOException e ) { LOGGER . error ( "Failed to load UDFs from location {}" , pluginDir , e ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( getClass ( ) + " " + infoLog + ", payload:\n" + soaException . getMessage ( ) ) ; } }
public void test() { try { messageProcessor . writeHeader ( transactionContext ) ; messageProcessor . writeMessageEnd ( ) ; transport . flush ( ) ; MdcCtxInfoUtil . putMdcToAppClassLoader ( application . getAppClasssLoader ( ) , SoaSystemEnvProperties . KEY_LOGGER_SESSION_TID , transactionContext . sessionTid ( ) . map ( DapengUtil :: longToHexStr ) . orElse ( "0" ) ) ; String infoLog = "response[seqId:" + transactionContext . seqId ( ) + ", respCode:" + soaHeader . getRespCode ( ) . get ( ) + "]:" + "service[" + soaHeader . getServiceName ( ) + "]:version[" + soaHeader . getVersionName ( ) + "]:method[" + soaHeader . getMethodName ( ) + "]" + ( soaHeader . getOperatorId ( ) . isPresent ( ) ? " operatorId:" + soaHeader . getOperatorId ( ) . get ( ) : "" ) + ( soaHeader . getUserId ( ) . isPresent ( ) ? " userId:" + soaHeader . getUserId ( ) . get ( ) : "" ) ; code_block = IfStatement ; code_block = IfStatement ; } catch ( Throwable e ) { LOGGER . error ( e . getMessage ( ) , e ) ; } finally { container . requestCounter ( ) . decrementAndGet ( ) ; MdcCtxInfoUtil . removeMdcToAppClassLoader ( application . getAppClasssLoader ( ) , SoaSystemEnvProperties . KEY_LOGGER_SESSION_TID ) ; MDC . remove ( SoaSystemEnvProperties . KEY_LOGGER_SESSION_TID ) ; } }
private synchronized CswSubscription deleteCswSubscription ( String subscriptionId ) throws CswException { String methodName = "deleteCswSubscription" ; LogSanitizer logSanitizedId = LogSanitizer . sanitize ( subscriptionId ) ; LOGGER . trace ( ENTERING_STR , methodName ) ; LOGGER . trace ( "subscriptionId = {}" , logSanitizedId ) ; code_block = IfStatement ; CswSubscription subscription = getSubscription ( subscriptionId ) ; code_block = TryStatement ;  LOGGER . trace ( "EXITING: {}    (status = {})" , methodName , false ) ; return subscription ; }
public void test() { try { LOGGER . debug ( "Removing (unregistering) subscription: {}" , logSanitizedId ) ; ServiceRegistration sr = registeredSubscriptions . remove ( subscriptionId ) ; code_block = IfStatement ; Configuration subscriptionConfig = getSubscriptionConfiguration ( subscriptionId ) ; code_block = TryStatement ;  LOGGER . debug ( "Subscription removal complete" ) ; } catch ( Exception e ) { LOGGER . debug ( "Could not delete subscription for {}" , logSanitizedId , e ) ; } }
public void test() { if ( sr != null ) { sr . unregister ( ) ; } else { LOGGER . debug ( "No ServiceRegistration found for subscription: {}" , logSanitizedId ) ; } }
public void test() { if ( subscriptionConfig != null ) { LOGGER . debug ( "Deleting subscription for subscriptionId = {}" , logSanitizedId ) ; subscriptionConfig . delete ( ) ; } else { LOGGER . debug ( "subscriptionConfig is NULL for ID = {}" , logSanitizedId ) ; } }
public void test() { if ( sr != null ) { sr . unregister ( ) ; } else { LOGGER . debug ( "No ServiceRegistration found for subscription: {}" , logSanitizedId ) ; } }
public void test() { if ( subscriptionConfig != null ) { LOGGER . debug ( "Deleting subscription for subscriptionId = {}" , logSanitizedId ) ; subscriptionConfig . delete ( ) ; } else { LOGGER . debug ( "subscriptionConfig is NULL for ID = {}" , logSanitizedId ) ; } }
public void test() { if ( sr != null ) { sr . unregister ( ) ; } else { LOGGER . debug ( "No ServiceRegistration found for subscription: {}" , logSanitizedId ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( IOException e ) { LOGGER . debug ( "IOException trying to delete subscription's configuration for subscription ID {}" , subscriptionId , e ) ; } }
public void test() { try { LOGGER . debug ( "Removing (unregistering) subscription: {}" , logSanitizedId ) ; ServiceRegistration sr = registeredSubscriptions . remove ( subscriptionId ) ; code_block = IfStatement ; Configuration subscriptionConfig = getSubscriptionConfiguration ( subscriptionId ) ; code_block = TryStatement ;  LOGGER . debug ( "Subscription removal complete" ) ; } catch ( Exception e ) { LOGGER . debug ( "Could not delete subscription for {}" , logSanitizedId , e ) ; } }
private synchronized CswSubscription deleteCswSubscription ( String subscriptionId ) throws CswException { String methodName = "deleteCswSubscription" ; LogSanitizer logSanitizedId = LogSanitizer . sanitize ( subscriptionId ) ; LOGGER . trace ( ENTERING_STR , methodName ) ; LOGGER . trace ( "subscriptionId = {}" , logSanitizedId ) ; code_block = IfStatement ; CswSubscription subscription = getSubscription ( subscriptionId ) ; code_block = TryStatement ;  LOGGER . trace ( "EXITING: {}    (status = {})" , methodName , false ) ; return subscription ; }
public void routine ( String schemaName , String routineName , String language , Routine . CallingConvention callingConvention ) { LOG . trace ( "routine: {}.{} " , schemaName , routineName ) ; Routine routine = Routine . create ( ais , schemaName , routineName , language , callingConvention ) ; }
public void test() { try { Class hbaseCleanUpUtil = Class . forName ( "org.apache.kylin.rest.job.StorageCleanJobHbaseUtil" ) ; Method cleanUnusedHBaseTables = hbaseCleanUpUtil . getDeclaredMethod ( "cleanUnusedHBaseTables" , boolean . class , int . class , int . class ) ; hbaseGarbageTables = ( List < String > ) cleanUnusedHBaseTables . invoke ( hbaseCleanUpUtil , delete , deleteTimeoutMin , threadsNum ) ; } catch ( Throwable e ) { logger . error ( "Error during HBase clean up" , e ) ; } }
public void test() { try { log . info ( "Skipping " + fixtureId + " tests. Fixture file " + fixtureFile . getCanonicalPath ( ) + " not found." ) ; } catch ( Exception e ) { log . debug ( e ) ; } }
public void test() { try { log . info ( "Skipping " + fixtureId + " tests. Fixture file " + fixtureFile . getCanonicalPath ( ) + " not found." ) ; } catch ( Exception e ) { log . debug ( e ) ; } }
public void test() { try { DriverManager . deregisterDriver ( driver ) ; } catch ( SQLException e ) { log . warn ( "SQL driver deregistration failed" , e ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Set field " + field . getName ( ) + " to null in class " + clazz . getName ( ) ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Could not set field " + field . getName ( ) + " to null in class " + clazz . getName ( ) , t ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Could not clean fields for class " + clazz . getName ( ) , t ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Could not set field " + field . getName ( ) + " to null in class " + clazz . getName ( ) , t ) ; } }
@ RequestMapping ( value = "/all" , method = RequestMethod . GET ) public List < ModuleTypeDto > getModuleTypes ( ) { log . debug ( "getModuleTypes()" ) ; List < ModuleType > moduleTypes = moduleTypeService . findModuleTypes ( ) ; List < ModuleTypeDto > moduleTypeDtos = moduleTypeToModuleTypeDtoConverter . convertToList ( moduleTypes ) ; return moduleTypeDtos ; }
public void test() { if ( hueBridge != null ) { hueBridge . setSensorState ( sensor , stateUpdate ) . thenAccept ( result code_block = LoopStatement ; ) . exceptionally ( e code_block = LoopStatement ; ) ; } else { logger . debug ( "No bridge connected or selected. Cannot set sensor state." ) ; } }
public void test() { for ( TimelineLayer layer : myLayers ) { long t0 = System . nanoTime ( ) ; layer . paint ( g2d ) ; LOGGER . trace ( StringUtilities . formatTimingMessage ( "Time to paint layer " + layer . getClass ( ) . getSimpleName ( ) + ": " , System . nanoTime ( ) - t0 ) ) ; } }
public void test() { try { rs . absolute ( itemIndex ) ; } catch ( SQLException e ) { log . warn ( "The JDBC driver does not appear to support ResultSet.absolute(). Consider" + " reverting to the default behavior setting the driverSupportsAbsolute to false" , e ) ; moveCursorToRow ( itemIndex ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { ActiveMQRALogger . LOGGER . trace ( "destroy()" ) ; } }
public void test() { try { connection . setExceptionListener ( null ) ; } catch ( JMSException e ) { logger . debug ( "Error unsetting the exception listener " + this , e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { logger . debug ( e . getMessage ( ) , e ) ; } }
public void test() { try { connection . close ( ) ; code_block = IfStatement ; code_block = IfStatement ; } catch ( JMSException e ) { ActiveMQRALogger . LOGGER . debug ( "Error closing session " + this , e ) ; } }
public CompletableFuture < Void > checkAndReconnect ( Throwable t ) { LOG . error ( "Exception calling mesos ({} so far)" , failedMesosCalls . incrementAndGet ( ) , t ) ; String message = t . getMessage ( ) ; code_block = IfStatement ; return CompletableFuture . completedFuture ( null ) ; }
@ Test public void testOperationWithProfiledDatatypeParam ( ) { IParser p = ourCtx . newXmlParser ( ) ; Parameters outParams = new Parameters ( ) ; outParams . addParameter ( ) . setValue ( new StringType ( "STRINGVALOUT1" ) ) ; outParams . addParameter ( ) . setValue ( new StringType ( "STRINGVALOUT2" ) ) ; final String respString = p . encodeResourceToString ( outParams ) ; ourResponseContentType = Constants . CT_FHIR_XML + "; charset=UTF-8" ; ourResponseBody = respString ; IGenericClient client = ourCtx . newRestfulGenericClient ( "http://localhost:" + ourPort + "/fhir" ) ; client . operation ( ) . onInstance ( new IdType ( "http://foo/Patient/1" ) ) . named ( "validate-code" ) . withParameter ( Parameters . class , "code" , new CodeType ( "8495-4" ) ) . andParameter ( "system" , new UriType ( "http://loinc.org" ) ) . useHttpGet ( ) . execute ( ) ; assertEquals ( "http://localhost:" + ourPort + "/fhir/Patient/1/$validate-code?code=8495-4&system=http%3A%2F%2Floinc.org" , ourRequestUri ) ; client . operation ( ) . onInstance ( new IdType ( "http://foo/Patient/1" ) ) . named ( "validate-code" ) . withParameter ( Parameters . class , "code" , new CodeType ( "8495-4" ) ) . andParameter ( "system" , new UriType ( "http://loinc.org" ) ) . encodedXml ( ) . execute ( ) ; assertEquals ( "http://localhost:" + ourPort + "/fhir/Patient/1/$validate-code" , ourRequestUri ) ; ourLog . info ( ourRequestBodyString ) ; assertEquals ( "<Parameters xmlns=\"http://hl7.org/fhir\"><parameter><name value=\"code\"/><valueCode value=\"8495-4\"/></parameter><parameter><name value=\"system\"/><valueUri value=\"http://loinc.org\"/></parameter></Parameters>" , ourRequestBodyString ) ; }
@ Override protected void afterTest ( ) throws Exception { super . afterTest ( ) ; log . info ( "Test output to " + getName ( ) ) ; log . info ( "----------------------------------------" ) ; setOut ( sysOut ) ; log . info ( testOut . toString ( ) ) ; resetTestOut ( ) ; }
@ Override protected void afterTest ( ) throws Exception { super . afterTest ( ) ; log . info ( "Test output to " + getName ( ) ) ; log . info ( "----------------------------------------" ) ; setOut ( sysOut ) ; log . info ( testOut . toString ( ) ) ; resetTestOut ( ) ; }
public void test() { try { _publisher . destroy ( _context ) ; } catch ( Exception e ) { LOGGER . error ( e ) ; } }
public void test() { if ( head != null ) { head . moveEyelidsTo ( eyelidleftPos , eyelidrightPos ) ; } else { log . warn ( "moveEyelids - I have a null head" ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { LOGGER . error ( "DashboardMetaDataDao - getStatisticsType() :: ERROR" , e ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( MessageFormat . format ( "Start Direct I/O output job abort: job={0} ({1}), state={2}" , jobContext . getJobID ( ) , jobContext . getJobName ( ) , state ) ) ; } }
public void test() { if ( LOG . isInfoEnabled ( ) ) { long t1 = System . currentTimeMillis ( ) ; LOG . info ( MessageFormat . format ( "aborted Direct I/O output: job={0} ({1}), state={2}, elapsed={3}ms" , jobContext . getJobID ( ) , jobContext . getJobName ( ) , state , t1 - t0 ) ) ; } }
@ Test public void testSMILESFileWithSpacesAndTabs ( ) throws Exception { String filename = "data/smiles/tabs.smi" ; logger . info ( "Testing: " + filename ) ; InputStream ins = this . getClass ( ) . getClassLoader ( ) . getResourceAsStream ( filename ) ; IteratingSMILESReader reader = new IteratingSMILESReader ( ins , DefaultChemObjectBuilder . getInstance ( ) ) ; int molCount = 0 ; code_block = WhileStatement ; Assert . assertEquals ( 5 , molCount ) ; reader . close ( ) ; }
public void test() { try ( InputStream in = getClass ( ) . getResourceAsStream ( loc ) ) { p . load ( in ) ; } catch ( Exception e ) { log . warn ( "Error loading {}, possibly jar was not compiled with maven." , GIT_PROPS , e ) ; } }
public void test() { try { Set < String > control = new HashSet < > ( ) ; executor = Executors . newWorkStealingPool ( ) ; CompletionService < Void > completionService = new ExecutorCompletionService < > ( executor ) ; LOGGER . info ( "Running queries {}." , queries . stream ( ) . collect ( Collectors . joining ( ", " ) ) ) ; List < Runnable > runnables = new ArrayList < > ( ) ; code_block = ForStatement ; app . setStatus ( Messages . getString ( "GraphAnalysis.LinksSearching" , found ) ) ; code_block = ForStatement ; code_block = WhileStatement ; } catch ( Exception e ) { LOGGER . error ( e . getMessage ( ) , e ) ; } finally { code_block = IfStatement ; } }
public void test() { try { return configDao . getByKey ( KEY_ALERT_SYSTEM_ON ) . getUntil ( ) ; } catch ( DalException e ) { logger . error ( "[getAlertSystemRecovertIME]" , e ) ; return null ; } }
private void setUpKubernetes ( ) { System . setProperty ( "kubernetes.auth.tryKubeConfig" , "false" ) ; log . info ( "Creating k8s namespace: {}" , testNamespace ) ; k8s = StyxScheduler . getKubernetesClient ( schedulerConfig , "default" ) ; k8s . namespaces ( ) . createNew ( ) . withNewMetadata ( ) . withName ( testNamespace ) . endMetadata ( ) . done ( ) ; }
public void test() { try { User user = UserCacheHolder . getUserFromRequest ( request ) ; RequestStatus requestStatus = new ThriftClients ( ) . makeScheduleClient ( ) . unscheduleAllServices ( user ) ; setSessionMessage ( request , requestStatus , "Every task" , "unschedule" ) ; } catch ( TException e ) { log . error ( e ) ; } }
public void test() { if ( sessionStore == null ) { LOG . error ( "Unable to remove session from SSO Store. Session store is not configured in servlet context." ) ; } else { SsoClientPrincipal principal = ( SsoClientPrincipal ) se . getSession ( ) . getAttribute ( "principal" ) ; code_block = IfStatement ; sessionStore . removeSessionById ( se . getSession ( ) . getId ( ) ) ; } }
public void test() { try { @ SuppressWarnings ( "unchecked" ) List < Map < String , Object > > containers = ( List < Map < String , Object > > ) mBeanServer . invoke ( fabricMBean , "containers" , new Object [ ] code_block = "" ; , new String [ ] code_block = "" ; ) ; LOG . debug ( "Returned containers from MBean: {}" , containers ) ; code_block = ForStatement ; LOG . debug ( "Extracted allowlist: {}" , list ) ; } catch ( InstanceNotFoundException | MBeanException | ReflectionException e ) { LOG . error ( "Invocation to allowlist MBean failed: " + e . getMessage ( ) , e ) ; } }
private void prepareSalt ( PasswordSaltExtensionMessage msg ) { msg . setSalt ( chooser . getConfig ( ) . getDefaultServerPWDSalt ( ) ) ; LOGGER . debug ( "Salt: " + ArrayConverter . bytesToHexString ( msg . getSalt ( ) ) ) ; }
public void test() { if ( session != null ) { ( ( NioSession ) session ) . onEvent ( EventType . WRITEABLE , key . selector ( ) ) ; } else { log . warn ( "Could not find session for writable event,maybe it is closed" ) ; } }
public synchronized void writeConfigurationFile ( Path xmlFilePath ) { logger . info ( "Daten Schreiben nach: {}" , xmlFilePath . toString ( ) ) ; xmlDatenSchreiben ( xmlFilePath ) ; }
public void test() { if ( id == null ) { logger . debug ( "Bridge not discovered: id is null" ) ; return false ; } }
public void test() { if ( id . length ( ) < 10 ) { logger . debug ( "Bridge not discovered: id {} is shorter then 10." , id ) ; return false ; } }
public void test() { if ( ! id . substring ( 6 , 10 ) . equals ( BRIDGE_INDICATOR ) ) { logger . debug ( "Bridge not discovered: id {} does not contain bridge indicator {} or its at the wrong position." , id , BRIDGE_INDICATOR ) ; return false ; } }
public void test() { try { description = doGetRequest ( DESC_URL_PATTERN . replace ( "HOST" , host ) ) ; } catch ( IOException e ) { logger . debug ( "Bridge not discovered: Failure accessing description file for ip: {}" , host ) ; return false ; } }
public void test() { if ( ! description . contains ( MODEL_NAME_PHILIPS_HUE ) ) { logger . debug ( "Bridge not discovered: Description does not containing the model name: {}" , description ) ; return false ; } }
public void test() { if ( details . get ( DOMAIN_ID_KEY ) != null ) { log . warn ( "Adding domain when one already exists" ) ; } }
public void test() { if ( handler == null ) { final ClientEntity entity = getClient ( ) . getObjectFactory ( ) . newEntity ( new FullQualifiedName ( typeRef . getAnnotation ( Namespace . class ) . value ( ) , ClassUtils . getEntityTypeName ( typeRef ) ) ) ; handler = EntityInvocationHandler . getInstance ( entity , uri , uri , typeRef , service ) ; } else-if ( isDeleted ( handler ) ) { LOG . debug ( "Singleton '{}' has been deleted" , typeRef . getSimpleName ( ) ) ; handler = null ; } }
public void test() { try { final ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream ( ) ; objectMapper . writeValue ( byteArrayOutputStream , dataToPost ) ; httpPostRequest . setEntity ( new ByteArrayEntity ( byteArrayOutputStream . toByteArray ( ) , ContentType . APPLICATION_JSON ) ) ; httpResponse = closeableHttpClient . execute ( httpPostRequest ) ; final int statusCode = httpResponse . getStatusLine ( ) . getStatusCode ( ) ; code_block = IfStatement ; } catch ( IOException e ) { logger . error ( "Posting over http errored. Message: {}" , e . getMessage ( ) , e ) ; HttpClientUtils . closeQuietly ( httpResponse ) ; throw new RuntimeCommunicationException ( "Could not communicate with Flux runtime: " + fluxEndpoint ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Events that are going to be synchronized are: " + events ) ; } }
public void test() { if ( sendSyncRequest ( events ) ) { List < String > eventsUIDs = events . getEvents ( ) . stream ( ) . map ( Event :: getEvent ) . collect ( Collectors . toList ( ) ) ; log . info ( "The lastSynchronized flag of these Events will be updated: " + eventsUIDs ) ; eventService . updateEventsSyncTimestamp ( eventsUIDs , new Date ( clock . getStartTime ( ) ) ) ; } else { syncResult = false ; } }
@ Test public void testWithGeometryCollection ( ) throws CatalogTransformerException , IOException , ParseException { Date now = new Date ( ) ; MetacardImpl metacard = new MetacardImpl ( ) ; metacard . setLocation ( "GEOMETRYCOLLECTION(POINT(4 6),LINESTRING(4 6,7 10))" ) ; setupBasicMetacard ( now , metacard ) ; GeoJsonMetacardTransformer transformer = new GeoJsonMetacardTransformer ( ) ; BinaryContent content = transformer . transform ( metacard , null ) ; assertEquals ( content . getMimeTypeValue ( ) , GeoJsonMetacardTransformer . DEFAULT_MIME_TYPE . getBaseType ( ) ) ; String jsonText = new String ( content . getByteArray ( ) ) ; LOGGER . debug ( jsonText ) ; Object object = PARSER . parse ( jsonText ) ; JSONObject obj2 = ( JSONObject ) object ; Map geometryMap = ( Map ) obj2 . get ( "geometry" ) ; assertThat ( geometryMap . get ( CompositeGeometry . TYPE_KEY ) . toString ( ) , is ( GeometryCollection . TYPE ) ) ; assertThat ( geometryMap . get ( CompositeGeometry . GEOMETRIES_KEY ) , notNullValue ( ) ) ; verifyBasicMetacardJson ( now , obj2 ) ; }
public void test() { try { errorBean = new ErrorBean ( ) . setCode ( ErrorCode . EC_500 . code ( ) ) . setMessage ( ErrorCode . EC_500 . errorMessage ( ) ) ; } catch ( Exception e ) { logger . error ( "ERROR: AppUtil - httpResponseForInternalServerError()" , e ) ; } }
public void test() { try ( Transaction tx = ignite . transactions ( ) . txStart ( PESSIMISTIC , REPEATABLE_READ , 500 , 0 ) ) { int key1 = primaryKey ( cache1 ) ; log . info ( ">>> Performs put [node=" + ( ( IgniteKernal ) ignite ) . localNode ( ) + ", tx=" + tx + ", key=" + key1 + ", cache=" + cache1 . getName ( ) + ']' ) ; cache1 . put ( key1 , 0 ) ; barrier . await ( ) ; int key2 = primaryKey ( cache2 ) ; log . info ( ">>> Performs put [node=" + ( ( IgniteKernal ) ignite ) . localNode ( ) + ", tx=" + tx + ", key=" + key2 + ", cache=" + cache2 . getName ( ) + ']' ) ; cache2 . put ( key2 , 1 ) ; tx . commit ( ) ; } catch ( Throwable e ) { code_block = IfStatement ; } }
public void test() { try ( Transaction tx = ignite . transactions ( ) . txStart ( PESSIMISTIC , REPEATABLE_READ , 500 , 0 ) ) { int key1 = primaryKey ( cache1 ) ; log . info ( ">>> Performs put [node=" + ( ( IgniteKernal ) ignite ) . localNode ( ) + ", tx=" + tx + ", key=" + key1 + ", cache=" + cache1 . getName ( ) + ']' ) ; cache1 . put ( key1 , 0 ) ; barrier . await ( ) ; int key2 = primaryKey ( cache2 ) ; log . info ( ">>> Performs put [node=" + ( ( IgniteKernal ) ignite ) . localNode ( ) + ", tx=" + tx + ", key=" + key2 + ", cache=" + cache2 . getName ( ) + ']' ) ; cache2 . put ( key2 , 1 ) ; tx . commit ( ) ; } catch ( Throwable e ) { code_block = IfStatement ; } }
public void test() { try ( final Tx tx = app . tx ( ) ) { code_block = ForStatement ; tx . success ( ) ; } catch ( FrameworkException fex ) { logger . warn ( "" , fex ) ; } }
public void test() { try { return Optional . of ( wktReader . read ( wkt ) ) ; } catch ( ParseException e ) { LOGGER . debug ( "unable to convert WKT to a Geometry object: wkt={}" , wkt , e ) ; } }
public void test() { if ( expiredSites . size ( ) > 0 ) { logger . info ( "Found " + expiredSites . size ( ) + " expired approved sites." ) ; } }
@ Override public void removeTaskManager ( InstanceID instanceId ) { Preconditions . checkNotNull ( instanceId ) ; final FineGrainedTaskManagerRegistration taskManager = Preconditions . checkNotNull ( taskManagerRegistrations . remove ( instanceId ) ) ; totalRegisteredResource = totalRegisteredResource . subtract ( taskManager . getTotalResource ( ) ) ; LOG . debug ( "Remove task manager {}." , instanceId ) ; code_block = ForStatement ; }
public void test() { if ( removedHook != null ) { getBundleContext ( ) . ungetService ( hook ) ; logger . info ( "Hook unregistered: {}" , hookId ) ; } }
@ Test public void testProperties ( ) throws Exception { Logger logger = LogManager . getLogger ( "test" ) ; logger . debug ( "This is a test of the root logger" ) ; File file = new File ( "target/temp.A1" ) ; assertTrue ( "File A1 was not created" , file . exists ( ) ) ; assertTrue ( "File A1 is empty" , file . length ( ) > 0 ) ; file = new File ( "target/temp.A2" ) ; assertTrue ( "File A2 was not created" , file . exists ( ) ) ; assertTrue ( "File A2 is empty" , file . length ( ) > 0 ) ; }
public void test() { try { return getConfigConnectionURI ( replicationMysql ) ; } catch ( URISyntaxException e ) { LOGGER . error ( e . getMessage ( ) , e ) ; throw new RuntimeException ( "Unable to generate bootstrap's replication jdbc connection URI" , e ) ; } }
public void test() { try { com . liferay . commerce . model . CommerceOrderItem returnValue = CommerceOrderItemServiceUtil . updateCommerceOrderItemUnitPrice ( commerceOrderItemId , quantity , unitPrice ) ; return com . liferay . commerce . model . CommerceOrderItemSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( isInitialized ) { LOG . warn ( "RangerDefaultPolicyResourceMatcher is already initialized. init() must be done again after updating serviceDef" ) ; } }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; } catch ( Throwable t ) { _logger . error ( "Error creating user filter" , t ) ; throw new ApsSystemException ( "Error creating user filter" , t ) ; } }
public void test() { if ( transform == null ) { LOG . error ( String . format ( "Cannot find transform from %s to %s" , fromSrsId , toSrsId ) ) ; return new DirectPosition2D ( 0 , 0 ) ; } }
public void test() { try { DirectPosition src = new DirectPosition2D ( x , y ) ; MathTransform transform = getOrCreateTransform ( fromSrsId , toSrsId ) ; code_block = IfStatement ; DirectPosition directPosition = transform . transform ( src , null ) ; return directPosition ; } catch ( Throwable t ) { LOG . error ( "Error converting: x=" + x + " y=" + y + " srs=" + fromSrsId , t ) ; return new DirectPosition2D ( 0 , 0 ) ; } }
public void test() { if ( ! reply . isSuccess ( ) ) { logger . warn ( String . format ( "delete image [%s] failed after management node restarted" , msg . getResourceUuid ( ) ) ) ; } }
public void test() { if ( vo == null ) { log . debug ( "missing time {} in [{}] --> {}" , timeField . getField ( ) , bundle . getCount ( ) , bundle ) ; } else { bundleTime = timeField . toUnix ( vo ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "xasuspend on " + this . tx ) ; } }
public void test() { if ( ! accessEvaluator . isAllowedPropagationRepo ( repo ) ) { InApplicationMonitor . getInstance ( ) . incrementCounter ( APPMON_ACCESS_PREVENTION ) ; LOGGER . warn ( "preventing access to {}" , repo ) ; return false ; } }
public void test() { try { consumer . commitSync ( offsets ) ; } catch ( Exception e ) { logger . info ( "Error committing offsets." , e ) ; } finally { logger . trace ( "About to clear offsets map." ) ; offsets . clear ( ) ; } }
public void test() { try { storedPlan = ToscaEngine . resolvePlanReference ( csar , planId ) ; } catch ( NotFoundException e ) { LOG . error ( "Plan with ID {} does not exist in CSAR {}!" , planId , csar . id ( ) . csarName ( ) ) ; return null ; } }
public void test() { if ( heatParameters . isRight ( ) && ( heatParameters . right ( ) . value ( ) != ResultStatusEnum . ELEMENT_NOT_FOUND ) ) { log . info ( "failed to parse heat parameters " ) ; ResponseFormat responseFormat = componentsUtils . getResponseFormat ( ActionStatus . INVALID_DEPLOYMENT_ARTIFACT_HEAT , artifactInfo . getArtifactType ( ) ) ; return Either . right ( responseFormat ) ; } else-if ( heatParameters . isLeft ( ) && heatParameters . left ( ) . value ( ) != null ) { artifactInfo . setListHeatParameters ( heatParameters . left ( ) . value ( ) ) ; } }
public void test() { try { content = resourceAdminServiceStub . getTextContent ( path ) ; } catch ( RemoteException e ) { log . error ( "Unable get content : " + e . getMessage ( ) ) ; throw new RemoteException ( "Restore version error : " , e ) ; } catch ( ResourceAdminServiceExceptionException e ) { log . error ( "GetTextContent Error : " + e . getMessage ( ) ) ; throw new ResourceAdminServiceExceptionException ( "GetTextContent Error :  " , e ) ; } }
public void test() { try { content = resourceAdminServiceStub . getTextContent ( path ) ; } catch ( RemoteException e ) { log . error ( "Unable get content : " + e . getMessage ( ) ) ; throw new RemoteException ( "Restore version error : " , e ) ; } catch ( ResourceAdminServiceExceptionException e ) { log . error ( "GetTextContent Error : " + e . getMessage ( ) ) ; throw new ResourceAdminServiceExceptionException ( "GetTextContent Error :  " , e ) ; } }
public StgNmbZusatz merge ( StgNmbZusatz detachedInstance ) { log . debug ( "merging StgNmbZusatz instance" ) ; code_block = TryStatement ;  }
public void test() { try { StgNmbZusatz result = ( StgNmbZusatz ) sessionFactory . getCurrentSession ( ) . merge ( detachedInstance ) ; log . debug ( "merge successful" ) ; return result ; } catch ( RuntimeException re ) { log . error ( "merge failed" , re ) ; throw re ; } }
public void test() { try { StgNmbZusatz result = ( StgNmbZusatz ) sessionFactory . getCurrentSession ( ) . merge ( detachedInstance ) ; log . debug ( "merge successful" ) ; return result ; } catch ( RuntimeException re ) { log . error ( "merge failed" , re ) ; throw re ; } }
@ Override protected void afterStart ( ClusterActionEvent event ) throws IOException { ClusterSpec clusterSpec = event . getClusterSpec ( ) ; Cluster cluster = event . getCluster ( ) ; Configuration config = getConfiguration ( clusterSpec , SOLR_DEFAULT_CONFIG ) ; int jettyPort = config . getInt ( SOLR_JETTY_PORT ) ; LOG . info ( "Completed configuration of {}" , clusterSpec . getClusterName ( ) ) ; LOG . info ( "Solr Hosts: {}" , getHosts ( cluster . getInstancesMatching ( role ( SOLR_ROLE ) ) , jettyPort ) ) ; }
@ Override protected void afterStart ( ClusterActionEvent event ) throws IOException { ClusterSpec clusterSpec = event . getClusterSpec ( ) ; Cluster cluster = event . getCluster ( ) ; Configuration config = getConfiguration ( clusterSpec , SOLR_DEFAULT_CONFIG ) ; int jettyPort = config . getInt ( SOLR_JETTY_PORT ) ; LOG . info ( "Completed configuration of {}" , clusterSpec . getClusterName ( ) ) ; LOG . info ( "Solr Hosts: {}" , getHosts ( cluster . getInstancesMatching ( role ( SOLR_ROLE ) ) , jettyPort ) ) ; }
@ Test public void logInfo_shouldLogInfo ( ) { hazelcastLogger . info ( MESSAGE ) ; verify ( mockLogger , times ( 1 ) ) . logIfEnabled ( LOGGER_NAME , INFO , null , MESSAGE ) ; }
public boolean SaveVectorImageAsLocalFile ( JavaPairRDD < Integer , String > distributedImage , String outputPath , ImageType imageType ) throws Exception { logger . info ( "[Sedona-Viz][SaveVectormageAsLocalFile][Start]" ) ; JavaRDD < String > distributedVectorImageNoKey = distributedImage . map ( new Function < Tuple2 < Integer , String > , String > ( ) code_block = "" ; ) ; this . SaveVectorImageAsLocalFile ( distributedVectorImageNoKey . collect ( ) , outputPath , imageType ) ; logger . info ( "[Sedona-Viz][SaveVectormageAsLocalFile][Stop]" ) ; return true ; }
public boolean SaveVectorImageAsLocalFile ( JavaPairRDD < Integer , String > distributedImage , String outputPath , ImageType imageType ) throws Exception { logger . info ( "[Sedona-Viz][SaveVectormageAsLocalFile][Start]" ) ; JavaRDD < String > distributedVectorImageNoKey = distributedImage . map ( new Function < Tuple2 < Integer , String > , String > ( ) code_block = "" ; ) ; this . SaveVectorImageAsLocalFile ( distributedVectorImageNoKey . collect ( ) , outputPath , imageType ) ; logger . info ( "[Sedona-Viz][SaveVectormageAsLocalFile][Stop]" ) ; return true ; }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "Updated digest file to " + digest ) ; } }
@ Test public void testContainerSpecSerialization ( ) { final ContainerSpec spec = new ContainerSpec ( ) ; spec . setId ( "id" ) ; spec . setContainerName ( "name" ) ; spec . setStatus ( KieContainerStatus . STARTED ) ; spec . setReleasedId ( new ReleaseId ( "groupId" , "artifactId" , "1.0" ) ) ; final ProcessConfig processConfig = new ProcessConfig ( "runtimeStrategy" , "kBase" , "kSession" , "mergeMode" ) ; spec . addConfig ( Capability . PROCESS , processConfig ) ; final RuleConfig ruleConfig = new RuleConfig ( 1L , KieScannerStatus . SCANNING ) ; spec . addConfig ( Capability . RULE , ruleConfig ) ; final String specContent = WebSocketUtils . marshal ( spec ) ; LOGGER . info ( "JSON content\n{}" , specContent ) ; final ContainerSpec specResult = WebSocketUtils . unmarshal ( specContent , ContainerSpec . class ) ; assertNotNull ( specResult ) ; assertEquals ( spec , specResult ) ; assertEquals ( spec . getId ( ) , specResult . getId ( ) ) ; assertEquals ( spec . getStatus ( ) , specResult . getStatus ( ) ) ; assertEquals ( spec . getContainerName ( ) , specResult . getContainerName ( ) ) ; assertEquals ( spec . getReleasedId ( ) , specResult . getReleasedId ( ) ) ; assertNotNull ( specResult . getConfigs ( ) ) ; assertEquals ( spec . getConfigs ( ) . size ( ) , specResult . getConfigs ( ) . size ( ) ) ; final ContainerConfig processConfigResult = specResult . getConfigs ( ) . get ( Capability . PROCESS ) ; assertNotNull ( processConfigResult ) ; assertTrue ( processConfigResult instanceof ProcessConfig ) ; assertEquals ( processConfig , processConfigResult ) ; final ContainerConfig ruleConfigResult = specResult . getConfigs ( ) . get ( Capability . RULE ) ; assertNotNull ( ruleConfigResult ) ; assertTrue ( ruleConfigResult instanceof RuleConfig ) ; assertEquals ( ruleConfig , ruleConfigResult ) ; }
public void persist ( StgSysExportItv transientInstance ) { log . debug ( "persisting StgSysExportItv instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . persist ( transientInstance ) ; log . debug ( "persist successful" ) ; } catch ( RuntimeException re ) { log . error ( "persist failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . persist ( transientInstance ) ; log . debug ( "persist successful" ) ; } catch ( RuntimeException re ) { log . error ( "persist failed" , re ) ; throw re ; } }
public void test() { try { Instant instant = parseInstant ( dateString ) ; return Date . from ( instant ) ; } catch ( DateTimeParseException e ) { LOGGER . warn ( MessageFormat . format ( "Could not parse date string: \"{0}\"" , dateString ) , e ) ; } }
public void test() { try { csvTransformGenerator . dropTable ( modelInfo . getStageTableName ( ) ) ; } catch ( CsvTransformGeneratorException e ) { logger . info ( "Could not drop table before staging" ) ; } }
public void test() { if ( barrier != null ) { log . info ( "Wait data check." ) ; barrier . await ( 60_000 , TimeUnit . MILLISECONDS ) ; log . info ( "Finished wait data check." ) ; } }
public void test() { if ( barrier != null ) { log . info ( "Wait data check." ) ; barrier . await ( 60_000 , TimeUnit . MILLISECONDS ) ; log . info ( "Finished wait data check." ) ; } }
public void test() { if ( type == Transfer . Destination . OBJECT ) { WORKER_LOGGER . info ( "Received data " + dataId + " with associated object " + object ) ; this . dataManager . storeValue ( dataId , object ) ; } else { String nameId = ( new File ( dataId ) ) . getName ( ) ; WORKER_LOGGER . info ( "Received data " + nameId + " with path " + dataId ) ; this . dataManager . storeFile ( nameId , dataId ) ; } }
public void test() { if ( type == Transfer . Destination . OBJECT ) { WORKER_LOGGER . info ( "Received data " + dataId + " with associated object " + object ) ; this . dataManager . storeValue ( dataId , object ) ; } else { String nameId = ( new File ( dataId ) ) . getName ( ) ; WORKER_LOGGER . info ( "Received data " + nameId + " with path " + dataId ) ; this . dataManager . storeFile ( nameId , dataId ) ; } }
public void test() { if ( WORKER_LOGGER_DEBUG ) { WORKER_LOGGER . debug ( "Pending parameters: " + ( ( MultiOperationFetchListener ) wdr . getListener ( ) ) . getMissingOperations ( ) ) ; } }
public void test() { try { UserSelfRestClient . changePassword ( passwordField . getModelObject ( ) ) ; SyncopeEnduserSession . get ( ) . invalidate ( ) ; final PageParameters parameters = new PageParameters ( ) ; parameters . add ( Constants . NOTIFICATION_MSG_PARAM , getString ( "self.pwd.change.success" ) ) ; setResponsePage ( getApplication ( ) . getHomePage ( ) , parameters ) ; setResponsePage ( getApplication ( ) . getHomePage ( ) , parameters ) ; } catch ( Exception e ) { LOG . error ( "While changing password for {}" , SyncopeEnduserSession . get ( ) . getSelfTO ( ) . getUsername ( ) , e ) ; SyncopeEnduserSession . get ( ) . onException ( e ) ; notificationPanel . refresh ( target ) ; } }
public void test() { try { log . debug ( "Calling ConnectionsActionSetService" ) ; return ( IActionSet ) getServiceFromRegistry ( context , createFilterConnectionsActionSet ( name , version ) ) ; } catch ( InvalidSyntaxException e ) { throw new ActivatorException ( e ) ; } }
@ Override public List < SecurityRuleInstance > getAllSecurityRules ( Order order , SystemUser systemUser ) throws FogbowException { LOGGER . debug ( String . format ( Messages . Log . MAPPING_USER_OP_S , GET_ALL_SECURITY_RULES_OPERATION , order ) ) ; CloudUser cloudUser = this . mapperPlugin . map ( systemUser ) ; LOGGER . debug ( String . format ( Messages . Log . MAPPED_USER_S , cloudUser ) ) ; List < SecurityRuleInstance > securityRuleInstances = null ; String auditableResponse = null ; code_block = TryStatement ;  return securityRuleInstances ; }
@ Override public List < SecurityRuleInstance > getAllSecurityRules ( Order order , SystemUser systemUser ) throws FogbowException { LOGGER . debug ( String . format ( Messages . Log . MAPPING_USER_OP_S , GET_ALL_SECURITY_RULES_OPERATION , order ) ) ; CloudUser cloudUser = this . mapperPlugin . map ( systemUser ) ; LOGGER . debug ( String . format ( Messages . Log . MAPPED_USER_S , cloudUser ) ) ; List < SecurityRuleInstance > securityRuleInstances = null ; String auditableResponse = null ; code_block = TryStatement ;  return securityRuleInstances ; }
public void test() { try { securityRuleInstances = doGetAllSecurityRules ( order , cloudUser ) ; LOGGER . debug ( String . format ( Messages . Log . RESPONSE_RECEIVED_S , securityRuleInstances ) ) ; auditableResponse = securityRuleInstances . toString ( ) ; } catch ( Throwable e ) { LOGGER . debug ( String . format ( Messages . Exception . GENERIC_EXCEPTION_S , e + e . getMessage ( ) ) ) ; auditableResponse = e . getClass ( ) . getName ( ) ; throw e ; } finally { auditRequest ( Operation . GET_ALL , order . getType ( ) , systemUser , auditableResponse ) ; } }
public void test() { try { securityRuleInstances = doGetAllSecurityRules ( order , cloudUser ) ; LOGGER . debug ( String . format ( Messages . Log . RESPONSE_RECEIVED_S , securityRuleInstances ) ) ; auditableResponse = securityRuleInstances . toString ( ) ; } catch ( Throwable e ) { LOGGER . debug ( String . format ( Messages . Exception . GENERIC_EXCEPTION_S , e + e . getMessage ( ) ) ) ; auditableResponse = e . getClass ( ) . getName ( ) ; throw e ; } finally { auditRequest ( Operation . GET_ALL , order . getType ( ) , systemUser , auditableResponse ) ; } }
private Map < Tuple < ActivityFacility , Double > , Map < String , Double > > sortMeasurePointsByYAndXCoord ( ) { LOG . info ( "Start sorting measure points." ) ; Map < Double , List < Double > > coordMap = new TreeMap < > ( ) ; List < Double > yValues = new LinkedList < > ( ) ; code_block = ForStatement ; yValues . sort ( Comparator . naturalOrder ( ) ) ; code_block = ForStatement ; Map < Tuple < ActivityFacility , Double > , Map < String , Double > > accessibilitiesMap2 = new LinkedHashMap < > ( ) ; code_block = ForStatement ; LOG . info ( "Finish sorting measure points." ) ; return accessibilitiesMap2 ; }
private Map < Tuple < ActivityFacility , Double > , Map < String , Double > > sortMeasurePointsByYAndXCoord ( ) { LOG . info ( "Start sorting measure points." ) ; Map < Double , List < Double > > coordMap = new TreeMap < > ( ) ; List < Double > yValues = new LinkedList < > ( ) ; code_block = ForStatement ; yValues . sort ( Comparator . naturalOrder ( ) ) ; code_block = ForStatement ; Map < Tuple < ActivityFacility , Double > , Map < String , Double > > accessibilitiesMap2 = new LinkedHashMap < > ( ) ; code_block = ForStatement ; LOG . info ( "Finish sorting measure points." ) ; return accessibilitiesMap2 ; }
public void test() { try { Method parseFrom = protoClass . getMethod ( "parseFrom" , new Class [ ] code_block = "" ; ) ; return ( M ) parseFrom . invoke ( null , new Object [ ] code_block = "" ; ) ; } catch ( NoSuchMethodException e ) { LOG . error ( "Could not find method parseFrom in class " + protoClass , e ) ; throw new IllegalArgumentException ( e ) ; } catch ( IllegalAccessException e ) { LOG . error ( "Could not access method parseFrom in class " + protoClass , e ) ; throw new IllegalArgumentException ( e ) ; } catch ( InvocationTargetException e ) { LOG . error ( "Error invoking method parseFrom in class " + protoClass , e ) ; } }
public void test() { try { Method parseFrom = protoClass . getMethod ( "parseFrom" , new Class [ ] code_block = "" ; ) ; return ( M ) parseFrom . invoke ( null , new Object [ ] code_block = "" ; ) ; } catch ( NoSuchMethodException e ) { LOG . error ( "Could not find method parseFrom in class " + protoClass , e ) ; throw new IllegalArgumentException ( e ) ; } catch ( IllegalAccessException e ) { LOG . error ( "Could not access method parseFrom in class " + protoClass , e ) ; throw new IllegalArgumentException ( e ) ; } catch ( InvocationTargetException e ) { LOG . error ( "Error invoking method parseFrom in class " + protoClass , e ) ; } }
public void test() { try { Method parseFrom = protoClass . getMethod ( "parseFrom" , new Class [ ] code_block = "" ; ) ; return ( M ) parseFrom . invoke ( null , new Object [ ] code_block = "" ; ) ; } catch ( NoSuchMethodException e ) { LOG . error ( "Could not find method parseFrom in class " + protoClass , e ) ; throw new IllegalArgumentException ( e ) ; } catch ( IllegalAccessException e ) { LOG . error ( "Could not access method parseFrom in class " + protoClass , e ) ; throw new IllegalArgumentException ( e ) ; } catch ( InvocationTargetException e ) { LOG . error ( "Error invoking method parseFrom in class " + protoClass , e ) ; } }
public void test() { if ( _log . isWarnEnabled ( ) ) { _log . warn ( exception , exception ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( exception , exception ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "fromOid(" + internalId + ")" ) ; } }
public void test() { try ( Table table = connection . getTable ( tableName ) ) { ArrayList < RegionInfo > regionInfos = new ArrayList < > ( admin . getRegions ( selected . getTableName ( ) ) ) ; int numRegions = regionInfos . size ( ) ; int average_rows = 1 ; int numRows = average_rows * numRegions ; LOG . info ( "Adding " + numRows + " rows to table: " + selected ) ; code_block = ForStatement ; TableDescriptor freshTableDesc = admin . getDescriptor ( tableName ) ; Assert . assertTrue ( "After insert, Table: " + tableName + " in not enabled" , admin . isTableEnabled ( tableName ) ) ; enabledTables . put ( tableName , freshTableDesc ) ; LOG . info ( "Added " + numRows + " rows to table: " + selected ) ; } catch ( Exception e ) { LOG . warn ( "Caught exception in action: " + this . getClass ( ) ) ; throw e ; } finally { admin . close ( ) ; } }
public void test() { try ( Table table = connection . getTable ( tableName ) ) { ArrayList < RegionInfo > regionInfos = new ArrayList < > ( admin . getRegions ( selected . getTableName ( ) ) ) ; int numRegions = regionInfos . size ( ) ; int average_rows = 1 ; int numRows = average_rows * numRegions ; LOG . info ( "Adding " + numRows + " rows to table: " + selected ) ; code_block = ForStatement ; TableDescriptor freshTableDesc = admin . getDescriptor ( tableName ) ; Assert . assertTrue ( "After insert, Table: " + tableName + " in not enabled" , admin . isTableEnabled ( tableName ) ) ; enabledTables . put ( tableName , freshTableDesc ) ; LOG . info ( "Added " + numRows + " rows to table: " + selected ) ; } catch ( Exception e ) { LOG . warn ( "Caught exception in action: " + this . getClass ( ) ) ; throw e ; } finally { admin . close ( ) ; } }
public void test() { if ( condition . hasDescription ( ) ) { logger . info ( "Resolving condition with description: " + condition . getDescription ( ) ) ; } }
public void test() { try { Narrative n = this . narrativeProvider . getNarrative ( this . measureResourceProvider . getContext ( ) , cqfMeasure ) ; theResource . setText ( n . copy ( ) ) ; } catch ( Exception e ) { logger . info ( "Error generating narrative" , e ) ; } }
public void test() { try { ZipUtil . zip ( ( String [ ] ) null , destFile , true , 0 ) ; logger . error ( "Zip should fail when input String array is null" ) ; Assert . fail ( "Zip should fail when input String array is null" ) ; } catch ( IllegalArgumentException e ) { logger . debug ( "Detecting null input File array (String, File): OK" ) ; } }
public void test() { try { ZipUtil . zip ( ( String [ ] ) null , destFile , true , 0 ) ; logger . error ( "Zip should fail when input String array is null" ) ; Assert . fail ( "Zip should fail when input String array is null" ) ; } catch ( IllegalArgumentException e ) { logger . debug ( "Detecting null input File array (String, File): OK" ) ; } }
public void test() { try { ZipUtil . zip ( new String [ ] code_block = "" ; , destFile , true , ZipUtil . NO_COMPRESSION ) ; logger . error ( "Zip should fail when any input filename is null" ) ; Assert . fail ( "Zip should fail when any input filename is null" ) ; } catch ( IllegalArgumentException e ) { logger . debug ( "Detecting null input filename (String, File): OK" ) ; } }
public void test() { try { ZipUtil . zip ( new String [ ] code_block = "" ; , destFile , true , ZipUtil . NO_COMPRESSION ) ; logger . error ( "Zip should fail when any input filename is null" ) ; Assert . fail ( "Zip should fail when any input filename is null" ) ; } catch ( IllegalArgumentException e ) { logger . debug ( "Detecting null input filename (String, File): OK" ) ; } }
public void test() { try { ZipUtil . zip ( new String [ ] code_block = "" ; , destFile , true , ZipUtil . NO_COMPRESSION ) ; logger . error ( "Zip should fail when any input filename does not exist" ) ; Assert . fail ( "Zip should fail when any input filename does not exist" ) ; } catch ( FileNotFoundException e ) { logger . debug ( "Detecting non-existing input filename (String, File): OK" ) ; } }
public void test() { try { ZipUtil . zip ( new String [ ] code_block = "" ; , destFile , true , ZipUtil . NO_COMPRESSION ) ; logger . error ( "Zip should fail when any input filename does not exist" ) ; Assert . fail ( "Zip should fail when any input filename does not exist" ) ; } catch ( FileNotFoundException e ) { logger . debug ( "Detecting non-existing input filename (String, File): OK" ) ; } }
public void test() { try { ZipUtil . zip ( new String [ ] code_block = "" ; , ( File ) null , true , ZipUtil . NO_COMPRESSION ) ; logger . error ( "Zip should fail when destination File is null" ) ; Assert . fail ( "Zip should fail when destination File is null" ) ; } catch ( IllegalArgumentException e ) { logger . debug ( "Detecting null destination File (String, File): OK" ) ; } }
public void test() { try { ZipUtil . zip ( new String [ ] code_block = "" ; , ( File ) null , true , ZipUtil . NO_COMPRESSION ) ; logger . error ( "Zip should fail when destination File is null" ) ; Assert . fail ( "Zip should fail when destination File is null" ) ; } catch ( IllegalArgumentException e ) { logger . debug ( "Detecting null destination File (String, File): OK" ) ; } }
public void test() { try { ZipUtil . zip ( new String [ ] code_block = "" ; , sampleZip , true , ZipUtil . NO_COMPRESSION ) ; logger . error ( "Zip should fail when destination file already exists" ) ; Assert . fail ( "Zip should fail when destination file already exists" ) ; } catch ( IllegalArgumentException e ) { logger . debug ( "Detecting existing destination File (String, File): OK" ) ; } }
public void test() { try { ZipUtil . zip ( new String [ ] code_block = "" ; , sampleZip , true , ZipUtil . NO_COMPRESSION ) ; logger . error ( "Zip should fail when destination file already exists" ) ; Assert . fail ( "Zip should fail when destination file already exists" ) ; } catch ( IllegalArgumentException e ) { logger . debug ( "Detecting existing destination File (String, File): OK" ) ; } }
public void test() { try { ZipUtil . zip ( new String [ ] code_block = "" ; , dummieFile , true , ZipUtil . NO_COMPRESSION ) ; logger . error ( "Zip should fail when the destination File does not represent a zip file" ) ; Assert . fail ( "Zip should fail when the destination File does not represent a zip file" ) ; } catch ( IllegalArgumentException e ) { logger . debug ( "Detecting destination File not representing a valid zip file (String, File): OK" ) ; } }
public void test() { try { ZipUtil . zip ( new String [ ] code_block = "" ; , dummieFile , true , ZipUtil . NO_COMPRESSION ) ; logger . error ( "Zip should fail when the destination File does not represent a zip file" ) ; Assert . fail ( "Zip should fail when the destination File does not represent a zip file" ) ; } catch ( IllegalArgumentException e ) { logger . debug ( "Detecting destination File not representing a valid zip file (String, File): OK" ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "SBMLReader.readMathML called" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "saveProgramAccess:" + pa . getId ( ) ) ; } }
public void setMaxTuples ( int maxNumbers ) { LOG . debug ( "setting max tuples to {}" , maxNumbers ) ; this . maxTuples = maxNumbers ; }
public void test() { try { QueueBrowser browser = createBrowser ( broker , dest ) ; int count = browseMessages ( browser , broker ) ; code_block = IfStatement ; LOG . info ( "browser '" + broker + "' browsed " + totalCount ) ; Thread . sleep ( 1000 ) ; } catch ( Exception e ) { LOG . info ( "Exception browsing " + e , e ) ; } finally { code_block = TryStatement ;  } }
public void test() { try { QueueBrowser browser = createBrowser ( broker , dest ) ; int count = browseMessages ( browser , broker ) ; code_block = IfStatement ; LOG . info ( "browser '" + broker + "' browsed " + totalCount ) ; Thread . sleep ( 1000 ) ; } catch ( Exception e ) { LOG . info ( "Exception browsing " + e , e ) ; } finally { code_block = TryStatement ;  } }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; } catch ( Exception e ) { LOG . info ( "Exception closing browser " + e , e ) ; } }
public void test() { try { filter . initialize ( ) ; } catch ( Throwable t ) { logger . error ( "Filter failed to initialize" , t ) ; errors . addError ( _ ( "Failed to initialize filter. See log file for details." ) ) ; } }
protected void postCommit ( Xid arg0 ) { logger . info ( "In postCommit with: [" + arg0 + "]" ) ; }
public void test() { if ( _log . isWarnEnabled ( ) ) { _log . warn ( invalidDDMStructureException . getMessage ( ) ) ; } }
@ Override public IRODSRuleExecResult executeRuleFromParts ( final String ruleBody , final List < String > inputParameters , final List < String > outputParameters ) throws JargonException { code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "ruleBody:{}" , ruleBody ) ; log . info ( "inputParameters:{}" , inputParameters ) ; log . info ( "outputParameters:{}" , outputParameters ) ; String ruleAsString = buildRuleStringFromParts ( ruleBody , inputParameters , outputParameters ) ; RuleProcessingAO ruleProcessingAO = irodsAccessObjectFactory . getRuleProcessingAO ( getIrodsAccount ( ) ) ; RuleInvocationConfiguration ruleInvocationConfiguration = RuleInvocationConfiguration . instanceWithDefaultAutoSettings ( irodsAccessObjectFactory . getJargonProperties ( ) ) ; log . info ( "getting ready to submit rule:{}" , ruleAsString ) ; return ruleProcessingAO . executeRule ( ruleAsString , null , ruleInvocationConfiguration ) ; }
@ Override public IRODSRuleExecResult executeRuleFromParts ( final String ruleBody , final List < String > inputParameters , final List < String > outputParameters ) throws JargonException { code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "ruleBody:{}" , ruleBody ) ; log . info ( "inputParameters:{}" , inputParameters ) ; log . info ( "outputParameters:{}" , outputParameters ) ; String ruleAsString = buildRuleStringFromParts ( ruleBody , inputParameters , outputParameters ) ; RuleProcessingAO ruleProcessingAO = irodsAccessObjectFactory . getRuleProcessingAO ( getIrodsAccount ( ) ) ; RuleInvocationConfiguration ruleInvocationConfiguration = RuleInvocationConfiguration . instanceWithDefaultAutoSettings ( irodsAccessObjectFactory . getJargonProperties ( ) ) ; log . info ( "getting ready to submit rule:{}" , ruleAsString ) ; return ruleProcessingAO . executeRule ( ruleAsString , null , ruleInvocationConfiguration ) ; }
@ Override public IRODSRuleExecResult executeRuleFromParts ( final String ruleBody , final List < String > inputParameters , final List < String > outputParameters ) throws JargonException { code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "ruleBody:{}" , ruleBody ) ; log . info ( "inputParameters:{}" , inputParameters ) ; log . info ( "outputParameters:{}" , outputParameters ) ; String ruleAsString = buildRuleStringFromParts ( ruleBody , inputParameters , outputParameters ) ; RuleProcessingAO ruleProcessingAO = irodsAccessObjectFactory . getRuleProcessingAO ( getIrodsAccount ( ) ) ; RuleInvocationConfiguration ruleInvocationConfiguration = RuleInvocationConfiguration . instanceWithDefaultAutoSettings ( irodsAccessObjectFactory . getJargonProperties ( ) ) ; log . info ( "getting ready to submit rule:{}" , ruleAsString ) ; return ruleProcessingAO . executeRule ( ruleAsString , null , ruleInvocationConfiguration ) ; }
@ Override public IRODSRuleExecResult executeRuleFromParts ( final String ruleBody , final List < String > inputParameters , final List < String > outputParameters ) throws JargonException { code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "ruleBody:{}" , ruleBody ) ; log . info ( "inputParameters:{}" , inputParameters ) ; log . info ( "outputParameters:{}" , outputParameters ) ; String ruleAsString = buildRuleStringFromParts ( ruleBody , inputParameters , outputParameters ) ; RuleProcessingAO ruleProcessingAO = irodsAccessObjectFactory . getRuleProcessingAO ( getIrodsAccount ( ) ) ; RuleInvocationConfiguration ruleInvocationConfiguration = RuleInvocationConfiguration . instanceWithDefaultAutoSettings ( irodsAccessObjectFactory . getJargonProperties ( ) ) ; log . info ( "getting ready to submit rule:{}" , ruleAsString ) ; return ruleProcessingAO . executeRule ( ruleAsString , null , ruleInvocationConfiguration ) ; }
public void test() { try { Category category = this . getCategory ( selectedNode ) ; code_block = IfStatement ; this . setParentCategoryCode ( category . getParentCode ( ) ) ; this . setCategoryCode ( category . getCode ( ) ) ; this . setTitles ( category . getTitles ( ) ) ; } catch ( Throwable t ) { _logger . error ( "error in extractCategoryFormValues" , t ) ; return FAILURE ; } }
public void test() { if ( ! aggObjectFactory . objectDefined ( interfaceClass ) ) { Logger . warn ( PentahoSystem . class . getName ( ) , Messages . getInstance ( ) . getErrorString ( "PentahoSystem.WARN_OBJECT_NOT_CONFIGURED" , interfaceClass . getSimpleName ( ) ) ) ; return null ; } }
public void test() { try { code_block = IfStatement ; IPentahoSession curSession = ( session == null ) ? PentahoSessionHolder . getSession ( ) : session ; return aggObjectFactory . get ( interfaceClass , curSession , properties ) ; } catch ( ObjectFactoryException e ) { Logger . debug ( PentahoSystem . class . getName ( ) , Messages . getInstance ( ) . getErrorString ( "PentahoSystem.ERROR_0026_COULD_NOT_RETRIEVE_CONFIGURED_OBJECT" , interfaceClass . getSimpleName ( ) ) , e ) ; return null ; } }
public void test() { try { @ SuppressWarnings ( "unchecked" ) Class < ? extends InvalidListPruningDebug > clazz = ( Class < ? extends InvalidListPruningDebug > ) getClass ( ) . getClassLoader ( ) . loadClass ( PRUNING_TOOL_CLASS_NAME ) ; this . pruningDebug = clazz . newInstance ( ) ; pruningDebug . initialize ( configuration ) ; } catch ( Exception e ) { LOG . error ( "Not able to instantiate pruning debug class" , e ) ; responder . sendString ( HttpResponseStatus . INTERNAL_SERVER_ERROR , "Cannot instantiate the pruning debug tool: " + e . getMessage ( ) ) ; pruningDebug = null ; return false ; } }
synchronized boolean removeKey ( Integer keyId ) { requireNonNull ( keyId ) ; log . debug ( "Removing AuthenticatioKey with keyId {}" , keyId ) ; return allKeys . remove ( keyId ) != null ; }
@ Override public void update ( ActionDesignTrace actionDesignTrace ) { LOGGER . trace ( MessageFormat . format ( "Updating ActionDesignTrace {0}." , actionDesignTrace . getMetadataKey ( ) . toString ( ) ) ) ; String updateStatement = updateStatement ( actionDesignTrace ) ; getMetadataRepository ( ) . executeUpdate ( updateStatement ) ; }
public void test() { if ( LOGGER . isInfoEnabled ( ) ) { LOGGER . info ( "channel {} read idle." , ctx . channel ( ) ) ; } }
public void test() { try { String serverAddress = NetUtil . toStringAddress ( ctx . channel ( ) . remoteAddress ( ) ) ; clientChannelManager . invalidateObject ( serverAddress , ctx . channel ( ) ) ; } catch ( Exception exx ) { LOGGER . error ( exx . getMessage ( ) ) ; } finally { clientChannelManager . releaseChannel ( ctx . channel ( ) , getAddressFromContext ( ctx ) ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "will send ping msg,channel {}" , ctx . channel ( ) ) ; } }
public void test() { try { code_block = IfStatement ; AbstractNettyRemotingClient . this . sendAsyncRequest ( ctx . channel ( ) , HeartbeatMessage . PING ) ; } catch ( Throwable throwable ) { LOGGER . error ( "send request error: {}" , throwable . getMessage ( ) , throwable ) ; } }
public void test() { try { entPrior = new EntityPriority ( ) ; entPrior . init ( ) ; existsClassPriority = existsClassPriority && entPrior . getFilterLookups ( ) ; } catch ( Exception e ) { log . error ( "Cannot create instance of Priorities class" , e ) ; entPrior = null ; } }
public void test() { try { dataFeed . feedTo ( entityListener ) ; } catch ( KIMQueryException e ) { log . error ( "Loading failed." , e ) ; throw new KIMRuntimeException ( "The loading failed." , e ) ; } finally { log . info ( "The loading from Sesame finished" ) ; } }
public void test() { try { dataFeed . feedTo ( entityListener ) ; } catch ( KIMQueryException e ) { log . error ( "Loading failed." , e ) ; throw new KIMRuntimeException ( "The loading failed." , e ) ; } finally { log . info ( "The loading from Sesame finished" ) ; } }
public void test() { if ( trustFactory != null ) { TrustManager [ ] trustManager = trustFactory . getTrustManagers ( ) ; code_block = IfStatement ; } else { LOG . debug ( "Trust Factory is empty" ) ; } }
public void test() { try { TrustManagerFactory trustFactory = getInstance ( ) . trustFactory ; code_block = IfStatement ; } catch ( IllegalStateException e ) { LOG . error ( e . getLocalizedMessage ( ) , e ) ; } }
@ Override public void writeFinished ( Connection c , Transfer t ) { LOGGER . debug ( "Finished sending " + ( t . isFile ( ) ? t . getFileName ( ) : t . getObject ( ) ) + " through connection " + c . hashCode ( ) ) ; this . agent . releaseSendSlot ( c ) ; }
public void test() { try { guiFragment = new GuiFragment ( ) ; guiFragment . setCode ( res . getString ( "code" ) ) ; guiFragment . setWidgetTypeCode ( res . getString ( "widgettypecode" ) ) ; guiFragment . setPluginCode ( res . getString ( "plugincode" ) ) ; guiFragment . setGui ( res . getString ( "gui" ) ) ; guiFragment . setDefaultGui ( res . getString ( "defaultgui" ) ) ; Integer locked = res . getInt ( "locked" ) ; guiFragment . setLocked ( null != locked && locked . intValue ( ) == 1 ) ; } catch ( Throwable t ) { logger . error ( "Error in buildGuiFragmentFromRes" , t ) ; } }
public void test() { if ( candidates . size ( ) > 1 && LOGGER . isWarnEnabled ( ) ) { LOGGER . warn ( "Multiple vocabularies found for uri {}: {}" , resourceId , candidates . stream ( ) . map ( Vocabulary :: getName ) . collect ( Collectors . joining ( ", " ) ) ) ; } }
public void test() { if ( candidates . size ( ) > 1 && LOGGER . isWarnEnabled ( ) ) { LOGGER . warn ( "Multiple vocabularies found for uri {}: {}" , resourceId , candidates . stream ( ) . map ( Vocabulary :: getName ) . collect ( Collectors . joining ( ", " ) ) ) ; } }
public void test() { if ( context . getConfiguration ( ) . get ( currentMimeTypePropName ) != null ) { String [ ] currentPortMimeTypes = StringUtils . split ( context . getConfiguration ( ) . get ( currentMimeTypePropName ) , WorkflowRuntimeParameters . DEFAULT_CSV_DELIMITER ) ; code_block = ForStatement ; } else { log . warn ( "undefined property '" + currentMimeTypePropName + "', no data will be dispatched to port '" + portName + "'" ) ; } }
public Study updateStudyDiseaseTraitByAccessionId ( String trait , String accessionId ) { Study study = this . getStudyByAccessionId ( accessionId ) . orElseThrow ( ( ) -> new ResourceNotFoundException ( "Study" , accessionId ) ) ; DiseaseTrait diseaseTrait = Optional . ofNullable ( diseaseTraitRepository . findByTraitIgnoreCase ( trait ) ) . orElseThrow ( ( ) -> new ResourceNotFoundException ( "Disease Trait" , trait ) ) ; study . setDiseaseTrait ( diseaseTrait ) ; studyRepository . save ( study ) ; log . info ( "Study with accession Id: {} found and updated" , accessionId ) ; return study ; }
@ Test @ Ignore public final void testSendProcessConfigurationRequest ( ) { ActiveRequestSenderTest . testType = TestType . CONFIG ; LOGGER . debug ( "Starting " + ActiveRequestSenderTest . testType . getName ( ) ) ; ProcessConfiguration processConfiguration = new ProcessConfiguration ( ) ; processConfiguration . setProcessName ( PROCESS_NAME ) ; processConfiguration . setprocessPIK ( PROCESS_PIK ) ; ProcessConfigurationHolder . setInstance ( processConfiguration ) ; ProcessConfigurationResponse processConfigurationResponse = this . activeRequestSender . sendProcessConfigurationRequest ( PROCESS_NAME ) ; compareConfiguration ( processConfigurationResponse ) ; }
public void test() { try { results = em . searchCollection ( em . getApplicationRef ( ) , "propertymaps" , q ) ; } catch ( Exception ex ) { logger . error ( "Error getting system properties" , ex ) ; return false ; } }
public void test() { try { em . update ( propsEntity ) ; } catch ( Exception ex ) { logger . error ( "Error updating service properties" , ex ) ; return false ; } }
public void test() { try { int cuboidCount = CuboidCLI . simulateCuboidGeneration ( createdDesc ) ; logger . info ( "New cube " + cubeName + " has " + cuboidCount + " cuboids" ) ; } catch ( Exception e ) { getCubeDescManager ( ) . removeCubeDesc ( createdDesc ) ; throw new InternalErrorException ( "Failed to deal with the request." , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Using ThemeResolver [" + this . themeResolver + "]" ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Using ThemeResolver [" + this . themeResolver + "]" ) ; } }
public void test() { if ( entities . getCount ( ) > 0 ) { LOGGER . debug ( "{} to go." , entities . getCount ( ) ) ; } else { more = false ; } }
public int assertStatusCode ( Response res , String testName ) { int statusCode = res . getStatus ( ) ; logger . debug ( testName + ": status = " + statusCode ) ; Assert . assertTrue ( testRequestType . isValidStatusCode ( statusCode ) , invalidStatusCodeMessage ( testRequestType , statusCode ) ) ; Assert . assertEquals ( statusCode , testExpectedStatusCode ) ; return statusCode ; }
public void test() { try { final Object response = externalContext . getResponse ( ) ; final Object request = externalContext . getRequest ( ) ; code_block = IfStatement ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; } }
@ Override public void callCrawlerService ( ) { LOGGER . info ( "Launching crawler for page " + getUrl ( ) ) ; getCrawlerService ( ) . crawlSite ( getAudit ( ) , getUrl ( ) ) ; }
public void test() { try { java . util . List < com . liferay . calendar . model . CalendarResource > returnValue = CalendarResourceServiceUtil . search ( companyId , groupIds , classNameIds , code , name , description , active , andOperator , start , end , orderByComparator ) ; return com . liferay . calendar . model . CalendarResourceSoap . toSoapModels ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
@ Override public void encode ( Object value , OutputStream outputStream ) throws IOException { code_block = IfStatement ; IndexedRecord ir = converter . convertToAvro ( ( T ) value ) ; code_block = IfStatement ; LOG . debug ( "Internal AvroCoder's schema is {}" , internalAvroCoder . getSchema ( ) ) ; LOG . debug ( "Encode value is {}" , value ) ; internalAvroCoder . encode ( convertToAvro ( value ) , outputStream ) ; }
@ Override public void encode ( Object value , OutputStream outputStream ) throws IOException { code_block = IfStatement ; IndexedRecord ir = converter . convertToAvro ( ( T ) value ) ; code_block = IfStatement ; LOG . debug ( "Internal AvroCoder's schema is {}" , internalAvroCoder . getSchema ( ) ) ; LOG . debug ( "Encode value is {}" , value ) ; internalAvroCoder . encode ( convertToAvro ( value ) , outputStream ) ; }
public void test() { try { sql . append ( "UPDATE processing SET status = " ) ; sql . append ( "'" ) . append ( status . name ( ) ) . append ( "'" ) ; sql . append ( ", update_tstmp='" ) . append ( new Timestamp ( System . currentTimeMillis ( ) ) ) . append ( "' " ) ; sql . append ( " WHERE processing_id = " ) . append ( processingID ) ; executeUpdate ( sql . toString ( ) ) ; } catch ( SQLException e ) { logger . error ( "SQL Command failed: " + sql . toString ( ) + ":" + e . getMessage ( ) ) ; return new ReturnValue ( null , "Could not execute one of the SQL commands: " + sql . toString ( ) + "\nException: " + e . getMessage ( ) , ReturnValue . SQLQUERYFAILED ) ; } }
public void test() { if ( this . metadataRecorders . isEmpty ( ) ) { LOG . debug ( "No metadata recorder registerd yet - ingoring event: {}" , event ) ; } else { this . metadataRecorders . forEach ( r -> r . initializationStatusChanged ( event . getFeedId ( ) , event . getStatus ( ) ) ) ; } }
public void test() { if ( logToStdErr ) { System . err . println ( "Unexpected file visiting failure: " + path ) ; e . printStackTrace ( ) ; } else { LOGGER . error ( "Unexpected file visiting failure: " + path , e ) ; } }
@ Override public void doConfigure ( ServiceProfile < ? > profile ) throws InterruptedException , IOException { LOG . debug ( "Configuring file sessions: {}" , profile . getPrefix ( ) ) ; directory = prepareDirectory ( profile ) ; LOG . debug ( "Configured file sessions: {}" , directory ) ; }
@ Override public void doConfigure ( ServiceProfile < ? > profile ) throws InterruptedException , IOException { LOG . debug ( "Configuring file sessions: {}" , profile . getPrefix ( ) ) ; directory = prepareDirectory ( profile ) ; LOG . debug ( "Configured file sessions: {}" , directory ) ; }
public void test() { try { ois = new ObjectInputStream ( fs . open ( modelPath ) ) ; MLModel model = ( MLModel ) ois . readObject ( ) ; log . info ( "Loaded model {} from location {}" , model . getId ( ) , modelPath ) ; return model ; } catch ( ClassNotFoundException e ) { throw new IOException ( e ) ; } finally { IOUtils . closeQuietly ( ois ) ; } }
public List < ViewResult . Row > getDBViewQueryResult ( String id , String docEntityType ) { logger . info ( MessageFormat . format ( "Trying to load entityType: {0}, with id: {1}" , docEntityType , id ) ) ; List < ViewResult . Row > rows = db . queryView ( new ViewQuery ( ) . viewName ( CASE_ID_VIEW_NAME ) . designDocId ( "_design/" + docEntityType ) . key ( id ) . queryParam ( ID_FIELD_ON_ENTITY , id ) . includeDocs ( true ) ) . getRows ( ) ; logger . debug ( MessageFormat . format ( "Found these rows for entityType: {0}, with id: {1}, rows: {2}" , docEntityType , id , rows ) ) ; return rows ; }
public List < ViewResult . Row > getDBViewQueryResult ( String id , String docEntityType ) { logger . info ( MessageFormat . format ( "Trying to load entityType: {0}, with id: {1}" , docEntityType , id ) ) ; List < ViewResult . Row > rows = db . queryView ( new ViewQuery ( ) . viewName ( CASE_ID_VIEW_NAME ) . designDocId ( "_design/" + docEntityType ) . key ( id ) . queryParam ( ID_FIELD_ON_ENTITY , id ) . includeDocs ( true ) ) . getRows ( ) ; logger . debug ( MessageFormat . format ( "Found these rows for entityType: {0}, with id: {1}, rows: {2}" , docEntityType , id , rows ) ) ; return rows ; }
public void test() { try { String [ ] cleanupTypeString = statusMessage . split ( "\\s+" ) ; code_block = IfStatement ; } catch ( Throwable t ) { LOG . info ( "Could not parse cleanup type from {} for {}" , statusMessage , taskId ) ; } }
public void test() { switch ( method ) { case DELETE : return Role . DELETE ; case GET : code_block = IfStatement ; return Role . READ ; case HEAD : return Role . NONE ; case PATCH : return Role . UPDATE ; case POST : return Role . CREATE ; case PUT : return Role . UPDATE ; case OPTIONS : return Role . NONE ; default : LOGGER . error ( "Unknown method: {}" , method ) ; return Role . ERROR ; } }
public void test() { try { deploymentContext = new AdapterDeploymentContext ( Utils . resolveDeployment ( coreSettings ) ) ; } catch ( RuntimeException exc ) { LOGGER . error ( "Failed to initialise Keycloak. There is a problem with the configuration." ) ; throw new IllegalArgumentException ( "Exception initialising keycloak." , exc ) ; } }
public void test() { try { method = testClass . getMethod ( methodName , ( Class < ? > [ ] ) null ) ; } catch ( Exception e ) { log . warn ( "Could not get method by reflection. This could happen if you are using @Parameters in combination with annotations." , e ) ; return ; } }
public void test() { if ( loggingInitialized . get ( ) ) { log . warn ( message ) ; } else { warnings . add ( message ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "execute: " + MemoryUtils . getRuntimeMemoryStats ( ) ) ; } }
public void test() { if ( LOG . isInfoEnabled ( ) ) { LOG . info ( "execute: " + numPartitions + " partitions to process with " + numThreads + " compute thread(s), originally " + numComputeThreads + " thread(s) on superstep " + superstep ) ; } }
public void test() { if ( LOG . isInfoEnabled ( ) ) { LOG . info ( "execute: BSP application done (global vertices marked done)" ) ; } }
public void test() { try { process ( ) ; } catch ( Throwable ex ) { log . error ( "Exception happened while monitoring EntityId" , ex ) ; } finally { this . isActive . set ( false ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( InterruptedException | ExecutionException | TimeoutException e ) { logger . debug ( "Exception, previous server on {} query interrupted or timed out, restoring playlist anyway" , thing . getLabel ( ) ) ; } }
@ Test public void TestCreateDupGenericVnfFailure_1002 ( ) { new MockAAIGenericVnfSearch ( wireMockServer ) ; MockAAICreateGenericVnf ( wireMockServer ) ; MockAAIVfModulePUT ( wireMockServer , true ) ; Map < String , Object > variables = new HashMap < > ( ) ; variables . put ( "mso-request-id" , UUID . randomUUID ( ) . toString ( ) ) ; variables . put ( "isDebugLogEnabled" , "true" ) ; variables . put ( "isVidRequest" , "false" ) ; variables . put ( "vnfName" , "STMTN5MMSC21" ) ; variables . put ( "serviceId" , "00000000-0000-0000-0000-000000000000" ) ; variables . put ( "personaModelId" , "973ed047-d251-4fb9-bf1a-65b8949e0a73" ) ; variables . put ( "personaModelVersion" , "1.0" ) ; variables . put ( "vfModuleName" , "STMTN5MMSC21-MMSC::module-0-0" ) ; variables . put ( "vfModuleModelName" , "MMSC::module-0" ) ; String processId = invokeSubProcess ( "CreateAAIVfModule" , variables ) ; WorkflowException exception = BPMNUtil . getRawVariable ( processEngine , "CreateAAIVfModule" , "WorkflowException" , processId ) ; Assert . assertEquals ( 1002 , exception . getErrorCode ( ) ) ; Assert . assertEquals ( true , exception . getErrorMessage ( ) . contains ( "Invalid request for new Generic VNF which already exists" ) ) ; logger . debug ( exception . getErrorMessage ( ) ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Enter MapDeserializer::startElement()" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Exit: MapDeserializer::startElement()" ) ; } }
@ Override public void publishApi ( Api api , IAsyncResultHandler < Void > handler ) { super . publishApi ( api , handler ) ; proxy . publishApi ( api ) ; log . info ( "Published an API {0}" , api ) ; }
public void test() { if ( listenerContainer != null ) { startListenerContainer ( ) ; } else { LOG . warn ( "The listenerContainer is not instantiated. Probably there was a timeout during the Suspend operation. Please restart your consumer route." ) ; } }
public void test() { try { final String exceptionMessage = Utils . getCauseString ( cause ) ; List < ServiceManagementListener > serviceListeners = getManagementListeners ( ) ; code_block = ForStatement ; markChanged ( ) ; } catch ( Exception ex ) { logger . warn ( "Error during doExceptionCaught service listener notifications:" , ex ) ; } }
protected MsoException runtimeExceptionToMsoException ( RuntimeException e , String context ) { MsoAdapterException me = new MsoAdapterException ( e . getMessage ( ) , e ) ; me . addContext ( context ) ; me . setCategory ( MsoExceptionCategory . INTERNAL ) ; logger . error ( "{} {} An exception occured on {}: " , MessageEnum . RA_GENERAL_EXCEPTION_ARG , ErrorCode . DataError . getValue ( ) , context , e ) ; return me ; }
public void test() { try { service . init ( ) ; } catch ( Throwable t ) { LOG . error ( "Failed to initialize service {}" , serviceClassName , t ) ; throw new FalconException ( t ) ; } }
public void test() { try { calendar = ( T ) contentProcessor . getIntermediateCalendar ( interval , stream ) ; } catch ( CalendarException e ) { log . error ( "Calendar parsing exception: " + e . getCause ( ) . getMessage ( ) + " from calendar at " + url ) ; throw e ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Storing calendar cache, key:" + intermediateCacheKey ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Retrieving calendar from cache, key:" + intermediateCacheKey ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Retrieving calendar event set from cache, key:" + processorCacheKey ) ; } }
public void test() { try { Device di = getDevice ( cMessage . getSerialNumber ( ) ) ; code_block = IfStatement ; } catch ( NullPointerException e ) { logger . debug ( "Unexpected NPE cought. Please report stacktrace" , e ) ; } catch ( Exception e ) { logger . error ( "An exception occurred while calling the DeviceStatusListener" , e ) ; unregisterDeviceStatusListener ( deviceStatusListener ) ; } }
public void test() { try { Device di = getDevice ( cMessage . getSerialNumber ( ) ) ; code_block = IfStatement ; } catch ( NullPointerException e ) { logger . debug ( "Unexpected NPE cought. Please report stacktrace" , e ) ; } catch ( Exception e ) { logger . error ( "An exception occurred while calling the DeviceStatusListener" , e ) ; unregisterDeviceStatusListener ( deviceStatusListener ) ; } }
@ Override public IRODSFile getTrashHomeForLoggedInUser ( ) throws JargonException { log . info ( "getTrashHomeForLoggedInUser())" ) ; log . info ( "for user:{}" , getIRODSAccount ( ) ) ; String trashHomePath = MiscIRODSUtils . buildTrashHome ( getIRODSAccount ( ) . getUserName ( ) , getIRODSAccount ( ) . getZone ( ) ) ; log . info ( "getting file at:{}" , trashHomePath ) ; IRODSFile trashFile = getIRODSAccessObjectFactory ( ) . getIRODSFileFactory ( getIRODSAccount ( ) ) . instanceIRODSFile ( trashHomePath ) ; return trashFile ; }
@ Override public IRODSFile getTrashHomeForLoggedInUser ( ) throws JargonException { log . info ( "getTrashHomeForLoggedInUser())" ) ; log . info ( "for user:{}" , getIRODSAccount ( ) ) ; String trashHomePath = MiscIRODSUtils . buildTrashHome ( getIRODSAccount ( ) . getUserName ( ) , getIRODSAccount ( ) . getZone ( ) ) ; log . info ( "getting file at:{}" , trashHomePath ) ; IRODSFile trashFile = getIRODSAccessObjectFactory ( ) . getIRODSFileFactory ( getIRODSAccount ( ) ) . instanceIRODSFile ( trashHomePath ) ; return trashFile ; }
@ Override public IRODSFile getTrashHomeForLoggedInUser ( ) throws JargonException { log . info ( "getTrashHomeForLoggedInUser())" ) ; log . info ( "for user:{}" , getIRODSAccount ( ) ) ; String trashHomePath = MiscIRODSUtils . buildTrashHome ( getIRODSAccount ( ) . getUserName ( ) , getIRODSAccount ( ) . getZone ( ) ) ; log . info ( "getting file at:{}" , trashHomePath ) ; IRODSFile trashFile = getIRODSAccessObjectFactory ( ) . getIRODSFileFactory ( getIRODSAccount ( ) ) . instanceIRODSFile ( trashHomePath ) ; return trashFile ; }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "individualACK messageID=" + messageID ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "individualACK starting new TX" ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "ACKing ref " + ref + " on tx= " + tx + ", consumer=" + this ) ; } }
public void test() { try { MidPointApplication application = ( MidPointApplication ) MidPointApplication . get ( ) ; return application . getAuditService ( ) . reconstructObject ( type , oid , eventIdentifier , task , result ) ; } catch ( Exception ex ) { LOGGER . debug ( "Error occurred while reconsructing the object, " + ex . getMessage ( ) ) ; } }
public void test() { try { return Pattern . compile ( nonProxyHosts ) ; } catch ( Exception e ) { logger . error ( "Creating the nonProxyHosts pattern failed for http.nonProxyHosts=" + nonProxyHosts , e ) ; return null ; } }
@ Override public void call ( final Object ... args ) { logger . debug ( "Listener: Disconnected from the ambient weather service)" ) ; handleError ( Socket . EVENT_DISCONNECT , args ) ; isConnected = false ; }
public void test() { try { listener . expired ( e . getKey ( ) , null ) ; } catch ( Exception ex ) { log . error ( "Listener " + listener + " has thrown an exception" , ex ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "getRoleSets using rolesQuery: " + rolesQuery + ", username: " + username ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Executing query: " + rolesQuery + ", with username: " + username ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "No roles found" ) ; } }
public void test() { try { LoadTestDataSimpleResponseMessageType response = ( LoadTestDataSimpleResponseMessageType ) invokeClientPort ( AdminWSConstants . ADMIN_LTD_SAVEADDRESS , request ) ; logDebug ( AdminWSConstants . ADMIN_LTD_SAVEADDRESS , response . isStatus ( ) , response . getMessage ( ) ) ; return response . isStatus ( ) ; } catch ( Exception e ) { LOG . error ( "error during save address: {}" , e . getLocalizedMessage ( ) , e ) ; } }
public void test() { try { com . liferay . commerce . inventory . model . CommerceInventoryWarehouseItem returnValue = CommerceInventoryWarehouseItemServiceUtil . addCommerceInventoryWarehouseItem ( userId , commerceInventoryWarehouseId , sku , quantity ) ; return com . liferay . commerce . inventory . model . CommerceInventoryWarehouseItemSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try { MDCSetup mdcSetup = new MDCSetup ( ) ; Exception ex = message . getContent ( Exception . class ) ; code_block = IfStatement ; mdcSetup . setLogTimestamp ( ) ; mdcSetup . setElapsedTime ( ) ; logger . info ( ONAPLogConstants . Markers . EXIT , "Exiting" ) ; } catch ( Exception e ) { logger . warn ( "Error in incoming SOAP Message Inteceptor" , e ) ; } }
@ Override public void onActivityTestGPRSRequest ( ActivityTestGPRSRequest ind ) { this . logger . debug ( "ActivityTestGPRSRequest" ) ; TestEvent te = TestEvent . createReceivedEvent ( EventType . ActivityTestGPRSRequest , ind , sequence ++ ) ; this . observerdEvents . add ( te ) ; }
public void test() { for ( int i = 0 ; i < 3 ; i ++ ) { DataPacket packet = new DataPacketBuilder ( ) . contents ( "Example contents from client." ) . attr ( "Client attr 1" , "Client attr 1 value" ) . attr ( "Client attr 2" , "Client attr 2 value" ) . build ( ) ; transaction . send ( packet ) ; long written = ( ( Peer ) transaction . getCommunicant ( ) ) . getCommunicationsSession ( ) . getBytesWritten ( ) ; logger . info ( "{} bytes have been written." , written ) ; Thread . sleep ( 50 ) ; } }
public List < UUID > testEntityCollections ( UUID applicationId , UUID entityId , String entityType , String collectionName , int expectedCount ) throws Exception { logger . info ( "----------------------------------------------------" ) ; logger . info ( "Checking collection " + collectionName + " to " + entityId . toString ( ) ) ; EntityManager em = setup . getEmf ( ) . getEntityManager ( applicationId ) ; Entity en = em . get ( new SimpleEntityRef ( entityType , entityId ) ) ; int i = 0 ; Results entities = em . getCollection ( en , collectionName , null , 100 , Level . IDS , false ) ; code_block = ForStatement ; logger . info ( "----------------------------------------------------" ) ; assertEquals ( "Expected " + expectedCount + " connections" , expectedCount , entities . getIds ( ) != null ? entities . getIds ( ) . size ( ) : 0 ) ; return entities . getIds ( ) ; }
public void test() { for ( UUID id : entities . getIds ( ) ) { logger . info ( ( i ++ ) + " " + id . toString ( ) ) ; } }
public void test() { if ( retryTimer . isPresent ( ) ) { LOG . warn ( "Snapshot restore timed out, failed to restore snapshot for %s, snapshot %s" , queryId . getId ( ) , lastTriedId . toString ( ) ) ; retryTimer = Optional . empty ( ) ; } else { return ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( String . format ( "Cannot retrieve document '%s', probably deleted in the meanwhile" , docId ) ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( String . format ( "Cannot retrieve document '%s', probably deleted in the meanwhile" , docId ) ) ; } }
public static void deleteKeycloak ( String namespace ) { LOGGER . info ( "Teardown Keycloak in namespace: {}" , namespace ) ; Exec . exec ( true , "/bin/bash" , PATH_TO_KEYCLOAK_TEARDOWN_SCRIPT , namespace ) ; }
public void test() { try { final Connection conn = citrixResourceBase . getConnection ( ) ; final Network nw = citrixResourceBase . findOrCreateTunnelNetwork ( conn , command . getBridgeName ( ) ) ; citrixResourceBase . cleanUpTmpDomVif ( conn , nw ) ; citrixResourceBase . destroyTunnelNetwork ( conn , nw , command . getHostId ( ) ) ; s_logger . debug ( "OVS Bridge destroyed" ) ; return new Answer ( command , true , null ) ; } catch ( final Exception e ) { s_logger . warn ( "caught execption when destroying ovs bridge" , e ) ; return new Answer ( command , false , e . getMessage ( ) ) ; } }
public void test() { try { final Connection conn = citrixResourceBase . getConnection ( ) ; final Network nw = citrixResourceBase . findOrCreateTunnelNetwork ( conn , command . getBridgeName ( ) ) ; citrixResourceBase . cleanUpTmpDomVif ( conn , nw ) ; citrixResourceBase . destroyTunnelNetwork ( conn , nw , command . getHostId ( ) ) ; s_logger . debug ( "OVS Bridge destroyed" ) ; return new Answer ( command , true , null ) ; } catch ( final Exception e ) { s_logger . warn ( "caught execption when destroying ovs bridge" , e ) ; return new Answer ( command , false , e . getMessage ( ) ) ; } }
public void test() { if ( taskTracker == null ) { log . warn ( "[TaskTrackerActor] receive ServerStopInstanceReq({}) but system can't find TaskTracker." , req ) ; return ; } }
public void test() { if ( optionalCtx . isPresent ( ) && optionalCtx . get ( ) instanceof ZooKeeper ) { this . ledgersRootPath = conf . getZkLedgersRootPath ( ) ; log . info ( "Initialize zookeeper metadata driver with external zookeeper client : ledgersRootPath = {}." , ledgersRootPath ) ; this . zk = ( ZooKeeper ) ( optionalCtx . get ( ) ) ; this . ownZKHandle = false ; } else { final String metadataServiceUriStr ; code_block = TryStatement ;  URI metadataServiceUri = URI . create ( metadataServiceUriStr ) ; this . ledgersRootPath = metadataServiceUri . getPath ( ) ; final String bookieRegistrationPath = ledgersRootPath + "/" + AVAILABLE_NODE ; final String bookieReadonlyRegistrationPath = bookieRegistrationPath + "/" + READONLY ; final String zkServers ; code_block = TryStatement ;  log . info ( "Initialize zookeeper metadata driver at metadata service uri {} :" + " zkServers = {}, ledgersRootPath = {}." , metadataServiceUriStr , zkServers , ledgersRootPath ) ; code_block = TryStatement ;  this . ownZKHandle = true ; } }
public void test() { if ( optionalCtx . isPresent ( ) && optionalCtx . get ( ) instanceof ZooKeeper ) { this . ledgersRootPath = conf . getZkLedgersRootPath ( ) ; log . info ( "Initialize zookeeper metadata driver with external zookeeper client : ledgersRootPath = {}." , ledgersRootPath ) ; this . zk = ( ZooKeeper ) ( optionalCtx . get ( ) ) ; this . ownZKHandle = false ; } else { final String metadataServiceUriStr ; code_block = TryStatement ;  URI metadataServiceUri = URI . create ( metadataServiceUriStr ) ; this . ledgersRootPath = metadataServiceUri . getPath ( ) ; final String bookieRegistrationPath = ledgersRootPath + "/" + AVAILABLE_NODE ; final String bookieReadonlyRegistrationPath = bookieRegistrationPath + "/" + READONLY ; final String zkServers ; code_block = TryStatement ;  log . info ( "Initialize zookeeper metadata driver at metadata service uri {} :" + " zkServers = {}, ledgersRootPath = {}." , metadataServiceUriStr , zkServers , ledgersRootPath ) ; code_block = TryStatement ;  this . ownZKHandle = true ; } }
private void jsonWriteTo ( OutputStream out ) throws IOException { log . debug ( "Starting Producing Stream Data In Json Format Thread ..." ) ; code_block = IfStatement ; log . debug ( "Ending Producing Stream Data Thread ..." ) ; }
public SysImport merge ( SysImport detachedInstance ) { log . debug ( "merging SysImport instance" ) ; code_block = TryStatement ;  }
public void test() { try { SysImport result = ( SysImport ) sessionFactory . getCurrentSession ( ) . merge ( detachedInstance ) ; log . debug ( "merge successful" ) ; return result ; } catch ( RuntimeException re ) { log . error ( "merge failed" , re ) ; throw re ; } }
public void test() { try { SysImport result = ( SysImport ) sessionFactory . getCurrentSession ( ) . merge ( detachedInstance ) ; log . debug ( "merge successful" ) ; return result ; } catch ( RuntimeException re ) { log . error ( "merge failed" , re ) ; throw re ; } }
public void test() { try { generateJobs ( new Date ( ) ) ; } catch ( Exception e ) { log . info ( "Exception caught at fault barrier while generating jobs." , e ) ; } }
public void test() { if ( cmdLine . hasOption ( "o" ) ) { String outputPath = cmdLine . getOptionValue ( "o" ) ; LOGGER . info ( outputPath ) ; outputFile = new File ( outputPath ) ; threat_instrumentor . instrument ( verdictDataModel , cmdLine ) ; VdmTranslator . marshalToXml ( verdictDataModel , outputFile ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( String . format ( "BroadcastServiceHandler: caught exception %s, probably because session was closed with pending writes" , cause ) ) ; } }
public void test() { if ( isCommit ) { String [ ] listHeuristicCommittedTransactions = jmxServer . listHeuristicCommittedTransactions ( ) ; Assert . assertEquals ( 1 , listHeuristicCommittedTransactions . length ) ; instanceLog . debug ( listHeuristicCommittedTransactions [ 0 ] ) ; } else { String [ ] listHeuristicRolledBackTransactions = jmxServer . listHeuristicRolledBackTransactions ( ) ; Assert . assertEquals ( 1 , listHeuristicRolledBackTransactions . length ) ; instanceLog . debug ( listHeuristicRolledBackTransactions [ 0 ] ) ; } }
public void test() { if ( isCommit ) { String [ ] listHeuristicCommittedTransactions = jmxServer . listHeuristicCommittedTransactions ( ) ; Assert . assertEquals ( 1 , listHeuristicCommittedTransactions . length ) ; instanceLog . debug ( listHeuristicCommittedTransactions [ 0 ] ) ; } else { String [ ] listHeuristicRolledBackTransactions = jmxServer . listHeuristicRolledBackTransactions ( ) ; Assert . assertEquals ( 1 , listHeuristicRolledBackTransactions . length ) ; instanceLog . debug ( listHeuristicRolledBackTransactions [ 0 ] ) ; } }
public void test() { try { workspace . delete ( url ) ; } catch ( Exception e ) { logger . warn ( "Could not delete {} from workspace: {}" , url , e . getMessage ( ) ) ; } }
private void dumpAvailableConsumers ( ) { Map < String , KnownRepositoryContentConsumer > availableConsumers = getConsumers ( ) ; LOGGER . info ( ".\\ Available Consumer List \\.______________________________" ) ; code_block = ForStatement ; }
public void test() { for ( Map . Entry < String , KnownRepositoryContentConsumer > entry : availableConsumers . entrySet ( ) ) { String consumerHint = entry . getKey ( ) ; RepositoryContentConsumer consumer = entry . getValue ( ) ; LOGGER . info ( "  {} : {} ({})" , consumerHint , consumer . getDescription ( ) , consumer . getClass ( ) . getName ( ) ) ; } }
@ Override public void writeAndFlush ( ByteBuf output ) throws IOException { checkConnected ( output ) ; LOG . trace ( "Attempted write and flush of buffer: {}" , output ) ; channel . writeAndFlush ( output , channel . voidPromise ( ) ) ; }
public void test() { try { code_block = IfStatement ; } catch ( UnsupportedEncodingException e ) { LOG . error ( "Error while converting the received string payload to byte[]." , e ) ; } }
public void test() { try { producer . send ( new ProducerRecord < > ( topic , partitionNo , key , payloadToSend ) ) ; } catch ( Exception e ) { LOG . error ( String . format ( "Failed to publish the message to [topic] %s. Error: %s. Sequence Number " + ": %d" , topic , e . getMessage ( ) , kafkaSinkState . lastSentSequenceNo . get ( ) - 1 ) , e ) ; } }
public void test() { if ( attachment . getCheckStatus ( ) != CheckStatus . ACCEPTED ) { LOGGER . info ( "Attachment with content id " + attachment . getAttachmentContentId ( ) + " is of correct type to be displayed as clearing report, but is not yet accepted. So not dispaying it." ) ; return SKIP_BODY ; } }
public void test() { if ( bundlesByLocation . get ( loc ) . getState ( ) == Bundle . ACTIVE ) { bundles . add ( bundleToBundleInfo ( bundlesByLocation . get ( loc ) ) ) ; } else { LOGGER . debug ( "Unable to find bundle {} of app {} in system." , loc , name ) ; } }
public void test() { if ( Boolean . TRUE . equals ( result ) ) { LOG . info ( "Node {} has been removed" , node . getNodeId ( ) . getValue ( ) ) ; } else { LOG . warn ( "Failed to remove node {}" , node . getNodeId ( ) . getValue ( ) ) ; } }
public void test() { if ( Boolean . TRUE . equals ( result ) ) { LOG . info ( "Node {} has been removed" , node . getNodeId ( ) . getValue ( ) ) ; } else { LOG . warn ( "Failed to remove node {}" , node . getNodeId ( ) . getValue ( ) ) ; } }
public void test() { if ( getName ( ) . equals ( formName ) ) { log . debug ( "Can reuse existing instance (BeanValidatorForm)" ) ; return true ; } else { return false ; } }
public void test() { try { Class formClass = form . getClass ( ) ; code_block = IfStatement ; Class configClass = ClassUtils . getApplicationClass ( this . getType ( ) ) ; code_block = IfStatement ; } catch ( Exception e ) { log . debug ( "Error testing existing instance for reusability; just create a new instance" , e ) ; } }
public void test() { try { GenSolvablePolynomial < SolvableResidue < C > > s = pt . nextSolvablePolynomial ( ) ; p = new ResidueSolvablePolynomial < C > ( this , s ) ; } catch ( IOException e ) { logger . error ( e . toString ( ) + " parse " + this ) ; p = ZERO ; } }
public void removeTenant ( String tenantId ) { tenantIds . remove ( tenantId ) ; log . info ( "[TRACKER] tenantId " + tenantId + " removed." ) ; }
public void test() { try { preJettyLifecycle . stop ( ) ; lifecycle . stop ( ) ; } catch ( Throwable t ) { LOG . error ( "While shutting down" , t ) ; } }
private void write ( final Collection < Long > items ) { code_block = IfStatement ; var value = state . getValue ( key ) . orElse ( "nothing" ) ; LOGGER . trace ( "'{}' wrote '{}'." , key , value ) ; }
public void test() { try ( Timer . Context timer = parseTime . time ( ) ) { message = new MappedMessage ( parser . parse ( cef . trim ( ) ) , useFullNames ) ; } catch ( Exception e ) { LOG . error ( "Error while parsing CEF message: {}" , cef , e ) ; return null ; } }
public void test() { try { EuropeanaGeneratedIdsMap europeanaGeneratedIdsMap = europeanIdCreator . constructEuropeanaId ( record . getXmlRecord ( ) , dataset . getDatasetId ( ) ) ; return new Record ( record . getEcloudId ( ) , transformer . transform ( record . getXmlRecord ( ) . getBytes ( StandardCharsets . UTF_8 ) , europeanaGeneratedIdsMap ) . toString ( ) ) ; } catch ( TransformationException e ) { LOGGER . info ( "Record from list failed transformation" , e ) ; return new Record ( record . getEcloudId ( ) , e . getMessage ( ) ) ; } catch ( EuropeanaIdException e ) { LOGGER . info ( CommonStringValues . EUROPEANA_ID_CREATOR_INITIALIZATION_FAILED , e ) ; return new Record ( record . getEcloudId ( ) , e . getMessage ( ) ) ; } }
public void test() { try { EuropeanaGeneratedIdsMap europeanaGeneratedIdsMap = europeanIdCreator . constructEuropeanaId ( record . getXmlRecord ( ) , dataset . getDatasetId ( ) ) ; return new Record ( record . getEcloudId ( ) , transformer . transform ( record . getXmlRecord ( ) . getBytes ( StandardCharsets . UTF_8 ) , europeanaGeneratedIdsMap ) . toString ( ) ) ; } catch ( TransformationException e ) { LOGGER . info ( "Record from list failed transformation" , e ) ; return new Record ( record . getEcloudId ( ) , e . getMessage ( ) ) ; } catch ( EuropeanaIdException e ) { LOGGER . info ( CommonStringValues . EUROPEANA_ID_CREATOR_INITIALIZATION_FAILED , e ) ; return new Record ( record . getEcloudId ( ) , e . getMessage ( ) ) ; } }
public void test() { try { application . getPrivileges ( ) . remove ( model . getObject ( ) ) ; ApplicationRestClient . update ( application ) ; SyncopeConsoleSession . get ( ) . success ( getString ( Constants . OPERATION_SUCCEEDED ) ) ; customActionOnFinishCallback ( target ) ; } catch ( SyncopeClientException e ) { LOG . error ( "While deleting {}" , model . getObject ( ) . getKey ( ) , e ) ; SyncopeConsoleSession . get ( ) . onException ( e ) ; } }
public void test() { try { final RaftClientReply reply = ClientProtoUtils . toRaftClientReply ( proto ) ; LOG . trace ( "{}: receive {}" , getName ( ) , reply ) ; final NotLeaderException nle = reply . getNotLeaderException ( ) ; code_block = IfStatement ; final LeaderNotReadyException lnre = reply . getLeaderNotReadyException ( ) ; code_block = IfStatement ; handleReplyFuture ( callId , f -> f . complete ( reply ) ) ; } catch ( Exception e ) { handleReplyFuture ( callId , f -> f . completeExceptionally ( e ) ) ; } }
@ Override public List < RecommendedItem > recommend ( long userID , int howMany , IDRescorer rescorer ) throws TasteException { Preconditions . checkArgument ( howMany >= 1 , "howMany must be at least 1" ) ; log . debug ( "Recommending items for user ID '{}'" , userID ) ; long [ ] theNeighborhood = neighborhood . getUserNeighborhood ( userID ) ; code_block = IfStatement ; FastIDSet allItemIDs = getAllOtherItems ( theNeighborhood , userID ) ; TopItems . Estimator < Long > estimator = new Estimator ( userID , theNeighborhood ) ; List < RecommendedItem > topItems = TopItems . getTopItems ( howMany , allItemIDs . iterator ( ) , rescorer , estimator ) ; log . debug ( "Recommendations are: {}" , topItems ) ; return topItems ; }
@ Override public List < RecommendedItem > recommend ( long userID , int howMany , IDRescorer rescorer ) throws TasteException { Preconditions . checkArgument ( howMany >= 1 , "howMany must be at least 1" ) ; log . debug ( "Recommending items for user ID '{}'" , userID ) ; long [ ] theNeighborhood = neighborhood . getUserNeighborhood ( userID ) ; code_block = IfStatement ; FastIDSet allItemIDs = getAllOtherItems ( theNeighborhood , userID ) ; TopItems . Estimator < Long > estimator = new Estimator ( userID , theNeighborhood ) ; List < RecommendedItem > topItems = TopItems . getTopItems ( howMany , allItemIDs . iterator ( ) , rescorer , estimator ) ; log . debug ( "Recommendations are: {}" , topItems ) ; return topItems ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Missing resource in payload. [resource=({}), issuer=({}), " + "messageId=({})]" , affectedResource , issuerConnector , messageId ) ; } }
public void test() { if ( leader == null ) { LOGGER . warn ( format ( "No leader for partition %s/%s found!" , metadata . topic ( ) , part . partitionId ( ) ) ) ; } else { HostAndPort leaderHost = HostAndPort . fromParts ( leader . host ( ) , leader . port ( ) ) ; SimpleConsumer leaderConsumer = consumerManager . getConsumer ( leaderHost ) ; long offset = findAllOffsets ( leaderConsumer , metadata . topic ( ) , part . partitionId ( ) ) [ 0 ] ; builder . put ( metadata . topic ( ) , offset ) ; } }
@ Override public synchronized void onLeaderElection ( ) { LOG . info ( "This node elected Active Cluster Coordinator" ) ; bulletinRepository . addBulletin ( BulletinFactory . createBulletin ( "Cluster Coordinator" , Severity . INFO . name ( ) , participantId + " has been elected the Cluster Coordinator" ) ) ; FlowController . this . heartbeatMonitor . purgeHeartbeats ( ) ; }
public void test() { try { String user = future . get ( 30 , java . util . concurrent . TimeUnit . SECONDS ) ; LOG . debug ( "[testGetDriveUser] Testing User properties: {}." , user ) ; } catch ( ExecutionException ee ) { vr . setStatus ( Result . ERROR ) . setMessage ( messages . getMessage ( "error.testConnection.failure" , ee . getMessage ( ) ) ) ; LOG . error ( "[testGetDriveUser] Execution error: {}." , ee . getMessage ( ) ) ; } catch ( TimeoutException | InterruptedException e ) { vr . setStatus ( Result . ERROR ) . setMessage ( messages . getMessage ( "error.testConnection.timeout" ) ) ; LOG . error ( "[testGetDriveUser] Operation Timeout." ) ; } }
public void test() { try { String user = future . get ( 30 , java . util . concurrent . TimeUnit . SECONDS ) ; LOG . debug ( "[testGetDriveUser] Testing User properties: {}." , user ) ; } catch ( ExecutionException ee ) { vr . setStatus ( Result . ERROR ) . setMessage ( messages . getMessage ( "error.testConnection.failure" , ee . getMessage ( ) ) ) ; LOG . error ( "[testGetDriveUser] Execution error: {}." , ee . getMessage ( ) ) ; } catch ( TimeoutException | InterruptedException e ) { vr . setStatus ( Result . ERROR ) . setMessage ( messages . getMessage ( "error.testConnection.timeout" ) ) ; LOG . error ( "[testGetDriveUser] Operation Timeout." ) ; } }
public void test() { try { String user = future . get ( 30 , java . util . concurrent . TimeUnit . SECONDS ) ; LOG . debug ( "[testGetDriveUser] Testing User properties: {}." , user ) ; } catch ( ExecutionException ee ) { vr . setStatus ( Result . ERROR ) . setMessage ( messages . getMessage ( "error.testConnection.failure" , ee . getMessage ( ) ) ) ; LOG . error ( "[testGetDriveUser] Execution error: {}." , ee . getMessage ( ) ) ; } catch ( TimeoutException | InterruptedException e ) { vr . setStatus ( Result . ERROR ) . setMessage ( messages . getMessage ( "error.testConnection.timeout" ) ) ; LOG . error ( "[testGetDriveUser] Operation Timeout." ) ; } }
public void test() { try { LoginContext context = new SecondaryLoginContext ( ) ; ( ( WaspSession ) Session . get ( ) ) . login ( context ) ; continueToOriginalDestination ( ) ; setResponsePage ( Application . get ( ) . getHomePage ( ) ) ; return true ; } catch ( LoginException e ) { log . error ( e . getMessage ( ) , e ) ; } }
@ Override public void showTree ( String prefix ) { LOGGER . debug ( prefix + "Operator: MProjectionOperator" ) ; LOGGER . debug ( prefix + "Argument 0: DocIdSet - " ) ; _docIdSetPlanNode . showTree ( prefix + "    " ) ; int i = 0 ; code_block = ForStatement ; }
@ Override public void showTree ( String prefix ) { LOGGER . debug ( prefix + "Operator: MProjectionOperator" ) ; LOGGER . debug ( prefix + "Argument 0: DocIdSet - " ) ; _docIdSetPlanNode . showTree ( prefix + "    " ) ; int i = 0 ; code_block = ForStatement ; }
public void test() { if ( controller . getTable ( ) != null ) { code_block = IfStatement ; } else { logger . error ( "TableManagerImpl.userQuitTournamentSubTables table == null - userId " + userId ) ; } }
@ Test public void testI01RexCompat ( ) { logger . info ( "{}" , Thread . currentThread ( ) . getStackTrace ( ) [ 1 ] . getMethodName ( ) ) ; setup ( VehicleType . ELECTRIC_REX . toString ( ) , false ) ; String content = FileReader . readFileInString ( "src/test/resources/api/vehicle/vehicle-ccm.json" ) ; VehicleAttributesContainer vac = Converter . getGson ( ) . fromJson ( content , VehicleAttributesContainer . class ) ; assertTrue ( testVehicle ( Converter . transformLegacyStatus ( vac ) , STATUS_ELECTRIC + DOORS + RANGE_HYBRID + SERVICE_AVAILABLE + CHECK_AVAILABLE + POSITION , Optional . empty ( ) ) ) ; }
public void test() { try { JsonNode jsonNode = MAPPER . readValue ( field , JsonNode . class ) ; metadata = MAPPER . convertValue ( jsonNode , Map . class ) ; } catch ( Exception ex ) { LOGGER . warn ( "failed in parseMap: " + ex . getMessage ( ) ) ; } }
public void test() { try { new SubServer ( customPortServerManager , server ) . bind ( customPortServerManager . getPort ( ) ) ; } catch ( Throwable e ) { LOG . error ( e ) ; } }
public void test() { try { code_block = ForStatement ; } catch ( DocumentReferenceException de ) { logger . info ( String . format ( "Failed to soft-delete %s (transition from %s to %s): item is referenced, and will be deprecated instead" , localItemCsid , localItemWorkflowState , sasWorkflowState ) ) ; localItemDocModel . refresh ( ) ; AuthorityServiceUtils . setAuthorityItemDeprecated ( ctx , authorityResource , localParentCsid , localItemCsid , localItemDocModel ) ; } }
public void test() { try { bpmWidgetInfo = this . getBpmWidgetInfoDAO ( ) . loadBpmWidgetInfo ( id ) ; } catch ( Throwable t ) { _logger . error ( "Error loading bpmWidgetInfo with id '{}'" , id , t ) ; throw new ApsSystemException ( "Error loading bpmWidgetInfo with id: " + id , t ) ; } }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "DOCID: " + d . getProperty ( "docno" ) ) ; } }
public void test() { try { outputObject . put ( JsonKeys . updateType . name ( ) , "PublishPresetUpdate" ) ; outputObject . put ( JsonKeys . fileUrl . name ( ) , contextParameters . getParameterValue ( ContextParameter . JSON_PUBLISH_RELATIVE_DIR ) + jsonFileName ) ; outputObject . put ( JsonKeys . worksheetId . name ( ) , wsht . getId ( ) ) ; pw . println ( outputObject . toString ( 4 ) ) ; } catch ( JSONException e ) { logger . error ( "Error occured while generating JSON!" ) ; } }
@ Override public XAResource [ ] getXAResources ( ActivationSpec [ ] specs ) { logger . debug ( "Returning XAResource [null]..." ) ; return null ; }
public void test() { try { Object value = method . invoke ( obj , args ) ; code_block = IfStatement ; return ( T ) value ; } catch ( Throwable t ) { logger . warn ( "error calling {}.{}()" , method . getDeclaringClass ( ) . getName ( ) , method . getName ( ) , t ) ; return defaultValue ; } }
public boolean before ( Locale locale , String filename ) throws Exception { LOG . info ( "default locale: " + Locale . getDefault ( ) ) ; Locale . setDefault ( locale ) ; LOG . info ( "Set default locale to: " + locale ) ; LOG . info ( "Messages file: " + filename ) ; InputStream is = getClass ( ) . getClassLoader ( ) . getResourceAsStream ( filename ) ; code_block = IfStatement ; properties . load ( is ) ; return getExpectedNumberOfMethods ( ) == properties . size ( ) ; }
public boolean before ( Locale locale , String filename ) throws Exception { LOG . info ( "default locale: " + Locale . getDefault ( ) ) ; Locale . setDefault ( locale ) ; LOG . info ( "Set default locale to: " + locale ) ; LOG . info ( "Messages file: " + filename ) ; InputStream is = getClass ( ) . getClassLoader ( ) . getResourceAsStream ( filename ) ; code_block = IfStatement ; properties . load ( is ) ; return getExpectedNumberOfMethods ( ) == properties . size ( ) ; }
@ Transactional ( rollbackFor = ArrowheadException . class ) public ChoreographerWorklog createWorklog ( final String message , final String exception ) { logger . debug ( "createWorklog started..." ) ; code_block = TryStatement ;  }
public void test() { try { code_block = IfStatement ; return choreographerWorklogRepository . saveAndFlush ( new ChoreographerWorklog ( message , exception ) ) ; } catch ( InvalidParameterException ex ) { throw ex ; } catch ( final Exception ex ) { logger . debug ( ex . getMessage ( ) , ex ) ; throw new ArrowheadException ( CoreCommonConstants . DATABASE_OPERATION_EXCEPTION_MSG ) ; } }
public void test() { if ( _logger . isDebugEnabled ( ) ) { _logger . debug ( "getNextBatch returns with a buffer of " + entries . length + " entries." ) ; } }
public boolean activityTabIsSelected ( ) { log . info ( "Query is Activity tab displayed" ) ; return getDriver ( ) . findElements ( By . cssSelector ( "activity.is-active" ) ) . size ( ) > 0 ; }
public void pauseQueue ( String queueName ) throws TimeoutException { log . info ( "Pausing queue {}" , queueName ) ; doOperation ( "queue." + queueName , "pause" ) ; }
@ Override public void output ( LocalDocument document ) { logger . info ( "Accepting document: " + document . getID ( ) ) ; }
public static void changeCharsetToUtf ( JdbcConnection jdbcCon ) throws DatabaseException , SQLException { Statement stmt = jdbcCon . createStatement ( ) ; String dbName = jdbcCon . getCatalog ( ) ; String sql = String . format ( "ALTER DATABASE `%s` CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci;" , dbName ) ; int result = stmt . executeUpdate ( sql ) ; LOGGER . info ( "ALTER charset execute result: {}" , result ) ; }
public void test() { try { URI serverUri = new URI ( getPreferences ( ) . getString ( PreferenceConstants . VNSERVER_URI ) ) ; IProxyService proxyService = getProxyService ( ) ; IProxyData [ ] proxyDataForHost = proxyService . select ( serverUri ) ; code_block = IfStatement ; } catch ( Exception t ) { LOG . error ( "Error while setting proxy." , t ) ; } }
public void test() { try { int returnValue = CommerceAddressServiceUtil . getShippingCommerceAddressesCount ( companyId , className , classPK , keywords ) ; return returnValue ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( ! transaction . isOpen ( ) ) { LOG . error ( "Transaction was already closed!" , new Throwable ( ) ) ; } else-if ( ownTransaction ) { code_block = IfStatement ; } }
public void test() { if ( requireCommit ) { LOG . error ( "Transaction was not closed, rolling back. Please add an explicit rollback so that we know this " + "was not a missing success()" ) ; } }
public void test() { if ( ReservedPropertyNames . contains ( property ) ) { LOGGER . warn ( "Element definition contains a reserved property name {}. " + "This may prevent some analytics from being used on this graph." , property ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( testName + ": Created an AccountRole instance for account with knownResourceId=" + knownResourceId ) ; } }
public void test() { try { code_block = ForStatement ; } catch ( ConfigurationError ex ) { LOG . error ( "Error reading metadata properties" , ex ) ; } }
public void test() { try { encoder . dispose ( session ) ; } catch ( Throwable t ) { LOGGER . warn ( "Failed to dispose: " + encoder . getClass ( ) . getName ( ) + " (" + encoder + ')' ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { LOGGER . error ( "ActivityMetaDataDao - getQuestionnaireFrequencyDetailsForOneTime() :: ERROR" , e ) ; } }
@ Test public void testTraceWithNArguments ( ) { buf . setLength ( 0 ) ; final VitamUILogger logger = VitamUILoggerFactory . getInstance ( VitamUITraceLoggerTest . class ) ; final String message = "message" ; final String format = message + " {} {} {}" ; final Integer object1 = 1 ; final Integer object2 = 2 ; final Integer object3 = 3 ; logger . trace ( format , object1 , object2 , object3 ) ; assertTrue ( "Log message should be written." , buf . length ( ) > 0 ) ; assertTrue ( "Log message should be written." , buf . lastIndexOf ( message ) > 0 ) ; assertTrue ( "Log message should be written." , buf . lastIndexOf ( message + " " + object1 . toString ( ) + " " + object2 . toString ( ) + " " + object3 . toString ( ) ) > 0 ) ; }
@ Override public void channelConnected ( ChannelHandlerContext ctx , ChannelStateEvent e ) throws Exception { LOG . debug ( "Channel connected {}" , e ) ; }
public void test() { try { ( ( XulWindow ) this . getXulDomContainer ( ) . getDocumentRoot ( ) . getRootElement ( ) ) . cut ( ) ; paste . setDisabled ( false ) ; } catch ( XulException e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( bucketName != null ) { _log . info ( "servlet context provided config bucketName=" + bucketName ) ; setBucketName ( bucketName ) ; } else { _log . info ( "servlet context missing bucketName, using " + getBucketName ( ) ) ; } }
public void test() { if ( bucketName != null ) { _log . info ( "servlet context provided config bucketName=" + bucketName ) ; setBucketName ( bucketName ) ; } else { _log . info ( "servlet context missing bucketName, using " + getBucketName ( ) ) ; } }
public void test() { if ( ! warnings . isEmpty ( ) ) { LOG . debug ( "appending [" + warnings . size ( ) + "] warning(s)" ) ; } }
public void test() { try { groupsNames = super . searchId ( filters ) ; } catch ( Throwable t ) { logger . error ( "error in search groups" , t ) ; throw new RuntimeException ( "error in search groups" , t ) ; } }
void authenticationError ( ChannelHandlerContext ctx , int errorCode ) { LOG . error ( "Error processing auth message, erroring connection {}" , errorCode ) ; ctx . fireExceptionCaught ( new AuthenticationException ( "Auth failed with error " + errorCode ) ) ; }
public void test() { try ( HistogramMetrics . Timer ignored = MESH_ANALYSIS_METRICS . createTimer ( ) ) { code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; doDispatch ( data ) ; } catch ( Exception e ) { MESH_ERROR_METRICS . inc ( ) ; log . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CalendarBookingServiceUtil . class , "getCalendarBookingsRSS" , _getCalendarBookingsRSSParameterTypes14 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , calendarId , startTime , endTime , max , type , version , displayStyle , themeDisplay ) ; Object returnObj = null ; code_block = TryStatement ;  return ( String ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( ( entry == null ) || ( entry . getValidity ( ) == null ) || ( ( entry . getValidity ( ) . isValid ( this . grammarSource . getValidity ( ) ) ) <= 0 ) ) { this . logger . info ( "(Re)building the automaton from '" + this . grammarSource . getURI ( ) + "'" ) ; if ( this . grammarSource . getInputStream ( ) == null ) throw new ProcessingException ( "Source '" + this . grammarSource . getURI ( ) + "' not found" ) ; GrammarFactory factory = new GrammarFactory ( ) ; SourceUtil . toSAX ( this . manager , this . grammarSource , null , factory ) ; Grammar grammar = factory . getGrammar ( ) ; if ( grammar == null ) throw new ProcessingException ( "Error while reading the grammar from " + src ) ; ParserAutomatonBuilder builder = new ParserAutomatonBuilder ( grammar ) ; ParserAutomaton automaton = builder . getParserAutomaton ( ) ; setParserAutomaton ( builder . getParserAutomaton ( ) ) ; this . logger . info ( "Store automaton into store to '" + this . grammarSource . getURI ( ) + "'" ) ; store . store ( this . grammarSource . getURI ( ) , new ParserAutomatonEntry ( automaton , this . grammarSource . getValidity ( ) ) ) ; } else { this . logger . info ( "Getting automaton from store to '" + this . grammarSource . getURI ( ) + "'" ) ; setParserAutomaton ( entry . getParserAutomaton ( ) ) ; } }
public void test() { if ( ( entry == null ) || ( entry . getValidity ( ) == null ) || ( ( entry . getValidity ( ) . isValid ( this . grammarSource . getValidity ( ) ) ) <= 0 ) ) { this . logger . info ( "(Re)building the automaton from '" + this . grammarSource . getURI ( ) + "'" ) ; if ( this . grammarSource . getInputStream ( ) == null ) throw new ProcessingException ( "Source '" + this . grammarSource . getURI ( ) + "' not found" ) ; GrammarFactory factory = new GrammarFactory ( ) ; SourceUtil . toSAX ( this . manager , this . grammarSource , null , factory ) ; Grammar grammar = factory . getGrammar ( ) ; if ( grammar == null ) throw new ProcessingException ( "Error while reading the grammar from " + src ) ; ParserAutomatonBuilder builder = new ParserAutomatonBuilder ( grammar ) ; ParserAutomaton automaton = builder . getParserAutomaton ( ) ; setParserAutomaton ( builder . getParserAutomaton ( ) ) ; this . logger . info ( "Store automaton into store to '" + this . grammarSource . getURI ( ) + "'" ) ; store . store ( this . grammarSource . getURI ( ) , new ParserAutomatonEntry ( automaton , this . grammarSource . getValidity ( ) ) ) ; } else { this . logger . info ( "Getting automaton from store to '" + this . grammarSource . getURI ( ) + "'" ) ; setParserAutomaton ( entry . getParserAutomaton ( ) ) ; } }
public void test() { if ( ( entry == null ) || ( entry . getValidity ( ) == null ) || ( ( entry . getValidity ( ) . isValid ( this . grammarSource . getValidity ( ) ) ) <= 0 ) ) { this . logger . info ( "(Re)building the automaton from '" + this . grammarSource . getURI ( ) + "'" ) ; if ( this . grammarSource . getInputStream ( ) == null ) throw new ProcessingException ( "Source '" + this . grammarSource . getURI ( ) + "' not found" ) ; GrammarFactory factory = new GrammarFactory ( ) ; SourceUtil . toSAX ( this . manager , this . grammarSource , null , factory ) ; Grammar grammar = factory . getGrammar ( ) ; if ( grammar == null ) throw new ProcessingException ( "Error while reading the grammar from " + src ) ; ParserAutomatonBuilder builder = new ParserAutomatonBuilder ( grammar ) ; ParserAutomaton automaton = builder . getParserAutomaton ( ) ; setParserAutomaton ( builder . getParserAutomaton ( ) ) ; this . logger . info ( "Store automaton into store to '" + this . grammarSource . getURI ( ) + "'" ) ; store . store ( this . grammarSource . getURI ( ) , new ParserAutomatonEntry ( automaton , this . grammarSource . getValidity ( ) ) ) ; } else { this . logger . info ( "Getting automaton from store to '" + this . grammarSource . getURI ( ) + "'" ) ; setParserAutomaton ( entry . getParserAutomaton ( ) ) ; } }
public void test() { { Thread . currentThread ( ) . setName ( "restart-thread" ) ; U . sleep ( 15_000 ) ; ThreadLocalRandom tlr = ThreadLocalRandom . current ( ) ; int idx = tlr . nextInt ( 1 , GRIDS_COUNT ) ; log . info ( "Stopping node " + idx ) ; stopGrid ( idx ) ; IgniteEx ig0 = grid ( 0 ) ; ig0 . cluster ( ) . setBaselineTopology ( baselineNodes ( ig0 . cluster ( ) . forServers ( ) . nodes ( ) ) ) ; U . sleep ( 3_000 ) ; return null ; } }
private void sendMessage ( final DistributionAutomationRequestMessage requestMessage ) { LOGGER . info ( "Sending message to the da requests queue" ) ; this . jmsTemplate . send ( new MessageCreator ( ) code_block = "" ; ) ; }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Change the safeguard implementation status to: " + ImplementationStatus . PARTIALLY ) ; } }
public void test() { try { listener . gotOAuthAccessToken ( token ) ; } catch ( Exception e ) { logger . warn ( "Exception at getOAuthRequestTokenAsync" , e ) ; } }
@ Override public boolean updateIndexMapping ( String indexName , final Map < String , Object > mapping ) throws IOException { indexName = formatIndexName ( indexName ) ; PutMappingRequest putMappingRequest = new PutMappingRequest ( indexName ) ; Gson gson = new Gson ( ) ; putMappingRequest . source ( gson . toJson ( mapping ) , XContentType . JSON ) ; putMappingRequest . type ( "_doc" ) ; AcknowledgedResponse response = client . indices ( ) . putMapping ( putMappingRequest , RequestOptions . DEFAULT ) ; log . debug ( "put {} index mapping finished, isAcknowledged: {}" , indexName , response . isAcknowledged ( ) ) ; return response . isAcknowledged ( ) ; }
public void test() { try { apiCommands . metrics ( request ) ; return true ; } catch ( Exception e ) { LOGGER . error ( "Naming health check fail." , e ) ; } }
public void test() { try { int ver = Integer . parseInt ( part . substring ( 0 , slash ) ) ; code_block = IfStatement ; } catch ( NumberFormatException ex ) { log . debug ( "Failed to parse META-INF/versions entry" , ex ) ; } }
public void test() { try { return this . _storageAdaptor . deleteStoragePool ( this ) ; } catch ( Exception e ) { s_logger . debug ( "Failed to delete storage pool" , e ) ; } }
public void test() { try { Password decryptedPassword = new Password ( this . cryptoService . decryptAes ( ( ( String ) value ) . toCharArray ( ) ) ) ; decryptedPropertiesMap . put ( key , decryptedPassword ) ; } catch ( Exception e ) { logger . info ( "Password is not encrypted" ) ; decryptedPropertiesMap . put ( key , new Password ( ( String ) value ) ) ; } }
public void test() { try { InstancesResult result = WorkflowEngineFactory . getWorkflowEngine ( ) . getJobDetails ( context . getClusterName ( ) , context . getWorkflowId ( ) ) ; Date startTime = result . getInstances ( ) [ 0 ] . startTime ; Date endTime = result . getInstances ( ) [ 0 ] . endTime ; Date now = new Date ( ) ; code_block = IfStatement ; code_block = IfStatement ; context . setValue ( WorkflowExecutionArgs . WF_START_TIME , Long . toString ( startTime . getTime ( ) ) ) ; context . setValue ( WorkflowExecutionArgs . WF_END_TIME , Long . toString ( endTime . getTime ( ) ) ) ; } catch ( FalconException e ) { LOG . error ( "Unable to retrieve job details to " + context . getWorkflowId ( ) + " on cluster " + context . getClusterName ( ) , e ) ; } }
public void test() { try { emailStream . reset ( ) ; mailboxService . storeInSent ( udr , multipleTimesReadable ( emailStream , email . getMimeMessage ( ) . getCharset ( ) ) ) ; } catch ( CollectionNotFoundException e ) { logger . warn ( "Cannot store an email in the Sent folder, the collection was not found: {}" , e . getMessage ( ) ) ; } catch ( Throwable t ) { logger . error ( "Cannot store an email in the Sent folder" , t ) ; } }
public void test() { try { emailStream . reset ( ) ; mailboxService . storeInSent ( udr , multipleTimesReadable ( emailStream , email . getMimeMessage ( ) . getCharset ( ) ) ) ; } catch ( CollectionNotFoundException e ) { logger . warn ( "Cannot store an email in the Sent folder, the collection was not found: {}" , e . getMessage ( ) ) ; } catch ( Throwable t ) { logger . error ( "Cannot store an email in the Sent folder" , t ) ; } }
public void test() { try { fcall . sendResponse ( fb , msg , msgType , seqid ) ; return ; } catch ( Exception ex ) { LOGGER . error ( "Exception writing to internal frame buffer" , ex ) ; } }
public void test() { if ( value instanceof String ) { return ( ( String ) value ) . length ( ) ; } else-if ( value instanceof Boolean ) { return 4 ; } else-if ( value instanceof Number || value instanceof Date ) { return 8 ; } else-if ( value instanceof Collection ) { return calculateSizeOfCollection ( ( Collection < ? > ) value ) ; } else-if ( value instanceof Map ) { return calculateSizeOfMap ( ( Map < ? , ? > ) value ) ; } else { LOGGER . warn ( "unhandled object to calculate size for: " + value . getClass ( ) . getName ( ) + ", defaulting to 100" ) ; return 100 ; } }
public void test() { if ( hasTenant ) { LOGGER . warn ( "[capacityManagement] tenant content is over maxSize, tenant: {}, maxSize: {}, currentSize: {}" , tenant , maxSize , currentSize ) ; } else { LOGGER . warn ( "[capacityManagement] group content is over maxSize, group: {}, maxSize: {}, currentSize: {}" , group , maxSize , currentSize ) ; } }
public void test() { if ( hasTenant ) { LOGGER . warn ( "[capacityManagement] tenant content is over maxSize, tenant: {}, maxSize: {}, currentSize: {}" , tenant , maxSize , currentSize ) ; } else { LOGGER . warn ( "[capacityManagement] group content is over maxSize, group: {}, maxSize: {}, currentSize: {}" , group , maxSize , currentSize ) ; } }
public void test() { try { attempt ++ ; LOG . info ( "Trying to reconnect to {} - attempt {}" , getEndpoint ( ) . getConnectionString ( ) , attempt ) ; session = createSession ( ) ; reconnected = true ; } catch ( IOException e ) { LOG . warn ( "Failed to reconnect to {}" , getEndpoint ( ) . getConnectionString ( ) ) ; closeSession ( ) ; code_block = TryStatement ;  } }
public void test() { try { attempt ++ ; LOG . info ( "Trying to reconnect to {} - attempt {}" , getEndpoint ( ) . getConnectionString ( ) , attempt ) ; session = createSession ( ) ; reconnected = true ; } catch ( IOException e ) { LOG . warn ( "Failed to reconnect to {}" , getEndpoint ( ) . getConnectionString ( ) ) ; closeSession ( ) ; code_block = TryStatement ;  } }
public void test() { if ( reconnected ) { LOG . info ( "Reconnected to {}" , getEndpoint ( ) . getConnectionString ( ) ) ; } }
private void listen ( ) { SocketAddress sockAddr = ds . getLocalSocketAddress ( ) ; Connection . LOG . info ( "Start UDP listener on {}" , sockAddr ) ; byte [ ] data = new byte [ MAX_PACKAGE_LEN ] ; code_block = TryStatement ;  Connection . LOG . info ( "Stop UDP listener on {}" , sockAddr ) ; }
public void test() { if ( conn . isBlackListed ( dp . getAddress ( ) ) ) { Connection . LOG . info ( "Ignore UDP datagram package received from blacklisted {}" , senderAddr ) ; } else { Connection . LOG . info ( "Received UDP datagram package from {}" , senderAddr ) ; code_block = TryStatement ;  } }
public void test() { try { handler . onReceive ( conn , dp ) ; } catch ( Throwable e ) { Connection . LOG . warn ( "Exception processing UDP received from {}:" , senderAddr , e ) ; } }
private void listen ( ) { SocketAddress sockAddr = ds . getLocalSocketAddress ( ) ; Connection . LOG . info ( "Start UDP listener on {}" , sockAddr ) ; byte [ ] data = new byte [ MAX_PACKAGE_LEN ] ; code_block = TryStatement ;  Connection . LOG . info ( "Stop UDP listener on {}" , sockAddr ) ; }
public void test() { try { subscriberJedis = JedisConnectionObject . pool . getResource ( ) ; connectionSetup = true ; } catch ( Exception e ) { subscriberJedis = null ; connectionSetup = false ; logger . error ( "Fatal error! Could not get a resource from the pool." ) ; } }
public void test() { try { long groupId = ParamUtil . getLong ( actionRequest , "groupId" ) ; long classNameId = ParamUtil . getLong ( actionRequest , "classNameId" ) ; String className = _portal . getClassName ( classNameId ) ; long classPK = ParamUtil . getLong ( actionRequest , "classPK" ) ; InfoItemReference infoItemReference = new InfoItemReference ( className , classPK ) ; InfoItemObjectProvider < Object > infoItemObjectProvider = _infoItemServiceTracker . getFirstInfoItemService ( InfoItemObjectProvider . class , infoItemReference . getClassName ( ) ) ; InfoItemFieldValues infoItemFieldValues = InfoItemFieldValues . builder ( ) . infoItemReference ( infoItemReference ) . infoFieldValues ( _getInfoFieldValues ( actionRequest , className , infoItemObjectProvider . getInfoItem ( classPK ) ) ) . build ( ) ; ServiceContext serviceContext = ServiceContextFactory . getInstance ( actionRequest ) ; _translationEntryService . addOrUpdateTranslationEntry ( groupId , _getTargetLanguageId ( actionRequest ) , infoItemReference , infoItemFieldValues , serviceContext ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; SessionErrors . add ( actionRequest , exception . getClass ( ) , exception ) ; actionResponse . setRenderParameter ( "mvcRenderCommandName" , "/translation/translate" ) ; } }
public void test() { { ResponseBuilder response = ResponseBuilder . startTiming ( ) ; indexBeanParam . adjustIndexes ( maxAPIFetchRecords ) ; log . trace ( "Started getAllPhasingOnlyControls : \t indexBeanParam={}" , indexBeanParam ) ; AccountControlPhasingResponse dto = new AccountControlPhasingResponse ( ) ; dto . phasingOnlyControls = accountControlPhasingService . getAllStream ( indexBeanParam . getFirstIndex ( ) , indexBeanParam . getLastIndex ( ) ) . map ( item -> accountControlPhasingConverter . convert ( item ) ) . collect ( Collectors . toList ( ) ) ; log . trace ( "getAllPhasingOnlyControls result: {}" , dto ) ; return response . bind ( dto ) . build ( ) ; } }
public void test() { { ResponseBuilder response = ResponseBuilder . startTiming ( ) ; indexBeanParam . adjustIndexes ( maxAPIFetchRecords ) ; log . trace ( "Started getAllPhasingOnlyControls : \t indexBeanParam={}" , indexBeanParam ) ; AccountControlPhasingResponse dto = new AccountControlPhasingResponse ( ) ; dto . phasingOnlyControls = accountControlPhasingService . getAllStream ( indexBeanParam . getFirstIndex ( ) , indexBeanParam . getLastIndex ( ) ) . map ( item -> accountControlPhasingConverter . convert ( item ) ) . collect ( Collectors . toList ( ) ) ; log . trace ( "getAllPhasingOnlyControls result: {}" , dto ) ; return response . bind ( dto ) . build ( ) ; } }
@ Activate public void activate ( ) { ScriptStandaloneSetup . doSetup ( scriptServiceUtil , this ) ; logger . debug ( "Registered 'script' configuration parser" ) ; }
public void test() { try { return getField ( fname , true ) ; } catch ( AmbiguousFieldException e ) { LOG . debug ( "Ignored error: " , e ) ; return null ; } }
private void initWebKeys ( Conf conf ) { final String jwksUri = conf . getDynamic ( ) . getJwksUri ( ) ; code_block = IfStatement ; final JSONObject keys = JwtUtil . getJSONWebKeys ( jwksUri ) ; log . trace ( "Downloaded external keys from " + jwksUri + ", keys: " + keys ) ; final JSONWebKeySet keySet = JSONWebKeySet . fromJSONObject ( keys ) ; jwks = new WebKeysConfiguration ( ) ; jwks . setKeys ( keySet . getKeys ( ) ) ; }
public void test() { for ( String message : messages ) { restClient . post ( ) . uri ( restUri ) . submit ( message ) . thenAccept ( it -> assertThat ( it . status ( ) , is ( Http . Status . NO_CONTENT_204 ) ) ) . toCompletableFuture ( ) . get ( ) ; LOGGER . info ( "Posting message '" + message + "'" ) ; } }
public void test() { if ( reference != null && reference instanceof QuantityWithUnit ) { return ! ( ( QuantityWithUnit ) reference ) . isSetUnits ( ) ; } else { logger . warn ( "??" ) ; return true ; } }
@ Test public void playerAutoTerminationTest ( ) throws Exception { String id = uploadFile ( new File ( "test-files/sample.txt" ) ) ; log . debug ( "File uploaded" ) ; RepositoryHttpPlayer player = getRepository ( ) . findRepositoryItemById ( id ) . createRepositoryHttpPlayer ( ) ; player . setAutoTerminationTimeout ( 1000 ) ; RestTemplate template = getRestTemplate ( ) ; assertEquals ( HttpStatus . OK , template . getForEntity ( player . getURL ( ) , byte [ ] . class ) . getStatusCode ( ) ) ; log . debug ( "Request 1 Passed" ) ; Thread . sleep ( 300 ) ; assertEquals ( HttpStatus . OK , template . getForEntity ( player . getURL ( ) , byte [ ] . class ) . getStatusCode ( ) ) ; log . debug ( "Request 2 Passed" ) ; Thread . sleep ( 1500 ) ; assertEquals ( HttpStatus . NOT_FOUND , template . getForEntity ( player . getURL ( ) , byte [ ] . class ) . getStatusCode ( ) ) ; log . debug ( "Request 3 Passed" ) ; }
@ Test public void playerAutoTerminationTest ( ) throws Exception { String id = uploadFile ( new File ( "test-files/sample.txt" ) ) ; log . debug ( "File uploaded" ) ; RepositoryHttpPlayer player = getRepository ( ) . findRepositoryItemById ( id ) . createRepositoryHttpPlayer ( ) ; player . setAutoTerminationTimeout ( 1000 ) ; RestTemplate template = getRestTemplate ( ) ; assertEquals ( HttpStatus . OK , template . getForEntity ( player . getURL ( ) , byte [ ] . class ) . getStatusCode ( ) ) ; log . debug ( "Request 1 Passed" ) ; Thread . sleep ( 300 ) ; assertEquals ( HttpStatus . OK , template . getForEntity ( player . getURL ( ) , byte [ ] . class ) . getStatusCode ( ) ) ; log . debug ( "Request 2 Passed" ) ; Thread . sleep ( 1500 ) ; assertEquals ( HttpStatus . NOT_FOUND , template . getForEntity ( player . getURL ( ) , byte [ ] . class ) . getStatusCode ( ) ) ; log . debug ( "Request 3 Passed" ) ; }
@ Test public void playerAutoTerminationTest ( ) throws Exception { String id = uploadFile ( new File ( "test-files/sample.txt" ) ) ; log . debug ( "File uploaded" ) ; RepositoryHttpPlayer player = getRepository ( ) . findRepositoryItemById ( id ) . createRepositoryHttpPlayer ( ) ; player . setAutoTerminationTimeout ( 1000 ) ; RestTemplate template = getRestTemplate ( ) ; assertEquals ( HttpStatus . OK , template . getForEntity ( player . getURL ( ) , byte [ ] . class ) . getStatusCode ( ) ) ; log . debug ( "Request 1 Passed" ) ; Thread . sleep ( 300 ) ; assertEquals ( HttpStatus . OK , template . getForEntity ( player . getURL ( ) , byte [ ] . class ) . getStatusCode ( ) ) ; log . debug ( "Request 2 Passed" ) ; Thread . sleep ( 1500 ) ; assertEquals ( HttpStatus . NOT_FOUND , template . getForEntity ( player . getURL ( ) , byte [ ] . class ) . getStatusCode ( ) ) ; log . debug ( "Request 3 Passed" ) ; }
@ Test public void playerAutoTerminationTest ( ) throws Exception { String id = uploadFile ( new File ( "test-files/sample.txt" ) ) ; log . debug ( "File uploaded" ) ; RepositoryHttpPlayer player = getRepository ( ) . findRepositoryItemById ( id ) . createRepositoryHttpPlayer ( ) ; player . setAutoTerminationTimeout ( 1000 ) ; RestTemplate template = getRestTemplate ( ) ; assertEquals ( HttpStatus . OK , template . getForEntity ( player . getURL ( ) , byte [ ] . class ) . getStatusCode ( ) ) ; log . debug ( "Request 1 Passed" ) ; Thread . sleep ( 300 ) ; assertEquals ( HttpStatus . OK , template . getForEntity ( player . getURL ( ) , byte [ ] . class ) . getStatusCode ( ) ) ; log . debug ( "Request 2 Passed" ) ; Thread . sleep ( 1500 ) ; assertEquals ( HttpStatus . NOT_FOUND , template . getForEntity ( player . getURL ( ) , byte [ ] . class ) . getStatusCode ( ) ) ; log . debug ( "Request 3 Passed" ) ; }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( format ( "unscheduling run once job: " + LOG_SUBJECT_EXTERNAL_ID , subject , externalId ) ) ; } }
public void test() { if ( getFileInLocalRepo ( artifact . getFile ( ) ) != null ) { LOG . trace ( "Removing artifact {}" , artifact ) ; it . remove ( ) ; } }
public void test() { try ( InputStream is = Files . newInputStream ( xmlFilePath ) ; InputStreamReader in = new InputStreamReader ( is , StandardCharsets . UTF_8 ) ) { parser = inFactory . createXMLStreamReader ( in ) ; code_block = WhileStatement ; ret = true ; } catch ( Exception ex ) { ret = false ; logger . error ( "datenLesen" , ex ) ; } finally { code_block = IfStatement ; } }
public void deleteEverything ( SqlSession session ) { log . info ( "Database contents will be completely deleted" ) ; Collection < String > ops = new TreeSet < > ( db . getMyBatisConfiguration ( ) . getMappedStatementNames ( ) ) ; for ( String name : ops ) if ( name . startsWith ( "deletedb-" ) ) session . update ( name ) ; for ( String name : ops ) if ( name . startsWith ( "resetIndex-" ) ) session . update ( name ) ; log . info ( "Database contents was completely deleted" ) ; createRootGroup ( session ) ; }
public void deleteEverything ( SqlSession session ) { log . info ( "Database contents will be completely deleted" ) ; Collection < String > ops = new TreeSet < > ( db . getMyBatisConfiguration ( ) . getMappedStatementNames ( ) ) ; for ( String name : ops ) if ( name . startsWith ( "deletedb-" ) ) session . update ( name ) ; for ( String name : ops ) if ( name . startsWith ( "resetIndex-" ) ) session . update ( name ) ; log . info ( "Database contents was completely deleted" ) ; createRootGroup ( session ) ; }
public void test() { if ( future == null || future . isDone ( ) || future . isCancelled ( ) ) { log . info ( "Scheduling connection retries." ) ; future = executorService . scheduleAtFixedRate ( this , 1 , monitorInterval , TimeUnit . MILLISECONDS ) ; } else { log . info ( "Monitor already running." ) ; } }
public void test() { if ( future == null || future . isDone ( ) || future . isCancelled ( ) ) { log . info ( "Scheduling connection retries." ) ; future = executorService . scheduleAtFixedRate ( this , 1 , monitorInterval , TimeUnit . MILLISECONDS ) ; } else { log . info ( "Monitor already running." ) ; } }
public void test() { try { Class . forName ( IGNITE_JDBC_DRIVER_NAME ) ; } catch ( ClassNotFoundException e ) { logger . error ( "Can't find Ignite JDBC driver" , e ) ; connEx = e ; return ; } }
public void test() { try { logger . info ( "connect to " + getProperty ( IGNITE_JDBC_URL ) ) ; conn = DriverManager . getConnection ( getProperty ( IGNITE_JDBC_URL ) ) ; connEx = null ; logger . info ( "Successfully created JDBC connection" ) ; } catch ( Exception e ) { logger . error ( "Can't open connection: " , e ) ; connEx = e ; } }
public void test() { try { logger . info ( "connect to " + getProperty ( IGNITE_JDBC_URL ) ) ; conn = DriverManager . getConnection ( getProperty ( IGNITE_JDBC_URL ) ) ; connEx = null ; logger . info ( "Successfully created JDBC connection" ) ; } catch ( Exception e ) { logger . error ( "Can't open connection: " , e ) ; connEx = e ; } }
public void test() { try { logger . info ( "connect to " + getProperty ( IGNITE_JDBC_URL ) ) ; conn = DriverManager . getConnection ( getProperty ( IGNITE_JDBC_URL ) ) ; connEx = null ; logger . info ( "Successfully created JDBC connection" ) ; } catch ( Exception e ) { logger . error ( "Can't open connection: " , e ) ; connEx = e ; } }
public void test() { if ( bucketName != null ) { _log . info ( "servlet context provided bucketName=" + bucketName ) ; setBucketName ( bucketName ) ; } else { _log . info ( "servlet context missing bucketName, using " + getBucketName ( ) ) ; } }
public void test() { if ( bucketName != null ) { _log . info ( "servlet context provided bucketName=" + bucketName ) ; setBucketName ( bucketName ) ; } else { _log . info ( "servlet context missing bucketName, using " + getBucketName ( ) ) ; } }
public void test() { if ( StringUtils . isBlank ( rememberMeKey ) && ! development ) { LOG . debug ( "Generating a new ephemeral 'remember me' key in a secure way." ) ; rememberMeKey = generateRememberMeKey ( ) ; } else-if ( StringUtils . isBlank ( rememberMeKey ) && development ) { LOG . warn ( "Using a fixed 'remember me' key because we're in development mode, this is INSECURE." ) ; rememberMeKey = DEVELOPMENT_REMEMBER_ME_KEY ; } else { LOG . info ( "Using a fixed 'remember me' key from system properties, this is insecure." ) ; } }
public void test() { if ( StringUtils . isBlank ( rememberMeKey ) && ! development ) { LOG . debug ( "Generating a new ephemeral 'remember me' key in a secure way." ) ; rememberMeKey = generateRememberMeKey ( ) ; } else-if ( StringUtils . isBlank ( rememberMeKey ) && development ) { LOG . warn ( "Using a fixed 'remember me' key because we're in development mode, this is INSECURE." ) ; rememberMeKey = DEVELOPMENT_REMEMBER_ME_KEY ; } else { LOG . info ( "Using a fixed 'remember me' key from system properties, this is insecure." ) ; } }
public void test() { if ( StringUtils . isBlank ( rememberMeKey ) && ! development ) { LOG . debug ( "Generating a new ephemeral 'remember me' key in a secure way." ) ; rememberMeKey = generateRememberMeKey ( ) ; } else-if ( StringUtils . isBlank ( rememberMeKey ) && development ) { LOG . warn ( "Using a fixed 'remember me' key because we're in development mode, this is INSECURE." ) ; rememberMeKey = DEVELOPMENT_REMEMBER_ME_KEY ; } else { LOG . info ( "Using a fixed 'remember me' key from system properties, this is insecure." ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Updating entity reference {} for reference attribute {}" , attributeDef . getName ( ) ) ; } }
public void test() { try { report ( ) ; } catch ( Exception e ) { LOG . warn ( "Final reporting of metrics failed." , e ) ; } }
public void test() { if ( ! executor . awaitTermination ( 1 , TimeUnit . SECONDS ) ) { LOG . warn ( "ScheduledExecutorService did not terminate." ) ; } }
@ Deprecated public String getFinancialYearId ( String estDate ) { logger . info ( "Obtained session" ) ; String result = "" ; Query query = getCurrentSession ( ) . createQuery ( "select cfinancialyear.id from CFinancialYear cfinancialyear where cfinancialyear.startingDate <= to_date('" + estDate + "','dd/MM/yyyy') and cfinancialyear.endingDate >= to_date('" + estDate + "','dd/MM/yyyy') " ) ; ArrayList list = ( ArrayList ) query . list ( ) ; if ( list . size ( ) > 0 ) result = list . get ( 0 ) . toString ( ) ; return result ; }
public void test() { if ( numCompletedSteps >= numSteps ) { logger . info ( "Completed {} comprised of {} steps in {} millis. Index found {} hits. Read {} events from Event Files." , query , numSteps , queryTime , hitCount , matchingRecords . size ( ) ) ; } else { logger . info ( "Completed {} comprised of {} steps in {} millis. Index found {} hits. Read {} events from Event Files. " + "Only completed {} steps because the maximum number of results was reached." , query , numSteps , queryTime , hitCount , matchingRecords . size ( ) , numCompletedSteps ) ; } }
public void test() { if ( numCompletedSteps >= numSteps ) { logger . info ( "Completed {} comprised of {} steps in {} millis. Index found {} hits. Read {} events from Event Files." , query , numSteps , queryTime , hitCount , matchingRecords . size ( ) ) ; } else { logger . info ( "Completed {} comprised of {} steps in {} millis. Index found {} hits. Read {} events from Event Files. " + "Only completed {} steps because the maximum number of results was reached." , query , numSteps , queryTime , hitCount , matchingRecords . size ( ) , numCompletedSteps ) ; } }
public void test() { if ( received % 10000 == 0 ) { logger . info ( toString ( ) + " is at: " + received ) ; } }
public void test() { try { Partitioner < BulkIngestKey , Value > partitionerForTable = cachePartitioner ( new Text ( tableName ) ) ; initializeJob ( job , partitionerForTable ) ; validTableNames . add ( tableName ) ; } catch ( Exception e ) { log . warn ( "Unable to create the partitioner to " + tableName + " despite its configuration." + "Will use the default partitioner for this table." , e ) ; lazyInitializeDefaultPartitioner ( job ) ; } }
public void test() { if ( response . getStatusLine ( ) . getStatusCode ( ) != HttpStatus . SC_OK ) { log . debug ( "No instance role found, return code was %d" , response . getStatusLine ( ) . getStatusCode ( ) ) ; return null ; } }
public void test() { if ( response . getStatusLine ( ) . getStatusCode ( ) != HttpStatus . SC_OK ) { log . debug ( "No security credential found, return code was %d" , response . getStatusLine ( ) . getStatusCode ( ) ) ; return null ; } }
public void test() { try { java . util . List < com . liferay . portal . kernel . model . EmailAddress > returnValue = EmailAddressServiceUtil . getEmailAddresses ( className , classPK ) ; return com . liferay . portal . kernel . model . EmailAddressSoap . toSoapModels ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
private static void convertTieLine ( UcteNetwork ucteNetwork , MergedXnode mergedXnode , UcteExporterContext context ) { Line line = mergedXnode . getExtendable ( ) ; LOGGER . trace ( "Converting TieLine {}" , line . getId ( ) ) ; convertXNode ( ucteNetwork , mergedXnode , context ) ; UcteElementId ucteElementId1 = context . getNamingStrategy ( ) . getUcteElementId ( mergedXnode . getLine1Name ( ) ) ; String elementName1 = line . getProperty ( ELEMENT_NAME_PROPERTY_KEY + "_1" , null ) ; UcteElementStatus status1 = line instanceof TieLine ? getStatusHalf ( ( TieLine ) line , Branch . Side . ONE ) : getStatus ( line , Branch . Side . ONE ) ; UcteLine ucteLine1 = new UcteLine ( ucteElementId1 , status1 , ( float ) line . getR ( ) * mergedXnode . getRdp ( ) , ( float ) line . getX ( ) * mergedXnode . getXdp ( ) , ( float ) line . getB1 ( ) , ( int ) line . getCurrentLimits1 ( ) . getPermanentLimit ( ) , elementName1 ) ; ucteNetwork . addLine ( ucteLine1 ) ; UcteElementId ucteElementId2 = context . getNamingStrategy ( ) . getUcteElementId ( mergedXnode . getLine2Name ( ) ) ; String elementName2 = line . getProperty ( ELEMENT_NAME_PROPERTY_KEY + "_2" , null ) ; UcteElementStatus status2 = line instanceof TieLine ? getStatusHalf ( ( TieLine ) line , Branch . Side . TWO ) : getStatus ( line , Branch . Side . TWO ) ; UcteLine ucteLine2 = new UcteLine ( ucteElementId2 , status2 , ( float ) line . getR ( ) * ( 1.0f - mergedXnode . getRdp ( ) ) , ( float ) line . getX ( ) * ( 1.0f - mergedXnode . getXdp ( ) ) , ( float ) line . getB2 ( ) , ( int ) line . getCurrentLimits2 ( ) . getPermanentLimit ( ) , elementName2 ) ; ucteNetwork . addLine ( ucteLine2 ) ; }
public void test() { if ( documentRes == null ) { log . error ( "Could not adapt document model to relation resource ; " + "check the service relation adapters configuration" ) ; return ; } }
public void test() { try { coreSession . removeDocument ( docModel . getRef ( ) ) ; log . debug ( "quote removal succeded for id: " + commentId ) ; } catch ( Exception e ) { log . error ( "quote removal failed" , e ) ; } }
public void test() { if ( docModel != null ) { code_block = TryStatement ;  } else { log . warn ( "quote/comment not found: id=" + commentId ) ; } }
public void test() { try { page = ( Page ) this . getPage ( this . getPageCode ( ) ) ; PageModel model = page . getMetadata ( ) . getModel ( ) ; Widget [ ] defaultWidgets = model . getDefaultWidget ( ) ; code_block = IfStatement ; Widget [ ] widgets = new Widget [ defaultWidgets . length ] ; code_block = ForStatement ; page . setWidgets ( widgets ) ; this . getPageManager ( ) . updatePage ( page ) ; } catch ( Throwable t ) { logger . error ( "Error setting default widget to page {}" , this . getPageCode ( ) , t ) ; return FAILURE ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { _log . error ( "Unable to process service alert" , e ) ; } }
public void test() { try { String loadedData = Files . lines ( datapath ) . collect ( Collectors . joining ( ) ) ; Document doc = Jsoup . parse ( loadedData ) ; Element table = doc . select ( "table" ) . get ( 5 ) ; Elements tableRows = table . select ( "tr" ) ; code_block = ForStatement ; result . remove ( 0 ) ; return result ; } catch ( IOException e ) { e . printStackTrace ( ) ; log . debug ( "loading failed." ) ; return result ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( _NO_SUCH_ENTITY_WITH_PRIMARY_KEY + primaryKey ) ; } }
public void test() { try { byte [ ] rowBytes = HBaseUtils . getBytes ( rowKey ) ; Delete delete = new Delete ( rowBytes ) ; hTable . delete ( delete ) ; } catch ( IOException e ) { logger . error ( "Error while delete on hbase for : " + rowKey ) ; throw new PersistenceException ( "Could not perform delete. Caused by: " , e ) ; } }
public void test() { if ( ! elementFile . exists ( ) ) { logger . info ( "Element {} from media package {} has already been removed or has never been distributed to " + "publication channel {}" , elementId , mediapackageId , channelId ) ; return element ; } }
public void test() { if ( ! FileUtils . deleteQuietly ( elementFile . getParentFile ( ) ) ) { logger . debug ( "Unable to delete folder {}" , elementFile . getParentFile ( ) . getAbsolutePath ( ) ) ; } }
public void test() { if ( mismatchedPartitioner != null ) { LoggerFactory . getLogger ( CassandraKeyspace . class ) . warn ( "Cassandra keyspace '{}' would perform better if it was configured with the {}.  It currently uses the {}." , getName ( ) , expectedPartitioner . getSimpleName ( ) , mismatchedPartitioner ) ; } }
public void test() { try { listener . beforeRefreshStart ( asynchronous ) ; } catch ( Exception e ) { LOG . error ( e ) ; } }
public void test() { try { listener . update ( newEventArr , oldEventArr , epStatement , runtime ) ; } catch ( Throwable t ) { String message = "Unexpected exception invoking listener update method on listener class '" + listener . getClass ( ) . getSimpleName ( ) + "' : " + t . getClass ( ) . getSimpleName ( ) + " : " + t . getMessage ( ) ; log . error ( message , t ) ; } }
public void test() { if ( type == LutronCommandType . MODE && parameters . length > 1 && ModeCommand . ACTION_STEP . toString ( ) . equals ( parameters [ 0 ] ) ) { Long step = Long . valueOf ( parameters [ 1 ] ) ; code_block = IfStatement ; updateState ( CHANNEL_STEP , new DecimalType ( step . longValue ( ) ) ) ; } else { logger . debug ( "Ignoring unexpected update for id {}" , integrationId ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( NumberFormatException e ) { logger . debug ( "Encountered number format exception while handling update for greenmode {}" , integrationId ) ; } }
public static void main ( String args [ ] ) { JMeterUtils . loadJMeterProperties ( "jmeter.properties" ) ; String dataset = JMeterUtils . getPropDefault ( "jmeterPlugin.sts.datasetDirectory" , HttpSimpleTableControl . DEFAULT_DATA_DIR ) ; int port = JMeterUtils . getPropDefault ( "jmeterPlugin.sts.port" , HttpSimpleTableControl . DEFAULT_PORT ) ; boolean timestamp = JMeterUtils . getPropDefault ( "jmeterPlugin.sts.addTimestamp" , HttpSimpleTableControl . DEFAULT_TIMESTAMP ) ; Configurator . setLevel ( log . getName ( ) , Level . INFO ) ; String loglevelStr = JMeterUtils . getPropDefault ( "loglevel" , HttpSimpleTableControl . DEFAULT_LOG_LEVEL ) ; System . out . println ( "loglevel=" + loglevelStr ) ; Configurator . setRootLevel ( Level . toLevel ( loglevelStr ) ) ; Configurator . setLevel ( log . getName ( ) , Level . toLevel ( loglevelStr ) ) ; bStartFromMain = true ; HttpSimpleTableServer serv = new HttpSimpleTableServer ( port , timestamp , dataset ) ; log . info ( "Creating HttpSimpleTable ..." ) ; log . info ( "------------------------------" ) ; log . info ( "SERVER_PORT : " + port ) ; log . info ( "DATASET_DIR : " + dataset ) ; log . info ( "TIMESTAMP : " + timestamp ) ; log . info ( "------------------------------" ) ; log . info ( "STS_VERSION : " + STS_VERSION ) ; ServerRunner . executeInstance ( serv ) ; }
public static void main ( String args [ ] ) { JMeterUtils . loadJMeterProperties ( "jmeter.properties" ) ; String dataset = JMeterUtils . getPropDefault ( "jmeterPlugin.sts.datasetDirectory" , HttpSimpleTableControl . DEFAULT_DATA_DIR ) ; int port = JMeterUtils . getPropDefault ( "jmeterPlugin.sts.port" , HttpSimpleTableControl . DEFAULT_PORT ) ; boolean timestamp = JMeterUtils . getPropDefault ( "jmeterPlugin.sts.addTimestamp" , HttpSimpleTableControl . DEFAULT_TIMESTAMP ) ; Configurator . setLevel ( log . getName ( ) , Level . INFO ) ; String loglevelStr = JMeterUtils . getPropDefault ( "loglevel" , HttpSimpleTableControl . DEFAULT_LOG_LEVEL ) ; System . out . println ( "loglevel=" + loglevelStr ) ; Configurator . setRootLevel ( Level . toLevel ( loglevelStr ) ) ; Configurator . setLevel ( log . getName ( ) , Level . toLevel ( loglevelStr ) ) ; bStartFromMain = true ; HttpSimpleTableServer serv = new HttpSimpleTableServer ( port , timestamp , dataset ) ; log . info ( "Creating HttpSimpleTable ..." ) ; log . info ( "------------------------------" ) ; log . info ( "SERVER_PORT : " + port ) ; log . info ( "DATASET_DIR : " + dataset ) ; log . info ( "TIMESTAMP : " + timestamp ) ; log . info ( "------------------------------" ) ; log . info ( "STS_VERSION : " + STS_VERSION ) ; ServerRunner . executeInstance ( serv ) ; }
public static void main ( String args [ ] ) { JMeterUtils . loadJMeterProperties ( "jmeter.properties" ) ; String dataset = JMeterUtils . getPropDefault ( "jmeterPlugin.sts.datasetDirectory" , HttpSimpleTableControl . DEFAULT_DATA_DIR ) ; int port = JMeterUtils . getPropDefault ( "jmeterPlugin.sts.port" , HttpSimpleTableControl . DEFAULT_PORT ) ; boolean timestamp = JMeterUtils . getPropDefault ( "jmeterPlugin.sts.addTimestamp" , HttpSimpleTableControl . DEFAULT_TIMESTAMP ) ; Configurator . setLevel ( log . getName ( ) , Level . INFO ) ; String loglevelStr = JMeterUtils . getPropDefault ( "loglevel" , HttpSimpleTableControl . DEFAULT_LOG_LEVEL ) ; System . out . println ( "loglevel=" + loglevelStr ) ; Configurator . setRootLevel ( Level . toLevel ( loglevelStr ) ) ; Configurator . setLevel ( log . getName ( ) , Level . toLevel ( loglevelStr ) ) ; bStartFromMain = true ; HttpSimpleTableServer serv = new HttpSimpleTableServer ( port , timestamp , dataset ) ; log . info ( "Creating HttpSimpleTable ..." ) ; log . info ( "------------------------------" ) ; log . info ( "SERVER_PORT : " + port ) ; log . info ( "DATASET_DIR : " + dataset ) ; log . info ( "TIMESTAMP : " + timestamp ) ; log . info ( "------------------------------" ) ; log . info ( "STS_VERSION : " + STS_VERSION ) ; ServerRunner . executeInstance ( serv ) ; }
public static void main ( String args [ ] ) { JMeterUtils . loadJMeterProperties ( "jmeter.properties" ) ; String dataset = JMeterUtils . getPropDefault ( "jmeterPlugin.sts.datasetDirectory" , HttpSimpleTableControl . DEFAULT_DATA_DIR ) ; int port = JMeterUtils . getPropDefault ( "jmeterPlugin.sts.port" , HttpSimpleTableControl . DEFAULT_PORT ) ; boolean timestamp = JMeterUtils . getPropDefault ( "jmeterPlugin.sts.addTimestamp" , HttpSimpleTableControl . DEFAULT_TIMESTAMP ) ; Configurator . setLevel ( log . getName ( ) , Level . INFO ) ; String loglevelStr = JMeterUtils . getPropDefault ( "loglevel" , HttpSimpleTableControl . DEFAULT_LOG_LEVEL ) ; System . out . println ( "loglevel=" + loglevelStr ) ; Configurator . setRootLevel ( Level . toLevel ( loglevelStr ) ) ; Configurator . setLevel ( log . getName ( ) , Level . toLevel ( loglevelStr ) ) ; bStartFromMain = true ; HttpSimpleTableServer serv = new HttpSimpleTableServer ( port , timestamp , dataset ) ; log . info ( "Creating HttpSimpleTable ..." ) ; log . info ( "------------------------------" ) ; log . info ( "SERVER_PORT : " + port ) ; log . info ( "DATASET_DIR : " + dataset ) ; log . info ( "TIMESTAMP : " + timestamp ) ; log . info ( "------------------------------" ) ; log . info ( "STS_VERSION : " + STS_VERSION ) ; ServerRunner . executeInstance ( serv ) ; }
public static void main ( String args [ ] ) { JMeterUtils . loadJMeterProperties ( "jmeter.properties" ) ; String dataset = JMeterUtils . getPropDefault ( "jmeterPlugin.sts.datasetDirectory" , HttpSimpleTableControl . DEFAULT_DATA_DIR ) ; int port = JMeterUtils . getPropDefault ( "jmeterPlugin.sts.port" , HttpSimpleTableControl . DEFAULT_PORT ) ; boolean timestamp = JMeterUtils . getPropDefault ( "jmeterPlugin.sts.addTimestamp" , HttpSimpleTableControl . DEFAULT_TIMESTAMP ) ; Configurator . setLevel ( log . getName ( ) , Level . INFO ) ; String loglevelStr = JMeterUtils . getPropDefault ( "loglevel" , HttpSimpleTableControl . DEFAULT_LOG_LEVEL ) ; System . out . println ( "loglevel=" + loglevelStr ) ; Configurator . setRootLevel ( Level . toLevel ( loglevelStr ) ) ; Configurator . setLevel ( log . getName ( ) , Level . toLevel ( loglevelStr ) ) ; bStartFromMain = true ; HttpSimpleTableServer serv = new HttpSimpleTableServer ( port , timestamp , dataset ) ; log . info ( "Creating HttpSimpleTable ..." ) ; log . info ( "------------------------------" ) ; log . info ( "SERVER_PORT : " + port ) ; log . info ( "DATASET_DIR : " + dataset ) ; log . info ( "TIMESTAMP : " + timestamp ) ; log . info ( "------------------------------" ) ; log . info ( "STS_VERSION : " + STS_VERSION ) ; ServerRunner . executeInstance ( serv ) ; }
@ Override public void start ( final Map < String , String > properties ) { logger . info ( "Starting Source Task with properties {}" , StatelessKafkaConnectorUtil . getLoggableProperties ( properties ) ) ; final String timeout = properties . getOrDefault ( StatelessKafkaConnectorUtil . DATAFLOW_TIMEOUT , StatelessKafkaConnectorUtil . DEFAULT_DATAFLOW_TIMEOUT ) ; timeoutMillis = ( long ) FormatUtils . getPreciseTimeDuration ( timeout , TimeUnit . MILLISECONDS ) ; topicName = properties . get ( StatelessNiFiSourceConnector . TOPIC_NAME ) ; topicNameAttribute = properties . get ( StatelessNiFiSourceConnector . TOPIC_NAME_ATTRIBUTE ) ; keyAttributeName = properties . get ( StatelessNiFiSourceConnector . KEY_ATTRIBUTE ) ; code_block = IfStatement ; final String headerRegex = properties . get ( StatelessNiFiSourceConnector . HEADER_REGEX ) ; headerAttributeNamePattern = headerRegex == null ? null : Pattern . compile ( headerRegex ) ; dataflow = StatelessKafkaConnectorUtil . createDataflow ( properties ) ; dataflow . initialize ( ) ; dataflowName = properties . get ( StatelessKafkaConnectorUtil . DATAFLOW_NAME ) ; outputPortName = properties . get ( StatelessNiFiSourceConnector . OUTPUT_PORT_NAME ) ; code_block = IfStatement ; final String taskIndex = properties . get ( STATE_MAP_KEY ) ; localStatePartitionMap . put ( STATE_MAP_KEY , taskIndex ) ; final Map < String , String > localStateMap = ( Map < String , String > ) ( Map ) context . offsetStorageReader ( ) . offset ( localStatePartitionMap ) ; final Map < String , String > clusterStateMap = ( Map < String , String > ) ( Map ) context . offsetStorageReader ( ) . offset ( clusterStatePartitionMap ) ; dataflow . setComponentStates ( localStateMap , Scope . LOCAL ) ; dataflow . setComponentStates ( clusterStateMap , Scope . CLUSTER ) ; }
public void test() { try { Document response = readXML ( url ) ; NodeList nodes = response . getElementsByTagName ( "IdList" ) ; code_block = IfStatement ; } catch ( Exception ex ) { this . logger . error ( "Error while trying to retrieve matches to " + query + " " + ex . getClass ( ) . getName ( ) + " " + ex . getMessage ( ) , ex ) ; } }
public void test() { if ( ObjectHelper . isEmpty ( deploymentName ) ) { LOG . error ( "Create a specific Deployment require specify a Deployment name" ) ; throw new IllegalArgumentException ( "Create a specific Deployment require specify a pod name" ) ; } }
public void test() { if ( ObjectHelper . isEmpty ( namespaceName ) ) { LOG . error ( "Create a specific Deployment require specify a namespace name" ) ; throw new IllegalArgumentException ( "Create a specific Deployment require specify a namespace name" ) ; } }
public void test() { if ( ObjectHelper . isEmpty ( deSpec ) ) { LOG . error ( "Create a specific Deployment require specify a Deployment spec bean" ) ; throw new IllegalArgumentException ( "Create a specific Deployment require specify a Deployment spec bean" ) ; } }
@ BeforeClass public void setup ( ) throws Exception { logger . info ( "Setup begin: " + this . getClass ( ) . getSimpleName ( ) ) ; String dataPath = rootPath + "/src/test/resources/alldatatype.csv" ; CarbonProperties . getInstance ( ) . addProperty ( CarbonCommonConstants . CARBON_WRITTEN_BY_APPNAME , "HetuTest" ) ; CarbonProperties . getInstance ( ) . addProperty ( CarbonCommonConstants . MAX_QUERY_EXECUTION_TIME , "0" ) ; CarbonProperties . getInstance ( ) . addProperty ( CarbonCommonConstants . CARBON_SEGMENT_LOCK_FILES_PRESERVE_HOURS , "0" ) ; CarbonProperties . getInstance ( ) . addProperty ( CarbonCommonConstants . CARBON_INVISIBLE_SEGMENTS_PRESERVE_COUNT , "1" ) ; Map < String , String > map = new HashMap < String , String > ( ) ; map . put ( "hive.metastore" , "file" ) ; map . put ( "hive.allow-drop-table" , "true" ) ; map . put ( "hive.metastore.catalog.dir" , "file://" + storePath + "/hive.store" ) ; map . put ( "carbondata.store-location" , "file://" + carbonStoreLocation ) ; map . put ( "carbondata.minor-vacuum-seg-count" , "4" ) ; map . put ( "carbondata.major-vacuum-seg-size" , "1" ) ; code_block = IfStatement ; hetuServer . startServer ( "testdb" , map ) ; hetuServer . execute ( "drop table if exists testdb.testtable" ) ; hetuServer . execute ( "drop table if exists testdb.testtable2" ) ; hetuServer . execute ( "drop table if exists testdb.testtable3" ) ; hetuServer . execute ( "drop table if exists testdb.testtable4" ) ; hetuServer . execute ( "drop schema if exists testdb" ) ; hetuServer . execute ( "drop schema if exists default" ) ; hetuServer . execute ( "create schema testdb" ) ; hetuServer . execute ( "create schema default" ) ; hetuServer . execute ( "create table testdb.testtable(ID int, date date, country varchar, " + "name varchar, phonetype varchar, serialname varchar,salary double, bonus decimal(10,4), " + "monthlyBonus decimal(18,4), dob timestamp, shortField smallint, iscurrentemployee boolean) " + "with(format='CARBON') " ) ; InsertIntoTableFromCSV ( dataPath ) ; String columnNames = "ID,date,country,name,phonetype,serialname,salary,bonus," + "monthlyBonus,dob,shortField,isCurrentEmployee" ; logger . info ( "CarbonStore created at location : " + storePath ) ; }
@ BeforeClass public void setup ( ) throws Exception { logger . info ( "Setup begin: " + this . getClass ( ) . getSimpleName ( ) ) ; String dataPath = rootPath + "/src/test/resources/alldatatype.csv" ; CarbonProperties . getInstance ( ) . addProperty ( CarbonCommonConstants . CARBON_WRITTEN_BY_APPNAME , "HetuTest" ) ; CarbonProperties . getInstance ( ) . addProperty ( CarbonCommonConstants . MAX_QUERY_EXECUTION_TIME , "0" ) ; CarbonProperties . getInstance ( ) . addProperty ( CarbonCommonConstants . CARBON_SEGMENT_LOCK_FILES_PRESERVE_HOURS , "0" ) ; CarbonProperties . getInstance ( ) . addProperty ( CarbonCommonConstants . CARBON_INVISIBLE_SEGMENTS_PRESERVE_COUNT , "1" ) ; Map < String , String > map = new HashMap < String , String > ( ) ; map . put ( "hive.metastore" , "file" ) ; map . put ( "hive.allow-drop-table" , "true" ) ; map . put ( "hive.metastore.catalog.dir" , "file://" + storePath + "/hive.store" ) ; map . put ( "carbondata.store-location" , "file://" + carbonStoreLocation ) ; map . put ( "carbondata.minor-vacuum-seg-count" , "4" ) ; map . put ( "carbondata.major-vacuum-seg-size" , "1" ) ; code_block = IfStatement ; hetuServer . startServer ( "testdb" , map ) ; hetuServer . execute ( "drop table if exists testdb.testtable" ) ; hetuServer . execute ( "drop table if exists testdb.testtable2" ) ; hetuServer . execute ( "drop table if exists testdb.testtable3" ) ; hetuServer . execute ( "drop table if exists testdb.testtable4" ) ; hetuServer . execute ( "drop schema if exists testdb" ) ; hetuServer . execute ( "drop schema if exists default" ) ; hetuServer . execute ( "create schema testdb" ) ; hetuServer . execute ( "create schema default" ) ; hetuServer . execute ( "create table testdb.testtable(ID int, date date, country varchar, " + "name varchar, phonetype varchar, serialname varchar,salary double, bonus decimal(10,4), " + "monthlyBonus decimal(18,4), dob timestamp, shortField smallint, iscurrentemployee boolean) " + "with(format='CARBON') " ) ; InsertIntoTableFromCSV ( dataPath ) ; String columnNames = "ID,date,country,name,phonetype,serialname,salary,bonus," + "monthlyBonus,dob,shortField,isCurrentEmployee" ; logger . info ( "CarbonStore created at location : " + storePath ) ; }
public void test() { try { MethodKey methodKey = new MethodKey ( UserServiceUtil . class , "updateIncompleteUser" , _updateIncompleteUserParameterTypes60 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , companyId , autoPassword , password1 , password2 , autoScreenName , screenName , emailAddress , facebookId , openId , locale , firstName , middleName , lastName , prefixId , suffixId , male , birthdayMonth , birthdayDay , birthdayYear , jobTitle , updateUserInformation , sendEmail , serviceContext ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . portal . kernel . model . User ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
@ Override public PortablePipelineResult run ( Pipeline pipeline , JobInfo jobInfo ) throws Exception { PortablePipelineOptions pipelineOptions = PipelineOptionsTranslation . fromProto ( jobInfo . pipelineOptions ( ) ) . as ( PortablePipelineOptions . class ) ; final String jobName = jobInfo . jobName ( ) ; File outputFile = new File ( checkArgumentNotNull ( pipelineOptions . getOutputExecutablePath ( ) ) ) ; LOG . info ( "Creating jar {} for job {}" , outputFile . getAbsolutePath ( ) , jobName ) ; outputStream = new JarOutputStream ( new FileOutputStream ( outputFile ) , createManifest ( mainClass , jobName ) ) ; outputChannel = Channels . newChannel ( outputStream ) ; PortablePipelineJarUtils . writeDefaultJobName ( outputStream , jobName ) ; copyResourcesFromJar ( new JarFile ( mainClass . getProtectionDomain ( ) . getCodeSource ( ) . getLocation ( ) . getPath ( ) ) ) ; writeAsJson ( PipelineOptionsTranslation . toProto ( pipelineOptions ) , PortablePipelineJarUtils . getPipelineOptionsUri ( jobName ) ) ; Pipeline pipelineWithClasspathArtifacts = writeArtifacts ( pipeline , jobName ) ; writeAsJson ( pipelineWithClasspathArtifacts , PortablePipelineJarUtils . getPipelineUri ( jobName ) ) ; outputChannel . close ( ) ; LOG . info ( "Jar {} created successfully." , outputFile . getAbsolutePath ( ) ) ; return new JarCreatorPipelineResult ( ) ; }
@ Override public PortablePipelineResult run ( Pipeline pipeline , JobInfo jobInfo ) throws Exception { PortablePipelineOptions pipelineOptions = PipelineOptionsTranslation . fromProto ( jobInfo . pipelineOptions ( ) ) . as ( PortablePipelineOptions . class ) ; final String jobName = jobInfo . jobName ( ) ; File outputFile = new File ( checkArgumentNotNull ( pipelineOptions . getOutputExecutablePath ( ) ) ) ; LOG . info ( "Creating jar {} for job {}" , outputFile . getAbsolutePath ( ) , jobName ) ; outputStream = new JarOutputStream ( new FileOutputStream ( outputFile ) , createManifest ( mainClass , jobName ) ) ; outputChannel = Channels . newChannel ( outputStream ) ; PortablePipelineJarUtils . writeDefaultJobName ( outputStream , jobName ) ; copyResourcesFromJar ( new JarFile ( mainClass . getProtectionDomain ( ) . getCodeSource ( ) . getLocation ( ) . getPath ( ) ) ) ; writeAsJson ( PipelineOptionsTranslation . toProto ( pipelineOptions ) , PortablePipelineJarUtils . getPipelineOptionsUri ( jobName ) ) ; Pipeline pipelineWithClasspathArtifacts = writeArtifacts ( pipeline , jobName ) ; writeAsJson ( pipelineWithClasspathArtifacts , PortablePipelineJarUtils . getPipelineUri ( jobName ) ) ; outputChannel . close ( ) ; LOG . info ( "Jar {} created successfully." , outputFile . getAbsolutePath ( ) ) ; return new JarCreatorPipelineResult ( ) ; }
public void test() { if ( ! allResolved ) { log . debugf ( "Ignoring resolveOptions, not all Alerts for Trigger %s are resolved" , trigger . toString ( ) ) ; return ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Ignoring setFiring, loaded Trigger already in firing mode " + loadedTrigger . toString ( ) ) ; } }
@ Override public void onFailure ( final Throwable throwable ) { LOG . debug ( "{}: getSchemaSource for {} failed" , id , sourceIdentifier , throwable ) ; sender . tell ( new Failure ( throwable ) , getSelf ( ) ) ; }
public void test() { if ( message . getExtensionLength ( ) . getValue ( ) > 65535 ) { LOGGER . warn ( "The SingedCertificateTimestamp length shouldn't exceed 2 bytes as defined in RFC 6962. " + "Length was " + message . getExtensionLength ( ) . getValue ( ) ) ; } }
public void test() { if ( inputStream == null ) { _log . error ( "Default user male portrait is not available" ) ; } }
public void test() { try { InputStream inputStream = classLoader . getResourceAsStream ( PropsUtil . get ( PropsKeys . IMAGE_DEFAULT_USER_MALE_PORTRAIT ) ) ; code_block = IfStatement ; _defaultUserMalePortrait = getImage ( inputStream ) ; } catch ( Exception exception ) { _log . error ( "Unable to configure the default user male portrait: " + exception . getMessage ( ) ) ; } }
public void test() { try { final Class < ? > declaringClass = methodInvoked . getDeclaringClass ( ) ; code_block = IfStatement ; methodNameBuilder . append ( methodInvoked . getName ( ) ) ; } catch ( final Exception exception ) { logger . error ( "An error occurred while fetching method details" , exception ) ; } }
public void test() { if ( isDebug ) { logger . debug ( "Trace sampling is true, Recording trace. methodInvoked:{}, remoteAddress:{}" , methodNameBuilder . toString ( ) , remoteAddress ) ; } }
public void test() { if ( isDebug ) { logger . debug ( "Trace sampling is false, Skip recording trace. methodInvoked:{}, remoteAddress:{}" , methodNameBuilder . toString ( ) , remoteAddress ) ; } }
public void test() { try { exporter . export ( AuthContextUtils . getDomain ( ) , os , uwfAdapter . getPrefix ( ) , gwfAdapter . getPrefix ( ) , awfAdapter . getPrefix ( ) ) ; LOG . debug ( "Internal storage content successfully exported" ) ; } catch ( Exception e ) { LOG . error ( "While exporting internal storage content" , e ) ; } }
public void test() { try { serverChannel . close ( ) ; } catch ( IOException e ) { LOG . warn ( "Unable to close {}" , serverChannel , e ) ; } }
public void test() { if ( existingIndex == null ) { List < AtlasPropertyKey > keys = new ArrayList < > ( 2 ) ; keys . add ( typePropertyKey ) ; keys . add ( propertyKey ) ; management . createVertexCompositeIndex ( indexName , isUnique , keys ) ; LOG . info ( "Created composite index for property {} of type {} and {}" , propertyKey . getName ( ) , propertyClass . getName ( ) , systemPropertyKey ) ; } }
public void test() { try { LOG . info ( "reading checkpoint info for:" + instant + " key: " + extraMetadataKey ) ; HoodieCommitMetadata commitMetadata = HoodieCommitMetadata . fromBytes ( metaClient . getCommitsTimeline ( ) . getInstantDetails ( instant ) . get ( ) , HoodieCommitMetadata . class ) ; return Option . ofNullable ( commitMetadata . getExtraMetadata ( ) . get ( extraMetadataKey ) ) ; } catch ( IOException e ) { throw new HoodieIOException ( "Unable to parse instant metadata " + instant , e ) ; } }
@ Override public void close ( ) throws IOException { LOG . info ( "Terminating Persistence Processor..." ) ; disruptor . halt ( ) ; disruptor . shutdown ( ) ; LOG . info ( "\tPersistence Processor Disruptor shutdown" ) ; disruptorExec . shutdownNow ( ) ; code_block = TryStatement ;  LOG . info ( "Persistence Processor terminated" ) ; }
public void test() { try { disruptorExec . awaitTermination ( 3 , SECONDS ) ; LOG . info ( "\tPersistence Processor Disruptor executor shutdown" ) ; } catch ( InterruptedException e ) { LOG . error ( "Interrupted whilst finishing Persistence Processor Disruptor executor" ) ; Thread . currentThread ( ) . interrupt ( ) ; } }
protected synchronized void update ( final Map < String , Object > properties ) { logger . debug ( "Updating GPIO Driver..." ) ; logger . debug ( "Updating GPIO Driver... Done" ) ; }
protected synchronized void update ( final Map < String , Object > properties ) { logger . debug ( "Updating GPIO Driver..." ) ; logger . debug ( "Updating GPIO Driver... Done" ) ; }
public void test() { try { stat = conn . prepareStatement ( DELETE_CONFIG ) ; stat . setString ( 1 , username ) ; stat . executeUpdate ( ) ; } catch ( Throwable t ) { _logger . error ( "Error deleting user config record by id {}" , username , t ) ; throw new RuntimeException ( "Error deleting user config record by id " + username , t ) ; } finally { this . closeDaoResources ( null , stat ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( EmailAddressServiceUtil . class , "getEmailAddress" , _getEmailAddressParameterTypes3 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , emailAddressId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . portal . kernel . model . EmailAddress ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { String defaultLangCode = this . getLangManager ( ) . getDefaultLang ( ) . getCode ( ) ; String langCode = properties . getProperty ( SystemConstants . API_LANG_CODE_PARAMETER ) ; String tagParamValue = properties . getProperty ( "tag" ) ; langCode = ( null != langCode && null != this . getLangManager ( ) . getLang ( langCode ) ) ? langCode : defaultLangCode ; Map < String , ApiService > masterServices = this . getApiCatalogManager ( ) . getServices ( tagParamValue ) ; Iterator < ApiService > iter = masterServices . values ( ) . iterator ( ) ; code_block = WhileStatement ; BeanComparator comparator = new BeanComparator ( "description" ) ; Collections . sort ( services , comparator ) ; } catch ( Throwable t ) { _logger . error ( "Error extracting services" , t ) ; throw new ApiException ( IApiErrorCodes . SERVER_ERROR , "Internal error" ) ; } }
public void test() { try { tokensMan . addToken ( CONFIRMATION_TOKEN_TYPE , token , state . getBytes ( StandardCharsets . UTF_8 ) , createDate , expires ) ; } catch ( Exception e ) { log . error ( "Cannot add token to db" , e ) ; throw e ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Failed to process request to read entry at {}:{}. Too many pending requests" , r . ledgerId , r . entryId ) ; } }
public void test() { try { return new KapuaApplicationBrokerFilter ( broker ) ; } catch ( Exception e ) { logger . error ( "Error in plugin installation." , e ) ; throw new SecurityException ( e ) ; } }
public void test() { if ( function . getParameters ( ) . size ( ) > 254 ) { log . warn ( "Too many parameters" , "Function " + function + " has more than 254 in parameters. Skipping generation of convenience method." ) ; return ; } }
public void test() { if ( auth != null && auth . startsWith ( BEARER_PREFIX ) ) { log . debug ( "Authentication using bearer token" ) ; String token = auth . substring ( BEARER_PREFIX . length ( ) ) ; TokenReview tokenReview = authApi . performTokenReview ( token ) ; code_block = IfStatement ; } else-if ( request != null && request . isSSL ( ) && findUserName ( apiHeaderConfig , requestContext ) != null ) { log . debug ( "Authenticating using client certificate" ) ; HttpConnection connection = request . connection ( ) ; String userName = findUserName ( apiHeaderConfig , requestContext ) ; Set < String > groups = findGroups ( apiHeaderConfig , requestContext ) ; Map < String , List < String > > extras = findExtra ( apiHeaderConfig , requestContext ) ; log . debug ( "Found username {}, groups {}, extra {}" , userName , groups , extras ) ; code_block = TryStatement ;  } else { requestContext . setSecurityContext ( new RbacSecurityContext ( new TokenReview ( "system:anonymous" , "" , null , null , false ) , authApi , requestContext . getUriInfo ( ) ) ) ; } }
public void test() { if ( auth != null && auth . startsWith ( BEARER_PREFIX ) ) { log . debug ( "Authentication using bearer token" ) ; String token = auth . substring ( BEARER_PREFIX . length ( ) ) ; TokenReview tokenReview = authApi . performTokenReview ( token ) ; code_block = IfStatement ; } else-if ( request != null && request . isSSL ( ) && findUserName ( apiHeaderConfig , requestContext ) != null ) { log . debug ( "Authenticating using client certificate" ) ; HttpConnection connection = request . connection ( ) ; String userName = findUserName ( apiHeaderConfig , requestContext ) ; Set < String > groups = findGroups ( apiHeaderConfig , requestContext ) ; Map < String , List < String > > extras = findExtra ( apiHeaderConfig , requestContext ) ; log . debug ( "Found username {}, groups {}, extra {}" , userName , groups , extras ) ; code_block = TryStatement ;  } else { requestContext . setSecurityContext ( new RbacSecurityContext ( new TokenReview ( "system:anonymous" , "" , null , null , false ) , authApi , requestContext . getUriInfo ( ) ) ) ; } }
public void test() { if ( auth != null && auth . startsWith ( BEARER_PREFIX ) ) { log . debug ( "Authentication using bearer token" ) ; String token = auth . substring ( BEARER_PREFIX . length ( ) ) ; TokenReview tokenReview = authApi . performTokenReview ( token ) ; code_block = IfStatement ; } else-if ( request != null && request . isSSL ( ) && findUserName ( apiHeaderConfig , requestContext ) != null ) { log . debug ( "Authenticating using client certificate" ) ; HttpConnection connection = request . connection ( ) ; String userName = findUserName ( apiHeaderConfig , requestContext ) ; Set < String > groups = findGroups ( apiHeaderConfig , requestContext ) ; Map < String , List < String > > extras = findExtra ( apiHeaderConfig , requestContext ) ; log . debug ( "Found username {}, groups {}, extra {}" , userName , groups , extras ) ; code_block = TryStatement ;  } else { requestContext . setSecurityContext ( new RbacSecurityContext ( new TokenReview ( "system:anonymous" , "" , null , null , false ) , authApi , requestContext . getUriInfo ( ) ) ) ; } }
public void test() { try { connection . peerCertificateChain ( ) ; log . debug ( "Client certificates trusted... impersonating {}" , userName ) ; requestContext . setSecurityContext ( new RbacSecurityContext ( new TokenReview ( userName , "" , groups , extras , true ) , authApi , requestContext . getUriInfo ( ) ) ) ; } catch ( SSLPeerUnverifiedException e ) { log . debug ( "Peer certificate not valid, proceeding as anonymous" ) ; requestContext . setSecurityContext ( new RbacSecurityContext ( new TokenReview ( "system:anonymous" , "" , null , null , false ) , authApi , requestContext . getUriInfo ( ) ) ) ; } }
public void test() { try { CAS cas = actionHandler . getEditorCas ( ) ; FeatureStructure selectedFS = selectFsByAddr ( cas , aItem . getModelObject ( ) . targetAddr ) ; WebAnnoCasUtil . setFeature ( selectedFS , linkedAnnotationFeature , value != null ? value . getIdentifier ( ) : value ) ; LOG . info ( "change the value" ) ; qualifierModel . detach ( ) ; actionHandler . actionCreateOrUpdate ( RequestCycle . get ( ) . find ( AjaxRequestTarget . class ) . get ( ) , cas ) ; } catch ( Exception e ) { LOG . error ( "Error: " + e . getMessage ( ) , e ) ; error ( "Error: " + e . getMessage ( ) ) ; } }
public void test() { try { CAS cas = actionHandler . getEditorCas ( ) ; FeatureStructure selectedFS = selectFsByAddr ( cas , aItem . getModelObject ( ) . targetAddr ) ; WebAnnoCasUtil . setFeature ( selectedFS , linkedAnnotationFeature , value != null ? value . getIdentifier ( ) : value ) ; LOG . info ( "change the value" ) ; qualifierModel . detach ( ) ; actionHandler . actionCreateOrUpdate ( RequestCycle . get ( ) . find ( AjaxRequestTarget . class ) . get ( ) , cas ) ; } catch ( Exception e ) { LOG . error ( "Error: " + e . getMessage ( ) , e ) ; error ( "Error: " + e . getMessage ( ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Published root WebApplicationContext as ServletContext attribute with name [" + WebApplicationContext . ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE + "]" ) ; } }
public void test() { if ( logger . isInfoEnabled ( ) ) { long elapsedTime = System . currentTimeMillis ( ) - getStartupDate ( ) ; logger . info ( "Root WebApplicationContext: initialization completed in " + elapsedTime + " ms" ) ; } }
public void test() { if ( _log . isInfoEnabled ( ) ) { _log . info ( "Starting remote app entries" ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Throwable t ) { String cantSendEmailMsg = "Can't send fail safe email: " + t . getMessage ( ) + "\n" + ExceptionUtils . getStackTrace ( t ) ; consoleWriter . newLine ( ) . fg ( Ansi . Color . RED ) . a ( cantSendEmailMsg ) . println ( 2 ) ; logger . error ( "Unexpected error" , t ) ; } }
public void test() { try { AutoMLConfig mlConfig = autoMLConfigDAL . getMLConfigByUsecase ( usecase ) ; String modelId = mlConfig . getModelId ( ) ; return h2oApiCommunicator . getLeaderBoard ( modelId ) ; } catch ( Exception e ) { log . error ( "Error getting leaderboard: {} " , usecase ) ; throw new InsightsCustomException ( "Error getting leaderboard: " + usecase ) ; } }
public void test() { try { final Enumeration < NetworkInterface > interfaces = NetworkInterface . getNetworkInterfaces ( ) ; code_block = WhileStatement ; } catch ( SocketException ex ) { LOGGER . error ( "Could not retrieve network interface: {}" , ex . getMessage ( ) , ex ) ; } }
public void test() { try { code_block = WhileStatement ; } catch ( IOException ex ) { code_block = IfStatement ; } finally { logger . debug ( "Reader for TCP port {} finished..." , port ) ; } }
public void test() { { log . info ( "Trying request with url : " + requestUrl ) ; response = HttpsClientRequest . doGet ( requestUrl , headers ) ; retryCount ++ ; } }
public void test() { if ( ( contentLength == - 1 ) || ( contentLength > limit ) ) { LOG . warn ( "Going to buffer response body of large or unknown size. " + "Using getResponseBodyAsStream instead is recommended." ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "==> RangerValidator.getAccessTypes(" + serviceDef + ")" ) ; } }
public void test() { if ( serviceDef == null ) { LOG . warn ( "serviceDef passed in was null!" ) ; } else-if ( CollectionUtils . isEmpty ( serviceDef . getAccessTypes ( ) ) ) { LOG . warn ( "AccessTypeDef collection on serviceDef was null!" ) ; } else { code_block = ForStatement ; } }
public void test() { if ( serviceDef == null ) { LOG . warn ( "serviceDef passed in was null!" ) ; } else-if ( CollectionUtils . isEmpty ( serviceDef . getAccessTypes ( ) ) ) { LOG . warn ( "AccessTypeDef collection on serviceDef was null!" ) ; } else { code_block = ForStatement ; } }
public void test() { if ( accessTypeDef == null ) { LOG . warn ( "Access type def was null!" ) ; } else { String accessType = accessTypeDef . getName ( ) ; code_block = IfStatement ; } }
public void test() { if ( StringUtils . isBlank ( accessType ) ) { LOG . warn ( "Access type def name was null/empty/blank!" ) ; } else { accessTypes . add ( accessType ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "<== RangerValidator.getAccessTypes(" + serviceDef + "): " + accessTypes ) ; } }
public void test() { if ( result . succeeded ( ) ) { log . info ( "HealthServer started" ) ; startPromise . complete ( ) ; } else { log . error ( "Error starting HealthServer" ) ; startPromise . fail ( result . cause ( ) ) ; } }
public void test() { if ( result . succeeded ( ) ) { log . info ( "HealthServer started" ) ; startPromise . complete ( ) ; } else { log . error ( "Error starting HealthServer" ) ; startPromise . fail ( result . cause ( ) ) ; } }
public void test() { if ( _log . isWarnEnabled ( ) ) { _log . warn ( "Unable to parse date " + text , exception ) ; } }
private void createBackupArchiveBundle ( ) { File file = new File ( ARTIFACTS_BUNDLE_BACKUP_FILE_PATH ) ; DeploymentGroupClient deploymentGroupClient = this . i3sClient . deploymentGroup ( ) ; ResourceCollection < DeploymentGroup > deploymentGroups = deploymentGroupClient . getAll ( ) ; String deploymentGrpUri = deploymentGroups . get ( 0 ) . getUri ( ) ; TaskResource task = this . artifactsBundleClient . createBackupArchiveBundle ( file , deploymentGrpUri ) ; LOGGER . info ( "Task object returned to client: {}" , task . toJsonString ( ) ) ; }
@ Test public void testInvokeNormal ( ) { NormalClass method = Container . getComp ( NormalClass . class ) ; String result = method . test ( ) ; LOG . info ( result ) ; assertEquals ( "RESULT" , result ) ; }
private void setJsonTimeseries ( JsonObject obj , String target , Pair < ZonedDateTime , ZonedDateTime > timeRange ) { List < TimeValues > timeValues = databaseConnectService . querySeries ( target , timeRange ) ; logger . info ( "query size: {}" , timeValues . size ( ) ) ; JsonArray dataPoints = new JsonArray ( ) ; code_block = ForStatement ; obj . add ( "datapoints" , dataPoints ) ; }
public void test() { try { UserEdit user = userDirectoryService . editUser ( id ) ; state . setAttribute ( "user" , user ) ; state . setAttribute ( "mode" , "edit" ) ; } catch ( UserNotDefinedException e ) { log . warn ( "UsersAction.doEdit: user not found: {}" , id ) ; Object [ ] params = new Object [ ] code_block = "" ; ; addAlert ( state , rb . getFormattedMessage ( "useact.use_notfou" , params ) ) ; state . removeAttribute ( "mode" ) ; } catch ( UserPermissionException e ) { addAlert ( state , rb . getFormattedMessage ( "useact.youdonot1" , new Object [ ] code_block = "" ; ) ) ; state . removeAttribute ( "mode" ) ; } catch ( UserLockedException e ) { addAlert ( state , rb . getFormattedMessage ( "useact.somels" , new Object [ ] code_block = "" ; ) ) ; state . removeAttribute ( "mode" ) ; } }
public void test() { if ( isDebug ) { logger . debug ( "Set AsyncContext {}" , asyncContext ) ; } }
public void test() { try { paraStyleName = AnyConverter . toString ( Utils . getProperty ( textRange , UnoProperty . PARA_STYLE_NAME ) ) ; } catch ( IllegalArgumentException e ) { LOGGER . trace ( "" , e ) ; } }
public synchronized void stopLoadBalancing ( ) { logger . debug ( "{} stopped. Will no longer distribute FlowFiles across the cluster" , this ) ; code_block = IfStatement ; stopped = true ; partitionReadLock . lock ( ) ; code_block = TryStatement ;  }
@ Override public void setJobContextInformation ( String jobId , Object object ) { runningJobs . put ( jobId , ( ActorRef ) object ) ; LOGGER . debug ( "Running jobs: {}" , runningJobs . keySet ( ) ) ; }
public void test() { try { CommerceDiscountRuleServiceUtil . deleteCommerceDiscountRule ( commerceDiscountRuleId ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public static void main ( String [ ] args ) throws UnknownHostException { Environment env = new SpringApplication ( DataXAdminApplication . class ) . run ( args ) . getEnvironment ( ) ; String envPort = env . getProperty ( "server.port" ) ; String envContext = env . getProperty ( "server.contextPath" ) ; String port = envPort == null ? "8080" : envPort ; String context = envContext == null ? "" : envContext ; String path = port + "" + context + "/doc.html" ; String externalAPI = InetAddress . getLocalHost ( ) . getHostAddress ( ) ; logger . info ( "Access URLs:\n----------------------------------------------------------\n\t" + "Local-API: \t\thttp://127.0.0.1:{}\n\t" + "External-API: \thttp://{}:{}\n\t" + "web-URL: \t\thttp://127.0.0.1:{}/index.html\n\t----------------------------------------------------------" , path , externalAPI , path , port ) ; }
@ Test public void shouldReturnEmptyBodyAndStatus404 ( ) { String response = given ( ) . header ( "Accept" , "application/json" ) . contentType ( ContentType . JSON ) . port ( getHttpPort ( ) ) . expect ( ) . statusCode ( Response . Status . NOT_FOUND . getStatusCode ( ) ) . when ( ) . get ( BASE_REST_PATH + "/does-not-exists" ) . asString ( ) ; logger . info ( response ) ; org . junit . Assert . assertTrue ( response . isEmpty ( ) ) ; }
public void test() { try { Field fRaf = FileImageOutputStream . class . getDeclaredField ( "raf" ) ; code_block = IfStatement ; } catch ( Exception e ) { LOGGER . error ( "getFileDescriptor from FileImageOutputStream" , e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CommercePriceModifierServiceUtil . class , "deleteCommercePriceModifier" , _deleteCommercePriceModifierParameterTypes1 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , commercePriceModifierId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . commerce . pricing . model . CommercePriceModifier ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( newFile . exists ( ) ) { logger . debug ( "delete uncomplated file {}" , newFile ) ; Files . delete ( newFile . toPath ( ) ) ; } }
public void test() { if ( ! newFile . createNewFile ( ) ) { logger . error ( "Create new TsFile {} failed because it exists" , newFile ) ; } }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; TsFileIOWriter writer = new TsFileIOWriter ( newFile ) ; code_block = IfStatement ; return writer ; } catch ( IOException e ) { logger . error ( "Create new TsFile {} failed " , newFile , e ) ; return null ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( String . format ( "http.proxy service is upgrading session %s" , session ) ) ; } }
private void loadLicenses ( String versionHome ) { LOGGER . info ( MessageFormat . format ( "Loading licenses into version home: {0}" , versionHome ) ) ; String licensesSource = repository + File . separator + "licenses" ; String licensesTarget = versionHome + File . separator + "licenses" ; FolderTools . copyFromFolderToFolder ( licensesSource , licensesTarget , true ) ; String licensesReportSource = repository + File . separator + "core" + File . separator + "java" + File . separator + "iesi-core" + File . separator + "target" + File . separator + "site" ; String licensesReportTarget = versionHome + File . separator + "licenses" + File . separator + "core" ; FolderTools . copyFromFolderToFolder ( licensesReportSource , licensesReportTarget , true ) ; }
public void test() { try { DistinctContinuationToken distinctContinuationToken = new DistinctContinuationToken ( serializedDistinctContinuationToken ) ; distinctContinuationToken . getSourceToken ( ) ; distinctContinuationToken . getLastHash ( ) ; outDistinctContinuationToken . v = distinctContinuationToken ; parsed = true ; } catch ( Exception ex ) { logger . debug ( "Received exception {} when trying to parse: {}" , ex . getMessage ( ) , serializedDistinctContinuationToken ) ; parsed = false ; outDistinctContinuationToken . v = null ; } }
public void test() { try { HBaseTestHelper . clearHBase ( ) ; TestHBasePutOperator thop = new TestHBasePutOperator ( ) ; thop . getStore ( ) . setTableName ( "table1" ) ; thop . getStore ( ) . setZookeeperQuorum ( "127.0.0.1" ) ; thop . getStore ( ) . setZookeeperClientPort ( 2181 ) ; HBaseTuple t1 = new HBaseTuple ( ) ; t1 . setColFamily ( "colfam0" ) ; t1 . setColName ( "street" ) ; t1 . setRow ( "row1" ) ; t1 . setColValue ( "ts" ) ; HBaseTuple t2 = new HBaseTuple ( ) ; t2 . setColFamily ( "colfam0" ) ; t2 . setColName ( "city" ) ; t2 . setRow ( "row2" ) ; t2 . setColValue ( "tc" ) ; thop . beginWindow ( 0 ) ; thop . input . process ( t1 ) ; AttributeMap . DefaultAttributeMap attributeMap = new AttributeMap . DefaultAttributeMap ( ) ; attributeMap . put ( OperatorContext . PROCESSING_MODE , ProcessingMode . AT_MOST_ONCE ) ; thop . setup ( mockOperatorContext ( 0 , attributeMap ) ) ; thop . input . process ( t2 ) ; thop . endWindow ( ) ; HBaseTuple tuple ; HBaseTuple tuple2 ; tuple = HBaseTestHelper . getHBaseTuple ( "row1" , "colfam0" , "street" ) ; tuple2 = HBaseTestHelper . getHBaseTuple ( "row2" , "colfam0" , "city" ) ; Assert . assertNull ( "Tuple" , tuple ) ; Assert . assertNotNull ( "Tuple2" , tuple2 ) ; Assert . assertEquals ( "Tuple row" , tuple2 . getRow ( ) , "row2" ) ; Assert . assertEquals ( "Tuple column family" , tuple2 . getColFamily ( ) , "colfam0" ) ; Assert . assertEquals ( "Tuple column name" , tuple2 . getColName ( ) , "city" ) ; Assert . assertEquals ( "Tuple column value" , tuple2 . getColValue ( ) , "tc" ) ; } catch ( IOException e ) { logger . error ( e . getMessage ( ) ) ; } }
public void test() { try { configManager . createConfig ( ZK_PROJECT , buildKey ( database ) , database ) ; } catch ( Throwable t ) { logger . warn ( "failed to create config" ) ; } finally { hostListString = "" ; } }
public void test() { try { SAMLBindings binding = isGet ? SAMLBindings . HTTP_REDIRECT : SAMLBindings . HTTP_POST ; LogoutRequestDocument reqDoc = LogoutRequestDocument . Factory . parse ( samlRequest ) ; SAMLVerifiableElement verifiableMessage = binding == SAMLBindings . HTTP_REDIRECT ? new RedirectedMessage ( httpReq . getQueryString ( ) ) : new XMLExpandedMessage ( reqDoc , reqDoc . getLogoutRequest ( ) ) ; SAMLMessage < LogoutRequestDocument > requestMessage = new SAMLMessage < > ( verifiableMessage , relayState , binding , reqDoc ) ; logoutProcessor . handleAsyncLogoutFromSAML ( requestMessage , httpResp ) ; } catch ( XmlException e ) { log . warn ( "Got a request to the SAML Single Logout endpoint, " + "with invalid request (XML is broken)" , e ) ; httpResp . sendError ( HttpServletResponse . SC_BAD_REQUEST , "Invalid SLO request (XML is malformed)" ) ; return ; } catch ( EopException e ) { } }
public void test() { if ( "localhost" . equalsIgnoreCase ( endpoint . getUri ( ) . getHost ( ) ) ) { LOG . warn ( "You use localhost interface! It means that no external connections will be available." + " Don't you want to use 0.0.0.0 instead (all network interfaces)?" ) ; } }
protected void execQueryUsingH2 ( String queryFolder , boolean needSort ) throws Exception { logger . info ( "---------- Running H2 queries: " + queryFolder ) ; List < File > sqlFiles = getFilesFromFolder ( new File ( queryFolder ) , ".sql" ) ; code_block = ForStatement ; }
public void test() { for ( File sqlFile : sqlFiles ) { String queryName = StringUtils . split ( sqlFile . getName ( ) , '.' ) [ 0 ] ; String sql = getTextFromFile ( sqlFile ) ; logger . info ( "Query Result from H2 - " + queryName ) ; executeQuery ( newH2Connection ( ) , queryName , sql , needSort ) ; } }
protected void writeProtocolVersion ( ) { appendBytes ( msg . getProtocolVersion ( ) . getValue ( ) ) ; LOGGER . debug ( "ProtocolVersion: " + ArrayConverter . bytesToHexString ( msg . getProtocolVersion ( ) . getValue ( ) ) ) ; }
public void test() { try { callback . onFailure ( failure ) ; } catch ( Throwable throwable ) { LOG . error ( "[{}] Unexpected error while failing {}" , logPrefix , callback , throwable ) ; } }
@ Override public Iterable < EntityBody > list ( NeutralQuery neutralQuery ) { LOG . debug ( ">>>BasicService.list(neutralQuery)" ) ; listSecurityCheck ( neutralQuery ) ; return listImplementationAfterSecurityChecks ( neutralQuery ) ; }
public void test() { try { loadData ( data , levelDatPath ) ; } catch ( IOException e ) { log . warn ( "Unable to load level.dat file, attempting to load backup." ) ; loadData ( data , levelDatOldPath ) ; } }
public void test() { if ( overallUsage >= 1 ) { LOG . warn ( "Cluster is overused, {}" , overallUsage ) ; } }
public void test() { try { com . liferay . layout . page . template . model . LayoutPageTemplateEntry returnValue = LayoutPageTemplateEntryServiceUtil . addLayoutPageTemplateEntry ( groupId , layoutPageTemplateCollectionId , classNameId , classTypeId , name , masterLayoutPlid , status , serviceContext ) ; return com . liferay . layout . page . template . model . LayoutPageTemplateEntrySoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try { uri = new URI ( service . configDescriptionURI ) ; } catch ( URISyntaxException e ) { logger . warn ( "Not a valid URI: {}" , service . configDescriptionURI ) ; return properties ; } }
@ Test ( enabled = false ) public void testSqoopImportUsingDefaultCredential ( ) throws Exception { TestContext context = new TestContext ( ) ; Map < String , String > overlay = context . getUniqueOverlay ( ) ; String filePath = TestContext . overlayParametersOverTemplate ( TestContext . CLUSTER_TEMPLATE , overlay ) ; context . setCluster ( filePath ) ; LOG . info ( "entity -submit -type cluster -file " + filePath ) ; Assert . assertEquals ( TestContext . executeWithURL ( "entity -submit -type cluster -file " + filePath ) , 0 ) ; String dsName = "datasource-test-4" ; overlay . put ( DATASOURCE_NAME_KEY , dsName ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . DATASOURCE_TEMPLATE4 , overlay ) ; LOG . info ( "Submit datatsource entity {} via entity -submit -type datasource -file {}" , dsName , filePath ) ; Assert . assertEquals ( TestContext . executeWithURL ( "entity -submit -type datasource -file " + filePath ) , 0 ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . FEED_TEMPLATE3 , overlay ) ; LOG . info ( "Submit import feed with datasource {} via entity -submitAndSchedule -type feed -file {}" , dsName , filePath ) ; Assert . assertEquals ( 0 , TestContext . executeWithURL ( "entity -submitAndSchedule -type feed -file " + filePath ) ) ; }
@ Test ( enabled = false ) public void testSqoopImportUsingDefaultCredential ( ) throws Exception { TestContext context = new TestContext ( ) ; Map < String , String > overlay = context . getUniqueOverlay ( ) ; String filePath = TestContext . overlayParametersOverTemplate ( TestContext . CLUSTER_TEMPLATE , overlay ) ; context . setCluster ( filePath ) ; LOG . info ( "entity -submit -type cluster -file " + filePath ) ; Assert . assertEquals ( TestContext . executeWithURL ( "entity -submit -type cluster -file " + filePath ) , 0 ) ; String dsName = "datasource-test-4" ; overlay . put ( DATASOURCE_NAME_KEY , dsName ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . DATASOURCE_TEMPLATE4 , overlay ) ; LOG . info ( "Submit datatsource entity {} via entity -submit -type datasource -file {}" , dsName , filePath ) ; Assert . assertEquals ( TestContext . executeWithURL ( "entity -submit -type datasource -file " + filePath ) , 0 ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . FEED_TEMPLATE3 , overlay ) ; LOG . info ( "Submit import feed with datasource {} via entity -submitAndSchedule -type feed -file {}" , dsName , filePath ) ; Assert . assertEquals ( 0 , TestContext . executeWithURL ( "entity -submitAndSchedule -type feed -file " + filePath ) ) ; }
@ Test ( enabled = false ) public void testSqoopImportUsingDefaultCredential ( ) throws Exception { TestContext context = new TestContext ( ) ; Map < String , String > overlay = context . getUniqueOverlay ( ) ; String filePath = TestContext . overlayParametersOverTemplate ( TestContext . CLUSTER_TEMPLATE , overlay ) ; context . setCluster ( filePath ) ; LOG . info ( "entity -submit -type cluster -file " + filePath ) ; Assert . assertEquals ( TestContext . executeWithURL ( "entity -submit -type cluster -file " + filePath ) , 0 ) ; String dsName = "datasource-test-4" ; overlay . put ( DATASOURCE_NAME_KEY , dsName ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . DATASOURCE_TEMPLATE4 , overlay ) ; LOG . info ( "Submit datatsource entity {} via entity -submit -type datasource -file {}" , dsName , filePath ) ; Assert . assertEquals ( TestContext . executeWithURL ( "entity -submit -type datasource -file " + filePath ) , 0 ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . FEED_TEMPLATE3 , overlay ) ; LOG . info ( "Submit import feed with datasource {} via entity -submitAndSchedule -type feed -file {}" , dsName , filePath ) ; Assert . assertEquals ( 0 , TestContext . executeWithURL ( "entity -submitAndSchedule -type feed -file " + filePath ) ) ; }
@ Override public void onFailure ( Throwable arg0 ) { LOG . error ( "Failure reading ActionDefinition {}" , id . getValue ( ) ) ; }
private void updateSelectedObject ( OWLObject selObj ) { selectedObject = selObj ; updateLastSelection ( ) ; logger . debug ( "Set the selected object to: {}" , selObj ) ; fireSelectionChanged ( ) ; }
public void test() { try { Git git = ( ( JGitPathImpl ) pathUtil . convert ( repository . getDefaultBranch ( ) . get ( ) . getPath ( ) ) ) . getFileSystem ( ) . getGit ( ) ; new RemoveRemote ( git , "origin" , REMOTE_ORIGIN_REF ) . execute ( ) ; } catch ( GitException e ) { log . warn ( "Error cleaning up origin for repository '{}': {}" , repository . getAlias ( ) , e ) ; } }
public void test() { try { long totalRowsCount = 0 ; final HarvestStatusQueryBuilder harvestStatusQueryBuilder = buildSqlQuery ( query , true ) ; log . debug ( "Unpopulated query is {}." , harvestStatusQueryBuilder ) ; s = harvestStatusQueryBuilder . getPopulatedStatement ( c ) ; log . debug ( "Query is {}." , s ) ; ResultSet res = s . executeQuery ( ) ; res . next ( ) ; totalRowsCount = res . getLong ( 1 ) ; s = buildSqlQuery ( query , false ) . getPopulatedStatement ( c ) ; res = s . executeQuery ( ) ; List < JobStatusInfo > jobs = makeJobStatusInfoListFromResultset ( res ) ; log . debug ( "Harveststatus constructed based on given query." ) ; return new HarvestStatus ( totalRowsCount , jobs ) ; } catch ( SQLException e ) { String message = "SQL error asking for job status list in database" + "\n" + ExceptionUtils . getSQLExceptionCause ( e ) ; log . warn ( message , e ) ; throw new IOFailure ( message , e ) ; } finally { HarvestDBConnection . release ( c ) ; } }
public void test() { try { long totalRowsCount = 0 ; final HarvestStatusQueryBuilder harvestStatusQueryBuilder = buildSqlQuery ( query , true ) ; log . debug ( "Unpopulated query is {}." , harvestStatusQueryBuilder ) ; s = harvestStatusQueryBuilder . getPopulatedStatement ( c ) ; log . debug ( "Query is {}." , s ) ; ResultSet res = s . executeQuery ( ) ; res . next ( ) ; totalRowsCount = res . getLong ( 1 ) ; s = buildSqlQuery ( query , false ) . getPopulatedStatement ( c ) ; res = s . executeQuery ( ) ; List < JobStatusInfo > jobs = makeJobStatusInfoListFromResultset ( res ) ; log . debug ( "Harveststatus constructed based on given query." ) ; return new HarvestStatus ( totalRowsCount , jobs ) ; } catch ( SQLException e ) { String message = "SQL error asking for job status list in database" + "\n" + ExceptionUtils . getSQLExceptionCause ( e ) ; log . warn ( message , e ) ; throw new IOFailure ( message , e ) ; } finally { HarvestDBConnection . release ( c ) ; } }
public void test() { try { long totalRowsCount = 0 ; final HarvestStatusQueryBuilder harvestStatusQueryBuilder = buildSqlQuery ( query , true ) ; log . debug ( "Unpopulated query is {}." , harvestStatusQueryBuilder ) ; s = harvestStatusQueryBuilder . getPopulatedStatement ( c ) ; log . debug ( "Query is {}." , s ) ; ResultSet res = s . executeQuery ( ) ; res . next ( ) ; totalRowsCount = res . getLong ( 1 ) ; s = buildSqlQuery ( query , false ) . getPopulatedStatement ( c ) ; res = s . executeQuery ( ) ; List < JobStatusInfo > jobs = makeJobStatusInfoListFromResultset ( res ) ; log . debug ( "Harveststatus constructed based on given query." ) ; return new HarvestStatus ( totalRowsCount , jobs ) ; } catch ( SQLException e ) { String message = "SQL error asking for job status list in database" + "\n" + ExceptionUtils . getSQLExceptionCause ( e ) ; log . warn ( message , e ) ; throw new IOFailure ( message , e ) ; } finally { HarvestDBConnection . release ( c ) ; } }
public void test() { try { long totalRowsCount = 0 ; final HarvestStatusQueryBuilder harvestStatusQueryBuilder = buildSqlQuery ( query , true ) ; log . debug ( "Unpopulated query is {}." , harvestStatusQueryBuilder ) ; s = harvestStatusQueryBuilder . getPopulatedStatement ( c ) ; log . debug ( "Query is {}." , s ) ; ResultSet res = s . executeQuery ( ) ; res . next ( ) ; totalRowsCount = res . getLong ( 1 ) ; s = buildSqlQuery ( query , false ) . getPopulatedStatement ( c ) ; res = s . executeQuery ( ) ; List < JobStatusInfo > jobs = makeJobStatusInfoListFromResultset ( res ) ; log . debug ( "Harveststatus constructed based on given query." ) ; return new HarvestStatus ( totalRowsCount , jobs ) ; } catch ( SQLException e ) { String message = "SQL error asking for job status list in database" + "\n" + ExceptionUtils . getSQLExceptionCause ( e ) ; log . warn ( message , e ) ; throw new IOFailure ( message , e ) ; } finally { HarvestDBConnection . release ( c ) ; } }
public void test() { try { in . close ( ) ; } catch ( IOException ex ) { LOGGER . trace ( "Ignorable error" , ex ) ; } }
public void test() { try { CnATreeElement personConfiguration = ( CnATreeElement ) configurationCurrent . getPerson ( ) ; code_block = IfStatement ; } catch ( Exception e ) { log . error ( "Error" , e ) ; } }
public void test() { if ( isEnabled ( ) ) { Stopwatch stopwatch = Stopwatch . createStarted ( ) ; timer . record ( ( ) -> jdbcTemplate . execute ( SQL , callback ( topicMessages ) ) ) ; log . info ( "Finished notifying {} messages in {}" , topicMessages . size ( ) , stopwatch ) ; } }
public void test() { try { stopKafkaServer ( ) ; stopZookeeper ( ) ; } catch ( Exception ex ) { logger . debug ( "LSHIL {}" , ex . getLocalizedMessage ( ) ) ; } }
public boolean hasPerfectConfiguration ( IAtom atom , IAtomContainer ac ) throws CDKException { double bondOrderSum = ac . getBondOrderSum ( atom ) ; IBond . Order maxBondOrder = ac . getMaximumBondOrder ( atom ) ; IAtomType [ ] atomTypes = getAtomTypeFactory ( atom . getBuilder ( ) ) . getAtomTypes ( atom . getSymbol ( ) ) ; if ( atomTypes . length == 0 ) return true ; logger . debug ( "*** Checking for perfect configuration ***" ) ; code_block = TryStatement ;  code_block = ForStatement ; code_block = TryStatement ;  return false ; }
public void test() { try { logger . debug ( "Checking configuration of atom " + ac . indexOf ( atom ) ) ; logger . debug ( "Atom has bondOrderSum = " + bondOrderSum ) ; logger . debug ( "Atom has max = " + bondOrderSum ) ; } catch ( Exception exc ) { } }
public void test() { try { logger . debug ( "Checking configuration of atom " + ac . indexOf ( atom ) ) ; logger . debug ( "Atom has bondOrderSum = " + bondOrderSum ) ; logger . debug ( "Atom has max = " + bondOrderSum ) ; } catch ( Exception exc ) { } }
public void test() { try { logger . debug ( "Checking configuration of atom " + ac . indexOf ( atom ) ) ; logger . debug ( "Atom has bondOrderSum = " + bondOrderSum ) ; logger . debug ( "Atom has max = " + bondOrderSum ) ; } catch ( Exception exc ) { } }
public void test() { try { logger . debug ( "Atom " + ac . indexOf ( atom ) + " has perfect configuration" ) ; } catch ( Exception exc ) { } }
public void test() { try { logger . debug ( "*** Atom " + ac . indexOf ( atom ) + " has imperfect configuration ***" ) ; } catch ( Exception exc ) { } }
public void test() { try { code_block = WhileStatement ; code_block = ForStatement ; } catch ( Throwable t ) { LOGGER . error ( "run() exiting due to uncaught error" , t ) ; } finally { TThreadedSelectorServer . this . stop ( ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( BlogsEntryServiceUtil . class , "getGroupEntriesCount" , _getGroupEntriesCountParameterTypes14 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , displayDate , status ) ; Object returnObj = null ; code_block = TryStatement ;  return ( ( Integer ) returnObj ) . intValue ( ) ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( tmpHqlPath != null && logger . isDebugEnabled ( ) ) { logger . debug ( "The SQL to execute in beeline: {} \n" , hql ) ; } }
public void test() { try { code_block = IfStatement ; final InvokeTraceable listener = ( InvokeTraceable ) adviceListener ; listener . invokeBeforeTracing ( classLoader , owner , methodName , methodDesc , Integer . parseInt ( info [ 3 ] ) ) ; } catch ( Throwable e ) { logger . error ( "class: {}, invokeInfo: {}" , clazz . getName ( ) , invokeInfo , e ) ; } }
public void sleep ( ) throws InterruptedException { long sleepMs = sleepPerConnectionMillis * getConnections ( ) ; logger . info ( "" ) ; logger . info ( "Created total {} connections with {} deployed clients, waiting {} s for system to react" , getConnections ( ) , getClients ( ) , sleepMs / 1000 ) ; logger . info ( "" ) ; Thread . sleep ( sleepMs ) ; }
public void test() { if ( function != null && logger . isInfoEnabled ( ) ) { logger . info ( "Located function " + function . getFunctionDefinition ( ) ) ; } }
public void test() { if ( ! data . isNamed ( ) ) { this . logger . error ( ERROR_MESSAGE_DATA_IN_MEMORY_IN_WRONG_FORMAT ) ; return ; } }
public void test() { try { final XWikiContext context = this . xcontextProvider . get ( ) ; final BaseObject xobject = patient . getXDocument ( ) . getXObject ( Patient . CLASS_REFERENCE , true , context ) ; final PatientData < List < VocabularyTerm > > data = patient . getData ( getName ( ) ) ; code_block = IfStatement ; } catch ( final Exception ex ) { this . logger . error ( "Failed to save global qualifiers data: {}" , ex . getMessage ( ) , ex ) ; } }
public void test() { try { field = TaskRunner . class . getDeclaredField ( "result" ) ; field . setAccessible ( true ) ; } catch ( Exception e ) { LOG . fatal ( "Can't access to TaskResult at " + TaskRunner . class . getName ( ) + "!" ) ; throw new RuntimeException ( "Incompatible Hive API found!" , e ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( exception , exception ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Throwable e ) { log . warn ( "sink task stop error while closing connection to {}" , "jdbc" , e ) ; } }
public void test() { try { event = pluggableSearchEngine . acceptEvent ( event , version ) ; } catch ( Exception ex ) { logger . error ( ex . getMessage ( ) , ex ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( "Response XML\n" + xml ) ; } }
public void test() { if ( _log . isWarnEnabled ( ) ) { _log . warn ( exception , exception ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( "No resource found to " + storage . getRootPath ( ) + webDAVRequest . getPath ( ) ) ; } }
public void test() { if ( _log . isWarnEnabled ( ) ) { _log . warn ( exception , exception ) ; } }
@ Override public boolean isSameFile ( FileObject a , FileObject b ) { logger . debug ( "isSameFile({},{})" , a , b ) ; return a . equals ( b ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( String . format ( "Send command %s" , command ) ) ; } }
public void test() { if ( log . isInfoEnabled ( ) ) { log . info ( String . format ( "Command %s returned no errors" , command ) ) ; } }
public void test() { if ( _log . isWarnEnabled ( ) ) { _log . warn ( noSuchFolderException , noSuchFolderException ) ; } }
public void test() { if ( pinDef . getMode ( ) == "OUTPUT" ) { code_block = IfStatement ; } else { log . error ( "Can't write to a pin in input mode. Change direction to OUTPUT ({}) with pinMode first." , OUTPUT ) ; } }
void cleanupTempResource ( BigQueryOptions bqOptions , String stepUuid ) throws Exception { Optional < String > queryTempDatasetOpt = Optional . ofNullable ( tempDatasetId ) ; TableReference tableToRemove = createTempTableReference ( bqOptions . getProject ( ) , BigQueryResourceNaming . createJobIdPrefix ( bqOptions . getJobName ( ) , stepUuid , JobType . QUERY ) , queryTempDatasetOpt ) ; BigQueryServices . DatasetService tableService = bqServices . getDatasetService ( bqOptions ) ; LOG . info ( "Deleting temporary table with query results {}" , tableToRemove ) ; tableService . deleteTable ( tableToRemove ) ; boolean datasetCreatedByBeam = ! queryTempDatasetOpt . isPresent ( ) ; code_block = IfStatement ; }
public void test() { try { OpenIdConnectConfiguration openIdConfiguration = _configurationProvider . getConfiguration ( OpenIdConnectConfiguration . class , new CompanyServiceSettingsLocator ( companyId , OpenIdConnectConstants . SERVICE_NAME ) ) ; return openIdConfiguration . enabled ( ) ; } catch ( ConfigurationException configurationException ) { _log . error ( StringBundler . concat ( "Unable to get OpenId configuration for company " , companyId , ": " , configurationException . getMessage ( ) ) , configurationException ) ; } }
public void test() { try ( CliRunner runner = new CliRunner ( arguments ) ) { runner . run ( configuration ) ; } catch ( final Throwable e ) { logger . error ( "Error occurred while running DataCleaner command line mode" , e ) ; exitCode = 1 ; } finally { exitCommandLine ( configuration , exitCode ) ; } }
public void test() { try { verifyJarSignature ( certificate , jarFilePath ) ; return true ; } catch ( SecurityException | IOException e ) { LOG . debug ( "Certificate is not appropriate." + UpdaterUtil . getStringRepresentation ( certificate ) ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "sessionRemoved: " + session . getId ( ) + " timedout:" + timedout + " channels: " + channelsAsString ( session . getSubscriptions ( ) ) ) ; } }
@ Override public void exceptionCaught ( ChannelHandlerContext ctx , Throwable cause ) { logger . error ( "Exception in API http handler" , cause ) ; writeJsonResponse ( ctx , INTERNAL_SERVER_ERROR , INTERNAL_SERVER_ERROR_RESPONSE ) ; }
private void startDownload ( ) { logger . trace ( "starting modem database download" ) ; port . clearModemDB ( ) ; lastMessageTimestamp = System . currentTimeMillis ( ) ; messageCount = 0 ; getFirstLinkRecord ( ) ; }
public void test() { if ( val . isPresent ( ) ) { LOG . warn ( "Ignoring deprecated config value(s) on " + obj + " because contains value to " + "'" + key . getName ( ) + "', other deprecated name(s) present were: " + deprecatedValues . keySet ( ) ) ; } else-if ( deprecatedValues . size ( ) == 1 ) { LOG . warn ( "Using deprecated config value on " + obj + ", should use '" + key . getName ( ) + "', but used " + "'" + Iterables . getOnlyElement ( deprecatedValues . keySet ( ) ) + "'" ) ; } else { LOG . warn ( "Using deprecated config value on " + obj + ", should use '" + key . getName ( ) + "', but used " + "'" + Iterables . get ( deprecatedValues . keySet ( ) , 1 ) + "' and ignored values present for other " + "deprecated name(s) " + Iterables . skip ( deprecatedValues . keySet ( ) , 1 ) ) ; } }
public void test() { if ( val . isPresent ( ) ) { LOG . warn ( "Ignoring deprecated config value(s) on " + obj + " because contains value to " + "'" + key . getName ( ) + "', other deprecated name(s) present were: " + deprecatedValues . keySet ( ) ) ; } else-if ( deprecatedValues . size ( ) == 1 ) { LOG . warn ( "Using deprecated config value on " + obj + ", should use '" + key . getName ( ) + "', but used " + "'" + Iterables . getOnlyElement ( deprecatedValues . keySet ( ) ) + "'" ) ; } else { LOG . warn ( "Using deprecated config value on " + obj + ", should use '" + key . getName ( ) + "', but used " + "'" + Iterables . get ( deprecatedValues . keySet ( ) , 1 ) + "' and ignored values present for other " + "deprecated name(s) " + Iterables . skip ( deprecatedValues . keySet ( ) , 1 ) ) ; } }
public void test() { if ( val . isPresent ( ) ) { LOG . warn ( "Ignoring deprecated config value(s) on " + obj + " because contains value to " + "'" + key . getName ( ) + "', other deprecated name(s) present were: " + deprecatedValues . keySet ( ) ) ; } else-if ( deprecatedValues . size ( ) == 1 ) { LOG . warn ( "Using deprecated config value on " + obj + ", should use '" + key . getName ( ) + "', but used " + "'" + Iterables . getOnlyElement ( deprecatedValues . keySet ( ) ) + "'" ) ; } else { LOG . warn ( "Using deprecated config value on " + obj + ", should use '" + key . getName ( ) + "', but used " + "'" + Iterables . get ( deprecatedValues . keySet ( ) , 1 ) + "' and ignored values present for other " + "deprecated name(s) " + Iterables . skip ( deprecatedValues . keySet ( ) , 1 ) ) ; } }
public void test() { try { HttpHelper . streamURLToFile ( address , fileOnDisk ) ; } catch ( FrameworkException ex ) { logger . warn ( null , ex ) ; } }
public void test() { try { code_block = IfStatement ; PageModel model = event . getPageModel ( ) ; String pageModelCode = ( null != model ) ? model . getCode ( ) : null ; code_block = IfStatement ; } catch ( Throwable t ) { _logger . error ( "Error during refres pages" , t ) ; } }
public void setSortByRelevanceFeatureProperty ( String relevanceFeatureProperty ) { LOGGER . debug ( "Setting sortByRelevanceFeatureProperty to: {}" , relevanceFeatureProperty ) ; this . sortByRelevanceFeatureProperty = relevanceFeatureProperty ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( _NO_SUCH_ENTITY_WITH_PRIMARY_KEY + primaryKey ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { long delta = TimeUnit . NANOSECONDS . toMicros ( System . nanoTime ( ) - start ) ; LOGGER . debug ( "Applied feature actions to " + ids . size ( ) + " elements in " + delta + " Î¼s" ) ; } }
public void test() { if ( loc == null ) { log . warn ( "" + this + " call to unmanage null location; skipping" , new IllegalStateException ( "source of null unmanagement call to " + this ) ) ; return true ; } }
public void persist ( StgMbZielobjSubtypTxt transientInstance ) { log . debug ( "persisting StgMbZielobjSubtypTxt instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . persist ( transientInstance ) ; log . debug ( "persist successful" ) ; } catch ( RuntimeException re ) { log . error ( "persist failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . persist ( transientInstance ) ; log . debug ( "persist successful" ) ; } catch ( RuntimeException re ) { log . error ( "persist failed" , re ) ; throw re ; } }
public void test() { try { cmsInfo = ( ClusterManagementServiceInfo ) client . requestToServer ( new HostAndPort ( locator . getHostString ( ) , locator . getPort ( ) ) , new ClusterManagementServiceInfoRequest ( ) , 1000 , true ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . warn ( "unable to discover the ClusterManagementService on locator " + locator . toString ( ) ) ; } }
public void test() { if ( authentication . authenticate ( tc ) ) { tokenCredentials = tc ; tokenInfo = authentication . getTokenInfo ( ) ; principal = authentication . getUserPrincipal ( ) ; log . debug ( "Login: adding login name to shared state." ) ; sharedState . put ( SHARED_KEY_LOGIN_NAME , tokenInfo . getUserId ( ) ) ; return true ; } }
public void test() { if ( cacheChangeFailureMsgSent ) return ; FinishState finishState0 ; synchronized ( mux ) { finishState0 = finishState ; } }
public void test() { if ( ! TOKENS_NT_NAME . equals ( nt ) ) { log . debug ( "Unexpected node type of .tokens node {}." , nt ) ; } }
public void persist ( StgMbBauMasGef transientInstance ) { log . debug ( "persisting StgMbBauMasGef instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . persist ( transientInstance ) ; log . debug ( "persist successful" ) ; } catch ( RuntimeException re ) { log . error ( "persist failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . persist ( transientInstance ) ; log . debug ( "persist successful" ) ; } catch ( RuntimeException re ) { log . error ( "persist failed" , re ) ; throw re ; } }
public void test() { try ( CheckedInputStream snapIS = SnapStream . getInputStream ( snap ) ) { InputArchive ia = BinaryInputArchive . getArchive ( snapIS ) ; deserialize ( dt , sessions , ia ) ; SnapStream . checkSealIntegrity ( snapIS , ia ) ; code_block = IfStatement ; foundValid = true ; break ; } catch ( IOException e ) { LOG . warn ( "problem reading snap file {}" , snap , e ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Committing JDBC Connection [" + connection + "]" ) ; } }
@ Override public void process ( Exchange exchange ) throws Exception { Message in = exchange . getIn ( ) ; PullRequest pullRequest = ( PullRequest ) in . getBody ( ) ; User pullRequestUser = pullRequest . getUser ( ) ; pullRequest . getTitle ( ) ; pullRequest . getHtmlUrl ( ) ; pullRequest . getUser ( ) . getLogin ( ) ; pullRequest . getUser ( ) . getHtmlUrl ( ) ; LOG . debug ( "Got PullRequest " + pullRequest . getHtmlUrl ( ) + " [" + pullRequest . getTitle ( ) + "] From " + pullRequestUser . getLogin ( ) ) ; }
public List < Issue > getIssues ( String project , List < String > labels , String reporter , long lookBackMillis ) { List < Issue > issues = new ArrayList < > ( ) ; StringBuilder jiraQuery = new StringBuilder ( ) ; jiraQuery . append ( "project=" ) . append ( project ) ; jiraQuery . append ( " and " ) . append ( "reporter IN (\"" ) . append ( reporter ) . append ( "\")" ) ; jiraQuery . append ( " and " ) . append ( buildQueryOnLabels ( labels ) ) ; jiraQuery . append ( " and " ) . append ( buildQueryOnCreatedBy ( lookBackMillis ) ) ; Iterable < Issue > jiraIssuesIt = restClient . getSearchClient ( ) . searchJql ( jiraQuery . toString ( ) ) . claim ( ) . getIssues ( ) ; jiraIssuesIt . forEach ( issues :: add ) ; LOG . info ( "Fetched {} Jira tickets using query - {}" , issues . size ( ) , jiraQuery . toString ( ) ) ; return issues ; }
public void test() { try { code_block = IfStatement ; this . getSocialActivityStreamManager ( ) . deleteActionCommentRecord ( commentId , recordId ) ; ActionLogRecordDto dto = this . getDtoBuilder ( ) . toDto ( this . getActionLogManager ( ) . getActionRecord ( recordId ) , this . getSocialActivityStreamManager ( ) . getActionLikeRecords ( recordId ) , this . getSocialActivityStreamManager ( ) . getActionCommentRecords ( recordId ) ) ; return dto ; } catch ( Throwable t ) { logger . error ( "error in delete comment for id {}" , recordId , t ) ; throw new RestServerError ( "error in remove comment" , t ) ; } }
public void test() { try { GetObjectRequest request = new GetObjectRequest ( this . bucketName , key ) ; request . setRange ( byteRangeStart , byteRangeEnd ) ; COSObject cosObject = ( COSObject ) this . callCOSClientWithRetry ( request ) ; return cosObject . getObjectContent ( ) ; } catch ( CosServiceException e ) { String errMsg = String . format ( "Retrieving key [%s] with byteRangeStart [%d] occurs " + "an CosServiceException: [%s]." , key , byteRangeStart , e . toString ( ) ) ; LOG . error ( errMsg ) ; handleException ( new Exception ( errMsg ) , key ) ; return null ; } catch ( CosClientException e ) { String errMsg = String . format ( "Retrieving key [%s] with byteRangeStart [%d] " + "occurs an exception: [%s]." , key , byteRangeStart , e . toString ( ) ) ; LOG . error ( "Retrieving COS key: [{}] with byteRangeStart: [{}] " + "occurs an exception: [{}]." , key , byteRangeStart , e ) ; handleException ( new Exception ( errMsg ) , key ) ; } }
public void test() { try { GetObjectRequest request = new GetObjectRequest ( this . bucketName , key ) ; request . setRange ( byteRangeStart , byteRangeEnd ) ; COSObject cosObject = ( COSObject ) this . callCOSClientWithRetry ( request ) ; return cosObject . getObjectContent ( ) ; } catch ( CosServiceException e ) { String errMsg = String . format ( "Retrieving key [%s] with byteRangeStart [%d] occurs " + "an CosServiceException: [%s]." , key , byteRangeStart , e . toString ( ) ) ; LOG . error ( errMsg ) ; handleException ( new Exception ( errMsg ) , key ) ; return null ; } catch ( CosClientException e ) { String errMsg = String . format ( "Retrieving key [%s] with byteRangeStart [%d] " + "occurs an exception: [%s]." , key , byteRangeStart , e . toString ( ) ) ; LOG . error ( "Retrieving COS key: [{}] with byteRangeStart: [{}] " + "occurs an exception: [{}]." , key , byteRangeStart , e ) ; handleException ( new Exception ( errMsg ) , key ) ; } }
private Decision checkAppLevelThrottled ( String throttleKey , String tier ) { Decision decision = dataHolder . isThrottled ( throttleKey ) ; log . debug ( "Application Level throttle decision is {} for key:tier {}:{}" , decision . isThrottled ( ) , throttleKey , tier ) ; return decision ; }
@ Override public void onNext ( final SuspendEvent suspendEvent ) { LOG . info ( "NoopTask.TaskSuspendHandler.send() invoked." ) ; NoopTask . this . stopTask ( ) ; }
@ Override public void actionDone ( ) { logger . debug ( "[actionDone][already done]{}" , migrationState ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "TX: Committing: {}" , txId ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "TX: found a previously committed transaction:{}" , txId ) ; } }
private void logAndWait ( CompleteUpdate update , CompleteUpdate waitFor ) throws OwsExceptionReport { LOGGER . trace ( "{} waiting for {}" , update , waitFor ) ; waitFor . waitForCompletion ( ) ; LOGGER . trace ( "{} stopped waiting for {}" , update , waitFor ) ; }
private void logAndWait ( CompleteUpdate update , CompleteUpdate waitFor ) throws OwsExceptionReport { LOGGER . trace ( "{} waiting for {}" , update , waitFor ) ; waitFor . waitForCompletion ( ) ; LOGGER . trace ( "{} stopped waiting for {}" , update , waitFor ) ; }
public void test() { if ( expectedLanguage == null ) { logger . info ( "IGNORE (null expected): {}" , logmessage ) ; } else-if ( expectedLanguage . equalsIgnoreCase ( detectedLanguage ) ) { logger . debug ( logmessage ) ; } else { logger . warn ( logmessage ) ; } }
public void test() { if ( expectedLanguage == null ) { logger . info ( "IGNORE (null expected): {}" , logmessage ) ; } else-if ( expectedLanguage . equalsIgnoreCase ( detectedLanguage ) ) { logger . debug ( logmessage ) ; } else { logger . warn ( logmessage ) ; } }
public void test() { if ( expectedLanguage == null ) { logger . info ( "IGNORE (null expected): {}" , logmessage ) ; } else-if ( expectedLanguage . equalsIgnoreCase ( detectedLanguage ) ) { logger . debug ( logmessage ) ; } else { logger . warn ( logmessage ) ; } }
public void test() { if ( detectedLanguages . size ( ) == 0 ) { logger . warn ( "no language detected for {}" , textualQuestion ) ; } else-if ( detectedLanguages . size ( ) > 1 ) { logger . warn ( "many ({}) languages detected for {}" , detectedLanguages . size ( ) , textualQuestion ) ; } else { } }
public void test() { if ( detectedLanguages . size ( ) == 0 ) { logger . warn ( "no language detected for {}" , textualQuestion ) ; } else-if ( detectedLanguages . size ( ) > 1 ) { logger . warn ( "many ({}) languages detected for {}" , detectedLanguages . size ( ) , textualQuestion ) ; } else { } }
public StgNZielobjektRollen merge ( StgNZielobjektRollen detachedInstance ) { log . debug ( "merging StgNZielobjektRollen instance" ) ; code_block = TryStatement ;  }
public void test() { try { StgNZielobjektRollen result = ( StgNZielobjektRollen ) sessionFactory . getCurrentSession ( ) . merge ( detachedInstance ) ; log . debug ( "merge successful" ) ; return result ; } catch ( RuntimeException re ) { log . error ( "merge failed" , re ) ; throw re ; } }
public void test() { try { StgNZielobjektRollen result = ( StgNZielobjektRollen ) sessionFactory . getCurrentSession ( ) . merge ( detachedInstance ) ; log . debug ( "merge successful" ) ; return result ; } catch ( RuntimeException re ) { log . error ( "merge failed" , re ) ; throw re ; } }
@ Override public IBond getBond ( IAtom atom1 , IAtom atom2 ) { logger . debug ( "Getting bond for atoms: atom1=" + atom1 , " atom2=" + atom2 ) ; return super . getBond ( atom1 , atom2 ) ; }
public void test() { if ( content == null ) { log . warn ( "Missing request content in context, skipping stripWhitespaces" ) ; } else { content = XmlUtils . stripWhitespaces ( content ) ; context . setProperty ( BaseHttpRequestTransport . REQUEST_CONTENT , content ) ; } }
public void test() { try { PortletJSONUtil . populatePortletJSONObject ( _httpServletRequest , StringPool . BLANK , portlet , jsonObject ) ; PortletJSONUtil . writeHeaderPaths ( pipingServletResponse , jsonObject ) ; } catch ( Exception exception ) { _log . error ( "Unable to write portlet footer paths " + portlet . getPortletId ( ) , exception ) ; } }
public void test() { try { Object nextItem = ( ( Task < ? > ) source ) . get ( ) ; code_block = IfStatement ; } catch ( InterruptedException e ) { throw Exceptions . propagate ( e ) ; } catch ( ExecutionException e ) { LOG . warn ( "Unexpected exception getting done (and non-error) task result to " + source + " " + getContextDescription ( context ) + "; continuing: " + e , e ) ; } }
public void test() { if ( ! loggedTaskWarning ) { LOG . warn ( "Intercepting and skipping request to serialize a Task" + getContextDescription ( context ) + " (only logging this once): " + source ) ; loggedTaskWarning = true ; } }
@ Test public void test_02 ( ) { Log . debug ( "Test" ) ; Itree intForest = newItree ( markers ) ; intForest . build ( ) ; Markers queries = createRandomLargeMarkers ( chromosome , 10000 ) ; int i = 0 ; int totalResults = 0 ; code_block = ForStatement ; Assert . assertTrue ( "Not a signle result found in all queries!" , totalResults > 0 ) ; System . err . println ( "" ) ; }
@ Test public void mailTest ( TestContext testContext ) { this . testContext = testContext ; StringBuilder sb = new StringBuilder ( "*******************************\n" ) ; code_block = ForStatement ; String message = sb . toString ( ) ; log . info ( "message size is " + message . length ( ) ) ; testException ( new MailMessage ( "user@example.com" , "user@example.com" , "Subject" , message ) ) ; }
public void test() { try { Future < AnalysisSubmission > cleanedSubmissionFuture = analysisExecutionService . cleanupSubmission ( submission ) ; cleanedSubmissions . add ( cleanedSubmissionFuture ) ; } catch ( ExecutionManagerException e ) { logger . error ( "Error cleaning submission " + submission , e ) ; } }
@ Test public void m_logArtifactsNegativeTest ( ) { LOGGER . info ( " Log Artifacts in Experiment Negative test start................................" ) ; List < Artifact > artifacts = new ArrayList < > ( ) ; Artifact artifact1 = Artifact . newBuilder ( ) . setKey ( "Google Pay Artifact " + Calendar . getInstance ( ) . getTimeInMillis ( ) ) . setPath ( "This is new added data artifact type in Google Pay artifact" ) . setArtifactType ( ArtifactType . MODEL ) . build ( ) ; artifacts . add ( artifact1 ) ; Artifact artifact2 = Artifact . newBuilder ( ) . setKey ( "Google Pay Artifact " + Calendar . getInstance ( ) . getTimeInMillis ( ) ) . setPath ( "This is new added data artifact type in Google Pay artifact" ) . setArtifactType ( ArtifactType . DATA ) . build ( ) ; artifacts . add ( artifact2 ) ; LogExperimentArtifacts logArtifactRequest = LogExperimentArtifacts . newBuilder ( ) . addAllArtifacts ( artifacts ) . build ( ) ; code_block = TryStatement ;  logArtifactRequest = LogExperimentArtifacts . newBuilder ( ) . setId ( "asda" ) . addAllArtifacts ( artifacts ) . build ( ) ; code_block = TryStatement ;  logArtifactRequest = LogExperimentArtifacts . newBuilder ( ) . setId ( experiment . getId ( ) ) . addAllArtifacts ( experiment . getArtifactsList ( ) ) . build ( ) ; code_block = TryStatement ;  LOGGER . info ( "Log Artifacts in Experiment tags Negative test stop................................" ) ; }
@ Test public void m_logArtifactsNegativeTest ( ) { LOGGER . info ( " Log Artifacts in Experiment Negative test start................................" ) ; List < Artifact > artifacts = new ArrayList < > ( ) ; Artifact artifact1 = Artifact . newBuilder ( ) . setKey ( "Google Pay Artifact " + Calendar . getInstance ( ) . getTimeInMillis ( ) ) . setPath ( "This is new added data artifact type in Google Pay artifact" ) . setArtifactType ( ArtifactType . MODEL ) . build ( ) ; artifacts . add ( artifact1 ) ; Artifact artifact2 = Artifact . newBuilder ( ) . setKey ( "Google Pay Artifact " + Calendar . getInstance ( ) . getTimeInMillis ( ) ) . setPath ( "This is new added data artifact type in Google Pay artifact" ) . setArtifactType ( ArtifactType . DATA ) . build ( ) ; artifacts . add ( artifact2 ) ; LogExperimentArtifacts logArtifactRequest = LogExperimentArtifacts . newBuilder ( ) . addAllArtifacts ( artifacts ) . build ( ) ; code_block = TryStatement ;  logArtifactRequest = LogExperimentArtifacts . newBuilder ( ) . setId ( "asda" ) . addAllArtifacts ( artifacts ) . build ( ) ; code_block = TryStatement ;  logArtifactRequest = LogExperimentArtifacts . newBuilder ( ) . setId ( experiment . getId ( ) ) . addAllArtifacts ( experiment . getArtifactsList ( ) ) . build ( ) ; code_block = TryStatement ;  LOGGER . info ( "Log Artifacts in Experiment tags Negative test stop................................" ) ; }
public void test() { try { StringBundler sb = new StringBundler ( 10 ) ; sb . append ( "<column><model><model-name>" ) ; sb . append ( "com.liferay.dynamic.data.mapping.model.DDMContent" ) ; sb . append ( "</model-name>" ) ; DDMFormValuesSerializerSerializeResponse ddmFormValuesSerializerSerializeResponse = _ddmFormValuesSerializer . serialize ( DDMFormValuesSerializerSerializeRequest . Builder . newBuilder ( ddmFormInstanceRecord . getDDMFormValues ( ) ) . build ( ) ) ; JSONObject dataJSONObject = JSONFactoryUtil . createJSONObject ( ddmFormValuesSerializerSerializeResponse . getContent ( ) ) ; JSONArray fieldValuesJSONArray = dataJSONObject . getJSONArray ( "fieldValues" ) ; fieldValuesJSONArray . forEach ( fieldValue code_block = LoopStatement ; ) ; sb . append ( "</model></column>" ) ; return sb . toString ( ) ; } catch ( PortalException portalException ) { _log . error ( "Unable to get field values from dynamic data mapping form " + "instance record " + ddmFormInstanceRecord . getFormInstanceRecordId ( ) , portalException ) ; } }
public void test() { if ( null != skipDryRun && skipDryRun ) { LOG . info ( "Skipping dryrun as directed by param in cli/RestApi." ) ; return ; } else { String skipDryRunStr = RuntimeProperties . get ( ) . getProperty ( FALCON_SKIP_DRYRUN , "false" ) . toLowerCase ( ) ; code_block = IfStatement ; } }
public void test() { if ( Boolean . valueOf ( skipDryRunStr ) ) { LOG . info ( "Skipping dryrun as directed by Runtime properties." ) ; return ; } }
public void test() { try { code_block = IfStatement ; } catch ( IOException e ) { LOG . trace ( "IGNORED" , e ) ; } }
public void test() { try { long configId = addConfigInfoAtomic ( - 1 , srcIp , srcUser , configInfo , time , configAdvanceInfo ) ; String configTags = configAdvanceInfo == null ? null : ( String ) configAdvanceInfo . get ( "config_tags" ) ; addConfigTagsRelation ( configId , configTags , configInfo . getDataId ( ) , configInfo . getGroup ( ) , configInfo . getTenant ( ) ) ; insertConfigHistoryAtomic ( 0 , configInfo , srcIp , srcUser , time , "I" ) ; } catch ( CannotGetJdbcConnectionException e ) { LogUtil . FATAL_LOG . error ( "[db-error] " + e . toString ( ) , e ) ; throw e ; } }
public void test() { try { WorkflowTrace trace = new WorkflowTrace ( ) ; AliasedConnection con = new OutboundConnection ( "theAlias" , 1111 , "host1111" ) ; trace . addConnection ( con ) ; action = new SendAction ( new ClientHelloMessage ( config ) ) ; action . setConnectionAlias ( con . getAlias ( ) ) ; trace . addTlsAction ( action ) ; StringWriter sw = new StringWriter ( ) ; PrintWriter pw = new PrintWriter ( sw ) ; pw . println ( "<workflowTrace>" ) ; pw . println ( "    <OutboundConnection>" ) ; pw . println ( "        <alias>theAlias</alias>" ) ; pw . println ( "        <port>1111</port>" ) ; pw . println ( "        <hostname>host1111</hostname>" ) ; pw . println ( "    </OutboundConnection>" ) ; pw . println ( "    <Send>" ) ; pw . println ( "        <messages>" ) ; pw . println ( "            <ClientHello>" ) ; pw . println ( "                <extensions>" ) ; pw . println ( "                    <ECPointFormat/>" ) ; pw . println ( "                    <EllipticCurves/>" ) ; pw . println ( "                    <SignatureAndHashAlgorithmsExtension/>" ) ; pw . println ( "                    <RenegotiationInfoExtension/>" ) ; pw . println ( "                </extensions>" ) ; pw . println ( "            </ClientHello>" ) ; pw . println ( "        </messages>" ) ; pw . println ( "    </Send>" ) ; pw . println ( "</workflowTrace>" ) ; pw . close ( ) ; String expected = sw . toString ( ) ; DefaultNormalizeFilter . normalizeAndFilter ( trace , config ) ; String actual = WorkflowTraceSerializer . write ( trace ) ; Assert . assertEquals ( expected , actual ) ; } catch ( JAXBException | IOException ex ) { LOGGER . error ( ex . getLocalizedMessage ( ) , ex ) ; Assert . fail ( ) ; } }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "{} REQ {} {} {} {} {}" , reqId , ctx . request ( ) . remoteAddress ( ) , tenant , ctx . request ( ) . method ( ) , ctx . request ( ) . path ( ) , mods ) ; } }
public void test() { for ( String namespace : namespaces ) { final Logger logger = Logging . getLogger ( namespace ) ; logger . severe ( "Don't worry, just a test" ) ; logger . warning ( "This is an imaginary warning" ) ; logger . info ( "This is a pseudo-information message" ) ; logger . config ( "Not really configuring anything..." ) ; logger . fine ( "This is a detailed (but useless) message\nWe log this one on two lines!" ) ; logger . finer ( "This is a debug message" ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( RejectedExecutionException e ) { LOG . warn ( "Can't update cache due to EventHub is too busy" ) ; } }
public void test() { if ( line == null || ( element . getVoltageLevelId ( ) != null && ! ( element . getVoltageLevelId ( ) . equals ( line . getTerminal1 ( ) . getVoltageLevel ( ) . getId ( ) ) || element . getVoltageLevelId ( ) . equals ( line . getTerminal2 ( ) . getVoltageLevel ( ) . getId ( ) ) ) ) ) { LOGGER . warn ( "Line '{}' of contingency '{}' not found" , element . getId ( ) , contingency . getId ( ) ) ; return false ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "==> mapStructValue({})" , ctx ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "<== mapStructValue({})" , ctx ) ; } }
@ Override public void log ( String dataSourceName , boolean isForceClosing , long startTimeMilliseconds ) { long cost = getElapsedMilliSeconds ( ) ; LOGGER . info ( String . format ( "**********DataSource %s has been closed,cost:%s ms.**********" , name , cost ) ) ; }
public void test() { try { DirUtils . deleteDirectoryContents ( stageDir . toFile ( ) ) ; } catch ( IOException e ) { LOG . warn ( "Exception raised while deleting directory {}" , stageDir , e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( IOException e ) { log . info ( "IO error occurred" , e ) ; return TeaVMProgressFeedback . CANCEL ; } }
public void test() { try { return partitionTable . partitionByPathTime ( new PartialPath ( path ) , 0 ) ; } catch ( MetadataException e ) { LOGGER . error ( "The storage group of path {} doesn't exist." , path , e ) ; return new PartitionGroup ( ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CPOptionServiceUtil . class , "fetchCPOption" , _fetchCPOptionParameterTypes3 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , cpOptionId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . commerce . product . model . CPOption ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceAccountServiceUtil . class , "getPersonalCommerceAccount" , _getPersonalCommerceAccountParameterTypes6 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , userId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . commerce . account . model . CommerceAccount ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( restoreEntryException , restoreEntryException ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Throwable t ) { log . warn ( "Error while closing the {} dialect: " , dialect . name ( ) , t ) ; } finally { dialect = null ; } }
public void test() { if ( _log . isTraceEnabled ( ) ) { StringBundler sb = new StringBundler ( 6 ) ; sb . append ( "Recommended item: " ) ; sb . append ( recommendedEntryClassPK ) ; sb . append ( " rank: " ) ; sb . append ( productContentCommerceMLRecommendation . getRank ( ) ) ; sb . append ( " score: " ) ; sb . append ( productContentCommerceMLRecommendation . getScore ( ) ) ; _log . trace ( sb . toString ( ) ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( portalException , portalException ) ; } }
public void test() { { log . warn ( "Server started with errors" ) ; setServerStopped ( ) ; service . unregisterAction ( getStartOK ( ) ) ; service . unregisterAction ( getStartError ( ) ) ; kill ( ) ; } }
public void test() { if ( isRunning ( ) ) { log . warn ( "Process is already running" ) ; return ; } }
@ Override public void destroy ( ) { LOGGER . debug ( "Destroying MessageListenerContainerRegistry" ) ; this . messageListenerContainerRegistry . destroy ( ) ; LOGGER . debug ( "Destroying ConnectionFactoryRegistry" ) ; this . connectionFactoryRegistry . destroy ( ) ; }
@ Override public void destroy ( ) { LOGGER . debug ( "Destroying MessageListenerContainerRegistry" ) ; this . messageListenerContainerRegistry . destroy ( ) ; LOGGER . debug ( "Destroying ConnectionFactoryRegistry" ) ; this . connectionFactoryRegistry . destroy ( ) ; }
public void test() { try { camundaClient . post ( requestClientParameter , orchestrationUri ) ; } catch ( ApiException e ) { logger . error ( "Error Calling Workflow Engine" , e ) ; throw new WorkflowEngineConnectionException ( "Error Calling Workflow Engine" , e ) ; } }
public Map < String , Object > createCustomSource ( String sourceName ) throws Exception { checkStarted ( ) ; String response = callApi ( HttpMethod . POST , "/ws/org/sources/form_create" , "{service_type: \"custom\", name: \"" + sourceName + "\"}" ) ; response = "{" + "\"id\":\"FAKE_ID\"," + "\"acces_token\":\"FAKE_TOKEN\"," + "\"key\":\"FAKE_KEY\"" + "}" ; JsonMapper mapper = JsonMapper . builder ( ) . build ( ) ; Map < String , Object > map = mapper . readValue ( response , Map . class ) ; logger . debug ( "Source [{}] created. id={}, acces_token={}, key={}" , sourceName , map . get ( "id" ) , map . get ( "accessToken" ) , map . get ( "key" ) ) ; return map ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( exception , exception ) ; } }
@ Then ( "^I should receive a notification$" ) public void iShouldReceiveANotification ( ) throws Throwable { LOGGER . info ( "Waiting for a notification for at most {} milliseconds." , MAX_WAIT_FOR_UNKNOWN_NOTIFICATION ) ; final Notification notification = this . notificationService . getNotification ( MAX_WAIT_FOR_UNKNOWN_NOTIFICATION , TimeUnit . MILLISECONDS ) ; code_block = IfStatement ; LOGGER . info ( "Received notification for correlation UID {} for type {} with result {}." , notification . getCorrelationUid ( ) , notification . getNotificationType ( ) , notification . getResult ( ) ) ; ScenarioContext . current ( ) . put ( PlatformKeys . KEY_CORRELATION_UID , notification . getCorrelationUid ( ) ) ; ScenarioContext . current ( ) . put ( PlatformKeys . KEY_ORGANIZATION_IDENTIFICATION , PlatformDefaults . DEFAULT_ORGANIZATION_IDENTIFICATION ) ; ScenarioContext . current ( ) . put ( PlatformKeys . KEY_USER_NAME , PlatformDefaults . DEFAULT_USER_NAME ) ; this . notificationService . handleNotification ( notification , PlatformDefaults . DEFAULT_ORGANIZATION_IDENTIFICATION ) ; }
public void test() { if ( socketId == - 1 ) { logger . warn ( "SocketId not exist. header:{}" , header ) ; return ; } }
public void test() { try { final AgentProperty agentProperty = newChannelProperties ( header , pingSession . getServiceType ( ) ) ; long eventIdentifier = AgentLifeCycleAsyncTaskService . createEventIdentifier ( ( int ) socketId , ( int ) pingSession . nextEventIdAllocator ( ) ) ; this . agentLifeCycleAsyncTask . handleLifeCycleEvent ( agentProperty , pingTimestamp , agentLifeCycleState , eventIdentifier ) ; this . agentEventAsyncTask . handleEvent ( agentProperty , pingTimestamp , agentEventType ) ; } catch ( Exception e ) { logger . warn ( "Failed to update state. closeState:{} lifeCycle={} {}/{}" , closeState , pingSession , agentLifeCycleState , agentEventType , e ) ; } }
public void test() { try { UserServiceUtil . unsetOrganizationUsers ( organizationId , userIds ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try { dbType = JDBCTools . getDatabaseProductType ( JDBCTools . CSADMIN_DATASOURCE_NAME , getServiceConfig ( ) . getDbCsadminName ( ) ) ; databaseProductName = dbType . getName ( ) ; databaseScriptsPath = getServerRootDir ( ) + File . separator + JEEServerDeployment . DATABASE_SCRIPTS_DIR_PATH + File . separator + databaseProductName ; } catch ( Exception e ) { logger . warn ( String . format ( "Could not get database product type: %s" , e . getMessage ( ) ) ) ; } }
public void test() { try { String xml = this . extractConfigFile ( disablingCodesFileName ) ; code_block = IfStatement ; } catch ( Throwable t ) { _logger . error ( "Error loading disabling codes from file {}" , disablingCodesFileName , t ) ; } }
public void test() { if ( ctx . getValue ( ) != null ) { LOG . warn ( "mapSoftRefValue: Was expecting AtlasObjectId, but found: {}" , ctx . getValue ( ) . getClass ( ) ) ; } }
public static void main ( String [ ] args ) throws Exception { String webappsPath = args [ 0 ] ; int port = Integer . parseInt ( args [ 1 ] ) ; File dataDir = Files . createTempDir ( ) ; dataDir . deleteOnExit ( ) ; Tomcat tomcat = new Tomcat ( ) ; tomcat . setBaseDir ( dataDir . getAbsolutePath ( ) ) ; tomcat . setPort ( port ) ; tomcat . getConnector ( ) . setAttribute ( "maxThreads" , "1000" ) ; tomcat . addWebapp ( "/" , new File ( webappsPath ) . getAbsolutePath ( ) ) ; log . info ( "-----------------------------------------------------------------" ) ; log . info ( "Starting Tomcat port {} dir {}" , port , webappsPath ) ; log . info ( "-----------------------------------------------------------------" ) ; tomcat . start ( ) ; code_block = WhileStatement ; }
public void test() { try { java . util . List < com . liferay . asset . category . property . model . AssetCategoryProperty > returnValue = AssetCategoryPropertyServiceUtil . getCategoryPropertyValues ( companyId , key ) ; return com . liferay . asset . category . property . model . AssetCategoryPropertySoap . toSoapModels ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( log ) { Logger . trace ( String . format ( "< %02x" , d & 0xff ) ) ; } }
public void rollbackOnError ( Message message , NotificationContext context , StateTransitionError error ) { logger . error ( "Default rollback method invoked on error. Error Code: " + error . getCode ( ) ) ; }
@ Override protected void onSubtreeModified ( DataObjectModification < OfOverlayL3Context > rootNode , InstanceIdentifier < OfOverlayL3Context > rootIdentifier ) { Name newPortName = rootNode . getDataAfter ( ) . getPortName ( ) ; Name oldPortName = rootNode . getDataBefore ( ) . getPortName ( ) ; LOG . trace ( "on update: \n old OfOverlayL3Context: {} \n new OfOverlayL3Context: {} \n rootIdentifier: {}" , rootNode . getDataBefore ( ) , rootNode . getDataAfter ( ) , rootIdentifier ) ; code_block = IfStatement ; code_block = IfStatement ; updateLocationBasedOnPortName ( newPortName , rootIdentifier ) ; }
public void test() { if ( oldPortName != null && newPortName != null && oldPortName . equals ( newPortName ) ) { LOG . debug ( "No need to update location for L3EP {} because port-name {} was not changed." , rootIdentifier . firstKeyOf ( EndpointL3 . class ) , oldPortName . getValue ( ) ) ; return ; } }
public void test() { if ( oldPortName != null && newPortName != null && oldPortName . equals ( newPortName ) ) { LOG . debug ( "No need to update location for L3EP {} because port-name {} was not changed." , rootIdentifier . firstKeyOf ( EndpointL3 . class ) , oldPortName . getValue ( ) ) ; return ; } }
public void test() { try { return buildOrganization ( FHIRTransformHelper . extractFhirOrgResourceList ( bundle ) ) ; } catch ( Exception ex ) { LOG . error ( "Transforming FHIR data resulted in exception: {}" , ex . getLocalizedMessage ( ) , ex ) ; throw new ExchangeTransformException ( ex ) ; } }
public void test() { try { config . loadFromXml ( information ) ; } catch ( IOException e ) { logger . error ( "Error load configuration  {} " , e . getMessage ( ) ) ; } }
@ Override public void spaceRemoved ( SpaceLifeCycleEvent event ) { LOG . debug ( "space " + event . getSpace ( ) . getDisplayName ( ) + " was removed!" ) ; }
public void test() { try { String labelData = json . get ( "publish" ) . getAsJsonObject ( ) . get ( "data" ) . getAsString ( ) . toUpperCase ( ) ; String labelHealth = json . get ( "publish" ) . getAsJsonObject ( ) . get ( "health" ) . getAsString ( ) . toUpperCase ( ) ; code_block = IfStatement ; code_block = IfStatement ; labelDataValue = Arrays . asList ( labelData . split ( MQMessageConstants . ROUTING_KEY_SEPERATOR ) ) ; } catch ( Exception e ) { log . error ( "Invalid label Name " , e ) ; throw new InsightsCustomException ( e . getMessage ( ) ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Release Write {} lock on resource {} and {}" , resource . name , firstUser , secondUser ) ; } }
public void test() { if ( portPublisher == null ) { log . info ( "here" ) ; } }
@ Override public VolumeInstance getInstance ( VolumeOrder volumeOrder , CloudStackUser cloudStackUser ) throws FogbowException { LOGGER . info ( String . format ( Messages . Log . GETTING_INSTANCE_S , volumeOrder . getInstanceId ( ) ) ) ; GetVolumeRequest request = new GetVolumeRequest . Builder ( ) . id ( volumeOrder . getInstanceId ( ) ) . build ( this . cloudStackUrl ) ; return doGetInstance ( request , cloudStackUser ) ; }
public void test() { if ( GovpayConfig . getInstance ( ) . getClusterId ( ) == null ) { log . trace ( "ClusterId non impostato. Gestione concorrenza non abilitata. Aggiornamento non necessario" ) ; return ; } }
public void test() { try { batchBD = new BatchBD ( configWrapper ) ; batchBD . setupConnection ( configWrapper . getTransactionID ( ) ) ; batchBD . setAutoCommit ( false ) ; batchBD . enableSelectForUpdate ( ) ; batchBD . setAtomica ( false ) ; Batch batch = getRunningBatch ( batchBD , codBatch ) ; code_block = IfStatement ; } catch ( NotFoundException e ) { log . error ( "Errore nell'aggiornamento del semaforo di concorrenza per il batch " + codBatch , e ) ; return ; } finally { code_block = IfStatement ; } }
@ Test public void test14ReadRead ( ) { LOGGER . info ( "  test14ReadRead" ) ; test09WriteCreate ( ) ; EntityUtils . filterAndCheck ( serviceRead . things ( ) , "" , THINGS ) ; }
@ Override public void deleteInstance ( PublicIpOrder publicIpOrder , CloudStackUser cloudStackUser ) throws FogbowException { LOGGER . info ( String . format ( Messages . Log . DELETING_INSTANCE_S , publicIpOrder . getInstanceId ( ) ) ) ; doDeleteInstance ( publicIpOrder , cloudStackUser ) ; }
public void test() { if ( conf != null ) { logger . debug ( "Starting OpenTherm Gateway connector" ) ; connector = new OpenThermGatewaySocketConnector ( this , conf . ipaddress , conf . port ) ; Thread thread = new Thread ( connector , "OpenTherm Gateway Binding - socket listener thread" ) ; thread . setDaemon ( true ) ; thread . start ( ) ; logger . debug ( "OpenTherm Gateway connector started" ) ; } }
@ Override public void onTccTransactionEnded ( GrpcTccTransactionEndedEvent request , StreamObserver < GrpcAck > responseObserver ) { LOG . info ( "Received transaction end event, global tx id: {}" , request . getGlobalTxId ( ) ) ; events . offer ( request ) ; sleep ( ) ; responseObserver . onNext ( ALLOW ) ; responseObserver . onCompleted ( ) ; }
public void test() { if ( activeTx != null && ! activeTx . equals ( id ) ) { LOGGER . warn ( RelationalProviderI18n . threadAssociatedWithAnotherTransaction , Thread . currentThread ( ) . getName ( ) , activeTx , id ) ; } }
public void test() { try { assignments . clearMetadataCache ( ) ; } catch ( Exception e ) { LOGGER . error ( "Exception occurred while clearing assignments metadata cache..." , e ) ; result = Boolean . FALSE ; } }
public void test() { try { UserInfo . Username userName = authorization . getUser ( authorizationHeader ) ; authorization . checkSuperAdmin ( userName ) ; boolean result = Boolean . TRUE ; code_block = TryStatement ;  return httpHeader . headers ( ) . entity ( result ) . build ( ) ; } catch ( Exception exception ) { LOGGER . error ( "clearMetadataCache failed with error:" , exception ) ; throw exception ; } }
private String generateConsumerInfo ( final SystemRequestDTO consumer , final CloudRequestDTO consumerCloud ) { logger . debug ( "generateConsumerInfo started..." ) ; final StringBuilder sb = new StringBuilder ( consumer . getSystemName ( ) ) ; code_block = IfStatement ; return sb . toString ( ) . toLowerCase ( ) ; }
public void test() { try { m . parseArgs ( args ) ; m . configure ( ) ; long start = System . nanoTime ( ) ; m . process ( ) ; long end = System . nanoTime ( ) ; logger . info ( String . format ( "Took: %6.3f seconds" , ( end - start ) / NANOS ) ) ; } catch ( Exception x ) { logger . log ( Level . SEVERE , "Failed to run" , x ) ; } finally { m . shutdown ( ) ; System . exit ( 0 ) ; } }
public void test() { if ( ActiveMQXARecoveryLogger . LOGGER . isDebugEnabled ( ) ) { ActiveMQXARecoveryLogger . LOGGER . debug ( "end " + xaResource + " xid " ) ; } }
public void test() { try { List < Group > userGroups = ( null != currentUser ) ? this . getAuthorizationManager ( ) . getUserGroups ( currentUser ) : new ArrayList < > ( ) ; code_block = IfStatement ; } catch ( Throwable t ) { _logger . error ( "Error while rendering content {}" , contentId , t ) ; return null ; } }
public void test() { try { return esRepository . getDataFromES ( AssetConstants . AWS_EC2 , Constants . QUALYS_INFO , mustFilter , null , null , Arrays . asList ( "lastVulnScan" , "totalMemory" , "account.list.hostAssetAccount.username" ) , null ) ; } catch ( Exception e ) { LOGGER . error ( "Exception in getQualysDetail " , e ) ; throw new DataException ( ) ; } }
public void test() { if ( log . isTraceEnable ( ) ) { log . info ( this , "Will execute cmd: " + cmd ) ; } }
public void test() { try { System . out . println ( "|   ( ok )  " + this . getDatabaseName ( ) + "." + tableName ) ; this . createTable ( tableClass , connectionSource ) ; tables . add ( tableName ) ; } catch ( Throwable t ) { schemaReport . getDatabaseStatus ( ) . put ( this . getDatabaseName ( ) , SystemInstallationReport . Status . INCOMPLETE ) ; String message = "Error creating table " + this . getDatabaseName ( ) + "/" + tableClassName + " - " + t . getMessage ( ) ; _logger . error ( "Error creating table {}/{}" , this . getDatabaseName ( ) , tableClassName , t ) ; throw new ApsSystemException ( message , t ) ; } }
public void test() { try { List < String > tables = schemaReport . getDataSourceTables ( ) . get ( this . getDatabaseName ( ) ) ; code_block = IfStatement ; code_block = ForStatement ; } catch ( Throwable t ) { schemaReport . getDatabaseStatus ( ) . put ( this . getDatabaseName ( ) , SystemInstallationReport . Status . INCOMPLETE ) ; _logger . error ( "Error on setup Database - {}" , this . getDatabaseName ( ) , t ) ; throw new ApsSystemException ( "Error on setup Database" , t ) ; } }
public void test() { try { KeycloakConfigResolver resolver = resolverClass . newInstance ( ) ; log . info ( "Using " + resolver + " to resolve Keycloak configuration on a per-request basis." ) ; this . deploymentContext = new AdapterDeploymentContext ( resolver ) ; } catch ( Exception e ) { throw new RuntimeException ( "Unable to instantiate resolver " + resolverClass ) ; } }
public void test() { if ( vm . isPoweredOn ( ) && ! vm . shutdownGuest ( Constants . VM_FAST_SHUTDOWN_WAITING_SEC * 1000 ) ) { logger . info ( "shutdown " + vm . getName ( ) + " guest OS failed, power off directly" ) ; vm . powerOff ( ) ; } }
public void test() { if ( ! forceReload && ( reloadCheckIntervalInMs < 0 || System . currentTimeMillis ( ) < ( lastReloadCheck + reloadCheckIntervalInMs ) ) ) { LOG . debug ( "reload not forced and reload check disabled or check interval not yet elapsed" ) ; return ; } }
public void consume ( ) throws InterruptedException { Item item = queue . take ( ) ; LOGGER . info ( "Consumer [{}] consume item [{}] produced by [{}]" , name , item . getId ( ) , item . getProducer ( ) ) ; }
private void setupLoginTty ( ) throws Exception { String setupTtyScriptName = Configuration . getString ( Constants . SERENGETI_SETUP_LOGIN_TTY_SCRIPT , Constants . SERENGETI_DEFAULT_SETUP_LOGIN_TTY_SCRIPT ) ; String setupTtyScript = getScriptName ( setupTtyScriptName ) ; String cmd = sudoCmd + " " + setupTtyScript ; String action = "Setup login tty to " + nodeIP ; logger . info ( action + " command is: " + cmd ) ; SSHUtil sshUtil = new SSHUtil ( ) ; String errMsg = null ; code_block = ForStatement ; logger . info ( action + " failed" ) ; throw SetPasswordException . FAIL_TO_SETUP_LOGIN_TTY ( nodeIP , errMsg ) ; }
public void test() { try { Thread . sleep ( 3000 ) ; } catch ( InterruptedException e1 ) { logger . info ( "Interrupted when waiting for setup login tty, retry immediately..." ) ; } }
public void test() { if ( policy . toString ( ) . contains ( "CADES" ) || policy . toString ( ) . contains ( "PADES" ) ) { logger . info ( factory . loadPolicy ( policy ) . toString ( ) ) ; } }
public void test() { try { deck . exec ( agent ) ; } catch ( final Throwable ex ) { Logger . error ( this , "%s" , ExceptionUtils . getStackTrace ( ex ) ) ; failure . add ( agent . getClass ( ) . getSimpleName ( ) ) ; } }
public void test() { if ( hasJobEntityExpired ( jobEntity ) ) { LOGGER . info ( format ( Messages . JOB_WITH_ID_AND_TASK_NAME_EXPIRED , jobEntity . getProcessInstanceId ( ) , jobEntity . getElementName ( ) ) ) ; } }
public void test() { try { element = CnAElementHome . getInstance ( ) . loadById ( MassnahmenUmsetzung . TYPE_ID , selection . getDbId ( ) ) ; openEditor ( element . getId ( ) , new BSIElementEditorInput ( element ) , BSIElementEditorMultiPage . EDITOR_ID ) ; } catch ( CommandException e ) { log . error ( "Error while opening editor." , e ) ; ExceptionUtil . log ( e , Messages . EditorFactory_2 ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Jumping to page " + getPage ( ) + " and index " + current ) ; } }
public void test() { if ( LOGGER . isTraceEnabled ( ) ) { LOGGER . trace ( "Checking tracker overlaps to " + tracker + " to " + list . size ( ) + " other trackers" ) ; } }
public void test() { if ( LOGGER . isTraceEnabled ( ) ) { LOGGER . trace ( "Checking tracker overlaps to " + tracker + " to " + list . size ( ) + " other trackers" ) ; } }
public void test() { if ( ! other . isDone ( ) && ! tracker . equals ( other ) ) { tracker . checkOverlap ( other ) ; code_block = IfStatement ; } else-if ( LOGGER . isTraceEnabled ( ) ) { LOGGER . trace ( "Other tracker is done or equal: " + other ) ; } }
public void test() { if ( LOGGER . isTraceEnabled ( ) ) { LOGGER . trace ( "No other trackers to check overlaps with" ) ; } }
public org . talend . mdm . webservice . WSStringArray getConceptsInDataCluster ( org . talend . mdm . webservice . WSGetConceptsInDataCluster arg0 ) { LOG . info ( "Executing operation getConceptsInDataCluster" ) ; System . out . println ( arg0 ) ; code_block = TryStatement ;  }
public < T extends YamcsService > void addGlobalService ( String name , Class < T > serviceClass , YConfiguration args ) throws ValidationException , InitException { code_block = ForStatement ; LOG . info ( "Loading service {}" , name ) ; ServiceWithConfig swc = createService ( null , serviceClass . getName ( ) , name , args , true ) ; swc . service . init ( null , name , swc . args ) ; YAMCS . globalServiceList . add ( swc ) ; ManagementService managementService = ManagementService . getInstance ( ) ; managementService . registerService ( null , name , swc . service ) ; }
public void test() { try { return readGmlFile ( file , targetCRS , Version . GML3 ) ; } catch ( IOException | RuntimeException e ) { LOG . info ( "Failure reading with GML3 parser. Trying with GML2" ) ; } }
@ Override @ Transactional public void deleteAlert ( Alert alert ) { requireNotDisposed ( ) ; requireArgument ( alert != null , "Alert cannot be null." ) ; _logger . debug ( "Deleting an alert {}." , alert ) ; EntityManager em = _emProvider . get ( ) ; deleteEntity ( em , alert ) ; em . flush ( ) ; }
public void test() { try { defaultProperties = ComponentUtil . getDefaultProperties ( ocd , this . ctx ) ; } catch ( Exception e ) { logger . warn ( "Failed to get default properties for component: {}" , pid , e ) ; } }
public void test() { try { kssServerId = kieServerState . getConfiguration ( ) . getConfigItemValue ( KIE_SERVER_ID ) ; } catch ( Exception e ) { logger . error ( "Failed to retrieve server id from KieServerState" , e ) ; } }
public void test() { try { final YamlService11 service11 = dockstoreYaml11 . getService ( ) ; BeanUtils . copyProperties ( service12 , service11 ) ; final DescriptorLanguageSubclass descriptorLanguageSubclass = DescriptorLanguageSubclass . convertShortNameStringToEnum ( service11 . getType ( ) ) ; service12 . setSubclass ( descriptorLanguageSubclass ) ; dockstoreYaml12 . setService ( service12 ) ; validate ( dockstoreYaml12 ) ; return dockstoreYaml12 ; } catch ( UnsupportedOperationException | InvocationTargetException | IllegalAccessException e ) { final String msg = "Error converting ; " + e . getMessage ( ) ; LOG . error ( msg , e ) ; throw new DockstoreYamlException ( msg ) ; } }
public void test() { try { requestBodyUrl = "grant_type=refresh_token&refresh_token=" . concat ( URLEncoder . encode ( refreshToken , "UTF-8" ) ) ; return generateAccessToken ( requestBodyUrl , oauth2ClientId ) ; } catch ( UnsupportedEncodingException exception ) { log . error ( "Exception in loginProxy: " + exception . getMessage ( ) ) ; return response ( false , "Unexpected Error Occured!!!" ) ; } }
protected void assertBroker ( InfraConfiguration brokerConfig ) { log . info ( "Checking broker infra" ) ; List < Pod > brokerPods = TestUtils . listBrokerPods ( kubernetes , exampleAddressSpace ) ; assertEquals ( 1 , brokerPods . size ( ) ) ; Pod broker = brokerPods . stream ( ) . findFirst ( ) . get ( ) ; ResourceRequirements resources = broker . getSpec ( ) . getContainers ( ) . stream ( ) . filter ( container -> container . getName ( ) . equals ( "broker" ) ) . findFirst ( ) . map ( Container :: getResources ) . get ( ) ; assertEquals ( new Quantity ( brokerConfig . getMemory ( ) ) , resources . getLimits ( ) . get ( "memory" ) , "Broker memory limit incorrect" ) ; assertEquals ( new Quantity ( brokerConfig . getMemory ( ) ) , resources . getRequests ( ) . get ( "memory" ) , "Broker memory requests incorrect" ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; }
public QueryEndpointsResponse queryEndpoints ( QueryEndpoints parameters ) { LOG . info ( "Executing operation queryEndpoints" ) ; return null ; }
public void test() { if ( accessor . isAnnotationPresent ( Label . class ) ) { String text = accessor . getAnnotation ( Label . class ) . value ( ) ; logger . debug ( "Label annotation present with value: {}" , text ) ; label = textProvider . getText ( text ) ; } else { label = Util . guessToWords ( accessor . getName ( ) ) ; logger . debug ( "Setting label from property name: {}" , label ) ; } }
public void test() { if ( accessor . isAnnotationPresent ( Label . class ) ) { String text = accessor . getAnnotation ( Label . class ) . value ( ) ; logger . debug ( "Label annotation present with value: {}" , text ) ; label = textProvider . getText ( text ) ; } else { label = Util . guessToWords ( accessor . getName ( ) ) ; logger . debug ( "Setting label from property name: {}" , label ) ; } }
public void test() { try ( final RestHighLevelClient restHighLevelClient = elasticsearchRestHighLevelClientFactory . getRestHighLevelClient ( ) ) { getResponse = restHighLevelClient . get ( getRequest , RequestOptions . DEFAULT ) ; } catch ( final IOException ioException ) { LOGGER . error ( "Caught IOException while attempting to use the ElasticsearchRestHighLevelClient." , ioException ) ; throw new ElasticsearchRestClientException ( "Caught IOException while attempting to use the ElasticsearchRestHighLevelClient." , ioException ) ; } }
public void test() { try ( IMqttClient client = clientBuilder . create ( ) ) { log . info ( "Connecting" ) ; client . connect ( ) ; List < CompletableFuture < MqttMessage > > receiveFutures = MqttUtils . subscribeAndReceiveMessages ( client , dest . getSpec ( ) . getAddress ( ) , messages . size ( ) , subscriberQos ) ; List < CompletableFuture < Void > > publishFutures = MqttUtils . publish ( client , dest . getSpec ( ) . getAddress ( ) , messages ) ; int publishCount = MqttUtils . awaitAndReturnCode ( publishFutures , 1 , TimeUnit . MINUTES ) ; assertThat ( "Incorrect count of messages published" , publishCount , is ( messages . size ( ) ) ) ; int receivedCount = MqttUtils . awaitAndReturnCode ( receiveFutures , 2 , TimeUnit . MINUTES ) ; assertThat ( "Incorrect count of messages received" , receivedCount , is ( messages . size ( ) ) ) ; } }
public void test() { try { syncContext ( true ) ; code_block = WhileStatement ; } catch ( InterruptedException ex ) { log . debug ( "Monitoring thread was interrupted" , ex ) ; } }
@ Test @ Order ( order = 6 ) public void testCheckUserNotification ( ) throws Exception { LOG . info ( "éç¥ç¢ºèª" ) ; NotificationsEntity notification = NotificationsDao . get ( ) . selectOnKey ( new Long ( 1 ) ) ; Assert . assertNotNull ( notification ) ; Assert . assertEquals ( MailLogic . NOTIFY_INSERT_KNOWLEDGE , notification . getTitle ( ) ) ; UsersEntity user = UsersDao . get ( ) . selectOnUserKey ( "integration-test-user-01" ) ; UserNotificationsEntity userNotification = UserNotificationsDao . get ( ) . selectOnKey ( notification . getNo ( ) , user . getUserId ( ) ) ; Assert . assertNotNull ( userNotification ) ; user = UsersDao . get ( ) . selectOnUserKey ( "integration-test-user-03" ) ; userNotification = UserNotificationsDao . get ( ) . selectOnKey ( notification . getNo ( ) , user . getUserId ( ) ) ; Assert . assertNotNull ( userNotification ) ; user = UsersDao . get ( ) . selectOnUserKey ( "integration-test-user-02" ) ; userNotification = UserNotificationsDao . get ( ) . selectOnKey ( notification . getNo ( ) , user . getUserId ( ) ) ; Assert . assertNull ( userNotification ) ; int count = MailsDao . get ( ) . selectCountAll ( ) ; Assert . assertEquals ( 2 , count ) ; }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { try { String sHomeCommunityId = PropertyAccessor . getInstance ( ) . getProperty ( NhincConstants . GATEWAY_PROPERTY_FILE , NhincConstants . HOME_COMMUNITY_ID_PROPERTY ) ; OrganizationType org = InternalExchangeManager . getInstance ( ) . getOrganization ( sHomeCommunityId ) ; apiLevels = getAPILevelsFromOrganization ( org , serviceName ) ; } catch ( ExchangeManagerException | PropertyAccessException ex ) { LOG . error ( "Error getting API Level by Service Name: {}" , ex . getLocalizedMessage ( ) , ex ) ; } }
public void test() { try { logger . debug ( "Received get server templates" ) ; String response = marshal ( contentType , specManagementService . listServerTemplates ( ) ) ; logger . debug ( "Returning response for get server templates: {}" , response ) ; return createCorrectVariant ( response , headers , Response . Status . OK ) ; } catch ( KieServerControllerIllegalArgumentException e ) { return createCorrectVariant ( e . getMessage ( ) , headers , Response . Status . NOT_FOUND ) ; } catch ( KieServerControllerException e ) { return createCorrectVariant ( REQUEST_FAILED_TOBE_PROCESSED + e . getMessage ( ) , headers , Response . Status . BAD_REQUEST ) ; } catch ( Exception e ) { logger . error ( "Get server templates failed due to {}" , e . getMessage ( ) , e ) ; return createCorrectVariant ( "Unknown error " + e . getMessage ( ) , headers , Response . Status . INTERNAL_SERVER_ERROR ) ; } }
public void test() { try { logger . debug ( "Received get server templates" ) ; String response = marshal ( contentType , specManagementService . listServerTemplates ( ) ) ; logger . debug ( "Returning response for get server templates: {}" , response ) ; return createCorrectVariant ( response , headers , Response . Status . OK ) ; } catch ( KieServerControllerIllegalArgumentException e ) { return createCorrectVariant ( e . getMessage ( ) , headers , Response . Status . NOT_FOUND ) ; } catch ( KieServerControllerException e ) { return createCorrectVariant ( REQUEST_FAILED_TOBE_PROCESSED + e . getMessage ( ) , headers , Response . Status . BAD_REQUEST ) ; } catch ( Exception e ) { logger . error ( "Get server templates failed due to {}" , e . getMessage ( ) , e ) ; return createCorrectVariant ( "Unknown error " + e . getMessage ( ) , headers , Response . Status . INTERNAL_SERVER_ERROR ) ; } }
public void test() { try { logger . debug ( "Received get server templates" ) ; String response = marshal ( contentType , specManagementService . listServerTemplates ( ) ) ; logger . debug ( "Returning response for get server templates: {}" , response ) ; return createCorrectVariant ( response , headers , Response . Status . OK ) ; } catch ( KieServerControllerIllegalArgumentException e ) { return createCorrectVariant ( e . getMessage ( ) , headers , Response . Status . NOT_FOUND ) ; } catch ( KieServerControllerException e ) { return createCorrectVariant ( REQUEST_FAILED_TOBE_PROCESSED + e . getMessage ( ) , headers , Response . Status . BAD_REQUEST ) ; } catch ( Exception e ) { logger . error ( "Get server templates failed due to {}" , e . getMessage ( ) , e ) ; return createCorrectVariant ( "Unknown error " + e . getMessage ( ) , headers , Response . Status . INTERNAL_SERVER_ERROR ) ; } }
public void test() { if ( retVal == null ) { LOGGER . warn ( "Get data " + dataId + " is null." ) ; } }
public void test() { if ( ns == null ) { log . error ( "Invalid/unknown ND4J namespace provided: " + s ) ; } else { usedNamespaces . add ( ns ) ; } }
public void test() { if ( ns == null ) { log . error ( "Invalid/unknown SD namespace provided: " + s ) ; } else { usedNamespaces . add ( ns ) ; } }
public void test() { if ( outputDir != null ) { File outputPath = new File ( outputDir , basePackagePath + javaClassName + ".java" ) ; log . info ( "Output path: {}" , outputPath . getAbsolutePath ( ) ) ; if ( NS_PROJECT . ND4J == project ) Nd4jNamespaceGenerator . generate ( ops , null , outputDir , javaClassName , basePackage , docsdir ) ; else Nd4jNamespaceGenerator . generate ( ops , null , outputDir , javaClassName , basePackage , "org.nd4j.autodiff.samediff.ops.SDOps" , docsdir ) ; } }
public void test() { for ( Certificate cert : certificates ) { LOG . info ( "Looking for SANs in cert: " + cert . getTBSCertificate ( ) . getSubject ( ) ) ; } }
public void test() { try { LOG . debug ( "Copying folder(id={}) to destination_folder(id={}) {}" , folderId , destinationFolderId , newName == null ? "" : " with new name '" + newName + "'" ) ; code_block = IfStatement ; code_block = IfStatement ; BoxFolder folderToCopy = new BoxFolder ( boxConnection , folderId ) ; BoxFolder destinationFolder = new BoxFolder ( boxConnection , destinationFolderId ) ; code_block = IfStatement ; } catch ( BoxAPIException e ) { throw new RuntimeException ( String . format ( "Box API returned the error code %d%n%n%s" , e . getResponseCode ( ) , e . getResponse ( ) ) , e ) ; } }
public void test() { try { return Long . parseLong ( lifetimeProp . substring ( 0 , lifetimeProp . length ( ) - 1 ) ) * factor ; } catch ( NumberFormatException nfe ) { Log . warn ( "Unable to parse back-expiry for cache: " + cacheInfo . getCacheName ( ) ) ; } }
public void test() { if ( noOfRecordsToKeep == 0 ) { logger . info ( "Truncating table {}..." , sqlTableName ) ; this . dbHelper . execute ( c , MessageFormat . format ( SQL_TRUNCATE_TABLE , sqlTableName ) ) ; } else { logger . info ( "Partially emptying table {}" , sqlTableName ) ; this . dbHelper . execute ( c , MessageFormat . format ( SQL_DELETE_RANGE_TABLE , sqlTableName , Integer . toString ( noOfRecordsToKeep ) ) ) ; } }
public void test() { if ( noOfRecordsToKeep == 0 ) { logger . info ( "Truncating table {}..." , sqlTableName ) ; this . dbHelper . execute ( c , MessageFormat . format ( SQL_TRUNCATE_TABLE , sqlTableName ) ) ; } else { logger . info ( "Partially emptying table {}" , sqlTableName ) ; this . dbHelper . execute ( c , MessageFormat . format ( SQL_DELETE_RANGE_TABLE , sqlTableName , Integer . toString ( noOfRecordsToKeep ) ) ) ; } }
public void test() { try { this . dbHelper . withConnection ( c code_block = LoopStatement ; ) ; } catch ( final SQLException sqlException ) { logger . error ( "Error in truncating the table {}..." , sqlTableName , sqlException ) ; } }
public void test() { if ( dsConfig . getRedirectDbName ( ) . isPresent ( ) ) { dsWrapper . setRedirectDb ( new RedirectDaoInitializer ( connection , dsConfig . getRedirectDbName ( ) . get ( ) ) ) ; LOG . info ( "Registered RecordRedirectDao for data source: {}, redirect-dbName={}" , dsConfig . getId ( ) , dsConfig . getRedirectDbName ( ) . get ( ) ) ; } else { LOG . info ( "No redirect db configured for data source: {}" , dsConfig . getId ( ) ) ; } }
public void test() { if ( dsConfig . getRedirectDbName ( ) . isPresent ( ) ) { dsWrapper . setRedirectDb ( new RedirectDaoInitializer ( connection , dsConfig . getRedirectDbName ( ) . get ( ) ) ) ; LOG . info ( "Registered RecordRedirectDao for data source: {}, redirect-dbName={}" , dsConfig . getId ( ) , dsConfig . getRedirectDbName ( ) . get ( ) ) ; } else { LOG . info ( "No redirect db configured for data source: {}" , dsConfig . getId ( ) ) ; } }
public void test() { if ( dsConfig . getRedirectDbName ( ) . isPresent ( ) ) { dsWrapper . setRedirectDb ( new RedirectDaoInitializer ( connection , dsConfig . getRedirectDbName ( ) . get ( ) ) ) ; LOG . info ( "Registered RecordRedirectDao for data source: {}, redirect-dbName={}" , dsConfig . getId ( ) , dsConfig . getRedirectDbName ( ) . get ( ) ) ; } else { LOG . info ( "No redirect db configured for data source: {}" , dsConfig . getId ( ) ) ; } }
public void test() { if ( dsConfig . getRedirectDbName ( ) . isPresent ( ) ) { dsWrapper . setRedirectDb ( new RedirectDaoInitializer ( connection , dsConfig . getRedirectDbName ( ) . get ( ) ) ) ; LOG . info ( "Registered RecordRedirectDao for data source: {}, redirect-dbName={}" , dsConfig . getId ( ) , dsConfig . getRedirectDbName ( ) . get ( ) ) ; } else { LOG . info ( "No redirect db configured for data source: {}" , dsConfig . getId ( ) ) ; } }
public void test() { if ( message != null ) { getLogger ( ) . debug ( String . valueOf ( message ) , t ) ; } }
public static void info ( Logger logger , String eventName , String format , Object arg ) { logger . info ( constructFormatOrMsg ( eventName , format ) , arg ) ; }
public void test() { switch ( flags ) { case SPECIAL_BOOLEAN : rv = Boolean . valueOf ( tu . decodeBoolean ( data ) ) ; break ; case SPECIAL_INT : rv = Integer . valueOf ( tu . decodeInt ( data ) ) ; break ; case SPECIAL_LONG : rv = Long . valueOf ( tu . decodeLong ( data ) ) ; break ; case SPECIAL_DATE : rv = new Date ( tu . decodeLong ( data ) ) ; break ; case SPECIAL_BYTE : rv = Byte . valueOf ( tu . decodeByte ( data ) ) ; break ; case SPECIAL_FLOAT : rv = new Float ( Float . intBitsToFloat ( tu . decodeInt ( data ) ) ) ; break ; case SPECIAL_DOUBLE : rv = new Double ( Double . longBitsToDouble ( tu . decodeLong ( data ) ) ) ; break ; case SPECIAL_BYTEARRAY : rv = data ; break ; default : getLogger ( ) . warn ( "Undecodeable with flags %x" , flags ) ; } }
public void test() { if ( result == null || result . isEmpty ( ) ) { log . warn ( "Found no data for job={} checkPath={}; returning zero" , jobId , checkPath ) ; return 0 ; } else-if ( result . size ( ) > 1 ) { log . warn ( "Found multiple results for job={} checkPath={}; using first row" , jobId , checkPath ) ; } }
public void test() { if ( result == null || result . isEmpty ( ) ) { log . warn ( "Found no data for job={} checkPath={}; returning zero" , jobId , checkPath ) ; return 0 ; } else-if ( result . size ( ) > 1 ) { log . warn ( "Found multiple results for job={} checkPath={}; using first row" , jobId , checkPath ) ; } }
public void test() { try { stream . write ( info . getBytes ( ) ) ; stream . write ( " " . getBytes ( ) ) ; } catch ( IOException e ) { log . error ( e , e ) ; } }
public void test() { try { await ( ) ; return ; } catch ( final InterruptedException e ) { logger . warn ( "Interrupted while waiting for event latch." , e ) ; } }
public void test() { try { GefaehrdungsUmsetzung parent = ( GefaehrdungsUmsetzung ) massnahme . getParent ( ) ; code_block = IfStatement ; } catch ( Exception e ) { LOG . error ( e ) ; } }
@ ExceptionHandler ( HttpMediaTypeNotAcceptableException . class ) @ ResponseStatus ( value = HttpStatus . BAD_REQUEST ) protected ErrorResponse handleHttpMediaTypeNotAcceptableExceptionException ( HttpMediaTypeNotAcceptableException ex ) { logger . debug ( ExceptionUtils . getStackTrace ( ex ) ) ; return new ViewObjectErrorResponse ( ErrorCode . INVALID_ARGUMENTS , new InvalidParameterView ( "Invalid Media Request Type" , "required type is application/pdf" , "" , Objects . toString ( ex ) ) ) ; }
public void test() { try ( CloseableIterator < KeyMessage < String , String > > data = new ConsumeData ( UPDATE_TOPIC , getKafkaBrokerPort ( ) ) . iterator ( ) ; BatchLayer < ? , ? , ? > batchLayer = new BatchLayer < > ( config ) ) { log . info ( "Starting batch layer" ) ; batchLayer . start ( ) ; sleepSeconds ( 3 ) ; log . info ( "Starting consumer thread" ) ; ConsumeTopicRunnable consumeInput = new ConsumeTopicRunnable ( data ) ; new Thread ( LoggingCallable . log ( consumeInput ) . asRunnable ( ) , "ConsumeInputThread" ) . start ( ) ; consumeInput . awaitRun ( ) ; log . info ( "Producing data" ) ; produce . start ( ) ; int genIntervalSec = config . getInt ( "oryx.batch.streaming.generation-interval-sec" ) ; sleepSeconds ( genIntervalSec ) ; keyMessages = consumeInput . getKeyMessages ( ) ; } }
public void test() { try ( CloseableIterator < KeyMessage < String , String > > data = new ConsumeData ( UPDATE_TOPIC , getKafkaBrokerPort ( ) ) . iterator ( ) ; BatchLayer < ? , ? , ? > batchLayer = new BatchLayer < > ( config ) ) { log . info ( "Starting batch layer" ) ; batchLayer . start ( ) ; sleepSeconds ( 3 ) ; log . info ( "Starting consumer thread" ) ; ConsumeTopicRunnable consumeInput = new ConsumeTopicRunnable ( data ) ; new Thread ( LoggingCallable . log ( consumeInput ) . asRunnable ( ) , "ConsumeInputThread" ) . start ( ) ; consumeInput . awaitRun ( ) ; log . info ( "Producing data" ) ; produce . start ( ) ; int genIntervalSec = config . getInt ( "oryx.batch.streaming.generation-interval-sec" ) ; sleepSeconds ( genIntervalSec ) ; keyMessages = consumeInput . getKeyMessages ( ) ; } }
public void test() { try ( CloseableIterator < KeyMessage < String , String > > data = new ConsumeData ( UPDATE_TOPIC , getKafkaBrokerPort ( ) ) . iterator ( ) ; BatchLayer < ? , ? , ? > batchLayer = new BatchLayer < > ( config ) ) { log . info ( "Starting batch layer" ) ; batchLayer . start ( ) ; sleepSeconds ( 3 ) ; log . info ( "Starting consumer thread" ) ; ConsumeTopicRunnable consumeInput = new ConsumeTopicRunnable ( data ) ; new Thread ( LoggingCallable . log ( consumeInput ) . asRunnable ( ) , "ConsumeInputThread" ) . start ( ) ; consumeInput . awaitRun ( ) ; log . info ( "Producing data" ) ; produce . start ( ) ; int genIntervalSec = config . getInt ( "oryx.batch.streaming.generation-interval-sec" ) ; sleepSeconds ( genIntervalSec ) ; keyMessages = consumeInput . getKeyMessages ( ) ; } }
public void test() { try { wireTaskInfoList . add ( new TaskInfo ( taskInfo ) ) ; } catch ( IOException e ) { LOG . error ( "task info deserialization failed " + e ) ; } }
public void test() { try { final SimpleDateFormat format = new SimpleDateFormat ( dateFormat ) ; final long initial = format . parse ( format . format ( 3600 ) ) . getTime ( ) ; code_block = ForStatement ; return max ; } catch ( ParseException pex ) { logger . warn ( "" , pex ) ; } }
public void test() { if ( statusError . isSimpleError ( ) ) { logger . info ( "Failed to ping stream, streamId={}, cause={}" , streamId , statusError . getMessage ( ) ) ; } else { logger . info ( "Failed to ping stream, streamId={}, cause={}" , streamId , statusError . getMessage ( ) , statusError . getThrowable ( ) ) ; } }
public void test() { if ( statusError . isSimpleError ( ) ) { logger . info ( "Failed to ping stream, streamId={}, cause={}" , streamId , statusError . getMessage ( ) ) ; } else { logger . info ( "Failed to ping stream, streamId={}, cause={}" , streamId , statusError . getMessage ( ) , statusError . getThrowable ( ) ) ; } }
public void test() { try { Response resp = evt . getResponse ( ) ; Dialog dlg = evt . getDialog ( ) ; CSeqHeader cseqHead = ( CSeqHeader ) resp . getHeader ( CSeqHeader . NAME ) ; Request ack = dlg . createAck ( cseqHead . getSeqNumber ( ) ) ; addSdp ( ack , answer ) ; dlg . sendAck ( ack ) ; } catch ( Exception e ) { log . error ( "ack {}" , evt , e ) ; } }
public void test() { for ( int i = 0 ; i < hist . length ; i ++ ) { String label = String . valueOf ( i + 1 ) ; if ( i == hist . length - 1 ) label += "+" ; double current = ( hist [ i ] * 100 ) / total ; String format = String . format ( "%.2f" , current ) ; String error = String . format ( "%+.2f" , current - target . get ( i ) ) ; log . info ( "Size {}: {} = {} % (diff: {}%)" , label , hist [ i ] , format , error ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( "Unable to parse JSON" , jsonException ) ; } }
public long getSize ( ) { logger . trace ( "[{}] getSize() -> {}" , name , size ) ; return size ; }
public void test() { try { OrganizationalUnit orgUnit = Optional . ofNullable ( organizationalUnitService . getOrganizationalUnit ( space . getName ( ) ) ) . orElseThrow ( ( ) -> new IllegalArgumentException ( String . format ( "The given space [%s] does not exist." , space . getName ( ) ) ) ) ; doRemoveRepository ( orgUnit , alias , config , repo -> repositoryRemovedEvent . fire ( new RepositoryRemovedEvent ( repo ) ) , true ) ; } catch ( final Exception e ) { logger . error ( "Error during remove repository" , e ) ; throw new RuntimeException ( e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "No ViewResolvers found in servlet '" + getServletName ( ) + "': using default" ) ; } }
public void test() { try { String query = "UPDATE " + tablename + " SET updatedat = ? WHERE code = ?" ; stat = conn . prepareStatement ( query ) ; stat . setTimestamp ( 1 , new Timestamp ( date . getTime ( ) ) ) ; stat . setString ( 2 , pageCode ) ; stat . executeUpdate ( ) ; } catch ( Throwable t ) { _logger . error ( "Error while updating the page metadata record for table {} and page {}" , PageMetadataDraft . TABLE_NAME , pageCode , t ) ; throw new RuntimeException ( "Error while updating the page metadata record for table " + PageMetadataDraft . TABLE_NAME + " and page " + pageCode , t ) ; } finally { closeDaoResources ( null , stat ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( String . format ( "Active instance is removed and added to the " + "termination pending instance list. [Instance Id] %s" , instanceId ) ) ; } }
@ Test public void turnOnLocks ( ) throws IOException { ServiceReference < PaxLoggingService > sr = context . getServiceReference ( PaxLoggingService . class ) ; PaxLoggingService paxLogging = context . getService ( sr ) ; assertNotNull ( paxLogging ) ; final Object [ ] paxLoggingServiceImpl = new Object [ 1 ] ; Arrays . stream ( paxLogging . getClass ( ) . getDeclaredFields ( ) ) . forEach ( new FieldConsumer ( paxLoggingServiceImpl , paxLogging ) ) ; assertNull ( Helpers . getField ( paxLoggingServiceImpl [ 0 ] , "m_configLock" , ReadWriteLock . class ) ) ; LoggerFactory . getLogger ( Log4J1LockingConfigurationTest . class ) . info ( "Hello without locking" ) ; Helpers . updateLoggingConfig ( context , cm , Helpers . LoggingLibrary . LOG4J1 , "locks" ) ; sr = context . getServiceReference ( PaxLoggingService . class ) ; paxLogging = context . getService ( sr ) ; assertNotNull ( paxLogging ) ; paxLoggingServiceImpl [ 0 ] = null ; Arrays . stream ( paxLogging . getClass ( ) . getDeclaredFields ( ) ) . forEach ( new FieldConsumer ( paxLoggingServiceImpl , paxLogging ) ) ; assertNotNull ( Helpers . getField ( paxLoggingServiceImpl [ 0 ] , "m_configLock" , ReadWriteLock . class ) ) ; LoggerFactory . getLogger ( Log4J1LockingConfigurationTest . class ) . info ( "Hello with locking" ) ; List < String > lines = readLines ( ) ; assertTrue ( lines . contains ( "[main] INFO org.ops4j.pax.logging.it.Log4J1LockingConfigurationTest - Hello without locking" ) ) ; assertTrue ( lines . contains ( "[main] INFO org.ops4j.pax.logging.it.Log4J1LockingConfigurationTest - Hello with locking" ) ) ; }
@ Test public void turnOnLocks ( ) throws IOException { ServiceReference < PaxLoggingService > sr = context . getServiceReference ( PaxLoggingService . class ) ; PaxLoggingService paxLogging = context . getService ( sr ) ; assertNotNull ( paxLogging ) ; final Object [ ] paxLoggingServiceImpl = new Object [ 1 ] ; Arrays . stream ( paxLogging . getClass ( ) . getDeclaredFields ( ) ) . forEach ( new FieldConsumer ( paxLoggingServiceImpl , paxLogging ) ) ; assertNull ( Helpers . getField ( paxLoggingServiceImpl [ 0 ] , "m_configLock" , ReadWriteLock . class ) ) ; LoggerFactory . getLogger ( Log4J1LockingConfigurationTest . class ) . info ( "Hello without locking" ) ; Helpers . updateLoggingConfig ( context , cm , Helpers . LoggingLibrary . LOG4J1 , "locks" ) ; sr = context . getServiceReference ( PaxLoggingService . class ) ; paxLogging = context . getService ( sr ) ; assertNotNull ( paxLogging ) ; paxLoggingServiceImpl [ 0 ] = null ; Arrays . stream ( paxLogging . getClass ( ) . getDeclaredFields ( ) ) . forEach ( new FieldConsumer ( paxLoggingServiceImpl , paxLogging ) ) ; assertNotNull ( Helpers . getField ( paxLoggingServiceImpl [ 0 ] , "m_configLock" , ReadWriteLock . class ) ) ; LoggerFactory . getLogger ( Log4J1LockingConfigurationTest . class ) . info ( "Hello with locking" ) ; List < String > lines = readLines ( ) ; assertTrue ( lines . contains ( "[main] INFO org.ops4j.pax.logging.it.Log4J1LockingConfigurationTest - Hello without locking" ) ) ; assertTrue ( lines . contains ( "[main] INFO org.ops4j.pax.logging.it.Log4J1LockingConfigurationTest - Hello with locking" ) ) ; }
public void test() { try { String timePeriod = null ; WorkflowRunStatus status = options . valueOf ( statusSpec ) ; code_block = IfStatement ; Date firstDate = null , lastDate = null ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; } catch ( IOException e ) { Log . error ( e . getMessage ( ) , e ) ; ret . setExitStatus ( ReturnValue . FILENOTREADABLE ) ; ret . setDescription ( e . getMessage ( ) ) ; } }
public void test() { for ( Object str : queue . toArray ( ) ) { _logger . debug ( "pruneQueueIfNeeded - (" + attributeName + ") queue entry (" + str + " " + new java . util . Date ( Long . parseLong ( ( String ) str ) ) ) ; } }
public void test() { try { pool . close ( ) ; } catch ( Exception e ) { LOG . error ( "Close channel for location " + loc + " error " , e ) ; } }
public void test() { try ( PersistenceManager persistenceManager = PersistenceManagerFactory . getInstance ( settings ) . create ( ) ) { subscriptions . get ( entityType ) . handleEntityChanged ( persistenceManager , entity , fields ) ; } catch ( Exception ex ) { LOGGER . error ( "error handling MQTT subscriptions" , ex ) ; } }
public void test() { if ( format . equalsIgnoreCase ( "csv" ) ) { log . info ( "Sending Map messages on '" + topicName + "' topic" ) ; publishMapMessages ( producer , session , messagesList ) ; } else { log . info ( "Sending  " + format + " messages on '" + topicName + "' topic" ) ; publishTextMessage ( producer , session , messagesList ) ; } }
public void test() { if ( format . equalsIgnoreCase ( "csv" ) ) { log . info ( "Sending Map messages on '" + topicName + "' topic" ) ; publishMapMessages ( producer , session , messagesList ) ; } else { log . info ( "Sending  " + format + " messages on '" + topicName + "' topic" ) ; publishTextMessage ( producer , session , messagesList ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( JMSException e ) { log . error ( "Can not subscribe." + e . getMessage ( ) , e ) ; } finally { producer . close ( ) ; session . close ( ) ; topicConnection . stop ( ) ; topicConnection . close ( ) ; } }
public void test() { try { Properties properties = new Properties ( ) ; String filePath = getTestDataFileLocation ( testCaseFolderName , dataFileName ) ; properties . load ( ClassLoader . getSystemClassLoader ( ) . getResourceAsStream ( "activemq.properties" ) ) ; Context context = new InitialContext ( properties ) ; TopicConnectionFactory connFactory = ( TopicConnectionFactory ) context . lookup ( "ConnectionFactory" ) ; TopicConnection topicConnection = connFactory . createTopicConnection ( ) ; topicConnection . start ( ) ; Session session = topicConnection . createTopicSession ( false , Session . AUTO_ACKNOWLEDGE ) ; Topic topic = session . createTopic ( topicName ) ; MessageProducer producer = session . createProducer ( topic ) ; List < String > messagesList = readFile ( filePath ) ; code_block = TryStatement ;  } catch ( Exception e ) { log . error ( "Error when publishing messages" + e . getMessage ( ) , e ) ; } }
public void test() { try { File file = new File ( location ) ; code_block = IfStatement ; } catch ( Exception e ) { LOGGER . error ( "Encrypt password in '" + location + "' error." , e ) ; } }
public void test() { try { IUserProfile userProfile = null ; UserDetails currentUser = this . getCurrentUser ( ) ; Object object = currentUser . getProfile ( ) ; code_block = IfStatement ; IUserProfile currentProfile = this . getUserProfile ( ) ; code_block = IfStatement ; } catch ( Throwable t ) { _logger . error ( "error in edit" , t ) ; return FAILURE ; } }
public void test() { if ( cause instanceof FileNotFoundException ) { code_block = IfStatement ; log . debug ( msg , e ) ; } else-if ( cause instanceof MergeConflictException ) { MergeConflictInfo info = new MergeConflictInfo ( ( MergeConflictException ) cause , project ) ; ConflictUtils . saveMergeConflict ( info ) ; msg = "Failed to save the project because of merge conflict." ; log . debug ( msg , e ) ; return ; } else { msg = "Failed to save the project. See logs for details." ; log . error ( msg , e ) ; } }
public void test() { try { systemDotProperties = new Properties ( systemPropertiesFile ) ; properties . add ( getSystemPropertyDetails ( SystemBaseUrl . EXTERNAL_HOST , EXTERNAL_HOST_TITLE , EXTERNAL_HOST_DESCRIPTION , null , systemDotProperties ) ) ; properties . add ( getSystemPropertyDetails ( SystemBaseUrl . EXTERNAL_HTTP_PORT , EXTERNAL_HTTP_PORT_TITLE , EXTERNAL_HTTP_PORT_DESCRIPTION , null , systemDotProperties ) ) ; properties . add ( getSystemPropertyDetails ( SystemBaseUrl . EXTERNAL_HTTPS_PORT , EXTERNAL_HTTPS_PORT_TITLE , EXTERNAL_HTTPS_PORT_DESCRIPTION , null , systemDotProperties ) ) ; properties . add ( getSystemPropertyDetails ( SystemBaseUrl . INTERNAL_HOST , INTERNAL_HOST_TITLE , INTERNAL_HOST_DESCRIPTION , null , systemDotProperties ) ) ; properties . add ( getSystemPropertyDetails ( SystemBaseUrl . INTERNAL_HTTP_PORT , INTERNAL_HTTP_PORT_TITLE , INTERNAL_HTTP_PORT_DESCRIPTION , null , systemDotProperties ) ) ; properties . add ( getSystemPropertyDetails ( SystemBaseUrl . INTERNAL_HTTPS_PORT , INTERNAL_HTTPS_PORT_TITLE , INTERNAL_HTTPS_PORT_DESCRIPTION , null , systemDotProperties ) ) ; properties . add ( getSystemPropertyDetails ( SystemInfo . ORGANIZATION , ORGANIZATION_TITLE , ORGANIZATION_DESCRIPTION , null , systemDotProperties ) ) ; properties . add ( getSystemPropertyDetails ( SystemInfo . SITE_CONTACT , SITE_CONTACT_TITLE , SITE_CONTACT_DESCRIPTION , null , systemDotProperties ) ) ; properties . add ( getSystemPropertyDetails ( SystemInfo . SITE_NAME , SITE_NAME_TITLE , SITE_NAME_DESCRIPTION , null , systemDotProperties ) ) ; properties . add ( getSystemPropertyDetails ( SystemInfo . VERSION , VERSION_TITLE , VERSION_DESCRIPTION , null , systemDotProperties ) ) ; } catch ( IOException e ) { LOGGER . warn ( "Exception while reading the system.properties file." , e ) ; } }
public void test() { try { String deleteCommentEncoded = request . getParameter ( PortalConstants . MODERATION_REQUEST_COMMENT ) ; User user = UserCacheHolder . getUserFromRequest ( request ) ; code_block = IfStatement ; ComponentService . Iface client = new ThriftClients ( ) . makeComponentClient ( ) ; return client . deleteRelease ( releaseId , UserCacheHolder . getUserFromRequest ( request ) ) ; } catch ( TException e ) { log . error ( "Could not delete release from DB" , e ) ; } }
@ GetMapping ( "featured" ) @ ResponseBody @ Deprecated public List < Object > getFeaturedOccurrences ( ) { LOG . warn ( "Featured occurrences have been removed." ) ; return Lists . newArrayList ( ) ; }
private JavaPairRDD < HoodieKey , HoodieRecordLocation > lookupIndex ( JavaPairRDD < String , String > partitionRecordKeyPairRDD , final HoodieEngineContext context , final HoodieTable hoodieTable ) { Map < String , Long > recordsPerPartition = partitionRecordKeyPairRDD . countByKey ( ) ; List < String > affectedPartitionPathList = new ArrayList < > ( recordsPerPartition . keySet ( ) ) ; List < Tuple2 < String , BloomIndexFileInfo > > fileInfoList = loadInvolvedFiles ( affectedPartitionPathList , context , hoodieTable ) ; final Map < String , List < BloomIndexFileInfo > > partitionToFileInfo = fileInfoList . stream ( ) . collect ( groupingBy ( Tuple2 :: _1 , mapping ( Tuple2 :: _2 , toList ( ) ) ) ) ; JavaRDD < Tuple2 < String , HoodieKey > > fileComparisonsRDD = explodeRecordRDDWithFileComparisons ( partitionToFileInfo , partitionRecordKeyPairRDD ) ; Map < String , Long > comparisonsPerFileGroup = computeComparisonsPerFileGroup ( recordsPerPartition , partitionToFileInfo , fileComparisonsRDD , context ) ; int inputParallelism = partitionRecordKeyPairRDD . partitions ( ) . size ( ) ; int joinParallelism = Math . max ( inputParallelism , config . getBloomIndexParallelism ( ) ) ; LOG . info ( "InputParallelism: ${" + inputParallelism + "}, IndexParallelism: ${" + config . getBloomIndexParallelism ( ) + "}" ) ; return findMatchingFilesForRecordKeys ( fileComparisonsRDD , joinParallelism , hoodieTable , comparisonsPerFileGroup ) ; }
@ Test public void printLogWhenNoSchema ( ) throws SQLException { StatusSummary status = StatusSummary . status ( Status . NOT_INITIALIZED ) . build ( ) ; expect ( databaseConnectionProvider . getConnection ( ) ) . andReturn ( databaseConnection ) ; expect ( cassandraSchemaService . getStatus ( ) ) . andReturn ( status ) ; expect ( injector . getInstance ( NoopServer . class ) ) . andReturn ( noopServer ) ; logger . error ( "Cassandra schema not installed, starting administration services only" ) ; expectLastCall ( ) ; mocks . replay ( ) ; new ServerFactoryModule . LateInjectionServer ( injector , serverConfiguration ) . createServer ( ) ; mocks . verify ( ) ; }
protected String addColumns ( String queryString ) { StringBuilder select = new StringBuilder ( "SELECT " ) ; int from = 0 ; code_block = ForStatement ; select . deleteCharAt ( select . length ( ) - 1 ) ; select . append ( " " ) ; logger . debug ( "Replacing current select statement with " + select . toString ( ) ) ; from = queryString . indexOf ( "FROM" ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; queryString = queryString . substring ( from , queryString . length ( ) ) ; queryString = select . toString ( ) + queryString ; logger . debug ( "Final query with columns " + queryString ) ; return queryString ; }
public void test() { if ( from < 0 ) { logger . warn ( "Syntax error with the MySQL FROM statement. Syntax permitted is FROM, from or From " ) ; } }
public void test() { if ( sodium == null ) { logger . debug ( "Unable to decrypt event because libsodium is not loaded" ) ; return ; } }
public void test() { if ( bb . capacity ( ) != 70 ) { logger . info ( "Received malformed version 1 doorbell event, length not 70 bytes" ) ; return ; } }
public void test() { try { logger . trace ( "Calling cryptoPwHash with passwordFirstFive='{}', opslimit={}, memlimit={}, salt='{}'" , password5 , opslimit , memlimit , HexUtils . bytesToHex ( salt , " " ) ) ; String hashAsString = sodium . cryptoPwHash ( password5 , 32 , salt , opslimit , new NativeLong ( memlimit ) , PwHash . Alg . PWHASH_ALG_ARGON2I13 ) ; hash = HexUtils . hexToBytes ( hashAsString ) ; } catch ( SodiumException e ) { logger . info ( "Got SodiumException" , e ) ; return ; } }
public void test() { if ( decryptedTextLength != 18L ) { logger . info ( "Length of decrypted text is invalid, must be 18 bytes" ) ; return ; } }
public void startSahiTestSuite ( ) throws SakuliInitException { logger . info ( "Start Sakuli-Test-Suite from folder \"" + testSuite . getTestSuiteFolder ( ) . toAbsolutePath ( ) . toString ( ) + "\"" ) ; checkTestSuiteFile ( ) ; TestRunner runner = getTestRunner ( ) ; runner . setIsSingleSession ( false ) ; runner . addReport ( new Report ( "html" , sakuliProperties . getLogFolder ( ) . toAbsolutePath ( ) . toString ( ) ) ) ; runner . setInitJS ( getInitJSString ( ) ) ; code_block = TryStatement ;  }
public void test() { try { countConnections ++ ; logger . info ( "Sahi-Script-Runner starts!\n" ) ; String output = runner . execute ( ) ; testSuite . setStopDate ( new Date ( ) ) ; logger . info ( "test suite '" + testSuite . getId ( ) + "' stopped at " + TestSuite . GUID_DATE_FORMATE . format ( testSuite . getStopDate ( ) ) ) ; logger . info ( "Sahi-Script-Runner executed with " + output ) ; code_block = IfStatement ; } catch ( ConnectException | IllegalMonitorStateException e ) { this . reconnect ( e ) ; } }
public void test() { try { countConnections ++ ; logger . info ( "Sahi-Script-Runner starts!\n" ) ; String output = runner . execute ( ) ; testSuite . setStopDate ( new Date ( ) ) ; logger . info ( "test suite '" + testSuite . getId ( ) + "' stopped at " + TestSuite . GUID_DATE_FORMATE . format ( testSuite . getStopDate ( ) ) ) ; logger . info ( "Sahi-Script-Runner executed with " + output ) ; code_block = IfStatement ; } catch ( ConnectException | IllegalMonitorStateException e ) { this . reconnect ( e ) ; } }
public void test() { if ( isSahiScriptTimout ( testSuite . getException ( ) ) ) { logger . warn ( "Sahi-Script-Runner timeout detected, start retry!" ) ; SakuliCheckedException causingError = new SakuliCheckedException ( testSuite . getException ( ) ) ; InitializingServiceHelper . invokeInitializingServcies ( ) ; this . reconnect ( causingError ) ; } else-if ( testSuite . getException ( ) == null ) { throw new SakuliInitException ( "SAHI-Proxy returned 'FAILURE' " ) ; } }
public void test() { if ( dirPath != null ) { this . logger . warn ( "You're using the deprecated [{}] configuration property. You should instead use the " + "newer [{}] one" , PROPERTY_DEPRECATED_PERMANENTDIRECTORY , "environment.permanentDirectory" ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Cache remove: " + userCert . getSubjectDN ( ) ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "addBooleanField fieldName: {}; value: {}" , fieldName , value ) ; } }
public void test() { try { reader . setFeature ( XMLConstants . FEATURE_SECURE_PROCESSING , true ) ; reader . setFeature ( "http://xml.org/sax/features/external-general-entities" , false ) ; reader . setFeature ( "http://xml.org/sax/features/external-parameter-entities" , false ) ; reader . setFeature ( "http://apache.org/xml/features/nonvalidating/load-external-dtd" , false ) ; } catch ( SAXException e ) { logger . error ( "Some parser properties are not supported." ) ; } }
public void test() { try ( Tx tx = StructrApp . getInstance ( securityContext ) . tx ( ) ) { isRegularFile = file instanceof File ; tx . success ( ) ; } catch ( FrameworkException fex ) { logger . error ( "" , fex ) ; } }
public void test() { try { _jedisClusterClient . expire ( key , ttl ) ; } catch ( Exception ex ) { _logger . error ( "Exception in cache service: {} " , ex . getMessage ( ) ) ; } }
public void test() { if ( controlled ) { log . info ( processDescription + " is being shutdown" ) ; } else { log . warn ( processDescription + ": unexpected shutdown!" ) ; } }
public void test() { if ( controlled ) { log . info ( processDescription + " is being shutdown" ) ; } else { log . warn ( processDescription + ": unexpected shutdown!" ) ; } }
public void test() { try { webhookManager . saveDeployUpdateForRetry ( deployUpdate ) ; } catch ( Throwable t2 ) { LOG . error ( "Could not save update to zk for retry, dropping" , t2 ) ; } }
@ PayloadRoot ( localPart = "ActivateOrganisationRequest" , namespace = DEVICE_MANAGEMENT_NAMESPACE ) @ ResponsePayload public ActivateOrganisationResponse activateOrganisation ( @ OrganisationIdentification final String organisationIdentification , @ RequestPayload final ActivateOrganisationRequest request ) throws OsgpException { LOGGER . info ( "Activate organisation: {}." , request . getOrganisationIdentification ( ) ) ; code_block = TryStatement ;  return new ActivateOrganisationResponse ( ) ; }
@ Override public void onResourceChange ( TbResource resource , TbQueueCallback callback ) { TenantId tenantId = resource . getTenantId ( ) ; log . trace ( "[{}][{}][{}] Processing change resource" , tenantId , resource . getResourceType ( ) , resource . getResourceKey ( ) ) ; TransportProtos . ResourceUpdateMsg resourceUpdateMsg = TransportProtos . ResourceUpdateMsg . newBuilder ( ) . setTenantIdMSB ( tenantId . getId ( ) . getMostSignificantBits ( ) ) . setTenantIdLSB ( tenantId . getId ( ) . getLeastSignificantBits ( ) ) . setResourceType ( resource . getResourceType ( ) . name ( ) ) . setResourceKey ( resource . getResourceKey ( ) ) . build ( ) ; ToTransportMsg transportMsg = ToTransportMsg . newBuilder ( ) . setResourceUpdateMsg ( resourceUpdateMsg ) . build ( ) ; broadcast ( transportMsg , callback ) ; }
public void register ( ) { Freedomotic . INJECTOR . injectMembers ( this ) ; LOG . info ( "Registering the trigger named \"{}\"" , getName ( ) ) ; listener = new BusMessagesListener ( this , busService ) ; listener . consumeEventFrom ( channel ) ; numberOfExecutions = 0 ; suspensionStart = System . currentTimeMillis ( ) ; }
public void test() { if ( isNullOrEmpty ( model ) || isNullOrEmpty ( namespaceName ) ) { LOGGER . warn ( String . format ( Locale . ROOT , "Could not import: '%s' - invalid namespace: '%s'" , model , entry . getValue ( ) ) ) ; importMapping . remove ( model ) ; } else { LOGGER . info ( String . format ( Locale . ROOT , "Importing: '%s' from '%s' namespace." , model , namespaceName ) ) ; importMapping . put ( model , namespaceName ) ; } }
public void test() { if ( error == Errors . NONE ) { log . debug ( "LeaveGroup response with {} returned successfully: {}" , sentGeneration , response ) ; future . complete ( null ) ; } else { log . error ( "LeaveGroup request with {} failed with error: {}" , sentGeneration , error . message ( ) ) ; future . raise ( error ) ; } }
public void test() { if ( error == Errors . NONE ) { log . debug ( "LeaveGroup response with {} returned successfully: {}" , sentGeneration , response ) ; future . complete ( null ) ; } else { log . error ( "LeaveGroup request with {} failed with error: {}" , sentGeneration , error . message ( ) ) ; future . raise ( error ) ; } }
@ BeforeClass public static void extractTestFiles ( ) throws URISyntaxException { ZipUtils . unZipFile ( new File ( CustomCRSKDERasterResizeIT . class . getClassLoader ( ) . getResource ( TEST_DATA_ZIP_RESOURCE_PATH ) . toURI ( ) ) , TestUtils . TEST_CASE_BASE ) ; startMillis = System . currentTimeMillis ( ) ; LOGGER . warn ( "-------------------------------------------------" ) ; LOGGER . warn ( "*                                               *" ) ; LOGGER . warn ( "*         RUNNING CustomCRSKDERasterResizeIT    *" ) ; LOGGER . warn ( "*                                               *" ) ; LOGGER . warn ( "-------------------------------------------------" ) ; code_block = TryStatement ;  }
public void test() { try { SparkTestEnvironment . getInstance ( ) . tearDown ( ) ; } catch ( final Exception e ) { LOGGER . warn ( "Unable to tear down default spark session" , e ) ; } }
public void test() { try { S instance = iterator . next ( ) ; services . add ( instance ) ; } catch ( ServiceConfigurationError serviceConfigurationError ) { logger . error ( "Error while loading implementations of {}" , service . getName ( ) , serviceConfigurationError ) ; } }
public void test() { if ( reclaimMaxAge < 1 ) { log . warn ( "Reclaim max age parameter is less then 1, are your sure?" ) ; } }
public void test() { if ( reclaimMaxAge < 1 ) { log . warn ( "Reclaim max age parameter is less then 1, are your sure?" ) ; } }
@ Override public long takeSnapshot ( ) throws IOException { TermIndex lastTermIndex = getLastAppliedTermIndex ( ) ; long lastAppliedIndex = lastTermIndex . getIndex ( ) ; code_block = IfStatement ; long startTime = Time . monotonicNow ( ) ; TransactionInfo latestTrxInfo = transactionBuffer . getLatestTrxInfo ( ) ; TransactionInfo lastAppliedTrxInfo = TransactionInfo . fromTermIndex ( lastTermIndex ) ; code_block = IfStatement ; transactionBuffer . flush ( ) ; LOG . info ( "Current Snapshot Index {}, takeSnapshot took {} ms" , lastAppliedIndex , Time . monotonicNow ( ) - startTime ) ; return lastAppliedIndex ; }
public void test() { try { ObjectAndMethod objectAndMethod = taskQueue . poll ( THREAD_SHUTDOWN_CHECK_INTERVAL , TimeUnit . MILLISECONDS ) ; code_block = IfStatement ; } catch ( InterruptedException e ) { log . warn ( "Cache Listener thread interrupted in MultiThreadedListener." , e ) ; } }
public void test() { try { gpio . setValue ( configuration . invert != ( OnOffType . ON . equals ( command ) ) ) ; } catch ( PigpioException e ) { logger . warn ( "An error occured while changing the gpio value: {}" , e . getMessage ( ) ) ; } }
public void test() { try { gpio . setValue ( configuration . invert != ( OnOffType . ON . equals ( command ) ) ) ; } catch ( PigpioException e ) { logger . warn ( "An error occured while changing the gpio value: {}" , e . getMessage ( ) ) ; } }
public void test() { try { listener . accept ( propertyEntry . getKey ( ) , propertyEntry . getValue ( ) ) ; } catch ( RuntimeException re ) { logger . warn ( "calling property change listener {} failed. {}" , listener , re . getMessage ( ) ) ; } }
public void test() { if ( ( propertyStatus != null ) && ( propertyStatus . messageType != null ) && ( propertyStatus . messageType . equals ( "propertyStatus" ) ) ) { code_block = ForStatement ; } else { logger . debug ( "Ignoring received message of unknown type: {}" , message ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( JsonSyntaxException se ) { logger . warn ( "received invalid message: {}" , message ) ; } }
public void test() { try { Matcher matcher = pattern . matcher ( value . getString ( ) ) ; return matcher . matches ( ) ; } catch ( RepositoryException e ) { log . warn ( "Error checking string constraint " + this , e ) ; return false ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "addNULLField fieldValue: {}" , fieldValue ) ; } }
public void test() { if ( annotationType == null ) { logger . debug ( "Couldn't resolve " + visibleAnnotation . desc + " annotation type whilst searching for hints on " + getName ( ) ) ; } else { annotationType . collectHints ( visibleAnnotation , hints , visited , annotationChain ) ; } }
public void test() { try { return httpRequest ( logger , url , method , requestBodyData , responseFormat ) ; } catch ( IOException e ) { logger . info ( "{} {}: error: {}" , method , url , e . getMessage ( ) ) ; exc = e ; } }
@ Override protected DataTable doInBackground ( ) throws Exception { long rowCount ; code_block = IfStatement ; publish ( new NodeProgress ( 0.0 , "Starting table sort..." ) ) ; Collection < String > sortColNames = new ArrayList < String > ( 2 ) ; DataTableSpec spec = m_inputTable . getDataTableSpec ( ) ; code_block = ForStatement ; long start = System . currentTimeMillis ( ) ; LOGGER . debug ( "Starting interactive table sorting on column(s) " + sortColNames ) ; boolean [ ] sortOrders = m_sortOrder . getSortColumnOrder ( ) ; DataTableSorter sorter = new DataTableSorter ( m_inputTable , rowCount , sortColNames , sortOrders , false ) ; NodeProgressListener progLis = new NodeProgressListener ( ) code_block = "" ; ; m_nodeProgressMonitor = new DefaultNodeProgressMonitor ( ) ; ExecutionMonitor exec = new ExecutionMonitor ( m_nodeProgressMonitor ) ; m_nodeProgressMonitor . addProgressListener ( progLis ) ; code_block = TryStatement ;  }
public void test() { try { DataTable result = sorter . sort ( exec ) ; long elapsedMS = System . currentTimeMillis ( ) - start ; String time = StringFormat . formatElapsedTime ( elapsedMS ) ; LOGGER . debug ( "Interactive table sorting finished (" + time + ")" ) ; return result ; } finally { m_nodeProgressMonitor . removeProgressListener ( progLis ) ; } }
public void test() { if ( result == null ) { logger . info ( "Outbound interface is NULL! Looks like there was no " + transport + " in the list of connectors" ) ; } else { logger . info ( "Outbound interface found: " + result . toString ( ) ) ; } }
public void test() { if ( result == null ) { logger . info ( "Outbound interface is NULL! Looks like there was no " + transport + " in the list of connectors" ) ; } else { logger . info ( "Outbound interface found: " + result . toString ( ) ) ; } }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "ServletContext return null or empty list of connectors" ) ; } }
public void test() { try { System . setProperty ( IGNITE_PDS_CHECKPOINT_TEST_SKIP_SYNC , "true" ) ; final int gridsCnt = 5 ; final int groupsCnt = 2 ; final IgniteEx node = ( IgniteEx ) startGridsMultiThreaded ( gridsCnt ) ; final List < CacheConfiguration > cfgs = Arrays . asList ( cacheConfiguration ( "g1c1" , TRANSACTIONAL , PARTITIONED , gridsCnt , "testGrp1" ) , cacheConfiguration ( "g1c2" , TRANSACTIONAL , PARTITIONED , gridsCnt , "testGrp1" ) , cacheConfiguration ( "g2c1" , TRANSACTIONAL , PARTITIONED , gridsCnt , "testGrp2" ) , cacheConfiguration ( "g2c2" , TRANSACTIONAL , PARTITIONED , gridsCnt , "testGrp2" ) ) ; node . getOrCreateCaches ( cfgs ) ; validateDepIds ( groupsCnt ) ; stopAllGrids ( ) ; IgniteEx node2 = ( IgniteEx ) startGridsMultiThreaded ( gridsCnt ) ; validateDepIds ( groupsCnt ) ; final int restartIdxFrom = 2 ; final AtomicInteger idx = new AtomicInteger ( restartIdxFrom ) ; IgniteInternalFuture fut = GridTestUtils . runMultiThreadedAsync ( new Callable < Void > ( ) code_block = "" ; , gridsCnt - restartIdxFrom , "stop-node" ) ; fut . get ( ) ; awaitPartitionMapExchange ( ) ; checkAffinity ( ) ; idx . set ( restartIdxFrom ) ; fut = GridTestUtils . runMultiThreadedAsync ( new Callable < Void > ( ) code_block = "" ; , gridsCnt - restartIdxFrom , "start-node" ) ; fut . get ( ) ; awaitPartitionMapExchange ( ) ; AffinityTopologyVersion topVer = node2 . context ( ) . cache ( ) . context ( ) . exchange ( ) . readyAffinityVersion ( ) ; log . info ( "Using version: " + topVer ) ; checkAffinity ( ) ; } finally { System . clearProperty ( IGNITE_PDS_CHECKPOINT_TEST_SKIP_SYNC ) ; } }
public void test() { try { connection = DBUtil . getConnection ( ) ; statement = connection . prepareStatement ( sql ) ; statement . setLong ( 1 , messageId ) ; int count = statement . executeUpdate ( ) ; LOG . info ( "Update rows {}" , count ) ; } catch ( SQLException e ) { e . printStackTrace ( ) ; Utility . printExecption ( LOG , e , RDBS_Exception ) ; } finally { DBUtil . closeDB ( connection , statement ) ; } }
public void test() { try { getService ( ResourceService . class ) . check ( resourceTO ) ; check = true ; } catch ( Exception e ) { LOG . error ( "Connector not found {}" , resourceTO . getConnector ( ) , e ) ; errorMessage = e . getMessage ( ) ; } }
private void getServerProfileTransformation ( ) { ServerProfile serverProfile = this . serverProfileClient . getByName ( SERVER_PROFILE_NAME ) . get ( 0 ) ; String enclosureGroupUri = enclosureGroupClient . getByName ( EnclosureGroupClientSample . ENCLOSURE_GROUP_NAME ) . get ( 0 ) . getUri ( ) ; ServerProfile serverProfileUpdated = serverProfileClient . getTransformation ( serverProfile . getResourceId ( ) , ServerHardwareTypeClientSample . SERVER_HARDWARE_TYPE_URI , enclosureGroupUri ) ; LOGGER . info ( "ServerProfile object returned to client : " + serverProfileUpdated . toJsonString ( ) ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "updateProcessBusinessKeyInHistory : {}" , processInstance . getId ( ) ) ; } }
public void test() { try { scan ( ) ; code_block = IfStatement ; } catch ( Throwable e ) { logger . error ( "scan failed" , e ) ; } }
public void test() { if ( instanceofLAL ) { FormattingTuple ft = MessageFormatter . arrayFormat ( format , argArray ) ; ( ( LocationAwareLogger ) logger ) . log ( marker , fqcn , LocationAwareLogger . DEBUG_INT , ft . getMessage ( ) , argArray , ft . getThrowable ( ) ) ; } else { logger . debug ( marker , format , argArray ) ; } }
public void test() { if ( LOGGER . isLoggable ( Level . INFO ) ) { LOGGER . info ( "Loaded coverage store '" + cs . getName ( ) + "', " + ( cs . isEnabled ( ) ? "enabled" : "disabled" ) ) ; } }
public void test() { if ( processHandle == null ) { log . error ( "processHandle is null when looking up elapsed time" ) ; return "" ; } else { long elapsedTime = processHandle . getExecutingTime ( ) ; return formatTimePeriod ( elapsedTime ) ; } }
public void test() { if ( System . currentTimeMillis ( ) - this . startTime >= this . options . getPeriod ( ) * 1000 ) { logger . info ( "startLeScan" ) ; this . bluetoothAdapter . startLeScan ( this ) ; this . startTime = System . currentTimeMillis ( ) ; } }
void publishHintEvents ( Collection < HintEvent > hintEvents , String atomURI ) { BulkHintEvent bulkHintEvent = new BulkHintEvent ( ) ; bulkHintEvent . addHintEvents ( hintEvents ) ; pubSubMediator . tell ( new DistributedPubSubMediator . Publish ( bulkHintEvent . getClass ( ) . getName ( ) , bulkHintEvent ) , getSelf ( ) ) ; log . debug ( "sparql-based matching for atom {} (found {} matches)" , atomURI , bulkHintEvent . getHintEvents ( ) . size ( ) ) ; }
@ Override public void discoverObjectInstance ( ObjectInstanceHandle theObject , ObjectClassHandle theObjectClass , String objectName , FederateHandle producingFederate ) throws FederateInternalError { LOGGER . info ( "Discover Object Instance : " + "Object = " + theObject . toString ( ) + ", Object class = " + theObjectClass . toString ( ) + ", Object name = " + objectName + ", Producing federate = " + producingFederate . toString ( ) ) ; System . out . println ( ) ; }
public void destroy ( ) { String methodName = "destroy" ; LOGGER . trace ( ENTERING , methodName ) ; LOGGER . trace ( EXITING , methodName ) ; }
public void destroy ( ) { String methodName = "destroy" ; LOGGER . trace ( ENTERING , methodName ) ; LOGGER . trace ( EXITING , methodName ) ; }
private void setNetworkStateOnline ( ) { logger . debug ( "Network state ONLINE: Process running. {} Nodes in network." , networkNodes . size ( ) ) ; localNwkAddress = transport . getNwkAddress ( ) ; localIeeeAddress = transport . getIeeeAddress ( ) ; addLocalNode ( ) ; code_block = IfStatement ; code_block = ForStatement ; code_block = ForStatement ; code_block = ForStatement ; }
public void test() { try { URL url = classLoader . getResource ( "META-INF/javadocs-rt.xml" ) ; code_block = IfStatement ; code_block = TryStatement ;  } catch ( Exception exception ) { _log . error ( exception , exception ) ; } }
public void test() { if ( username != null ) { properties . put ( "quarkus.datasource.username" , username ) ; } else { log . debug ( "Property 'username' was not found" ) ; } }
public void test() { if ( password != null ) { properties . put ( "quarkus.datasource.password" , password ) ; } else { log . debug ( "Property 'password' was not found" ) ; } }
public void test() { if ( ( host != null ) && ( database != null ) ) { String portPart = "" ; code_block = IfStatement ; properties . put ( "quarkus.datasource.jdbc.url" , String . format ( "jdbc:%s://%s%s/%s" , urlType , host , portPart , database ) ) ; } else { log . debug ( "One or more of 'host' or 'database' properties were not found" ) ; } }
@ Override public void createTopic ( String topic , int partitions , int replicationFactor ) { AdminUtils . createTopic ( zookeeperClient , topic , partitions , replicationFactor , new Properties ( ) ) ; LOGGER . debug ( "Creating topic: " + topic + " , partitions: " + partitions + " , " + "replication factor: " + replicationFactor + "." ) ; }
@ Override public void flush ( ) { log . debug ( "Going to store validation result in key-value storage" ) ; Namespace namespace = namespace ( ) ; KeyValueStorage keyValueStorage = kernelContext . getService ( KeyValueStorage . class ) ; keyValueStorage . put ( namespace , RESULT , ValidationResult . create ( validator . getName ( ) , invoked , failed ) ) ; log . debug ( "invoked {} failed {}" , invoked , failed ) ; }
@ Override public void flush ( ) { log . debug ( "Going to store validation result in key-value storage" ) ; Namespace namespace = namespace ( ) ; KeyValueStorage keyValueStorage = kernelContext . getService ( KeyValueStorage . class ) ; keyValueStorage . put ( namespace , RESULT , ValidationResult . create ( validator . getName ( ) , invoked , failed ) ) ; log . debug ( "invoked {} failed {}" , invoked , failed ) ; }
@ Override public void validate ( final InstallServiceValidationContext validationContext ) throws RestErrorException { logger . info ( "Validating service name" ) ; final String serviceName = validationContext . getService ( ) . getName ( ) ; code_block = IfStatement ; code_block = IfStatement ; }
public void test() { try { int returnValue = DDLRecordSetServiceUtil . searchCount ( companyId , groupId , keywords , scope ) ; return returnValue ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try ( final Tx tx = app . tx ( ) ) { code_block = ForStatement ; tx . success ( ) ; } catch ( FrameworkException fex ) { logger . warn ( "" , fex ) ; } }
public void test() { try { LOGGER . debug ( "FÃ¼ge Textfragment von URL '{}' ein." , urlStr ) ; insertDocumentFromURL ( cmd , WollMuxFiles . makeURL ( urlStr ) ) ; } catch ( java . lang . Exception e ) { AbstractExecutor . insertErrorField ( cmd , documentCommandInterpreter . getModel ( ) . doc , e ) ; cmd . setErrorState ( true ) ; return 1 ; } }
@ Override public void init ( NodeEngine engine , Properties hzProperties ) { this . nodeEngine = ( NodeEngineImpl ) engine ; this . jetInstance = new JetInstanceImpl ( nodeEngine . getNode ( ) . hazelcastInstance , config ) ; taskletExecutionService = new TaskletExecutionService ( nodeEngine , config . getInstanceConfig ( ) . getCooperativeThreadCount ( ) , nodeEngine . getProperties ( ) ) ; jobRepository = new JobRepository ( jetInstance ) ; jobExecutionService = new JobExecutionService ( nodeEngine , taskletExecutionService , jobRepository ) ; jobCoordinationService = createJobCoordinationService ( ) ; MetricsService metricsService = nodeEngine . getService ( MetricsService . SERVICE_NAME ) ; metricsService . registerPublisher ( nodeEngine -> new JobMetricsPublisher ( jobExecutionService , nodeEngine . getLocalMember ( ) ) ) ; nodeEngine . getMetricsRegistry ( ) . registerDynamicMetricsProvider ( jobExecutionService ) ; networking = new Networking ( engine , jobExecutionService , config . getInstanceConfig ( ) . getFlowControlPeriodMs ( ) ) ; ClientEngine clientEngine = engine . getService ( ClientEngineImpl . SERVICE_NAME ) ; ClientExceptionFactory clientExceptionFactory = clientEngine . getExceptionFactory ( ) ; code_block = IfStatement ; logger . info ( "Setting number of cooperative threads and default parallelism to " + config . getInstanceConfig ( ) . getCooperativeThreadCount ( ) ) ; code_block = IfStatement ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( exception , exception ) ; } }
public void test() { try { result . setState ( SaveState . COMBINING ) ; combine ( saveContext , result ) ; } catch ( Throwable e ) { LOG . error ( "Master combine model files failed " , e ) ; saveFailed ( result , StringUtils . stringifyException ( e ) ) ; } }
public void test() { if ( canCombine ( ) ) { ModelSaveContext saveContext = saveContexts . get ( subResult . getRequestId ( ) ) ; code_block = TryStatement ;  } else { String failedMsg = combineFailedLogs ( ) ; LOG . error ( "PS save model failed. " + failedMsg ) ; saveFailed ( result , failedMsg ) ; } }
public void test() { if ( ActiveMQRALogger . LOGGER . isTraceEnabled ( ) ) { ActiveMQRALogger . LOGGER . trace ( "readShort()" ) ; } }
public void test() { try { return getChildren ( path , true ) ; } catch ( ZkNoNodeException e ) { LOG . info ( "zkclient{} watchForChilds path not existing:{} skipWatchingNodeNoteExist: {}" , _uid , path , skipWatchingNonExistNode ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Updating entity {}:{}  app {}\n" , entityId . getType ( ) , entityId . getUuid ( ) , appId ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Wrote {}:{} version {}" , cpEntity . getId ( ) . getType ( ) , cpEntity . getId ( ) . getUuid ( ) , cpEntity . getVersion ( ) ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "WriteUniqueVerifyException encountered during update of entity with id {}" , cpEntity . getId ( ) . getUuid ( ) ) ; } }
public void test() { try { FileUtils . writeStringToFile ( f , toWrite ) ; } catch ( IOException e2 ) { log . error ( "Error writing memory crash dump information to disk: {}" , f . getAbsolutePath ( ) , e2 ) ; } }
public static void writeMemoryCrashDump ( @ NonNull Model net , @ NonNull Throwable e ) { code_block = IfStatement ; long now = System . currentTimeMillis ( ) ; long tid = Thread . currentThread ( ) . getId ( ) ; String threadName = Thread . currentThread ( ) . getName ( ) ; crashDumpRootDirectory . mkdirs ( ) ; File f = new File ( crashDumpRootDirectory , "dl4j-memory-crash-dump-" + now + "_" + tid + ".txt" ) ; StringBuilder sb = new StringBuilder ( ) ; SimpleDateFormat sdf = new SimpleDateFormat ( "yyyy-MM-dd HH:mm:ss.SSS" ) ; sb . append ( "Deeplearning4j OOM Exception Encountered to " ) . append ( net . getClass ( ) . getSimpleName ( ) ) . append ( "\n" ) . append ( f ( "Timestamp: " , sdf . format ( now ) ) ) . append ( f ( "Thread ID" , tid ) ) . append ( f ( "Thread Name" , threadName ) ) . append ( "\n\n" ) ; sb . append ( "Stack Trace:\n" ) . append ( ExceptionUtils . getStackTrace ( e ) ) ; code_block = TryStatement ;  String toWrite = sb . toString ( ) ; code_block = TryStatement ;  log . error ( ">>> Out of Memory Exception Detected. Memory crash dump written to: {}" , f . getAbsolutePath ( ) ) ; log . warn ( "Memory crash dump reporting can be disabled with CrashUtil.crashDumpsEnabled(false) or using system " + "property -D" + DL4JSystemProperties . CRASH_DUMP_ENABLED_PROPERTY + "=false" ) ; log . warn ( "Memory crash dump reporting output location can be set with CrashUtil.crashDumpOutputDirectory(File) or using system " + "property -D" + DL4JSystemProperties . CRASH_DUMP_OUTPUT_DIRECTORY_PROPERTY + "=<path>" ) ; }
public static void writeMemoryCrashDump ( @ NonNull Model net , @ NonNull Throwable e ) { code_block = IfStatement ; long now = System . currentTimeMillis ( ) ; long tid = Thread . currentThread ( ) . getId ( ) ; String threadName = Thread . currentThread ( ) . getName ( ) ; crashDumpRootDirectory . mkdirs ( ) ; File f = new File ( crashDumpRootDirectory , "dl4j-memory-crash-dump-" + now + "_" + tid + ".txt" ) ; StringBuilder sb = new StringBuilder ( ) ; SimpleDateFormat sdf = new SimpleDateFormat ( "yyyy-MM-dd HH:mm:ss.SSS" ) ; sb . append ( "Deeplearning4j OOM Exception Encountered to " ) . append ( net . getClass ( ) . getSimpleName ( ) ) . append ( "\n" ) . append ( f ( "Timestamp: " , sdf . format ( now ) ) ) . append ( f ( "Thread ID" , tid ) ) . append ( f ( "Thread Name" , threadName ) ) . append ( "\n\n" ) ; sb . append ( "Stack Trace:\n" ) . append ( ExceptionUtils . getStackTrace ( e ) ) ; code_block = TryStatement ;  String toWrite = sb . toString ( ) ; code_block = TryStatement ;  log . error ( ">>> Out of Memory Exception Detected. Memory crash dump written to: {}" , f . getAbsolutePath ( ) ) ; log . warn ( "Memory crash dump reporting can be disabled with CrashUtil.crashDumpsEnabled(false) or using system " + "property -D" + DL4JSystemProperties . CRASH_DUMP_ENABLED_PROPERTY + "=false" ) ; log . warn ( "Memory crash dump reporting output location can be set with CrashUtil.crashDumpOutputDirectory(File) or using system " + "property -D" + DL4JSystemProperties . CRASH_DUMP_OUTPUT_DIRECTORY_PROPERTY + "=<path>" ) ; }
public static void writeMemoryCrashDump ( @ NonNull Model net , @ NonNull Throwable e ) { code_block = IfStatement ; long now = System . currentTimeMillis ( ) ; long tid = Thread . currentThread ( ) . getId ( ) ; String threadName = Thread . currentThread ( ) . getName ( ) ; crashDumpRootDirectory . mkdirs ( ) ; File f = new File ( crashDumpRootDirectory , "dl4j-memory-crash-dump-" + now + "_" + tid + ".txt" ) ; StringBuilder sb = new StringBuilder ( ) ; SimpleDateFormat sdf = new SimpleDateFormat ( "yyyy-MM-dd HH:mm:ss.SSS" ) ; sb . append ( "Deeplearning4j OOM Exception Encountered to " ) . append ( net . getClass ( ) . getSimpleName ( ) ) . append ( "\n" ) . append ( f ( "Timestamp: " , sdf . format ( now ) ) ) . append ( f ( "Thread ID" , tid ) ) . append ( f ( "Thread Name" , threadName ) ) . append ( "\n\n" ) ; sb . append ( "Stack Trace:\n" ) . append ( ExceptionUtils . getStackTrace ( e ) ) ; code_block = TryStatement ;  String toWrite = sb . toString ( ) ; code_block = TryStatement ;  log . error ( ">>> Out of Memory Exception Detected. Memory crash dump written to: {}" , f . getAbsolutePath ( ) ) ; log . warn ( "Memory crash dump reporting can be disabled with CrashUtil.crashDumpsEnabled(false) or using system " + "property -D" + DL4JSystemProperties . CRASH_DUMP_ENABLED_PROPERTY + "=false" ) ; log . warn ( "Memory crash dump reporting output location can be set with CrashUtil.crashDumpOutputDirectory(File) or using system " + "property -D" + DL4JSystemProperties . CRASH_DUMP_OUTPUT_DIRECTORY_PROPERTY + "=<path>" ) ; }
public void test() { try { InvocationContextImpl invocationContext = ( InvocationContextImpl ) filterContext . getAttribute ( "context" ) ; filterContext . setAttribute ( "startTime" , System . currentTimeMillis ( ) ) ; InvocationInfoImpl invocationInfo = new InvocationInfoImpl ( ) ; invocationContext . lastInvocationInfo ( invocationInfo ) ; code_block = IfStatement ; String logLevel = invocationContext . cookie ( SoaSystemEnvProperties . THREAD_LEVEL_KEY ) ; code_block = IfStatement ; MDC . put ( SoaSystemEnvProperties . KEY_LOGGER_SESSION_TID , invocationContext . sessionTid ( ) . map ( DapengUtil :: longToHexStr ) . orElse ( "0" ) ) ; String infoLog = "request[seqId:" + invocationContext . seqId ( ) + ", server:" + filterContext . getAttribute ( "serverInfo" ) + "]:" + "service[" + invocationContext . serviceName ( ) + "]:version[" + invocationContext . versionName ( ) + "]:method[" + invocationContext . methodName ( ) + "]" ; LOGGER . info ( getClass ( ) . getSimpleName ( ) + "::onEntry," + infoLog ) ; } finally { next . onEntry ( filterContext ) ; } }
public void test() { if ( commandSpec != null && ! groupAddressesWriteBlockedOnce . remove ( commandSpec . getGroupAddress ( ) ) ) { getClient ( ) . writeToKNX ( commandSpec ) ; code_block = IfStatement ; } else { logger . debug ( "None of the configured GAs on channel '{}' could handle the command '{}' of type '{}'" , channelUID , command , command . getClass ( ) . getSimpleName ( ) ) ; } }
public void test() { if ( classedModel instanceof FileEntry ) { className = FileEntry . class . getName ( ) ; } }
public void test() { try { super . init ( ) ; NotificationEdit edit = m_notificationService . addTransientNotification ( ) ; edit . setFunction ( eventId ( SECURE_ADD ) ) ; edit . addFunction ( eventId ( SECURE_UPDATE_OWN ) ) ; edit . addFunction ( eventId ( SECURE_UPDATE_ANY ) ) ; edit . setResourceFilter ( getAccessPoint ( true ) + Entity . SEPARATOR + REF_TYPE_MESSAGE ) ; edit . setAction ( siteEmailNotificationAnnc ) ; functionManager . registerFunction ( eventId ( SECURE_READ ) , true ) ; functionManager . registerFunction ( eventId ( SECURE_ADD ) , true ) ; functionManager . registerFunction ( eventId ( SECURE_REMOVE_ANY ) , true ) ; functionManager . registerFunction ( eventId ( SECURE_REMOVE_OWN ) , true ) ; functionManager . registerFunction ( eventId ( SECURE_UPDATE_ANY ) , true ) ; functionManager . registerFunction ( eventId ( SECURE_UPDATE_OWN ) , true ) ; functionManager . registerFunction ( eventId ( SECURE_ALL_GROUPS ) , true ) ; functionManager . registerFunction ( eventId ( SECURE_READ_DRAFT ) , true ) ; m_entityManager . registerEntityProducer ( this , REFERENCE_ROOT ) ; DocumentBuilderFactory factory = DocumentBuilderFactory . newInstance ( ) ; factory . setIgnoringComments ( true ) ; factory . setNamespaceAware ( true ) ; factory . setValidating ( false ) ; docBuilder = factory . newDocumentBuilder ( ) ; TransformerFactory tFactory = TransformerFactory . newInstance ( ) ; docTransformer = tFactory . newTransformer ( ) ; log . info ( "init()" ) ; } catch ( Throwable t ) { log . warn ( "init(): " , t ) ; } }
public void test() { try { logger . info ( "Shutting down the proxy..." ) ; } catch ( Throwable t ) { } }
public void test() { if ( size > 32768 ) { log . warn ( "[warning] skipping search @ " + size ) ; lastEnd = pos ; return false ; } else-if ( size > 4096 ) { log . warn ( "[warning] searching > 4k space @ " + size ) ; } }
public void test() { if ( size > 32768 ) { log . warn ( "[warning] skipping search @ " + size ) ; lastEnd = pos ; return false ; } else-if ( size > 4096 ) { log . warn ( "[warning] searching > 4k space @ " + size ) ; } }
public void test() { try { long len = file . length ( ) ; code_block = IfStatement ; } catch ( Exception ex ) { log . warn ( "" , ex ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( String . format ( "Incremented Redis key %s to %d" , sizeKey , incremented ) ) ; } }
public void test() { if ( method . equalsIgnoreCase ( ModelDBConstants . PUT ) ) { LOGGER . trace ( "NFSService - generatePresignedUrl - put url returned" ) ; return getUploadUrl ( parameters , config . artifactStoreConfig . protocol , config . artifactStoreConfig . artifactEndpoint . getArtifact , config . artifactStoreConfig . pickArtifactStoreHostFromConfig , config . artifactStoreConfig . host ) ; } else-if ( method . equalsIgnoreCase ( ModelDBConstants . GET ) ) { LOGGER . trace ( "NFSService - generatePresignedUrl - get url returned" ) ; String filename = artifactPath . substring ( artifactPath . lastIndexOf ( "/" ) ) ; parameters . put ( ModelDBConstants . FILENAME , filename ) ; return getDownloadUrl ( parameters , config . artifactStoreConfig . protocol , config . artifactStoreConfig . artifactEndpoint . getArtifact , config . artifactStoreConfig . pickArtifactStoreHostFromConfig , config . artifactStoreConfig . host ) ; } else { String errorMessage = "Unsupported HTTP Method for NFS Presigned URL" ; throw new InvalidArgumentException ( errorMessage ) ; } }
public void test() { if ( method . equalsIgnoreCase ( ModelDBConstants . PUT ) ) { LOGGER . trace ( "NFSService - generatePresignedUrl - put url returned" ) ; return getUploadUrl ( parameters , config . artifactStoreConfig . protocol , config . artifactStoreConfig . artifactEndpoint . getArtifact , config . artifactStoreConfig . pickArtifactStoreHostFromConfig , config . artifactStoreConfig . host ) ; } else-if ( method . equalsIgnoreCase ( ModelDBConstants . GET ) ) { LOGGER . trace ( "NFSService - generatePresignedUrl - get url returned" ) ; String filename = artifactPath . substring ( artifactPath . lastIndexOf ( "/" ) ) ; parameters . put ( ModelDBConstants . FILENAME , filename ) ; return getDownloadUrl ( parameters , config . artifactStoreConfig . protocol , config . artifactStoreConfig . artifactEndpoint . getArtifact , config . artifactStoreConfig . pickArtifactStoreHostFromConfig , config . artifactStoreConfig . host ) ; } else { String errorMessage = "Unsupported HTTP Method for NFS Presigned URL" ; throw new InvalidArgumentException ( errorMessage ) ; } }
@ Override public String requestInstance ( NetworkOrder networkOrder , AzureUser azureUser ) throws FogbowException { LOGGER . info ( Messages . Log . REQUESTING_INSTANCE_FROM_PROVIDER ) ; String resourceName = AzureGeneralUtil . generateResourceName ( ) ; String cidr = networkOrder . getCidr ( ) ; String name = networkOrder . getName ( ) ; Map tags = Collections . singletonMap ( AzureConstants . TAG_NAME , name ) ; String instanceId = AzureGeneralUtil . defineInstanceId ( resourceName ) ; AzureCreateVirtualNetworkRef azureCreateVirtualNetworkRef = AzureCreateVirtualNetworkRef . builder ( ) . resourceName ( resourceName ) . cidr ( cidr ) . tags ( tags ) . checkAndBuild ( ) ; AsyncInstanceCreationManager . Callbacks finishCreationCallbacks = startInstanceCreation ( instanceId ) ; doCreateInstance ( azureUser , azureCreateVirtualNetworkRef , finishCreationCallbacks ) ; waitAndCheckForInstanceCreationFailed ( instanceId ) ; return instanceId ; }
public void test() { try { code_block = IfStatement ; } catch ( Throwable ex ) { log . warn ( "Exception occurred while shutting down HSQLDB :" + StringUtils . stringifyException ( ex ) ) ; } }
public void test() { if ( validationTimeout < SECONDS . toMillis ( 1 ) ) { logger . warn ( "{} - A validationTimeout of less than 1 second cannot be honored on drivers without setNetworkTimeout() support." , poolName ) ; } else-if ( validationTimeout % SECONDS . toMillis ( 1 ) != 0 ) { logger . warn ( "{} - A validationTimeout with fractional second granularity cannot be honored on drivers without setNetworkTimeout() support." , poolName ) ; } }
public void test() { if ( validationTimeout < SECONDS . toMillis ( 1 ) ) { logger . warn ( "{} - A validationTimeout of less than 1 second cannot be honored on drivers without setNetworkTimeout() support." , poolName ) ; } else-if ( validationTimeout % SECONDS . toMillis ( 1 ) != 0 ) { logger . warn ( "{} - A validationTimeout with fractional second granularity cannot be honored on drivers without setNetworkTimeout() support." , poolName ) ; } }
private boolean isHostValid ( ) { logger . debug ( "Bridge: Checking for valid Zoneminder host: {}" , host ) ; VersionDTO version = getVersion ( ) ; code_block = IfStatement ; return false ; }
public void test() { try { LOG . warn ( "Failure passing provided arguments (" + getIllegalArgumentsErrorMessage ( constructor , argValues ) + "; " + e + "); attempting to reconstitute" ) ; argValues = ( Object [ ] ) updateFromNewClassLoader ( argValues ) ; return constructor . newInstance ( argValues ) ; } catch ( Throwable e2 ) { LOG . warn ( "Reconstitution attempt failed (will rethrow original excaption): " + e2 , e2 ) ; throw e ; } }
public void test() { try { in . close ( ) ; } catch ( IOException e ) { log . error ( "caught exception closing InputStream: " + e ) ; } }
public void test() { if ( partial ) { logger . error ( "partial reversed weight order not implemented" ) ; } }
public Model getConciseBoundedDescription ( LiteralLabel literal , CBDStructureTree structureTree ) throws Exception { logger . trace ( "Computing CBD for {} ..." , literal ) ; long start = System . currentTimeMillis ( ) ; String query = generateQuery ( literal , structureTree ) ; System . out . println ( query ) ; code_block = IfStatement ; code_block = TryStatement ;  }
public void test() { try ( QueryExecution qe = qef . createQueryExecution ( query ) ) { Model model = qe . execConstruct ( ) ; long end = System . currentTimeMillis ( ) ; logger . trace ( "Got {} triples in {} ms." , model . size ( ) , ( end - start ) ) ; return model ; } catch ( Exception e ) { logger . error ( "CBD retrieval failed when using query\n{}" , query ) ; throw new Exception ( "CBD retrieval failed when using query\n" + query , e ) ; } }
public void test() { try ( QueryExecution qe = qef . createQueryExecution ( query ) ) { Model model = qe . execConstruct ( ) ; long end = System . currentTimeMillis ( ) ; logger . trace ( "Got {} triples in {} ms." , model . size ( ) , ( end - start ) ) ; return model ; } catch ( Exception e ) { logger . error ( "CBD retrieval failed when using query\n{}" , query ) ; throw new Exception ( "CBD retrieval failed when using query\n" + query , e ) ; } }
public void test() { if ( session != null ) { X509Certificate cert = ( X509Certificate ) session . getPeerCertificates ( ) [ 0 ] ; Principal principal = cert . getSubjectDN ( ) ; log . info ( "Client Cert SubjectDN: {}" , principal . getName ( ) ) ; exchange . getOut ( ) . setBody ( "When You Go Home, Tell Them Of Us And Say, For Your Tomorrow, We Gave Our Today." ) ; } else { exchange . getOut ( ) . setBody ( "Cannot start conversion without SSLSession" ) ; } }
public List findByExample ( StgMbZeiteinheiten instance ) { log . debug ( "finding StgMbZeiteinheiten instance by example" ) ; code_block = TryStatement ;  }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.StgMbZeiteinheiten" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.StgMbZeiteinheiten" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
public void test() { if ( session . getUserProperties ( ) . containsKey ( EndpointConfigurator . LOCALE_KEY ) ) { LoginedUser loginuser = ( LoginedUser ) session . getUserProperties ( ) . get ( EndpointConfigurator . LOGIN_USER_KEY ) ; LOG . debug ( "websocket open: " + session . getId ( ) + " : " + loginuser . getUserId ( ) ) ; } }
public void test() { if ( ! sessionObserver . isOpen ( ) ) { LOG . debug ( "websocket is allready closed: " + session . getId ( ) ) ; notify . deleteObserver ( map . get ( session . getId ( ) ) ) ; map . remove ( session . getId ( ) ) ; } }
public StgMbZeiteinheitenTxt findById ( sernet . gs . reveng . StgMbZeiteinheitenTxtId id ) { log . debug ( "getting StgMbZeiteinheitenTxt instance with id: " + id ) ; code_block = TryStatement ;  }
public void test() { if ( instance == null ) { log . debug ( "get successful, no instance found" ) ; } else { log . debug ( "get successful, instance found" ) ; } }
public void test() { if ( instance == null ) { log . debug ( "get successful, no instance found" ) ; } else { log . debug ( "get successful, instance found" ) ; } }
public void test() { try { StgMbZeiteinheitenTxt instance = ( StgMbZeiteinheitenTxt ) sessionFactory . getCurrentSession ( ) . get ( "sernet.gs.reveng.StgMbZeiteinheitenTxt" , id ) ; code_block = IfStatement ; return instance ; } catch ( RuntimeException re ) { log . error ( "get failed" , re ) ; throw re ; } }
public void test() { try { config = inherit ( buildConfig ( buildBaseConfig ( BASE_ROUTING ) , file ) ) ; register ( vertx , env , config ) ; router = createRouter ( env ) ; addRoutes ( router , config . getRoutes ( ) , env ) ; code_block = IfStatement ; startHttpServer ( ) ; dynamicRoute ( ) ; logger . info ( String . format ( "success starting routing verticle %d at %s" , id , deploymentID ( ) ) ) ; } catch ( Exception e ) { logger . error ( String . format ( "failed starting routing verticle %d at %s" , id , deploymentID ( ) ) , e ) ; throw e ; } }
public void test() { try { config = inherit ( buildConfig ( buildBaseConfig ( BASE_ROUTING ) , file ) ) ; register ( vertx , env , config ) ; router = createRouter ( env ) ; addRoutes ( router , config . getRoutes ( ) , env ) ; code_block = IfStatement ; startHttpServer ( ) ; dynamicRoute ( ) ; logger . info ( String . format ( "success starting routing verticle %d at %s" , id , deploymentID ( ) ) ) ; } catch ( Exception e ) { logger . error ( String . format ( "failed starting routing verticle %d at %s" , id , deploymentID ( ) ) , e ) ; throw e ; } }
public void test() { if ( topicSubs == null ) { logger . warn ( "Received a Consume op for topic: {} that the server does not own." , topic . toStringUtf8 ( ) ) ; cb . operationFinished ( ctx , null ) ; return ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Only advanced consume pointer in memory, will persist later, topic: " + topic . toStringUtf8 ( ) + " subscriberId: " + subscriberId . toStringUtf8 ( ) + " persistentState: " + SubscriptionStateUtils . toString ( subState . getSubscriptionState ( ) ) + " in-memory consume-id: " + MessageIdUtils . msgIdToReadableString ( subState . getLastConsumeSeqId ( ) ) ) ; } }
public void test() { if ( ( entry == null ) || entry . isEmpty ( ) ) { LOGGER . warn ( "Cannot index null enum, skipping entry" ) ; return new InsertionIds ( ) ; } }
public void test() { if ( user != null ) { logger . info ( "Cannot add contact with said '" + said + "' [tenant=" + tenant . getId ( ) + "]:" + " there's already another User with id " + user . getId ( ) + " whose accountUri is " + user . getAccountUri ( ) ) ; } else { user = entityFactory . buildUser ( ) ; user . setUsername ( said ) ; user . setPassword ( null ) ; user . setEnabled ( false ) ; user . setRole ( Role . GUEST ) ; user . setAccountUri ( "urn:uuid:" + UUID . randomUUID ( ) ) ; user . setTenant ( tenant ) ; user . merge ( ) ; user . flush ( ) ; logger . debug ( "Created new user [id=" + user . getId ( ) + ", username=" + user . getUsername ( ) + ", role=" + user . getRole ( ) + "]" ) ; } }
public void test() { try { ApplicationContext context = ApplicationContextProvider . getApplicationContext ( ) ; code_block = IfStatement ; Collection < ServiceOps > serviceOpsList = context . getBeansOfType ( ServiceOps . class ) . values ( ) ; code_block = IfStatement ; ServiceOps serviceOps = serviceOpsList . iterator ( ) . next ( ) ; return Optional . of ( serviceOps . get ( NetworkService . Type . CORE ) ) ; } catch ( KeymasterException e ) { LOG . trace ( e . getMessage ( ) ) ; } }
public void test() { if ( registration . getTimeoutAt ( ) . isBefore ( now ) ) { logger . debug ( "Removing {} as its timed out (timeout of {} was set till {} and now is {})" , registration , registration . getTimeout ( ) , registration . getTimeoutAt ( ) , now ) ; registration . getOnTimeoutConsumer ( ) . accept ( new TimeoutException ( ) ) ; iter . remove ( ) ; continue ; } }
public void test() { if ( predicate . test ( instance ) == false ) { logger . trace ( "Registration {} with predicate {} does not match object {} (currently wrapped to {})" , registration , predicate , t . getClass ( ) . getSimpleName ( ) , instance . getClass ( ) . getSimpleName ( ) ) ; continue registrations ; } }
public void test() { if ( ! hospitalIdsInAddressList . contains ( CPCT ) ) { allCorrect = false ; LOGGER . warn ( "CPCT hospital ID is not present in hospital address list: '{}'" , CPCT ) ; } }
public void test() { if ( ! hospitalIdsInAddressList . contains ( DRUP ) ) { allCorrect = false ; LOGGER . warn ( "DRUP hospital ID is not present in hospital address list: '{}'" , DRUP ) ; } }
public void test() { if ( ! hospitalIdsInAddressList . contains ( COREDB ) ) { allCorrect = false ; LOGGER . warn ( "COREDB hospital ID is not present in hospital address list: '{}'" , COREDB ) ; } }
public void test() { if ( ! hospitalIdsInAddressList . contains ( sampleMapping ) ) { allCorrect = false ; LOGGER . warn ( "Sample mapping hospital ID is not present in hospital address list: '{}'" , sampleMapping ) ; } }
public void test() { try { if ( event . getAccessPolicy ( ) != null ) activeAcl = AccessControlParser . parseAcl ( event . getAccessPolicy ( ) ) ; } catch ( Exception e ) { logger . error ( "Unable to parse access policy" , e ) ; } }
public void test() { if ( this . isDebug ( ) ) { log . info ( "Element [" + this + "] set next write value to [" + valueOpt . orElse ( null ) + "]." ) ; } }
public void test() { try { TypedQuery < IServiceProperties > query = entityManager . createNamedQuery ( ServiceProperties . QUERY_FIND_BY_NAME , IServiceProperties . class ) ; query . setParameter ( "name" , name ) ; IServiceProperties serviceProperties = new ServiceProperties ( ) ; serviceProperties = query . getSingleResult ( ) ; return serviceProperties ; } catch ( NoResultException e ) { logger . debug ( "No Result found: " + e ) ; return null ; } }
public void test() { try ( ByteArrayInputStream stringStream = new ByteArrayInputStream ( content . getBytes ( ) ) ) { logger . trace ( "Uploading string to file [" + filename + "], data_connection_mode [" + client . getDataConnectionMode ( ) + "]" ) ; client . storeFile ( filename , stringStream ) ; logger . trace ( "Finished uploading string to file [" + filename + "]" + ", response [" + client . getReplyString ( ) + "]" ) ; done = true ; } catch ( Exception e ) { String reply = client . getReplyString ( ) ; code_block = IfStatement ; logger . error ( "Error uploading file: " + reply , e ) ; code_block = TryStatement ;  } }
public void onNewConnection ( ChannelHandlerContext ctx ) { logger . debug ( "onNewConnection" ) ; }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { logger . warn ( "Failed to insert address result in the DB " + e . getMessage ( ) ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( UserNotFoundException e ) { Log . error ( "Unable to send presence information of user '{}' to unblocked entity '{}' as local user is not found." , user . getUsername ( ) , recipient ) ; } }
public void test() { if ( e instanceof org . apache . airavata . model . error . InvalidRequestException ) { result . ire = ( org . apache . airavata . model . error . InvalidRequestException ) e ; result . setIreIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AiravataClientException ) { result . ace = ( org . apache . airavata . model . error . AiravataClientException ) e ; result . setAceIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AiravataSystemException ) { result . ase = ( org . apache . airavata . model . error . AiravataSystemException ) e ; result . setAseIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AuthorizationException ) { result . ae = ( org . apache . airavata . model . error . AuthorizationException ) e ; result . setAeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof org . apache . airavata . model . error . InvalidRequestException ) { result . ire = ( org . apache . airavata . model . error . InvalidRequestException ) e ; result . setIreIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AiravataClientException ) { result . ace = ( org . apache . airavata . model . error . AiravataClientException ) e ; result . setAceIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AiravataSystemException ) { result . ase = ( org . apache . airavata . model . error . AiravataSystemException ) e ; result . setAseIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AuthorizationException ) { result . ae = ( org . apache . airavata . model . error . AuthorizationException ) e ; result . setAeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof org . apache . airavata . model . error . InvalidRequestException ) { result . ire = ( org . apache . airavata . model . error . InvalidRequestException ) e ; result . setIreIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AiravataClientException ) { result . ace = ( org . apache . airavata . model . error . AiravataClientException ) e ; result . setAceIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AiravataSystemException ) { result . ase = ( org . apache . airavata . model . error . AiravataSystemException ) e ; result . setAseIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AuthorizationException ) { result . ae = ( org . apache . airavata . model . error . AuthorizationException ) e ; result . setAeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { try { fcall . sendResponse ( fb , msg , msgType , seqid ) ; } catch ( java . lang . Exception ex ) { _LOGGER . error ( "Exception writing to internal frame buffer" , ex ) ; fb . close ( ) ; } }
public void test() { try { oldIds = ( Set < Integer > ) userPrefsObj ; } catch ( final ClassCastException ex ) { log . error ( "Oups, Set of task id's is not of type Set<Integer>, can't migrate this list." ) ; } }
public void test() { if ( bitMode > 0 ) { logger . debug ( "-Dio.netty.bitMode: {} (os.arch: {})" , bitMode , arch ) ; } }
@ Test public void operationsWithDifferentStaticInputTypes ( ) throws Exception { removeTypeResolversInformationModelPropertyfromMock ( operation ) ; MetadataCacheId keyParts = getIdForComponentInputMetadata ( getBaseApp ( ) , OPERATION_LOCATION , LIST_NAME ) ; LOGGER . debug ( keyParts . toString ( ) ) ; removeTypeResolversInformationModelPropertyfromMock ( anotherOperation ) ; MetadataCacheId otherKeyParts = getIdForComponentInputMetadata ( getBaseApp ( ) , ANOTHER_OPERATION_LOCATION , LIST_NAME ) ; LOGGER . debug ( otherKeyParts . toString ( ) ) ; assertThat ( keyParts , not ( otherKeyParts ) ) ; }
@ Test public void operationsWithDifferentStaticInputTypes ( ) throws Exception { removeTypeResolversInformationModelPropertyfromMock ( operation ) ; MetadataCacheId keyParts = getIdForComponentInputMetadata ( getBaseApp ( ) , OPERATION_LOCATION , LIST_NAME ) ; LOGGER . debug ( keyParts . toString ( ) ) ; removeTypeResolversInformationModelPropertyfromMock ( anotherOperation ) ; MetadataCacheId otherKeyParts = getIdForComponentInputMetadata ( getBaseApp ( ) , ANOTHER_OPERATION_LOCATION , LIST_NAME ) ; LOGGER . debug ( otherKeyParts . toString ( ) ) ; assertThat ( keyParts , not ( otherKeyParts ) ) ; }
public void test() { if ( sequencingObjectJoin != null ) { return samplePermission . isAllowed ( authentication , sequencingObjectJoin . getSubject ( ) ) ; } else { logger . trace ( "Permission denied for reading sequencing object id=" + sf . getId ( ) + " by user=" + authentication . getName ( ) + ", no joined sample found." ) ; return false ; } }
public void test() { try { log . debug ( "FAILED connect to peer: {} attempt '{}' , DELAY to '{}' ms before next attempt..." , p . getAnnouncedAddress ( ) , i , PeersService . connectTimeout ) ; Thread . sleep ( PeersService . connectTimeout + 20 ) ; } catch ( InterruptedException ex ) { Thread . currentThread ( ) . interrupt ( ) ; } }
@ Override public void doWith ( Method m ) throws IllegalArgumentException , IllegalAccessException { log . info ( "The method [ {} ] of bean [ {} ] is candidate to be overridden by plugin child contexts" , m . toString ( ) , id ) ; registerProxyCandidate ( bean , id ) ; }
public void test() { try { AtlasVertex vertex = AtlasGraphUtilsV2 . findByUniqueAttributes ( this . graph , entityType , attrValues ) ; code_block = IfStatement ; return entityRetriever . toAtlasEntityHeaderWithClassifications ( vertex ) ; } catch ( AtlasBaseException e ) { LOG . warn ( "{}:{} could not be processed!" , entityType , qualifiedName ) ; return null ; } catch ( Exception ex ) { LOG . error ( "{}:{} could not be processed!" , entityType , qualifiedName , ex ) ; return null ; } }
public void test() { try { AtlasVertex vertex = AtlasGraphUtilsV2 . findByUniqueAttributes ( this . graph , entityType , attrValues ) ; code_block = IfStatement ; return entityRetriever . toAtlasEntityHeaderWithClassifications ( vertex ) ; } catch ( AtlasBaseException e ) { LOG . warn ( "{}:{} could not be processed!" , entityType , qualifiedName ) ; return null ; } catch ( Exception ex ) { LOG . error ( "{}:{} could not be processed!" , entityType , qualifiedName , ex ) ; return null ; } }
public void test() { try { Pair < Class < ? extends WebPage > , PageParameters > selfRegInfo = getSelfRegInfo ( newUser ) ; throw new RestartResponseException ( selfRegInfo . getLeft ( ) , selfRegInfo . getRight ( ) ) ; } catch ( JsonProcessingException e ) { LOG . error ( "Could not serialize new user {}" , newUser , e ) ; throw new WicketRuntimeException ( e ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( workflowException , workflowException ) ; } }
public void test() { try { int deletedCount = discoveryEntryStore . removeStale ( clusterControllerId , maxLastSeenDateMs ) ; logger . info ( "RemoveStale(ccId={}, maxLastSeenDateMs={}) deleted {} stale entries." , clusterControllerId , maxLastSeenDateMs , deletedCount ) ; deferred . resolve ( ) ; } catch ( Exception e ) { logger . error ( "RemoveStale(ccId={}, maxLastSeenDateMs={}) failed." , clusterControllerId , maxLastSeenDateMs , e ) ; deferred . reject ( new ProviderRuntimeException ( "RemoveStale failed: " + e . toString ( ) ) ) ; } }
public void test() { try { int deletedCount = discoveryEntryStore . removeStale ( clusterControllerId , maxLastSeenDateMs ) ; logger . info ( "RemoveStale(ccId={}, maxLastSeenDateMs={}) deleted {} stale entries." , clusterControllerId , maxLastSeenDateMs , deletedCount ) ; deferred . resolve ( ) ; } catch ( Exception e ) { logger . error ( "RemoveStale(ccId={}, maxLastSeenDateMs={}) failed." , clusterControllerId , maxLastSeenDateMs , e ) ; deferred . reject ( new ProviderRuntimeException ( "RemoveStale failed: " + e . toString ( ) ) ) ; } }
void removePendingCompactionInstant ( HoodieTimeline timeline , HoodieInstant instant ) throws IOException { LOG . info ( "Removing completed compaction instant (" + instant + ")" ) ; HoodieCompactionPlan plan = CompactionUtils . getCompactionPlan ( metaClient , instant . getTimestamp ( ) ) ; removePendingCompactionOperations ( CompactionUtils . getPendingCompactionOperations ( instant , plan ) . map ( instantPair -> Pair . of ( instantPair . getValue ( ) . getKey ( ) , CompactionOperation . convertFromAvroRecordInstance ( instantPair . getValue ( ) . getValue ( ) ) ) ) ) ; }
public void test() { try { ServerBootstrap b = new ServerBootstrap ( ) ; b . group ( bossGroup , workerGroup ) . channel ( NioServerSocketChannel . class ) . childHandler ( clientInitializer ) ; ChannelFuture f = b . bind ( port ) . sync ( ) ; this . port = ( ( InetSocketAddress ) f . channel ( ) . localAddress ( ) ) . getPort ( ) ; isOnlineFuture . set ( true ) ; synchronized ( scenarioHandler ) code_block = "" ; } catch ( Exception ex ) { LOG . error ( ex . getMessage ( ) , ex ) ; } finally { LOG . debug ( "listening client shutting down" ) ; code_block = TryStatement ;  } }
public void test() { try { workerGroup . shutdownGracefully ( ) . get ( ) ; bossGroup . shutdownGracefully ( ) . get ( ) ; LOG . debug ( "listening client shutdown succesful" ) ; } catch ( InterruptedException | ExecutionException e ) { LOG . error ( e . getMessage ( ) , e ) ; } }
private List < String > getOutputLines ( JobSnapshot snapshot ) throws IOException { String dataGuid = snapshot . getGeneratedDataGuids ( ) . stream ( ) . collect ( SingleCollector . single ( ) ) ; JobDataWithByteSource jobSource = jobService . tryObtainData ( dataGuid ) . get ( ) ; LOGGER . info ( "actual output;\n{}" , jobSource . getByteSource ( ) . asCharSource ( Charsets . UTF_8 ) . read ( ) ) ; return jobSource . getByteSource ( ) . asCharSource ( Charsets . UTF_8 ) . readLines ( ) ; }
public void test() { if ( LOG . isInfoEnabled ( ) ) { LOG . info ( "JPSONIC_HOME directory will be {}" , jpsonicHomeDirForTest . getAbsolutePath ( ) ) ; } }
public void test() { if ( usbSerialDiscovery != null ) { usbSerialDiscovery . doSingleScan ( ) ; } else { logger . info ( "Could not scan, as there is no USB-Serial discovery service configured." ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "cuboid-{} saved dimension:{}, took: {}ms" , cuboidId , dimension . getName ( ) , stopwatch . elapsed ( MILLISECONDS ) ) ; } }
public synchronized void reloadPackage ( String packageName ) throws RuleBaseException { long start = System . currentTimeMillis ( ) ; reloadDeclarations ( ) ; packageStrings . clear ( ) ; StringBuffer packageString = initNewPackageString ( packageName ) ; code_block = ForStatement ; code_block = ForStatement ; Collection < String > flows = queryFlows ( packageName ) ; Collection < KnowledgePackage > compiledPackage = compileDrlString ( packageString . toString ( ) , flows ) ; lockRuleBase ( ) ; code_block = IfStatement ; base . addKnowledgePackages ( compiledPackage ) ; unlockRuleBase ( ) ; LOGGER . info ( "Reloading only package {} took {}ms" , packageName , System . currentTimeMillis ( ) - start ) ; }
public void test() { if ( structure instanceof DestinationInfo ) { DestinationInfo destinationInfo = ( DestinationInfo ) structure ; LOG . info ( "Received: {}" , destinationInfo ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Unable to createCq. Error: {}" , cqe . getMessage ( ) , cqe ) ; } }
public void test() { try { Files . createDirectories ( destPath . getParent ( ) ) ; } catch ( final IOException e ) { LOG . warn ( "Exception hit while trying to recreate directory: " + destPath . getParent ( ) . toString ( ) ) ; } }
public void test() { try ( InputStream inputStream = part . getInputStream ( ) ) { ContentDisposition contentDisposition = new ContentDisposition ( part . getHeader ( HEADER_CONTENT_DISPOSITION ) ) ; return new ImmutablePair < > ( attachmentParser . generateAttachmentInfo ( inputStream , part . getContentType ( ) , contentDisposition . getParameter ( FILENAME_CONTENT_DISPOSITION_PARAMETER_NAME ) ) , null ) ; } catch ( IOException e ) { LOGGER . debug ( "IOException reading stream from file attachment in multipart body." , e ) ; } }
public void test() { try ( InputStream inputStream = part . getInputStream ( ) ) { ContentDisposition contentDisposition = new ContentDisposition ( part . getHeader ( HEADER_CONTENT_DISPOSITION ) ) ; code_block = SwitchStatement ; } catch ( IOException e ) { LOGGER . debug ( "Unable to get input stream for mime attachment. Ignoring override attribute: {}" , name , e ) ; } }
public void test() { if ( ( null != object ) && logger . isDebugEnabled ( ) ) { logger . debug ( "{}: Peeked {}->{}" , this , currentKey , object ) ; } }
public void test() { if ( ( null != object ) && logger . isDebugEnabled ( ) ) { logger . debug ( "{}: Peeked {}->{}" , this , currentKey , object ) ; } }
public void test() { if ( ex == null ) { ex = e ; } else { logger . fatal ( "Unknown error closing streamer: {}" , e . getMessage ( ) , e ) ; } }
public void test() { if ( existing != null && ! existing . equals ( content ) ) { log . warn ( "Multiple versions of the same content received during refresh; " + "discarding previous: {} => {}, {}" , content . getId ( ) , existing , content ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Error sending CQ request to peers. {}" , ex . getLocalizedMessage ( ) , ex ) ; } }
public void test() { try { final CompiledScript cScript = manager . getScript ( filename ) ; final Bindings bindings = cScript . getEngine ( ) . createBindings ( ) ; bindings . put ( "input" , source ) ; result = String . valueOf ( cScript . eval ( bindings ) ) ; return result ; } catch ( ScriptException e ) { throw new TransformationException ( "An error occurred while executing script. " + e . getMessage ( ) , e ) ; } finally { logger . trace ( "JavaScript execution elapsed {} ms. Result: {}" , System . currentTimeMillis ( ) - startTime , result ) ; } }
public void test() { try { HttpResourceStream stream = new HttpResourceStream ( new ResponseHolder ( ClientBuilder . newClient ( ) . target ( RequestCycle . get ( ) . getUrlRenderer ( ) . renderFullUrl ( Url . parse ( UrlUtils . rewriteToContextRelative ( SAML2SP4UIConstants . URL_CONTEXT + "/metadata" , RequestCycle . get ( ) ) ) ) ) . request ( ) . get ( ) ) ) ; ResourceStreamRequestHandler rsrh = new ResourceStreamRequestHandler ( stream ) ; rsrh . setFileName ( stream . getFilename ( ) == null ? SyncopeConsoleSession . get ( ) . getDomain ( ) + "-SAML-SP-Metadata.xml" : stream . getFilename ( ) ) ; rsrh . setContentDisposition ( ContentDisposition . ATTACHMENT ) ; getRequestCycle ( ) . scheduleRequestHandlerAfterCurrent ( rsrh ) ; } catch ( Exception e ) { LOG . error ( "While exporting SAML 2.0 SP metadata" , e ) ; SyncopeConsoleSession . get ( ) . onException ( e ) ; } }
public void test() { try { String incomingTaskMessage = new String ( body , StandardCharsets . UTF_8 ) ; JsonObject incomingTaskMessageJson = new JsonParser ( ) . parse ( incomingTaskMessage ) . getAsJsonObject ( ) ; String htmlDataComponentResponseBuffer = componentHTML ( healthUtil . getDataComponentStatus ( ) , "Data Component" ) ; String htmlServiceResponseBuffer = componentHTML ( healthUtil . getServiceStatus ( ) , "Services" ) ; String htmlAgentResponseBuffer = getAgentHTML ( ) ; Map < String , String > idDataMap = new LinkedHashMap < > ( ) ; idDataMap . put ( "table_agent" , htmlAgentResponseBuffer ) ; idDataMap . put ( "table_services" , htmlServiceResponseBuffer ) ; idDataMap . put ( "table_data_components" , htmlDataComponentResponseBuffer ) ; String mailHTML = createEmailHTML ( idDataMap ) ; Map < String , String > valuesMap = new HashMap < > ( ) ; valuesMap . put ( "date" , InsightsUtils . specficTimeFormat ( incomingTaskMessageJson . get ( "executionId" ) . getAsLong ( ) , "yyyy-MM-dd" ) ) ; StringSubstitutor sub = new StringSubstitutor ( valuesMap , "{" , "}" ) ; mailHTML = sub . replace ( mailHTML ) ; setDetailsInEmailHistory ( incomingTaskMessageJson , mailHTML ) ; InsightsStatusProvider . getInstance ( ) . createInsightStatusNode ( "SystemNotificationDetailSubscriber completed" , PlatformServiceConstants . SUCCESS ) ; } catch ( InsightsJobFailedException ijfe ) { log . error ( "Worlflow Detail ==== SystemNotificationDetail Subscriber Completed with error " , ijfe ) ; InsightsStatusProvider . getInstance ( ) . createInsightStatusNode ( "SystemNotificationDetail Completed with error " + ijfe . getMessage ( ) , PlatformServiceConstants . FAILURE ) ; statusLog = ijfe . getMessage ( ) ; throw ijfe ; } catch ( Exception e ) { log . error ( "Worlflow Detail ==== SystemNotificationDetail Subscriber Completed with error " , e ) ; InsightsStatusProvider . getInstance ( ) . createInsightStatusNode ( "SystemNotificationDetail Completed with error " + e . getMessage ( ) , PlatformServiceConstants . FAILURE ) ; throw new InsightsJobFailedException ( e . getMessage ( ) ) ; } }
public void test() { try { String incomingTaskMessage = new String ( body , StandardCharsets . UTF_8 ) ; JsonObject incomingTaskMessageJson = new JsonParser ( ) . parse ( incomingTaskMessage ) . getAsJsonObject ( ) ; String htmlDataComponentResponseBuffer = componentHTML ( healthUtil . getDataComponentStatus ( ) , "Data Component" ) ; String htmlServiceResponseBuffer = componentHTML ( healthUtil . getServiceStatus ( ) , "Services" ) ; String htmlAgentResponseBuffer = getAgentHTML ( ) ; Map < String , String > idDataMap = new LinkedHashMap < > ( ) ; idDataMap . put ( "table_agent" , htmlAgentResponseBuffer ) ; idDataMap . put ( "table_services" , htmlServiceResponseBuffer ) ; idDataMap . put ( "table_data_components" , htmlDataComponentResponseBuffer ) ; String mailHTML = createEmailHTML ( idDataMap ) ; Map < String , String > valuesMap = new HashMap < > ( ) ; valuesMap . put ( "date" , InsightsUtils . specficTimeFormat ( incomingTaskMessageJson . get ( "executionId" ) . getAsLong ( ) , "yyyy-MM-dd" ) ) ; StringSubstitutor sub = new StringSubstitutor ( valuesMap , "{" , "}" ) ; mailHTML = sub . replace ( mailHTML ) ; setDetailsInEmailHistory ( incomingTaskMessageJson , mailHTML ) ; InsightsStatusProvider . getInstance ( ) . createInsightStatusNode ( "SystemNotificationDetailSubscriber completed" , PlatformServiceConstants . SUCCESS ) ; } catch ( InsightsJobFailedException ijfe ) { log . error ( "Worlflow Detail ==== SystemNotificationDetail Subscriber Completed with error " , ijfe ) ; InsightsStatusProvider . getInstance ( ) . createInsightStatusNode ( "SystemNotificationDetail Completed with error " + ijfe . getMessage ( ) , PlatformServiceConstants . FAILURE ) ; statusLog = ijfe . getMessage ( ) ; throw ijfe ; } catch ( Exception e ) { log . error ( "Worlflow Detail ==== SystemNotificationDetail Subscriber Completed with error " , e ) ; InsightsStatusProvider . getInstance ( ) . createInsightStatusNode ( "SystemNotificationDetail Completed with error " + e . getMessage ( ) , PlatformServiceConstants . FAILURE ) ; throw new InsightsJobFailedException ( e . getMessage ( ) ) ; } }
public void test() { try { List < Authorization > auths = this . getAuthorizationManager ( ) . getUserAuthorizations ( username ) ; code_block = IfStatement ; } catch ( ApsSystemException e ) { logger . error ( "Error extracting auths for user {}" , username , e ) ; throw new RestServerError ( "Error extracting auths for user " + username , e ) ; } }
public void test() { if ( logger . isTraceEnabled ( LogMarker . SERIALIZER_VERBOSE ) ) { logger . trace ( LogMarker . SERIALIZER_VERBOSE , "Read Boolean {}" , value ) ; } }
@ PayloadRoot ( localPart = "SetScheduleAsyncRequest" , namespace = NAMESPACE ) @ ResponsePayload public SetScheduleResponse getSetScheduleResponse ( @ OrganisationIdentification final String organisationIdentification , @ RequestPayload final SetScheduleAsyncRequest request ) throws OsgpException { LOGGER . info ( "Get Tariff Schedule Response Request received from organisation: {} for correlationUID: {}." , organisationIdentification , request . getAsyncRequest ( ) . getCorrelationUid ( ) ) ; final SetScheduleResponse response = new SetScheduleResponse ( ) ; code_block = TryStatement ;  return response ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "printenv({}) {}={}" , getServerChannelSession ( ) , varName , varValue ) ; } }
public void test() { try { return getLastRevision ( ) ; } catch ( Exception e ) { log . warn ( e . getMessage ( ) , e ) ; return null ; } }
public void test() { if ( entry . getUpdated ( ) != null && entry . getUpdated ( ) . equals ( previousEntryUpdate ) ) { logger . warn ( "Skipping entry with id " + entry . getId ( ) + " since it has the same date as our last feed update." ) ; } else { code_block = IfStatement ; code_block = IfStatement ; geom = entry . getWhere ( ) ; matrix . setMasksForGeometry ( geom ) ; } }
public void test() { try { vertexGuid = AtlasGraphUtilsV2 . getGuidByUniqueAttributes ( atlasGraph , entityType , objectId . getUniqueAttributes ( ) ) ; } catch ( AtlasBaseException e ) { LOG . warn ( "Entity: {}: Does not exist!" , objectId ) ; return ; } }
public void test() { switch ( e . errorCode ) { case ErrorCodes . NO_SUCH_RESOURCE : case ErrorCodes . NO_SUCH_COLLECTION : case ErrorCodes . INVALID_COLLECTION : case ErrorCodes . INVALID_RESOURCE : LOG . debug ( e . getMessage ( ) ) ; return null ; default : LOG . error ( e . getMessage ( ) , e ) ; throw e ; } }
public void test() { switch ( e . errorCode ) { case ErrorCodes . NO_SUCH_RESOURCE : case ErrorCodes . NO_SUCH_COLLECTION : case ErrorCodes . INVALID_COLLECTION : case ErrorCodes . INVALID_RESOURCE : LOG . debug ( e . getMessage ( ) ) ; return null ; default : LOG . error ( e . getMessage ( ) , e ) ; throw e ; } }
public void test() { if ( LOGGER . isTraceEnabled ( ) ) { LOGGER . trace ( "Purge Occured: " + cmd ) ; } }
public void initiateChannelBufferManager ( final String channelRegEx ) { Configurator configurator = AnalyticsConfigurator . getInstance ( ) ; redisLoadShedder = new ConcurrentHashMap < String , LoadShedder > ( ) ; redisHost = configurator . getProperty ( AnalyticsConfigurationProperty . REDIS_HOST ) ; redisPort = Integer . parseInt ( configurator . getProperty ( AnalyticsConfigurationProperty . REDIS_PORT ) ) ; CHANNEL_PREFIX_STRING = configurator . getProperty ( AnalyticsConfigurationProperty . TAGGER_CHANNEL_BASENAME ) + "." ; PERSISTER_LOAD_CHECK_INTERVAL_MINUTES = Integer . parseInt ( configurator . getProperty ( AnalyticsConfigurationProperty . PERSISTER_LOAD_CHECK_INTERVAL_MINUTES ) ) ; PERSISTER_LOAD_LIMIT = Integer . parseInt ( configurator . getProperty ( AnalyticsConfigurationProperty . PERSISTER_LOAD_LIMIT ) ) ; AnalyticsConfigurator analyticsConfigurator = AnalyticsConfigurator . getInstance ( ) ; granularityList = analyticsConfigurator . getGranularities ( ) ; tagDataMap = new ConcurrentHashMap < CounterKey , Object > ( ) ; confDataMap = new ConcurrentHashMap < CounterKey , Object > ( ) ; channelMap = new ConcurrentHashMap < String , Long > ( ) ; logger . info ( "Initializing channel buffer manager." ) ; bufferSize = - 1 ; executorServicePool = Executors . newCachedThreadPool ( ) ; logger . info ( "Created thread pool: " + executorServicePool ) ; jedisConn = new JedisConnectionObject ( redisHost , redisPort ) ; code_block = TryStatement ;  code_block = IfStatement ; }
public void test() { try { subscriberJedis = jedisConn . getJedisResource ( ) ; if ( subscriberJedis != null ) isConnected = true ; } catch ( JedisConnectionException e ) { subscriberJedis = null ; isConnected = false ; logger . error ( "Fatal error! Couldn't establish connection to REDIS!" , e ) ; AnalyticsErrorLog . sendErrorMail ( "Redis" , e . getMessage ( ) ) ; } }
public void test() { try { subscribeToChannel ( channelRegEx ) ; isSubscribed = true ; aidrSubscriber . setChannelName ( channelRegEx ) ; logger . info ( "Created pattern subscription for pattern: " + channelRegEx ) ; } catch ( Exception e ) { isSubscribed = false ; logger . error ( "Fatal exception occurred attempting subscription: " + e . toString ( ) , e ) ; AnalyticsErrorLog . sendErrorMail ( "Redis" , e . getMessage ( ) ) ; } }
public void test() { try { subscribeToChannel ( channelRegEx ) ; isSubscribed = true ; aidrSubscriber . setChannelName ( channelRegEx ) ; logger . info ( "Created pattern subscription for pattern: " + channelRegEx ) ; } catch ( Exception e ) { isSubscribed = false ; logger . error ( "Fatal exception occurred attempting subscription: " + e . toString ( ) , e ) ; AnalyticsErrorLog . sendErrorMail ( "Redis" , e . getMessage ( ) ) ; } }
public void test() { if ( newPollRate > 0 && pollRate != newPollRate ) { log . info ( "Poll rate {} changed to {} ms" , pollRate , newPollRate ) ; pollRate = newPollRate ; } }
public void test() { try { PackResponse packResponse = exchangeClient . exchange ( ) ; long newPollRate = packResponse != null ? packResponse . getNewPollRate ( ) : 0 ; code_block = IfStatement ; } catch ( IOException e ) { log . error ( "Error in exchange client" , e ) ; throw new TechnicalException ( "IOException" , e ) ; } catch ( NodeNotFound e ) { log . warn ( "Agent {} didn't registered on current coordinator! Reset agent registration." , nodeContext . getId ( ) ) ; exchangeClient . getPackExchanger ( ) . clean ( ) ; AgentStarter . resetAgent ( Agent . this ) ; } catch ( Throwable e ) { alive = false ; log . error ( "Agent " + nodeContext . getId ( ) + " got an exception from coordinator" , e ) ; } }
public void test() { try { PackResponse packResponse = exchangeClient . exchange ( ) ; long newPollRate = packResponse != null ? packResponse . getNewPollRate ( ) : 0 ; code_block = IfStatement ; } catch ( IOException e ) { log . error ( "Error in exchange client" , e ) ; throw new TechnicalException ( "IOException" , e ) ; } catch ( NodeNotFound e ) { log . warn ( "Agent {} didn't registered on current coordinator! Reset agent registration." , nodeContext . getId ( ) ) ; exchangeClient . getPackExchanger ( ) . clean ( ) ; AgentStarter . resetAgent ( Agent . this ) ; } catch ( Throwable e ) { alive = false ; log . error ( "Agent " + nodeContext . getId ( ) + " got an exception from coordinator" , e ) ; } }
public void test() { try { String userName = AlluxioFuseUtils . getGroupName ( uid ) ; return userName . isEmpty ( ) ? DEFAULT_USER_NAME : userName ; } catch ( IOException e ) { LOG . error ( "Failed to get user name from uid {}, fallback to {}" , uid , DEFAULT_USER_NAME ) ; return DEFAULT_USER_NAME ; } }
@ Override public void persistInterface ( final MetaDataRegisterDTO metadata ) { String rpcType = metadata . getRpcType ( ) ; String contextPath = metadata . getContextPath ( ) . substring ( 1 ) ; registerMetadata ( rpcType , contextPath , metadata ) ; code_block = IfStatement ; log . info ( "{} zookeeper client register success: {}" , rpcType , metadata . toString ( ) ) ; }
public void test() { try { flush ( ) ; code_block = ForStatement ; logid2FileChannel . clear ( ) ; entryLogManager . close ( ) ; synchronized ( compactionLogLock ) code_block = "" ; } catch ( IOException ie ) { LOG . error ( "Error flush entry log during shutting down, which may cause entry log corrupted." , ie ) ; } finally { code_block = ForStatement ; entryLogManager . forceClose ( ) ; synchronized ( compactionLogLock ) code_block = "" ; } }
@ Override public void handle ( PacketClientCacheStatus packet , long currentTimeMillis , PlayerConnection connection ) { LOGGER . debug ( "Setting client caching status to {}" , packet . isEnabled ( ) ) ; connection . cachingSupported ( packet . isEnabled ( ) && connection . server ( ) . serverConfig ( ) . enableClientCache ( ) ) ; }
public void test() { try { doPortalInit ( ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new IllegalStateException ( "Unable to initialize portal" , exception ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( sb . toString ( ) ) ; } }
public void test() { try { uri = connection . getHeatEndpoint ( ) + "/stacks/" + URLEncoder . encode ( stackName , "UTF-8" ) + "/resources" ; RESTResponse response = connection . processRequest ( uri , "GET" ) ; String body = response . getResponseBody ( ) ; JSONObject responseJson = new JSONObject ( body ) ; JSONArray resources = responseJson . getJSONArray ( "resources" ) ; code_block = ForStatement ; } catch ( UnsupportedEncodingException e ) { throw new RuntimeException ( e ) ; } catch ( OpenStackConnectionException ex ) { throw new HeatException ( "Failed to connect to Heat: " + ex . getMessage ( ) , ex . getResponseCode ( ) ) ; } catch ( JSONException e ) { logger . error ( "HeatClient.getStackDetails()" , e ) ; throw new HeatException ( e . getMessage ( ) ) ; } }
@ NotNull public List < T > read ( @ NotNull String dir ) throws IOException { List < T > entries = Lists . newArrayList ( ) ; File [ ] files = new File ( dir ) . listFiles ( ) ; LOGGER . debug ( " {} files found in directory {}" , files . length , dir ) ; int currentFileIndex = 0 ; code_block = WhileStatement ; LOGGER . debug ( "  Done reading {} files " , currentFileIndex ) ; return entries ; }
@ NotNull public List < T > read ( @ NotNull String dir ) throws IOException { List < T > entries = Lists . newArrayList ( ) ; File [ ] files = new File ( dir ) . listFiles ( ) ; LOGGER . debug ( " {} files found in directory {}" , files . length , dir ) ; int currentFileIndex = 0 ; code_block = WhileStatement ; LOGGER . debug ( "  Done reading {} files " , currentFileIndex ) ; return entries ; }
public void test() { try { MethodKey methodKey = new MethodKey ( DLAppServiceUtil . class , "getFileEntry" , _getFileEntryParameterTypes39 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , folderId , title ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . portal . kernel . repository . model . FileEntry ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try ( ObjectOutputStream oos = new ObjectOutputStream ( new BufferedOutputStream ( new FileOutputStream ( new File ( sessionsDir , deploymentName ) ) ) ) ) { oos . writeObject ( map ) ; } catch ( Exception e ) { LOG . info ( "Error persisting sessions for deployment " + deploymentName , e ) ; } }
public void test() { if ( sessionData . size ( ) > 0 ) { code_block = TryStatement ;  } else { LOG . debug ( "No sessions to persist for deployment " + deploymentName ) ; } }
public void test() { try { psIdToAttemptIndexMap = appStateStorage . loadPSMeta ( ) ; } catch ( Exception e ) { LOG . error ( "load task meta from file failed." , e ) ; } }
public void test() { try { event . state . addException ( e ) ; event . state . setError ( true ) ; event . state . eventScheduler . destroyPlanWithError ( ) ; } catch ( RemoteException ex ) { log . error ( "Cannot destroy plan" , ex ) ; } }
public void test() { for ( SettingsProblem problem : result . getProblems ( ) ) { logger . debug ( problem . getMessage ( ) , problem . getException ( ) ) ; } }
public void test() { if ( this . workerState . changeStateInitializing ( ) ) { logger . info ( "start() started." ) ; worker . start ( ) ; workerState . changeStateStarted ( ) ; logger . info ( "start() completed." ) ; break ; } }
public void test() { switch ( this . workerState . getCurrentState ( ) ) { case NEW : code_block = IfStatement ; case INITIALIZING : logger . info ( "start() failed. caused:already initializing." ) ; break ; case STARTED : logger . info ( "start() failed. caused:already started." ) ; break ; case DESTROYING : throw new IllegalStateException ( "Already destroying." ) ; case STOPPED : throw new IllegalStateException ( "Already stopped." ) ; case ILLEGAL_STATE : throw new IllegalStateException ( "Invalid State." ) ; } }
public void test() { switch ( this . workerState . getCurrentState ( ) ) { case NEW : code_block = IfStatement ; case INITIALIZING : logger . info ( "start() failed. caused:already initializing." ) ; break ; case STARTED : logger . info ( "start() failed. caused:already started." ) ; break ; case DESTROYING : throw new IllegalStateException ( "Already destroying." ) ; case STOPPED : throw new IllegalStateException ( "Already stopped." ) ; case ILLEGAL_STATE : throw new IllegalStateException ( "Invalid State." ) ; } }
public void blockAllInbound ( ) { inboundSettings . clear ( ) ; setDefaultInboundSettings ( false ) ; LOGGER . debug ( "[{}] Blocked inbound from all destinations" , address ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Loading font " + ttf ) ; } }
public void test() { try { Version . parseVersion ( value ) ; } catch ( RuntimeException e ) { LOGGER . error ( "An exception occurred during validation of field - {}, objectName - {}, value - {}" , field , objectName , value ) ; errors . add ( new TaskError ( VERSION , field , objectName ) ) ; } }
public void test() { switch ( notificationLevel ) { case DEBUG : logger . debug ( message ) ; break ; case INFO : logger . info ( message ) ; break ; case WARN : logger . warn ( message ) ; break ; case ERROR : logger . error ( message ) ; break ; case OFF : break ; } }
public void test() { switch ( notificationLevel ) { case DEBUG : logger . debug ( message ) ; break ; case INFO : logger . info ( message ) ; break ; case WARN : logger . warn ( message ) ; break ; case ERROR : logger . error ( message ) ; break ; case OFF : break ; } }
public void test() { switch ( notificationLevel ) { case DEBUG : logger . debug ( message ) ; break ; case INFO : logger . info ( message ) ; break ; case WARN : logger . warn ( message ) ; break ; case ERROR : logger . error ( message ) ; break ; case OFF : break ; } }
public void test() { switch ( notificationLevel ) { case DEBUG : logger . debug ( message ) ; break ; case INFO : logger . info ( message ) ; break ; case WARN : logger . warn ( message ) ; break ; case ERROR : logger . error ( message ) ; break ; case OFF : break ; } }
@ BeforeClass public void createObjects ( ) { log = LoggerFactory . getLogger ( OptimizerTOSCADecember2015MultipleInputPointsTest . class ) ; log . info ( "Starting TEST optimizer for the TOSCA syntax of September 2015 Of an " + TEST_CHARACTERISTIC ) ; openInputFiles ( TestConstants . APP_MODEL_FILENAME_MULTIPLE_INPUT_POINT , TestConstants . CLOUD_OFFER_FILENAME_IN_JSON_ATOS_7_MODULES ) ; }
public void test() { if ( ! id . equals ( bootstrapId ) ) { logger . warn ( "Instance ID has changed from " + bootstrapId + " to " + id ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . warn ( "" , e ) ; } }
public void test() { if ( isPenalized ( peerStatus ) ) { logger . debug ( "{} {} is penalized; will not communicate with this peer" , this , peerStatus ) ; } else { return peerStatus ; } }
public void test() { try { field . setAccessible ( true ) ; logger . info ( "Accessing SystemProperty field {}{}" , className , field . getName ( ) ) ; field . get ( null ) ; } catch ( final Throwable t ) { logger . warn ( "Unable to access field {}{}" , className , field . getName ( ) , t ) ; } }
public void test() { try { final Class < ? > clazz = classInfo . load ( ) ; final Field [ ] fields = clazz . getDeclaredFields ( ) ; code_block = ForStatement ; } catch ( final Throwable t ) { logger . warn ( "Unable to load class {}" , className , t ) ; } }
public void test() { try { final Set < ClassPath . ClassInfo > classesInPackage = ClassPath . from ( getClass ( ) . getClassLoader ( ) ) . getTopLevelClassesRecursive ( "org.jivesoftware.openfire" ) ; code_block = ForStatement ; } catch ( final Throwable t ) { logger . warn ( "Unable to scan classpath for SystemProperty classes" , t ) ; } }
@ Override public void onPaymentError ( final PaymentErrorInternalEvent event ) { log . info ( "Got event PaymentError token='{}'" , event . getUserToken ( ) ) ; notifyForCompletion ( ) ; }
public void test() { try { com . liferay . commerce . inventory . model . CommerceInventoryWarehouseItem returnValue = CommerceInventoryWarehouseItemServiceUtil . addCommerceInventoryWarehouseItem ( externalReferenceCode , userId , commerceInventoryWarehouseId , sku , quantity ) ; return com . liferay . commerce . inventory . model . CommerceInventoryWarehouseItemSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try { com . hazelcast . core . Hazelcast . shutdownAll ( ) ; } catch ( Exception ex ) { logger . error ( "error shutting down GTAS Home Controller." , ex ) ; } }
public void test() { if ( LOG . isErrorEnabled ( ) ) { LOG . error ( "===============================================" ) ; LOG . error ( "An exception occurred during database migration" ) ; LOG . error ( "A rollback file has been generated at " + rollbackFile ) ; LOG . error ( "Execute it within your database to rollback any changes" ) ; LOG . error ( "The exception is as follows\n" , e ) ; LOG . error ( "===============================================" ) ; } }
public void test() { if ( LOG . isErrorEnabled ( ) ) { LOG . error ( "===============================================" ) ; LOG . error ( "An exception occurred during database migration" ) ; LOG . error ( "A rollback file has been generated at " + rollbackFile ) ; LOG . error ( "Execute it within your database to rollback any changes" ) ; LOG . error ( "The exception is as follows\n" , e ) ; LOG . error ( "===============================================" ) ; } }
public void test() { if ( LOG . isErrorEnabled ( ) ) { LOG . error ( "===============================================" ) ; LOG . error ( "An exception occurred during database migration" ) ; LOG . error ( "A rollback file has been generated at " + rollbackFile ) ; LOG . error ( "Execute it within your database to rollback any changes" ) ; LOG . error ( "The exception is as follows\n" , e ) ; LOG . error ( "===============================================" ) ; } }
public void test() { if ( LOG . isErrorEnabled ( ) ) { LOG . error ( "===============================================" ) ; LOG . error ( "An exception occurred during database migration" ) ; LOG . error ( "A rollback file has been generated at " + rollbackFile ) ; LOG . error ( "Execute it within your database to rollback any changes" ) ; LOG . error ( "The exception is as follows\n" , e ) ; LOG . error ( "===============================================" ) ; } }
public void simplePipeline ( String newBranch ) { logger . info ( "Creating and editing simple pipeline" ) ; wait . click ( By . id ( "pipeline-node-hittarget-2-add" ) ) ; wait . sendKeys ( By . cssSelector ( "input.stage-name-edit" ) , "simplePipeline creating Test stage" ) ; wait . click ( By . cssSelector ( "button.btn-primary.add" ) ) ; logger . info ( "Adding an echo step" ) ; wait . click ( By . cssSelector ( ".editor-step-selector div[data-functionName=\"echo\"]" ) ) ; wait . sendKeys ( By . cssSelector ( "input.TextInput-control" ) , "simplePipeline creating echo message" ) ; wait . click ( By . cssSelector ( "div.sheet.active a.back-from-sheet" ) ) ; logger . info ( "Pipeline created and ready to be saved" ) ; }
public void simplePipeline ( String newBranch ) { logger . info ( "Creating and editing simple pipeline" ) ; wait . click ( By . id ( "pipeline-node-hittarget-2-add" ) ) ; wait . sendKeys ( By . cssSelector ( "input.stage-name-edit" ) , "simplePipeline creating Test stage" ) ; wait . click ( By . cssSelector ( "button.btn-primary.add" ) ) ; logger . info ( "Adding an echo step" ) ; wait . click ( By . cssSelector ( ".editor-step-selector div[data-functionName=\"echo\"]" ) ) ; wait . sendKeys ( By . cssSelector ( "input.TextInput-control" ) , "simplePipeline creating echo message" ) ; wait . click ( By . cssSelector ( "div.sheet.active a.back-from-sheet" ) ) ; logger . info ( "Pipeline created and ready to be saved" ) ; }
public void simplePipeline ( String newBranch ) { logger . info ( "Creating and editing simple pipeline" ) ; wait . click ( By . id ( "pipeline-node-hittarget-2-add" ) ) ; wait . sendKeys ( By . cssSelector ( "input.stage-name-edit" ) , "simplePipeline creating Test stage" ) ; wait . click ( By . cssSelector ( "button.btn-primary.add" ) ) ; logger . info ( "Adding an echo step" ) ; wait . click ( By . cssSelector ( ".editor-step-selector div[data-functionName=\"echo\"]" ) ) ; wait . sendKeys ( By . cssSelector ( "input.TextInput-control" ) , "simplePipeline creating echo message" ) ; wait . click ( By . cssSelector ( "div.sheet.active a.back-from-sheet" ) ) ; logger . info ( "Pipeline created and ready to be saved" ) ; }
public void channelActive ( final ChannelHandlerContext ctx ) throws Exception { LOGGER . info ( "Channel active: {}" , ctx . channel ( ) ) ; }
public void test() { try { s3 . setBucketVersioningConfiguration ( new SetBucketVersioningConfigurationRequest ( bucketName , new BucketVersioningConfiguration ( BucketVersioningConfiguration . ENABLED ) ) ) ; } catch ( SdkClientException e ) { String message = "Bucket versioning status: " + status + ". Cannot enable versioning. Error message: " + e . getMessage ( ) ; log . warn ( message ) ; } }
public void test() { try { String status = s3 . getBucketVersioningConfiguration ( bucketName ) . getStatus ( ) ; code_block = IfStatement ; } catch ( SdkClientException e ) { log . warn ( "Cannot detect bucket versioning configuration." ) ; } }
public void test() { try { MetricRegistry registry = Metrics . getInstance ( ) . getRegistry ( ) ; HoodieGauge guage = ( HoodieGauge ) registry . gauge ( metricName , ( ) -> new HoodieGauge < > ( value ) ) ; guage . setValue ( value ) ; } catch ( Exception e ) { LOG . error ( "Failed to send metrics: " , e ) ; } }
void addHandler ( String key , EventHandlerMethod handler ) { Preconditions . checkArgument ( ! this . handlers . containsKey ( key ) , "EventHandler can't be registered twice. Other instance: " + this . handlers . get ( key ) ) ; this . handlers . put ( key , handler ) ; LOGGER . debug ( "Registering handler {} -> {}" , key , handler ) ; EventHandlerMethod [ ] newArray = new EventHandlerMethod [ this . insertIndex + 1 ] ; code_block = IfStatement ; newArray [ this . insertIndex ++ ] = handler ; this . sortedHandlerList = newArray ; this . dirty = true ; }
public void test() { try { task . run ( ) ; } catch ( Throwable t ) { _logger . warn ( String . format ( "Unhandled exception happened when running %s" , task . getClass ( ) . getName ( ) ) , t ) ; } }
public void test() { try { final Reader reader = source . getReader ( ) ; code_block = IfStatement ; return "" ; } catch ( final IOException e ) { LOG . error ( e . getMessage ( ) , e ) ; return "" ; } }
public void test() { try { code_block = IfStatement ; } catch ( ArrayIndexOutOfBoundsException E ) { LOGGER . warn ( "Something going wrong here..." ) ; } }
private void createScope ( ) { Scope scope = new Scope ( ) ; scope . setName ( SCOPE_NAME ) ; scope . setType ( Scope . TYPE ) ; scope . setDescription ( "Sample scope description." ) ; Scope createdScope = this . client . create ( scope ) ; LOGGER . info ( "Created scope object returned to client : " + createdScope . toJsonString ( ) ) ; }
@ Activate protected void activate ( Config config ) throws RepositoryException { List < SyncHandler > newSyncSpecs = new LinkedList < SyncHandler > ( ) ; code_block = ForStatement ; syncHandlers = newSyncSpecs . toArray ( new SyncHandler [ newSyncSpecs . size ( ) ] ) ; enabled = config . vault_sync_enabled ( ) ; checkDelay = config . vault_sync_fscheckinterval ( ) * 1000 ; log . info ( "Vault Sync service is {}" , enabled ? "enabled" : "disabled" ) ; code_block = IfStatement ; }
public void test() { try { Path createdPath = Files . createDirectories ( parentPath . resolve ( folderName ) ) ; BasicFileAttributes attrs = Files . readAttributes ( createdPath , BasicFileAttributes . class ) ; TransferredResource resource = createTransferredResource ( createdPath , attrs , 0L , basePath , new Date ( ) ) ; index . create ( TransferredResource . class , resource ) ; return resource ; } catch ( IOException e ) { LOGGER . error ( "Cannot create folder" , e ) ; throw new GenericException ( "Cannot create folder" , e ) ; } }
public void test() { try { registerAndInitializeHandler ( child , getThingHandlerFactory ( child ) ) ; } catch ( Exception ex ) { logger . error ( "Registration resp. initialization of child '{}' of bridge '{}' has been failed: {}" , child . getUID ( ) , bridge . getUID ( ) , ex . getMessage ( ) , ex ) ; } }
public void test() { if ( logger . isInfoEnabled ( ) ) { this . logger . info ( "Rx :  DialogRelease=" + evt ) ; } }
@ Test ( timeout = 60000 ) public void testDisableCompaction ( ) throws Exception { LedgerHandle [ ] lhs = prepareData ( 3 , false ) ; baseConf . setMinorCompactionThreshold ( 0.0f ) ; baseConf . setMajorCompactionThreshold ( 0.0f ) ; restartBookies ( baseConf ) ; bkc . deleteLedger ( lhs [ 1 ] . getId ( ) ) ; bkc . deleteLedger ( lhs [ 2 ] . getId ( ) ) ; LOG . info ( "Finished deleting the ledgers contains most entries." ) ; Thread . sleep ( baseConf . getMajorCompactionInterval ( ) * 1000 + baseConf . getGcWaitTime ( ) ) ; code_block = ForStatement ; }
public String enqueueUpdateDeviceSslCertificationRequest ( final String organisationIdentification , final String deviceIdentification , final Certification certification , final int messagePriority ) throws FunctionalException { final Organisation organisation = this . domainHelperService . findOrganisation ( organisationIdentification ) ; final Device device = this . domainHelperService . findActiveDevice ( deviceIdentification ) ; this . domainHelperService . isAllowed ( organisation , device , DeviceFunction . UPDATE_DEVICE_SSL_CERTIFICATION ) ; this . domainHelperService . isInMaintenance ( device ) ; LOGGER . debug ( "enqueueUpdateDeviceSslCertificationRequest called with organisation {} and device {}" , organisationIdentification , deviceIdentification ) ; final String correlationUid = this . correlationIdProviderService . getCorrelationId ( organisationIdentification , deviceIdentification ) ; final DeviceMessageMetadata deviceMessageMetadata = new DeviceMessageMetadata ( deviceIdentification , organisationIdentification , correlationUid , MessageType . UPDATE_DEVICE_SSL_CERTIFICATION . name ( ) , messagePriority ) ; final CommonRequestMessage message = new CommonRequestMessage . Builder ( ) . deviceMessageMetadata ( deviceMessageMetadata ) . request ( certification ) . build ( ) ; this . commonRequestMessageSender . send ( message ) ; return correlationUid ; }
private void waitToExitSafeMode ( JobClient client ) throws IOException { LOG . info ( "Waiting to exit safe mode..." ) ; FileSystem fs = client . getFs ( ) ; DistributedFileSystem dfs = ( DistributedFileSystem ) fs ; boolean inSafeMode = true ; code_block = WhileStatement ; LOG . info ( "Exited safe mode" ) ; }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( MessageFormat . format ( "Truncating Direct I/O resources: {0}:{1} (id={2})" , fullPath , resourcePattern , id ) ) ; } }
public void doCreateConsumer ( RoutingContext routingContext , JsonObject bodyAsJson , Handler < SinkBridgeEndpoint < K , V > > handler ) { this . groupId = routingContext . pathParam ( "groupid" ) ; this . name = bodyAsJson . getString ( "name" , bridgeConfig . getBridgeID ( ) == null ? "kafka-bridge-consumer-" + UUID . randomUUID ( ) : bridgeConfig . getBridgeID ( ) + "-" + UUID . randomUUID ( ) ) ; this . consumerInstanceId = new ConsumerInstanceId ( this . groupId , this . name ) ; code_block = IfStatement ; String requestUri = this . buildRequestUri ( routingContext ) ; code_block = IfStatement ; String consumerBaseUri = requestUri + "instances/" + this . name ; Properties config = new Properties ( ) ; addConfigParameter ( ConsumerConfig . AUTO_OFFSET_RESET_CONFIG , bodyAsJson . getString ( ConsumerConfig . AUTO_OFFSET_RESET_CONFIG , null ) , config ) ; Object enableAutoCommit = bodyAsJson . getValue ( ConsumerConfig . ENABLE_AUTO_COMMIT_CONFIG ) ; addConfigParameter ( ConsumerConfig . ENABLE_AUTO_COMMIT_CONFIG , enableAutoCommit != null ? String . valueOf ( enableAutoCommit ) : null , config ) ; Object fetchMinBytes = bodyAsJson . getValue ( ConsumerConfig . FETCH_MIN_BYTES_CONFIG ) ; addConfigParameter ( ConsumerConfig . FETCH_MIN_BYTES_CONFIG , fetchMinBytes != null ? String . valueOf ( fetchMinBytes ) : null , config ) ; Object requestTimeoutMs = bodyAsJson . getValue ( "consumer." + ConsumerConfig . REQUEST_TIMEOUT_MS_CONFIG ) ; addConfigParameter ( ConsumerConfig . REQUEST_TIMEOUT_MS_CONFIG , requestTimeoutMs != null ? String . valueOf ( requestTimeoutMs ) : null , config ) ; addConfigParameter ( ConsumerConfig . CLIENT_ID_CONFIG , this . name , config ) ; this . initConsumer ( false , config ) ; handler . handle ( this ) ; log . info ( "Created consumer {} in group {}" , this . name , this . groupId ) ; JsonObject body = new JsonObject ( ) . put ( "instance_id" , this . name ) . put ( "base_uri" , consumerBaseUri ) ; HttpUtils . sendResponse ( routingContext , HttpResponseStatus . OK . code ( ) , BridgeContentType . KAFKA_JSON , body . toBuffer ( ) ) ; }
public void test() { if ( ! file . exists ( ) ) { log . info ( "Starting checkout of [" + _path + "]" ) ; file = this . checkoutFile ( _rev_branch , _path ) ; } else { log . info ( "[" + _path + "] already exists, no checkout needed" ) ; } }
public void test() { if ( ! file . exists ( ) ) { log . info ( "Starting checkout of [" + _path + "]" ) ; file = this . checkoutFile ( _rev_branch , _path ) ; } else { log . info ( "[" + _path + "] already exists, no checkout needed" ) ; } }
@ Test public void test ( ) throws Exception { MockEndpoint result = getMockEndpoint ( "mock:result" ) ; result . setExpectedCount ( 1 ) ; final ProducerTemplate producerTemplate = context . createProducerTemplate ( ) ; ClassLoader tccl = Thread . currentThread ( ) . getContextClassLoader ( ) ; InputStream payloadIs = tccl . getResourceAsStream ( "json-source.json" ) ; producerTemplate . sendBody ( "direct:start" , payloadIs ) ; assertMockEndpointsSatisfied ( ) ; final String body = result . getExchanges ( ) . get ( 0 ) . getIn ( ) . getBody ( String . class ) ; assertNotNull ( body ) ; LOG . debug ( ">>>>> {}" , body ) ; InputStream schemaIs = tccl . getResourceAsStream ( "xml-target-schemaset.xml" ) ; AtlasXmlSchemaSetParser schemaParser = new AtlasXmlSchemaSetParser ( tccl ) ; Validator validator = schemaParser . createSchema ( schemaIs ) . newValidator ( ) ; StreamSource source = new StreamSource ( new StringReader ( body ) ) ; validator . validate ( source ) ; }
public void test() { if ( logger . isTraceEnabled ( LogMarker . TOMBSTONE_COUNT_VERBOSE ) ) { logger . trace ( LogMarker . TOMBSTONE_COUNT_VERBOSE , "tombstone for {} was resurrected with v{}; destroyed version was v{}; count is {}; entryMap size is {}" , re . getKey ( ) , re . getVersionStamp ( ) . getEntryVersion ( ) , destroyedVersion , this . _getOwner ( ) . getTombstoneCount ( ) , size ( ) ) ; } }
public void test() { if ( entryVersion == destroyedVersion ) { logger . trace ( LogMarker . TOMBSTONE_COUNT_VERBOSE , "removing tombstone for {} with v{} rv{}; count is {}" , re . getKey ( ) , destroyedVersion , version . getRegionVersion ( ) , ( this . _getOwner ( ) . getTombstoneCount ( ) - 1 ) ) ; } else { logger . trace ( LogMarker . TOMBSTONE_COUNT_VERBOSE , "removing entry (v{}) that is older than an expiring tombstone (v{} rv{}) for {}" , entryVersion , destroyedVersion , version . getRegionVersion ( ) , re . getKey ( ) ) ; } }
public void test() { if ( entryVersion == destroyedVersion ) { logger . trace ( LogMarker . TOMBSTONE_COUNT_VERBOSE , "removing tombstone for {} with v{} rv{}; count is {}" , re . getKey ( ) , destroyedVersion , version . getRegionVersion ( ) , ( this . _getOwner ( ) . getTombstoneCount ( ) - 1 ) ) ; } else { logger . trace ( LogMarker . TOMBSTONE_COUNT_VERBOSE , "removing entry (v{}) that is older than an expiring tombstone (v{} rv{}) for {}" , entryVersion , destroyedVersion , version . getRegionVersion ( ) , re . getKey ( ) ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "" + index + " " + aField . getName ( ) + " " + aField . getType ( ) ) ; } }
@ Override public void serviceInit ( Configuration conf ) throws Exception { AsyncDispatcher dispatcher ; TajoWorkerClientService tajoWorkerClientService ; TajoWorkerManagerService tajoWorkerManagerService ; ShutdownHookManager . get ( ) . addShutdownHook ( new ShutdownHook ( ) , SHUTDOWN_HOOK_PRIORITY ) ; this . systemConf = TUtil . checkTypeAndGet ( conf , TajoConf . class ) ; RackResolver . init ( systemConf ) ; serviceTracker = ServiceTrackerFactory . get ( systemConf ) ; this . workerContext = new TajoWorkerContext ( ) ; this . lDirAllocator = new LocalDirAllocator ( ConfVars . WORKER_TEMPORAL_DIR . varname ) ; dispatcher = new AsyncDispatcher ( ) ; addIfService ( dispatcher ) ; tajoWorkerManagerService = new TajoWorkerManagerService ( workerContext ) ; addIfService ( tajoWorkerManagerService ) ; tajoWorkerClientService = new TajoWorkerClientService ( workerContext ) ; addIfService ( tajoWorkerClientService ) ; queryMasterManagerService = new QueryMasterManagerService ( workerContext ) ; addIfService ( queryMasterManagerService ) ; code_block = IfStatement ; this . taskManager = new TaskManager ( dispatcher , workerContext , pullService ) ; addService ( taskManager ) ; this . taskExecutor = new TaskExecutor ( workerContext ) ; addService ( taskExecutor ) ; AsyncDispatcher rmDispatcher = new AsyncDispatcher ( ) ; addService ( rmDispatcher ) ; this . nodeResourceManager = new NodeResourceManager ( rmDispatcher , workerContext ) ; addService ( nodeResourceManager ) ; addService ( new NodeStatusUpdater ( workerContext ) ) ; int httpPort = 0 ; code_block = IfStatement ; super . serviceInit ( conf ) ; int pullServerPort = systemConf . getIntVar ( ConfVars . PULLSERVER_PORT ) ; code_block = IfStatement ; this . connectionInfo = new WorkerConnectionInfo ( tajoWorkerManagerService . getBindAddr ( ) . getHostName ( ) , tajoWorkerManagerService . getBindAddr ( ) . getPort ( ) , pullServerPort , tajoWorkerClientService . getBindAddr ( ) . getPort ( ) , queryMasterManagerService . getBindAddr ( ) . getPort ( ) , httpPort ) ; LOG . info ( "Tajo Worker is initialized." + " connection :" + connectionInfo . toString ( ) ) ; code_block = TryStatement ;  taskHistoryWriter = new HistoryWriter ( workerContext . getWorkerName ( ) , false ) ; addIfService ( taskHistoryWriter ) ; taskHistoryWriter . init ( conf ) ; historyReader = new HistoryReader ( workerContext . getWorkerName ( ) , this . systemConf ) ; FunctionLoader . loadUserDefinedFunctions ( systemConf ) ; HiveFunctionLoader . loadHiveUDFs ( systemConf ) ; PythonScriptEngine . initPythonScriptEngineFiles ( ) ; diagnoseTajoWorker ( ) ; }
public void test() { try { hashShuffleAppenderManager = new HashShuffleAppenderManager ( systemConf ) ; } catch ( IOException e ) { LOG . fatal ( e . getMessage ( ) , e ) ; System . exit ( - 1 ) ; } }
@ Override public < K , V > BlockDiskCache < K , V > createCache ( final AuxiliaryCacheAttributes iaca , final ICompositeCacheManager cacheMgr , final ICacheEventLogger cacheEventLogger , final IElementSerializer elementSerializer ) { final BlockDiskCacheAttributes idca = ( BlockDiskCacheAttributes ) iaca ; log . debug ( "Creating DiskCache for attributes = {0}" , idca ) ; final BlockDiskCache < K , V > cache = new BlockDiskCache < > ( idca , elementSerializer ) ; cache . setCacheEventLogger ( cacheEventLogger ) ; return cache ; }
protected void loadTempStorage ( String name , Map < String , String > properties ) { requireNonNull ( name , "name is null" ) ; requireNonNull ( properties , "properties is null" ) ; log . info ( "-- Loading temp storage %s --" , name ) ; String tempStorageFactoryName = null ; ImmutableMap . Builder < String , String > tempStorageProperties = ImmutableMap . builder ( ) ; code_block = ForStatement ; checkState ( tempStorageFactoryName != null , "Configuration for tempStorage %s does not contain temp-storage-factory.name" , name ) ; TempStorageFactory factory = tempStorageFactories . get ( tempStorageFactoryName ) ; checkState ( factory != null , "Temp Storage Factory %s is not registered" , tempStorageFactoryName ) ; TempStorage tempStorage = factory . create ( tempStorageProperties . build ( ) , new TempStorageContext ( nodeManager ) ) ; code_block = IfStatement ; log . info ( "-- Loaded temp storage %s --" , name ) ; }
protected void loadTempStorage ( String name , Map < String , String > properties ) { requireNonNull ( name , "name is null" ) ; requireNonNull ( properties , "properties is null" ) ; log . info ( "-- Loading temp storage %s --" , name ) ; String tempStorageFactoryName = null ; ImmutableMap . Builder < String , String > tempStorageProperties = ImmutableMap . builder ( ) ; code_block = ForStatement ; checkState ( tempStorageFactoryName != null , "Configuration for tempStorage %s does not contain temp-storage-factory.name" , name ) ; TempStorageFactory factory = tempStorageFactories . get ( tempStorageFactoryName ) ; checkState ( factory != null , "Temp Storage Factory %s is not registered" , tempStorageFactoryName ) ; TempStorage tempStorage = factory . create ( tempStorageProperties . build ( ) , new TempStorageContext ( nodeManager ) ) ; code_block = IfStatement ; log . info ( "-- Loaded temp storage %s --" , name ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Looking up endpoint for [" + key + "]" ) ; } }
public void test() { if ( key == null ) { return null ; } }
public void test() { try { int actualIndex = message . getIntProperty ( TestAmqpPeer . MESSAGE_NUMBER ) ; LOG . debug ( "Got message {}" , actualIndex ) ; assertEquals ( "Received Message Out Of Order" , expectedIndex , actualIndex ) ; code_block = IfStatement ; } catch ( Throwable t ) { complete = true ; asyncError . set ( t ) ; latch . countDown ( ) ; } }
public void test() { if ( messageSeen < recoverCount ) { LOG . debug ( "Ignoring message " + actualIndex + " and calling recover" ) ; session . recover ( ) ; messageSeen ++ ; } else { messageSeen = 0 ; expectedIndex ++ ; testPeer . expectDisposition ( true , new AcceptedMatcher ( ) , expectedIndex , expectedIndex ) ; LOG . debug ( "Acknowledging message {}" , actualIndex ) ; message . acknowledge ( ) ; code_block = IfStatement ; } }
public void test() { if ( messageSeen < recoverCount ) { LOG . debug ( "Ignoring message " + actualIndex + " and calling recover" ) ; session . recover ( ) ; messageSeen ++ ; } else { messageSeen = 0 ; expectedIndex ++ ; testPeer . expectDisposition ( true , new AcceptedMatcher ( ) , expectedIndex , expectedIndex ) ; LOG . debug ( "Acknowledging message {}" , actualIndex ) ; message . acknowledge ( ) ; code_block = IfStatement ; } }
public void test() { if ( activeCount == 0 ) { LOG . info ( "Number of active threads = " + activeCount + "." ) ; break ; } else { LOG . info ( "Number of active threads = " + activeCount + ". Waiting for all threads to shutdown ..." ) ; code_block = TryStatement ;  } }
public void shutdownServer ( ) { int timeWaitForShutdownInSeconds = EmbeddedServerUtil . getIntConfig ( "service.waitTimeForForceShutdownInSeconds" , 0 ) ; code_block = IfStatement ; LOG . info ( "Shuting down the Server." ) ; System . exit ( 0 ) ; }
private File customAnimation ( final Job job , final URI input , final Map < String , String > metadata ) throws IOException , NotFoundException { logger . debug ( "Start customizing the animation" ) ; File output = new File ( workspace . rootDirectory ( ) , String . format ( "animate/%d/%s.%s" , job . getId ( ) , FilenameUtils . getBaseName ( input . getPath ( ) ) , FilenameUtils . getExtension ( input . getPath ( ) ) ) ) ; FileUtils . forceMkdirParent ( output ) ; String animation ; code_block = TryStatement ;  code_block = ForStatement ; FileUtils . write ( output , animation , "utf-8" ) ; return output ; }
public void test() { try { animation = FileUtils . readFileToString ( new File ( input ) , "UTF-8" ) ; } catch ( IOException e ) { logger . debug ( "Falling back to workspace to read {}" , input ) ; code_block = TryStatement ;  } }
public FilterResZob merge ( FilterResZob detachedInstance ) { log . debug ( "merging FilterResZob instance" ) ; code_block = TryStatement ;  }
public void test() { try { FilterResZob result = ( FilterResZob ) sessionFactory . getCurrentSession ( ) . merge ( detachedInstance ) ; log . debug ( "merge successful" ) ; return result ; } catch ( RuntimeException re ) { log . error ( "merge failed" , re ) ; throw re ; } }
public void test() { try { FilterResZob result = ( FilterResZob ) sessionFactory . getCurrentSession ( ) . merge ( detachedInstance ) ; log . debug ( "merge successful" ) ; return result ; } catch ( RuntimeException re ) { log . error ( "merge failed" , re ) ; throw re ; } }
public void test() { if ( _log . isWarnEnabled ( ) ) { _log . warn ( "Unable to index object entry " + objectEntry . getObjectEntryId ( ) , exception ) ; } }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; } catch ( IllegalAccessException e ) { LOGGER . warn ( UNABLE_TO_ACCESS_FIELD_ON_OBJECT , f . getName ( ) , target ) ; } }
public void test() { try { code_block = WhileStatement ; } catch ( Exception e ) { LOG . error ( "fail to scan peer table in cleanup" , e ) ; } finally { replicatedScanner . close ( ) ; replicatedScanner = null ; } }
public void test() { try { sourceTable . close ( ) ; } catch ( IOException e ) { LOG . error ( "fail to close source table in cleanup" , e ) ; } }
public void test() { try { sourceConnection . close ( ) ; } catch ( Exception e ) { LOG . error ( "fail to close source connection in cleanup" , e ) ; } }
public void test() { try { replicatedTable . close ( ) ; } catch ( Exception e ) { LOG . error ( "fail to close replicated table in cleanup" , e ) ; } }
public void test() { try { replicatedConnection . close ( ) ; } catch ( Exception e ) { LOG . error ( "fail to close replicated connection in cleanup" , e ) ; } }
@ Override public synchronized void shutdown ( ) throws Exception { logger . info ( "bot is shutting down" ) ; this . scheduledExecution . cancel ( true ) ; super . shutdown ( ) ; }
public void test() { try { code_block = IfStatement ; } catch ( StorageException se ) { LOG . error ( "error while retrieving suite from mongo db: '{}', correlationId: '{}', suiteName: '{}'" , dbKey , patternCorrelationId , patternSuiteName , se ) ; } }
public void test() { try { final Object evaluationResult = this . evalWithR ( input ) ; final REXPDouble doubleResult = ( REXPDouble ) evaluationResult ; return doubleResult . asDouble ( ) ; } catch ( final REXPMismatchException exc ) { RBridgeControl . LOGGER . error ( "Error casting value from R: {} Cause: {}" , input , exc ) ; return resultOnFailure ; } catch ( final InvalidREvaluationResultException exc ) { RBridgeControl . LOGGER . error ( exc . getMessage ( ) , exc ) ; return resultOnFailure ; } }
public void test() { try { final Object evaluationResult = this . evalWithR ( input ) ; final REXPDouble doubleResult = ( REXPDouble ) evaluationResult ; return doubleResult . asDouble ( ) ; } catch ( final REXPMismatchException exc ) { RBridgeControl . LOGGER . error ( "Error casting value from R: {} Cause: {}" , input , exc ) ; return resultOnFailure ; } catch ( final InvalidREvaluationResultException exc ) { RBridgeControl . LOGGER . error ( exc . getMessage ( ) , exc ) ; return resultOnFailure ; } }
public void test() { try { return session . getRoomUsers ( roomId ) ; } catch ( MageRemoteException e ) { logger . info ( e ) ; return Collections . emptyList ( ) ; } }
public void test() { if ( dataType != null ) { TimelineRegistry timelineRegistry = myToolbox . getUIRegistry ( ) . getTimelineRegistry ( ) ; timelineRegistry . removeData ( dataType . getOrderKey ( ) , ids ) ; } else { LOGGER . error ( "Failed to remove timeline data - no data type found to " + dataTypeKey ) ; } }
@ BeforeClass public static void startServers ( ) { log . debug ( "Starting KDC..." ) ; testKdc = new TestKDC ( true ) ; testKdc . startDirectoryService ( ) ; testKdc . startKDC ( ) ; serverKeyTab = testKdc . generateKeyTab ( CommunicationSuiteChild . SERVER_KEY_TAB , "sasl/test_server_1@WILDFLY.ORG" , "servicepwd" ) ; log . debug ( "serverKeyTab written to:" + serverKeyTab ) ; serverUnboundKeyTab = testKdc . generateKeyTab ( CommunicationSuiteChild . SERVER_UNBOUND_KEY_TAB , "sasl/test_server_1@WILDFLY.ORG" , "servicepwd" , "*@WILDFLY.ORG" , "dummy" ) ; log . debug ( "serverUnboundKeyTab written to:" + serverUnboundKeyTab ) ; }
@ BeforeClass public static void startServers ( ) { log . debug ( "Starting KDC..." ) ; testKdc = new TestKDC ( true ) ; testKdc . startDirectoryService ( ) ; testKdc . startKDC ( ) ; serverKeyTab = testKdc . generateKeyTab ( CommunicationSuiteChild . SERVER_KEY_TAB , "sasl/test_server_1@WILDFLY.ORG" , "servicepwd" ) ; log . debug ( "serverKeyTab written to:" + serverKeyTab ) ; serverUnboundKeyTab = testKdc . generateKeyTab ( CommunicationSuiteChild . SERVER_UNBOUND_KEY_TAB , "sasl/test_server_1@WILDFLY.ORG" , "servicepwd" , "*@WILDFLY.ORG" , "dummy" ) ; log . debug ( "serverUnboundKeyTab written to:" + serverUnboundKeyTab ) ; }
@ BeforeClass public static void startServers ( ) { log . debug ( "Starting KDC..." ) ; testKdc = new TestKDC ( true ) ; testKdc . startDirectoryService ( ) ; testKdc . startKDC ( ) ; serverKeyTab = testKdc . generateKeyTab ( CommunicationSuiteChild . SERVER_KEY_TAB , "sasl/test_server_1@WILDFLY.ORG" , "servicepwd" ) ; log . debug ( "serverKeyTab written to:" + serverKeyTab ) ; serverUnboundKeyTab = testKdc . generateKeyTab ( CommunicationSuiteChild . SERVER_UNBOUND_KEY_TAB , "sasl/test_server_1@WILDFLY.ORG" , "servicepwd" , "*@WILDFLY.ORG" , "dummy" ) ; log . debug ( "serverUnboundKeyTab written to:" + serverUnboundKeyTab ) ; }
@ Override protected void onPostExecute ( Void result ) { super . onPostExecute ( result ) ; logger . trace ( "onPostExecute()" ) ; activity . showKeyFingerprints ( ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Successfully recorded subscription for topic: " + topic . toStringUtf8 ( ) + " subscriberId: " + subscriberId . toStringUtf8 ( ) + " data: " + SubscriptionStateUtils . toString ( data ) ) ; } }
public void test() { try ( OutputStream outputStream = jobDataWithByteSink . getByteSink ( ) . openBufferedStream ( ) ; OutputStreamWriter outputStreamWriter = new OutputStreamWriter ( outputStream ) ; CSVWriter writer = new CSVWriter ( outputStreamWriter , ',' ) ) { String [ ] headings = new String [ ] code_block = "" ; ; writer . writeNext ( headings ) ; String [ ] cells = new String [ 4 ] ; long startMs = System . currentTimeMillis ( ) ; LOGGER . info ( "will produce spreadsheet spreadsheet report" ) ; long count = pkgService . eachPkg ( context , false , pkg code_block = LoopStatement ; ) ; LOGGER . info ( "did produce spreadsheet report for {} packages in {}ms" , count , System . currentTimeMillis ( ) - startMs ) ; } }
public void test() { try ( OutputStream outputStream = jobDataWithByteSink . getByteSink ( ) . openBufferedStream ( ) ; OutputStreamWriter outputStreamWriter = new OutputStreamWriter ( outputStream ) ; CSVWriter writer = new CSVWriter ( outputStreamWriter , ',' ) ) { String [ ] headings = new String [ ] code_block = "" ; ; writer . writeNext ( headings ) ; String [ ] cells = new String [ 4 ] ; long startMs = System . currentTimeMillis ( ) ; LOGGER . info ( "will produce spreadsheet spreadsheet report" ) ; long count = pkgService . eachPkg ( context , false , pkg code_block = LoopStatement ; ) ; LOGGER . info ( "did produce spreadsheet report for {} packages in {}ms" , count , System . currentTimeMillis ( ) - startMs ) ; } }
@ Override public MCCIIN000002UV01 processPatientDiscoveryAsyncResp ( PRPAIN201306UV02 request , AssertionType assertion , NhinTargetCommunitiesType target ) { LOG . debug ( "Begin EntityPatientDiscoveryDeferredResponseProxyWebServiceUnsecuredImpl.processPatientDiscoveryAsyncResp(...)" ) ; MCCIIN000002UV01 response = new MCCIIN000002UV01 ( ) ; String serviceName = NhincConstants . PATIENT_DISCOVERY_ENTITY_ASYNC_RESP_SERVICE_NAME ; code_block = TryStatement ;  LOG . debug ( "End EntityPatientDiscoveryDeferredResponseProxyWebServiceUnsecuredImpl.processPatientDiscoveryAsyncResp(...)" ) ; return response ; }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "After target system URL look up. URL for service: {} is: {}" , serviceName , url ) ; } }
@ Override public MCCIIN000002UV01 processPatientDiscoveryAsyncResp ( PRPAIN201306UV02 request , AssertionType assertion , NhinTargetCommunitiesType target ) { LOG . debug ( "Begin EntityPatientDiscoveryDeferredResponseProxyWebServiceUnsecuredImpl.processPatientDiscoveryAsyncResp(...)" ) ; MCCIIN000002UV01 response = new MCCIIN000002UV01 ( ) ; String serviceName = NhincConstants . PATIENT_DISCOVERY_ENTITY_ASYNC_RESP_SERVICE_NAME ; code_block = TryStatement ;  LOG . debug ( "End EntityPatientDiscoveryDeferredResponseProxyWebServiceUnsecuredImpl.processPatientDiscoveryAsyncResp(...)" ) ; return response ; }
public void test() { try { res = Long . parseLong ( envVal ) ; } catch ( NumberFormatException ex ) { log . debug ( "Invalid shard ID:{}" , envVal ) ; } }
public void test() { if ( x . length != 1 ) { String msg = "Laguerre(n,m;x) needs one argument" ; logger . warn ( msg ) ; throw new IllegalArgumentException ( msg ) ; } }
@ Override public void setUserQuotaTotal ( final String userName , final long quotaValue ) throws JargonException { log . info ( "setUserQuotaTotal()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "userName:{}" , userName ) ; log . info ( "quotaValue:{}" , quotaValue ) ; GeneralAdminInp adminPI = GeneralAdminInp . instanceForSetUserQuotaTotal ( userName , quotaValue ) ; log . debug ( "executing admin PI" ) ; getIRODSProtocol ( ) . irodsFunction ( adminPI ) ; log . info ( "quota set" ) ; }
@ Override public void setUserQuotaTotal ( final String userName , final long quotaValue ) throws JargonException { log . info ( "setUserQuotaTotal()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "userName:{}" , userName ) ; log . info ( "quotaValue:{}" , quotaValue ) ; GeneralAdminInp adminPI = GeneralAdminInp . instanceForSetUserQuotaTotal ( userName , quotaValue ) ; log . debug ( "executing admin PI" ) ; getIRODSProtocol ( ) . irodsFunction ( adminPI ) ; log . info ( "quota set" ) ; }
@ Override public void setUserQuotaTotal ( final String userName , final long quotaValue ) throws JargonException { log . info ( "setUserQuotaTotal()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "userName:{}" , userName ) ; log . info ( "quotaValue:{}" , quotaValue ) ; GeneralAdminInp adminPI = GeneralAdminInp . instanceForSetUserQuotaTotal ( userName , quotaValue ) ; log . debug ( "executing admin PI" ) ; getIRODSProtocol ( ) . irodsFunction ( adminPI ) ; log . info ( "quota set" ) ; }
@ Override public void setUserQuotaTotal ( final String userName , final long quotaValue ) throws JargonException { log . info ( "setUserQuotaTotal()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "userName:{}" , userName ) ; log . info ( "quotaValue:{}" , quotaValue ) ; GeneralAdminInp adminPI = GeneralAdminInp . instanceForSetUserQuotaTotal ( userName , quotaValue ) ; log . debug ( "executing admin PI" ) ; getIRODSProtocol ( ) . irodsFunction ( adminPI ) ; log . info ( "quota set" ) ; }
@ Override public void setUserQuotaTotal ( final String userName , final long quotaValue ) throws JargonException { log . info ( "setUserQuotaTotal()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "userName:{}" , userName ) ; log . info ( "quotaValue:{}" , quotaValue ) ; GeneralAdminInp adminPI = GeneralAdminInp . instanceForSetUserQuotaTotal ( userName , quotaValue ) ; log . debug ( "executing admin PI" ) ; getIRODSProtocol ( ) . irodsFunction ( adminPI ) ; log . info ( "quota set" ) ; }
public void test() { if ( logger . isTraceEnabled ( LogMarker . DM_VERBOSE ) ) { logger . trace ( LogMarker . DM_VERBOSE , "Shutting down conduit" ) ; } }
public void test() { if ( t != null && t . isAlive ( ) ) { logger . warn ( "Unable to shut down listener within {}ms.  Unable to interrupt socket.accept() due to JDK bug. Giving up." , LISTENER_CLOSE_TIMEOUT ) ; } }
public void test() { try { InternetAddress [ ] originalToInternetAddresses = InternetAddress . parse ( toHeader , false ) ; return MailAddressUtils . from ( originalToInternetAddresses ) ; } catch ( MessagingException ae ) { LOGGER . warn ( "Unable to parse a \"TO\" header address in the original message: {}; ignoring." , toHeader ) ; } }
public void test() { try { String [ ] toHeaders = mail . getMessage ( ) . getHeader ( RFC2822Headers . TO ) ; code_block = IfStatement ; return ImmutableList . of ( ) ; } catch ( MessagingException ae ) { LOGGER . warn ( "Unable to parse the \"TO\" header  in the original message; ignoring." ) ; return ImmutableList . of ( ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( final IOException closeException ) { logger . error ( "Failed to close httprox request: " + error . getMessage ( ) , error ) ; } }
private void bindPrimitive ( final Named key , final long value ) { bindConstant ( ) . annotatedWith ( key ) . to ( value ) ; LOG . trace ( "bound {} to {}" , key . value ( ) , value ) ; }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; } catch ( Exception e ) { Logger . error ( XmlHelper . class . getName ( ) , "Error loading localized file: " + fullPath ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Check command context {}" , commandContext ) ; } }
public void test() { if ( storage == null ) { LOG . error ( "Can't fetch relevant Csar from storage: StorageService not available" ) ; return false ; } }
public void test() { try { JAXBElement < ? > job = ( JAXBElement < ? > ) JAXBContext . newInstance ( org . apache . cxf . outofband . header . ObjectFactory . class ) . createUnmarshaller ( ) . unmarshal ( ( Node ) hdr1 . getObject ( ) ) ; hdrToTest . add ( ( OutofBandHeader ) job . getValue ( ) ) ; } catch ( JAXBException ex ) { LOG . warn ( "JAXB error: {}" , ex . getMessage ( ) , ex ) ; } }
public void test() { try { List items = upload . parseRequest ( request ) ; Iterator it = items . iterator ( ) ; code_block = WhileStatement ; code_block = IfStatement ; return data ; } catch ( FileUploadException e ) { log . error ( e . getMessage ( ) , e ) ; } }
public void raiseNodeReconciliationAlarm ( final Long nodeId ) { String alarmText = getAlarmText ( nodeId , " started reconciliation" ) ; String source = getSourceText ( nodeId ) ; LOG . debug ( "Raising NodeReconciliationOperationOngoing alarm, alarmText {} source {}" , alarmText , source ) ; invokeFMRaiseMethod ( "NodeReconciliationOperationOngoing" , alarmText , source ) ; }
public void test() { try { code_block = IfStatement ; } catch ( IllegalArgumentException e ) { LOGGER . info ( "heartbeat handler is already part of this pipeline" , e ) ; } }
public void test() { try { ctx . getPipeline ( ) . remove ( NettyConstants . HEARTBEAT_HANDLER ) ; } catch ( NoSuchElementException e ) { LOGGER . info ( "Heartbeat handler was concurrently removed" ) ; } }
public void test() { try { final FlowFileRequest request = new FlowFileRequest ( peer , serverProtocol ) ; code_block = IfStatement ; scheduler . registerEvent ( this ) ; ProcessingResult result = null ; code_block = TryStatement ;  result = request . getResponseQueue ( ) . take ( ) ; final Exception problem = result . getProblem ( ) ; code_block = IfStatement ; } catch ( final NotAuthorizedException | BadRequestException | RequestExpiredException e ) { throw e ; } catch ( final ProtocolException e ) { throw new BadRequestException ( e ) ; } catch ( final IOException | FlowFileAccessException e ) { final String REQUEST_TOO_LONG_MSG = "Request input stream longer than" ; code_block = IfStatement ; } catch ( final InterruptedException e ) { logger . error ( "The NiFi DoS filter has interrupted a long running session. If this is undesired, configure a longer web request timeout value in nifi.properties using '{}'" , NiFiProperties . WEB_REQUEST_TIMEOUT ) ; throw new ProcessException ( e ) ; } catch ( final Exception e ) { throw new ProcessException ( e ) ; } }
public void test() { try { final List < FileItem > items = upload . parseRequest ( servletRequest ) ; code_block = ForStatement ; } catch ( final FileUploadException e ) { LOG . error ( e ) ; } }
public void test() { try { String statusResponse = executeGetRequest ( "/bha-api/sip.cgi&action=status" ) ; logger . debug ( "Doorbird returned json response: {}" , statusResponse ) ; sipStatus = new SipStatus ( statusResponse ) ; } catch ( IOException e ) { logger . info ( "Unable to communicate with Doorbird: {}" , e . getMessage ( ) ) ; } catch ( JsonSyntaxException e ) { logger . info ( "Unable to parse Doorbird response: {}" , e . getMessage ( ) ) ; } catch ( DoorbirdUnauthorizedException e ) { logAuthorizationError ( "getSipStatus" ) ; } }
public void test() { try { String statusResponse = executeGetRequest ( "/bha-api/sip.cgi&action=status" ) ; logger . debug ( "Doorbird returned json response: {}" , statusResponse ) ; sipStatus = new SipStatus ( statusResponse ) ; } catch ( IOException e ) { logger . info ( "Unable to communicate with Doorbird: {}" , e . getMessage ( ) ) ; } catch ( JsonSyntaxException e ) { logger . info ( "Unable to parse Doorbird response: {}" , e . getMessage ( ) ) ; } catch ( DoorbirdUnauthorizedException e ) { logAuthorizationError ( "getSipStatus" ) ; } }
public void test() { try { String statusResponse = executeGetRequest ( "/bha-api/sip.cgi&action=status" ) ; logger . debug ( "Doorbird returned json response: {}" , statusResponse ) ; sipStatus = new SipStatus ( statusResponse ) ; } catch ( IOException e ) { logger . info ( "Unable to communicate with Doorbird: {}" , e . getMessage ( ) ) ; } catch ( JsonSyntaxException e ) { logger . info ( "Unable to parse Doorbird response: {}" , e . getMessage ( ) ) ; } catch ( DoorbirdUnauthorizedException e ) { logAuthorizationError ( "getSipStatus" ) ; } }
@ Override public void processMessage ( final ObjectMessage message ) { LOGGER . debug ( "Processing common get configuration message" ) ; MessageMetadata messageMetadata ; String configurationBank ; code_block = TryStatement ;  this . printDomainInfo ( messageMetadata . getMessageType ( ) , messageMetadata . getDomain ( ) , messageMetadata . getDomainVersion ( ) ) ; final SwitchConfigurationBankRequest deviceRequest = new SwitchConfigurationBankRequest ( DeviceRequest . newBuilder ( ) . messageMetaData ( messageMetadata ) , configurationBank ) ; this . deviceService . switchConfiguration ( deviceRequest ) ; }
public void test() { try { messageMetadata = MessageMetadata . fromMessage ( message ) ; configurationBank = ( String ) message . getObject ( ) ; } catch ( final JMSException e ) { LOGGER . error ( "UNRECOVERABLE ERROR, unable to read ObjectMessage instance, giving up." , e ) ; return ; } }
public void test() { try { loadReport ( ) ; } catch ( Exception e ) { LOG . debug ( "The project report cannot be found." , e ) ; } }
public void test() { try { code_block = ForStatement ; } catch ( Exception ex ) { gotFailures . compareAndSet ( false , true ) ; log . error ( "Exception during datastore cutover" , ex ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( noprob ) ; } }
@ Override public void setValue ( TestdataValue value ) { super . setValue ( value ) ; code_block = IfStatement ; latch . countDown ( ) ; long start = System . currentTimeMillis ( ) ; LOGGER . info ( "{}.setValue() started and going to sleep." , TestdataSleepingEntity . class . getSimpleName ( ) ) ; code_block = TryStatement ;  LOGGER . info ( "{}.setValue() interrupted after {}ms." , TestdataSleepingEntity . class . getSimpleName ( ) , System . currentTimeMillis ( ) - start ) ; }
@ Override public void setValue ( TestdataValue value ) { super . setValue ( value ) ; code_block = IfStatement ; latch . countDown ( ) ; long start = System . currentTimeMillis ( ) ; LOGGER . info ( "{}.setValue() started and going to sleep." , TestdataSleepingEntity . class . getSimpleName ( ) ) ; code_block = TryStatement ;  LOGGER . info ( "{}.setValue() interrupted after {}ms." , TestdataSleepingEntity . class . getSimpleName ( ) , System . currentTimeMillis ( ) - start ) ; }
public void test() { try { invoiceApi . recordChargebackReversal ( paymentControlContext . getPaymentId ( ) , paymentControlContext . getAttemptPaymentId ( ) , paymentControlContext . getTransactionExternalKey ( ) , internalContext ) ; } catch ( final InvoiceApiException e ) { log . warn ( "onFailureCall failed for attemptId='{}', transactionType='{}'" , paymentControlContext . getAttemptPaymentId ( ) , transactionType , e ) ; } }
public void test() { try { invoiceApi . recordChargebackReversal ( paymentControlContext . getPaymentId ( ) , paymentControlContext . getAttemptPaymentId ( ) , paymentControlContext . getTransactionExternalKey ( ) , internalContext ) ; } catch ( final InvoiceApiException e ) { log . warn ( "onFailureCall failed for attemptId='{}', transactionType='{}'" , paymentControlContext . getAttemptPaymentId ( ) , transactionType , e ) ; } }
@ Override public void trace ( String format , Object arg1 , Object arg2 ) { traceMessages . add ( new LogMessage ( null , format , null , arg1 , arg2 ) ) ; logger . trace ( format , arg1 , arg2 ) ; }
@ Override public IndexClient getIndexClient ( HetuFileSystemClient fs , HetuMetastore metastore , Path root ) { requireNonNull ( root , "No root path specified" ) ; LOG . debug ( "Creating IndexClient with given filesystem client with root path %s" , root ) ; return new HeuristicIndexClient ( fs , metastore , root ) ; }
public void test() { if ( channel == null ) { logger . warn ( "failed to get channel to " + operand ) ; } }
public void test() { try { projects = taigaClient . getProjects ( ) ; } catch ( Exception e ) { LOG . warn ( "Failed to load chat projects! " + e , e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { LOG . warn ( "Failed to get issue project names: " + e , e ) ; } }
public void test() { try { conn = this . getConnection ( ) ; conn . setAutoCommit ( false ) ; super . executeQueryWithoutResultset ( conn , DELETE_USERMESSAGES_ANSWERS , username ) ; super . executeQueryWithoutResultset ( conn , DELETE_USERMESSAGES_SEARCH_RECORD , username ) ; super . executeQueryWithoutResultset ( conn , DELETE_USERMESSAGES_ROLES , username ) ; super . executeQueryWithoutResultset ( conn , DELETE_USERMESSAGES , username ) ; conn . commit ( ) ; } catch ( Throwable t ) { this . executeRollback ( conn ) ; _logger . error ( "Error removing messages for user {}" , username , t ) ; throw new RuntimeException ( "Error removing messages for user " + username , t ) ; } finally { this . closeConnection ( conn ) ; } }
public void test() { if ( isMergedInto ( advertisedRef . getObjectId ( ) , localRef . getObjectId ( ) , false ) ) { log . warn ( "Advertised commit is already merged into current head in branch '{}'. Current HEAD: {}, advertised ref: {}" , branch , localRef . getObjectId ( ) . name ( ) , advertisedRef . getObjectId ( ) . name ( ) ) ; } else { log . warn ( "Found commits that are not fast forwarded in branch '{}'. Current HEAD: {}, advertised ref: {}" , branch , localRef . getObjectId ( ) . name ( ) , advertisedRef . getObjectId ( ) . name ( ) ) ; checkoutForced ( branch ) ; git . merge ( ) . include ( advertisedRef ) . setFastForward ( MergeCommand . FastForwardMode . FF_ONLY ) . call ( ) ; } }
@ Override public void open ( FunctionContext context ) { LOG . info ( "start open ..." ) ; final ExecutorService threadPool = Executors . newFixedThreadPool ( THREAD_POOL_SIZE , new ExecutorThreadFactory ( "hbase-aysnc-lookup-worker" , Threads . LOGGING_EXCEPTION_HANDLER ) ) ; Configuration config = prepareRuntimeConfiguration ( ) ; CompletableFuture < AsyncConnection > asyncConnectionFuture = ConnectionFactory . createAsyncConnection ( config ) ; code_block = TryStatement ;  this . serde = new HBaseSerde ( hbaseTableSchema , nullStringLiteral ) ; LOG . info ( "end open." ) ; }
@ Override public void open ( FunctionContext context ) { LOG . info ( "start open ..." ) ; final ExecutorService threadPool = Executors . newFixedThreadPool ( THREAD_POOL_SIZE , new ExecutorThreadFactory ( "hbase-aysnc-lookup-worker" , Threads . LOGGING_EXCEPTION_HANDLER ) ) ; Configuration config = prepareRuntimeConfiguration ( ) ; CompletableFuture < AsyncConnection > asyncConnectionFuture = ConnectionFactory . createAsyncConnection ( config ) ; code_block = TryStatement ;  this . serde = new HBaseSerde ( hbaseTableSchema , nullStringLiteral ) ; LOG . info ( "end open." ) ; }
public void test() { if ( excludedIdsCSV != null && ! excludedIdsCSV . trim ( ) . isEmpty ( ) && ! WorkflowRuntimeParameters . UNDEFINED_NONEMPTY_VALUE . equals ( excludedIdsCSV ) ) { log . info ( "got excluded ids: " + excludedIdsCSV ) ; excludedIds = new HashSet < String > ( Arrays . asList ( StringUtils . split ( excludedIdsCSV . trim ( ) , ',' ) ) ) ; } else { log . info ( "got no excluded ids" ) ; } }
public void test() { if ( excludedIdsCSV != null && ! excludedIdsCSV . trim ( ) . isEmpty ( ) && ! WorkflowRuntimeParameters . UNDEFINED_NONEMPTY_VALUE . equals ( excludedIdsCSV ) ) { log . info ( "got excluded ids: " + excludedIdsCSV ) ; excludedIds = new HashSet < String > ( Arrays . asList ( StringUtils . split ( excludedIdsCSV . trim ( ) , ',' ) ) ) ; } else { log . info ( "got no excluded ids" ) ; } }
private synchronized void logTimingInfo ( ) { code_block = IfStatement ; final StringBuilder sb = new StringBuilder ( ) ; sb . append ( String . format ( "For %s %s Timing Info is as follows:\n" , method , uri ) ) ; code_block = ForStatement ; logger . debug ( sb . toString ( ) ) ; }
public void test() { try { verifyLedgerFragment ( r , allFragmentsCb ) ; } catch ( InvalidFragmentException ife ) { LOG . error ( "Invalid fragment found : {}" , r ) ; allFragmentsCb . operationComplete ( BKException . Code . IncorrectParameterException , r ) ; } catch ( BKException e ) { LOG . error ( "BKException when checking fragment : {}" , r , e ) ; } }
public void test() { try { verifyLedgerFragment ( r , allFragmentsCb ) ; } catch ( InvalidFragmentException ife ) { LOG . error ( "Invalid fragment found : {}" , r ) ; allFragmentsCb . operationComplete ( BKException . Code . IncorrectParameterException , r ) ; } catch ( BKException e ) { LOG . error ( "BKException when checking fragment : {}" , r , e ) ; } }
public void test() { try { InetAddress inetAddress = getByName ( host ) ; InetSocketAddress updateAddress = new InetSocketAddress ( inetAddress , port ) ; checkDnsUpdate ( updateAddress ) ; return updateAddress ; } catch ( UnknownHostException e ) { logger . info ( "dns lookup fail. host:{}" , host ) ; return InetSocketAddress . createUnresolved ( host , port ) ; } }
public void test() { try { code_block = ForStatement ; return true ; } catch ( Exception e ) { log . error ( "Error in enable rules" , e ) ; return false ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CountryServiceUtil . class , "fetchCountryByA2" , _fetchCountryByA2ParameterTypes4 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , companyId , a2 ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . portal . kernel . model . Country ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( getLogger ( ) . isDebugEnabled ( ) ) { getLogger ( ) . debug ( "Request in ascii protocol: " + ( new String ( bb . array ( ) ) ) . replace ( "\r\n" , "\\r\\n" ) ) ; } }
public void test() { if ( oakName == null ) { LOG . warn ( "Ignoring invalid property name: {}" , name ) ; } else { oakPropertyNames . add ( oakName ) ; } }
public void test() { if ( oakPath == null ) { LOG . warn ( "Cannot listen for changes on invalid path: {}" , path ) ; return ; } }
public void test() { try { long start = PERF_LOGGER . start ( ) ; NamePathMapper namePathMapper = new NamePathMapperImpl ( new GlobalNameMapper ( RootFactory . createReadOnlyRoot ( root ) ) ) ; Set < String > oakPropertyNames = Sets . newHashSet ( ) ; code_block = ForStatement ; NodeState before = previousRoot ; NodeState after = root ; EventHandler handler = new FilteredHandler ( VISIBLE_FILTER , new NodeEventHandler ( "/" , info , namePathMapper , oakPropertyNames ) ) ; String oakPath = namePathMapper . getOakPath ( path ) ; code_block = IfStatement ; code_block = ForStatement ; EventGenerator generator = new EventGenerator ( before , after , handler ) ; code_block = WhileStatement ; PERF_LOGGER . end ( start , 100 , "Generated events (before: {}, after: {})" , previousRoot , root ) ; } catch ( Exception e ) { LOG . warn ( "Error while dispatching observation events" , e ) ; } }
public void test() { try { final FetchResponse response = builder . build ( fetch , result , mailbox , selected , mailboxSession ) ; responder . respond ( response ) ; } catch ( MessageRangeException e ) { LOGGER . debug ( "Unable to find message with uid {}" , result . getUid ( ) , e ) ; } catch ( MailboxException e ) { LOGGER . error ( "Unable to fetch message with uid {}, so skip it" , result . getUid ( ) , e ) ; } }
public void test() { try { final FetchResponse response = builder . build ( fetch , result , mailbox , selected , mailboxSession ) ; responder . respond ( response ) ; } catch ( MessageRangeException e ) { LOGGER . debug ( "Unable to find message with uid {}" , result . getUid ( ) , e ) ; } catch ( MailboxException e ) { LOGGER . error ( "Unable to fetch message with uid {}, so skip it" , result . getUid ( ) , e ) ; } }
public void test() { switch ( status ) { case Status . STATUS_COMMITTED : log . debug ( "Transaction committed" ) ; this . dispatcher . commit ( ) ; break ; case Status . STATUS_ROLLEDBACK : log . debug ( "Transaction rolled back" ) ; this . dispatcher . rollback ( ) ; break ; default : log . debug ( "Received unexpected transaction completion status: {}" , status ) ; } }
public void test() { switch ( status ) { case Status . STATUS_COMMITTED : log . debug ( "Transaction committed" ) ; this . dispatcher . commit ( ) ; break ; case Status . STATUS_ROLLEDBACK : log . debug ( "Transaction rolled back" ) ; this . dispatcher . rollback ( ) ; break ; default : log . debug ( "Received unexpected transaction completion status: {}" , status ) ; } }
public void test() { switch ( status ) { case Status . STATUS_COMMITTED : log . debug ( "Transaction committed" ) ; this . dispatcher . commit ( ) ; break ; case Status . STATUS_ROLLEDBACK : log . debug ( "Transaction rolled back" ) ; this . dispatcher . rollback ( ) ; break ; default : log . debug ( "Received unexpected transaction completion status: {}" , status ) ; } }
public void test() { try { code_block = SwitchStatement ; } catch ( JobMessageDispatchException e ) { log . error ( "Error occurred while attempting to synchronize database and message bus" , e ) ; } }
public void test() { if ( log . isInfoEnabled ( ) ) { log . info ( formatLogString ( "sending " + WorkerDoneEvent . class . getSimpleName ( ) + " to sync" ) ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Thing \"{}\" changes something in its status (eg: a behavior value)" , this . getPojo ( ) . getName ( ) ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceVirtualOrderItemServiceUtil . class , "getFile" , _getFileParameterTypes0 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , commerceVirtualOrderItemId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( java . io . File ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { com . liferay . portal . kernel . model . Layout returnValue = LayoutServiceUtil . getLayoutByUuidAndGroupId ( uuid , groupId , privateLayout ) ; return com . liferay . portal . kernel . model . LayoutSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( con != null && logger . isDebugEnabled ( ) ) { logger . debug ( "handleNewPendingConnection {} myAddr={} theirAddr={}" , con , getConduit ( ) . getMemberId ( ) , con . getRemoteAddress ( ) ) ; } }
public void test() { try { new DataPurge ( ) . execute ( runtime ) ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { basePath = new Path ( new URI ( strBasePath ) ) ; } catch ( URISyntaxException e ) { log . error ( "Invalid HDFS base path for vectors: " + strBasePath , e ) ; } }
public void test() { try { logger . info ( "executing: ifconfig and looking for {}" , primaryNetworkInterfaceName ) ; List < String > out = Arrays . asList ( runSystemCommand ( "ifconfig" , false , this . executorService ) . split ( "\\r?\\n" ) ) ; ListIterator < String > iterator = out . listIterator ( ) ; String line ; code_block = WhileStatement ; } catch ( IOException e ) { logger . error ( "Failed to get network interfaces" , e ) ; } }
public void test() { try { logger . info ( "connecting to %s:%s ..." , ips . get ( index ) , ports . get ( index ) ) ; URL url = buildUrl ( endpoint , param , ips . get ( index ) , ports . get ( index ) ) ; HttpURLConnection conn = openConnection ( url ) ; conn . setConnectTimeout ( DEFAULT_CONNECTION_TIMEOUT ) ; code_block = IfStatement ; conn . setRequestMethod ( method ) ; conn . setUseCaches ( false ) ; conn . setDoInput ( true ) ; code_block = ForStatement ; } catch ( URISyntaxException | IOException | InterruptedException e ) { logger . error ( e ) ; } }
public void test() { try { code_block = IfStatement ; int code = conn . getResponseCode ( ) ; code_block = IfStatement ; logger . error ( "code: %d, msg: %s" , code , conn . getResponseMessage ( ) ) ; code_block = IfStatement ; break ; } catch ( IOException e ) { } }
public void test() { try { logger . info ( "connecting to %s:%s ..." , ips . get ( index ) , ports . get ( index ) ) ; URL url = buildUrl ( endpoint , param , ips . get ( index ) , ports . get ( index ) ) ; HttpURLConnection conn = openConnection ( url ) ; conn . setConnectTimeout ( DEFAULT_CONNECTION_TIMEOUT ) ; code_block = IfStatement ; conn . setRequestMethod ( method ) ; conn . setUseCaches ( false ) ; conn . setDoInput ( true ) ; code_block = ForStatement ; } catch ( URISyntaxException | IOException | InterruptedException e ) { logger . error ( e ) ; } }
public void test() { try { return binder . getSRARouteTO ( routeDAO . find ( key ) ) ; } catch ( Throwable ignore ) { LOG . debug ( "Unresolved reference" , ignore ) ; throw new UnresolvedReferenceException ( ignore ) ; } }
public void test() { if ( handler != null ) { logger . debug ( "Removing device: {}" , thing . getLabel ( ) ) ; handler . handleRemoval ( ) ; } else { logger . warn ( "Removing device failed (DeviceHandler is null): {}" , thing . getLabel ( ) ) ; } }
public List < String > discoverUebHosts ( String opEnvKey ) throws DME2Exception { String lookupUriFormat = configurationManager . getConfiguration ( ) . getDmeConfiguration ( ) . getLookupUriFormat ( ) ; String environment = configurationManager . getConfiguration ( ) . getDmaapConsumerConfiguration ( ) . getEnvironment ( ) ; String lookupURI = String . format ( lookupUriFormat , opEnvKey , environment ) ; log . debug ( "DME2 GRM URI: {}" , lookupURI ) ; List < String > uebHosts = new LinkedList < > ( ) ; DME2EndpointIterator iterator = epIterCreator . create ( lookupURI ) ; code_block = WhileStatement ; return uebHosts ; }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { log . warn ( "[OmsJarContainer-{}] delete jarFile({}) failed." , containerId , localJarFile . getPath ( ) , e ) ; } }
public void test() { try { processorCache . clear ( ) ; container . close ( ) ; containerClassLoader . close ( ) ; log . info ( "[OmsJarContainer-{}] container destroyed successfully" , containerId ) ; } catch ( Exception e ) { log . error ( "[OmsJarContainer-{}] container destroyed failed" , containerId , e ) ; } }
public void test() { try { processorCache . clear ( ) ; container . close ( ) ; containerClassLoader . close ( ) ; log . info ( "[OmsJarContainer-{}] container destroyed successfully" , containerId ) ; } catch ( Exception e ) { log . error ( "[OmsJarContainer-{}] container destroyed failed" , containerId , e ) ; } }
public void test() { try { return ( bytes == null ) ? null : HFileMeta . deserialize ( bytes ) ; } catch ( Exception internal ) { log . warn ( "Argument 'bytes={}' was not a serialized HFileMeta!" , CommonUtils . hex ( bytes ) ) ; throw new IllegalArgumentException ( internal ) ; } }
public static < T extends Container > void setContainerImplementation ( String containerType , Class < T > containerClass ) { logger . info ( "Using <{}> for container type <{}>" , containerClass . getName ( ) , containerType ) ; containerImplementations . put ( containerType , containerClass ) ; }
protected void handleException ( String message , Exception e ) throws Exception { LOG . error ( message , e ) ; File scrFile = ( ( TakesScreenshot ) driver ) . getScreenshotAs ( OutputType . FILE ) ; LOG . error ( "ScreenShot::\ndata:image/png;base64," + new String ( Base64 . encodeBase64 ( FileUtils . readFileToByteArray ( scrFile ) ) ) ) ; throw e ; }
protected void handleException ( String message , Exception e ) throws Exception { LOG . error ( message , e ) ; File scrFile = ( ( TakesScreenshot ) driver ) . getScreenshotAs ( OutputType . FILE ) ; LOG . error ( "ScreenShot::\ndata:image/png;base64," + new String ( Base64 . encodeBase64 ( FileUtils . readFileToByteArray ( scrFile ) ) ) ) ; throw e ; }
private void runAutoML ( String dataFrameResponse ) throws InsightsCustomException { log . info ( "AutoML Executor === AutoML is running on usecase {} wait for a while to finish" , autoMlConfig . getUseCaseName ( ) ) ; JsonObject responseJson = new JsonParser ( ) . parse ( dataFrameResponse ) . getAsJsonObject ( ) ; JsonArray destinationFrames = responseJson . get ( "destination_frames" ) . getAsJsonArray ( ) ; String trainingFrame = destinationFrames . get ( 0 ) . getAsJsonObject ( ) . get ( "name" ) . getAsString ( ) ; JsonObject mlResponse = trainUtils . runAutoML ( autoMlConfig . getUseCaseName ( ) , trainingFrame , autoMlConfig . getPredictionColumn ( ) , autoMlConfig . getNumOfModels ( ) ) ; int status = mlResponse . get ( "status" ) . getAsInt ( ) ; code_block = IfStatement ; }
public void test() { try { constructor = placementClass . getDeclaredConstructor ( NodeManager . class , ConfigurationSource . class , NetworkTopology . class , boolean . class , SCMContainerPlacementMetrics . class ) ; LOG . info ( "Create container placement policy of type {}" , placementClass . getCanonicalName ( ) ) ; } catch ( NoSuchMethodException e ) { String msg = "Failed to find constructor(NodeManager, Configuration, " + "NetworkTopology, boolean) for class " + placementClass . getCanonicalName ( ) ; LOG . error ( msg ) ; throw new SCMException ( msg , SCMException . ResultCodes . FAILED_TO_INIT_CONTAINER_PLACEMENT_POLICY ) ; } }
public void test() { try { root = engine . complexRootRefinement ( root , algebraic . modul , e ) ; } catch ( InvalidBoundaryException e1 ) { logger . warn ( "new eps not set: " + e ) ; return ; } }
public void test() { try { ( ( AbstractNpmAnalyzer ) a ) . prepareFileTypeAnalyzer ( engine ) ; } catch ( InitializationException ex ) { LOGGER . debug ( "Error initializing the {}" , a . getName ( ) ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { long contentLength = 0 ; code_block = TryStatement ;  LOGGER . debug ( "Content-Length is " + contentLength + " bytes" ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( DDMFormInstanceRecordVersionServiceUtil . class , "getFormInstanceRecordVersions" , _getFormInstanceRecordVersionsParameterTypes4 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , ddmFormInstanceRecordId , start , end , orderByComparator ) ; Object returnObj = null ; code_block = TryStatement ;  return ( java . util . List < com . liferay . dynamic . data . mapping . model . DDMFormInstanceRecordVersion > ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { DLAppHelperLocalServiceUtil . getFileAsStream ( PrincipalThreadLocal . getUserId ( ) , this , true ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; } }
public static void main ( String [ ] args ) { Character warrior = CharacterStepBuilder . newBuilder ( ) . name ( "Amberjill" ) . fighterClass ( "Paladin" ) . withWeapon ( "Sword" ) . noAbilities ( ) . build ( ) ; LOGGER . info ( warrior . toString ( ) ) ; Character mage = CharacterStepBuilder . newBuilder ( ) . name ( "Riobard" ) . wizardClass ( "Sorcerer" ) . withSpell ( "Fireball" ) . withAbility ( "Fire Aura" ) . withAbility ( "Teleport" ) . noMoreAbilities ( ) . build ( ) ; LOGGER . info ( mage . toString ( ) ) ; Character thief = CharacterStepBuilder . newBuilder ( ) . name ( "Desmond" ) . fighterClass ( "Rogue" ) . noWeapon ( ) . build ( ) ; LOGGER . info ( thief . toString ( ) ) ; }
public static void main ( String [ ] args ) { Character warrior = CharacterStepBuilder . newBuilder ( ) . name ( "Amberjill" ) . fighterClass ( "Paladin" ) . withWeapon ( "Sword" ) . noAbilities ( ) . build ( ) ; LOGGER . info ( warrior . toString ( ) ) ; Character mage = CharacterStepBuilder . newBuilder ( ) . name ( "Riobard" ) . wizardClass ( "Sorcerer" ) . withSpell ( "Fireball" ) . withAbility ( "Fire Aura" ) . withAbility ( "Teleport" ) . noMoreAbilities ( ) . build ( ) ; LOGGER . info ( mage . toString ( ) ) ; Character thief = CharacterStepBuilder . newBuilder ( ) . name ( "Desmond" ) . fighterClass ( "Rogue" ) . noWeapon ( ) . build ( ) ; LOGGER . info ( thief . toString ( ) ) ; }
public static void main ( String [ ] args ) { Character warrior = CharacterStepBuilder . newBuilder ( ) . name ( "Amberjill" ) . fighterClass ( "Paladin" ) . withWeapon ( "Sword" ) . noAbilities ( ) . build ( ) ; LOGGER . info ( warrior . toString ( ) ) ; Character mage = CharacterStepBuilder . newBuilder ( ) . name ( "Riobard" ) . wizardClass ( "Sorcerer" ) . withSpell ( "Fireball" ) . withAbility ( "Fire Aura" ) . withAbility ( "Teleport" ) . noMoreAbilities ( ) . build ( ) ; LOGGER . info ( mage . toString ( ) ) ; Character thief = CharacterStepBuilder . newBuilder ( ) . name ( "Desmond" ) . fighterClass ( "Rogue" ) . noWeapon ( ) . build ( ) ; LOGGER . info ( thief . toString ( ) ) ; }
public void failed ( Throwable exception ) { logger . info ( "Failed " + getProgress ( ) + "% -- " + getTask ( ) + " [" + getPrintable ( getOwner ( ) ) + "]" ) ; }
public void test() { if ( debugEnabled ) { LOG . debug ( "message encoding took {} nanoseconds" , y ) ; } }
public void test() { if ( debugEnabled ) { String sentData = ( ( DebugOutputStream ) stream ) . getContent ( StandardCharsets . UTF_8 . name ( ) ) ; int lastWrittenCharacter = ( ( DebugOutputStream ) stream ) . getLastWrittenCharacter ( ) ; Throwable lastThrownException = ( ( DebugOutputStream ) stream ) . getLastThrownException ( ) ; LOG . debug ( "lastWrittenCharacter={}, lastThrownException={}, sentData: {}" , lastWrittenCharacter , lastThrownException , sentData ) ; } }
public void test() { if ( isWorkspaceImport ) { log . debug ( "AccessControlImporter may not be used with the WorkspaceImporter" ) ; return false ; } }
public void test() { try { final Remote server = RemoteCacheServerFactory . getRemoteCacheServer ( ) ; RemoteCacheServerFactory . registerServer ( serviceName , server ) ; final String message = "Successfully rebound server to registry [" + serviceName + "]." ; code_block = IfStatement ; log . info ( message ) ; } catch ( final RemoteException e ) { final String message = "Could not rebind server to registry [" + serviceName + "]." ; log . error ( message , e ) ; code_block = IfStatement ; } }
public void test() { if ( isDebugEnabled_DLS ) { logger . trace ( LogMarker . DLS_VERBOSE , "[becomeLockGrantor] creating lockGrantorFutureResult" ) ; } }
public void test() { if ( ! ownLockGrantorFutureResult ) { waitForLockGrantorFutureResult ( lockGrantorFutureResultRef , 0 , TimeUnit . MILLISECONDS ) ; } }
public void test() { if ( isDebugEnabled_DLS ) { logger . trace ( LogMarker . DLS_VERBOSE , "[becomeLockGrantor] Calling makeLocalGrantor" ) ; } }
public MbZielobjRelation merge ( MbZielobjRelation detachedInstance ) { log . debug ( "merging MbZielobjRelation instance" ) ; code_block = TryStatement ;  }
public void test() { try { MbZielobjRelation result = ( MbZielobjRelation ) sessionFactory . getCurrentSession ( ) . merge ( detachedInstance ) ; log . debug ( "merge successful" ) ; return result ; } catch ( RuntimeException re ) { log . error ( "merge failed" , re ) ; throw re ; } }
public void test() { try { MbZielobjRelation result = ( MbZielobjRelation ) sessionFactory . getCurrentSession ( ) . merge ( detachedInstance ) ; log . debug ( "merge successful" ) ; return result ; } catch ( RuntimeException re ) { log . error ( "merge failed" , re ) ; throw re ; } }
public void test() { try { bazaarEventListener . onUnregister ( ) ; } catch ( Exception e ) { log . error ( "Error notifying event listener" , e ) ; } }
public void test() { try { sessionTimeoutSec = Integer . parseInt ( timeout ) ; logger . info ( "idle timeout set to " + sessionTimeoutSec ) ; } catch ( NumberFormatException e ) { logger . error ( e ) ; } }
public void test() { if ( result . succeeded ( ) ) { log . info ( result . result ( ) . toString ( ) ) ; vertx . setTimer ( 1000 , v code_block = LoopStatement ; ) ; } else { log . warn ( "got exception" , result . cause ( ) ) ; testContext . fail ( result . cause ( ) ) ; } }
public void test() { if ( result . succeeded ( ) ) { log . info ( result . result ( ) . toString ( ) ) ; vertx . setTimer ( 1000 , v code_block = LoopStatement ; ) ; } else { log . warn ( "got exception" , result . cause ( ) ) ; testContext . fail ( result . cause ( ) ) ; } }
public void test() { -> { testContext . assertEquals ( 1 , mailClient . getConnectionPool ( ) . connCount ( ) ) ; log . info ( "closing mail service" ) ; mailClient . close ( ) ; testContext . assertEquals ( 0 , mailClient . getConnectionPool ( ) . connCount ( ) ) ; async2 . complete ( ) ; } }
public void test() { try { conn = this . getConnection ( ) ; stat = conn . createStatement ( ) ; res = stat . executeQuery ( this . getLoadCategoriesQuery ( ) ) ; code_block = WhileStatement ; } catch ( Throwable t ) { _logger . error ( "Error loading categories" , t ) ; throw new RuntimeException ( "Error loading categories" , t ) ; } finally { closeDaoResources ( res , stat , conn ) ; } }
public void test() { try { return validate ( ( Class < T > ) Class . forName ( val . toString ( ) ) , property . getHelper ( ) . getBaseClass ( ) ) ; } catch ( final ClassNotFoundException e ) { LOGGER . error ( "Class not found for property " + property , e ) ; } catch ( final java . lang . IllegalArgumentException ex ) { LOGGER . error ( "Invalid class for property" + property , ex ) ; throw new IllegalArgumentException ( "Invalid class for property" + property ) ; } }
public void test() { try { return validate ( ( Class < T > ) Class . forName ( val . toString ( ) ) , property . getHelper ( ) . getBaseClass ( ) ) ; } catch ( final ClassNotFoundException e ) { LOGGER . error ( "Class not found for property " + property , e ) ; } catch ( final java . lang . IllegalArgumentException ex ) { LOGGER . error ( "Invalid class for property" + property , ex ) ; throw new IllegalArgumentException ( "Invalid class for property" + property ) ; } }
public void test() { try { com . liferay . commerce . discount . model . CommerceDiscount returnValue = CommerceDiscountServiceUtil . updateCommerceDiscount ( commerceDiscountId , title , target , useCouponCode , couponCode , usePercentage , maximumDiscountAmount , level , level1 , level2 , level3 , level4 , limitationType , limitationTimes , limitationTimesPerAccount , rulesConjunction , active , displayDateMonth , displayDateDay , displayDateYear , displayDateHour , displayDateMinute , expirationDateMonth , expirationDateDay , expirationDateYear , expirationDateHour , expirationDateMinute , neverExpire , serviceContext ) ; return com . liferay . commerce . discount . model . CommerceDiscountSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try ( BufferedReader reader = new BufferedReader ( new InputStreamReader ( stream , StandardCharsets . UTF_8 ) ) ) { fileContent = reader . lines ( ) . collect ( Collectors . joining ( "\n" ) ) ; } catch ( IOException e ) { log . error ( String . format ( "Error reading file: \"%s\"" , filename ) , e ) ; } }
public void test() { try { modulePath = new URL ( moduleBaseURL ) . getPath ( ) ; } catch ( MalformedURLException ex ) { logger . error ( Messages . getInstance ( ) . getErrorString ( "GwtRpcPluginProxyServlet.ERROR_0004_MALFORMED_URL" , moduleBaseURL ) , ex ) ; return null ; } }
public void test() { try { rpcFileInputStream = serializationPolicyUrl . openStream ( ) ; code_block = IfStatement ; } catch ( IOException e ) { logger . error ( Messages . getInstance ( ) . getErrorString ( "GwtRpcPluginProxyServlet.ERROR_0007_FAILED_TO_OPEN_FILE" , serializationPolicyFilename ) , e ) ; } catch ( ParseException e ) { logger . error ( Messages . getInstance ( ) . getErrorString ( "GwtRpcPluginProxyServlet.ERROR_0008_FAILED_TO_PARSE_FILE" , serializationPolicyFilename ) , e ) ; } finally { code_block = IfStatement ; } }
public void test() { try { rpcFileInputStream = serializationPolicyUrl . openStream ( ) ; code_block = IfStatement ; } catch ( IOException e ) { logger . error ( Messages . getInstance ( ) . getErrorString ( "GwtRpcPluginProxyServlet.ERROR_0007_FAILED_TO_OPEN_FILE" , serializationPolicyFilename ) , e ) ; } catch ( ParseException e ) { logger . error ( Messages . getInstance ( ) . getErrorString ( "GwtRpcPluginProxyServlet.ERROR_0008_FAILED_TO_PARSE_FILE" , serializationPolicyFilename ) , e ) ; } finally { code_block = IfStatement ; } }
public void test() { try { OperationResult result = new OperationResult ( OPERATION_CHECK_DELEGATE_AUTHORIZATION ) ; Task task = getPageBase ( ) . createSimpleTask ( OPERATION_CHECK_DELEGATE_AUTHORIZATION ) ; return WebComponentUtil . runUnderPowerOfAttorneyIfNeeded ( ( ) -> getPageBase ( ) . getWorkflowManager ( ) . isCurrentUserAuthorizedToDelegate ( getModelObject ( ) , task , result ) , getPowerDonor ( ) , getPageBase ( ) , task , result ) ; } catch ( Exception ex ) { LOGGER . error ( "Cannot check current user authorization to submit work item: {}" , ex . getLocalizedMessage ( ) , ex ) ; return false ; } }
public void test() { try { Opt < MediaPackage > upcoming = service . getUpcomingRecording ( agentId ) ; code_block = IfStatement ; } catch ( UnauthorizedException e ) { throw e ; } catch ( Exception e ) { logger . error ( "Unable to get the upcoming recording for agent '{}'" , agentId , e ) ; throw new WebApplicationException ( Response . Status . INTERNAL_SERVER_ERROR ) ; } }
public void test() { if ( log . isWarnEnabled ( ) ) { log . warn ( "Could not process request message. [exception=({}), artifact=({}), " + "contract=({}), issuer=({}), messageId=({})]" , exception . getMessage ( ) , requestedArtifact , transferContract , issuerConnector , messageId , exception ) ; } }
public static void measureAndAssert ( int nRuns , float expectedFactor , float allowedVariation ) { long duration = System . currentTimeMillis ( ) - currentMark ; float perRun = ( duration / ( float ) nRuns ) ; log . info ( "Average time per run: {" + perRun + "} ms" ) ; float factor = perRun / computeBaseline ( ) ; assertThat ( expectedFactor ) . isCloseTo ( factor , Percentage . withPercentage ( allowedVariation ) ) ; }
@ Test public void testPrepareSSL3 ( ) throws IOException { CertificateMessage certmessage = new CertificateMessage ( ) ; certmessage . setCertificatesListBytes ( ArrayConverter . hexStringToByteArray ( "00027a30820276308201dfa003020102020438918374300d06092a864886f70d01010b0500306e3110300e06035504061307556e6b6e6f776e3110300e06035504081307556e6b6e6f776e3110300e06035504071307556e6b6e6f776e3110300e060355040a1307556e6b6e6f776e3110300e060355040b1307556e6b6e6f776e3112301006035504031309616e6f6e796d6f7573301e170d3135303830343133353731375a170d3235303830313133353731375a306e3110300e06035504061307556e6b6e6f776e3110300e06035504081307556e6b6e6f776e3110300e06035504071307556e6b6e6f776e3110300e060355040a1307556e6b6e6f776e3110300e060355040b1307556e6b6e6f776e3112301006035504031309616e6f6e796d6f757330819f300d06092a864886f70d010101050003818d00308189028181008a4ee023df569ce17c504cbb828f16bae5040ccef4b59ef96733dfe34693530d4062f9b4873c72f933607f8ceea01ad2215dab44eaac207f45de5835a8db4e21b35d5e2757f652eaaa25d71a60c37725cddf877427cc9e60e240d0429e708bc4b6017726734b2c03f404d5fea407d91bbe4e86a0ebc685e8078f8657b5830ab30203010001a321301f301d0603551d0e04160414611782c41da8bd62a49ce58580194baa5d8c764f300d06092a864886f70d01010b0500038181005f9708702b8adb185b2db0d05845af5df1f7d13e7a94647a8653187e7a55753f5c19772a994f53136ab04cdad266683bf65a1b78fca418899e44c0e8f75add9df5b432e92a6a0668b16d6278a67c78f8ea30ca587e1dc314d8312d41808284e22df19c7f4bb3086e74b42c9473df8b82449643a4e2fbb05cf8b1b41acec44fe9" ) ) ; certmessage . setCertificatesListLength ( 637 ) ; Security . addProvider ( new BouncyCastleProvider ( ) ) ; CertificateMessageHandler handler = new CertificateMessageHandler ( context ) ; handler . adjustTLSContext ( certmessage ) ; Certificate cert = parseCertificate ( certmessage . getCertificatesListLength ( ) . getValue ( ) , certmessage . getCertificatesListBytes ( ) . getValue ( ) ) ; context . setClientRsaModulus ( CertificateUtils . extractRSAModulus ( cert ) ) ; String preMasterSecret = "1a4dc552ddd7e1e25dbaff38dd447b3a6fdc85120e2f760fefdab88e5adbbc710f3d0843f07c9f4f5ac01bc4cea02c4030c272074aa04b1b80a71123b73ea4efbe928b54a83fe4b39472bf66a953c7dc11cfb13ea08f92047996799ce702eb72a7c69bdfd98b91a09bcb836414752d93d3641740f8ed5cfff682225434052230" ; String keyEx = " 100000801a4dc552ddd7e1e25dbaff38dd447b3a6fdc85120e2f760fefdab88e5adbbc710f3d0843f07c9f4f5ac01bc4cea02c4030c272074aa04b1b80a71123b73ea4efbe928b54a83fe4b39472bf66a953c7dc11cfb13ea08f92047996799ce702eb72a7c69bdfd98b91a09bcb836414752d93d3641740f8ed5cfff682225434052230" ; LOGGER . debug ( keyEx . length ( ) ) ; context . setSelectedCipherSuite ( CipherSuite . TLS_RSA_WITH_NULL_MD5 ) ; context . setSelectedProtocolVersion ( ProtocolVersion . SSL3 ) ; context . setClientRandom ( ArrayConverter . hexStringToByteArray ( "405e2a60cefcb557edd6d41336a3fa4b2dfdae20f4ac7adacbb29c13456e2800" ) ) ; LOGGER . debug ( "405e2a60cefcb557edd6d41336a3fa4b2dfdae20f4ac7adacbb29c13456e2800" . length ( ) ) ; context . setServerRandom ( ArrayConverter . hexStringToByteArray ( "a63cd22a46e4fc22b1f03d579c5f0e43cadfda01ef615fd52a9cdbaed3f6c6c2" ) ) ; preparator . prepareHandshakeMessageContents ( ) ; LOGGER . info ( ArrayConverter . bytesToHexString ( message . getComputations ( ) . getPlainPaddedPremasterSecret ( ) , false ) ) ; }
@ Test public void testPrepareSSL3 ( ) throws IOException { CertificateMessage certmessage = new CertificateMessage ( ) ; certmessage . setCertificatesListBytes ( ArrayConverter . hexStringToByteArray ( "00027a30820276308201dfa003020102020438918374300d06092a864886f70d01010b0500306e3110300e06035504061307556e6b6e6f776e3110300e06035504081307556e6b6e6f776e3110300e06035504071307556e6b6e6f776e3110300e060355040a1307556e6b6e6f776e3110300e060355040b1307556e6b6e6f776e3112301006035504031309616e6f6e796d6f7573301e170d3135303830343133353731375a170d3235303830313133353731375a306e3110300e06035504061307556e6b6e6f776e3110300e06035504081307556e6b6e6f776e3110300e06035504071307556e6b6e6f776e3110300e060355040a1307556e6b6e6f776e3110300e060355040b1307556e6b6e6f776e3112301006035504031309616e6f6e796d6f757330819f300d06092a864886f70d010101050003818d00308189028181008a4ee023df569ce17c504cbb828f16bae5040ccef4b59ef96733dfe34693530d4062f9b4873c72f933607f8ceea01ad2215dab44eaac207f45de5835a8db4e21b35d5e2757f652eaaa25d71a60c37725cddf877427cc9e60e240d0429e708bc4b6017726734b2c03f404d5fea407d91bbe4e86a0ebc685e8078f8657b5830ab30203010001a321301f301d0603551d0e04160414611782c41da8bd62a49ce58580194baa5d8c764f300d06092a864886f70d01010b0500038181005f9708702b8adb185b2db0d05845af5df1f7d13e7a94647a8653187e7a55753f5c19772a994f53136ab04cdad266683bf65a1b78fca418899e44c0e8f75add9df5b432e92a6a0668b16d6278a67c78f8ea30ca587e1dc314d8312d41808284e22df19c7f4bb3086e74b42c9473df8b82449643a4e2fbb05cf8b1b41acec44fe9" ) ) ; certmessage . setCertificatesListLength ( 637 ) ; Security . addProvider ( new BouncyCastleProvider ( ) ) ; CertificateMessageHandler handler = new CertificateMessageHandler ( context ) ; handler . adjustTLSContext ( certmessage ) ; Certificate cert = parseCertificate ( certmessage . getCertificatesListLength ( ) . getValue ( ) , certmessage . getCertificatesListBytes ( ) . getValue ( ) ) ; context . setClientRsaModulus ( CertificateUtils . extractRSAModulus ( cert ) ) ; String preMasterSecret = "1a4dc552ddd7e1e25dbaff38dd447b3a6fdc85120e2f760fefdab88e5adbbc710f3d0843f07c9f4f5ac01bc4cea02c4030c272074aa04b1b80a71123b73ea4efbe928b54a83fe4b39472bf66a953c7dc11cfb13ea08f92047996799ce702eb72a7c69bdfd98b91a09bcb836414752d93d3641740f8ed5cfff682225434052230" ; String keyEx = " 100000801a4dc552ddd7e1e25dbaff38dd447b3a6fdc85120e2f760fefdab88e5adbbc710f3d0843f07c9f4f5ac01bc4cea02c4030c272074aa04b1b80a71123b73ea4efbe928b54a83fe4b39472bf66a953c7dc11cfb13ea08f92047996799ce702eb72a7c69bdfd98b91a09bcb836414752d93d3641740f8ed5cfff682225434052230" ; LOGGER . debug ( keyEx . length ( ) ) ; context . setSelectedCipherSuite ( CipherSuite . TLS_RSA_WITH_NULL_MD5 ) ; context . setSelectedProtocolVersion ( ProtocolVersion . SSL3 ) ; context . setClientRandom ( ArrayConverter . hexStringToByteArray ( "405e2a60cefcb557edd6d41336a3fa4b2dfdae20f4ac7adacbb29c13456e2800" ) ) ; LOGGER . debug ( "405e2a60cefcb557edd6d41336a3fa4b2dfdae20f4ac7adacbb29c13456e2800" . length ( ) ) ; context . setServerRandom ( ArrayConverter . hexStringToByteArray ( "a63cd22a46e4fc22b1f03d579c5f0e43cadfda01ef615fd52a9cdbaed3f6c6c2" ) ) ; preparator . prepareHandshakeMessageContents ( ) ; LOGGER . info ( ArrayConverter . bytesToHexString ( message . getComputations ( ) . getPlainPaddedPremasterSecret ( ) , false ) ) ; }
@ Test public void testPrepareSSL3 ( ) throws IOException { CertificateMessage certmessage = new CertificateMessage ( ) ; certmessage . setCertificatesListBytes ( ArrayConverter . hexStringToByteArray ( "00027a30820276308201dfa003020102020438918374300d06092a864886f70d01010b0500306e3110300e06035504061307556e6b6e6f776e3110300e06035504081307556e6b6e6f776e3110300e06035504071307556e6b6e6f776e3110300e060355040a1307556e6b6e6f776e3110300e060355040b1307556e6b6e6f776e3112301006035504031309616e6f6e796d6f7573301e170d3135303830343133353731375a170d3235303830313133353731375a306e3110300e06035504061307556e6b6e6f776e3110300e06035504081307556e6b6e6f776e3110300e06035504071307556e6b6e6f776e3110300e060355040a1307556e6b6e6f776e3110300e060355040b1307556e6b6e6f776e3112301006035504031309616e6f6e796d6f757330819f300d06092a864886f70d010101050003818d00308189028181008a4ee023df569ce17c504cbb828f16bae5040ccef4b59ef96733dfe34693530d4062f9b4873c72f933607f8ceea01ad2215dab44eaac207f45de5835a8db4e21b35d5e2757f652eaaa25d71a60c37725cddf877427cc9e60e240d0429e708bc4b6017726734b2c03f404d5fea407d91bbe4e86a0ebc685e8078f8657b5830ab30203010001a321301f301d0603551d0e04160414611782c41da8bd62a49ce58580194baa5d8c764f300d06092a864886f70d01010b0500038181005f9708702b8adb185b2db0d05845af5df1f7d13e7a94647a8653187e7a55753f5c19772a994f53136ab04cdad266683bf65a1b78fca418899e44c0e8f75add9df5b432e92a6a0668b16d6278a67c78f8ea30ca587e1dc314d8312d41808284e22df19c7f4bb3086e74b42c9473df8b82449643a4e2fbb05cf8b1b41acec44fe9" ) ) ; certmessage . setCertificatesListLength ( 637 ) ; Security . addProvider ( new BouncyCastleProvider ( ) ) ; CertificateMessageHandler handler = new CertificateMessageHandler ( context ) ; handler . adjustTLSContext ( certmessage ) ; Certificate cert = parseCertificate ( certmessage . getCertificatesListLength ( ) . getValue ( ) , certmessage . getCertificatesListBytes ( ) . getValue ( ) ) ; context . setClientRsaModulus ( CertificateUtils . extractRSAModulus ( cert ) ) ; String preMasterSecret = "1a4dc552ddd7e1e25dbaff38dd447b3a6fdc85120e2f760fefdab88e5adbbc710f3d0843f07c9f4f5ac01bc4cea02c4030c272074aa04b1b80a71123b73ea4efbe928b54a83fe4b39472bf66a953c7dc11cfb13ea08f92047996799ce702eb72a7c69bdfd98b91a09bcb836414752d93d3641740f8ed5cfff682225434052230" ; String keyEx = " 100000801a4dc552ddd7e1e25dbaff38dd447b3a6fdc85120e2f760fefdab88e5adbbc710f3d0843f07c9f4f5ac01bc4cea02c4030c272074aa04b1b80a71123b73ea4efbe928b54a83fe4b39472bf66a953c7dc11cfb13ea08f92047996799ce702eb72a7c69bdfd98b91a09bcb836414752d93d3641740f8ed5cfff682225434052230" ; LOGGER . debug ( keyEx . length ( ) ) ; context . setSelectedCipherSuite ( CipherSuite . TLS_RSA_WITH_NULL_MD5 ) ; context . setSelectedProtocolVersion ( ProtocolVersion . SSL3 ) ; context . setClientRandom ( ArrayConverter . hexStringToByteArray ( "405e2a60cefcb557edd6d41336a3fa4b2dfdae20f4ac7adacbb29c13456e2800" ) ) ; LOGGER . debug ( "405e2a60cefcb557edd6d41336a3fa4b2dfdae20f4ac7adacbb29c13456e2800" . length ( ) ) ; context . setServerRandom ( ArrayConverter . hexStringToByteArray ( "a63cd22a46e4fc22b1f03d579c5f0e43cadfda01ef615fd52a9cdbaed3f6c6c2" ) ) ; preparator . prepareHandshakeMessageContents ( ) ; LOGGER . info ( ArrayConverter . bytesToHexString ( message . getComputations ( ) . getPlainPaddedPremasterSecret ( ) , false ) ) ; }
public void test() { if ( ! fs . exists ( interpreterSettingPath ) ) { LOGGER . warn ( "Interpreter Setting file {} is not existed" , interpreterSettingPath ) ; return null ; } }
@ Override public InterpreterInfoSaving loadInterpreterSettings ( ) throws IOException { code_block = IfStatement ; LOGGER . info ( "Load Interpreter Setting from file: {}" , interpreterSettingPath ) ; String json = fs . readFile ( interpreterSettingPath ) ; return buildInterpreterInfoSaving ( json ) ; }
public void test() { if ( i == 10 ) { LOG . error ( "Couldn't save notification: {}" , notification . getDocument ( ) , e ) ; } else { notification . reload ( ) ; } }
public void test() { try { logger . info ( "Creating new manager instance of {}" , clz ) ; Method method = clz . getDeclaredMethod ( "newInstance" , KylinConfig . class ) ; method . setAccessible ( true ) ; mgr = method . invoke ( null , this ) ; } catch ( Exception e ) { throw new RuntimeException ( e ) ; } }
public void test() { if ( vfsLog != null ) { vfsLog . debug ( message , t ) ; } else-if ( commonsLog != null ) { commonsLog . debug ( message , t ) ; } }
public void test() { if ( vfsLog != null ) { vfsLog . debug ( message , t ) ; } else-if ( commonsLog != null ) { commonsLog . debug ( message , t ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Loading from mrsImageProviderCache" ) ; log . debug ( "   cacheKey: {}" , cacheKey ) ; log . debug ( "   name: {}" , name ) ; log . debug ( "   accessMode: {}" , accessMode . name ( ) ) ; log . debug ( "   conf: {}" , conf ) ; log . debug ( "   provider properties: {}" , providerProperties ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Loading from mrsImageProviderCache" ) ; log . debug ( "   cacheKey: {}" , cacheKey ) ; log . debug ( "   name: {}" , name ) ; log . debug ( "   accessMode: {}" , accessMode . name ( ) ) ; log . debug ( "   conf: {}" , conf ) ; log . debug ( "   provider properties: {}" , providerProperties ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Loading from mrsImageProviderCache" ) ; log . debug ( "   cacheKey: {}" , cacheKey ) ; log . debug ( "   name: {}" , name ) ; log . debug ( "   accessMode: {}" , accessMode . name ( ) ) ; log . debug ( "   conf: {}" , conf ) ; log . debug ( "   provider properties: {}" , providerProperties ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Loading from mrsImageProviderCache" ) ; log . debug ( "   cacheKey: {}" , cacheKey ) ; log . debug ( "   name: {}" , name ) ; log . debug ( "   accessMode: {}" , accessMode . name ( ) ) ; log . debug ( "   conf: {}" , conf ) ; log . debug ( "   provider properties: {}" , providerProperties ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Loading from mrsImageProviderCache" ) ; log . debug ( "   cacheKey: {}" , cacheKey ) ; log . debug ( "   name: {}" , name ) ; log . debug ( "   accessMode: {}" , accessMode . name ( ) ) ; log . debug ( "   conf: {}" , conf ) ; log . debug ( "   provider properties: {}" , providerProperties ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Loading from mrsImageProviderCache" ) ; log . debug ( "   cacheKey: {}" , cacheKey ) ; log . debug ( "   name: {}" , name ) ; log . debug ( "   accessMode: {}" , accessMode . name ( ) ) ; log . debug ( "   conf: {}" , conf ) ; log . debug ( "   provider properties: {}" , providerProperties ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "ClientHealthMonitor: Registering client with member id {}" , proxyID ) ; } }
public void testMapSubkeyUsage ( ) throws Exception { entity . config ( ) . set ( TestEntity . CONF_MAP_THING_OBJECT . subKey ( "a" ) , 1 ) ; log . info ( "Map-SubKey: " + MutableMap . copyOf ( entity . getConfigMap ( ) . asMapWithStringKeys ( ) ) ) ; Assert . assertEquals ( entity . getConfig ( TestEntity . CONF_MAP_THING_OBJECT ) , ImmutableMap . < String , Object > of ( "a" , 1 ) ) ; }
public void test() { try { session . invalidate ( ) ; } catch ( Exception e ) { LOG . warn ( "Invalidating session {} found to be expired when requested" , id , e ) ; } }
public void test() { try { getSessionIdManager ( ) . invalidateAll ( id ) ; } catch ( Exception x ) { LOG . warn ( "Error cross-context invalidating unreadable session {}" , id , x ) ; } }
public void test() { try { Session session = _sessionCache . get ( id ) ; code_block = IfStatement ; return session ; } catch ( UnreadableSessionDataException e ) { LOG . warn ( "Error loading session {}" , id , e ) ; code_block = TryStatement ;  return null ; } catch ( Exception other ) { LOG . warn ( "Unable to get Session" , other ) ; return null ; } }
public void test() { switch ( channelUID . getId ( ) ) { case CHANNEL_POWER : code_block = IfStatement ; scheduler . schedule ( updateRunnable , 4 , SECONDS ) ; break ; case CHANNEL_VOLUME_PERCENT : code_block = IfStatement ; scheduler . schedule ( updateRunnable , 1 , SECONDS ) ; break ; case CHANNEL_VOLUME_ABSOLUTE : code_block = IfStatement ; scheduler . schedule ( updateRunnable , 1 , SECONDS ) ; break ; case CHANNEL_MODE : code_block = IfStatement ; break ; case CHANNEL_PRESET : code_block = IfStatement ; break ; case CHANNEL_MUTE : code_block = IfStatement ; break ; default : logger . warn ( "Ignoring unknown command: {}" , command ) ; } }
public void test() { try { StructuredRecord record = StructuredRecordStringConverter . fromJsonString ( recordString , schema ) ; sample . add ( record ) ; } catch ( IOException e ) { LOG . warn ( "Error converting the json string {} to StructuredRecord" , recordString , e ) ; } }
public void installEnmasseBundle ( ) throws Exception { LOGGER . info ( "***********************************************************" ) ; LOGGER . info ( "                  Enmasse operator install" ) ; LOGGER . info ( "***********************************************************" ) ; installOperators ( ) ; installExamplesBundle ( kube . getInfraNamespace ( ) ) ; waitUntilOperatorReady ( kube . getInfraNamespace ( ) ) ; LOGGER . info ( "***********************************************************" ) ; }
public void test() { try { clazz = Class . forName ( clazzName ) ; ctor = clazz . getConstructor ( long . class ) ; } catch ( Exception e ) { log . trace ( "Could not initialize generator " + this , e ) ; } }
public void test() { if ( ! ( Boolean ) zookeeperLocalCluster . getClass ( ) . getMethod ( "cleanupTestDir" ) . invoke ( zookeeperLocalCluster ) ) { LOGGER . warn ( "Unable to delete mini zookeeper temporary directory" ) ; } }
public void test() { try { zookeeperLocalCluster . getClass ( ) . getMethod ( "shutdownMiniZKCluster" ) . invoke ( zookeeperLocalCluster ) ; code_block = IfStatement ; } catch ( final Exception e ) { LOGGER . warn ( "Unable to delete or shutdown mini zookeeper temporary directory" , e ) ; } }
public void test() { try { return RefreshScopeConfigurationScaleTests . this . service . getMessage ( ) ; } finally { latch . countDown ( ) ; logger . debug ( "Background done." ) ; } }
public void test() { if ( cause != null ) { LOG . error ( cause . getMessage ( ) , cause ) ; } }
public void test() { try { VersionsBean vb = resourceAdminServiceStub . getVersionsBean ( path ) ; versionPaths = vb . getVersionPaths ( ) ; } catch ( RemoteException e ) { log . error ( "No versions for created path : " + e . getMessage ( ) ) ; throw new RemoteException ( "Get version error : " , e ) ; } catch ( ResourceAdminServiceExceptionException e ) { log . error ( "Get version error : " + e . getMessage ( ) ) ; throw new ResourceAdminServiceExceptionException ( "Get version error : " , e ) ; } }
public void test() { try { VersionsBean vb = resourceAdminServiceStub . getVersionsBean ( path ) ; versionPaths = vb . getVersionPaths ( ) ; } catch ( RemoteException e ) { log . error ( "No versions for created path : " + e . getMessage ( ) ) ; throw new RemoteException ( "Get version error : " , e ) ; } catch ( ResourceAdminServiceExceptionException e ) { log . error ( "Get version error : " + e . getMessage ( ) ) ; throw new ResourceAdminServiceExceptionException ( "Get version error : " , e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( IOException e ) { LOG . warn ( MessageFormat . format ( JGitText . get ( ) . unableToReadPackfile , p . getPackFile ( ) . getAbsolutePath ( ) ) , e ) ; remove ( p ) ; } }
public void test() { { LOG . trace ( "Entering wait" ) ; latch . await ( ) ; LOG . trace ( "Wait completed" ) ; code_block = IfStatement ; Object object = result . get ( ) ; code_block = IfStatement ; } }
public void test() { { LOG . trace ( "Entering wait" ) ; latch . await ( ) ; LOG . trace ( "Wait completed" ) ; code_block = IfStatement ; Object object = result . get ( ) ; code_block = IfStatement ; } }
@ Bean ( destroyMethod = "stop" , name = "wsDistributionAutomationOutboundDomainRequestsConnectionFactory" ) public ConnectionFactory connectionFactory ( ) { LOGGER . info ( "Initializing wsDistributionAutomationOutboundDomainRequestsConnectionFactory bean." ) ; return this . jmsConfigurationFactory . getPooledConnectionFactory ( ) ; }
public void test() { if ( count % 1000 == 0 ) { log . debug ( "received " + count ) ; } }
public void test() { try { result = em . find ( PermissionEntity . class , id ) ; } catch ( Exception e ) { LOG . error ( e . getMessage ( ) ) ; } finally { daoManager . closeEntityManager ( em ) ; } }
public void test() { if ( new File ( modelFilename ) . exists ( ) ) { log . info ( "Load model..." ) ; model = ModelSerializer . restoreComputationGraph ( modelFilename ) ; } else { log . info ( "Model not found." ) ; } }
public void test() { if ( new File ( modelFilename ) . exists ( ) ) { log . info ( "Load model..." ) ; model = ModelSerializer . restoreComputationGraph ( modelFilename ) ; } else { log . info ( "Model not found." ) ; } }
public void test() { if ( ! capture . get ( ) . open ( 0 ) ) { log . error ( "Can not open the cam !!!" ) ; } }
public void test() { try { Thread . sleep ( 20 ) ; } catch ( InterruptedException ex ) { log . error ( ex . getMessage ( ) ) ; } }
public void test() { try { messageMetadata = MessageMetadata . fromMessage ( message ) ; lightValueMessageDataContainer = ( LightValueMessageDataContainerDto ) message . getObject ( ) ; } catch ( final JMSException e ) { LOGGER . error ( "UNRECOVERABLE ERROR, unable to read ObjectMessage instance, giving up." , e ) ; return ; } }
@ Test public void test_filesize_limit ( ) throws Exception { logger . info ( " ---> Creating a smaller file small.txt" ) ; Files . write ( currentTestResourceDir . resolve ( "small.txt" ) , "This is a second file smaller than the previous one" . getBytes ( ) ) ; Fs fs = startCrawlerDefinition ( ) . setIgnoreAbove ( ByteSizeValue . parseBytesSizeValue ( "10kb" ) ) . build ( ) ; startCrawler ( getCrawlerName ( ) , fs , endCrawlerDefinition ( getCrawlerName ( ) ) , null ) ; countTestHelper ( new ESSearchRequest ( ) . withIndex ( getCrawlerName ( ) ) , 1L , null ) ; }
public void test() { if ( fingerprint == null || fingerprint . trim ( ) . isEmpty ( ) ) { logger . info ( "Inheriting Empty Policies, Users & Groups. Will backup existing Policies, Users & Groups first." ) ; backupPoliciesUsersAndGroups ( ) ; purgePoliciesUsersAndGroups ( ) ; return ; } }
public void test() { if ( isInheritable ( policiesUsersAndGroups ) ) { logger . debug ( "Inheriting Polciies, Users & Groups" ) ; inheritPoliciesUsersAndGroups ( policiesUsersAndGroups ) ; } else { logger . info ( "Cannot directly inherit Policies, Users & Groups. Will backup existing Policies, Users & Groups, and then replace with proposed configuration" ) ; backupPoliciesUsersAndGroups ( ) ; purgePoliciesUsersAndGroups ( ) ; addPoliciesUsersAndGroups ( policiesUsersAndGroups ) ; } }
public void test() { if ( isInheritable ( policiesUsersAndGroups ) ) { logger . debug ( "Inheriting Polciies, Users & Groups" ) ; inheritPoliciesUsersAndGroups ( policiesUsersAndGroups ) ; } else { logger . info ( "Cannot directly inherit Policies, Users & Groups. Will backup existing Policies, Users & Groups, and then replace with proposed configuration" ) ; backupPoliciesUsersAndGroups ( ) ; purgePoliciesUsersAndGroups ( ) ; addPoliciesUsersAndGroups ( policiesUsersAndGroups ) ; } }
public void test() { try { association . stopAnonymousAssociation ( ) ; } catch ( Exception ex ) { logger . error ( "Error closing channel: " + ex . getMessage ( ) ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( NodeStoreException e ) { LOGGER . error ( e ) ; response . getElement ( ) . remove ( pubsub ) ; setErrorCondition ( PacketError . Type . wait , PacketError . Condition . internal_server_error ) ; } }
private void writeSignatureHandshakeAlgorithms ( CertificateRequestMessage msg ) { appendBytes ( msg . getSignatureHashAlgorithms ( ) . getValue ( ) ) ; LOGGER . debug ( "SignatureHashAlgorithms: " + ArrayConverter . bytesToHexString ( msg . getSignatureHashAlgorithms ( ) . getValue ( ) ) ) ; }
@ Test public void testWrappedStreamSerialization ( ) throws Exception { String result = checkSerializesAs ( BrooklynTaskTags . tagForStream ( "TEST" , Streams . byteArrayOfString ( "x" ) ) , null ) ; log . info ( "WRAPPED STREAM json is: " + result ) ; Assert . assertFalse ( result . contains ( "error" ) , "Shouldn't have had an error, instead got: " + result ) ; }
public void test() { if ( ! config . isIS_QUIET_MODE ( ) ) { LOGGER . info ( "{} query SQL: {}" , Thread . currentThread ( ) . getName ( ) , sql ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Invoking function {}" , function ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Invoking object+function {}" , name ) ; } }
public void test() { if ( member . canExecute ( ) ) { code_block = IfStatement ; return ( T ) member . getMember ( "call" ) . execute ( coalesce ( self , arguments ) ) ; } else { logger . info ( "Member {} not a function in {}" , name , jsThis ) ; return null ; } }
public void test() { if ( self . hasMember ( name ) ) { Value member = self . getMember ( name ) ; code_block = IfStatement ; } else { logger . info ( "{} not a member in {}" , name , jsThis ) ; return null ; } }
public void test() { try { Value self = Value . asValue ( jsThis ) ; Value funktion = Value . asValue ( function ) ; code_block = IfStatement ; String name = funktion . asString ( ) ; code_block = IfStatement ; } catch ( Throwable x ) { logger . info ( "Exception while trying to invoke " + function , x ) ; throw x ; } finally { context . leave ( ) ; } }
public void test() { if ( ! sentryConfig . inAppPackages . isPresent ( ) ) { LOG . warn ( "No 'quarkus.sentry.in-app-packages' was configured, this option is highly recommended as it affects stacktrace grouping and display on Sentry. See https://quarkus.io/guides/logging-sentryin-app-packages" ) ; } else { List < String > inAppPackages = sentryConfig . inAppPackages . get ( ) ; code_block = IfStatement ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Resolve name {} to rack {}." , n , rack ) ; } }
public void test() { try { Method method = webStat . getClass ( ) . getMethod ( "getSessionStatDataList" ) ; Object obj = method . invoke ( webStat ) ; return ( List < Map < String , Object > > ) obj ; } catch ( Exception e ) { LOG . error ( "getSessionStatDataList error" , e ) ; return null ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( TeamServiceUtil . class , "getTeam" , _getTeamParameterTypes4 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , name ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . portal . kernel . model . Team ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public ManageSearchPage performSelectedActions ( ) { log . info ( "Click Perform Actions" ) ; clickElement ( performButton ) ; waitForAMoment ( ) . withMessage ( "displayed abort button" ) . until ( it -> readyElement ( cancelButton ) . isDisplayed ( ) ) ; return new ManageSearchPage ( getDriver ( ) ) ; }
@ Transactional ( rollbackFor = ArrowheadException . class ) public AuthorizationIntraCloudListResponseDTO createBulkAuthorizationIntraCloudResponse ( final long consumerId , final Set < Long > providerIds , final Set < Long > serviceDefinitionIds , final Set < Long > interfaceIds ) { logger . debug ( "createBulkAuthorizationIntraCloudResponse started..." ) ; final List < AuthorizationIntraCloud > entries = createBulkAuthorizationIntraCloud ( consumerId , providerIds , serviceDefinitionIds , interfaceIds ) ; final Page < AuthorizationIntraCloud > entryPage = new PageImpl < > ( entries ) ; return DTOConverter . convertAuthorizationIntraCloudListToAuthorizationIntraCloudListResponseDTO ( entryPage ) ; }
public void test() { if ( maybeCached != null ) { LOG . info ( "Offer {} on {} rescinded" , offerId . getValue ( ) , maybeCached . getOffer ( ) . getHostname ( ) ) ; } else { LOG . info ( "Offer {} rescinded (not in cache)" , offerId . getValue ( ) ) ; } }
public void test() { if ( maybeCached != null ) { LOG . info ( "Offer {} on {} rescinded" , offerId . getValue ( ) , maybeCached . getOffer ( ) . getHostname ( ) ) ; } else { LOG . info ( "Offer {} rescinded (not in cache)" , offerId . getValue ( ) ) ; } }
@ RequestMapping ( value = "/migration/events/{eventId}" , method = RequestMethod . GET ) public List < MigrationClusterModel > getEventDetailsWithEventId ( @ PathVariable Long eventId ) { logger . info ( "[getEventDetailsWithEventId][begin] eventId: {}" , eventId ) ; List < MigrationClusterModel > res = new LinkedList < > ( ) ; code_block = IfStatement ; logger . info ( "[getEventDetailsWithEventId][end] eventId: {}" , eventId ) ; return res ; }
public void test() { if ( null != eventId ) { res = migrationService . getMigrationClusterModel ( eventId ) ; } else { logger . error ( "[GetEvent][fail]Cannot findRedisHealthCheckInstance with null event id." ) ; } }
@ RequestMapping ( value = "/migration/events/{eventId}" , method = RequestMethod . GET ) public List < MigrationClusterModel > getEventDetailsWithEventId ( @ PathVariable Long eventId ) { logger . info ( "[getEventDetailsWithEventId][begin] eventId: {}" , eventId ) ; List < MigrationClusterModel > res = new LinkedList < > ( ) ; code_block = IfStatement ; logger . info ( "[getEventDetailsWithEventId][end] eventId: {}" , eventId ) ; return res ; }
public void test() { try ( HetuFileSystemClient hetuFileSystemClient = fileSystemClientManager . getFileSystemClient ( config . getShareFileSystemProfile ( ) , Paths . get ( "/" ) ) ) { int lastIndex = file . lastIndexOf ( File . separator ) ; String tmpFileDir = file . substring ( 0 , lastIndex ) ; code_block = IfStatement ; hetuFileSystemClient . createDirectories ( Paths . get ( tmpFileDir ) ) ; LOG . info ( "success to create the store directories..." ) ; } catch ( IOException e ) { LOG . error ( "fail to create the store directories: %s" , e . getMessage ( ) ) ; throw new RuntimeException ( "fail to create the store directories." ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( IllegalStateException e ) { logger . trace ( "Handler for id {} not initialized" , integrationId ) ; } }
public void test() { try { if ( use_external_key_exchange && ! attach_fetch_key_header ) return Processing . PROCESS ; Message encr_msg = encrypt ( msg ) ; code_block = IfStatement ; down_prot . down ( encr_msg ) ; return Processing . DROP ; } catch ( Exception ex ) { log . warn ( "%s: unable to send message to %s: %s" , msg . getDest ( ) == null ? "all" : msg . getDest ( ) , local_addr , ex ) ; return Processing . PROCESS ; } }
private void obtainKeys ( final KeyStore keyStore , final ApplicationContext appContext ) { logger . debug ( "obtainKeys started..." ) ; @ SuppressWarnings ( "unchecked" ) final Map < String , Object > context = appContext . getBean ( CommonConstants . ARROWHEAD_CONTEXT , Map . class ) ; final X509Certificate serverCertificate = Utilities . getSystemCertFromKeyStore ( keyStore ) ; publicKey = serverCertificate . getPublicKey ( ) ; context . put ( CommonConstants . SERVER_PUBLIC_KEY , publicKey ) ; final PrivateKey privateKey = Utilities . getPrivateKey ( keyStore , sslProperties . getKeyPassword ( ) ) ; context . put ( CommonConstants . SERVER_PRIVATE_KEY , privateKey ) ; context . put ( CommonConstants . SERVER_CERTIFICATE , serverCertificate ) ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( _NO_SUCH_ENTITY_WITH_PRIMARY_KEY + primaryKey ) ; } }
@ Override protected void shutDown ( ) throws Exception { logger . info ( "Shutting down job scheduler" ) ; service . shutdown ( ) ; logger . info ( "Job scheduler shut down" ) ; super . shutDown ( ) ; }
@ Override protected void shutDown ( ) throws Exception { logger . info ( "Shutting down job scheduler" ) ; service . shutdown ( ) ; logger . info ( "Job scheduler shut down" ) ; super . shutDown ( ) ; }
public void test() { if ( StringUtils . isBlank ( ticket ) ) { log . error ( "Ticket is null or blank." ) ; throw new UmaWebException ( claimsRedirectUri , errorResponseFactory , INVALID_TICKET , state ) ; } }
public void test() { if ( StringUtils . isBlank ( ticket ) ) { log . error ( "Ticket is null or blank." ) ; throw new UmaWebException ( claimsRedirectUri , errorResponseFactory , INVALID_TICKET , state ) ; } }
public void test() { try { java . util . List < com . liferay . commerce . model . CommerceSubscriptionEntry > returnValue = CommerceSubscriptionEntryServiceUtil . getCommerceSubscriptionEntries ( companyId , groupId , userId , start , end , orderByComparator ) ; return com . liferay . commerce . model . CommerceSubscriptionEntrySoap . toSoapModels ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
@ PostMapping ( CommonConstants . PATH_IMPORT ) public JsonNode importAgencies ( @ RequestParam ( "fileName" ) String fileName , @ RequestParam ( "file" ) MultipartFile file ) { LOGGER . debug ( "import agency file {}" , fileName ) ; SafeFileChecker . checkSafeFilePath ( file . getOriginalFilename ( ) ) ; final VitamContext vitamContext = securityService . buildVitamContext ( securityService . getTenantIdentifier ( ) ) ; return agencyInternalService . importAgencies ( vitamContext , fileName , file ) ; }
@ Override public void discoverObjectInstance ( ObjectInstanceHandle theObject , ObjectClassHandle theObjectClass , String objectName ) throws FederateInternalError { LOGGER . info ( "Discover Object Instance : " + "Object = " + theObject . toString ( ) + ", Object class = " + theObjectClass . toString ( ) + ", Object name = " + objectName ) ; }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Received PubAck packageID: {}" + packageId ) ; } }
private void handleIncreaseDecrease ( ChannelUID channelUID , IncreaseDecreaseType command ) { logger . debug ( "handleIncreaseDecrease called for channel: {}, command: {}" , channelUID , command ) ; sendOmnilinkCommand ( IncreaseDecreaseType . INCREASE . equals ( command ) ? CommandMessage . CMD_UNIT_UPB_BRIGHTEN_STEP_1 : CommandMessage . CMD_UNIT_UPB_DIM_STEP_1 , 0 , thingID ) ; }
public void task ( ) throws Exception { FromDefinition from = route . getInput ( ) ; LOG . info ( "AdviceWith replace input from [{}] --> [{}]" , from . getEndpointUri ( ) , uri ) ; from . setEndpoint ( null ) ; from . setUri ( uri ) ; }
public void test() { try { return Streams . readFullyStringAndClose ( getResourceFromUrl ( url ) ) ; } catch ( Exception e ) { log . debug ( "ResourceUtils got error reading " + url + ( context == null ? "" : " " + context ) + " (rethrowing): " + e ) ; throw Throwables . propagate ( e ) ; } }
public void test() { if ( e instanceof org . apache . accumulo . core . clientImpl . thrift . ThriftSecurityException ) { result . sec = ( org . apache . accumulo . core . clientImpl . thrift . ThriftSecurityException ) e ; result . setSecIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . accumulo . core . clientImpl . thrift . ThriftTableOperationException ) { result . tope = ( org . apache . accumulo . core . clientImpl . thrift . ThriftTableOperationException ) e ; result . setTopeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . accumulo . core . clientImpl . thrift . ThriftNotActiveServiceException ) { result . tnase = ( org . apache . accumulo . core . clientImpl . thrift . ThriftNotActiveServiceException ) e ; result . setTnaseIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof org . apache . accumulo . core . clientImpl . thrift . ThriftSecurityException ) { result . sec = ( org . apache . accumulo . core . clientImpl . thrift . ThriftSecurityException ) e ; result . setSecIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . accumulo . core . clientImpl . thrift . ThriftTableOperationException ) { result . tope = ( org . apache . accumulo . core . clientImpl . thrift . ThriftTableOperationException ) e ; result . setTopeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . accumulo . core . clientImpl . thrift . ThriftNotActiveServiceException ) { result . tnase = ( org . apache . accumulo . core . clientImpl . thrift . ThriftNotActiveServiceException ) e ; result . setTnaseIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof org . apache . accumulo . core . clientImpl . thrift . ThriftSecurityException ) { result . sec = ( org . apache . accumulo . core . clientImpl . thrift . ThriftSecurityException ) e ; result . setSecIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . accumulo . core . clientImpl . thrift . ThriftTableOperationException ) { result . tope = ( org . apache . accumulo . core . clientImpl . thrift . ThriftTableOperationException ) e ; result . setTopeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . accumulo . core . clientImpl . thrift . ThriftNotActiveServiceException ) { result . tnase = ( org . apache . accumulo . core . clientImpl . thrift . ThriftNotActiveServiceException ) e ; result . setTnaseIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { try { fcall . sendResponse ( fb , msg , msgType , seqid ) ; } catch ( java . lang . Exception ex ) { _LOGGER . error ( "Exception writing to internal frame buffer" , ex ) ; fb . close ( ) ; } }
public void test() { if ( iterationType instanceof Text ) { return iterationType . toString ( ) ; } else { LOGGER . warn ( MessageFormat . format ( this . getActionExecution ( ) . getAction ( ) . getType ( ) + " does not accept {0} as type for iterationType" , iterationType . getClass ( ) ) ) ; return iterationType . toString ( ) ; } }
public void test() { try { resource = convert ( request , lockSimulator . getResourceClass ( ) ) ; } catch ( Exception e ) { LOG . error ( "Error during resource conversion" , e ) ; return 500 ; } }
public void test() { if ( resource == null ) { LOG . error ( "No resource received" ) ; return 500 ; } }
public void test() { if ( resource . getMetadata ( ) == null || ! lockSimulator . getResourceName ( ) . equals ( resource . getMetadata ( ) . getName ( ) ) ) { LOG . error ( "Illegal resource received" ) ; return 500 ; } }
protected final void log ( String text ) { logger . info ( text ) ; }
public void test() { try { double d = parse ( value ) ; Log . debug ( "CoordinateEditor: '" + value + "' -> " + d ) ; return d ; } catch ( CoordinateFormatException e ) { return null ; } }
@ Override public void invalidateCollectionSyncRootMemberCache ( ) { log . debug ( "Invalidating collection sync root member cache for all users" ) ; getCollectionSyncRootMemberCache ( ) . invalidateAll ( ) ; }
public void test() { try ( ClientSession toClose = session ) { toClose . rollback ( ) ; } catch ( ActiveMQException e ) { log . error ( "Error rolling back ActiveMQ transaction" , e ) ; } }
public static Set < Class < ? > > getTypesAnnotatedWith ( Class < ? extends Annotation > clazz ) { Set < Class < ? > > classes = reflections . getTypesAnnotatedWith ( clazz ) ; log . info ( "Found {} classes annotated with {} " , ( classes != null ? classes . size ( ) : 0 ) , clazz ) ; return classes ; }
public void test() { if ( bridgeUidString == null || bridgeUidString . isEmpty ( ) ) { logger . warn ( "Cannot create bridge: Bridge UID is missing." ) ; return "/mielecloud/failure?" + FailureServlet . MISSING_BRIDGE_UID_PARAMETER_NAME + "=true" ; } }
public void test() { if ( email == null || email . isEmpty ( ) ) { logger . warn ( "Cannot create bridge: E-mail address is missing." ) ; return "/mielecloud/failure?" + FailureServlet . MISSING_EMAIL_PARAMETER_NAME + "=true" ; } }
public void test() { try { bridgeUid = new ThingUID ( bridgeUidString ) ; } catch ( IllegalArgumentException e ) { logger . warn ( "Cannot create bridge: Bridge UID '{}' is malformed." , bridgeUid ) ; return "/mielecloud/failure?" + FailureServlet . MALFORMED_BRIDGE_UID_PARAMETER_NAME + "=true" ; } }
public void test() { if ( ! EmailValidator . isValid ( email ) ) { logger . warn ( "Cannot create bridge: E-mail address '{}' is malformed." , email ) ; return "/mielecloud/failure?" + FailureServlet . MALFORMED_EMAIL_PARAMETER_NAME + "=true" ; } }
public void test() { try { Thing bridge = pairOrReconfigureBridge ( locale , bridgeUid , email ) ; waitForBridgeToComeOnline ( bridge ) ; return "/mielecloud" ; } catch ( BridgeReconfigurationFailedException e ) { logger . warn ( "{}" , e . getMessage ( ) ) ; return "/mielecloud/success?" + SuccessServlet . BRIDGE_RECONFIGURATION_FAILED_PARAMETER_NAME + "=true&" + SuccessServlet . BRIDGE_UID_PARAMETER_NAME + "=" + bridgeUidString + "&" + SuccessServlet . EMAIL_PARAMETER_NAME + "=" + email ; } catch ( BridgeCreationFailedException e ) { logger . warn ( "Thing creation failed because there was no binding available that supports the thing." ) ; return "/mielecloud/success?" + SuccessServlet . BRIDGE_CREATION_FAILED_PARAMETER_NAME + "=true&" + SuccessServlet . BRIDGE_UID_PARAMETER_NAME + "=" + bridgeUidString + "&" + SuccessServlet . EMAIL_PARAMETER_NAME + "=" + email ; } }
public void test() { try { Thing bridge = pairOrReconfigureBridge ( locale , bridgeUid , email ) ; waitForBridgeToComeOnline ( bridge ) ; return "/mielecloud" ; } catch ( BridgeReconfigurationFailedException e ) { logger . warn ( "{}" , e . getMessage ( ) ) ; return "/mielecloud/success?" + SuccessServlet . BRIDGE_RECONFIGURATION_FAILED_PARAMETER_NAME + "=true&" + SuccessServlet . BRIDGE_UID_PARAMETER_NAME + "=" + bridgeUidString + "&" + SuccessServlet . EMAIL_PARAMETER_NAME + "=" + email ; } catch ( BridgeCreationFailedException e ) { logger . warn ( "Thing creation failed because there was no binding available that supports the thing." ) ; return "/mielecloud/success?" + SuccessServlet . BRIDGE_CREATION_FAILED_PARAMETER_NAME + "=true&" + SuccessServlet . BRIDGE_UID_PARAMETER_NAME + "=" + bridgeUidString + "&" + SuccessServlet . EMAIL_PARAMETER_NAME + "=" + email ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( I18n . msg ( I18n . MSG_04102_BIND_REQUEST , name ) ) ; } }
void removeSampleReferences ( ClientSession clientSession , long studyUid , long sampleUid ) throws CatalogDBException , CatalogParameterException , CatalogAuthorizationException { Query query = new Query ( ) . append ( QueryParams . STUDY_UID . key ( ) , studyUid ) . append ( QueryParams . SAMPLE_UIDS . key ( ) , sampleUid ) ; ObjectMap params = new ObjectMap ( ) . append ( QueryParams . SAMPLES . key ( ) , Collections . singletonList ( new Sample ( ) . setUid ( sampleUid ) ) ) ; QueryOptions queryOptions = new QueryOptions ( Constants . ACTIONS , new ObjectMap ( QueryParams . SAMPLES . key ( ) , ParamUtils . BasicUpdateAction . REMOVE . name ( ) ) ) ; Bson update ; code_block = TryStatement ;  QueryOptions multi = new QueryOptions ( MongoDBCollection . MULTI , true ) ; Bson bsonQuery = parseQuery ( query ) ; logger . debug ( "Sample references extraction. Query: {}, update: {}" , bsonQuery . toBsonDocument ( Document . class , MongoClient . getDefaultCodecRegistry ( ) ) , update . toBsonDocument ( Document . class , MongoClient . getDefaultCodecRegistry ( ) ) ) ; DataResult updateResult = individualCollection . update ( clientSession , bsonQuery , update , multi ) ; logger . debug ( "Sample uid '" + sampleUid + "' references removed from " + updateResult . getNumUpdated ( ) + " out of " + updateResult . getNumMatches ( ) + " individuals" ) ; }
public void test() { try { code_block = SwitchStatement ; } catch ( Exception e ) { log . debug ( DataFormatter . class . getName ( ) + " could not apply mask to data. The original data was returned" ) ; return String . valueOf ( data ) ; } }
@ Override public void setBlobStore ( BlobStore blobStore ) { log . info ( "Internal blob store set: {}" , blobStore ) ; splitBlobStore = new DefaultSplitBlobStore ( repositoryDir , blobStore , newBlobStore ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Creating region {}" , this . newRegion ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "CreateRegionProcessor.initializeRegion, no recipients, msg not sent" ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "initialized bucket event tracker: {}" , ( ( LocalRegion ) this . newRegion ) . getEventTracker ( ) ) ; } }
public void test() { try { LOG . debug ( "FULL Updated paths seq Num [old=" + authzPaths . getLastUpdatedSeqNum ( ) + "], [new=" + newAuthzPaths . getLastUpdatedSeqNum ( ) + "]" ) ; authzPaths = newAuthzPaths ; LOG . debug ( "FULL Updated perms seq Num [old=" + authzPermissions . getLastUpdatedSeqNum ( ) + "], [new=" + newAuthzPerms . getLastUpdatedSeqNum ( ) + "]" ) ; authzPermissions = newAuthzPerms ; } finally { lock . writeLock ( ) . unlock ( ) ; } }
public void test() { try { LOG . debug ( "FULL Updated paths seq Num [old=" + authzPaths . getLastUpdatedSeqNum ( ) + "], [new=" + newAuthzPaths . getLastUpdatedSeqNum ( ) + "]" ) ; authzPaths = newAuthzPaths ; LOG . debug ( "FULL Updated perms seq Num [old=" + authzPermissions . getLastUpdatedSeqNum ( ) + "], [new=" + newAuthzPerms . getLastUpdatedSeqNum ( ) + "]" ) ; authzPermissions = newAuthzPerms ; } finally { lock . writeLock ( ) . unlock ( ) ; } }
public void test() { if ( _logger . isDebugEnabled ( ) ) { _logger . debug ( "in Rule " + context . getRule ( ) . getName ( ) + " added to context: [" + t . toString ( ) + "], deductions notified " + deductionsNotified ) ; } }
public void test() { if ( warningReport != null ) { logger . warn ( warningReport ) ; } }
public void test() { if ( errorReport != null ) { logger . error ( errorReport ) ; } }
public void test() { try { prototype . setDescription ( this . getDescription ( ) ) ; prototype . setId ( this . getId ( ) ) ; prototype . setMainGroup ( this . getMainGroup ( ) ) ; prototype . setTypeCode ( this . getTypeCode ( ) ) ; prototype . setTypeDescription ( this . getTypeDescription ( ) ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = ForStatement ; } catch ( Throwable t ) { _logger . error ( "Error creating Entity" , t ) ; throw new RuntimeException ( "Error creating Entity" , t ) ; } }
@ Override @ Transactional public void markJobAsReadyForActivation ( String theJobId ) { ourLog . info ( "Activating bulk import job {}" , theJobId ) ; BulkImportJobEntity job = findJobByJobId ( theJobId ) ; ValidateUtil . isTrueOrThrowInvalidRequest ( job . getStatus ( ) == BulkImportJobStatusEnum . STAGING , "Bulk import job %s can not be activated in status: %s" , theJobId , job . getStatus ( ) ) ; job . setStatus ( BulkImportJobStatusEnum . READY ) ; myJobDao . save ( job ) ; }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { LOGGER . error ( "Error while checking (Path) URI: {}" , absoluteSystemId , e ) ; } }
public void test() { if ( isFileSystemAvailable ( absoluteSystemId . getScheme ( ) ) ) { Path pathTest = Paths . get ( absoluteSystemId ) ; LOGGER . debug ( "Checking: {}" , pathTest ) ; return Files . exists ( pathTest ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { LOGGER . error ( "Error while checking (Path) URI: {}" , absoluteSystemId , e ) ; } }
public void test() { if ( ! directory . exists ( ) ) { log . error ( "Cannot load stream definitions from " + directory . getAbsolutePath ( ) + " directory not exist" ) ; return streamDefinitions ; } }
public void test() { if ( ! directory . isDirectory ( ) ) { log . error ( "Cannot load stream definitions from " + directory . getAbsolutePath ( ) + " not a directory" ) ; return streamDefinitions ; } }
public void test() { try { bufferedReader = new BufferedReader ( new FileReader ( fullPathToStreamDefinitionFile ) ) ; String line ; code_block = WhileStatement ; StreamDefinition streamDefinition = EventDefinitionConverterUtils . convertFromJson ( stringBuilder . toString ( ) . trim ( ) ) ; streamDefinitions . add ( streamDefinition ) ; } catch ( IOException e ) { log . error ( "Error in reading file : " + fullPathToStreamDefinitionFile , e ) ; } catch ( MalformedStreamDefinitionException e ) { log . error ( "Error in converting Stream definition : " + e . getMessage ( ) , e ) ; } finally { code_block = TryStatement ;  } }
public void test() { try { bufferedReader = new BufferedReader ( new FileReader ( fullPathToStreamDefinitionFile ) ) ; String line ; code_block = WhileStatement ; StreamDefinition streamDefinition = EventDefinitionConverterUtils . convertFromJson ( stringBuilder . toString ( ) . trim ( ) ) ; streamDefinitions . add ( streamDefinition ) ; } catch ( IOException e ) { log . error ( "Error in reading file : " + fullPathToStreamDefinitionFile , e ) ; } catch ( MalformedStreamDefinitionException e ) { log . error ( "Error in converting Stream definition : " + e . getMessage ( ) , e ) ; } finally { code_block = TryStatement ;  } }
public void test() { try { code_block = IfStatement ; } catch ( IOException e ) { log . error ( "Error occurred when reading the file : " + e . getMessage ( ) , e ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( SSL_SOCKET_CIPHER_SUITE_LOG_MSG , socket , enabledCipherSuites , enabledCipherSuitePatterns , socket . getSSLParameters ( ) . getCipherSuites ( ) , socket . getEnabledCipherSuites ( ) , defaultEnabledCipherSuitePatterns , filteredCipherSuites ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( SSL_SOCKET_PROTOCOL_LOG_MSG , socket , enabledSecureSocketProtocols , enabledSecureSocketProtocolsPatterns , socket . getSSLParameters ( ) . getProtocols ( ) , socket . getEnabledProtocols ( ) , defaultEnabledSecureSocketProtocolsPatterns , filteredSecureSocketProtocols ) ; } }
public void test() { try { String customIP = System . getProperty ( "qmq.ip" ) ; if ( ! Strings . isNullOrEmpty ( customIP ) ) return customIP ; final Enumeration < NetworkInterface > interfaces = NetworkInterface . getNetworkInterfaces ( ) ; final ArrayList < String > ipv4Result = new ArrayList < > ( ) ; final ArrayList < String > ipv6Result = new ArrayList < > ( ) ; code_block = WhileStatement ; code_block = IfStatement ; return InetAddress . getLocalHost ( ) . getHostAddress ( ) ; } catch ( Exception e ) { LOG . error ( "get local address failed" , e ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public Page < WebContent > findAll ( final Pageable pageable ) { log . debug ( "findAll() - pageable: {}" , pageable ) ; return webContentRepository . findAll ( pageable ) ; }
public void test() { try { JavaType mapType = Mapper . mapper ( ) . getTypeFactory ( ) . constructMapType ( HashMap . class , keyClass , valueClass ) ; return Optional . ofNullable ( Mapper . mapper ( ) . readValue ( event . getData ( ) , mapType ) ) ; } catch ( IOException e ) { LOG . error ( "Unable to decode CloudEvent data to Map<" + keyClass . getName ( ) + "," + valueClass . getName ( ) + ">" , e ) ; return Optional . empty ( ) ; } }
public void test() { if ( this . revisionId == null ) { logger . debug ( "sendCommand getRevision :: {}" , SierraMc87xxAtCommands . getFirmwareVersion . getCommand ( ) ) ; byte [ ] reply ; CommConnection commAtConnection = openSerialPort ( getAtPort ( ) ) ; code_block = IfStatement ; code_block = TryStatement ;  closeSerialPort ( commAtConnection ) ; code_block = IfStatement ; } }
public void test() { if ( ActiveMQRALogger . LOGGER . isTraceEnabled ( ) ) { ActiveMQRALogger . LOGGER . trace ( "setDoubleProperty(" + name + ", " + value + ")" ) ; } }
public void test() { if ( parsingError . isPresent ( ) ) { log . warn ( "Failed to parse UI input '{}': {}" , input , parsingError . get ( ) ) ; return null ; } }
public void test() { if ( isVerbose ( ) ) { logger . info ( "Provided private key is in PKCS 8 format" ) ; } }
public void test() { if ( removed != null && removed . isSubmitted ( ) && ! removed . isDone ( ) ) { log . warn ( "Deleting submitted task before completion: " + removed + "; this task will continue to run in the background outwith " + this + ", but perhaps it should have been cancelled?" ) ; } }
public void test() { if ( plan instanceof InsertPlan ) { processPlanWithTolerance ( ( InsertPlan ) plan , dataGroupMember ) ; } else-if ( plan != null && ! plan . isQuery ( ) ) { code_block = TryStatement ;  } else-if ( plan != null ) { logger . error ( "Unsupported physical plan: {}" , plan ) ; } }
public void test() { try { List < String > draftUtilizers = this . getPageManager ( ) . getDraftWidgetUtilizerCodes ( widgetTypeCode ) ; code_block = IfStatement ; List < String > onlineUtilizers = this . getPageManager ( ) . getOnlineWidgetUtilizerCodes ( widgetTypeCode ) ; code_block = IfStatement ; } catch ( Throwable t ) { _logger . error ( "Error on extracting widgetUtilizers : widget type code {}" , t ) ; throw new RuntimeException ( "Error on extracting widgetUtilizers : widget type code " + widgetTypeCode , t ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceDiscountCommerceAccountGroupRelServiceUtil . class , "deleteCommerceDiscountCommerceAccountGroupRelsByCommerceDiscountId" , _deleteCommerceDiscountCommerceAccountGroupRelsByCommerceDiscountIdParameterTypes2 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , commerceDiscountId ) ; code_block = TryStatement ;  } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( auditMaxSize >= 0 && auditSize > auditMaxSize ) { LOG . warn ( "audit record still too long: entityType={}, guid={}, size={}; maxSize={}. audit will have only summary details" , entity . getTypeName ( ) , entity . getId ( ) . _getId ( ) , auditSize , auditMaxSize ) ; Referenceable shallowEntity = new Referenceable ( entity . getId ( ) , entity . getTypeName ( ) , null , entity . getSystemAttributes ( ) , null , null ) ; auditString = auditPrefix + AtlasType . toJson ( shallowEntity ) ; } }
public void test() { try { prepareConfigFile ( ) ; StringBuilder apiProxyUrl = new StringBuilder ( ) ; apiProxyUrl . append ( "http://localhost:" ) . append ( getProxyPort ( ) ) . append ( "/v1-api-interceptor/reload" ) ; Request . Post ( apiProxyUrl . toString ( ) ) . execute ( ) ; } catch ( IOException e ) { log . error ( "Failed to reload api proxy service: {}" , e . getMessage ( ) ) ; } }
public void test() { try { LoadModel < CatalogModel > loadModel = new LoadModel < > ( CatalogModel . class ) ; loadModel = getCommandService ( ) . executeCommand ( loadModel ) ; model = loadModel . getModel ( ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Error loading the CatalogModel" , e ) ; throw new RuntimeException ( "Error loading the CatalogModel" , e ) ; } }
public void test() { if ( highWatermarkUpdateOffset > currentHighWatermarkMetadata . offset || ( highWatermarkUpdateOffset == currentHighWatermarkMetadata . offset && ! highWatermarkUpdateMetadata . metadata . equals ( currentHighWatermarkMetadata . metadata ) ) ) { highWatermark = highWatermarkUpdateOpt ; return true ; } else-if ( highWatermarkUpdateOffset < currentHighWatermarkMetadata . offset ) { log . error ( "The latest computed high watermark {} is smaller than the current " + "value {}, which suggests that one of the voters has lost committed data. " + "Full voter replication state: {}" , highWatermarkUpdateOffset , currentHighWatermarkMetadata . offset , voterStates . values ( ) ) ; return false ; } else { return false ; } }
private Product writeProductFile ( Product targetProduct , ProductFormatter productFormatter , Mapper . Context context , Configuration jobConfig , String outputFormat , ProgressMonitor pm ) throws IOException { long t0 = System . currentTimeMillis ( ) ; Map < String , Object > bandSubsetParameter = createBandSubsetParameter ( targetProduct , jobConfig ) ; code_block = IfStatement ; File productFile = productFormatter . createTemporaryProductFile ( ) ; LOG . info ( "Start writing product to file: " + productFile . getName ( ) ) ; GPF . writeProduct ( targetProduct , productFile , outputFormat , false , pm ) ; LOG . info ( "formatting done in [ms]: " + ( System . currentTimeMillis ( ) - t0 ) ) ; t0 = System . currentTimeMillis ( ) ; context . setStatus ( "Copying" ) ; productFormatter . compressToHDFS ( context , productFile ) ; context . getCounter ( COUNTER_GROUP_NAME_PRODUCTS , "Product formatted" ) . increment ( 1 ) ; LOG . info ( "archiving done in [ms]: " + ( System . currentTimeMillis ( ) - t0 ) ) ; return targetProduct ; }
private Product writeProductFile ( Product targetProduct , ProductFormatter productFormatter , Mapper . Context context , Configuration jobConfig , String outputFormat , ProgressMonitor pm ) throws IOException { long t0 = System . currentTimeMillis ( ) ; Map < String , Object > bandSubsetParameter = createBandSubsetParameter ( targetProduct , jobConfig ) ; code_block = IfStatement ; File productFile = productFormatter . createTemporaryProductFile ( ) ; LOG . info ( "Start writing product to file: " + productFile . getName ( ) ) ; GPF . writeProduct ( targetProduct , productFile , outputFormat , false , pm ) ; LOG . info ( "formatting done in [ms]: " + ( System . currentTimeMillis ( ) - t0 ) ) ; t0 = System . currentTimeMillis ( ) ; context . setStatus ( "Copying" ) ; productFormatter . compressToHDFS ( context , productFile ) ; context . getCounter ( COUNTER_GROUP_NAME_PRODUCTS , "Product formatted" ) . increment ( 1 ) ; LOG . info ( "archiving done in [ms]: " + ( System . currentTimeMillis ( ) - t0 ) ) ; return targetProduct ; }
private Product writeProductFile ( Product targetProduct , ProductFormatter productFormatter , Mapper . Context context , Configuration jobConfig , String outputFormat , ProgressMonitor pm ) throws IOException { long t0 = System . currentTimeMillis ( ) ; Map < String , Object > bandSubsetParameter = createBandSubsetParameter ( targetProduct , jobConfig ) ; code_block = IfStatement ; File productFile = productFormatter . createTemporaryProductFile ( ) ; LOG . info ( "Start writing product to file: " + productFile . getName ( ) ) ; GPF . writeProduct ( targetProduct , productFile , outputFormat , false , pm ) ; LOG . info ( "formatting done in [ms]: " + ( System . currentTimeMillis ( ) - t0 ) ) ; t0 = System . currentTimeMillis ( ) ; context . setStatus ( "Copying" ) ; productFormatter . compressToHDFS ( context , productFile ) ; context . getCounter ( COUNTER_GROUP_NAME_PRODUCTS , "Product formatted" ) . increment ( 1 ) ; LOG . info ( "archiving done in [ms]: " + ( System . currentTimeMillis ( ) - t0 ) ) ; return targetProduct ; }
public void handleGetDataResponse ( final GetDataResponseDto dataResponseDto , final CorrelationIds ids , final String messageType , final ResponseMessageResultType responseMessageResultType , final OsgpException osgpException ) { LOGGER . info ( "handleResponse for MessageType: {}" , messageType ) ; ResponseMessageResultType result = ResponseMessageResultType . OK ; GetDataResponse dataResponse = null ; OsgpException exception = null ; code_block = TryStatement ;  String actualCorrelationUid = ids . getCorrelationUid ( ) ; code_block = IfStatement ; final ResponseMessage responseMessage = ResponseMessage . newResponseMessageBuilder ( ) . withIds ( ids ) . withCorrelationUid ( actualCorrelationUid ) . withMessageType ( messageType ) . withResult ( result ) . withOsgpException ( exception ) . withDataObject ( dataResponse ) . build ( ) ; this . webServiceResponseMessageSender . send ( responseMessage , messageType ) ; }
public void test() { try { code_block = IfStatement ; this . handleResponseMessageReceived ( ids . getDeviceIdentification ( ) ) ; dataResponse = this . mapper . map ( dataResponseDto , GetDataResponse . class ) ; } catch ( final Exception e ) { LOGGER . error ( "Unexpected Exception" , e ) ; result = ResponseMessageResultType . NOT_OK ; exception = this . ensureOsgpException ( e , "Exception occurred while getting data" ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Free slot {}." , taskSlot , cause ) ; } else { LOG . info ( "Free slot {}." , taskSlot ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Free slot {}." , taskSlot , cause ) ; } else { LOG . info ( "Free slot {}." , taskSlot ) ; } }
@ Override public void start ( ) { logger . debug ( "Starting KeepLatestContainerOnlyPolicy policy..." ) ; TimeUnit timeUnit = TimeUnit . valueOf ( System . getProperty ( INTERVAL_TIME_UNIT , TimeUnit . MILLISECONDS . toString ( ) ) ) ; long givenInterval = Long . parseLong ( System . getProperty ( INTERVAL_VALUE , "0" ) ) ; code_block = IfStatement ; logger . debug ( "Started {} policy" , this ) ; }
public void test() { try { TTransport transport = new TFastFramedTransport ( new TSocket ( host , port ) ) ; transport . open ( ) ; TProtocol protocol = new TCompactProtocol ( transport ) ; log . info ( "Former leader was reachable at " + host + ":" + port ) ; return new OracleService . Client ( protocol ) ; } catch ( TTransportException e ) { log . debug ( "Exception thrown in getOracleClient()" , e ) ; } catch ( Exception e ) { throw new RuntimeException ( e ) ; } }
public void test() { try { TTransport transport = new TFastFramedTransport ( new TSocket ( host , port ) ) ; transport . open ( ) ; TProtocol protocol = new TCompactProtocol ( transport ) ; log . info ( "Former leader was reachable at " + host + ":" + port ) ; return new OracleService . Client ( protocol ) ; } catch ( TTransportException e ) { log . debug ( "Exception thrown in getOracleClient()" , e ) ; } catch ( Exception e ) { throw new RuntimeException ( e ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "[IRAC] Topology Updated. Checking pending keys." ) ; } }
public void test() { if ( ! responseMessage . getBooleanValue ( "checked" ) ) { LOG . error ( "allocation {} scheduleJob response error. responseMessage:{}" , analyzeInstance . toString ( ) , responseMessage . toJSONString ( ) ) ; deleteResult ( rdbAnalyze , scheduleID ) ; } else { status = Boolean . TRUE ; } }
public void test() { try { ResponseEntity < String > executeResult = restTemplate . postForEntity ( url , httpEntity , String . class ) ; JSONObject responseMessage = JSONObject . parseObject ( executeResult . getBody ( ) ) ; code_block = IfStatement ; } catch ( Exception e ) { LOG . error ( "allocation {} scheduleJob fail. " , analyzeInstance . toString ( ) , e ) ; deleteResult ( rdbAnalyze , scheduleID ) ; } }
@ PostConstruct public void postConstruct ( ) { LOGGER . debug ( "init vitam tenant is mandatory : {}" , mandatory ) ; this . contractResources = Map . of ( HOLDING_ACCESS_CONTRACT_NAME , holdingAccessContract , HOLDING_INGEST_CONTRACT_NAME , holdingIngestContract , LOGBOOK_ACCESS_CONTRACT_NAME , logbookAccessContract , ITEMS_INGEST_CONTRACT_NAME , itemsIngestContract , FULL_ACCESS_CONTRACT_NAME , fullAccessAccessContract ) ; }
public void test() { switch ( this . workerState . getCurrentState ( ) ) { case NEW : code_block = IfStatement ; case INITIALIZING : logger . info ( "{} already initializing." , this . getClass ( ) . getSimpleName ( ) ) ; break ; case STARTED : logger . info ( "{} already started." , this . getClass ( ) . getSimpleName ( ) ) ; break ; case DESTROYING : throw new IllegalStateException ( "Already destroying." ) ; case STOPPED : throw new IllegalStateException ( "Already stopped." ) ; case ILLEGAL_STATE : throw new IllegalStateException ( "Invalid State." ) ; } }
public void test() { switch ( this . workerState . getCurrentState ( ) ) { case NEW : code_block = IfStatement ; case INITIALIZING : logger . info ( "{} already initializing." , this . getClass ( ) . getSimpleName ( ) ) ; break ; case STARTED : logger . info ( "{} already started." , this . getClass ( ) . getSimpleName ( ) ) ; break ; case DESTROYING : throw new IllegalStateException ( "Already destroying." ) ; case STOPPED : throw new IllegalStateException ( "Already stopped." ) ; case ILLEGAL_STATE : throw new IllegalStateException ( "Invalid State." ) ; } }
public void test() { try { completionScriptText = text . substring ( 0 , cursor ) ; } catch ( Exception e ) { LOGGER . error ( e . toString ( ) ) ; return null ; } }
public void test() { try { code_block = IfStatement ; mContext . setError ( error ) ; cleanupRequest ( mContext ) ; } catch ( Exception e ) { LOG . warn ( "Failed to cleanup states with error {}." , e . toString ( ) ) ; } finally { replyError ( ) ; } }
public boolean isLinux ( ) { String guestid = configSpec . getGuestId ( ) ; boolean isLinux = guestid . startsWith ( "cent" ) || guestid . startsWith ( "debian" ) || guestid . startsWith ( "freebsd" ) || guestid . startsWith ( "oracle" ) || guestid . startsWith ( "other24xLinux" ) || guestid . startsWith ( "other26xLinux" ) || guestid . startsWith ( "otherLinux" ) || guestid . startsWith ( "redhat" ) || guestid . startsWith ( "rhel" ) || guestid . startsWith ( "sles" ) || guestid . startsWith ( "suse" ) || guestid . startsWith ( "ubuntu" ) ; LOG . debug ( "instanceName: " + instanceName + " isLinux: " + isLinux + " guestid: " + configSpec . getGuestId ( ) + " OS: " + configSpec . getGuestFullName ( ) ) ; return isLinux ; }
public SignInPage clickSignInExpectError ( ) { log . info ( "Click Sign In" ) ; clickElement ( signInButton ) ; return new SignInPage ( getDriver ( ) ) ; }
@ Override public void debug ( final String format , final Object arg1 , final Object arg2 ) { log . debug ( format , arg1 , arg2 ) ; }
public void test() { try { boolean hasTenant = hasTenant ( tenant ) ; Capacity capacity = getCapacity ( group , tenant , hasTenant ) ; code_block = IfStatement ; } catch ( Exception e ) { LOGGER . error ( "[capacityManagement] do4Update " , e ) ; } }
public void test() { if ( Objects . equals ( oldJobManagerConnection . getJobMasterId ( ) , jobMasterGateway . getFencingToken ( ) ) ) { log . debug ( "Ignore JobManager gained leadership message for {} because we are already connected to it." , jobMasterGateway . getFencingToken ( ) ) ; return ; } else { disconnectJobManagerConnection ( oldJobManagerConnection , new Exception ( "Found new job leader for job id " + jobId + '.' ) ) ; } }
public void test() { try { schedulerNG . notifyKvStateUnregistered ( jobId , jobVertexId , keyGroupRange , registrationName ) ; return CompletableFuture . completedFuture ( Acknowledge . get ( ) ) ; } catch ( FlinkJobNotFoundException e ) { log . info ( "Error while receiving notification about key-value state de-registration" , e ) ; return FutureUtils . completedExceptionally ( e ) ; } }
@ Test public void test_00 ( ) throws IOException { Log . debug ( "Test" ) ; String fileName = path ( "testLukas.vcf" ) ; long hashExp = calcHashBufferedReader ( fileName ) ; long hash = calcHash ( fileName ) ; System . out . println ( String . format ( "%016x\t%016x\t%s" , hashExp , hash , fileName ) ) ; Assert . assertEquals ( hashExp , hash ) ; }
public void test() { try { final HSSFWorkbook myWorkBook = createGovermentBodyWorkBook ( ) ; code_block = ForStatement ; myWorkBook . close ( ) ; } catch ( final IOException e ) { LOGGER . warn ( "Problem loading" , e ) ; } }
public void test() { try { Document metsDocument = getMets ( id ) ; LOGGER . debug ( ( ) -> new XMLOutputter ( Format . getPrettyFormat ( ) ) . outputString ( metsDocument ) ) ; return getConverter ( id , metsDocument ) . convert ( ) ; } catch ( IOException | JDOMException | SAXException e ) { throw new MCRException ( e ) ; } }
private void printlnError ( Object o ) { s_logger . error ( "" + o ) ; }
public void test() { try { handleMessage ( in . read ( ) , in ) ; } catch ( IOException ex ) { log . error ( "Error reading message channel" , ex ) ; } }
public void test() { try { return documentStoreService . saveDocumentStream ( spMetadataFile , stream ) ; } catch ( Exception ex ) { log . error ( "Failed to write meta-data file '{}'" , spMetadataFile , ex ) ; } finally { IOUtils . closeQuietly ( stream ) ; } }
public void test() { if ( ioException . getSuppressed ( ) . length >= MAX_COPY_ATTEMPTS ) { LOGGER . warn ( "Unable to copy {} to {} after " + MAX_COPY_ATTEMPTS + " attempts; skipping file" , child , destChild , e ) ; return ; } }
@ Override public void perform ( ) throws Exception { getLogger ( ) . info ( "Performing action: Rolling restarting non-master region servers" ) ; List < ServerName > selectedServers = selectServers ( ) ; getLogger ( ) . info ( "Disabling balancer to make unloading possible" ) ; setBalancer ( false , true ) ; code_block = ForStatement ; getLogger ( ) . info ( "Enabling balancer" ) ; setBalancer ( true , true ) ; }
public void test() { try ( RegionMover rm = new RegionMover . RegionMoverBuilder ( rsName , getConf ( ) ) . ack ( true ) . build ( ) ) { getLogger ( ) . info ( "Unloading {}" , server ) ; rm . unload ( ) ; getLogger ( ) . info ( "Restarting {}" , server ) ; gracefulRestartRs ( server , sleepTime ) ; getLogger ( ) . info ( "Loading {}" , server ) ; rm . load ( ) ; } catch ( Shell . ExitCodeException e ) { getLogger ( ) . info ( "Problem restarting but presume successful; code={}" , e . getExitCode ( ) , e ) ; } }
public void test() { try ( RegionMover rm = new RegionMover . RegionMoverBuilder ( rsName , getConf ( ) ) . ack ( true ) . build ( ) ) { getLogger ( ) . info ( "Unloading {}" , server ) ; rm . unload ( ) ; getLogger ( ) . info ( "Restarting {}" , server ) ; gracefulRestartRs ( server , sleepTime ) ; getLogger ( ) . info ( "Loading {}" , server ) ; rm . load ( ) ; } catch ( Shell . ExitCodeException e ) { getLogger ( ) . info ( "Problem restarting but presume successful; code={}" , e . getExitCode ( ) , e ) ; } }
@ Override public void perform ( ) throws Exception { getLogger ( ) . info ( "Performing action: Rolling restarting non-master region servers" ) ; List < ServerName > selectedServers = selectServers ( ) ; getLogger ( ) . info ( "Disabling balancer to make unloading possible" ) ; setBalancer ( false , true ) ; code_block = ForStatement ; getLogger ( ) . info ( "Enabling balancer" ) ; setBalancer ( true , true ) ; }
public void test() { if ( keyInfo == null ) { LOG . debug ( "key:{} is non-existent parent, permit access to user:{}" , keyName , context . getClientUgi ( ) ) ; return true ; } }
public void test() { if ( keyInfo == null ) { LOG . debug ( "key:{} is non-existent parent, permit access to user:{}" , keyName , context . getClientUgi ( ) ) ; return true ; } }
public void test() { try { code_block = TryStatement ;  } catch ( IOException | IllegalStateException th ) { LOG . trace ( "IGNORED" , th ) ; } finally { ac . complete ( ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "register transaction member {}" , memberId ) ; } }
public void test() { if ( LOG . isWarnEnabled ( ) ) { LOG . warn ( "releasing overwritten transaction member {} / {}." , memberId , old . getMemberId ( ) ) ; } }
@ Override public void resolveCollision ( CompositeMap < IOption , Object > composite , Map < IOption , Object > existing , Map < IOption , Object > added , Collection < IOption > intersect ) { LOGGER . debug ( "resolveCollision: {}, {}, {}, {}" , composite , existing , added , intersect ) ; }
public void test() { try { createGUI ( ) ; LOGGER . info ( "GUI created" ) ; } catch ( Throwable t ) { t . printStackTrace ( ) ; } }
public void test() { try { sqlQueryHelper . testQueriesFromFile ( queryFilePath ) ; } catch ( Exception e ) { LOG . error ( e , "Error while testing" ) ; throw new RuntimeException ( e ) ; } }
private void initiateSort ( String sortId , String source , final String destination ) throws KeeperException , InterruptedException { String work = source + "|" + destination ; new DistributedWorkQueue ( manager . getZooKeeperRoot ( ) + Constants . ZRECOVERY , manager . getConfiguration ( ) ) . addWork ( sortId , work . getBytes ( UTF_8 ) ) ; synchronized ( this ) code_block = "" ; final String path = manager . getZooKeeperRoot ( ) + Constants . ZRECOVERY + "/" + sortId ; log . info ( "Created zookeeper entry {} with data {}" , path , work ) ; }
public void test() { if ( Duration . ZERO . equals ( refreshInterval ) ) { LOG . debug ( "Skipping index set with ZERO refresh interval <{}/{}>" , indexSetTitle , indexSetId ) ; return ; } }
public void test() { if ( ! indexSet . getConfig ( ) . isWritable ( ) ) { LOG . debug ( "Skipping non-writable index set <{}/{}>" , indexSetTitle , indexSetId ) ; return ; } }
public void test() { if ( activeWriteIndex != null ) { LOG . debug ( "Updating index field types for active write index <{}> in index set <{}/{}>" , activeWriteIndex , indexSetTitle , indexSetId ) ; poller . pollIndex ( activeWriteIndex , indexSetId ) . ifPresent ( dbService :: upsert ) ; } else { LOG . warn ( "Active write index for index set \"{}\" ({}) doesn't exist yet" , indexSetTitle , indexSetId ) ; } }
public void test() { try { final String activeWriteIndex = indexSet . getActiveWriteIndex ( ) ; code_block = IfStatement ; } catch ( TooManyAliasesException e ) { LOG . error ( "Couldn't get active write index" , e ) ; } catch ( Exception e ) { LOG . error ( "Couldn't update field types for index set <{}/{}>" , indexSetTitle , indexSetId , e ) ; } }
public void test() { try { final String activeWriteIndex = indexSet . getActiveWriteIndex ( ) ; code_block = IfStatement ; } catch ( TooManyAliasesException e ) { LOG . error ( "Couldn't get active write index" , e ) ; } catch ( Exception e ) { LOG . error ( "Couldn't update field types for index set <{}/{}>" , indexSetTitle , indexSetId , e ) ; } }
public void test() { try { code_block = WhileStatement ; } catch ( Throwable e ) { LOGGER . error ( "An error have been encountered while watching resources - leaving the redeploy mode" , e ) ; close ( ) ; } }
public void test() { if ( isDebug ) { logger . debug ( "Skip async servlet request event. isAsyncStarted={}, dispatcherType={}" , request . isAsyncStarted ( ) , request . getDispatcherType ( ) ) ; } }
public void test() { if ( isInfo ) { logger . info ( "Failed to servlet request event handle." , t ) ; } }
public WebContentModuleConfig findByModuleId ( final Long moduleId ) { log . debug ( "findByModuleId() - moduleId: {}" , moduleId ) ; return webContentModuleConfigRepository . findByModuleId ( moduleId ) ; }
public void test() { try { baos = new ByteArrayOutputStream ( ) ; ObjectOutputStream oos = new ObjectOutputStream ( baos ) ; oos . writeObject ( object ) ; oos . close ( ) ; return baos . toByteArray ( ) ; } catch ( IOException e ) { log . error ( "IO exception, Caused by {}." , e ) ; throw new PropertyAccessException ( e ) ; } }
@ Override public void delete ( final LogicalDatastoreType store , final YangInstanceIdentifier path ) { checkOpen ( ) ; LOG . debug ( "{}: Delete {} {}" , id , store , path ) ; processTransactionOperation ( facade -> facade . delete ( store , path ) ) ; }
public void test() { if ( command instanceof RefreshType ) { client . refreshChannel ( ) ; updateState ( channelUID , new StringType ( client . getChannel ( ) ) ) ; } else-if ( command instanceof StringType ) { client . setChannel ( command . toString ( ) ) ; } else { logger . info ( "Channel {} only accepts StringType, RefreshType. Type was {}." , channelUID , command . getClass ( ) ) ; } }
public void test() { try { helper . mergeTokenIntoJobConf ( jobConf , token ) ; } catch ( IOException e ) { log . info ( "Ignoring exception, likely coming from Hadoop 1" , e ) ; return ; } }
public void test() { if ( getStreamingURLforCurrentOrg ( ) == null ) { logger . warn ( String . format ( "Trying to distribute to streaming from tenant where streaming url or port aren't set." , securityService . getOrganization ( ) . getId ( ) ) ) ; return Collections . emptyList ( ) ; } }
public void test() { if ( distributionDirectory == null ) { logger . warn ( "Streaming distribution directory isn't set (org.opencastproject.streaming.directory)" ) ; return Collections . emptyList ( ) ; } }
public void activate ( final String userHashKey ) { log . info ( "Activation user with hash key {}" , userHashKey ) ; final User user = userRepository . findByHashKey ( userHashKey ) ; code_block = IfStatement ; user . activate ( ) ; userRepository . save ( user ) ; }
public void test() { if ( ! partitionBase . renameTo ( new File ( partitionBase . getParent ( ) , partitionBase . getName ( ) + ".d." + SystemClock . now ( ) ) ) ) { logger . warn ( "Rename directory {} failed!" , partitionBase . getAbsolutePath ( ) ) ; } }
protected void onLinkDeletePost ( final String networkId , final Link link , final HashMap < String , Response > respList ) { log . debug ( "" ) ; }
public void test() { try { return ResponseUtils . buildSucessResponse ( targetTypesService . getTargetTypesByName ( targetTypeName ) ) ; } catch ( Exception exception ) { log . error ( UNEXPECTED_ERROR_OCCURRED , exception ) ; return ResponseUtils . buildFailureResponse ( new Exception ( UNEXPECTED_ERROR_OCCURRED ) , exception . getMessage ( ) ) ; } }
public void attachDirty ( SysExportBau instance ) { log . debug ( "attaching dirty SysExportBau instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { propertyService . setAsString ( PropertyIdEditPopup . this . getModelObject ( ) , valueModel . getObject ( ) ) ; Session . get ( ) . success ( getString ( "common.propertyId.action.edit.success" ) ) ; closePopup ( target ) ; target . addChildren ( getPage ( ) , PropertyIdListPanel . class ) ; } catch ( Exception e ) { LOGGER . error ( "Erreur lors la modification de la valeur d'une propriÃ©tÃ©." ) ; Session . get ( ) . error ( getString ( "common.error.unexpected" ) ) ; } }
public void test() { else { log . warn ( "Spec " + this + " ignoring unknown config key " + entry . getKey ( ) ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( WikiPageServiceUtil . class , "updatePage" , _updatePageParameterTypes47 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , nodeId , title , version , content , summary , minorEdit , format , parentTitle , redirectTitle , serviceContext ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . wiki . model . WikiPage ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { com . liferay . portal . kernel . model . Address returnValue = AddressServiceUtil . getAddress ( addressId ) ; return com . liferay . portal . kernel . model . AddressSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( checksumFile . lastModified ( ) > lastModifiedChecksumFile ) { log . warn ( "Archive in memory out of sync with archive in file." ) ; checksumArchive . clear ( ) ; loadFile ( ) ; } }
public void test() { if ( findHandler ( spec . prefix ) != null ) { log . error ( "Request handler for path specification with prefix {} is already registered" , spec . prefix ) ; } else { res = true ; HandlerRecord hrec = new HandlerRecord ( ) ; hrec . pathSpec = pathSpec ; hrec . handler = handler ; hrec . rqMapping = mapper . constructType ( rqMapping ) ; hrec . respMapping = mapper . constructType ( respMapping ) ; rqHandlers . putIfAbsent ( spec , hrec ) ; } }
public void test() { if ( debugEnabled ) { logger . debug ( "tryAfter() returns false: interceptorScopeTransaction: {}, executionPoint: {}. Skip interceptor {}" , transaction , policy , interceptor . getClass ( ) ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Testing Journal Rolling" ) ; } }
public void test() { if ( log . isInfoEnabled ( ) ) { log . info ( "Publishing application instance active event: [application] " + appId + " [instance] " + instanceId ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( LayoutSetPrototypeServiceUtil . class , "fetchLayoutSetPrototype" , _fetchLayoutSetPrototypeParameterTypes5 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , layoutSetPrototypeId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . portal . kernel . model . LayoutSetPrototype ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
@ Override protected void processEmptyMessage ( ) throws Exception { Exchange exchange = getEndpoint ( ) . createExchange ( ) ; exchange . setProperty ( ExchangePropertyKey . BATCH_INDEX , 0 ) ; exchange . setProperty ( ExchangePropertyKey . BATCH_SIZE , 1 ) ; exchange . setProperty ( ExchangePropertyKey . BATCH_COMPLETE , true ) ; LOG . debug ( "Sending empty message as there were no messages from polling: {}" , this . getEndpoint ( ) ) ; getProcessor ( ) . process ( exchange ) ; }
public void test() { try { dumpFileDirectories = this . dumpfileDirectoryManager . getSubdirectories ( directoryPattern ) ; } catch ( IOException e ) { logger . error ( "Unable to access dump directory: " + e . toString ( ) ) ; return Collections . emptyList ( ) ; } }
public void test() { if ( dumpFile . isAvailable ( ) ) { result . add ( dumpFile ) ; } else { logger . error ( "Incomplete local dump file data. Maybe delete " + dumpFile . getDumpfileDirectory ( ) + " to attempt fresh download." ) ; } }
public SysDatadict findById ( sernet . gs . reveng . SysDatadictId id ) { log . debug ( "getting SysDatadict instance with id: " + id ) ; code_block = TryStatement ;  }
public void test() { if ( instance == null ) { log . debug ( "get successful, no instance found" ) ; } else { log . debug ( "get successful, instance found" ) ; } }
public void test() { if ( instance == null ) { log . debug ( "get successful, no instance found" ) ; } else { log . debug ( "get successful, instance found" ) ; } }
public void test() { try { SysDatadict instance = ( SysDatadict ) sessionFactory . getCurrentSession ( ) . get ( "sernet.gs.reveng.SysDatadict" , id ) ; code_block = IfStatement ; return instance ; } catch ( RuntimeException re ) { log . error ( "get failed" , re ) ; throw re ; } }
public void test() { if ( protonTransportErrorHandled ) { LOG . trace ( "Skipping data processing, proton transport previously errored." ) ; return ; } }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; do code_block = "" ; while ( input . isReadable ( ) ) ; processUpdates ( ) ; pumpToProtonTransport ( ) ; } catch ( Throwable t ) { LOG . warn ( "Caught problem during data processing: {}" , t . getMessage ( ) , t ) ; fireProviderException ( ProviderExceptionSupport . createOrPassthroughFatal ( t ) ) ; } }
public void test() { try { Configuration conf = new Configuration ( ) ; conf . setBoolean ( "mapred.mapper.new-api" , true ) ; conf . setBoolean ( AngelConf . ANGEL_JOB_OUTPUT_PATH_DELETEONEXIST , true ) ; conf . set ( AngelConf . ANGEL_TASK_USER_TASKCLASS , DummyTask . class . getName ( ) ) ; conf . set ( AngelConf . ANGEL_DEPLOY_MODE , "LOCAL" ) ; conf . setBoolean ( AngelConf . ANGEL_AM_USE_DUMMY_DATASPLITER , true ) ; conf . set ( AngelConf . ANGEL_INPUTFORMAT_CLASS , CombineTextInputFormat . class . getName ( ) ) ; conf . set ( AngelConf . ANGEL_SAVE_MODEL_PATH , LOCAL_FS + TMP_PATH + "/out" ) ; conf . set ( AngelConf . ANGEL_TRAIN_DATA_PATH , LOCAL_FS + TMP_PATH + "/in" ) ; conf . set ( AngelConf . ANGEL_LOG_PATH , LOCAL_FS + TMP_PATH + "/log" ) ; conf . setInt ( AngelConf . ANGEL_WORKERGROUP_NUMBER , 1 ) ; conf . setInt ( AngelConf . ANGEL_PS_NUMBER , 1 ) ; conf . setInt ( AngelConf . ANGEL_WORKER_TASK_NUMBER , 2 ) ; conf . setInt ( AngelConf . ANGEL_WORKER_HEARTBEAT_INTERVAL_MS , 1000 ) ; conf . setInt ( AngelConf . ANGEL_PS_HEARTBEAT_INTERVAL_MS , 1000 ) ; angelClient = AngelClientFactory . get ( conf ) ; MatrixContext mMatrix = new MatrixContext ( ) ; mMatrix . setName ( "w1" ) ; mMatrix . setRowNum ( 1 ) ; mMatrix . setColNum ( 100000 ) ; mMatrix . setMaxRowNumInBlock ( 1 ) ; mMatrix . setMaxColNumInBlock ( 50000 ) ; mMatrix . setRowType ( RowType . T_INT_DENSE ) ; mMatrix . set ( MatrixConf . MATRIX_OPLOG_ENABLEFILTER , "false" ) ; mMatrix . set ( MatrixConf . MATRIX_HOGWILD , "true" ) ; mMatrix . set ( MatrixConf . MATRIX_AVERAGE , "false" ) ; mMatrix . set ( MatrixConf . MATRIX_OPLOG_TYPE , "DENSE_INT" ) ; angelClient . addMatrix ( mMatrix ) ; MatrixContext mMatrix2 = new MatrixContext ( ) ; mMatrix2 . setName ( "w2" ) ; mMatrix2 . setRowNum ( 1 ) ; mMatrix2 . setColNum ( 100000 ) ; mMatrix2 . setMaxRowNumInBlock ( 1 ) ; mMatrix2 . setMaxColNumInBlock ( 50000 ) ; mMatrix2 . setRowType ( RowType . T_DOUBLE_DENSE ) ; mMatrix2 . set ( MatrixConf . MATRIX_OPLOG_ENABLEFILTER , "false" ) ; mMatrix2 . set ( MatrixConf . MATRIX_HOGWILD , "false" ) ; mMatrix2 . set ( MatrixConf . MATRIX_AVERAGE , "false" ) ; mMatrix2 . set ( MatrixConf . MATRIX_OPLOG_TYPE , "DENSE_DOUBLE" ) ; angelClient . addMatrix ( mMatrix2 ) ; angelClient . startPSServer ( ) ; angelClient . run ( ) ; Thread . sleep ( 5000 ) ; group0Id = new WorkerGroupId ( 0 ) ; worker0Id = new WorkerId ( group0Id , 0 ) ; worker0Attempt0Id = new WorkerAttemptId ( worker0Id , 0 ) ; task0Id = new TaskId ( 0 ) ; task1Id = new TaskId ( 1 ) ; } catch ( Exception x ) { LOG . error ( "setup failed " , x ) ; throw x ; } }
public void test() { { barrier . await ( ) ; Ignite ignite = startGrid ( startIdx . getAndIncrement ( ) ) ; assertTrue ( ignite . configuration ( ) . isClientMode ( ) ) ; log . info ( "Started node: " + ignite . name ( ) ) ; return null ; } }
@ Override protected String transform ( String value ) { String result = PREFIX + URLEncoder . encode ( value , StandardCharsets . UTF_8 ) ; LOG . debug ( "{} -> {}" , value , result ) ; return result ; }
public void test() { try { return Utils . filesystemSafe ( new URI ( getTerm ( url ) . replaceAll ( "&tags=" , "" ) ) . getPath ( ) ) ; } catch ( URISyntaxException ex ) { logger . error ( ex ) ; } }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; } catch ( IOException continued ) { _log . warn ( "File-close threw the exception: " , continued ) ; } }
public static TestSuite runTestSuite ( String testSuiteFolderPath , String sakuliHomeFolderPath , String browser , String sahiHomeFolder ) throws FileNotFoundException { LOGGER . info ( String . format ( "\n\n=========== START new SAKULI Testsuite from '%s' =================" , testSuiteFolderPath ) ) ; String tempLogCache = "" ; tempLogCache = SakuliFolderHelper . checkTestSuiteFolderAndSetContextVariables ( testSuiteFolderPath , tempLogCache ) ; tempLogCache = SakuliFolderHelper . checkSakuliHomeFolderAndSetContextVariables ( sakuliHomeFolderPath , tempLogCache ) ; code_block = IfStatement ; code_block = IfStatement ; SahiConnector sahiConnector = BeanLoader . loadBean ( SahiConnector . class ) ; LOGGER . debug ( tempLogCache ) ; InitializingServiceHelper . invokeInitializingServcies ( ) ; TestSuite result = BeanLoader . loadBean ( TestSuite . class ) ; code_block = TryStatement ;  return result ; }
public static TestSuite runTestSuite ( String testSuiteFolderPath , String sakuliHomeFolderPath , String browser , String sahiHomeFolder ) throws FileNotFoundException { LOGGER . info ( String . format ( "\n\n=========== START new SAKULI Testsuite from '%s' =================" , testSuiteFolderPath ) ) ; String tempLogCache = "" ; tempLogCache = SakuliFolderHelper . checkTestSuiteFolderAndSetContextVariables ( testSuiteFolderPath , tempLogCache ) ; tempLogCache = SakuliFolderHelper . checkSakuliHomeFolderAndSetContextVariables ( sakuliHomeFolderPath , tempLogCache ) ; code_block = IfStatement ; code_block = IfStatement ; SahiConnector sahiConnector = BeanLoader . loadBean ( SahiConnector . class ) ; LOGGER . debug ( tempLogCache ) ; InitializingServiceHelper . invokeInitializingServcies ( ) ; TestSuite result = BeanLoader . loadBean ( TestSuite . class ) ; code_block = TryStatement ;  return result ; }
public void test() { try { sahiConnector . init ( ) ; LOGGER . debug ( "start new sakuli test suite" ) ; sahiConnector . startSahiTestSuite ( ) ; } catch ( SakuliInitException e ) { LOGGER . error ( "Unexpected error occurred:" , e ) ; System . exit ( 99 ) ; } finally { LOGGER . info ( "========== TEAR-DOWN SAKULI TEST SUITE '{}' ==========" , result . getId ( ) ) ; TeardownServiceHelper . invokeTeardownServices ( result , false ) ; result = BeanLoader . loadBean ( TestSuite . class ) ; BeanLoader . releaseContext ( ) ; } }
public void test() { try { sahiConnector . init ( ) ; LOGGER . debug ( "start new sakuli test suite" ) ; sahiConnector . startSahiTestSuite ( ) ; } catch ( SakuliInitException e ) { LOGGER . error ( "Unexpected error occurred:" , e ) ; System . exit ( 99 ) ; } finally { LOGGER . info ( "========== TEAR-DOWN SAKULI TEST SUITE '{}' ==========" , result . getId ( ) ) ; TeardownServiceHelper . invokeTeardownServices ( result , false ) ; result = BeanLoader . loadBean ( TestSuite . class ) ; BeanLoader . releaseContext ( ) ; } }
public void test() { try { sahiConnector . init ( ) ; LOGGER . debug ( "start new sakuli test suite" ) ; sahiConnector . startSahiTestSuite ( ) ; } catch ( SakuliInitException e ) { LOGGER . error ( "Unexpected error occurred:" , e ) ; System . exit ( 99 ) ; } finally { LOGGER . info ( "========== TEAR-DOWN SAKULI TEST SUITE '{}' ==========" , result . getId ( ) ) ; TeardownServiceHelper . invokeTeardownServices ( result , false ) ; result = BeanLoader . loadBean ( TestSuite . class ) ; BeanLoader . releaseContext ( ) ; } }
public void test() { try { result = engine . unwrap ( peerNetData , peerAppData ) ; } catch ( SSLException e ) { log . error ( "SSLException during unwrap" , e ) ; throw e ; } }
public void test() { if ( ! grantedByScopes ) { LOG . warn ( "Difference in auth of user {} for READ, scopes authorizer: {}, groups authorizer: true, user: {}" , user . getId ( ) , grantedByScopes , user ) ; } }
public void test() { if ( ! grantedByScopes ) { LOG . warn ( "Difference in auth of user {} for READ, scopes authorizer: {}, groups authorizer: true, user: {}" , user . getId ( ) , grantedByScopes , user ) ; } }
public void test() { try { StoreClient store = imapClientProvider . getImapClient ( udr ) ; assertMoveItemIsSupported ( store ) ; logger . debug ( "Moving email, USER:{} UIDs:{} SRC:{} DST:{}" , udr . getUser ( ) . getLoginAtDomain ( ) , messages , srcFolder , dstFolder ) ; store . select ( srcFolder . getPath ( ) ) ; MessageSet newUids = store . uidCopy ( messages , dstFolder . getPath ( ) ) ; deleteMessage ( store , messages ) ; return newUids ; } catch ( IMAPException e ) { throw new MailException ( e ) ; } catch ( MailboxNotFoundException e ) { throw new CollectionNotFoundException ( e ) ; } catch ( ImapTimeoutException e ) { throw new TimeoutException ( e ) ; } }
public void test() { try { HttpServletRequest request = ( HttpServletRequest ) serv ; SessionBean session = getSession ( request ) ; code_block = IfStatement ; } catch ( Exception e ) { LOGGER . info ( "We had some exception in Authentication filter" , e ) ; } finally { chain . doFilter ( serv , resp ) ; } }
public void test() { if ( responseFlag == AckSignalFlag . DUPLICATE_REQUEST ) { LOG . info ( "messageReceived: Already completed request (taskId = " + senderId + ", requestId = " + requestId + ")" ) ; } else-if ( responseFlag != AckSignalFlag . NEW_REQUEST ) { throw new IllegalStateException ( "messageReceived: Got illegal response " + response ) ; } }
public void test() { if ( requestInfo == null ) { LOG . info ( "messageReceived: Already received response for (taskId = " + senderId + ", requestId = " + requestId + ")" ) ; } else { code_block = IfStatement ; flowControl . messageAckReceived ( senderId , requestId , response ) ; synchronized ( clientRequestIdRequestInfoMap ) code_block = "" ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "messageReceived: Completed (taskId = " + senderId + ")" + requestInfo + ".  Waiting on " + clientRequestIdRequestInfoMap . size ( ) + " requests" ) ; } }
public void test() { try { Session session = getCurrentSession ( ) ; I id = ( I ) session . save ( e ) ; session . flush ( ) ; session . evict ( e ) ; return id ; } catch ( Exception ex ) { logger . error ( "Save failed" , ex ) ; throw new HibernateException ( "Save failed" ) ; } }
public void test() { if ( cachedHostUuid != null ) { logger . debug ( String . format ( "[Vm Tracer] detects stranger vm[identity:%s, state:%s] but it's already in cache, skip firing event" , vmUuid , actualState ) ) ; return ; } }
public List findByExample ( StgMapMas instance ) { log . debug ( "finding StgMapMas instance by example" ) ; code_block = TryStatement ;  }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.StgMapMas" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.StgMapMas" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
public void test() { try { Query q = em . createNamedQuery ( "CustomAttribute.deleteForOrg" ) ; q . setParameter ( "oid" , organizationId ) ; q . executeUpdate ( ) ; CustomAttribute ca ; code_block = ForStatement ; return provResult . newOkBaseResult ( ) ; } catch ( Exception e ) { logger . warn ( e . getMessage ( ) , e ) ; return provResult . getErrorResult ( BaseResult . class , e , getLocale ( requestingUser ) , null , null ) ; } }
public void test() { try { return em . unwrap ( Session . class ) ; } catch ( Exception e ) { logger . error ( "Error in getCurrentSession()" , e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceAddressServiceUtil . class , "updateCommerceAddress" , _updateCommerceAddressParameterTypes23 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , commerceAddressId , name , description , street1 , street2 , street3 , city , zip , regionId , countryId , phoneNumber , defaultBilling , defaultShipping , serviceContext ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . commerce . model . CommerceAddress ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
ActionResult < Wo > execute ( EffectivePerson effectivePerson , ServletContext servletContext ) throws Exception { ActionResult < Wo > result = new ActionResult < > ( ) ; com . x . base . core . project . Context ctx = ( com . x . base . core . project . Context ) servletContext . getAttribute ( com . x . base . core . project . AbstractContext . class . getName ( ) ) ; logger . info ( "{} change logger level to DEBUG." , ctx . clazz ( ) . getName ( ) ) ; LoggerFactory . setLevelDebug ( ) ; result . setData ( new Wo ( true ) ) ; return result ; }
@ Override public void replaceAtomContainer ( int position , IAtomContainer container ) { logger . debug ( "Replacing atom container at pos: " , position ) ; super . replaceAtomContainer ( position , container ) ; }
public void test() { try { connection = SolrConnection . getConnection ( conf , BOLT_TYPE ) ; } catch ( Exception e ) { LOG . error ( "Can't connect to Solr: {}" , e ) ; throw new RuntimeException ( e ) ; } }
public void test() { if ( log . isInfoEnabled ( ) ) { log . info ( String . format ( "Valid receipt %s in %s" , receipt , file ) ) ; } }
public void test() { try { file . copy ( LocalFactory . get ( support , String . format ( "%s.cyberduckreceipt" , receipt . getName ( ) ) ) ) ; } catch ( AccessDeniedException e ) { log . warn ( e . getMessage ( ) ) ; } }
public void test() { try { streamingSensor . setState ( mode ) ; } catch ( jmri . JmriException ex ) { log . error ( "Exception while setting ISSOUNDSTREAMING sensor {} to {}" , streamingSensor . getDisplayName ( ) , mode ) ; } }
public void test() { try { List < CubeAssignment > cubeAssignmentList = Lists . newArrayList ( ) ; List < String > cubes = client . getChildren ( ) . forPath ( cubeRoot ) ; code_block = ForStatement ; readSuccess . getAndIncrement ( ) ; return cubeAssignmentList ; } catch ( Exception e ) { readFail . getAndIncrement ( ) ; logger . error ( "Error when get assignments" , e ) ; throw new StoreException ( e ) ; } }
public void test() { for ( Logger logger : this . loggers ) { logger . debug ( msg ) ; } }
public void test() { try { Object v = computing . apply ( producer . getAttribute ( sourceSensor ) ) ; code_block = IfStatement ; code_block = IfStatement ; } catch ( Throwable t ) { LOG . warn ( "Error calculating map update for enricher " + this , t ) ; throw Exceptions . propagate ( t ) ; } }
public void test() { switch ( field . getType ( ) ) { case STATE : changeOfStateSubscriptions . values ( ) . stream ( ) . filter ( pair -> pair . getKey ( ) . equals ( field ) ) . map ( Pair :: getValue ) . forEach ( baseDefaultPlcValueConsumer -> baseDefaultPlcValueConsumer . accept ( value ) ) ; state . put ( field , value ) ; return ; case STDOUT : logger . info ( "TEST PLC STDOUT [{}]: {}" , field . getName ( ) , value . getString ( ) ) ; return ; case RANDOM : code_block = SwitchStatement ; logger . info ( "TEST PLC RANDOM [{}]: {}" , field . getName ( ) , value . toString ( ) ) ; return ; } }
public void test() { try { DataItemIO . staticSerialize ( value , field . getPlcDataType ( ) , field . getNumberOfElements ( ) , false ) ; } catch ( ParseException e ) { logger . info ( "Write failed" ) ; } }
public void test() { switch ( field . getType ( ) ) { case STATE : changeOfStateSubscriptions . values ( ) . stream ( ) . filter ( pair -> pair . getKey ( ) . equals ( field ) ) . map ( Pair :: getValue ) . forEach ( baseDefaultPlcValueConsumer -> baseDefaultPlcValueConsumer . accept ( value ) ) ; state . put ( field , value ) ; return ; case STDOUT : logger . info ( "TEST PLC STDOUT [{}]: {}" , field . getName ( ) , value . getString ( ) ) ; return ; case RANDOM : code_block = SwitchStatement ; logger . info ( "TEST PLC RANDOM [{}]: {}" , field . getName ( ) , value . toString ( ) ) ; return ; } }
public void test() { try { String varName = actionParameter . getColumn ( ) ; code_block = IfStatement ; } catch ( EcmaError e ) { LOGGER . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { fileStore . delete ( fileMeta . getId ( ) ) ; } catch ( UncheckedIOException e ) { LOG . warn ( "Could not delete file '{}' from file store" , fileMeta . getId ( ) ) ; } }
public void debug ( String arg0 , Object arg1 , Object arg2 ) { log . debug ( format ( arg0 , arg1 , arg2 ) ) ; }
public void test() { if ( ! groupsWithAggregation . contains ( group ) ) { LOGGER . info ( "Sorting data for group {} and id {} ({} input files, results will be stored in {})" , group , id , files . size ( ) , sortedFiles ) ; new SortGroupSplit ( fs , sparkSession , schemaUtils . columnsToSortBy ( group , reversed ) , files , sortedFiles , compressionCodecName ) . call ( ) ; } else { final String aggregatedFiles = outputDir + AGGREGATED ; LOGGER . info ( "Aggregating data for group {} and id {} ({} input files, results will be stored in {})" , group , id , files . size ( ) , aggregatedFiles ) ; final CallableResult result = new AggregateDataForGroup ( fs , schemaUtils , group , files , aggregatedFiles , sparkSession ) . call ( ) ; code_block = IfStatement ; } }
public void test() { if ( ! groupsWithAggregation . contains ( group ) ) { LOGGER . info ( "Sorting data for group {} and id {} ({} input files, results will be stored in {})" , group , id , files . size ( ) , sortedFiles ) ; new SortGroupSplit ( fs , sparkSession , schemaUtils . columnsToSortBy ( group , reversed ) , files , sortedFiles , compressionCodecName ) . call ( ) ; } else { final String aggregatedFiles = outputDir + AGGREGATED ; LOGGER . info ( "Aggregating data for group {} and id {} ({} input files, results will be stored in {})" , group , id , files . size ( ) , aggregatedFiles ) ; final CallableResult result = new AggregateDataForGroup ( fs , schemaUtils , group , files , aggregatedFiles , sparkSession ) . call ( ) ; code_block = IfStatement ; } }
public void test() { if ( fs . exists ( new Path ( sortedFiles ) ) ) { LOGGER . info ( "Moving files of sorted data from {} to {} (group {}, id {})" , sortedFiles , outputDir , group , id ) ; final FileStatus [ ] files = fs . listStatus ( new Path ( sortedFiles ) ) ; code_block = ForStatement ; } else { LOGGER . info ( "No files of sorted data so there is nothing to move" ) ; } }
public void test() { if ( fs . exists ( new Path ( sortedFiles ) ) ) { LOGGER . info ( "Moving files of sorted data from {} to {} (group {}, id {})" , sortedFiles , outputDir , group , id ) ; final FileStatus [ ] files = fs . listStatus ( new Path ( sortedFiles ) ) ; code_block = ForStatement ; } else { LOGGER . info ( "No files of sorted data so there is nothing to move" ) ; } }
public void test() { if ( fs . exists ( new Path ( sortedFiles ) ) ) { LOGGER . info ( "Moving files of sorted data from {} to {} (group {}, id {})" , sortedFiles , outputDir , group , id ) ; final FileStatus [ ] files = fs . listStatus ( new Path ( sortedFiles ) ) ; code_block = ForStatement ; } else { LOGGER . info ( "No files of sorted data so there is nothing to move" ) ; } }
public void test() { if ( oVal . toUpperCase ( ) . endsWith ( "UNDEF" ) || oVal . toUpperCase ( Locale . ROOT ) . endsWith ( "UNSET" ) ) { String removed = result . remove ( oKey ) ; logger . trace ( "Removed key '" + oKey + "': '" + removed + "' from script params because it was " + oVal + " in overrides" ) ; } else { String was = result . get ( oKey ) ; was = ( was == null ? "NULL" : was ) ; result . put ( oKey , oVal ) ; logger . trace ( "Overrode key '" + oKey + "': from '" + was + " to " + oVal ) ; } }
public void test() { if ( oVal . toUpperCase ( ) . endsWith ( "UNDEF" ) || oVal . toUpperCase ( Locale . ROOT ) . endsWith ( "UNSET" ) ) { String removed = result . remove ( oKey ) ; logger . trace ( "Removed key '" + oKey + "': '" + removed + "' from script params because it was " + oVal + " in overrides" ) ; } else { String was = result . get ( oKey ) ; was = ( was == null ? "NULL" : was ) ; result . put ( oKey , oVal ) ; logger . trace ( "Overrode key '" + oKey + "': from '" + was + " to " + oVal ) ; } }
public void test() { if ( ret . getItem ( ) != null ) { long insertedQueueId = ret . getItem ( ) . getId ( ) ; AnswerItem < Integer > retDep = testCaseExecutionQueueDepService . insertFromExeQueueIdDep ( insertedQueueId , exeQueueId ) ; LOG . debug ( "Dep inserted from old entries : " + retDep . getItem ( ) ) ; } }
public void test() { try { final String filename = mConfig . OutputDir + REF_FILE_SIG_PERC ; mRefDataWriter = createBufferedWriter ( filename , false ) ; mRefDataWriter . write ( "CancerType,DataType" ) ; code_block = ForStatement ; mRefDataWriter . newLine ( ) ; } catch ( IOException e ) { CUP_LOGGER . error ( "failed to write signatures ref data output: {}" , e . toString ( ) ) ; } }
public void test() { if ( _Employee . LOG . isDebugEnabled ( ) ) { _Employee . LOG . debug ( "updating lastName from " + lastName ( ) + " to " + value ) ; } }
public void test() { if ( encryptionKey != null ) { idpDescriptor . getKeyDescriptors ( ) . add ( getKeyDescriptor ( UsageType . ENCRYPTION , getServerKeyInfo ( encryptionKey ) ) ) ; } else { log . info ( "Generating metadata without encryption key, KeyStore doesn't contain any default private key, or the encryptionKey specified in ExtendedMetadata cannot be found" ) ; } }
public void test() { if ( encryptionKey != null ) { idpDescriptor . getKeyDescriptors ( ) . add ( getKeyDescriptor ( UsageType . ENCRYPTION , getServerKeyInfo ( encryptionKey ) ) ) ; } else { log . info ( "Generating metadata without encryption key, KeyStore doesn't contain any default private key, or the encryptionKey specified in ExtendedMetadata cannot be found" ) ; } }
private boolean testForDuplicateSize ( SearchResultItem result1 , SearchResultItem result2 , float duplicateSizeDifference ) { code_block = IfStatement ; long sizeDifference = Math . abs ( result1 . getSize ( ) - result2 . getSize ( ) ) ; float sizeAverage = ( result1 . getSize ( ) + result2 . getSize ( ) ) / 2F ; float sizeDiffPercent = Math . abs ( sizeDifference / sizeAverage ) * 100 ; boolean sameSize = sizeDiffPercent <= duplicateSizeDifference ; logger . debug ( LoggingMarkers . DUPLICATES , "Same size: {}" , sameSize ) ; return sameSize ; }
@ Override public void replicateCollectionAsynchronously ( final String irodsCollectionAbsolutePath , final String resourceName , final int delayInMinutes ) throws JargonException { log . info ( "replicateCollectionAsynchronously()" ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "irodsCollectionAbsolutePath:{}" , irodsCollectionAbsolutePath ) ; log . info ( "resourceName:{}" , resourceName ) ; log . info ( "delayInMinutes:{}" , delayInMinutes ) ; code_block = IfStatement ; RuleProcessingAO ruleProcessingAO = getIRODSAccessObjectFactory ( ) . getRuleProcessingAO ( getIRODSAccount ( ) ) ; List < IRODSRuleParameter > irodsRuleParameters = new ArrayList < IRODSRuleParameter > ( ) ; irodsRuleParameters . add ( new IRODSRuleParameter ( "*SourceFile" , MiscIRODSUtils . wrapStringInQuotes ( irodsCollectionAbsolutePath ) ) ) ; irodsRuleParameters . add ( new IRODSRuleParameter ( "*Resource" , MiscIRODSUtils . wrapStringInQuotes ( resourceName ) ) ) ; irodsRuleParameters . add ( new IRODSRuleParameter ( "*DelayInfo" , RuleUtils . buildDelayParamForMinutes ( delayInMinutes ) ) ) ; RuleInvocationConfiguration ruleInvocationConfiguration = RuleInvocationConfiguration . instanceWithDefaultAutoSettings ( getJargonProperties ( ) ) ; ruleInvocationConfiguration . setRuleProcessingType ( RuleProcessingType . EXTERNAL ) ; IRODSRuleExecResult result = ruleProcessingAO . executeRuleFromResource ( "/rules/rulemsiCollReplAsync.r" , irodsRuleParameters , ruleInvocationConfiguration ) ; log . info ( "result of action:{}" , result . getRuleExecOut ( ) . trim ( ) ) ; }
@ Override public void replicateCollectionAsynchronously ( final String irodsCollectionAbsolutePath , final String resourceName , final int delayInMinutes ) throws JargonException { log . info ( "replicateCollectionAsynchronously()" ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "irodsCollectionAbsolutePath:{}" , irodsCollectionAbsolutePath ) ; log . info ( "resourceName:{}" , resourceName ) ; log . info ( "delayInMinutes:{}" , delayInMinutes ) ; code_block = IfStatement ; RuleProcessingAO ruleProcessingAO = getIRODSAccessObjectFactory ( ) . getRuleProcessingAO ( getIRODSAccount ( ) ) ; List < IRODSRuleParameter > irodsRuleParameters = new ArrayList < IRODSRuleParameter > ( ) ; irodsRuleParameters . add ( new IRODSRuleParameter ( "*SourceFile" , MiscIRODSUtils . wrapStringInQuotes ( irodsCollectionAbsolutePath ) ) ) ; irodsRuleParameters . add ( new IRODSRuleParameter ( "*Resource" , MiscIRODSUtils . wrapStringInQuotes ( resourceName ) ) ) ; irodsRuleParameters . add ( new IRODSRuleParameter ( "*DelayInfo" , RuleUtils . buildDelayParamForMinutes ( delayInMinutes ) ) ) ; RuleInvocationConfiguration ruleInvocationConfiguration = RuleInvocationConfiguration . instanceWithDefaultAutoSettings ( getJargonProperties ( ) ) ; ruleInvocationConfiguration . setRuleProcessingType ( RuleProcessingType . EXTERNAL ) ; IRODSRuleExecResult result = ruleProcessingAO . executeRuleFromResource ( "/rules/rulemsiCollReplAsync.r" , irodsRuleParameters , ruleInvocationConfiguration ) ; log . info ( "result of action:{}" , result . getRuleExecOut ( ) . trim ( ) ) ; }
@ Override public void replicateCollectionAsynchronously ( final String irodsCollectionAbsolutePath , final String resourceName , final int delayInMinutes ) throws JargonException { log . info ( "replicateCollectionAsynchronously()" ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "irodsCollectionAbsolutePath:{}" , irodsCollectionAbsolutePath ) ; log . info ( "resourceName:{}" , resourceName ) ; log . info ( "delayInMinutes:{}" , delayInMinutes ) ; code_block = IfStatement ; RuleProcessingAO ruleProcessingAO = getIRODSAccessObjectFactory ( ) . getRuleProcessingAO ( getIRODSAccount ( ) ) ; List < IRODSRuleParameter > irodsRuleParameters = new ArrayList < IRODSRuleParameter > ( ) ; irodsRuleParameters . add ( new IRODSRuleParameter ( "*SourceFile" , MiscIRODSUtils . wrapStringInQuotes ( irodsCollectionAbsolutePath ) ) ) ; irodsRuleParameters . add ( new IRODSRuleParameter ( "*Resource" , MiscIRODSUtils . wrapStringInQuotes ( resourceName ) ) ) ; irodsRuleParameters . add ( new IRODSRuleParameter ( "*DelayInfo" , RuleUtils . buildDelayParamForMinutes ( delayInMinutes ) ) ) ; RuleInvocationConfiguration ruleInvocationConfiguration = RuleInvocationConfiguration . instanceWithDefaultAutoSettings ( getJargonProperties ( ) ) ; ruleInvocationConfiguration . setRuleProcessingType ( RuleProcessingType . EXTERNAL ) ; IRODSRuleExecResult result = ruleProcessingAO . executeRuleFromResource ( "/rules/rulemsiCollReplAsync.r" , irodsRuleParameters , ruleInvocationConfiguration ) ; log . info ( "result of action:{}" , result . getRuleExecOut ( ) . trim ( ) ) ; }
@ Override public void replicateCollectionAsynchronously ( final String irodsCollectionAbsolutePath , final String resourceName , final int delayInMinutes ) throws JargonException { log . info ( "replicateCollectionAsynchronously()" ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "irodsCollectionAbsolutePath:{}" , irodsCollectionAbsolutePath ) ; log . info ( "resourceName:{}" , resourceName ) ; log . info ( "delayInMinutes:{}" , delayInMinutes ) ; code_block = IfStatement ; RuleProcessingAO ruleProcessingAO = getIRODSAccessObjectFactory ( ) . getRuleProcessingAO ( getIRODSAccount ( ) ) ; List < IRODSRuleParameter > irodsRuleParameters = new ArrayList < IRODSRuleParameter > ( ) ; irodsRuleParameters . add ( new IRODSRuleParameter ( "*SourceFile" , MiscIRODSUtils . wrapStringInQuotes ( irodsCollectionAbsolutePath ) ) ) ; irodsRuleParameters . add ( new IRODSRuleParameter ( "*Resource" , MiscIRODSUtils . wrapStringInQuotes ( resourceName ) ) ) ; irodsRuleParameters . add ( new IRODSRuleParameter ( "*DelayInfo" , RuleUtils . buildDelayParamForMinutes ( delayInMinutes ) ) ) ; RuleInvocationConfiguration ruleInvocationConfiguration = RuleInvocationConfiguration . instanceWithDefaultAutoSettings ( getJargonProperties ( ) ) ; ruleInvocationConfiguration . setRuleProcessingType ( RuleProcessingType . EXTERNAL ) ; IRODSRuleExecResult result = ruleProcessingAO . executeRuleFromResource ( "/rules/rulemsiCollReplAsync.r" , irodsRuleParameters , ruleInvocationConfiguration ) ; log . info ( "result of action:{}" , result . getRuleExecOut ( ) . trim ( ) ) ; }
@ Override public void replicateCollectionAsynchronously ( final String irodsCollectionAbsolutePath , final String resourceName , final int delayInMinutes ) throws JargonException { log . info ( "replicateCollectionAsynchronously()" ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "irodsCollectionAbsolutePath:{}" , irodsCollectionAbsolutePath ) ; log . info ( "resourceName:{}" , resourceName ) ; log . info ( "delayInMinutes:{}" , delayInMinutes ) ; code_block = IfStatement ; RuleProcessingAO ruleProcessingAO = getIRODSAccessObjectFactory ( ) . getRuleProcessingAO ( getIRODSAccount ( ) ) ; List < IRODSRuleParameter > irodsRuleParameters = new ArrayList < IRODSRuleParameter > ( ) ; irodsRuleParameters . add ( new IRODSRuleParameter ( "*SourceFile" , MiscIRODSUtils . wrapStringInQuotes ( irodsCollectionAbsolutePath ) ) ) ; irodsRuleParameters . add ( new IRODSRuleParameter ( "*Resource" , MiscIRODSUtils . wrapStringInQuotes ( resourceName ) ) ) ; irodsRuleParameters . add ( new IRODSRuleParameter ( "*DelayInfo" , RuleUtils . buildDelayParamForMinutes ( delayInMinutes ) ) ) ; RuleInvocationConfiguration ruleInvocationConfiguration = RuleInvocationConfiguration . instanceWithDefaultAutoSettings ( getJargonProperties ( ) ) ; ruleInvocationConfiguration . setRuleProcessingType ( RuleProcessingType . EXTERNAL ) ; IRODSRuleExecResult result = ruleProcessingAO . executeRuleFromResource ( "/rules/rulemsiCollReplAsync.r" , irodsRuleParameters , ruleInvocationConfiguration ) ; log . info ( "result of action:{}" , result . getRuleExecOut ( ) . trim ( ) ) ; }
public void test() { if ( line . hasOption ( 'f' ) ) { expectedFailure = true ; logger . info ( "expectedFailure = " + expectedFailure ) ; } }
public void test() { try { line = parser . parse ( options , args ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; } catch ( ParseException e ) { logger . error ( "failed to parse command line: " + e ) ; HelpFormatter formatter = new HelpFormatter ( ) ; formatter . printHelp ( ProviderApplication . class . getName ( ) , options , true ) ; System . exit ( 1 ) ; } }
public void test() { if ( ! gbidsParameter . isEmpty ( ) ) { gbids = Arrays . stream ( gbidsParameter . split ( "," ) ) . map ( a -> a . trim ( ) ) . toArray ( String [ ] :: new ) ; logger . debug ( "Searching for providers on domain \"{}\", gbids \"{}\"" , providerDomain , gbids ) ; } else { gbids = new String [ 0 ] ; logger . debug ( "Searching for providers on domain \"{}\"" , providerDomain ) ; } }
public static void main ( String [ ] args ) throws IOException { CommandLine line ; Options options = new Options ( ) ; Options helpOptions = new Options ( ) ; setupOptions ( options , helpOptions ) ; CommandLineParser parser = new DefaultParser ( ) ; code_block = TryStatement ;  code_block = TryStatement ;  code_block = IfStatement ; Properties joynrConfig = new Properties ( ) ; Module runtimeModule = getRuntimeModule ( args , joynrConfig ) ; logger . debug ( "Using the following runtime module: " + runtimeModule . getClass ( ) . getSimpleName ( ) ) ; logger . debug ( "Searching for providers on domain \"{}\"" , providerDomain ) ; joynrConfig . setProperty ( MessagingPropertyKeys . PERSISTENCE_FILE , STATIC_PERSISTENCE_FILE ) ; joynrConfig . setProperty ( PROPERTY_JOYNR_DOMAIN_LOCAL , providerDomain ) ; Properties appConfig = new Properties ( ) ; appConfig . setProperty ( SYSTEMINTEGRATIONTEST_PROVIDER_DOMAIN , providerDomain ) ; JoynrApplication myConsumerApp = new JoynrInjectorFactory ( joynrConfig , runtimeModule ) . createApplication ( new JoynrApplicationModule ( ConsumerApplication . class , appConfig ) ) ; myConsumerApp . run ( ) ; myConsumerApp . shutdown ( ) ; }
public void test() { try { final StopWatch elapsedTime = StopWatch . createStarted ( ) ; elasticClientLogger . debug ( ">>> delete({})" , delete ) ; final QueryBuilder deleteQuery = ElasticQueryBuilder . buildFilterQuery ( delete . getQuery ( ) , factory , delete . getUpdateContext ( ) , this . currentFootprint ) ; final BulkByScrollResponse response = elasticSearchClient . deleteByQuery ( deleteQuery ) ; code_block = IfStatement ; elapsedTime . stop ( ) ; return new DeleteResult ( response . getTook ( ) . getMillis ( ) ) . setElapsedTime ( elapsedTime . getTime ( ) ) ; } catch ( ElasticsearchException | IOException e ) { log . error ( "Cannot delete with query {}" , delete . getQuery ( ) , e ) ; throw new SearchServerException ( String . format ( "Cannot delete with query %s" , delete . getQuery ( ) . toString ( ) ) , e ) ; } }
public void test() { try { final StopWatch elapsedTime = StopWatch . createStarted ( ) ; elasticClientLogger . debug ( ">>> delete({})" , delete ) ; final QueryBuilder deleteQuery = ElasticQueryBuilder . buildFilterQuery ( delete . getQuery ( ) , factory , delete . getUpdateContext ( ) , this . currentFootprint ) ; final BulkByScrollResponse response = elasticSearchClient . deleteByQuery ( deleteQuery ) ; code_block = IfStatement ; elapsedTime . stop ( ) ; return new DeleteResult ( response . getTook ( ) . getMillis ( ) ) . setElapsedTime ( elapsedTime . getTime ( ) ) ; } catch ( ElasticsearchException | IOException e ) { log . error ( "Cannot delete with query {}" , delete . getQuery ( ) , e ) ; throw new SearchServerException ( String . format ( "Cannot delete with query %s" , delete . getQuery ( ) . toString ( ) ) , e ) ; } }
public void test() { try { testStatement . close ( ) ; } catch ( Exception e ) { LOG . error ( "Failed to close the connection" , e ) ; } }
public void test() { try { ret = convertPathToContainer ( basePath , next ) ; } catch ( NoSuchElementException | GenericException | RequestNotValidException e ) { LOGGER . error ( "Error while listing containers, while parsing resource " + next , e ) ; ret = null ; } }
public void test() { try { String type = tika . detect ( input ) ; code_block = IfStatement ; } catch ( IOException e ) { LOGGER . error ( "Failed to handle image ByteArrayInputStream" , e ) ; } }
public void test() { try { PackageUpdateService pus = Framework . getLocalService ( PackageUpdateService . class ) ; pus . removePackage ( pkgId ) ; return getView ( "removeDone" ) . arg ( "pkgId" , pkgId ) . arg ( "source" , source ) ; } catch ( Exception e ) { log . error ( "Error during first step of installation" , e ) ; return getView ( "removeError" ) . arg ( "e" , e ) ; } }
public void test() { if ( xp != null ) { String superCo = xp . getSuperComponent ( ) ; code_block = IfStatement ; } else { log . error ( "Warning: target extension point '" + extension . getExtensionPoint ( ) + "' of '" + extension . getTargetComponent ( ) . getName ( ) + "' is unknown. Check your extension in component " + extension . getComponent ( ) . getName ( ) ) ; Framework . handleDevError ( null ) ; } }
public void test() { try ( PreparedStatement query = getConn ( "config" ) . prepareStatement ( "SELECT count(id) FROM organizations WHERE orgId = ?" ) ) { query . setString ( 1 , orgId ) ; code_block = IfStatement ; } catch ( PSQLException pe ) { ServerErrorMessage em = pe . getServerErrorMessage ( ) ; logger . warn ( em . toString ( ) ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . warn ( String . format ( "%s" , LogUtil . getStackTrace ( e ) ) ) ; } finally { closeConfig ( ) ; } }
public void test() { try ( PreparedStatement query = getConn ( "config" ) . prepareStatement ( "SELECT count(id) FROM organizations WHERE orgId = ?" ) ) { query . setString ( 1 , orgId ) ; code_block = IfStatement ; } catch ( PSQLException pe ) { ServerErrorMessage em = pe . getServerErrorMessage ( ) ; logger . warn ( em . toString ( ) ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . warn ( String . format ( "%s" , LogUtil . getStackTrace ( e ) ) ) ; } finally { closeConfig ( ) ; } }
public void test() { try { processCommand ( command , host . getState ( ) ) ; } catch ( IOException e ) { LOG . error ( "Failed to process command: " + command , e ) ; break ; } }
public void test() { try { HostCommand command = commandQueue . poll ( MAIN_THREAD_STEP_SLEEP_MS , TimeUnit . MILLISECONDS ) ; code_block = IfStatement ; } catch ( InterruptedException e ) { LOG . info ( "Interrupted in main loop. Exiting." , e ) ; break ; } }
public void run ( ) throws IOException , InterruptedException { connectToRing ( ) ; addShutdownHook ( ) ; setStateSynchronized ( HostState . IDLE ) ; addServerOfflineWatcher ( ) ; code_block = WhileStatement ; processCommandOnStartup ( ) ; code_block = WhileStatement ; LOG . info ( "Partition server main thread is stopping." ) ; stopServingData ( ) ; stopUpdating ( ) ; setStateSynchronized ( HostState . OFFLINE ) ; removeShutdownHook ( ) ; coordinator . close ( ) ; }
public void test() { try { code_block = IfStatement ; } catch ( URISyntaxException e ) { LOGGER . error ( "Exception in WebDriverManager while getWebDriver " , e ) ; } }
public void test() { try { container = toCredential ( ) ; cert = container . getCert ( ) ; key = container . getKey ( ) ; } catch ( CertificateException e ) { LOG . warn ( "Cert Container conversion failed: " + e . getLocalizedMessage ( ) , e ) ; } }
public void test() { try { readByteOffset ++ ; offset ++ ; code_block = IfStatement ; byteRead = inBuffer [ readByteOffset ] ; } catch ( IOException ioe ) { logger . error ( "Input/Output exception while reading from a random access file. Stack trace follows" , ioe ) ; } }
public void test() { try { execute ( new Command < Void > ( ) code_block = "" ; ) ; } catch ( KafkaException kex ) { LOG . error ( "Failed to remove acls." , kex ) ; return false ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( "Registered service: {}..." , brokerPoolService . getClass ( ) . getSimpleName ( ) ) ; } }
public void test() { if ( unavailableNode && t . getMessage ( ) . startsWith ( "UNAVAILABLE" ) ) { log . info ( "Node {} is unavailable as expected!" , HapiPropertySource . asAccountString ( node . get ( ) ) ) ; return Optional . empty ( ) ; } }
public void test() { if ( verboseLoggingOn ) { log . warn ( spec . logPrefix ( ) + this + " failed - {}" , t ) ; } else-if ( ! loggingOff ) { log . warn ( spec . logPrefix ( ) + this + " failed {}!" , t . getMessage ( ) ) ; } }
public void test() { if ( verboseLoggingOn ) { log . warn ( spec . logPrefix ( ) + this + " failed - {}" , t ) ; } else-if ( ! loggingOff ) { log . warn ( spec . logPrefix ( ) + this + " failed {}!" , t . getMessage ( ) ) ; } }
public void test() { if ( unavailableNode ) { String message = String . format ( "Node %s is NOT unavailable as expected!!!" , HapiPropertySource . asAccountString ( node . get ( ) ) ) ; log . error ( message ) ; return Optional . of ( new RuntimeException ( message ) ) ; } }
public void test() { try { String result = this . checkUser ( ) ; if ( null != result ) return result ; code_block = IfStatement ; UserAuthsFormBean authsBean = this . getUserAuthsFormBean ( ) ; this . getAuthorizationManager ( ) . updateUserAuthorizations ( username , authsBean . getAuthorizations ( ) ) ; this . getRequest ( ) . getSession ( ) . removeAttribute ( CURRENT_FORM_USER_AUTHS_PARAM_NAME ) ; } catch ( Throwable t ) { logger . error ( "error in save" , t ) ; return FAILURE ; } }
public void init ( String [ ] args ) { initLogger ( ) ; conf = loadConfig ( args ) ; conf . startCleanup ( ) ; LOG . debug ( "Loaded conf " + conf ) ; }
public void test() { try { revisionNote = getRepo ( 0 ) . get ( noteId , revId , subject ) ; } catch ( IOException e ) { LOG . error ( "Failed to get revision {} of note {}" , revId , noteId , e ) ; } }
public void test() { try ( Connection connection = DriverManager . getConnection ( Config . IOTDB_URL_PREFIX + "127.0.0.1:6667/" , "root" , "root" ) ; Statement statement = connection . createStatement ( ) ) { Thread . sleep ( 500 ) ; statement . execute ( "create trigger trigger-1 before insert on root.vehicle.d1.s1 as \"org.apache.iotdb.db.engine.trigger.example.Counter\"" ) ; statement . execute ( "create trigger trigger-2 after insert on root.vehicle.d1.s2 as \"org.apache.iotdb.db.engine.trigger.example.Counter\"" ) ; statement . execute ( "create trigger trigger-3 before insert on root.vehicle.d1.s3 as \"org.apache.iotdb.db.engine.trigger.example.Counter\"" ) ; Thread . sleep ( 500 ) ; int [ ] counters1 = getCounters ( 3 ) ; LOGGER . info ( Arrays . toString ( counters1 ) ) ; code_block = ForStatement ; Thread . sleep ( 500 ) ; statement . execute ( "create trigger trigger-4 after insert on root.vehicle.d1.s4 as \"org.apache.iotdb.db.engine.trigger.example.Counter\"" ) ; statement . execute ( "create trigger trigger-5 before insert on root.vehicle.d1.s5 as \"org.apache.iotdb.db.engine.trigger.example.Counter\"" ) ; statement . execute ( "create trigger trigger-6 after insert on root.vehicle.d1.s6 as \"org.apache.iotdb.db.engine.trigger.example.Counter\"" ) ; int [ ] counters2 = getCounters ( 3 ) ; LOGGER . info ( Arrays . toString ( counters2 ) ) ; code_block = ForStatement ; } catch ( SQLException | TriggerManagementException e ) { fail ( e . getMessage ( ) ) ; } }
public void test() { try ( Connection connection = DriverManager . getConnection ( Config . IOTDB_URL_PREFIX + "127.0.0.1:6667/" , "root" , "root" ) ; Statement statement = connection . createStatement ( ) ) { Thread . sleep ( 500 ) ; statement . execute ( "create trigger trigger-1 before insert on root.vehicle.d1.s1 as \"org.apache.iotdb.db.engine.trigger.example.Counter\"" ) ; statement . execute ( "create trigger trigger-2 after insert on root.vehicle.d1.s2 as \"org.apache.iotdb.db.engine.trigger.example.Counter\"" ) ; statement . execute ( "create trigger trigger-3 before insert on root.vehicle.d1.s3 as \"org.apache.iotdb.db.engine.trigger.example.Counter\"" ) ; Thread . sleep ( 500 ) ; int [ ] counters1 = getCounters ( 3 ) ; LOGGER . info ( Arrays . toString ( counters1 ) ) ; code_block = ForStatement ; Thread . sleep ( 500 ) ; statement . execute ( "create trigger trigger-4 after insert on root.vehicle.d1.s4 as \"org.apache.iotdb.db.engine.trigger.example.Counter\"" ) ; statement . execute ( "create trigger trigger-5 before insert on root.vehicle.d1.s5 as \"org.apache.iotdb.db.engine.trigger.example.Counter\"" ) ; statement . execute ( "create trigger trigger-6 after insert on root.vehicle.d1.s6 as \"org.apache.iotdb.db.engine.trigger.example.Counter\"" ) ; int [ ] counters2 = getCounters ( 3 ) ; LOGGER . info ( Arrays . toString ( counters2 ) ) ; code_block = ForStatement ; } catch ( SQLException | TriggerManagementException e ) { fail ( e . getMessage ( ) ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) && SerialPortEvent . DATA_AVAILABLE != seEvent . getEventType ( ) ) { logger . trace ( "Serial event: {}, new value:{}" , seEvent . getEventType ( ) , seEvent . getNewValue ( ) ) ; } }
public void test() { try { code_block = SwitchStatement ; } catch ( RuntimeException e ) { logger . warn ( "RuntimeException during handling serial event: {}" , seEvent . getEventType ( ) , e ) ; } }
public void test() { if ( diff > CHANNEL_EXPIRED_TIMEOUT ) { log . warn ( "SCAN: remove expired channel from ConsumerManager consumerTable. channel={}, consumerGroup={}" , RemotingHelper . parseChannelRemoteAddr ( clientChannelInfo . getChannel ( ) ) , group ) ; RemotingUtil . closeChannel ( clientChannelInfo . getChannel ( ) ) ; itChannel . remove ( ) ; } }
public void test() { if ( ! dumpDirectory . mkdirs ( ) ) { LOGGER . error ( "Failed to create dump directory; please double-check your filesystem permissions" ) ; return false ; } }
public void test() { if ( ! dumpDirectory . exists ( ) ) { code_block = IfStatement ; } else-if ( ! dumpDirectory . isDirectory ( ) ) { LOGGER . error ( "Dump directory path does not point to a valid directory" ) ; return false ; } }
public void test() { try { this . networkManager . initialize ( this . serverConfig . maxPlayers ( ) , host , port ) ; code_block = IfStatement ; } catch ( Exception e ) { LOGGER . error ( "Failed to initialize networking" , e ) ; return false ; } }
public void test() { try { String managementV6Address = execution . getVariable ( "oamManagementV6Address" ) ; code_block = IfStatement ; } catch ( Exception ex ) { logger . error ( "Exception occurred in AAIUpdateTasks updateManagementV6AddressVnf" , ex ) ; exceptionUtil . buildAndThrowWorkflowException ( execution , 7000 , ex ) ; } }
@ Override public void addProduct ( IAtomContainer product , Double coefficient ) { logger . debug ( "Adding product with coefficient: " , product , "" + coefficient ) ; super . addProduct ( product , coefficient ) ; }
public void test() { if ( notRepliedYet . isEmpty ( ) || pendingRemovals . containsAll ( notRepliedYet ) ) { logger . debug ( "All anticipated view responses received - notifying waiting thread" ) ; waiting = false ; notifyAll ( ) ; } else { logger . debug ( "Still waiting for these view replies: {}" , notRepliedYet ) ; } }
public void test() { if ( notRepliedYet . isEmpty ( ) || pendingRemovals . containsAll ( notRepliedYet ) ) { logger . debug ( "All anticipated view responses received - notifying waiting thread" ) ; waiting = false ; notifyAll ( ) ; } else { logger . debug ( "Still waiting for these view replies: {}" , notRepliedYet ) ; } }
private Set < SystemResponseDTO > getAuthorizedPublishers ( final SystemRequestDTO subscriberSystem ) { logger . debug ( "getAuthorizedPublishers started..." ) ; Assert . notNull ( subscriberSystem , "subscriberSystem is null." ) ; final UriComponents checkUri = getAuthSubscriptionCheckUri ( ) ; final AuthorizationSubscriptionCheckRequestDTO payload = new AuthorizationSubscriptionCheckRequestDTO ( subscriberSystem , null ) ; final ResponseEntity < AuthorizationSubscriptionCheckResponseDTO > response = httpService . sendRequest ( checkUri , HttpMethod . POST , AuthorizationSubscriptionCheckResponseDTO . class , payload ) ; return response . getBody ( ) . getPublishers ( ) ; }
public void test() { try { final NodeTemplateInstanceProperty property = this . convertDocumentToProperty ( properties , NodeTemplateInstanceProperty . class ) ; node . addProperty ( property ) ; this . nodeTemplateInstanceRepository . update ( node ) ; } catch ( InstantiationException | IllegalAccessException e ) { final String msg = String . format ( "An error occurred while instantiating an instance of the %s class." , NodeTemplateInstanceProperty . class ) ; logger . debug ( msg ) ; throw e ; } }
public void test() { try { _waitStopFinishCountDown . await ( ) ; _started = false ; } catch ( InterruptedException e ) { LOG . error ( "Interrupted waiting for finish" , e ) ; } }
public void printHeader ( List < ArtifactRepository > repositories ) { log . info ( "" ) ; log . info ( "dependencies, and where they are available:" ) ; final int repCount = repositories . size ( ) ; code_block = ForStatement ; log . info ( StringUtils . repeat ( "|" + sep , repCount ) ) ; }
public void printHeader ( List < ArtifactRepository > repositories ) { log . info ( "" ) ; log . info ( "dependencies, and where they are available:" ) ; final int repCount = repositories . size ( ) ; code_block = ForStatement ; log . info ( StringUtils . repeat ( "|" + sep , repCount ) ) ; }
public void test() { for ( int i = 0 ; i < repCount ; i ++ ) { final ArtifactRepository rep = repositories . get ( i ) ; log . info ( String . format ( "%s%s (%s)" , StringUtils . repeat ( "|" + sep , i ) , rep . getId ( ) , rep . getUrl ( ) ) ) ; } }
public void printHeader ( List < ArtifactRepository > repositories ) { log . info ( "" ) ; log . info ( "dependencies, and where they are available:" ) ; final int repCount = repositories . size ( ) ; code_block = ForStatement ; log . info ( StringUtils . repeat ( "|" + sep , repCount ) ) ; }
@ Override @ POST @ Path ( UNITS_RULES_URI ) @ Consumes ( MediaType . APPLICATION_JSON ) @ Produces ( MediaType . APPLICATION_JSON ) public Response massUpdateUnitsRules ( MassUpdateUnitRuleRequest massUpdateUnitRuleRequest ) { JsonNode queryDsl = massUpdateUnitRuleRequest . getDslRequest ( ) . deepCopy ( ) ; RuleActions ruleActions = massUpdateUnitRuleRequest . getRuleActions ( ) ; LOGGER . debug ( "Start mass updating archive units with Dsl query {}" , queryDsl ) ; Status status ; code_block = TryStatement ;  }
public void test() { try ( ProcessingManagementClient processingClient = processingManagementClientFactory . getClient ( ) ; LogbookOperationsClient logbookOperationsClient = logbookOperationsClientFactory . getClient ( ) ; WorkspaceClient workspaceClient = workspaceClientFactory . getClient ( ) ) { SanityChecker . checkJsonAll ( queryDsl ) ; code_block = IfStatement ; String operationId = getVitamSession ( ) . getRequestId ( ) ; final LogbookOperationParameters initParameters = LogbookParameterHelper . newLogbookOperationParameters ( GUIDReader . getGUID ( operationId ) , Contexts . MASS_UPDATE_UNIT_RULE . getEventType ( ) , GUIDReader . getGUID ( operationId ) , LogbookTypeProcess . MASS_UPDATE , STARTED , VitamLogbookMessages . getCodeOp ( Contexts . MASS_UPDATE_UNIT_RULE . getEventType ( ) , STARTED ) , GUIDReader . getGUID ( operationId ) ) ; addRightsStatementIdentifier ( initParameters ) ; logbookOperationsClient . create ( initParameters ) ; workspaceClient . createContainer ( operationId ) ; workspaceClient . putObject ( operationId , OperationContextMonitor . OperationContextFileName , writeToInpustream ( OperationContextModel . get ( massUpdateUnitRuleRequest ) ) ) ; workspaceClient . putObject ( operationId , QUERY_FILE , writeToInpustream ( applyAccessContractRestrictionForUnitForUpdate ( queryDsl , getVitamSession ( ) . getContract ( ) ) ) ) ; workspaceClient . putObject ( operationId , "actions.json" , writeToInpustream ( ruleActions ) ) ; OperationContextMonitor . compressInWorkspace ( workspaceClientFactory , operationId , Contexts . MASS_UPDATE_UNIT_RULE . getLogbookTypeProcess ( ) , OperationContextMonitor . OperationContextFileName ) ; processingClient . initVitamProcess ( operationId , Contexts . MASS_UPDATE_UNIT_RULE . name ( ) ) ; RequestResponse < ItemStatus > requestResponse = processingClient . executeOperationProcess ( operationId , Contexts . MASS_UPDATE_UNIT_RULE . name ( ) , RESUME . getValue ( ) ) ; return requestResponse . toResponse ( ) ; } catch ( ContentAddressableStorageServerException | LogbookClientBadRequestException | LogbookClientAlreadyExistsException | InvalidGuidOperationException | LogbookClientServerException | VitamClientException | InternalServerException | OperationContextException e ) { LOGGER . error ( "An error occured while mass updating archive units" , e ) ; return Response . status ( INTERNAL_SERVER_ERROR ) . entity ( getErrorEntity ( INTERNAL_SERVER_ERROR , e . getMessage ( ) ) ) . build ( ) ; } catch ( InvalidParseOperationException | InvalidCreateOperationException | BadRequestException e ) { LOGGER . error ( BAD_REQUEST_EXCEPTION , e ) ; status = Status . BAD_REQUEST ; return Response . status ( status ) . entity ( getErrorEntity ( status , e . getMessage ( ) ) ) . build ( ) ; } }
public void test() { try ( ProcessingManagementClient processingClient = processingManagementClientFactory . getClient ( ) ; LogbookOperationsClient logbookOperationsClient = logbookOperationsClientFactory . getClient ( ) ; WorkspaceClient workspaceClient = workspaceClientFactory . getClient ( ) ) { SanityChecker . checkJsonAll ( queryDsl ) ; code_block = IfStatement ; String operationId = getVitamSession ( ) . getRequestId ( ) ; final LogbookOperationParameters initParameters = LogbookParameterHelper . newLogbookOperationParameters ( GUIDReader . getGUID ( operationId ) , Contexts . MASS_UPDATE_UNIT_RULE . getEventType ( ) , GUIDReader . getGUID ( operationId ) , LogbookTypeProcess . MASS_UPDATE , STARTED , VitamLogbookMessages . getCodeOp ( Contexts . MASS_UPDATE_UNIT_RULE . getEventType ( ) , STARTED ) , GUIDReader . getGUID ( operationId ) ) ; addRightsStatementIdentifier ( initParameters ) ; logbookOperationsClient . create ( initParameters ) ; workspaceClient . createContainer ( operationId ) ; workspaceClient . putObject ( operationId , OperationContextMonitor . OperationContextFileName , writeToInpustream ( OperationContextModel . get ( massUpdateUnitRuleRequest ) ) ) ; workspaceClient . putObject ( operationId , QUERY_FILE , writeToInpustream ( applyAccessContractRestrictionForUnitForUpdate ( queryDsl , getVitamSession ( ) . getContract ( ) ) ) ) ; workspaceClient . putObject ( operationId , "actions.json" , writeToInpustream ( ruleActions ) ) ; OperationContextMonitor . compressInWorkspace ( workspaceClientFactory , operationId , Contexts . MASS_UPDATE_UNIT_RULE . getLogbookTypeProcess ( ) , OperationContextMonitor . OperationContextFileName ) ; processingClient . initVitamProcess ( operationId , Contexts . MASS_UPDATE_UNIT_RULE . name ( ) ) ; RequestResponse < ItemStatus > requestResponse = processingClient . executeOperationProcess ( operationId , Contexts . MASS_UPDATE_UNIT_RULE . name ( ) , RESUME . getValue ( ) ) ; return requestResponse . toResponse ( ) ; } catch ( ContentAddressableStorageServerException | LogbookClientBadRequestException | LogbookClientAlreadyExistsException | InvalidGuidOperationException | LogbookClientServerException | VitamClientException | InternalServerException | OperationContextException e ) { LOGGER . error ( "An error occured while mass updating archive units" , e ) ; return Response . status ( INTERNAL_SERVER_ERROR ) . entity ( getErrorEntity ( INTERNAL_SERVER_ERROR , e . getMessage ( ) ) ) . build ( ) ; } catch ( InvalidParseOperationException | InvalidCreateOperationException | BadRequestException e ) { LOGGER . error ( BAD_REQUEST_EXCEPTION , e ) ; status = Status . BAD_REQUEST ; return Response . status ( status ) . entity ( getErrorEntity ( status , e . getMessage ( ) ) ) . build ( ) ; } }
public void test() { try { LOGGER . debug ( "Registering application service MBean under object name: {}" , objectName ) ; mBeanServer . registerMBean ( this , objectName ) ; } catch ( InstanceAlreadyExistsException iaee ) { LOGGER . debug ( "Re-registering Application Service MBean" ) ; mBeanServer . unregisterMBean ( objectName ) ; mBeanServer . registerMBean ( this , objectName ) ; } }
public void test() { try { LOGGER . debug ( "Registering application service MBean under object name: {}" , objectName ) ; mBeanServer . registerMBean ( this , objectName ) ; } catch ( InstanceAlreadyExistsException iaee ) { LOGGER . debug ( "Re-registering Application Service MBean" ) ; mBeanServer . unregisterMBean ( objectName ) ; mBeanServer . registerMBean ( this , objectName ) ; } }
public void test() { try { IContentListWidgetHelper helper = ( IContentListWidgetHelper ) ApsWebApplicationUtils . getBean ( JacmsSystemConstants . CONTENT_LIST_HELPER , this . pageContext ) ; List < UserFilterOptionBean > defaultUserFilterOptions = helper . getConfiguredUserFilters ( this , reqCtx ) ; this . addUserFilterOptions ( defaultUserFilterOptions ) ; this . extractExtraWidgetParameters ( reqCtx ) ; code_block = IfStatement ; List < String > contents = this . getContentsId ( helper , reqCtx ) ; this . pageContext . setAttribute ( this . getListName ( ) , contents ) ; } catch ( Throwable t ) { _logger . error ( "error in end tag" , t ) ; throw new JspException ( "Error detected while finalising the tag" , t ) ; } }
public String saveContent ( String containerId , String userId , Number taskId , String payload , String marshallingType ) { userId = getUser ( userId ) ; containerId = context . getContainerId ( containerId , new ByTaskIdContainerLocator ( taskId . longValue ( ) ) ) ; logger . debug ( "About to unmarshal task content parameters from payload: '{}'" , payload ) ; Map < String , Object > parameters = marshallerHelper . unmarshal ( containerId , payload , marshallingType , Map . class ) ; logger . debug ( "About to set content of a task with id '{}' with data {}" , taskId , parameters ) ; Long contentId = userTaskService . saveContentFromUser ( taskId . longValue ( ) , userId , parameters ) ; String response = marshallerHelper . marshal ( containerId , marshallingType , contentId ) ; return response ; }
public String saveContent ( String containerId , String userId , Number taskId , String payload , String marshallingType ) { userId = getUser ( userId ) ; containerId = context . getContainerId ( containerId , new ByTaskIdContainerLocator ( taskId . longValue ( ) ) ) ; logger . debug ( "About to unmarshal task content parameters from payload: '{}'" , payload ) ; Map < String , Object > parameters = marshallerHelper . unmarshal ( containerId , payload , marshallingType , Map . class ) ; logger . debug ( "About to set content of a task with id '{}' with data {}" , taskId , parameters ) ; Long contentId = userTaskService . saveContentFromUser ( taskId . longValue ( ) , userId , parameters ) ; String response = marshallerHelper . marshal ( containerId , marshallingType , contentId ) ; return response ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Executing complete tenant extension" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Complete tenant script returned:" + output ) ; } }
public void test() { if ( log . isErrorEnabled ( ) ) { log . error ( "Could not execute complete tenant extension" , e ) ; } }
public List findByExample ( MYesno instance ) { log . debug ( "finding MYesno instance by example" ) ; code_block = TryStatement ;  }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.MYesno" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.MYesno" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
@ EventListener @ Order ( 5 ) public void onApplicationEvent ( final ContextRefreshedEvent event ) { logger . info ( "STANDALONE mode is set..." ) ; arrowheadContext . put ( CoreCommonConstants . SERVER_STANDALONE_MODE , true ) ; }
public void test() { try { GoogleUtils . deleteObjectsInPath ( storage , inputDataConfig , accountConfig . getBucket ( ) , accountConfig . getPrefix ( ) , Predicates . alwaysTrue ( ) ) ; } catch ( Exception e ) { LOG . error ( "Error occurred while deleting task log files from gs. Error: %s" , e . getMessage ( ) ) ; throw new IOException ( e ) ; } }
public void test() { if ( mainWorker == null ) { LOG . error ( "Unexpected no (null) main worker on job: {}" , job ) ; job . setResult ( null ) ; return ; } }
public void test() { try { future = mainWorker . call ( ) ; } catch ( Exception e ) { LOG . error ( "Direct Exception (not failed Future) when executing job, won't even retry: {}" , job , e ) ; job . setResult ( null ) ; return ; } }
public void test() { { LOG . error ( "Job {} failed {}" , job . getKey ( ) , cause . getMessage ( ) ) ; job . setFailure ( cause ) ; } }
public void test() { if ( displayWarning ) { logger . warn ( format ( "Creating missing gene product with id ''{0}'' because reaction ''{1}'' uses this id" + " in its gene-product association." , id , reactionId ) ) ; } }
public void test() { try { final StringWriter writer = new StringWriter ( ) ; templates . newTransformer ( ) . transform ( source , new StreamResult ( writer ) ) ; return writer . toString ( ) ; } catch ( TransformerException e ) { logger . error ( e . getMessage ( ) ) ; throw new SchematronValidationException ( "Failed to apply Schematron validation transform" , e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( JSONException e ) { LOGGER . error ( "Exception:" , e ) ; Assert . fail ( "An Exception occurred during testing!:\n" + e . getMessage ( ) ) ; } }
public void test() { try { ret = atlasGraph . indexQuery ( VERTEX_INDEX , indexQuery ) . vertexTotals ( ) ; } catch ( Exception e ) { LOG . error ( "Failed fetching using indexQuery: " + e . getMessage ( ) ) ; } }
public void createMultiBranchPipeline ( FolderJob folder , String pipelineName , String repositoryPath ) throws IOException { deletePipeline ( folder , pipelineName ) ; URL url = Resources . getResource ( this . getClass ( ) , "multibranch.xml" ) ; jenkins . createJob ( folder , pipelineName , Resources . toString ( url , Charsets . UTF_8 ) . replace ( "{{repo}}" , repositoryPath ) , true ) ; LOGGER . info ( "Created multibranch pipeline: " + pipelineName ) ; JobWithDetails job = jenkins . getJob ( folder , pipelineName ) ; job . build ( true ) ; }
public void test() { try { code_block = IfStatement ; } catch ( IOException e ) { SECURITY_LOGGER . trace ( "Unable to close ldapConnectionHandler" , e ) ; } }
public void test() { if ( jTable . getSelectedRow ( ) < 0 ) { MessageDialog . error ( null , "angal.common.pleaseselectarow.msg" ) ; } else { selectedrow = jTable . getSelectedRow ( ) ; operationType = ( OperationType ) ( model . getValueAt ( selectedrow , - 1 ) ) ; OperationTypeEdit newrecord = new OperationTypeEdit ( myFrame , operationType , false ) ; newrecord . addOperationTypeListener ( OperationTypeBrowser . this ) ; newrecord . setVisible ( true ) ; } }
public void test() { try { HttpResponse response = client . execute ( request ) ; responseString = IOUtils . toString ( response . getEntity ( ) . getContent ( ) , "utf-8" ) ; } catch ( IOException e ) { logger . debug ( "Could not get version from GitHub" , e ) ; return ; } }
public void test() { try { Map < String , Object > [ ] data = gson . fromJson ( responseString , mapArrayType ) ; double latestVersion = 0 ; code_block = ForStatement ; lastUpdated = System . currentTimeMillis ( ) / 1000L ; } catch ( Exception e ) { logger . warn ( "Error while parsing the Opencast version from GitHub" , e ) ; } }
@ Override public void initialize ( ) { LOG . info ( "Trying to start the ZooKeeper container" ) ; container . start ( ) ; registerProperties ( ) ; LOG . info ( "ZooKeeper instance running at {}" , getConnectionString ( ) ) ; }
@ Override public void initialize ( ) { LOG . info ( "Trying to start the ZooKeeper container" ) ; container . start ( ) ; registerProperties ( ) ; LOG . info ( "ZooKeeper instance running at {}" , getConnectionString ( ) ) ; }
@ Override public void onNunchukRemovedEvent ( NunchukRemovedEvent arg0 ) { log . debug ( "NunchukRemovedEvent {}" , arg0 ) ; }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "\tFound " + children . size ( ) + " files in the repository" ) ; } }
public void test() { if ( logger . isWarnEnabled ( ) ) { logger . warn ( messages . getString ( "PentahoMetadataDomainRepository.WARN_0001_FILE_WITHOUT_METADATA" , child . getName ( ) ) ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "\tprocessing file [type=" + type + " : domainId=" + domainId + " : locale=" + locale + "]" ) ; } }
public void test() { for ( final CloseEvent closeEvent : _closeEvents ) { _logger . debug ( "Stack trace when close() no. {} of {}: " , i , numCloses ) ; _logger . debug ( "Thread name: {}" , closeEvent . threadName ) ; logStack ( closeEvent . stackTrace ) ; i ++ ; } }
public void test() { try { ongoingCalculation . cancel ( true ) ; } catch ( Exception e ) { LOG . error ( "error cancelling future to " + name , e ) ; } }
public void test() { try { StatementMirror mirror = resultSet . glowroot$getStatementMirror ( ) ; code_block = IfStatement ; QueryEntry lastQueryEntry = mirror . getLastQueryEntry ( ) ; code_block = IfStatement ; lastQueryEntry . setCurrRow ( ( ( ResultSet ) resultSet ) . getRow ( ) ) ; } catch ( Exception e ) { logger . warn ( e . getMessage ( ) , e ) ; } }
public void test() { if ( logTransactionExceptions ) { log . error ( "Failed to end transaction" , e ) ; } }
public void test() { if ( createSolrClientPerRequest ) { logger . debug ( "Creating a new Solr Client." ) ; return Solr6Index . instance . createSolrClient ( ) ; } else { logger . debug ( "Returning the solr client owned by Solr6Index." ) ; return Solr6Index . instance . solrClient ; } }
public void test() { if ( createSolrClientPerRequest ) { logger . debug ( "Creating a new Solr Client." ) ; return Solr6Index . instance . createSolrClient ( ) ; } else { logger . debug ( "Returning the solr client owned by Solr6Index." ) ; return Solr6Index . instance . solrClient ; } }
public void test() { if ( Solr6Index . instance != null ) { code_block = IfStatement ; } else { logger . debug ( " No Solr6Index available. Will return null" ) ; return null ; } }
@ Override public void error ( final SAXParseException e ) { logger . warn ( "Schema validation error parsing Flow Configuration at line {}, col {}: {}" , e . getLineNumber ( ) , e . getColumnNumber ( ) , e . getMessage ( ) ) ; }
private void cleanUpCreatedTopics ( final Set < String > topicsToCleanUp ) { log . info ( "Starting to clean up internal topics {}." , topicsToCleanUp ) ; final long now = time . milliseconds ( ) ; final long deadline = now + retryTimeoutMs ; final Set < String > topicsStillToCleanup = new HashSet < > ( topicsToCleanUp ) ; code_block = WhileStatement ; log . info ( "Completed cleanup of internal topics {}." , topicsToCleanUp ) ; }
public void test() { if ( cause instanceof UnknownTopicOrPartitionException ) { log . info ( "Internal topic {} to clean up is missing" , topicName ) ; } else-if ( cause instanceof LeaderNotAvailableException ) { log . info ( "The leader of internal topic {} to clean up is not available." , topicName ) ; } else-if ( cause instanceof TimeoutException ) { log . info ( "Cleaning up internal topic {} timed out." , topicName ) ; } else { log . error ( "Unexpected error during cleanup of internal topics: " , cause ) ; throw new StreamsException ( String . format ( "Could not clean up internal topics %s, because during the cleanup " + "of topic %s the following error occurred: " , topicsStillToCleanup , topicName ) , cause ) ; } }
public void test() { if ( cause instanceof UnknownTopicOrPartitionException ) { log . info ( "Internal topic {} to clean up is missing" , topicName ) ; } else-if ( cause instanceof LeaderNotAvailableException ) { log . info ( "The leader of internal topic {} to clean up is not available." , topicName ) ; } else-if ( cause instanceof TimeoutException ) { log . info ( "Cleaning up internal topic {} timed out." , topicName ) ; } else { log . error ( "Unexpected error during cleanup of internal topics: " , cause ) ; throw new StreamsException ( String . format ( "Could not clean up internal topics %s, because during the cleanup " + "of topic %s the following error occurred: " , topicsStillToCleanup , topicName ) , cause ) ; } }
public void test() { if ( cause instanceof UnknownTopicOrPartitionException ) { log . info ( "Internal topic {} to clean up is missing" , topicName ) ; } else-if ( cause instanceof LeaderNotAvailableException ) { log . info ( "The leader of internal topic {} to clean up is not available." , topicName ) ; } else-if ( cause instanceof TimeoutException ) { log . info ( "Cleaning up internal topic {} timed out." , topicName ) ; } else { log . error ( "Unexpected error during cleanup of internal topics: " , cause ) ; throw new StreamsException ( String . format ( "Could not clean up internal topics %s, because during the cleanup " + "of topic %s the following error occurred: " , topicsStillToCleanup , topicName ) , cause ) ; } }
public void test() { try { InputStream dataToValid = new ByteArrayInputStream ( getContent ( ) . getBytes ( "UTF-8" ) ) ; SchemaFactory factory = SchemaFactory . newInstance ( "http://www.w3.org/2001/XMLSchema" ) ; Schema schema = factory . newSchema ( new URL ( schemaURL ) ) ; Source source = new StreamSource ( dataToValid ) ; Validator validator = schema . newValidator ( ) ; validator . validate ( source ) ; return true ; } catch ( SAXException ex ) { throw new DocServiceException ( "File is not valid. " + ex . getMessage ( ) , HttpServletResponse . SC_UNSUPPORTED_MEDIA_TYPE ) ; } catch ( IOException e ) { LOG . error ( "Error while checking validity of the document" , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Client Http Session received OK" ) ; } }
public void test() { if ( response != null ) { Job r = JobParser . parseJob ( response . getEntity ( ) . getContent ( ) ) ; logger . info ( "Concat video job {} started on a remote composer" , r . getId ( ) ) ; return r ; } }
public void test() { try { tracciatiBD = new TracciatiBD ( configWrapper ) ; tracciatiBD . setupConnection ( configWrapper . getTransactionID ( ) ) ; tracciatiBD . setAtomica ( false ) ; tracciatiBD . setAutoCommit ( false ) ; SerializationConfig config = new SerializationConfig ( ) ; config . setDf ( SimpleDateFormatUtils . newSimpleDateFormatDataOreMinuti ( ) ) ; config . setIgnoreNullValues ( true ) ; IDeserializer deserializer = SerializationFactory . getDeserializer ( SERIALIZATION_TYPE . JSON_JACKSON , config ) ; serializer = SerializationFactory . getSerializer ( SERIALIZATION_TYPE . JSON_JACKSON , config ) ; beanDati = ( it . govpay . core . beans . tracciati . TracciatoPendenza ) deserializer . getObject ( tracciato . getBeanDati ( ) , it . govpay . core . beans . tracciati . TracciatoPendenza . class ) ; code_block = SwitchStatement ; } catch ( Throwable e ) { log . error ( "Errore durante l'elaborazione del tracciato " + formato + " [" + tracciato . getId ( ) + "]: " + e . getMessage ( ) , e ) ; tracciatiBD . rollback ( ) ; tracciato . setStato ( STATO_ELABORAZIONE . SCARTATO ) ; String descrizioneStato = "Errore durante l'elaborazione del tracciato: " + e . getMessage ( ) ; tracciato . setDescrizioneStato ( descrizioneStato . length ( ) > 256 ? descrizioneStato . substring ( 0 , 255 ) : descrizioneStato ) ; tracciato . setDataCompletamento ( new Date ( ) ) ; code_block = IfStatement ; tracciatiBD . updateFineElaborazione ( tracciato ) ; tracciatiBD . commit ( ) ; } finally { code_block = IfStatement ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( "Loading " + sourceURL ) ; } }
public void test() { if ( ! bindables . isEmpty ( ) || LaunchMode . current ( ) == LaunchMode . DEVELOPMENT ) { beans . produce ( AdditionalBeanBuildItem . unremovableOf ( GrpcContainer . class ) ) ; features . produce ( new FeatureBuildItem ( GRPC_SERVER ) ) ; } else { logger . debug ( "Unable to find beans exposing the `BindableService` interface - not starting the gRPC server" ) ; } }
public void test() { try { final ContentVersion contentVersion = versioningManager . getVersion ( versionId ) ; final Content content = versioningManager . getContent ( contentVersion ) ; final ContentDto contentDto = contentService . getDtoBuilder ( ) . convert ( content ) ; return contentDto ; } catch ( ApsSystemException e ) { logger . error ( "Error reading the content from version {} " , versionId , e ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "table:" + triStateName + " got " + sharedTriState + " from " + sharedTriStates ) ; log . trace ( "table:" + triStateName + " checking to see if sharedTriState " + sharedTriState + " is the same as expected value:" + expectedValue ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "table:" + triStateName + " got " + sharedTriState + " from " + sharedTriStates ) ; log . trace ( "table:" + triStateName + " checking to see if sharedTriState " + sharedTriState + " is the same as expected value:" + expectedValue ) ; } }
public void test() { if ( cacheMap . containsKey ( pid ) ) { TimestampedCacheEntry < DOReader > e = cacheMap . remove ( pid ) ; LOG . debug ( "cache hit for {}" , pid ) ; result = e . value ( ) ; cacheMap . put ( pid , e . refresh ( ) ) ; } else { LOG . debug ( "cache miss for {}" , pid ) ; } }
public void test() { if ( cacheMap . containsKey ( pid ) ) { TimestampedCacheEntry < DOReader > e = cacheMap . remove ( pid ) ; LOG . debug ( "cache hit for {}" , pid ) ; result = e . value ( ) ; cacheMap . put ( pid , e . refresh ( ) ) ; } else { LOG . debug ( "cache miss for {}" , pid ) ; } }
public void test() { if ( ! isSuccess ) { log . error ( "push data error, keyspaceName:{}, cfName:{}, entryType:{}, fieldMap:{}" , keyspaceName , cfName , fieldMap , entryType ) ; } }
public void test() { try { code_block = IfStatement ; log . info ( "Cassandra Sink Task trying to put()" ) ; code_block = ForStatement ; } catch ( Exception e ) { log . error ( "put sinkDataEntries error, {}" , e ) ; } }
public void test() { if ( listeningChannel == null && channelGroup == null ) { LOG . debug ( "Creating communication channel..." ) ; } else { LOG . debug ( "Reconnecting communication channel..." ) ; closeConnection ( ) ; } }
public void test() { if ( listeningChannel == null && channelGroup == null ) { LOG . debug ( "Creating communication channel..." ) ; } else { LOG . debug ( "Reconnecting communication channel..." ) ; closeConnection ( ) ; } }
void reconnect ( ) { code_block = IfStatement ; channelGroup = new DefaultChannelGroup ( TSOChannelHandler . class . getName ( ) ) ; LOG . debug ( "\tCreating channel to listening for incoming connections in port {}" , config . getPort ( ) ) ; listeningChannel = bootstrap . bind ( new InetSocketAddress ( config . getPort ( ) ) ) ; channelGroup . add ( listeningChannel ) ; LOG . debug ( "\tListening channel created and connected: {}" , listeningChannel ) ; }
void reconnect ( ) { code_block = IfStatement ; channelGroup = new DefaultChannelGroup ( TSOChannelHandler . class . getName ( ) ) ; LOG . debug ( "\tCreating channel to listening for incoming connections in port {}" , config . getPort ( ) ) ; listeningChannel = bootstrap . bind ( new InetSocketAddress ( config . getPort ( ) ) ) ; channelGroup . add ( listeningChannel ) ; LOG . debug ( "\tListening channel created and connected: {}" , listeningChannel ) ; }
protected Key toKey ( String expression , VariableResolver resolver ) { Set < String > variablesUsed = variableCache . get ( ) . computeIfAbsent ( expression , this :: variablesUsed ) ; Map < String , Object > input = new HashMap < > ( ) ; code_block = ForStatement ; Key cacheKey = new Key ( expression , input ) ; LOG . debug ( "Created cache key; {}" , cacheKey ) ; return cacheKey ; }
public void test() { if ( apiManagementProviderService == null ) { String msg = "API management provider service has not initialized." ; log . error ( msg ) ; throw new IllegalStateException ( msg ) ; } }
public void test() { try { Session session = sessions . get ( wrapper ) ; session . close ( ) ; } catch ( JMSException ex ) { log . error ( "Failed to unregister properly from a Tag update; subscriptions will be refreshed." ) ; startReconnectThread ( ) ; } finally { wrapper . stop ( ) ; sessions . remove ( wrapper ) ; topicToWrapper . remove ( subsribedToTag . getTopicName ( ) ) ; } }
public void test() { try { lifecycleErrorEvent . fire ( event ) ; } catch ( Exception ex ) { logger . warn ( "A lifecycle error observer threw an exception" , ex ) ; } }
public void test() { if ( authority . getAuthority ( ) . equals ( requiredAuthority ) ) { log . debug ( "ACCESS GRANTED [" + object . toString ( ) + "]" ) ; return ACCESS_GRANTED ; } }
public void test() { if ( ExceptionUtils . getRootCause ( e ) instanceof CommandTimeoutException ) { logger . error ( "[monitor] sentinel: {} : {}" , sentinel , e . getMessage ( ) ) ; } else { logger . error ( "[monitor] sentinel: {}" , sentinel , e ) ; } }
public void test() { if ( ExceptionUtils . getRootCause ( e ) instanceof CommandTimeoutException ) { logger . error ( "[monitor] sentinel: {} : {}" , sentinel , e . getMessage ( ) ) ; } else { logger . error ( "[monitor] sentinel: {}" , sentinel , e ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getDiscoveryInitialWaitTimeout()" ) ; } }
public void test() { try { connection . close ( ) ; } catch ( IOException e ) { WGLOG . warn ( e , "W13001" , profile . getResourceName ( ) , script . getName ( ) , path ) ; } }
@ Test public void testScheduleCampaignCloneFail ( ) throws Exception { iprops . campaignAction . setValue ( schedule ) ; iprops . campaignId . setValue ( BATCH_CAMPAIGN ) ; iprops . cloneToProgramName . setValue ( "undx_test_program" ) ; iprops . afterCampaignAction ( ) ; MarketoRecordResult rs = getClient ( iprops ) . scheduleCampaign ( iprops ) ; LOG . debug ( "[testScheduleCampaign] {}" , rs ) ; assertFalse ( rs . isSuccess ( ) ) ; assertEquals ( "{[611] System error}" , rs . getErrorsString ( ) ) ; }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Securing route {} using Shiro policy {}" , route . getRouteId ( ) , this ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( IOException ioex ) { log . error ( "Error during find All , Caused by: ." , ioex ) ; throw new KunderaException ( ioex ) ; } }
private ScheduledExecutorService createCleanUpExecutor ( ) { final int numCleaningThreads = quotaConfig . getMaxConcurrentCleanUps ( ) ; log . info ( "Setting up disk quota periodic enforcement task" ) ; CustomizableThreadFactory tf = new CustomizableThreadFactory ( "GWC DiskQuota clean up thread-" ) ; tf . setThreadPriority ( 1 + ( Thread . MAX_PRIORITY - Thread . MIN_PRIORITY ) / 5 ) ; ScheduledExecutorService executorService = Executors . newScheduledThreadPool ( numCleaningThreads , tf ) ; return executorService ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "asyncTraceContext.continueAsyncTraceObject() AsyncTrace:{}" , asyncTrace ) ; } }
public void test() { if ( logger . isWarnEnabled ( ) ) { logger . warn ( "Duplicated async trace scope={}." , oldScope . getName ( ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "start async trace scope" ) ; } }
public ScriptingGauge newGauge ( String name , double initialValue ) { ScriptingGauge scriptingGauge = new ScriptingGauge ( name , initialValue ) ; ActivityMetrics . gauge ( scriptContext , name , scriptingGauge ) ; logger . info ( "registered scripting gauge:" + name ) ; return scriptingGauge ; }
@ Override public int getReactionSchemeCount ( ) { logger . debug ( "Getting reactionScheme count: " , super . getReactionSchemeCount ( ) ) ; return super . getReactionSchemeCount ( ) ; }
public void test() { try { full_graph . newLine ( ) ; code_block = IfStatement ; } catch ( IOException e ) { LOGGER . error ( ERROR_ADDING_DATA , e ) ; } }
public void test() { if ( log . isInfoEnabled ( ) ) { log . info ( "Starting to terminate all members in cluster [" + getClusterId ( ) + "] " + "Network Partition [" + instanceContext . getNetworkPartitionId ( ) + "], Partition [" + partitionContext . getPartitionId ( ) + "]" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Moving pending member [member id] " + memberId + " to obsolete list" ) ; } }
public void test() { if ( event != null ) { FlowCallStack flowCallStack = event . getFlowCallStack ( ) ; code_block = IfStatement ; } else { LOGGER . warn ( "No event on flow completion" , messagingException ) ; } }
@ Transactional ( readOnly = true ) public ClusterCreate getClusterConfig ( String clusterName , boolean needAllocIp ) { ClusterEntity clusterEntity = clusterEntityMgr . findByName ( clusterName ) ; code_block = IfStatement ; ClusterCreate clusterConfig = new ClusterCreate ( ) ; clusterConfig . setName ( clusterEntity . getName ( ) ) ; clusterConfig . setAppManager ( clusterEntity . getAppManager ( ) ) ; clusterConfig . setDistro ( clusterEntity . getDistro ( ) ) ; Map < NetTrafficType , List < ClusterNetConfigInfo > > networkConfigInfo = clusterEntity . getNetworkConfigInfo ( ) ; code_block = IfStatement ; convertClusterConfig ( clusterEntity , clusterConfig , needAllocIp ) ; Gson gson = new GsonBuilder ( ) . excludeFieldsWithoutExposeAnnotation ( ) . create ( ) ; String manifest = gson . toJson ( clusterConfig ) ; logger . debug ( "final cluster spec: " + manifest ) ; return clusterConfig ; }
public void activate ( ) { logger . debug ( "activate()" ) ; }
public void test() { if ( ! resource . canLock ( lockSet . get ( ) ) ) { String errorMessage = getErrorMessage ( resource ) ; LOG . error ( errorMessage ) ; throw new RuntimeException ( errorMessage ) ; } else { lockFn . accept ( resourceName ) ; code_block = IfStatement ; lockSet . set ( resource . setLock ( lockSet . get ( ) ) ) ; return true ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Acquired {} {} lock on resource {}" , lockType , resource . name , resourceName ) ; } }
public void test() { try { ProjectService . Iface client = thriftClients . makeProjectClient ( ) ; project = client . getProjectByIdForEdit ( id , user ) ; usingProjects = client . searchLinkingProjects ( id , user ) ; allUsingProjectCount = client . getCountByProjectId ( id ) ; } catch ( TException e ) { log . error ( "Something went wrong with fetching the project" , e ) ; setSW360SessionError ( request , ErrorMessages . ERROR_GETTING_PROJECT ) ; return ; } }
public void test() { try { putDirectlyLinkedProjectsInRequest ( request , project , user ) ; putDirectlyLinkedReleasesInRequest ( request , project ) ; } catch ( TException e ) { log . error ( "Could not fetch linked projects or linked releases in projects view." , e ) ; return ; } }
public void test() { try { putDirectlyLinkedProjectsInRequest ( request , project , user ) ; putDirectlyLinkedReleasesInRequest ( request , project ) ; } catch ( TException e ) { log . error ( "Could not put empty linked projects or linked releases in projects view." , e ) ; } }
public void test() { if ( LOGGER . isWarnEnabled ( ) ) { LOGGER . warn ( e . toString ( ) , e ) ; } }
@ Post ( path = "admin.api/notices" , subscribeToken = "csrf" ) @ Auth ( roles = "admin" ) public Boundary postNotice ( ) throws InvalidParamException { LOG . trace ( "postNotice" ) ; NoticesEntity entity = HttpUtil . parseJson ( getRequest ( ) , NoticesEntity . class ) ; entity = NoticesLogic . get ( ) . insertNotice ( entity ) ; MessageResult result = new MessageResult ( MessageStatus . Success , HttpStatus . SC_200_OK , getResource ( "message.success.insert" ) , entity . getNo ( ) . toString ( ) ) ; return send ( result ) ; }
public void test() { try { instance . join ( TimeUnit . MINUTES . toMillis ( 5 ) ) ; } catch ( final InterruptedException e ) { log . warn ( e . getMessage ( ) , e ) ; Thread . currentThread ( ) . interrupt ( ) ; } finally { instance = null ; shutdown = null ; } }
public void test() { try { c . close ( ) ; } catch ( final Exception e ) { log . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { System . arraycopy ( loggable . data , 0 , page . getData ( ) , 0 , loggable . size ) ; } catch ( final ArrayIndexOutOfBoundsException e ) { LOG . warn ( "{}; {}; {}; {}" , loggable . data . length , page . getData ( ) . length , ph . getDataLength ( ) , loggable . size ) ; throw e ; } }
public void test() { try { SinglePage page = getSinglePageForRedo ( loggable , loggable . pageNum ) ; code_block = IfStatement ; } catch ( final IOException e ) { LOG . warn ( "An IOException occurred during redo: {}" , e . getMessage ( ) , e ) ; } }
public void test() { try { p . vendorExtensions . put ( "x-codegen-body-example" , mapper . writerWithDefaultPrettyPrinter ( ) . writeValueAsString ( definitions . get ( p . dataType ) . getExample ( ) ) ) ; } catch ( JsonProcessingException e ) { LOGGER . warn ( e . getMessage ( ) , e ) ; } }
public void test() { try { traits = fromJsonString ( PubAnnotationProviderTraits . class , aDocumentRepository . getProperties ( ) ) ; } catch ( IOException e ) { log . error ( "Error while reading traits" , e ) ; } }
@ Modified public void modify ( Map < String , Object > properties ) { Object portletName = properties . get ( "javax.portlet.name" ) ; log . info ( "Portlet [" + ( portletName != null ? portletName : this . getClass ( ) . getSimpleName ( ) ) + "] has been MODIFIED." ) ; }
public void test() { try { mailboxManager . deleteMailbox ( mailboxPath , mailboxSession ) ; } catch ( MailboxNotFoundException e ) { LOGGER . info ( "Attempt to delete mailbox {} for user {} that does not exists" , mailboxPath . getName ( ) , mailboxPath . getUser ( ) ) ; } }
@ Override public void objectInserted ( ObjectInsertedEvent event ) { log . debug ( event ) ; }
public void test() { if ( ActiveMQRALogger . LOGGER . isTraceEnabled ( ) ) { ActiveMQRALogger . LOGGER . trace ( "createConnectionConsumer(" + destination + ", " + pool + ", " + maxMessages + ")" ) ; } }
public void delete ( TmpBauSel persistentInstance ) { log . debug ( "deleting TmpBauSel instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . delete ( persistentInstance ) ; log . debug ( "delete successful" ) ; } catch ( RuntimeException re ) { log . error ( "delete failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . delete ( persistentInstance ) ; log . debug ( "delete successful" ) ; } catch ( RuntimeException re ) { log . error ( "delete failed" , re ) ; throw re ; } }
public void test() { try { return mapper . readValue ( json , ClusterCreate . class ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) ) ; throw e ; } }
public void test() { if ( LOG . isInfoEnabled ( ) ) { LOG . info ( "Model contains Equipment Core data profile in model {}" , m . get ( CgmesNames . FULL_MODEL ) ) ; } }
public void test() { if ( ! getConfig ( ) . isWritable ( ) ) { LOG . debug ( "Not setting up non-writable index set <{}> ({})" , getConfig ( ) . id ( ) , getConfig ( ) . title ( ) ) ; return ; } }
public void test() { if ( isUp ( ) ) { LOG . info ( "Found deflector alias <{}>. Using it." , getWriteIndexAlias ( ) ) ; } else { LOG . info ( "Did not find a deflector alias. Setting one up now." ) ; code_block = TryStatement ;  } }
public void test() { try { final String currentTarget = getNewestIndex ( ) ; LOG . info ( "Pointing to already existing index target <{}>" , currentTarget ) ; pointTo ( currentTarget ) ; } catch ( NoTargetIndexException ex ) { final String msg = "There is no index target to point to. Creating one now." ; LOG . info ( msg ) ; activityWriter . write ( new Activity ( msg , IndexSet . class ) ) ; cycle ( ) ; } }
public void test() { try { Queue queue = getQueue ( subscriberQueuePath , subscriberQueueId ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Unable to update index" , e ) ; } }
public void test() { if ( longRunningProcess == null || longRunningProcess . isDone ( ) ) { code_block = TryStatement ;  } else { getLogger ( ) . info ( "Read from long running process" ) ; } }
public void test() { if ( ! isScheduled ( ) ) { getLogger ( ) . info ( "User stopped processor; will terminate process immediately" ) ; longRunningProcess . cancel ( true ) ; return ; } }
public void test() { if ( flowFile . getSize ( ) == 0L ) { session . remove ( flowFile ) ; } else-if ( failure . get ( ) ) { session . remove ( flowFile ) ; getLogger ( ) . error ( "Failed to read data from Process, so will not generate FlowFile" ) ; } else { flowFile = session . putAttribute ( flowFile , ATTRIBUTE_COMMAND , command ) ; code_block = IfStatement ; session . getProvenanceReporter ( ) . create ( flowFile , "Created from command: " + commandString ) ; getLogger ( ) . info ( "Created {} and routed to success" , new Object [ ] code_block = "" ; ) ; session . transfer ( flowFile , REL_SUCCESS ) ; } }
public void test() { switch ( node . getType ( ) ) { case ACTION : String programName = ( ( WorkflowActionNode ) node ) . getProgram ( ) . getProgramName ( ) ; Resources runnableResources = runnablesResources . get ( programName ) ; code_block = IfStatement ; break ; case FORK : Resources forkResources = ( ( WorkflowForkNode ) node ) . getBranches ( ) . stream ( ) . map ( branches -> findDriverResources ( branches , runnablesResources ) ) . reduce ( this :: mergeForkResources ) . orElse ( resources ) ; resources = maxResources ( resources , forkResources ) ; break ; case CONDITION : Resources branchesResources = maxResources ( findDriverResources ( ( ( WorkflowConditionNode ) node ) . getIfBranch ( ) , runnablesResources ) , findDriverResources ( ( ( WorkflowConditionNode ) node ) . getElseBranch ( ) , runnablesResources ) ) ; resources = maxResources ( resources , branchesResources ) ; break ; default : LOG . warn ( "Ignoring unsupported Workflow node type {}" , node . getType ( ) ) ; } }
void processPublish ( MqttPublishMessage msg ) { final MqttQoS qos = msg . fixedHeader ( ) . qosLevel ( ) ; final String username = NettyUtils . userName ( channel ) ; final String topicName = msg . variableHeader ( ) . topicName ( ) ; final String clientId = getClientId ( ) ; final int messageID = msg . variableHeader ( ) . packetId ( ) ; LOG . trace ( "Processing PUBLISH message, topic: {}, messageId: {}, qos: {}" , topicName , messageID , qos ) ; ByteBuf payload = msg . payload ( ) ; final boolean retain = msg . fixedHeader ( ) . isRetain ( ) ; final Topic topic = new Topic ( topicName ) ; code_block = IfStatement ; code_block = SwitchStatement ; }
public void test() { if ( ! topic . isValid ( ) ) { LOG . debug ( "Drop connection because of invalid topic format" ) ; dropConnection ( ) ; } }
public void test() { switch ( qos ) { case AT_MOST_ONCE : postOffice . receivedPublishQos0 ( topic , username , clientId , payload , retain , msg ) ; break ; code_block = BranchStatement ; code_block = BranchStatement ; default : LOG . error ( "Unknown QoS-Type:{}" , qos ) ; break ; } }
public void test() { if ( resultHandler == null ) { LOG . debug ( "discarding unexpected response [correlation-id: {}]" , correlationId ) ; } else { code_block = TryStatement ;  } }
public void test() { if ( anyMatchingFeature ( features , withName ( name ) ) ) { featuresService . uninstallFeature ( name ) ; logger . debug ( "Uninstalled '{}'" , name ) ; postUninstalledEvent ( name ) ; return true ; } }
public void test() { try { Feature [ ] features = featuresService . listInstalledFeatures ( ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . debug ( "Failed uninstalling '{}': {}" , name , e . getMessage ( ) ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CommercePriceListCommerceAccountGroupRelServiceUtil . class , "deleteCommercePriceListAccountGroupRelsByCommercePriceListId" , _deleteCommercePriceListAccountGroupRelsByCommercePriceListIdParameterTypes1 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , commercePriceListId ) ; code_block = TryStatement ;  } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { boolean available = rs . absolute ( ( int ) pos + 1 ) ; code_block = IfStatement ; } catch ( SQLException e ) { logger . error ( "Error skipping to: " + pos + ": " + e . getMessage ( ) , e ) ; } }
@ Override public void onData ( Stream stream , DataFrame frame , Callback callback ) { byte [ ] bytes = new byte [ frame . getData ( ) . remaining ( ) ] ; frame . getData ( ) . get ( bytes ) ; response = new String ( bytes ) ; LOG . info ( "---------------- Response with content received from '{}' ----------------\n" + "---------------- START Response-Body ----------------\n" + "{}\n" + "---------------- END Response-Body ----------------" , request . getURI ( ) , response ) ; doAssert ( response ) ; callback . succeeded ( ) ; }
public void test() { try { String inputsFileContent = SlangSource . fromFile ( inputFile ) . getContent ( ) ; Boolean emptyContent = true ; code_block = IfStatement ; code_block = IfStatement ; } catch ( RuntimeException ex ) { logger . error ( "Error loading file: " + inputFile + ". Nested exception is: " + ex . getMessage ( ) , ex ) ; throw new RuntimeException ( ex ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "sendWindowChange({}) cols={}, lines={}, height={}, width={}" , this , columns , lines , height , width ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( MDRRuleGroupInstanceServiceUtil . class , "updateRuleGroupInstance" , _updateRuleGroupInstanceParameterTypes5 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , ruleGroupInstanceId , priority ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . mobile . device . rules . model . MDRRuleGroupInstance ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { int affectRow = jdbcTemplate . update ( sql , tenantCapacity . getGmtModified ( ) , tenantCapacity . getTenant ( ) ) ; return affectRow == 1 ; } catch ( CannotGetJdbcConnectionException e ) { FATAL_LOG . error ( "[db-error]" , e ) ; throw e ; } }
public void test() { if ( delete ( eventFile ) ) { logger . info ( "{} Deleted {} event file ({}) due to storage limits" , this , eventFile , FormatUtils . formatDataSize ( fileSize ) ) ; return fileSize ; } else { logger . warn ( "{} Failed to delete oldest event file {}. This file should be cleaned up manually." , this , eventFile ) ; continue ; } }
public void test() { try { transport = new TNonblockingServerSocket ( port ) ; LOG . debug ( "succeeded in binding server to port " + port ) ; break ; } catch ( TTransportException e ) { LOG . debug ( "failed to bind to port " + port ) ; port ++ ; } }
@ Test void testError ( ) { Exception exception = new SLException ( ERROR_MESSAGE ) ; processLogger . error ( ERROR_MESSAGE , exception ) ; verify ( logger , times ( 1 ) ) . error ( ( Object ) ERROR_MESSAGE , exception ) ; assertEquals ( ERROR_MESSAGE , exception . getMessage ( ) ) ; }
public void test() { if ( ! executorService . stop ( pid , signal ) ) { logger . warn ( "Failed to stop command with pid {}" , pid . getPid ( ) ) ; } }
public void test() { try { String clusterName = ( String ) getRequest ( ) . getAttributes ( ) . get ( "clusterName" ) ; String instanceName = ( String ) getRequest ( ) . getAttributes ( ) . get ( "instanceName" ) ; String resourceGroup = ( String ) getRequest ( ) . getAttributes ( ) . get ( "resourceName" ) ; presentation = getInstanceCurrentStateRepresentation ( clusterName , instanceName , resourceGroup ) ; } catch ( Exception e ) { String error = ClusterRepresentationUtil . getErrorAsJsonStringFromException ( e ) ; presentation = new StringRepresentation ( error , MediaType . APPLICATION_JSON ) ; LOG . error ( "" , e ) ; } }
@ Override public synchronized void deleteCompletedLogs ( ) throws IOException { LOG . info ( "Deleting all completed log files..." ) ; long logNumber = UfsJournal . FIRST_COMPLETED_LOG_NUMBER ; code_block = WhileStatement ; code_block = ForStatement ; LOG . info ( "Finished deleting all completed log files." ) ; mNextCompleteLogNumber = UfsJournal . FIRST_COMPLETED_LOG_NUMBER ; }
public void test() { for ( long i = logNumber - 1 ; i >= 0 ; i -- ) { URI log = mJournal . getCompletedLog ( i ) ; LOG . info ( "Deleting completed log: {}" , log ) ; mUfs . deleteFile ( log . toString ( ) ) ; } }
@ Override public synchronized void deleteCompletedLogs ( ) throws IOException { LOG . info ( "Deleting all completed log files..." ) ; long logNumber = UfsJournal . FIRST_COMPLETED_LOG_NUMBER ; code_block = WhileStatement ; code_block = ForStatement ; LOG . info ( "Finished deleting all completed log files." ) ; mNextCompleteLogNumber = UfsJournal . FIRST_COMPLETED_LOG_NUMBER ; }
public void test() { try { RegionSubRegionSnapshot subRegionSnapShot = new RegionSubRegionSnapshot ( subRegion ) ; parentSnapShot . addSubRegion ( subRegionSnapShot ) ; Set subRegions = subRegion . subregions ( false ) ; populateRegionSubRegions ( subRegionSnapShot , subRegions , cache ) ; } catch ( Exception e ) { logger . debug ( "Failed to create snapshot for region: {}. Continuing with next region." , subRegion . getFullPath ( ) , e ) ; } }
public void test() { try { DataBag categReal = ( DataBag ) input . get ( 0 ) ; DataBag categClassif = ( DataBag ) input . get ( 1 ) ; List < String > real = new ArrayList < String > ( ) ; List < String > classif = new ArrayList < String > ( ) ; code_block = ForStatement ; code_block = ForStatement ; int is = intersectSize ( real , classif ) ; double acc = ( double ) is / ( double ) sumSize ( real , classif ) ; double p = ( double ) is / ( double ) classif . size ( ) ; double r = ( double ) is / ( double ) real . size ( ) ; Double f1 = p + r != 0 ? 2 * p * r / ( p + r ) : null ; double hl = sumSize ( subs ( real , classif ) , subs ( classif , real ) ) ; int zol = is == real . size ( ) && is == classif . size ( ) ? 1 : 0 ; Object [ ] obj = new Object [ ] code_block = "" ; ; return TupleFactory . getInstance ( ) . newTuple ( Arrays . asList ( obj ) ) ; } catch ( Exception e ) { logger . error ( "Error in processing input row:" , e ) ; throw new IOException ( "Caught exception processing input row:\n" + StackTraceExtractor . getStackTrace ( e ) ) ; } }
public void test() { if ( LOGGER . isTraceEnabled ( ) ) { LOGGER . trace ( format ( "Encode raw '%s' tuple2 object %s" , fieldName , o ) ) ; } }
public void test() { if ( requestDataWriter == null ) { RecordLog . warn ( "[DefaultRequestEntityWriter] Cannot find matching request writer for type <{}>," + " dropping the request" , type ) ; return ; } }
public void sync ( ) { code_block = IfStatement ; syncInProgress = true ; log . warn ( "================================ sync !!! ==============================" ) ; code_block = TryStatement ;  syncInProgress = false ; log . info ( "Sync completed" ) ; }
public void test() { try { code_block = ForStatement ; List < PinDefinition > list = getPinList ( ) ; code_block = ForStatement ; } catch ( Exception e ) { log . error ( "sync threw" , e ) ; } }
public void sync ( ) { code_block = IfStatement ; syncInProgress = true ; log . warn ( "================================ sync !!! ==============================" ) ; code_block = TryStatement ;  syncInProgress = false ; log . info ( "Sync completed" ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "writeElement: " + sbmlElementToWrite . getClass ( ) . getSimpleName ( ) ) ; } }
public void test() { try { documentURI = new URI ( documentURL ) ; } catch ( URISyntaxException e ) { String message = "Document URL Syntax error:" + documentURL ; log . warn ( message , e ) ; throw new DataServiceFault ( e , message ) ; } }
public void test() { try ( final Tx tx = app . tx ( ) ) { code_block = WhileStatement ; tx . success ( ) ; } catch ( FrameworkException fex ) { logger . warn ( fex . getMessage ( ) ) ; logger . warn ( ExceptionUtils . getStackTrace ( fex ) ) ; } }
public void test() { try { PartnerConnection partnerConnection = getPartnerConnection ( ) ; ConnectorConfig connectorConfig = partnerConnection . getConfig ( ) ; Map < String , Object > options = new HashMap < > ( ) ; options . put ( ClientTransport . MAX_NETWORK_DELAY_OPTION , _transportTimeout * 6000 ) ; _httpClient . start ( ) ; URL url = new URL ( connectorConfig . getServiceEndpoint ( ) ) ; _bayeuxClient = new BayeuxClient ( StringBundler . concat ( url . getProtocol ( ) , "://" , url . getHost ( ) , "/cometd/37.0" ) , new SalesforceTransport ( connectorConfig . getSessionId ( ) , options , _httpClient ) ) ; ClientSessionChannel handshakeClientSessionChannel = _bayeuxClient . getChannel ( Channel . META_HANDSHAKE ) ; handshakeClientSessionChannel . addListener ( new SalesforceMessageListener ( ) ) ; ClientSessionChannel connectClientSessionChannel = _bayeuxClient . getChannel ( Channel . META_CONNECT ) ; connectClientSessionChannel . addListener ( new SalesforceMessageListener ( ) ) ; ClientSessionChannel subscribeClientSessionChannel = _bayeuxClient . getChannel ( Channel . META_SUBSCRIBE ) ; subscribeClientSessionChannel . addListener ( new SalesforceMessageListener ( ) ) ; } catch ( Exception exception ) { _log . error ( exception . getMessage ( ) , exception ) ; } }
public void test() { if ( configurationResourceManager != null && StringUtils . isNotEmptyTrimmed ( configResourceUri ) && StringUtils . isNotEmptyTrimmed ( schemaResourceName ) ) { log . info ( "Register resource and observer for config resource uri {}" , configResourceUri ) ; String schema = ResourcesUtils . loadResource ( schemaResourceName , true ) ; configurationResourceManager . registerResource ( configResourceUri , schema ) ; configurationResourceManager . registerObserver ( this , configResourceUri ) ; } else { log . warn ( "No configuration resource manager and/or no configuration resource uri and/or no schema " + "resource name defined. Not using this feature in this case" ) ; } }
public void test() { if ( configurationResourceManager != null && StringUtils . isNotEmptyTrimmed ( configResourceUri ) && StringUtils . isNotEmptyTrimmed ( schemaResourceName ) ) { log . info ( "Register resource and observer for config resource uri {}" , configResourceUri ) ; String schema = ResourcesUtils . loadResource ( schemaResourceName , true ) ; configurationResourceManager . registerResource ( configResourceUri , schema ) ; configurationResourceManager . registerObserver ( this , configResourceUri ) ; } else { log . warn ( "No configuration resource manager and/or no configuration resource uri and/or no schema " + "resource name defined. Not using this feature in this case" ) ; } }
private synchronized void nextGeneration ( ) throws IOException { close ( ) ; processFile = new File ( rootDir , prefix + IN_PROCESS . getFileNameSuffix ( ) ) ; writer = newWriter ( processFile , UTF_8 ) ; LOG . info ( "Created new process file and writer over {} " , processFile . getAbsolutePath ( ) ) ; }
public void test() { try { code_block = ForStatement ; code_block = IfStatement ; } catch ( final Exception e ) { LOG . debug ( "Exception while onResourceChange is invoked" , e ) ; } }
public void test() { try { bundleResponse . addStatusMessage ( "server started" ) ; bundleResponse . addStatusMessage ( "queueing" ) ; _executorService . execute ( new BuildThread ( bundleRequest , bundleResponse ) ) ; _buildMap . put ( id , bundleResponse ) ; final StringWriter sw = new StringWriter ( ) ; final MappingJsonFactory jsonFactory = new MappingJsonFactory ( ) ; final JsonGenerator jsonGenerator = jsonFactory . createJsonGenerator ( sw ) ; _mapper . writeValue ( jsonGenerator , bundleResponse ) ; response = Response . ok ( sw . toString ( ) ) . build ( ) ; } catch ( Exception any ) { _log . error ( "execption in build:" , any ) ; response = Response . serverError ( ) . build ( ) ; } }
public void test() { try { List < Future < String > > futures = getRepositoryStatistics ( con ) ; size = getResult ( "repository size." , futures . get ( 0 ) ) ; numContexts = getResult ( "labeled contexts." , futures . get ( 1 ) ) ; } catch ( InterruptedException e ) { LOGGER . warn ( "Interrupted while requesting repository statistics." , e ) ; } }
public void test() { if ( ActiveMQRALogger . LOGGER . isTraceEnabled ( ) ) { ActiveMQRALogger . LOGGER . trace ( "setFloat(" + name + ", " + value + ")" ) ; } }
public void test() { default : { logger . warn ( "unknown produce code {}" , code ) ; return JoyQueueCode . CN_NO_PERMISSION ; } }
public void test() { try { code_block = IfStatement ; } catch ( SecurityException e ) { LOG . error ( String . format ( "SecurityException while processing Java system property for propertyField=%s" , AtlasModelFactory . toString ( propertyField ) ) , e ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( OBJECT_MAPPER . writeValueAsString ( metrics ) ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( JsonProcessingException e ) { LOG . error ( "Unable to marshal data to JSON" , e ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "initChannel(" + ch + ") listener=" + listener + " ignoring abort event exception" , exc ) ; } }
public boolean runReboot ( ) { logger . trace ( "runReboot() called on {}" , getThing ( ) . getUID ( ) ) ; RunReboot bcp = thisBridge . bridgeAPI ( ) . runReboot ( ) ; code_block = IfStatement ; return false ; }
public void test() { if ( thisBridge . bridgeCommunicate ( bcp ) ) { logger . info ( "Reboot command {}sucessfully sent to {}" , bcp . isCommunicationSuccessful ( ) ? "" : "un" , getThing ( ) . getUID ( ) ) ; } }
public void samplePoint ( Integer x , Integer y ) { log . info ( "Sample point called " + x + " " + y ) ; }
public void test() { if ( cpuCoresLong != cpuCoresDouble ) { LOG . info ( "The amount of cpu cores must be a positive integer on Yarn. Rounding {} up to the closest positive integer {}." , cpuCoresDouble , cpuCoresLong ) ; } }
public void test() { if ( fromNode . getChild ( 0 ) . getType ( ) == TOK_SUBQUERY ) { log . warn ( "Subqueries in from clause not supported by {} Query : {}" , this , this . query ) ; throw new LensException ( "Subqueries in from clause not supported by " + this + " Query : " + this . query ) ; } else-if ( isOfTypeJoin ( fromNode . getChild ( 0 ) . getType ( ) ) ) { log . warn ( "Join in from clause not supported by {} Query : {}" , this , this . query ) ; throw new LensException ( "Join in from clause not supported by " + this + " Query : " + this . query ) ; } }
public void test() { if ( fromNode . getChild ( 0 ) . getType ( ) == TOK_SUBQUERY ) { log . warn ( "Subqueries in from clause not supported by {} Query : {}" , this , this . query ) ; throw new LensException ( "Subqueries in from clause not supported by " + this + " Query : " + this . query ) ; } else-if ( isOfTypeJoin ( fromNode . getChild ( 0 ) . getType ( ) ) ) { log . warn ( "Join in from clause not supported by {} Query : {}" , this , this . query ) ; throw new LensException ( "Join in from clause not supported by " + this + " Query : " + this . query ) ; } }
public void test() { try { buildDruidQuery ( conf , metastoreConf ) ; rewritternQueryText = rewrittenQuery . toString ( ) ; log . info ( "Rewritten query from build : " + rewritternQueryText ) ; } catch ( SemanticException e ) { throw new LensException ( e ) ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( "clearing JMX Cache" + StringUtils . stringifyException ( new Exception ( ) ) ) ; } }
@ ExceptionHandler ( Exception . class ) public ModelAndView handleError ( HttpServletRequest req , Exception exception ) { log . error ( "Request: {} raised following exception." , req . getRequestURL ( ) , exception ) ; ModelAndView mav = new ModelAndView ( ) ; mav . addObject ( "exception" , exception ) ; mav . setViewName ( "00-error" ) ; return mav ; }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( Thread . currentThread ( ) . getName ( ) + " points:" + uploadCounter . get ( ) + " uploaded before error, now release the lock." ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( Thread . currentThread ( ) . getName ( ) + " no more points, total points:" + uploadCounter . get ( ) + "  uploaded" ) ; } }
public void test() { if ( request != null ) { log . debug ( "This request is not a CommandHostRequest. Ignoring request for URI {}" , request . getURI ( ) ) ; } else { log . debug ( "This request is not a CommandHostRequest. Ignoring null request" ) ; } }
public void test() { if ( request != null ) { log . debug ( "This request is not a CommandHostRequest. Ignoring request for URI {}" , request . getURI ( ) ) ; } else { log . debug ( "This request is not a CommandHostRequest. Ignoring null request" ) ; } }
public void test() { if ( histogram != null ) { return histogram . getValue ( ) . get ( new Resolution ( new double [ ] code_block = "" ; ) ) ; } else { LOGGER . warn ( "Cannot find histogram for coverage '" + coverageName + "'" ) ; } }
@ Test public void testSimpleAttributePath2 ( ) throws Exception { AttributePathServiceTest . LOG . debug ( "start simple attribute path test 2" ) ; apstUtils . createAndPersistDefaultObject ( ) ; AttributePathServiceTest . LOG . debug ( "end simple attribute path test 2" ) ; }
@ Test public void testSimpleAttributePath2 ( ) throws Exception { AttributePathServiceTest . LOG . debug ( "start simple attribute path test 2" ) ; apstUtils . createAndPersistDefaultObject ( ) ; AttributePathServiceTest . LOG . debug ( "end simple attribute path test 2" ) ; }
public void test() { if ( ! task . condition ( ) . isPresent ( ) || task . condition ( ) . get ( ) . shouldExecuteTask ( ) ) { logger . info ( "Executing task - " + task . getShortDescription ( ) ) ; code_block = TryStatement ;  } else { logger . info ( "NOT Executing task - " + task . getShortDescription ( ) + ". Conditions not met." ) ; } }
public void test() { try { task . doUpgrade ( ) ; } catch ( UpgradeProblem problem ) { logger . error ( "Problem executing upgrade task" , problem ) ; } }
public void test() { if ( ! task . condition ( ) . isPresent ( ) || task . condition ( ) . get ( ) . shouldExecuteTask ( ) ) { logger . info ( "Executing task - " + task . getShortDescription ( ) ) ; code_block = TryStatement ;  } else { logger . info ( "NOT Executing task - " + task . getShortDescription ( ) + ". Conditions not met." ) ; } }
public void test() { try { outputObject . put ( JsonKeys . updateType . name ( ) , "DeleteModel" ) ; pw . println ( outputObject . toString ( ) ) ; } catch ( JSONException e ) { logger . error ( "Error occured while generating JSON!" ) ; } }
@ Override public void onServoSetSpeed ( ServoControl servo ) { int speed = - 1 ; code_block = IfStatement ; log . info ( "servoSetVelocity {} id {} velocity {}" , servo . getName ( ) , getDeviceId ( servo ) , speed ) ; Integer i = getDeviceId ( servo ) ; code_block = IfStatement ; msg . servoSetVelocity ( i , speed ) ; }
public void test() { if ( i == null ) { log . error ( "{} has null deviceId" , servo ) ; return ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( e . getMessage ( ) ) ; } }
public void test() { if ( ! file . delete ( ) ) { LOG . warn ( "Unable to delete repository lock file" ) ; } }
public void test() { try { System . getProperties ( ) . remove ( identifier ) ; } catch ( SecurityException e ) { LOG . error ( "Unable to clear system property: " + identifier , e ) ; } }
public void test() { if ( mapperException == null ) { mapperException = t ; } else { LOG . error ( "Suppressing exception from closing tables" , t ) ; } }
public void test() { try { reportingAdminServiceStub . copySavedReport ( saved , copy ) ; } catch ( Exception e ) { String msg = "Unable to copy the report" ; log . error ( msg ) ; throw new Exception ( msg , e ) ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( "Send response to request {} for topic {}" , requestId , topicName ( partitionId ) ) ; } }
public void test() { if ( completableFuture != null ) { code_block = IfStatement ; completableFuture . complete ( bytes ) ; } else-if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( "Wasn't able to send response to request {} for topic {}" , requestId , topicName ( partitionId ) ) ; } }
public void registerException ( final String method , final String path , final int code , final String error ) { final String key = method . toUpperCase ( ) + " " + path ; logger . info ( "Registering error: {}, code: {}, body:\n{}" , key , code , error ) ; this . errors . put ( key , new ContentResponse ( method , path , code , error ) ) ; }
public void test() { try ( Connection connection = this . connection ; PreparedStatement statement = this . statement ) { connection . rollback ( ) ; } catch ( SQLException e ) { log . debug ( e , "SQLException when abort" ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "RowKeyComparisonFilter: " + ( this . keepRow ? "KEEP" : "FILTER" ) + " row " + inputTuple ) ; } }
public void test() { if ( name == null ) { logger . error ( "getDoubleProperty(): argument 'name' must be non-null" ) ; return new DoublePropertyImpl ( "" ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "add margins to extent " + extent ) ; } }
public void test() { if ( grouping . length > 0 && tableConfig . getMinIdleStateRetentionTime ( ) < 0 ) { LOG . warn ( "No state retention interval configured for a query which accumulates state. " + "Please provide a query configuration with valid retention interval to prevent excessive " + "state size. You may specify a retention time of 0 to not clean up the state." ) ; } }
public void test() { if ( LOGGER . isInfoEnabled ( ) ) { LOGGER . info ( "Dropped a pending dataverse: " + dataverse . getDataverseName ( ) ) ; } }
private Dataset < Row > doOperation ( final GetDataFrameOfElements operation , final ParquetStore store , final SparkSession spark ) throws OperationException { code_block = IfStatement ; LOGGER . debug ( "Creating a Dataset<Row> from path {} with option mergeSchema=true" , store . getGraphPath ( ) ) ; final StructType schema = new SchemaUtils ( store . getSchema ( ) ) . getMergedSparkSchema ( store . getSchema ( ) . getGroups ( ) ) ; final Dataset < Row > dataframe = spark . read ( ) . schema ( schema ) . parquet ( store . getGraphPath ( ) ) ; return dataframe ; }
public void test() { if ( results . size ( ) > 1 ) { log . debug ( "Found more than one domain in repository {}, using first one." , session :: getRepositoryName ) ; } }
public static String getDefaultDomainPath ( CoreSession session ) { String query = "SELECT * FROM Document where ecm:primaryType = 'Domain'" ; DocumentModelList results = session . query ( query ) ; code_block = IfStatement ; code_block = IfStatement ; DocumentModel defaultDomain = results . get ( 0 ) ; String defaultDomainPath = defaultDomain . getPathAsString ( ) ; log . debug ( "Using default domain {}" , defaultDomainPath ) ; return defaultDomainPath ; }
public void test() { if ( ! response . getBody ( ) . getObject ( ) . getBoolean ( "success" ) ) { Logger . debug ( response . getBody ( ) . toString ( ) ) ; throw new Exception ( response . getBody ( ) . getObject ( ) . getJSONObject ( "data" ) . getString ( "error" ) ) ; } }
@ Override public void open ( FileInputSplit split ) throws IOException { super . open ( split ) ; this . wrapper = InstantiationUtil . instantiate ( avroWrapperTypeClass , AvroBaseValue . class ) ; DatumReader < E > datumReader ; code_block = IfStatement ; LOG . info ( "Opening split " + split ) ; SeekableInput in = new FSDataInputStreamWrapper ( stream , ( int ) split . getLength ( ) ) ; dataFileReader = DataFileReader . openReader ( in , datumReader ) ; dataFileReader . sync ( split . getStart ( ) ) ; reuseAvroValue = null ; }
public void test() { try { Futures . transform ( Futures . catchingAsync ( future , Throwable . class , Futures :: immediateFailedFuture , MoreExecutors . directExecutor ( ) ) , flowCapNodeOpt code_block = LoopStatement ; , MoreExecutors . directExecutor ( ) ) . get ( ) ; } catch ( InterruptedException | ExecutionException ex ) { LOG . debug ( "Failed to delete {} flows" , deviceFlowRegistry . size ( ) , ex ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { log . error ( "Failed to schedule task for class " + taskDefinition . getTaskClass ( ) , e ) ; } }
public void test() { try { raf . seek ( start ) ; raf . readFully ( data ) ; } catch ( IOException e ) { log . warn ( "Error reading from file" , e ) ; abort ( ) ; } }
public void test() { if ( failures > 0 ) { log . info ( "Recieved ping for [{}]" , agentId ) ; } }
protected void activate ( ComponentContext componentContext , Map < String , Object > properties ) { logger . info ( "activate..." ) ; this . options = new SslManagerServiceOptions ( properties ) ; this . sslContexts = new ConcurrentHashMap < > ( ) ; ServiceTracker < SslServiceListener , SslServiceListener > listenersTracker = new ServiceTracker < > ( componentContext . getBundleContext ( ) , SslServiceListener . class , null ) ; this . sslServiceListeners = new SslServiceListeners ( listenersTracker ) ; }
@ PreAuthorize ( "hasPermission(id, '" + AclClassName . Values . LAYOUT + "', '" + PermissionName . Values . LAYOUT_DELETE + "')" ) public void deleteLayout ( final Long id ) { log . debug ( "deleteLayout() - id={}" , id ) ; layoutService . delete ( id ) ; }
public void test() { try { bean . unsetEntityContext ( ) ; } catch ( final Exception e ) { logger . info ( getClass ( ) . getName ( ) + ".freeInstance: ignoring exception " + e + " on bean instance " + bean ) ; } finally { callContext . setCurrentOperation ( currentOp ) ; } }
public void test() { try { code_block = IfStatement ; super . messageReceived ( ctx , e ) ; } catch ( Exception ex ) { LOGGER . error ( "Exception while processing message in RelayStatisticsCollectingHandler" ) ; } }
public void test() { try { execution . setMessage ( doExecute ( dryRun , executor , context ) ) ; execution . setStatus ( TaskJob . Status . SUCCESS . name ( ) ) ; result = AuditElements . Result . SUCCESS ; } catch ( JobExecutionException e ) { LOG . error ( "While executing task {}" , taskKey , e ) ; result = AuditElements . Result . FAILURE ; execution . setMessage ( ExceptionUtils2 . getFullStackTrace ( e ) ) ; execution . setStatus ( TaskJob . Status . FAILURE . name ( ) ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( final InterruptedException e ) { logger . warn ( "Error shutting down user group refresh scheduler due to {}" , e . getMessage ( ) , e ) ; } }
public void test() { try { final Rule rule = new Rule ( stream , streamRule , stream . getMatchingType ( ) ) ; match . addRule ( rule ) ; } catch ( InvalidStreamRuleTypeException e ) { LOG . warn ( "Invalid stream rule type. Skipping matching for this rule. " + e . getMessage ( ) , e ) ; } }
public void test() { if ( key instanceof ConfigKeySelfExtracting ) { code_block = IfStatement ; } else { LOG . warn ( "Unexpected key type " + key + " (" + key . getClass ( ) + ") in " + bo + "; ignoring value" ) ; return Maybe . absent ( ) ; } }
public void test() { if ( ! result ) { logger . error ( "Error moving log file to archive folder. Unable to rename" + logFile + " to archiveFile=" + archiveFile ) ; } }
public void test() { try { logFile = new File ( indexRecord . filePath ) ; String fileName = logFile . getName ( ) ; archiveFile = new File ( archiveFolder , fileName ) ; logger . info ( "Moving logFile " + logFile + " to " + archiveFile ) ; boolean result = logFile . renameTo ( archiveFile ) ; code_block = IfStatement ; } catch ( Throwable t ) { logger . error ( "Error moving log file to archive folder. logFile=" + logFile + ", archiveFile=" + archiveFile , t ) ; } }
public void test() { if ( ! ret ) { logger . error ( "Error deleting archive file. archiveFile=" + archiveFile ) ; } }
public void test() { try { File [ ] logFiles = archiveFolder . listFiles ( new FileFilter ( ) code_block = "" ; ) ; code_block = IfStatement ; } catch ( Throwable t ) { logger . error ( "Error deleting older archive file. archiveFile=" + archiveFile , t ) ; } }
private void enableAZ ( BaragonAgentMetadata agent , String availabilityZone , LoadBalancerDescription elb ) { LOG . info ( "Enabling availability zone {} in preparation for agent {}" , availabilityZone , agent . getAgentId ( ) ) ; List < String > availabilityZones = elb . getAvailabilityZones ( ) ; availabilityZones . add ( availabilityZone ) ; EnableAvailabilityZonesForLoadBalancerRequest request = new EnableAvailabilityZonesForLoadBalancerRequest ( ) ; request . setAvailabilityZones ( availabilityZones ) ; request . setLoadBalancerName ( elb . getLoadBalancerName ( ) ) ; elbClient . enableAvailabilityZonesForLoadBalancer ( request ) ; }
public void test() { try { log . debug ( "handle message - for operational environment notification received: {}" , notification ) ; Gson gsonObj = new GsonBuilder ( ) . create ( ) ; IDmaapNotificationData notificationData = gsonObj . fromJson ( notification , DmaapNotificationDataImpl . class ) ; IDmaapAuditNotificationData auditNotificationData = gsonObj . fromJson ( notification , DmaapNotificationDataImpl . class ) ; AuditingActionEnum actionEnum ; code_block = SwitchStatement ; componentUtils . auditEnvironmentEngine ( actionEnum , notificationData . getOperationalEnvironmentId ( ) , notificationData . getOperationalEnvironmentType ( ) . getEventTypenName ( ) , notificationData . getAction ( ) . getActionName ( ) , auditNotificationData . getOperationalEnvironmentName ( ) , auditNotificationData . getTenantContext ( ) ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; } catch ( Exception e ) { log . debug ( "handle message for operational environment failed for notification: {} with error :{}" , notification , e . getMessage ( ) , e ) ; errorWrapper . setInnerElement ( false ) ; } }
protected void generateUDTPojos ( SchemaDefinition schema ) { log . info ( "Generating UDT POJOs" ) ; code_block = ForStatement ; watch . splitInfo ( "UDT POJOs generated" ) ; }
public void test() { try { generateUDTPojo ( udt ) ; } catch ( Exception e ) { log . error ( "Error while generating UDT POJO " + udt , e ) ; } }
public void test() { try { URI srcUri = new URI ( src ) , destUri = new URI ( dest ) ; Configuration config = new Configuration ( ) ; config . set ( "fs.default.name" , srcUri . resolve ( "/" ) . toString ( ) ) ; FileSystem dfs = FileSystem . get ( config ) ; Path destPath = new Path ( destUri . toString ( ) ) ; FileStatus [ ] files = dfs . listStatus ( new Path ( srcUri . toString ( ) ) ) ; if ( files == null || files . length == 0 ) return false ; code_block = ForStatement ; return true ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; return false ; } }
public void test() { if ( args . size ( ) != 2 ) { startupLogger . error ( "setup-admin-user requires two args (username and password), exiting" ) ; return ; } }
public void test() { if ( args . size ( ) != 2 ) { startupLogger . error ( "execute-range-deletes requires two args (partial table name" + " and rollup level), exiting" ) ; return ; } }
public void test() { if ( commandName . equals ( "setup-admin-user" ) ) { code_block = IfStatement ; command = Tools :: setupAdminUser ; } else-if ( commandName . equals ( "truncate-all-data" ) ) { code_block = IfStatement ; command = Tools :: truncateAllData ; } else-if ( commandName . equals ( "execute-range-deletes" ) ) { code_block = IfStatement ; String partialTableName = args . get ( 0 ) ; code_block = IfStatement ; command = Tools :: executeDeletes ; } else { startupLogger . error ( "unexpected command '{}', exiting" , commandName ) ; return ; } }
static void runCommand ( String commandName , List < String > args ) throws Exception { Directories directories = new Directories ( getCentralDir ( ) ) ; initLogging ( directories . getConfDir ( ) , directories . getLogDir ( ) ) ; Command command ; code_block = IfStatement ; String version = Version . getVersion ( CentralModule . class ) ; startupLogger . info ( "Glowroot version: {}" , version ) ; startupLogger . info ( "Java version: {}" , StandardSystemProperty . JAVA_VERSION . value ( ) ) ; CentralConfiguration centralConfig = getCentralConfiguration ( directories . getConfDir ( ) ) ; Session session = null ; Cluster cluster = null ; boolean success ; code_block = TryStatement ;  code_block = IfStatement ; }
public void test() { if ( initialSchemaVersion == null ) { startupLogger . info ( "creating glowroot central schema..." ) ; } else-if ( initialSchemaVersion != schemaUpgrade . getCurrentSchemaVersion ( ) ) { startupLogger . warn ( "running a version of glowroot central that does not match the" + " glowroot central schema version (expecting glowroot central schema" + " version {} but found version {}), exiting" , schemaUpgrade . getCurrentSchemaVersion ( ) , initialSchemaVersion ) ; return ; } }
public void test() { if ( initialSchemaVersion == null ) { startupLogger . info ( "creating glowroot central schema..." ) ; } else-if ( initialSchemaVersion != schemaUpgrade . getCurrentSchemaVersion ( ) ) { startupLogger . warn ( "running a version of glowroot central that does not match the" + " glowroot central schema version (expecting glowroot central schema" + " version {} but found version {}), exiting" , schemaUpgrade . getCurrentSchemaVersion ( ) , initialSchemaVersion ) ; return ; } }
public void test() { if ( success ) { startupLogger . info ( "{} completed successfully" , commandName ) ; } }
@ Override protected void hibernateMigrate ( ) throws DataMigrationException , XWikiException { XWikiContext context = this . getXWikiContext ( ) ; logger . info ( "Starting WatchlistLeftoversCleaner on wiki [{}]." , context . getWikiId ( ) ) ; code_block = TryStatement ;  logger . info ( "End of WatchlistLeftoversCleaner on wiki [{}]." , context . getWikiId ( ) ) ; }
public void test() { if ( mWorkerConnectWaitStartTimeMs . compareAndSet ( startTime , null , true , false ) ) { LOG . debug ( "Exiting safe mode." ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Assert container asserting exceptions of type " + exception ) ; } }
public void test() { try { TestAction action = this . action . build ( ) ; setActiveAction ( action ) ; action . execute ( context ) ; } catch ( Exception e ) { log . debug ( "Validating caught exception ..." ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "Assert exception validation successful: All values OK" ) ; return ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Asserted exception is as expected: " + e . getClass ( ) + ": " + e . getLocalizedMessage ( ) ) ; } }
public void test() { try { TestAction action = this . action . build ( ) ; setActiveAction ( action ) ; action . execute ( context ) ; } catch ( Exception e ) { log . debug ( "Validating caught exception ..." ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "Assert exception validation successful: All values OK" ) ; return ; } }
public void test() { if ( _rejectedExecutionHandler == null ) { _rejectedExecutionHandler = _createRejectionExecutionHandler ( ) ; } }
public void test() { if ( oldConfig != null && ! typeToIdentityStore . containsValue ( oldConfig ) ) { final IdentityStore store = identityStores . remove ( oldConfig ) ; code_block = IfStatement ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( this . getType ( ) . typeName ( ) + ": no fields have been set to index." ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( this . getType ( ) . typeName ( ) + ": no fields have been set to reverse index." ) ; } }
public void test() { try { securityHeader = APIUtil . getOAuthConfigurationFromAPIMConfig ( APIConstants . AUTHORIZATION_HEADER ) ; code_block = IfStatement ; } catch ( APIManagementException e ) { log . error ( "Error while reading authorization header from APIM configurations" , e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( KaleoProcessServiceUtil . class , "deleteKaleoProcess" , _deleteKaleoProcessParameterTypes1 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , kaleoProcessId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . portal . workflow . kaleo . forms . model . KaleoProcess ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { code_block = IfStatement ; } catch ( Throwable t ) { logger . error ( "Error while checking page for widget '{}'" , viewerWidgetCode , t ) ; } }
@ Override public void handleNewSession ( final String sessionId ) throws Exception { zkClient . waitUntilConnected ( HelixZkClient . DEFAULT_CONNECTION_TIMEOUT , TimeUnit . SECONDS ) ; LOG . info ( "handleNewSession. sessionId: {}." , sessionId ) ; waitNewSession . countDown ( ) ; }
public void test() { try { return Response . ok ( "" + service . getEventCount ( ) ) . build ( ) ; } catch ( UnauthorizedException e ) { throw e ; } catch ( Exception e ) { logger . error ( "Unable to get the event count" , e ) ; throw new WebApplicationException ( Response . Status . INTERNAL_SERVER_ERROR ) ; } }
public void test() { try { factory . setProperty ( XMLConstants . ACCESS_EXTERNAL_DTD , "" ) ; } catch ( SAXException e ) { LOG . warn ( "Property '{}' cannot be set in the current SchemaFactory: {}" , XMLConstants . ACCESS_EXTERNAL_DTD , factory . getClass ( ) . getName ( ) , e ) ; } }
public void test() { try { factory . setProperty ( XMLConstants . ACCESS_EXTERNAL_SCHEMA , "" ) ; } catch ( SAXException e ) { LOG . warn ( "Property '{}' cannot be set in the current SchemaFactory: {}" , XMLConstants . ACCESS_EXTERNAL_SCHEMA , factory . getClass ( ) . getName ( ) , e ) ; } }
@ VisibleForTesting void setAsyncRequestInstanceSecondStep ( SuccessfulAssociateIpAddressResponse response , AsyncRequestInstanceState asyncRequestInstanceState , String createFirewallRuleJobId ) { SuccessfulAssociateIpAddressResponse . IpAddress ipAddress = response . getIpAddress ( ) ; String ipAddressId = ipAddress . getId ( ) ; String ip = ipAddress . getIpAddress ( ) ; asyncRequestInstanceState . setIpInstanceId ( ipAddressId ) ; asyncRequestInstanceState . setIp ( ip ) ; asyncRequestInstanceState . setCurrentJobId ( createFirewallRuleJobId ) ; asyncRequestInstanceState . setState ( AsyncRequestInstanceState . StateType . CREATING_FIREWALL_RULE ) ; LOGGER . info ( String . format ( Messages . Log . ASYNCHRONOUS_PUBLIC_IP_STATE_S , asyncRequestInstanceState . getOrderInstanceId ( ) , AsyncRequestInstanceState . StateType . CREATING_FIREWALL_RULE ) ) ; }
public void test() { try { DiscoveryResult discoveryResult = DiscoveryResultBuilder . create ( thingUID ) . withBridge ( accountUID ) . withLabel ( PropertyUtils . getPropertyValue ( device , "common.attributes.name.value" , String . class ) ) . withProperty ( "id" , device . id ) . withProperty ( "type" , device . deviceType ) . withRepresentationProperty ( "id" ) . build ( ) ; thingDiscovered ( discoveryResult ) ; } catch ( GardenaException ex ) { logger . warn ( "{}" , ex . getMessage ( ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( StringUtils . EMPTY , exception ) ; } }
@ Override public void complete ( ) throws IOException { LOG . debug ( "Completing session: {}" , id ) ; delete ( ) ; }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceApplicationModelServiceUtil . class , "getCommerceApplicationModel" , _getCommerceApplicationModelParameterTypes2 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , commerceApplicationModelId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . commerce . application . model . CommerceApplicationModel ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { notebook . reloadAllNotes ( context . getAutheInfo ( ) ) ; } catch ( IOException e ) { LOGGER . error ( "Fail to reload notes from repository" , e ) ; } }
public void test() { if ( includeSamples == null ) { logger . info ( "Execute leaf-node '{}'" , node ) ; } else { logger . info ( "Execute leaf-node '{}' for {} samples" , node , includeSamples . size ( ) ) ; } }
public void test() { if ( includeSamples == null ) { logger . info ( "Execute leaf-node '{}'" , node ) ; } else { logger . info ( "Execute leaf-node '{}' for {} samples" , node , includeSamples . size ( ) ) ; } }
public void test() { try { return Optional . ofNullable ( pulsar . getTopicPoliciesService ( ) . getTopicPolicies ( topicName ) ) . map ( TopicPolicies :: getBackLogQuotaMap ) . map ( map -> map . get ( BacklogQuotaType . destination_storage . name ( ) ) ) . orElseGet ( ( ) -> getBacklogQuota ( topicName . getNamespace ( ) , policyPath ) ) ; } catch ( Exception e ) { log . warn ( "Failed to read topic policies data, will apply the namespace backlog quota: topicName={}" , topicName , e ) ; } }
public void test() { try { return getObjectMapper ( ) . readValue ( json , classOfT ) ; } catch ( IOException ex ) { log . error ( "Can't deserialize json object (may-be incompatible ProjectForge versions): " + ex . getMessage ( ) + " json=" + json , ex ) ; return null ; } }
private void runIteration ( Random rng , Data data , int m , int nbtrees ) { log . info ( "Splitting the data" ) ; Data train = data . clone ( ) ; Data test = train . rsplit ( rng , ( int ) ( data . size ( ) * 0.1 ) ) ; DefaultTreeBuilder treeBuilder = new DefaultTreeBuilder ( ) ; SequentialBuilder forestBuilder = new SequentialBuilder ( rng , treeBuilder , train ) ; treeBuilder . setM ( m ) ; long time = System . currentTimeMillis ( ) ; log . info ( "Growing a forest with m={}" , m ) ; DecisionForest forestM = forestBuilder . build ( nbtrees ) ; sumTimeM += System . currentTimeMillis ( ) - time ; numNodesM += forestM . nbNodes ( ) ; treeBuilder . setM ( 1 ) ; time = System . currentTimeMillis ( ) ; log . info ( "Growing a forest with m=1" ) ; DecisionForest forestOne = forestBuilder . build ( nbtrees ) ; sumTimeOne += System . currentTimeMillis ( ) - time ; numNodesOne += forestOne . nbNodes ( ) ; double [ ] testLabels = test . extractLabels ( ) ; double [ ] predictions = new double [ test . size ( ) ] ; forestM . classify ( test , predictions ) ; sumTestErrM += ErrorEstimate . errorRate ( testLabels , predictions ) ; forestOne . classify ( test , predictions ) ; sumTestErrOne += ErrorEstimate . errorRate ( testLabels , predictions ) ; }
private void runIteration ( Random rng , Data data , int m , int nbtrees ) { log . info ( "Splitting the data" ) ; Data train = data . clone ( ) ; Data test = train . rsplit ( rng , ( int ) ( data . size ( ) * 0.1 ) ) ; DefaultTreeBuilder treeBuilder = new DefaultTreeBuilder ( ) ; SequentialBuilder forestBuilder = new SequentialBuilder ( rng , treeBuilder , train ) ; treeBuilder . setM ( m ) ; long time = System . currentTimeMillis ( ) ; log . info ( "Growing a forest with m={}" , m ) ; DecisionForest forestM = forestBuilder . build ( nbtrees ) ; sumTimeM += System . currentTimeMillis ( ) - time ; numNodesM += forestM . nbNodes ( ) ; treeBuilder . setM ( 1 ) ; time = System . currentTimeMillis ( ) ; log . info ( "Growing a forest with m=1" ) ; DecisionForest forestOne = forestBuilder . build ( nbtrees ) ; sumTimeOne += System . currentTimeMillis ( ) - time ; numNodesOne += forestOne . nbNodes ( ) ; double [ ] testLabels = test . extractLabels ( ) ; double [ ] predictions = new double [ test . size ( ) ] ; forestM . classify ( test , predictions ) ; sumTestErrM += ErrorEstimate . errorRate ( testLabels , predictions ) ; forestOne . classify ( test , predictions ) ; sumTestErrOne += ErrorEstimate . errorRate ( testLabels , predictions ) ; }
private void runIteration ( Random rng , Data data , int m , int nbtrees ) { log . info ( "Splitting the data" ) ; Data train = data . clone ( ) ; Data test = train . rsplit ( rng , ( int ) ( data . size ( ) * 0.1 ) ) ; DefaultTreeBuilder treeBuilder = new DefaultTreeBuilder ( ) ; SequentialBuilder forestBuilder = new SequentialBuilder ( rng , treeBuilder , train ) ; treeBuilder . setM ( m ) ; long time = System . currentTimeMillis ( ) ; log . info ( "Growing a forest with m={}" , m ) ; DecisionForest forestM = forestBuilder . build ( nbtrees ) ; sumTimeM += System . currentTimeMillis ( ) - time ; numNodesM += forestM . nbNodes ( ) ; treeBuilder . setM ( 1 ) ; time = System . currentTimeMillis ( ) ; log . info ( "Growing a forest with m=1" ) ; DecisionForest forestOne = forestBuilder . build ( nbtrees ) ; sumTimeOne += System . currentTimeMillis ( ) - time ; numNodesOne += forestOne . nbNodes ( ) ; double [ ] testLabels = test . extractLabels ( ) ; double [ ] predictions = new double [ test . size ( ) ] ; forestM . classify ( test , predictions ) ; sumTestErrM += ErrorEstimate . errorRate ( testLabels , predictions ) ; forestOne . classify ( test , predictions ) ; sumTestErrOne += ErrorEstimate . errorRate ( testLabels , predictions ) ; }
@ Test public void testJmxDumpCBRRoutesAsXml ( ) throws Exception { MBeanServer mbeanServer = getMBeanServer ( ) ; ObjectName on = getContextObjectName ( ) ; String xml = ( String ) mbeanServer . invoke ( on , "dumpRoutesAsXml" , null , null ) ; assertNotNull ( xml ) ; log . info ( xml ) ; assertTrue ( xml . contains ( "myRoute" ) , xml ) ; assertTrue ( xml . matches ( "[\\S\\s]*<when id=\"when[0-9]+\">[\\S\\s]*" ) ) ; assertTrue ( xml . matches ( "[\\S\\s]*<otherwise id=\"otherwise[0-9]+\">[\\S\\s]*" ) ) ; assertTrue ( xml . contains ( "<route customId=\"true\" id=\"myRoute\">" ) || xml . contains ( "<route id=\"myRoute\" customId=\"true\">" ) ) ; assertTrue ( xml . contains ( "<choice customId=\"true\" id=\"myChoice\">" ) || xml . contains ( "<choice id=\"myChoice\" customId=\"true\">" ) ) ; }
public void test() { if ( response . getStatusCode ( ) == HttpStatus . SC_OK ) { robotsTxt = parseRobotsTxt ( doc . getInputStream ( ) , trimmedURL , response . getUserAgent ( ) ) ; LOG . debug ( "Fetched and parsed robots.txt: {}" , robotsURL ) ; } else { LOG . info ( "No robots.txt found for {}. ({} - {})" , robotsURL , response . getStatusCode ( ) , response . getReasonPhrase ( ) ) ; robotsTxt = new RobotsTxt ( ) ; } }
public void test() { if ( response . getStatusCode ( ) == HttpStatus . SC_OK ) { robotsTxt = parseRobotsTxt ( doc . getInputStream ( ) , trimmedURL , response . getUserAgent ( ) ) ; LOG . debug ( "Fetched and parsed robots.txt: {}" , robotsURL ) ; } else { LOG . info ( "No robots.txt found for {}. ({} - {})" , robotsURL , response . getStatusCode ( ) , response . getReasonPhrase ( ) ) ; robotsTxt = new RobotsTxt ( ) ; } }
public void test() { try { CrawlDoc doc = new CrawlDoc ( new HttpDocInfo ( robotsURL ) , fetcher . getStreamFactory ( ) . newInputStream ( ) ) ; IHttpFetchResponse response = fetcher . fetch ( doc , HttpMethod . GET ) ; String redirURL = response . getRedirectTarget ( ) ; code_block = IfStatement ; code_block = IfStatement ; } catch ( Exception e ) { LOG . warn ( "Not able to obtain robots.txt at: {}" , robotsURL , e ) ; robotsTxt = new RobotsTxt ( ) ; } }
@ Override public void afterDestroy ( EntryEvent < K , V > event ) { log . info ( "In region [" + event . getRegion ( ) . getName ( ) + "] destroyed key [" + event . getKey ( ) + "] " ) ; }
public void test() { if ( prediction . isEmpty ( ) ) { log . error ( "Could not find relation in [{}] with id [{}]" , document , recommendationVid ) ; aTarget . getPage ( ) . error ( "Could not find relation" ) ; aTarget . addChildren ( aTarget . getPage ( ) , IFeedback . class ) ; return ; } }
public void test() { if ( sample != null ) { Tag noteId = Tag . of ( "nodeid" , note . getId ( ) ) ; Tag name = Tag . of ( "name" , StringUtils . defaultString ( note . getName ( ) , "unknown" ) ) ; Tag statusTag = Tag . of ( "result" , result ) ; sample . stop ( Metrics . timer ( "cronjob" , Tags . of ( noteId , name , statusTag ) ) ) ; } else { LOGGER . warn ( "No Timer.Sample for NoteId {} found" , note . getId ( ) ) ; } }
private void startTestFamework ( ) throws Exception { log . debug ( "STARTING TEST FRAMEWORK" ) ; resetTestListener ( testListener ) ; resetClockToStartOfTest ( clock ) ; startBusAndRegisterListener ( busService , testListener ) ; restartSubscriptionService ( subscriptionBaseService ) ; restartEntitlementService ( entitlementService ) ; log . debug ( "STARTED TEST FRAMEWORK" ) ; }
private void startTestFamework ( ) throws Exception { log . debug ( "STARTING TEST FRAMEWORK" ) ; resetTestListener ( testListener ) ; resetClockToStartOfTest ( clock ) ; startBusAndRegisterListener ( busService , testListener ) ; restartSubscriptionService ( subscriptionBaseService ) ; restartEntitlementService ( entitlementService ) ; log . debug ( "STARTED TEST FRAMEWORK" ) ; }
public void test() { try { if ( ! receiptEndDate . isEmpty ( ) && remittanceDate != null && remittanceDate . before ( dateFomatter . parse ( receiptEndDate ) ) ) addActionError ( getText ( "bankremittance.before.receiptdate" ) ) ; } catch ( final ParseException e ) { LOGGER . debug ( "Exception in parsing date  " + receiptEndDate + " - " + e . getMessage ( ) ) ; throw new ApplicationRuntimeException ( "Exception while parsing receiptEndDate date" , e ) ; } }
public void test() { try { java . util . List < com . liferay . commerce . wish . list . model . CommerceWishListItem > returnValue = CommerceWishListItemServiceUtil . getCommerceWishListItems ( commerceWishListId , start , end , orderByComparator ) ; return com . liferay . commerce . wish . list . model . CommerceWishListItemSoap . toSoapModels ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( ! stopped . get ( ) ) { LOG . fatal ( "yarn container launch event handler is interrupted. " + e ) ; context . getEventHandler ( ) . handle ( new InternalErrorEvent ( context . getApplicationId ( ) , "yarn container launch event handler is interrupted. " + e . getMessage ( ) ) ) ; } }
public void test() { if ( poolSize < idealPoolSize ) { int newPoolSize = Math . min ( limitOnPoolSize , idealPoolSize + INITIAL_POOL_SIZE ) ; LOG . info ( "Setting ContainerLauncher pool size to " + newPoolSize + " as number-of-nodes to talk to is " + numNodes ) ; launcherPool . setCorePoolSize ( newPoolSize ) ; } }
public void test() { { ResourcesResource . LOG . debug ( "try to apply configuration for resource with uuid '{}'" , uuid ) ; ResourcesResource . LOG . debug ( "try to recieve resource with uuid '{}' for csv json configuration preview" , uuid ) ; final Optional < Resource > resourceOptional = dataModelUtil . fetchResource ( uuid ) ; code_block = IfStatement ; final Resource resource = resourceOptional . get ( ) ; ResourcesResource . LOG . debug ( "found resource with uuid '{}' for csv json configuration preview " , uuid ) ; code_block = IfStatement ; ResourcesResource . LOG . debug ( "try to apply configuration to resource with uuid '{}'" , uuid ) ; final String result = applyConfigurationForCSVJSONPreview ( resource , jsonObjectString ) ; code_block = IfStatement ; ResourcesResource . LOG . debug ( "applied configuration to resource with uuid '{}'" , uuid ) ; return buildResponse ( result ) ; } }
public void test() { if ( ResourcesResource . LOG . isTraceEnabled ( ) ) { ResourcesResource . LOG . trace ( "= '{}'" , ToStringBuilder . reflectionToString ( resource ) ) ; } }
public void test() { { ResourcesResource . LOG . debug ( "try to apply configuration for resource with uuid '{}'" , uuid ) ; ResourcesResource . LOG . debug ( "try to recieve resource with uuid '{}' for csv json configuration preview" , uuid ) ; final Optional < Resource > resourceOptional = dataModelUtil . fetchResource ( uuid ) ; code_block = IfStatement ; final Resource resource = resourceOptional . get ( ) ; ResourcesResource . LOG . debug ( "found resource with uuid '{}' for csv json configuration preview " , uuid ) ; code_block = IfStatement ; ResourcesResource . LOG . debug ( "try to apply configuration to resource with uuid '{}'" , uuid ) ; final String result = applyConfigurationForCSVJSONPreview ( resource , jsonObjectString ) ; code_block = IfStatement ; ResourcesResource . LOG . debug ( "applied configuration to resource with uuid '{}'" , uuid ) ; return buildResponse ( result ) ; } }
public void test() { { ResourcesResource . LOG . debug ( "try to apply configuration for resource with uuid '{}'" , uuid ) ; ResourcesResource . LOG . debug ( "try to recieve resource with uuid '{}' for csv json configuration preview" , uuid ) ; final Optional < Resource > resourceOptional = dataModelUtil . fetchResource ( uuid ) ; code_block = IfStatement ; final Resource resource = resourceOptional . get ( ) ; ResourcesResource . LOG . debug ( "found resource with uuid '{}' for csv json configuration preview " , uuid ) ; code_block = IfStatement ; ResourcesResource . LOG . debug ( "try to apply configuration to resource with uuid '{}'" , uuid ) ; final String result = applyConfigurationForCSVJSONPreview ( resource , jsonObjectString ) ; code_block = IfStatement ; ResourcesResource . LOG . debug ( "applied configuration to resource with uuid '{}'" , uuid ) ; return buildResponse ( result ) ; } }
public void test() { if ( element == null ) { LOGGER . debug ( "Unbekanntes Element." ) ; return ; } }
@ Before public void setUp ( ) throws Exception { SUT = new Payload2TcpProtocol ( ) ; channelHandlerContextMock = mock ( ChannelHandlerContext . class , RETURNS_DEEP_STUBS ) ; byte [ ] bytes = amsTCPPacket . getBytes ( ) ; LOGGER . info ( "amsPacket:\n{} has \n{}bytes\nHexDump:\n{}" , amsTCPPacket , bytes . length , amsTCPPacket . dump ( ) ) ; }
private void publishRegisterURI ( final List < URIRegisterDTO > registerDTOList ) { log . info ( "publish uri: {}" , registerDTOList ) ; publisher . publish ( registerDTOList ) ; }
@ Override public Future < Void > close ( final SpanContext spanContext ) { log . debug ( "MappingAndDelegatingCommandConsumer receiver link [tenant-id: {}] closed locally" , tenantId ) ; mappingAndDelegatingCommandConsumerFactory . removeClient ( tenantId ) ; consumerLinkTenants . remove ( tenantId ) ; final Promise < Void > result = Promise . promise ( ) ; connection . closeAndFree ( receiver , receiverClosed -> result . complete ( ) ) ; return result . future ( ) ; }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Starting with backup attributes for context: " + session . getServletContext ( ) . getContextPath ( ) ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Finished backup of attribute: " + attrName ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Getting free connection, hostConfig=" + hostConfiguration ) ; } }
public void test() { if ( ( hostPool != null ) && ( hostPool . freeConnections . size ( ) > 0 ) ) { connection = ( HttpConnectionWithReference ) hostPool . freeConnections . removeLast ( ) ; freeConnections . remove ( connection ) ; storeReferenceToConnection ( connection , hostConfiguration , this ) ; code_block = IfStatement ; idleConnectionHandler . remove ( connection ) ; } else-if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "There were no free connections to get, hostConfig=" + hostConfiguration ) ; } }
@ Override public List < User > findUsersLike ( final String userName , final boolean caseInsensitive ) throws JargonException { code_block = IfStatement ; log . info ( "findUserNameLike {}" , userName ) ; log . info ( "case insensitive?:{}" , caseInsensitive ) ; IRODSGenQueryBuilder builder = new IRODSGenQueryBuilder ( true , caseInsensitive , null ) ; StringBuilder userQuery = new StringBuilder ( ) ; userQuery . append ( userName . trim ( ) ) ; userQuery . append ( "%" ) ; IRODSQueryResultSet resultSet = null ; code_block = TryStatement ;  List < User > users = new ArrayList < User > ( ) ; User user ; code_block = ForStatement ; return users ; }
@ Override public List < User > findUsersLike ( final String userName , final boolean caseInsensitive ) throws JargonException { code_block = IfStatement ; log . info ( "findUserNameLike {}" , userName ) ; log . info ( "case insensitive?:{}" , caseInsensitive ) ; IRODSGenQueryBuilder builder = new IRODSGenQueryBuilder ( true , caseInsensitive , null ) ; StringBuilder userQuery = new StringBuilder ( ) ; userQuery . append ( userName . trim ( ) ) ; userQuery . append ( "%" ) ; IRODSQueryResultSet resultSet = null ; code_block = TryStatement ;  List < User > users = new ArrayList < User > ( ) ; User user ; code_block = ForStatement ; return users ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( searchException , searchException ) ; } }
protected CleanupJobState jobReachedTerminalState ( ExecutionGraphInfo executionGraphInfo ) { final ArchivedExecutionGraph archivedExecutionGraph = executionGraphInfo . getArchivedExecutionGraph ( ) ; Preconditions . checkArgument ( archivedExecutionGraph . getState ( ) . isTerminalState ( ) , "Job %s is in state %s which is not terminal." , archivedExecutionGraph . getJobID ( ) , archivedExecutionGraph . getState ( ) ) ; log . info ( "Job {} reached terminal state {}." , archivedExecutionGraph . getJobID ( ) , archivedExecutionGraph . getState ( ) ) ; archiveExecutionGraph ( executionGraphInfo ) ; return archivedExecutionGraph . getState ( ) . isGloballyTerminalState ( ) ? CleanupJobState . GLOBAL : CleanupJobState . LOCAL ; }
public void test() { try { UserDefinedFileAttributeView view = getAttributeView ( file ) ; view . write ( ATTRIBUTE_PAGE_TYPE , Charset . defaultCharset ( ) . encode ( pageType ) ) ; } catch ( Exception ex ) { log . debug ( "cannot set pageType for {}" , file , ex ) ; } }
public void test() { if ( calibrator instanceof PolynomialCalibrator ) { doc . writeStartElement ( "PolynomialCalibrator" ) ; double [ ] coefficients = ( ( PolynomialCalibrator ) calibrator ) . getCoefficients ( ) ; code_block = ForStatement ; doc . writeEndElement ( ) ; } else-if ( calibrator instanceof SplineCalibrator ) { doc . writeStartElement ( "SplineCalibrator" ) ; code_block = ForStatement ; doc . writeEndElement ( ) ; } else-if ( calibrator instanceof MathOperationCalibrator ) { doc . writeStartElement ( "MathOperationCalibrator" ) ; writeMathOperation ( doc , ( MathOperationCalibrator ) calibrator ) ; doc . writeEndElement ( ) ; } else { log . error ( "Unsupported calibrator  type " + calibrator . getClass ( ) ) ; } }
public void test() { try { final URI broadcastUri = nic . getBroadcastUri ( ) ; final String vlanId = BroadcastDomainType . getValue ( broadcastUri ) ; final int ethDeviceNum = getVmNics ( domrName , vlanId ) ; code_block = IfStatement ; } catch ( final Exception e ) { final String msg = "Prepare SetupGuestNetwork failed due to " + e . toString ( ) ; s_logger . warn ( msg , e ) ; return new ExecutionResult ( false , msg ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "problem processing policy" , pdme ) ; } }
public void test() { try { code_block = WhileStatement ; } catch ( InterruptedException e ) { log . error ( "Task could not be initialized hence not be executed." , e ) ; return ; } finally { lock . unlock ( ) ; } }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; ObjectInputStream ois ; ois = new ObjectInputStream ( new ByteArrayInputStream ( bytes ) ) ; Object o = ois . readObject ( ) ; ois . close ( ) ; return o ; } catch ( IOException e ) { log . error ( "IO exception, Caused by {}." , e ) ; throw new PropertyAccessException ( e ) ; } catch ( ClassNotFoundException e ) { log . error ( "Class not found exception, Caused by {}." , e ) ; throw new PropertyAccessException ( e ) ; } }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; ObjectInputStream ois ; ois = new ObjectInputStream ( new ByteArrayInputStream ( bytes ) ) ; Object o = ois . readObject ( ) ; ois . close ( ) ; return o ; } catch ( IOException e ) { log . error ( "IO exception, Caused by {}." , e ) ; throw new PropertyAccessException ( e ) ; } catch ( ClassNotFoundException e ) { log . error ( "Class not found exception, Caused by {}." , e ) ; throw new PropertyAccessException ( e ) ; } }
public void test() { try { mappings = this . getApiCatalogManager ( ) . getRelatedWidgetMethods ( ) ; } catch ( Throwable t ) { _logger . error ( "error in getWidgetTypeApiMappings" , t ) ; } }
public void test() { if ( simpleUpdateReference != null ) { String subject = change . getIdentifiable ( ) . getId ( ) ; String value = simpleUpdateReference . value ( change ) ; TripleStoreChangeParams updateParams = new TripleStoreChangeParams ( simpleUpdateReference , value ) ; TripleStoreChange tschange = new TripleStoreChange ( "update" , subject , updateParams ) ; return Collections . singletonList ( tschange ) ; } else-if ( ignoredAttributes . contains ( change . getAttribute ( ) ) ) { return Collections . emptyList ( ) ; } else { LOG . warn ( "Convert to CGMES a change on IIDM {}.{}" , change . getIdentifiable ( ) . getClass ( ) . getSimpleName ( ) , change . getAttribute ( ) ) ; return Collections . emptyList ( ) ; } }
public void test() { try { originalSgMembers = PublicAccessAutoFix . getSgListForClassicElbResource ( clientMap , resourceId ) ; code_block = ForStatement ; } catch ( Exception e ) { LOGGER . error ( "back up failed" , e . getMessage ( ) ) ; throw new AutoFixException ( "backup failed" ) ; } }
@ Override public boolean backupExistingConfigForResource ( final String resourceId , final String resourceType , Map < String , Object > clientMap , Map < String , String > ruleParams , Map < String , String > issue ) throws AutoFixException { StringBuilder oldConfig = new StringBuilder ( ) ; List < String > originalSgMembers ; code_block = TryStatement ;  DETACHED_SG = oldConfig . toString ( ) ; backupOldConfig ( resourceId , EXISTING_GROUPS , oldConfig . toString ( ) ) ; LOGGER . debug ( "backup complete for {}" , resourceId ) ; return true ; }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Refine query on documents is {}" , refine_query_on_doc ? "enabled." : "disabled." ) ; } }
public void test() { try { doc = ( DocumentImpl ) broker . getXMLResource ( XmldbURI . create ( rs . getString ( "DOCUMENT_URI" ) ) ) ; } catch ( PermissionDeniedException e ) { LOG . debug ( e ) ; result [ index ++ ] = null ; continue ; } }
public void test() { try { clientStartLatch . await ( ) ; Thread . sleep ( 10 ) ; client = true ; Ignite cl = startGrid ( "client0" ) ; IgniteCache < Object , Object > atomicCache = cl . cache ( CACHE_NAME_PREFIX + '0' ) ; IgniteCache < Object , Object > txCache = cl . cache ( CACHE_NAME_PREFIX + '1' ) ; assertEquals ( state == ACTIVE ? 100 : 0 , atomicCache . size ( ) ) ; assertEquals ( state == ACTIVE ? 100 : 0 , txCache . size ( ) ) ; } catch ( Exception e ) { log . error ( "Error occurred" , e ) ; fail ( "Error occurred in client thread. Msg: " + e . getMessage ( ) ) ; } }
public void test() { if ( Log . isDebugEnabled ( ) ) { Log . debug ( "AuthorizationManager: Trying " + am . name ( ) + ".map(" + principal + ")" ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { log . error ( e ) ; connectionStatusCache = null ; } }
public static Map < String , Object > shipData ( Map < String , String > params ) { String jobName = params . get ( "jobName" ) ; code_block = IfStatement ; List < Map < String , String > > errorList = new ArrayList < > ( ) ; code_block = TryStatement ;  errorList . addAll ( new RecommendationCollector ( ) . uploadRecommendationData ( ) ) ; Map < String , Object > status = ErrorManageUtil . formErrorCode ( jobName , errorList ) ; LOGGER . info ( "Job Return Status {} " , status ) ; return status ; }
public void test() { try { Response response = ElasticSearchManager . invokeAPI ( "GET" , endPoint , payLoad ) ; String responseJson = EntityUtils . toString ( response . getEntity ( ) ) ; JsonParser jsonParser = new JsonParser ( ) ; JsonObject resultJson = ( JsonObject ) jsonParser . parse ( responseJson ) ; JsonObject hitsJson = ( JsonObject ) jsonParser . parse ( resultJson . get ( "hits" ) . toString ( ) ) ; JsonArray jsonArray = hitsJson . getAsJsonObject ( ) . get ( "hits" ) . getAsJsonArray ( ) ; code_block = ForStatement ; } catch ( Exception e ) { LOGGER . error ( "Error in fetchVPCtoNatIPInfo" , e ) ; } }
public void test() { if ( logsController . isPodRunning ( podName ) ) { LOG . info ( "End of Log stream for running pod: {}" , podName ) ; } else { LOG . info ( "End of Log stream for terminated pod: {}" , podName ) ; } }
public void test() { if ( logsController . isPodRunning ( podName ) ) { LOG . info ( "End of Log stream for running pod: {}" , podName ) ; } else { LOG . info ( "End of Log stream for terminated pod: {}" , podName ) ; } }
@ Transition ( to = "MASTER" , from = "SLAVE" ) public void onBecomeMasterFromSlave ( Message message , NotificationContext context ) { String partitionName = message . getPartitionName ( ) ; String instanceName = message . getTgtName ( ) ; LOGGER . info ( instanceName + " becomes MASTER from SLAVE to " + partitionName ) ; }
public User find ( final String username ) { log . debug ( "find() - username: {}" , username ) ; return userRepository . findByUsername ( username ) ; }
@ Override protected Connection createListenerContainer ( ) throws Exception { LOG . trace ( "Creating connection" ) ; Connection conn = endpoint . connect ( executorService ) ; LOG . trace ( "Creating channel" ) ; Channel channel = conn . createChannel ( ) ; code_block = IfStatement ; DeclareOk result = channel . queueDeclare ( ) ; LOG . debug ( "Using temporary queue name: {}" , result . getQueue ( ) ) ; setReplyTo ( result . getQueue ( ) ) ; channel . queueBind ( getReplyTo ( ) , endpoint . getExchangeName ( ) , getReplyTo ( ) ) ; code_block = IfStatement ; consumer = new RabbitConsumer ( this , channel ) ; consumer . start ( ) ; return conn ; }
@ Override protected Connection createListenerContainer ( ) throws Exception { LOG . trace ( "Creating connection" ) ; Connection conn = endpoint . connect ( executorService ) ; LOG . trace ( "Creating channel" ) ; Channel channel = conn . createChannel ( ) ; code_block = IfStatement ; DeclareOk result = channel . queueDeclare ( ) ; LOG . debug ( "Using temporary queue name: {}" , result . getQueue ( ) ) ; setReplyTo ( result . getQueue ( ) ) ; channel . queueBind ( getReplyTo ( ) , endpoint . getExchangeName ( ) , getReplyTo ( ) ) ; code_block = IfStatement ; consumer = new RabbitConsumer ( this , channel ) ; consumer . start ( ) ; return conn ; }
@ Override protected Connection createListenerContainer ( ) throws Exception { LOG . trace ( "Creating connection" ) ; Connection conn = endpoint . connect ( executorService ) ; LOG . trace ( "Creating channel" ) ; Channel channel = conn . createChannel ( ) ; code_block = IfStatement ; DeclareOk result = channel . queueDeclare ( ) ; LOG . debug ( "Using temporary queue name: {}" , result . getQueue ( ) ) ; setReplyTo ( result . getQueue ( ) ) ; channel . queueBind ( getReplyTo ( ) , endpoint . getExchangeName ( ) , getReplyTo ( ) ) ; code_block = IfStatement ; consumer = new RabbitConsumer ( this , channel ) ; consumer . start ( ) ; return conn ; }
public void test() { try { channel . queueBind ( newName , endpoint . getExchangeName ( ) , newName ) ; channel . queueUnbind ( newName , endpoint . getExchangeName ( ) , oldName ) ; } catch ( IOException e ) { LOG . warn ( "Failed to bind or unbind a queue. This exception is ignored." , e ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( _NO_SUCH_ENTITY_WITH_PRIMARY_KEY + primaryKey ) ; } }
public void test() { try { DataAvailabilityEventFilter filter = ( DataAvailabilityEventFilter ) Class . forName ( filterClassName ) . newInstance ( ) ; filters . add ( filter ) ; LOG . info ( "Loaded event filter: {}" , filterClassName ) ; } catch ( ClassNotFoundException | InstantiationException | IllegalAccessException e ) { throw new IllegalArgumentException ( "Failed to initialize trigger event filter." , e . getCause ( ) ) ; } }
public void test() { if ( coordinatedStack != null && coordinatedStack . getRunnerCount ( ) == runnerCount ) { LOG . info ( "Stack {} is already registered" , stack . getName ( ) ) ; code_block = IfStatement ; } else-if ( coordinatedStack != null && coordinatedStack . getRunnerCount ( ) != runnerCount ) { LOG . info ( "Stack {} is registered with different runner count, first removing the old stack" , stack . getName ( ) ) ; registeredStacks . remove ( coordinatedStack . hashCode ( ) ) ; } }
public CoordinatedStack setupStack ( Stack stack , User user , Commit commit , Module module , int runnerCount ) { CoordinatedStack coordinatedStack = getCoordinatedStack ( stack , user , commit , module ) ; code_block = IfStatement ; LOG . info ( "Registering new stack {}..." , stack . getName ( ) ) ; coordinatedStack = new CoordinatedStack ( stack , user , commit , module , runnerCount ) ; LOG . info ( "Starting setup stack thread of {}..." , stack . getName ( ) ) ; synchronized ( coordinatedStack ) code_block = "" ; return coordinatedStack ; }
public void test() { if ( ! Files . exists ( eventFolder ) ) { log . warn ( "No events found at {}" , eventFolder ) ; return ; } }
public void test() { try { callback . accept ( p ) ; new EpisimEventsReader ( manager ) . readFile ( p . toString ( ) ) ; } catch ( UncheckedIOException e ) { log . warn ( "Caught UncheckedIOException. Could not read file {}" , p ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Not trying to connect to " + remoteId . getAddress ( ) + " this server is in the failed servers list" ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Connecting to " + remoteId . getAddress ( ) ) ; } }
public void test() { try { List < String > ancestors = SolrUtils . getAncestors ( aip . getParentId ( ) , model ) ; indexAIP ( aip , ancestors ) . addTo ( ret ) ; code_block = IfStatement ; } catch ( RequestNotValidException | GenericException | AuthorizationDeniedException e ) { LOGGER . error ( "Error getting ancestors when creating AIP" , e ) ; ret . add ( e ) ; } }
public void test() { if ( BRIDGE_THING_TYPE . equals ( thingTypeUID ) ) { return new BoxHandler ( ( Bridge ) thing , httpClient , commandDescriptionProvider ) ; } else-if ( PL546E_STANDALONE_THING_TYPE . equals ( thingTypeUID ) ) { return new Powerline546EHandler ( ( Bridge ) thing , httpClient , commandDescriptionProvider ) ; } else-if ( SUPPORTED_BUTTON_THING_TYPES_UIDS . contains ( thingTypeUID ) ) { return new AVMFritzButtonHandler ( thing ) ; } else-if ( SUPPORTED_HEATING_THING_TYPES . contains ( thingTypeUID ) ) { return new AVMFritzHeatingDeviceHandler ( thing ) ; } else-if ( SUPPORTED_DEVICE_THING_TYPES_UIDS . contains ( thingTypeUID ) ) { return new DeviceHandler ( thing ) ; } else-if ( GROUP_HEATING_THING_TYPE . equals ( thingTypeUID ) ) { return new AVMFritzHeatingGroupHandler ( thing ) ; } else-if ( SUPPORTED_GROUP_THING_TYPES_UIDS . contains ( thingTypeUID ) ) { return new GroupHandler ( thing ) ; } else { logger . error ( "ThingHandler not found for {}" , thingTypeUID ) ; } }
@ Transactional private void deleteRecovery ( ShardRecovery recovery ) { int shardRecoveryDeleted = shardRecoveryDao . hardDeleteShardRecovery ( recovery . getShardRecoveryId ( ) ) ; Shard shard = shardDao . getLastShard ( ) ; Objects . requireNonNull ( shard , "Shard record should exist!" ) ; shardDao . hardDeleteShard ( shard . getShardId ( ) ) ; log . debug ( "Cleared shard records : shardRecovery - {}, shard id = {}" , shardRecoveryDeleted , shard . getShardId ( ) ) ; }
public void test() { try { TransactionManager tm = TransactionHelper . lookupTransactionManager ( ) ; code_block = IfStatement ; } catch ( NamingException | IllegalStateException | SystemException | RollbackException e ) { log . error ( "Unable to register synchronization" , e ) ; return false ; } }
private String moduleTokens ( RoutingContext ctx ) { String modPermJson = ctx . request ( ) . getHeader ( XOkapiHeaders . MODULE_PERMISSIONS ) ; logger . debug ( "test-auth: moduleTokens: trying to decode '{}'" , modPermJson ) ; HashMap < String , String > tokens = new HashMap < > ( ) ; code_block = IfStatement ; code_block = IfStatement ; String alltokens = Json . encode ( tokens ) ; logger . debug ( "test-auth: module tokens for {}: {}" , modPermJson , alltokens ) ; return alltokens ; }
public void test() { try { int returnValue = CalendarResourceServiceUtil . searchCount ( companyId , groupIds , classNameIds , keywords , active ) ; return returnValue ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "getXAResources(" + Arrays . toString ( specs ) + ")" ) ; } }
public void warmCaches ( ) { evictCaches ( ) ; logger . info ( "Warming up bill cache." ) ; Optional < Range < SessionYear > > sessionRange = activeSessionRange ( ) ; code_block = IfStatement ; logger . info ( "Done warming up bill cache." ) ; }
public void test() { if ( sessionYear . equals ( SessionYear . current ( ) ) ) { logger . info ( "Caching Bill instances for current session year: {}" , sessionYear ) ; getBillIds ( sessionYear , LimitOffset . ALL ) . forEach ( id -> getBill ( id ) ) ; } else { logger . info ( "Caching Bill Info instances for session year: {}" , sessionYear ) ; getBillIds ( sessionYear , LimitOffset . ALL ) . forEach ( this :: getBillInfo ) ; } }
public void test() { if ( sessionYear . equals ( SessionYear . current ( ) ) ) { logger . info ( "Caching Bill instances for current session year: {}" , sessionYear ) ; getBillIds ( sessionYear , LimitOffset . ALL ) . forEach ( id -> getBill ( id ) ) ; } else { logger . info ( "Caching Bill Info instances for session year: {}" , sessionYear ) ; getBillIds ( sessionYear , LimitOffset . ALL ) . forEach ( this :: getBillInfo ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "selectLink() called ..." ) ; } }
public void test() { try { AccountID payerID = FileServiceTest . getRandomPayerAccount ( ) ; AccountID nodeID = FileServiceTest . getRandomNodeAccount ( ) ; log . info ( LOG_PREFIX + "Create file: creating a file with 1024 bytes ..." ) ; FileID fid = FileID . newBuilder ( ) . setFileNum ( 1 ) . setRealmNum ( 1 ) . setShardNum ( 0 ) . build ( ) ; getFileInfo ( fid , payerID , nodeID ) ; } catch ( Throwable e ) { log . info ( LOG_PREFIX + "Invalid FileID Test: passed! Caught expected exception = " + e ) ; } }
public void test() { try { AccountID payerID = FileServiceTest . getRandomPayerAccount ( ) ; AccountID nodeID = FileServiceTest . getRandomNodeAccount ( ) ; log . info ( LOG_PREFIX + "Create file: creating a file with 1024 bytes ..." ) ; FileID fid = FileID . newBuilder ( ) . setFileNum ( 1 ) . setRealmNum ( 1 ) . setShardNum ( 0 ) . build ( ) ; getFileInfo ( fid , payerID , nodeID ) ; } catch ( Throwable e ) { log . info ( LOG_PREFIX + "Invalid FileID Test: passed! Caught expected exception = " + e ) ; } }
public void test() { try { client . admin ( ) . indices ( ) . create ( new CreateIndexRequest ( indexName ) . mapping ( indexType , jsonMapping ) ) . actionGet ( ) ; indexCache . add ( indexName ) ; } catch ( IndexAlreadyExistsException e ) { logger . info ( "The index " + indexName + " already exists" ) ; } }
public void test() { try { code_block = ForStatement ; } catch ( Throwable e ) { LOG . warn ( "Error releasing exchange due to " + e . getMessage ( ) + ". This exception is ignored." , e ) ; } }
public void test() { try { disconnect ( ) ; } catch ( ConnectionException e ) { logger . warn ( "Unable to Disconnect..." ) ; } }
public void test() { if ( this . connectionManager . isPresent ( ) && this . connectionManager . get ( ) == manager ) { logger . debug ( "Unrecoverable failure, forcing disconnect" , ex ) ; code_block = TryStatement ;  } else { logger . debug ( "Ignoring failure from old connection" , ex ) ; } }
public void test() { if ( LOGGER . isInfoEnabled ( ) ) { LOGGER . info ( task ) ; } }
public void test() { try { MetadataExportParams exportParams = new MetadataExportParams ( ) ; code_block = IfStatement ; os = new ByteArrayOutputStream ( 1024 ) ; RootNode metadata = metadataExportService . getMetadataAsNode ( exportParams ) ; nodeService . serialize ( metadata , "application/json" , os ) ; } catch ( Exception ex ) { String message = "Exception occurred while exporting metadata for capturing a metadata version" + ex . getMessage ( ) ; log . error ( message , ex ) ; throw new MetadataVersionServiceException ( message , ex ) ; } }
@ Test public void testParseMinutePrecisionWithoutTimezone ( ) { DateParam param = new DateParam ( ) ; param . setValueAsString ( "2016-06-09T20:38" ) ; assertNull ( param . getPrefix ( ) ) ; assertEquals ( "2016-06-09T20:38" , param . getValueAsString ( ) ) ; ourLog . debug ( "PRE:  " + param . getValue ( ) ) ; ourLog . debug ( "PRE:  " + param . getValue ( ) . getTime ( ) ) ; InstantDt dt = new InstantDt ( new Date ( param . getValue ( ) . getTime ( ) ) ) ; dt . setTimeZone ( TimeZone . getTimeZone ( "America/Toronto" ) ) ; ourLog . debug ( "POST: " + dt . getValue ( ) ) ; assertThat ( dt . getValueAsString ( ) , startsWith ( "2016-06-09T" ) ) ; assertThat ( dt . getValueAsString ( ) , endsWith ( "8:00.000-04:00" ) ) ; }
@ Test public void testParseMinutePrecisionWithoutTimezone ( ) { DateParam param = new DateParam ( ) ; param . setValueAsString ( "2016-06-09T20:38" ) ; assertNull ( param . getPrefix ( ) ) ; assertEquals ( "2016-06-09T20:38" , param . getValueAsString ( ) ) ; ourLog . debug ( "PRE:  " + param . getValue ( ) ) ; ourLog . debug ( "PRE:  " + param . getValue ( ) . getTime ( ) ) ; InstantDt dt = new InstantDt ( new Date ( param . getValue ( ) . getTime ( ) ) ) ; dt . setTimeZone ( TimeZone . getTimeZone ( "America/Toronto" ) ) ; ourLog . debug ( "POST: " + dt . getValue ( ) ) ; assertThat ( dt . getValueAsString ( ) , startsWith ( "2016-06-09T" ) ) ; assertThat ( dt . getValueAsString ( ) , endsWith ( "8:00.000-04:00" ) ) ; }
@ Test public void testParseMinutePrecisionWithoutTimezone ( ) { DateParam param = new DateParam ( ) ; param . setValueAsString ( "2016-06-09T20:38" ) ; assertNull ( param . getPrefix ( ) ) ; assertEquals ( "2016-06-09T20:38" , param . getValueAsString ( ) ) ; ourLog . debug ( "PRE:  " + param . getValue ( ) ) ; ourLog . debug ( "PRE:  " + param . getValue ( ) . getTime ( ) ) ; InstantDt dt = new InstantDt ( new Date ( param . getValue ( ) . getTime ( ) ) ) ; dt . setTimeZone ( TimeZone . getTimeZone ( "America/Toronto" ) ) ; ourLog . debug ( "POST: " + dt . getValue ( ) ) ; assertThat ( dt . getValueAsString ( ) , startsWith ( "2016-06-09T" ) ) ; assertThat ( dt . getValueAsString ( ) , endsWith ( "8:00.000-04:00" ) ) ; }
@ Override public void addCompositePhenomenonForProcedure ( String procedure , Collection < String > compositePhenomenon ) { CacheValidation . notNullOrEmpty ( PROCEDURE , procedure ) ; CacheValidation . noNullOrEmptyValues ( COMPOSITE_PHENOMENON , compositePhenomenon ) ; LOG . trace ( "Adding composite phenomenons {} to procedure {}" , compositePhenomenon , procedure ) ; this . compositePhenomenonsForProcedure . computeIfAbsent ( procedure , createSynchronizedSet ( ) ) . addAll ( compositePhenomenon ) ; addCompositePhenomenon ( compositePhenomenon ) ; }
private void handleEventBusException ( Throwable exception , SubscriberExceptionContext context ) { logger . error ( "Event Bus Exception thrown during event handling within " + context . getSubscriberMethod ( ) , exception ) ; LocalDateTime occurred = LocalDateTime . now ( ) ; String summary = "Event Bus Exception within " + context . getSubscriberMethod ( ) + " at " + occurred + " - " + ExceptionUtils . getStackFrames ( exception ) [ 0 ] ; String message = "\nThe following exception occurred during event handling within " + context . getSubscriberMethod ( ) + " at " + occurred + ":\n" + ExceptionUtils . getStackTrace ( exception ) ; Notification notification = new Notification ( EVENT_BUS_EXCEPTION , occurred , summary , message ) ; eventBus ( ) . post ( notification ) ; }
public void attachClean ( BrAttribut instance ) { log . debug ( "attaching clean BrAttribut instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . lock ( instance , LockMode . NONE ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
@ Activate public void activate ( ) { logger . info ( "Document servlet started" ) ; resourcePath = "./target" ; }
public void test() { try { code_block = WhileStatement ; } catch ( Throwable error ) { log . error ( "Error during the invocation" , error ) ; } }
public void test() { try { java . util . List < com . liferay . commerce . bom . model . CommerceBOMDefinition > returnValue = CommerceBOMDefinitionServiceUtil . getCommerceBOMDefinitions ( commerceBOMFolderId , start , end ) ; return com . liferay . commerce . bom . model . CommerceBOMDefinitionSoap . toSoapModels ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( matchingServiceName != null ) { String serviceGroup [ ] = code_block = "" ; ; log . info ( "Service group name " + serviceGroup [ 0 ] ) ; serviceAdminStub . deleteServiceGroups ( serviceGroup ) ; } else { log . error ( "Service group name cannot be null" ) ; } }
public void test() { try { return LanguageUtil . get ( locale , "home" ) + " - " + group . getDescriptiveName ( locale ) ; } catch ( PortalException portalException ) { _log . error ( "Unable to get descriptive name for group " + group . getGroupId ( ) , portalException ) ; return LanguageUtil . get ( locale , "home" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Clear local statistics [key=" + key + ", columns=" + colNames + ']' ) ; } }
public void test() { try { startTransaction ( ) ; writeStressorLastOperation ( ) ; ongoingTx . commit ( ) ; lastConfirmedOperation = operationId ; } catch ( Exception e ) { log . error ( "Cannot write stressor last operation" , e ) ; } finally { clearTransaction ( ) ; } }
public void test() { if ( ie != null ) { throw ie ; } else-if ( e . getClass ( ) . getName ( ) . contains ( "SuspectException" ) ) { log . error ( "Request failed due to SuspectException: " + e . getMessage ( ) ) ; } else { log . error ( "Cache operation error" , e ) ; } }
public void test() { if ( ie != null ) { throw ie ; } else-if ( e . getClass ( ) . getName ( ) . contains ( "SuspectException" ) ) { log . error ( "Request failed due to SuspectException: " + e . getMessage ( ) ) ; } else { log . error ( "Cache operation error" , e ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( e . getMessage ( ) , e ) ; } }
public void test() { try { code_block = IfStatement ; return ProcessingStatus . OK ; } catch ( Exception e ) { log . warn ( "Exception in Solr onDocuments." , e ) ; return ProcessingStatus . DROP ; } }
@ Test public void testQueryWebPageQueryEmptyResults ( ) throws Exception { log . info ( "test method: testQueryEmptyResults" ) ; DataStoreTestUtil . testQueryWebPageEmptyResults ( webPageStore ) ; }
private void print ( PipelineInfo < RESPONSE > pipeline ) { log . debug ( "<pipeline name = '{}' layer = '{}'>" , pipeline . getPipelineName ( ) , pipeline . getLayerId ( ) ) ; code_block = ForStatement ; log . debug ( "</pipeline>" ) ; }
@ Test public void test_01_dup ( ) { Log . debug ( "Test" ) ; String genome = "testHg19Chr17" ; String vcf = path ( "hgvs_dup.vcf" ) ; String args [ ] = code_block = "" ; ; SnpEffCmdEff snpeff = new SnpEffCmdEff ( ) ; snpeff . parseArgs ( args ) ; snpeff . setDebug ( debug ) ; snpeff . setVerbose ( verbose ) ; snpeff . setSupressOutput ( ! verbose ) ; snpeff . setFormatVersion ( EffFormatVersion . FORMAT_EFF_4 ) ; snpeff . setUpDownStreamLength ( 0 ) ; List < VcfEntry > results = snpeff . run ( true ) ; code_block = ForStatement ; }
public void test() { try { store . close ( ) ; } catch ( Exception e ) { logger . error ( "Could not close the metrics service, some metrics may have not been written" , e ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "scheduling appendUpdateRecordTransactional::txID=" + txID + ",id=" + id + ", userRecordType=" + recordType + ", record = " + record ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "appendUpdateRecordTransactional::txID=" + txID + ",id=" + id + ", userRecordType=" + recordType + ", record = " + record + ", usedFile = " + usedFile ) ; } }
public void test() { try { tx . checkErrorCondition ( ) ; JournalInternalRecord updateRecordTX = new JournalAddRecordTX ( false , txID , id , recordType , persister , record ) ; JournalFile usedFile = appendRecord ( updateRecordTX , false , false , tx , null ) ; code_block = IfStatement ; tx . addPositive ( usedFile , id , updateRecordTX . getEncodeSize ( ) ) ; } catch ( Throwable e ) { logger . error ( "appendUpdateRecordTransactional:" + e . getMessage ( ) , e ) ; setErrorCondition ( null , tx , e ) ; } finally { journalLock . readLock ( ) . unlock ( ) ; } }
public void test() { try { String id = this . id ; code_block = IfStatement ; finalID = Integer . parseInt ( id ) ; String volume = this . volume ; code_block = IfStatement ; finalVolume = Integer . parseInt ( volume ) ; } catch ( NumberFormatException e ) { LOGGER . debug ( "Error parsing Integer" ) ; } }
private void warmUp ( ) throws IOException { LOG . info ( "Warming up data server..." ) ; List < Thread > threads = new ArrayList < Thread > ( ) ; code_block = ForStatement ; HankTimer timer = new HankTimer ( ) ; code_block = ForStatement ; code_block = ForStatement ; long warmupDurationMs = timer . getDurationMs ( ) ; LOG . info ( "Warming up data server took " + warmupDurationMs + " ms" ) ; }
public void test() { try { AbstractTableConfigHelperTest . TestAbstractTableConfigHelperImpl uut = new AbstractTableConfigHelperTest . TestAbstractTableConfigHelperImpl ( ) ; Assert . assertNotNull ( "AbstractTableConfigHelper.cTor failed to create an instance" , uut ) ; uut . parent = this ; uut . exposeSetCombinerConfigurationIfNecessaryForTest ( ) ; } finally { AbstractTableConfigHelperTest . logger . info ( "AbstractTableConfigHelperTest.testSetCombinerConfigurationIfNecessary() completed." ) ; } }
public void test() { try { return ( getPid ( ) != null ) ; } catch ( IOException e ) { log . error ( e ) ; return false ; } }
private List < String > getConnectedDevices ( ) { String deviceUDID = "(.*)\\tdevice$" ; String [ ] cmd = CmdLine . insertCommandsAfter ( executor . getDefaultCmd ( ) , "devices" ) ; List < String > cmdOutput = executor . execute ( cmd ) ; List < String > connectedDevices = cmdOutput . stream ( ) . parallel ( ) . filter ( ( d ) -> d . matches ( deviceUDID ) ) . collect ( Collectors . toList ( ) ) ; LOGGER . debug ( "Connected devices: " . concat ( connectedDevices . toString ( ) ) ) ; return connectedDevices ; }
public void test() { try { final JSONArray elementArray = new JSONArray ( ) ; code_block = ForStatement ; this . resultObject . put ( Tokens . RESULTS , elementArray ) ; this . resultObject . put ( Tokens . TOTAL_SIZE , counter ) ; this . resultObject . put ( Tokens . QUERY_TIME , this . sh . stopWatch ( ) ) ; } catch ( JSONException ex ) { logger . error ( ex ) ; final JSONObject error = generateErrorObjectJsonFail ( ex ) ; throw new WebApplicationException ( Response . status ( Response . Status . INTERNAL_SERVER_ERROR ) . entity ( error ) . build ( ) ) ; } finally { indexElements . close ( ) ; rag . tryCommit ( ) ; } }
public void test() { if ( null != index && key != null && value != null ) { final CloseableIterable < Element > indexElements = ( CloseableIterable < Element > ) index . get ( key , value ) ; code_block = TryStatement ;  } else-if ( null == index ) { final String msg = "Could not find index [" + indexName + "] on graph [" + graphName + "]" ; logger . info ( msg ) ; final JSONObject error = generateErrorObject ( msg ) ; throw new WebApplicationException ( Response . status ( Response . Status . NOT_FOUND ) . entity ( error ) . build ( ) ) ; } else { final HashMap map = new HashMap ( ) ; map . put ( Tokens . QUERY_TIME , this . sh . stopWatch ( ) ) ; map . put ( Tokens . RESULTS , createJSONObject ( index ) ) ; this . resultObject = new JSONObject ( map ) ; } }
private void loadMetadataDefinitions ( ) throws IOException { LOGGER . info ( "Loading metadata definitions" ) ; String metadataDefEditHome = repository + File . separator + "core" + File . separator + "metadata" + File . separator + "def" ; final File [ ] inputDefFolders = FolderTools . getFilesInFolder ( metadataDefEditHome , "all" , "" ) ; code_block = ForStatement ; }
public void test() { for ( final File inputDef : inputDefTables ) { LOGGER . debug ( "reading metadata definition " + inputDef . getName ( ) ) ; DataObjectOperation inputDataObjectOperation = new DataObjectOperation ( inputDef . getAbsolutePath ( ) ) ; ObjectMapper inputObjectMapper = new ObjectMapper ( ) ; defTableDataObjects . addAll ( inputDataObjectOperation . getDataObjects ( ) ) ; String metadataDefFileName = inputDefFolder . getName ( ) + "Tables.json" ; String metadataDefFilePath = metadataDefEditHome + File . separator + metadataDefFileName ; inputObjectMapper . writerWithDefaultPrettyPrinter ( ) . writeValue ( new File ( metadataDefFilePath ) , defTableDataObjects ) ; FileTools . copyFromFileToFile ( metadataDefFilePath , repository + File . separator + "docs" + File . separator + "_data" + File . separator + "datamodel" + File . separator + metadataDefFileName ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Reading taskana configuration from {} with separator {}" , propertiesFile , separator ) ; } }
public void test() { try { return binder . getDynRealmTO ( dynRealmDAO . find ( key ) ) ; } catch ( Throwable ignore ) { LOG . debug ( "Unresolved reference" , ignore ) ; throw new UnresolvedReferenceException ( ignore ) ; } }
public void test() { if ( ! running ) { LOG . debug ( "snapshotState() called on closed source; returning null." ) ; } else { code_block = IfStatement ; sequenceNumsStateForCheckpoint . clear ( ) ; code_block = IfStatement ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Snapshotting state ..." ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Snapshotted state, last processed sequence numbers: {}, checkpoint id: {}, timestamp: {}" , lastStateSnapshot , context . getCheckpointId ( ) , context . getCheckpointTimestamp ( ) ) ; } }
@ Override public void stop ( ) { logger . info ( "{} destroying started." , ClassUtils . simpleClassName ( this ) ) ; code_block = IfStatement ; logger . info ( "{} destroying completed." , ClassUtils . simpleClassName ( this ) ) ; }
@ Override public void stop ( ) { logger . info ( "{} destroying started." , ClassUtils . simpleClassName ( this ) ) ; code_block = IfStatement ; logger . info ( "{} destroying completed." , ClassUtils . simpleClassName ( this ) ) ; }
@ Override public List < GenericEntity > getStudentSummaries ( String token , List < String > studentIds , String sessionId , String sectionId ) { long startTime = System . nanoTime ( ) ; List < GenericEntity > studentSummaries = entityManager . getStudents ( token , sectionId ) ; LOG . warn ( "@@@@@@@@@@@@@@@@@@ Benchmark for student section view: {}" , ( System . nanoTime ( ) - startTime ) * 1.0e-9 ) ; return studentSummaries ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "  query[" + queryStr + "]  start(" + YMD_DateFormat . format ( startEndDate [ 0 ] ) + ")  end(" + YMD_DateFormat . format ( startEndDate [ 1 ] ) + ")" ) ; } }
public void test() { try { return bundleContext . installBundle ( location , inputStream ) ; } catch ( BundleException bundleException ) { _log . error ( bundleException , bundleException ) ; throw new PortalException ( bundleException ) ; } }
public void test() { if ( mailet . getInitParameters ( ) . isDebug ( ) ) { LOGGER . debug ( "apparentlyTo set to: {}" , ( Object ) internetAddresses ) ; } }
public void test() { if ( ! ( rootCause instanceof SocketTimeoutException || rootCause instanceof ConnectException ) ) { SAMPLING_LOG . warn ( "Error checking bootstrap state. " + "Bootstrap steps will not be run until state can be checked." , t ) ; } }
public void test() { if ( pos > 0 ) { String property = location . substring ( 2 , pos ) ; String rest = location . substring ( pos + 1 ) ; String value = System . getProperty ( property ) ; code_block = IfStatement ; result = value + rest ; } else { log . warn ( "Cache location {} looks like a property reference but closing } }
public void test() { if ( isDebugEnabled ) { logger . debug ( "{}: Received remove interest message of length ({} bytes)" , this , clientMessage . getPayloadLength ( ) ) ; } }
public void test() { if ( isDebugEnabled ) { logger . debug ( "{}: Region named {} does not exist" , this , regionName ) ; } }
public void test() { try { List < XAResource > resources = getXAResourcesForXid ( xid ) ; code_block = IfStatement ; String xidString = displayXid ( xid ) ; log . info ( "Initiating recovery 'forget' processing for Xid:\n" + xidString ) ; setBypassFailures ( Boolean . TRUE ) ; code_block = ForStatement ; log . info ( "Finished recovery 'forget' processing for Xid:\n" + xidString ) ; } finally { setBypassFailures ( Boolean . FALSE ) ; log . exiting ( this . getClass ( ) . getName ( ) , "forget" ) ; } }
public void test() { try { LateralUDPReceiver lur = new LateralUDPReceiver ( "228.5.6.7" , 6789 ) ; Thread t = new Thread ( lur ) ; t . start ( ) ; } catch ( Exception e ) { log . error ( e . toString ( ) ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "The candidate row keys to " + partitionPathFilePair + " => " + candidateRecordKeys ) ; } }
public KeyLookupResult getLookupResult ( ) { code_block = IfStatement ; HoodieBaseFile dataFile = getLatestDataFile ( ) ; List < String > matchingKeys = checkCandidatesAgainstFile ( hoodieTable . getHadoopConf ( ) , candidateRecordKeys , new Path ( dataFile . getPath ( ) ) ) ; LOG . info ( String . format ( "Total records (%d), bloom filter candidates (%d)/fp(%d), actual matches (%d)" , totalKeysChecked , candidateRecordKeys . size ( ) , candidateRecordKeys . size ( ) - matchingKeys . size ( ) , matchingKeys . size ( ) ) ) ; return new KeyLookupResult ( partitionPathFilePair . getRight ( ) , partitionPathFilePair . getLeft ( ) , dataFile . getCommitTime ( ) , matchingKeys ) ; }
public void test() { if ( documentAttribute == null ) { log . warn ( "documentAttribute for hitTerm:" + hitTerm + " is null in document:" + document ) ; } else-if ( documentAttribute instanceof Attributes ) { Attributes documentAttributes = ( Attributes ) documentAttribute ; code_block = ForStatement ; } else { code_block = IfStatement ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "recording bulkOp event {} {} {} op={}" , threadID . expensiveToString ( ) , eventID , tag , event . getOperation ( ) ) ; } }
public void test() { if ( view . size ( ) > 1 ) { List < ID > coords = view . getPreferredCoordinators ( Collections . emptySet ( ) , localAddress , 5 ) ; logger . debug ( "Sending my leave request to {}" , coords ) ; LeaveRequestMessage < ID > m = new LeaveRequestMessage < > ( coords , this . localAddress , "this member is shutting down" ) ; services . getMessenger ( ) . send ( m ) ; } }
private void updateRouterNetworkRef ( Connection conn ) { s_logger . debug ( "Updating router network ref" ) ; PreparedStatement pstmt = null ; ResultSet rs = null ; code_block = TryStatement ;  s_logger . debug ( "Done updating router/network references" ) ; }
public void test() { try { File path = new File ( dir ) ; if ( ! path . exists ( ) ) path . mkdirs ( ) ; if ( ! path . exists ( ) || ! path . isDirectory ( ) || ! path . canWrite ( ) ) throw new PermanentBackendException ( "Cannot access or write to directory: " + dir ) ; log . debug ( "Opening store directory [{}]" , path ) ; return FSDirectory . open ( path ) ; } catch ( IOException e ) { throw new PermanentBackendException ( "Could not open directory: " + dir , e ) ; } }
public void test() { if ( partitionToWriteStats . containsKey ( null ) ) { LOG . info ( "partition path is null to " + partitionToWriteStats . get ( null ) ) ; partitionToWriteStats . remove ( null ) ; } }
public void test() { if ( partitionToReplaceFileIds . containsKey ( null ) ) { LOG . info ( "partition path is null to " + partitionToReplaceFileIds . get ( null ) ) ; partitionToReplaceFileIds . remove ( null ) ; } }
public void test() { if ( workflowClass != null ) { Log . debug ( "requiresNewLauncher - byClass " + workflowClass ) ; return true ; } else-if ( ( workflowEngine != null && workflowEngine . contains ( "Oozie" ) && ! workflowEngine . contains ( "Pegasus" ) ) || ( workflowType != null && workflowType . contains ( "ftl2" ) ) ) { Log . debug ( "requiresNewLauncher - byEngine or Type " + workflowEngine + " " + workflowType ) ; return true ; } }
public void test() { if ( workflowClass != null ) { Log . debug ( "requiresNewLauncher - byClass " + workflowClass ) ; return true ; } else-if ( ( workflowEngine != null && workflowEngine . contains ( "Oozie" ) && ! workflowEngine . contains ( "Pegasus" ) ) || ( workflowType != null && workflowType . contains ( "ftl2" ) ) ) { Log . debug ( "requiresNewLauncher - byEngine or Type " + workflowEngine + " " + workflowType ) ; return true ; } }
public void test() { if ( line . getNetwork ( ) == null ) { log . error ( "line " + line . getObjectId ( ) + " : missing network" ) ; return false ; } }
public synchronized final void start ( Module ... modules ) throws Throwable { code_block = IfStatement ; logger . debug ( "appContext -> findModules." ) ; ArrayList < Module > findModules = new ArrayList < > ( ) ; findModules . addAll ( Arrays . asList ( this . findModules ( ) ) ) ; findModules . addAll ( Arrays . asList ( modules ) ) ; logger . debug ( "appContext -> doInitialize." ) ; this . getContainer ( ) . preInitialize ( ) ; ApiBinder apiBinder = newApiBinder ( ) ; doBindBefore ( apiBinder ) ; code_block = ForStatement ; logger . debug ( "appContext -> doBind." ) ; doBindAfter ( apiBinder ) ; this . getContainer ( ) . init ( ) ; doInitializeCompleted ( ) ; logger . debug ( "appContext -> doInitializeCompleted" ) ; Runtime . getRuntime ( ) . addShutdownHook ( shutdownHook ) ; logger . debug ( "appContext -> doStart" ) ; doStart ( ) ; logger . debug ( "appContext -> fireSyncEvent ,eventType = {}" , ContextEvent_Started ) ; getEnvironment ( ) . getEventContext ( ) . fireSyncEvent ( ContextEvent_Started , this ) ; doStartCompleted ( ) ; logger . info ( "Hasor StartCompleted!" ) ; status . compareAndSet ( Processing , Started ) ; }
public synchronized final void start ( Module ... modules ) throws Throwable { code_block = IfStatement ; logger . debug ( "appContext -> findModules." ) ; ArrayList < Module > findModules = new ArrayList < > ( ) ; findModules . addAll ( Arrays . asList ( this . findModules ( ) ) ) ; findModules . addAll ( Arrays . asList ( modules ) ) ; logger . debug ( "appContext -> doInitialize." ) ; this . getContainer ( ) . preInitialize ( ) ; ApiBinder apiBinder = newApiBinder ( ) ; doBindBefore ( apiBinder ) ; code_block = ForStatement ; logger . debug ( "appContext -> doBind." ) ; doBindAfter ( apiBinder ) ; this . getContainer ( ) . init ( ) ; doInitializeCompleted ( ) ; logger . debug ( "appContext -> doInitializeCompleted" ) ; Runtime . getRuntime ( ) . addShutdownHook ( shutdownHook ) ; logger . debug ( "appContext -> doStart" ) ; doStart ( ) ; logger . debug ( "appContext -> fireSyncEvent ,eventType = {}" , ContextEvent_Started ) ; getEnvironment ( ) . getEventContext ( ) . fireSyncEvent ( ContextEvent_Started , this ) ; doStartCompleted ( ) ; logger . info ( "Hasor StartCompleted!" ) ; status . compareAndSet ( Processing , Started ) ; }
public synchronized final void start ( Module ... modules ) throws Throwable { code_block = IfStatement ; logger . debug ( "appContext -> findModules." ) ; ArrayList < Module > findModules = new ArrayList < > ( ) ; findModules . addAll ( Arrays . asList ( this . findModules ( ) ) ) ; findModules . addAll ( Arrays . asList ( modules ) ) ; logger . debug ( "appContext -> doInitialize." ) ; this . getContainer ( ) . preInitialize ( ) ; ApiBinder apiBinder = newApiBinder ( ) ; doBindBefore ( apiBinder ) ; code_block = ForStatement ; logger . debug ( "appContext -> doBind." ) ; doBindAfter ( apiBinder ) ; this . getContainer ( ) . init ( ) ; doInitializeCompleted ( ) ; logger . debug ( "appContext -> doInitializeCompleted" ) ; Runtime . getRuntime ( ) . addShutdownHook ( shutdownHook ) ; logger . debug ( "appContext -> doStart" ) ; doStart ( ) ; logger . debug ( "appContext -> fireSyncEvent ,eventType = {}" , ContextEvent_Started ) ; getEnvironment ( ) . getEventContext ( ) . fireSyncEvent ( ContextEvent_Started , this ) ; doStartCompleted ( ) ; logger . info ( "Hasor StartCompleted!" ) ; status . compareAndSet ( Processing , Started ) ; }
public synchronized final void start ( Module ... modules ) throws Throwable { code_block = IfStatement ; logger . debug ( "appContext -> findModules." ) ; ArrayList < Module > findModules = new ArrayList < > ( ) ; findModules . addAll ( Arrays . asList ( this . findModules ( ) ) ) ; findModules . addAll ( Arrays . asList ( modules ) ) ; logger . debug ( "appContext -> doInitialize." ) ; this . getContainer ( ) . preInitialize ( ) ; ApiBinder apiBinder = newApiBinder ( ) ; doBindBefore ( apiBinder ) ; code_block = ForStatement ; logger . debug ( "appContext -> doBind." ) ; doBindAfter ( apiBinder ) ; this . getContainer ( ) . init ( ) ; doInitializeCompleted ( ) ; logger . debug ( "appContext -> doInitializeCompleted" ) ; Runtime . getRuntime ( ) . addShutdownHook ( shutdownHook ) ; logger . debug ( "appContext -> doStart" ) ; doStart ( ) ; logger . debug ( "appContext -> fireSyncEvent ,eventType = {}" , ContextEvent_Started ) ; getEnvironment ( ) . getEventContext ( ) . fireSyncEvent ( ContextEvent_Started , this ) ; doStartCompleted ( ) ; logger . info ( "Hasor StartCompleted!" ) ; status . compareAndSet ( Processing , Started ) ; }
public synchronized final void start ( Module ... modules ) throws Throwable { code_block = IfStatement ; logger . debug ( "appContext -> findModules." ) ; ArrayList < Module > findModules = new ArrayList < > ( ) ; findModules . addAll ( Arrays . asList ( this . findModules ( ) ) ) ; findModules . addAll ( Arrays . asList ( modules ) ) ; logger . debug ( "appContext -> doInitialize." ) ; this . getContainer ( ) . preInitialize ( ) ; ApiBinder apiBinder = newApiBinder ( ) ; doBindBefore ( apiBinder ) ; code_block = ForStatement ; logger . debug ( "appContext -> doBind." ) ; doBindAfter ( apiBinder ) ; this . getContainer ( ) . init ( ) ; doInitializeCompleted ( ) ; logger . debug ( "appContext -> doInitializeCompleted" ) ; Runtime . getRuntime ( ) . addShutdownHook ( shutdownHook ) ; logger . debug ( "appContext -> doStart" ) ; doStart ( ) ; logger . debug ( "appContext -> fireSyncEvent ,eventType = {}" , ContextEvent_Started ) ; getEnvironment ( ) . getEventContext ( ) . fireSyncEvent ( ContextEvent_Started , this ) ; doStartCompleted ( ) ; logger . info ( "Hasor StartCompleted!" ) ; status . compareAndSet ( Processing , Started ) ; }
public synchronized final void start ( Module ... modules ) throws Throwable { code_block = IfStatement ; logger . debug ( "appContext -> findModules." ) ; ArrayList < Module > findModules = new ArrayList < > ( ) ; findModules . addAll ( Arrays . asList ( this . findModules ( ) ) ) ; findModules . addAll ( Arrays . asList ( modules ) ) ; logger . debug ( "appContext -> doInitialize." ) ; this . getContainer ( ) . preInitialize ( ) ; ApiBinder apiBinder = newApiBinder ( ) ; doBindBefore ( apiBinder ) ; code_block = ForStatement ; logger . debug ( "appContext -> doBind." ) ; doBindAfter ( apiBinder ) ; this . getContainer ( ) . init ( ) ; doInitializeCompleted ( ) ; logger . debug ( "appContext -> doInitializeCompleted" ) ; Runtime . getRuntime ( ) . addShutdownHook ( shutdownHook ) ; logger . debug ( "appContext -> doStart" ) ; doStart ( ) ; logger . debug ( "appContext -> fireSyncEvent ,eventType = {}" , ContextEvent_Started ) ; getEnvironment ( ) . getEventContext ( ) . fireSyncEvent ( ContextEvent_Started , this ) ; doStartCompleted ( ) ; logger . info ( "Hasor StartCompleted!" ) ; status . compareAndSet ( Processing , Started ) ; }
public synchronized final void start ( Module ... modules ) throws Throwable { code_block = IfStatement ; logger . debug ( "appContext -> findModules." ) ; ArrayList < Module > findModules = new ArrayList < > ( ) ; findModules . addAll ( Arrays . asList ( this . findModules ( ) ) ) ; findModules . addAll ( Arrays . asList ( modules ) ) ; logger . debug ( "appContext -> doInitialize." ) ; this . getContainer ( ) . preInitialize ( ) ; ApiBinder apiBinder = newApiBinder ( ) ; doBindBefore ( apiBinder ) ; code_block = ForStatement ; logger . debug ( "appContext -> doBind." ) ; doBindAfter ( apiBinder ) ; this . getContainer ( ) . init ( ) ; doInitializeCompleted ( ) ; logger . debug ( "appContext -> doInitializeCompleted" ) ; Runtime . getRuntime ( ) . addShutdownHook ( shutdownHook ) ; logger . debug ( "appContext -> doStart" ) ; doStart ( ) ; logger . debug ( "appContext -> fireSyncEvent ,eventType = {}" , ContextEvent_Started ) ; getEnvironment ( ) . getEventContext ( ) . fireSyncEvent ( ContextEvent_Started , this ) ; doStartCompleted ( ) ; logger . info ( "Hasor StartCompleted!" ) ; status . compareAndSet ( Processing , Started ) ; }
@ Override public void initialize ( final ExtensionContext context ) { KeycloakLogger . ROOT_LOGGER . debug ( "Activating Keycloak Extension" ) ; final SubsystemRegistration subsystem = context . registerSubsystem ( SUBSYSTEM_NAME , MGMT_API_VERSION ) ; ManagementResourceRegistration registration = subsystem . registerSubsystemModel ( KEYCLOAK_SUBSYSTEM_RESOURCE ) ; registration . registerSubModel ( REALM_DEFINITION ) ; ManagementResourceRegistration secureDeploymentRegistration = registration . registerSubModel ( SECURE_DEPLOYMENT_DEFINITION ) ; secureDeploymentRegistration . registerSubModel ( CREDENTIAL_DEFINITION ) ; secureDeploymentRegistration . registerSubModel ( REDIRECT_RULE_DEFINITON ) ; ManagementResourceRegistration secureServerRegistration = registration . registerSubModel ( SECURE_SERVER_DEFINITION ) ; secureServerRegistration . registerSubModel ( CREDENTIAL_DEFINITION ) ; secureServerRegistration . registerSubModel ( REDIRECT_RULE_DEFINITON ) ; subsystem . registerXMLElementWriter ( PARSER ) ; }
public void test() { try ( BufferedReader reader = new BufferedReader ( new FileReader ( configFile ) , 1024 ) ) { String line ; code_block = WhileStatement ; } catch ( Exception e ) { log . error ( e ) ; } }
public void test() { try { PrivilegedCarbonContext . startTenantFlow ( ) ; PrivilegedCarbonContext . getThreadLocalCarbonContext ( ) . setTenantDomain ( DeviceTypeConstants . DEVICE_TYPE_PROVIDER_DOMAIN , true ) ; DeviceTypeManagementDataHolder . getInstance ( ) . getOutputEventAdapterService ( ) . create ( outputEventAdapterConfiguration ) ; } catch ( OutputEventAdapterException e ) { log . error ( "Unable to create Output Event Adapter : " + DeviceTypeConstants . MQTT_ADAPTER_NAME , e ) ; } finally { PrivilegedCarbonContext . endTenantFlow ( ) ; } }
public void test() { if ( volumeMap . containsKey ( hddsRoot ) ) { LOG . warn ( "Volume : {} already exists in VolumeMap" , hddsRoot ) ; success = false ; } else { code_block = IfStatement ; HddsVolume hddsVolume = createVolume ( volumeRoot , storageType ) ; volumeMap . put ( hddsVolume . getHddsRootDir ( ) . getPath ( ) , hddsVolume ) ; volumeStateMap . get ( hddsVolume . getStorageType ( ) ) . add ( hddsVolume ) ; LOG . info ( "Added Volume : {} to VolumeSet" , hddsVolume . getHddsRootDir ( ) . getPath ( ) ) ; success = true ; } }
public void test() { try { code_block = IfStatement ; } catch ( IOException ex ) { LOG . error ( "Failed to add volume " + volumeRoot + " to VolumeSet" , ex ) ; success = false ; } finally { this . writeUnlock ( ) ; } }
public void test() { if ( ! silent ) { logger . debug ( "Registering reflective access to " + typename + ": " + ( flags == null ? "" : Arrays . asList ( flags ) ) ) ; } }
public void test() { if ( ! isOK ) { logger . debug ( typename + " discarded due to access check by " + accessChecker . getClass ( ) . getName ( ) ) ; return ; } }
public void test() { try { listener . gotRetweetsOfMe ( statuses ) ; } catch ( Exception e ) { logger . warn ( "Exception at getRwtweetsOfMe" , e ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( _NO_SUCH_ENTITY_WITH_PRIMARY_KEY + primaryKey ) ; } }
public void test() { try { IoUtils . cleanDirectory ( tmp ) ; LOGGER . info ( "success delete " + envName + "-snapshot" ) ; } catch ( IOException e ) { LOGGER . info ( "fail delete " + envName + "-snapshot, " + e . toString ( ) ) ; e . printStackTrace ( ) ; } }
public void test() { switch ( event . encodeType ( EventTypes . class ) ) { case DISCONNECT_EVENT : setTimer ( REC_TIMEOUT ) ; switchToNextState ( FsmState . REOPEN ) ; break ; case TIMEOUT_EVENT : doDisconnect ( ) ; setTimer ( REC_TIMEOUT ) ; switchToNextState ( FsmState . REOPEN ) ; break ; case STOP_EVENT : clearTimer ( ) ; doDisconnect ( ) ; switchToNextState ( FsmState . DOWN ) ; break ; case CEA_EVENT : clearTimer ( ) ; code_block = IfStatement ; break ; case SEND_MSG_EVENT : throw new RuntimeException ( "Connection is down" ) ; default : logger . debug ( "Unknown event type: {} in state {}" , event . encodeType ( EventTypes . class ) , state ) ; return false ; } }
public static String compressBBSImage ( String oldFilePath ) { BitmapFactory . Options options = BitmapUtil . INSTANCE . getImageOptions ( oldFilePath ) ; int width = options . outWidth ; int height = options . outHeight ; int showWidth = width ; int showHeight = height ; code_block = IfStatement ; Bitmap bitmap = BitmapUtil . INSTANCE . getFitSampleBitmap ( oldFilePath , showWidth , showHeight ) ; String filePath = FileExtensionHelper . generateBBSTempFilePath ( ) ; XLog . debug ( "BBSä¸ä¼ æä»¶ åç¼©åçæ°æä»¶è·¯å¾ï¼" + filePath ) ; SDCardHelper . INSTANCE . bitmapToPNGFile ( bitmap , filePath ) ; return filePath ; }
public void test() { try { if ( ! Files . exists ( pathToDir ) ) Files . createDirectory ( pathToDir ) ; } catch ( IOException e ) { LOG . error ( "Cannot create folder: " + pathToDir . toAbsolutePath ( ) , e ) ; } }
private void checkDescribeOpportunities ( ) throws IOException { MarketoSource source = new MarketoSource ( ) ; source . initialize ( null , iprops ) ; MarketoRESTClient client = ( MarketoRESTClient ) source . getClientService ( null ) ; MarketoRecordResult opps = client . describeOpportunity ( iprops ) ; LOG . debug ( "opps = {}." , opps ) ; assertNotNull ( opps . getRecords ( ) ) ; assertNotNull ( opps . getRecords ( ) . get ( 0 ) ) ; IndexedRecord record = opps . getRecords ( ) . get ( 0 ) ; assertNotNull ( record . get ( record . getSchema ( ) . getField ( "idField" ) . pos ( ) ) ) ; assertNotNull ( record . get ( record . getSchema ( ) . getField ( "dedupeFields" ) . pos ( ) ) ) ; assertNotNull ( record . get ( record . getSchema ( ) . getField ( "searchableFields" ) . pos ( ) ) ) ; assertNotNull ( record . get ( record . getSchema ( ) . getField ( "fields" ) . pos ( ) ) ) ; }
public void test() { if ( _logger . isDebugEnabled ( ) ) { _logger . debug ( "Created ServerTypeDesc for type [" + typeName + "]." + getClientAddressAddition ( ) ) ; if ( _logger . isTraceEnabled ( ) ) logTypeMap ( localTypeMap ) ; else-if ( _logger . isDebugEnabled ( ) ) logServerTypeDesc ( serverTypeDesc , LogLevel . DEBUG ) ; } }
public void test() { try { return Optional . of ( dataSourceParameter . getHadoopFileSystem ( path ) . getFileStatus ( path ) ) ; } catch ( FileNotFoundException e ) { LOG . trace ( "not found: {}" , path , e ) ; return Optional . empty ( ) ; } catch ( IOException e ) { throw new CommandConfigurationException ( MessageFormat . format ( "error occurred while resolving Hadoop path: {0}" , path ) , e ) ; } }
@ Test public void sqlBench ( ) { int warmUpIterations = 100 ; int realIterations = 200 ; SqlService sqlService = inst . getSql ( ) ; logger . info ( "will submit " + warmUpIterations + " jobs" ) ; inst . getMap ( "m" ) . put ( 1 , 1 ) ; int numRows = 0 ; code_block = ForStatement ; logger . info ( "warmup jobs done, starting benchmark" ) ; long start = System . nanoTime ( ) ; code_block = ForStatement ; long elapsedMicros = NANOSECONDS . toMicros ( System . nanoTime ( ) - start ) ; System . out . println ( numRows ) ; System . out . println ( realIterations + " queries run in " + ( elapsedMicros / realIterations ) + " us/job" ) ; }
@ Test public void sqlBench ( ) { int warmUpIterations = 100 ; int realIterations = 200 ; SqlService sqlService = inst . getSql ( ) ; logger . info ( "will submit " + warmUpIterations + " jobs" ) ; inst . getMap ( "m" ) . put ( 1 , 1 ) ; int numRows = 0 ; code_block = ForStatement ; logger . info ( "warmup jobs done, starting benchmark" ) ; long start = System . nanoTime ( ) ; code_block = ForStatement ; long elapsedMicros = NANOSECONDS . toMicros ( System . nanoTime ( ) - start ) ; System . out . println ( numRows ) ; System . out . println ( realIterations + " queries run in " + ( elapsedMicros / realIterations ) + " us/job" ) ; }
public void test() { try { isStart = ( ( Boolean ) UnoProperty . getProperty ( textPortion , UnoProperty . IS_START ) ) . booleanValue ( ) ; isCollapsed = ( ( Boolean ) UnoProperty . getProperty ( textPortion , UnoProperty . IS_COLLAPSED ) ) . booleanValue ( ) ; code_block = IfStatement ; bookmark = UNO . XNamed ( UnoProperty . getProperty ( textPortion , UnoProperty . BOOKMARK ) ) ; } catch ( Exception x ) { LOGGER . trace ( "" , x ) ; return ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { Runtime runtime = Runtime . getRuntime ( ) ; logger . debug ( "Step: " + info + " memory: free / total / max MB " + runtime . freeMemory ( ) / ( 1000 * 1000 ) + " / " + runtime . totalMemory ( ) / ( 1000 * 1000 ) + " / " + runtime . maxMemory ( ) / ( 1000 * 1000 ) ) ; } }
public void test() { try { final Session < Client > session = pool . borrow ( BackgroundActionState . running ) ; code_block = TryStatement ;  } catch ( ConnectionCanceledException e ) { log . warn ( "Cancel processing scheduled task. %s" , e ) ; this . shutdown ( ) ; } catch ( BackgroundException e ) { log . warn ( String . format ( "Failure processing scheduled task. %s" , e . getMessage ( ) ) , e ) ; } catch ( Exception e ) { log . error ( String . format ( "Failure processing scheduled task. %s" , e . getMessage ( ) ) , e ) ; this . shutdown ( ) ; } }
public void test() { try { final Session < Client > session = pool . borrow ( BackgroundActionState . running ) ; code_block = TryStatement ;  } catch ( ConnectionCanceledException e ) { log . warn ( "Cancel processing scheduled task. %s" , e ) ; this . shutdown ( ) ; } catch ( BackgroundException e ) { log . warn ( String . format ( "Failure processing scheduled task. %s" , e . getMessage ( ) ) , e ) ; } catch ( Exception e ) { log . error ( String . format ( "Failure processing scheduled task. %s" , e . getMessage ( ) ) , e ) ; this . shutdown ( ) ; } }
public void test() { try { final Session < Client > session = pool . borrow ( BackgroundActionState . running ) ; code_block = TryStatement ;  } catch ( ConnectionCanceledException e ) { log . warn ( "Cancel processing scheduled task. %s" , e ) ; this . shutdown ( ) ; } catch ( BackgroundException e ) { log . warn ( String . format ( "Failure processing scheduled task. %s" , e . getMessage ( ) ) , e ) ; } catch ( Exception e ) { log . error ( String . format ( "Failure processing scheduled task. %s" , e . getMessage ( ) ) , e ) ; this . shutdown ( ) ; } }
public void test() { try { String dataSource = ( environment instanceof BazaarEnvironment || String . format ( "Of %s" , Common . BAZAAR_ID ) . equals ( environment . getName ( ) ) ) ? Common . BAZAAR_ID : Common . SUBUTAI_ID ; EnvironmentDto environmentDto = new EnvironmentDto ( environment . getId ( ) , environment . getName ( ) , environment . getStatus ( ) , convertContainersToContainerJson ( environment . getContainerHosts ( ) , dataSource ) , dataSource , environmentManager . getEnvironmentOwnerName ( environment ) ) ; environmentDtos . add ( environmentDto ) ; } catch ( Exception e ) { LOG . error ( "Error JSON-ifying environment {}: {}" , environment . getId ( ) , e . getMessage ( ) ) ; } }
@ Override public void run ( ) { LOG . trace ( "Consumer {} drain request timed out" , this ) ; Exception cause = new JmsOperationTimedOutException ( "Remote did not respond to a drain request in time" ) ; locallyClosed ( session . getConnection ( ) , cause ) ; stopRequest . onFailure ( cause ) ; session . pumpToProtonTransport ( stopRequest ) ; }
@ Override public void collisionResolve ( SpaceStationMir mir ) { LOGGER . info ( "{} hits {}." , mir . getClass ( ) . getSimpleName ( ) , this . getClass ( ) . getSimpleName ( ) ) ; }
public void test() { if ( GroupPermissionUtil . contains ( permissionChecker , scopeGroupId , ActionKeys . EXPORT_IMPORT_PORTLET_INFO ) && stagingGroupHelper . isStagingGroup ( scopeGroupId ) && stagingGroupHelper . isStagedPortlet ( scopeGroupId , JournalPortletKeys . JOURNAL ) ) { return true ; } }
public void test() { try { Integer bgColor = Integer . valueOf ( Integer . parseInt ( highlightColor , 16 ) ) ; XTextCursor cursor = getTextCursor ( ) ; Utils . setProperty ( cursor , UnoProperty . CHAR_BACK_COLOR , bgColor ) ; cursor . collapseToEnd ( ) ; UnoProperty . setPropertyToDefault ( cursor , UnoProperty . CHAR_BACK_COLOR ) ; } catch ( NumberFormatException | UnoHelperException e ) { LOGGER . error ( L . m ( "Fehler in Dokumentkommando '%1': Die Farbe HIGHLIGHT_COLOR mit dem Wert '%2' ist ungÃ¼ltig." , "" + this , highlightColor ) ) ; } }
public void test() { try { UnoProperty . setPropertyToDefault ( getTextCursor ( ) , UnoProperty . CHAR_BACK_COLOR ) ; } catch ( UnoHelperException e ) { LOGGER . error ( "Couldn't set background color." , e ) ; } }
public void test() { if ( state < 0 ) { LOG . trace ( "[{}] Got result but the request has been cancelled, ignoring" , logPrefix ) ; return ; } }
public void test() { if ( responseMessage instanceof Result ) { LOG . trace ( "[{}] Got result" , logPrefix ) ; processResultResponse ( ( Result ) responseMessage , response ) ; } else-if ( responseMessage instanceof Error ) { LOG . trace ( "[{}] Got error response" , logPrefix ) ; processErrorResponse ( ( Error ) responseMessage ) ; } else { IllegalStateException error = new IllegalStateException ( "Unexpected response " + responseMessage ) ; trackNodeError ( node , error ) ; abort ( error , false ) ; } }
public void test() { if ( responseMessage instanceof Result ) { LOG . trace ( "[{}] Got result" , logPrefix ) ; processResultResponse ( ( Result ) responseMessage , response ) ; } else-if ( responseMessage instanceof Error ) { LOG . trace ( "[{}] Got error response" , logPrefix ) ; processErrorResponse ( ( Error ) responseMessage ) ; } else { IllegalStateException error = new IllegalStateException ( "Unexpected response " + responseMessage ) ; trackNodeError ( node , error ) ; abort ( error , false ) ; } }
public void test() { if ( stop ( ) ) { logger . info ( "[OneHourPeriodTask] alert system is on, stop task" ) ; future . cancel ( true ) ; return ; } }
public void test() { try { code_block = IfStatement ; alert ( ) ; } catch ( Exception e ) { logger . error ( "[run] {}" , e ) ; } }
public void test() { try { String cmd = BashCommands . prependToEtcHosts ( "1.2.3.4" , "myhostnamefor1234.at.start" , "myhostnamefor1234b" ) ; execRequiringZeroAndReturningStdout ( loc , cmd ) . get ( ) ; String cmd2 = BashCommands . appendToEtcHosts ( "5.6.7.8" , "myhostnamefor5678.at.end" , "myhostnamefor5678" ) ; execRequiringZeroAndReturningStdout ( loc , cmd2 ) . get ( ) ; String grepFirst = execRequiringZeroAndReturningStdout ( loc , "grep -n myhostnamefor1234 /etc/hosts" ) . get ( ) ; String grepLast = execRequiringZeroAndReturningStdout ( loc , "grep -n myhostnamefor5678 /etc/hosts" ) . get ( ) ; int numLinesAfter = Integer . parseInt ( execRequiringZeroAndReturningStdout ( loc , "wc -l /etc/hosts" ) . get ( ) . trim ( ) . split ( "\\s" ) [ 0 ] ) ; log . info ( "result: numLinesBefore=" + numLinesOrig + "; numLinesAfter=" + numLinesAfter + "; first=" + grepFirst + "; last=" + grepLast ) ; assertTrue ( grepFirst . startsWith ( "1:" ) && grepFirst . contains ( "1.2.3.4 myhostnamefor1234.at.start myhostnamefor1234" ) , "first=" + grepFirst ) ; assertTrue ( grepLast . startsWith ( ( numLinesOrig + 2 ) + ":" ) && grepLast . contains ( "5.6.7.8 myhostnamefor5678.at.end myhostnamefor5678" ) , "last=" + grepLast ) ; assertEquals ( numLinesOrig + 2 , numLinesAfter , "lines orig=" + numLinesOrig + ", after=" + numLinesAfter ) ; } finally { execRequiringZeroAndReturningStdout ( loc , sudo ( "cp /etc/hosts-orig-testModifyEtcHosts /etc/hosts" ) ) . get ( ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( "addForward" ) ; } }
public void test() { if ( lastRunTime + 1000 < potentialWriteTime ) { logger . debug ( "Writing last partial histo log:" + this ) ; run ( ) ; } else { logger . debug ( "Not writing last partial histo log <1s:" + this ) ; } }
public void test() { if ( lastRunTime + 1000 < potentialWriteTime ) { logger . debug ( "Writing last partial histo log:" + this ) ; run ( ) ; } else { logger . debug ( "Not writing last partial histo log <1s:" + this ) ; } }
public void test() { try { value = field . get ( this ) ; code_block = IfStatement ; } catch ( final Exception ex ) { LOGGER . error ( "An error occurred validating plugin options for [" + this . getClass ( ) . getName ( ) + "]: " + ex . getLocalizedMessage ( ) , ex ) ; } }
public void test() { try { LOG . debug ( "Getting all enterprise events occurring between {} and {} {}" , after == null ? "unspecified date" : DateFormat . getDateTimeInstance ( ) . format ( after ) , before == null ? "unspecified date" : DateFormat . getDateTimeInstance ( ) . format ( before ) , position == null ? "" : ( " starting at " + position ) ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; EventLog eventLog = EventLog . getEnterpriseEvents ( boxConnection , position , after , before , types ) ; List < BoxEvent > results = new ArrayList < > ( ) ; code_block = ForStatement ; return results ; } catch ( BoxAPIException e ) { throw new RuntimeException ( String . format ( "Box API returned the error code %d%n%n%s" , e . getResponseCode ( ) , e . getResponse ( ) ) , e ) ; } }
public void test() { try { E entity = getController ( ) . getEntityClass ( ) . newInstance ( ) ; code_block = ForStatement ; return entity ; } catch ( Exception e ) { logger . error ( "failed to create class " + getController ( ) . getEntityClass ( ) + ": " + e ) ; e . printStackTrace ( ) ; } }
public void test() { try { code_block = IfStatement ; FileWriter out = new FileWriter ( file ) ; gson . toJson ( clients , new TypeToken < Map < String , RegisteredClient > > ( ) code_block = "" ; . getType ( ) , out ) ; out . close ( ) ; } catch ( IOException e ) { logger . error ( "Could not write to output file" , e ) ; } }
public void test() { if ( device == null ) { log . error ( "device is null" ) ; return false ; } }
public void test() { try { code_block = IfStatement ; } catch ( RuntimeException e ) { environment . getProcessingEnvironment ( ) . getMessager ( ) . printMessage ( Diagnostic . Kind . ERROR , MessageFormat . format ( Messages . getString ( "AbstractOperatorAnnotationProcessor.errorFailCompile" ) , e . toString ( ) ) ) ; LOG . error ( Messages . getString ( "AbstractOperatorAnnotationProcessor.logFailCompile" ) , e ) ; } }
public void test() { try { m = wrappedStreamClass . getDeclaredMethod ( "getNumCurrentReplicas" , new Class < ? > [ ] code_block = "" ; ) ; m . setAccessible ( true ) ; } catch ( NoSuchMethodException e ) { logger . info ( "FileSystem's output stream doesn't support" + " getNumCurrentReplicas; --HDFS-826 not available; fsOut=" + wrappedStreamClass . getName ( ) + "; err=" + e ) ; } catch ( SecurityException e ) { logger . info ( "Doesn't have access to getNumCurrentReplicas on " + "FileSystems's output stream --HDFS-826 not available; fsOut=" + wrappedStreamClass . getName ( ) , e ) ; m = null ; } }
public void test() { try { m = wrappedStreamClass . getDeclaredMethod ( "getNumCurrentReplicas" , new Class < ? > [ ] code_block = "" ; ) ; m . setAccessible ( true ) ; } catch ( NoSuchMethodException e ) { logger . info ( "FileSystem's output stream doesn't support" + " getNumCurrentReplicas; --HDFS-826 not available; fsOut=" + wrappedStreamClass . getName ( ) + "; err=" + e ) ; } catch ( SecurityException e ) { logger . info ( "Doesn't have access to getNumCurrentReplicas on " + "FileSystems's output stream --HDFS-826 not available; fsOut=" + wrappedStreamClass . getName ( ) , e ) ; m = null ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "step is FORCE_INTERFACE" ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "step is APPLICATION_FIRST shouldMigrate true get serviceInvokers" ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "step is APPLICATION_FIRST " + ( serviceInvokers . isEmpty ( ) ? "serviceInvokers is empty" : "shouldMigrate false" ) + " get interfaceInvokers" ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "step is FORCE_APPLICATION" ) ; } }
@ Override public void onFailure ( IMqttToken token , Throwable throwable ) { log . error ( "error - {} {}" , tokenToString ( token ) , throwable ) ; }
@ SuppressWarnings ( "deprecation" ) protected void internalUnsubscribeNamespaceBundle ( String subscription , String bundleRange , boolean authoritative ) { validateNamespaceOperation ( namespaceName , NamespaceOperation . UNSUBSCRIBE ) ; checkNotNull ( subscription , "Subscription should not be null" ) ; checkNotNull ( bundleRange , "BundleRange should not be null" ) ; Policies policies = getNamespacePolicies ( namespaceName ) ; code_block = IfStatement ; validateNamespaceBundleOwnership ( namespaceName , policies . bundles , bundleRange , authoritative , true ) ; unsubscribe ( namespaceName , bundleRange , subscription ) ; log . info ( "[{}] Successfully unsubscribed {} on namespace bundle {}/{}" , clientAppId ( ) , subscription , namespaceName , bundleRange ) ; }
public void test() { try { code_block = IfStatement ; } catch ( RuntimeException e ) { logger . debug ( type + " is not a known service of the tapestry registry" ) ; } }
public void test() { if ( ! dumpDir . mkdirs ( ) && ! dumpDir . exists ( ) ) { logger . warn ( "create dump directory:{} failed." , dumpDir . getAbsolutePath ( ) ) ; return ; } }
public void test() { try { FileUtils . writeByteArrayToFile ( dumpClassFile , data ) ; dumpResult . put ( clazz , dumpClassFile ) ; } catch ( IOException e ) { logger . warn ( "dump class:{} to file {} failed." , className , dumpClassFile , e ) ; } }
private void deployTestSpecificResources ( ExtensionContext extensionContext ) { LOGGER . info ( "Creating resources before the test class" ) ; prepareEnvForOperator ( extensionContext , CO_NAMESPACE , Arrays . asList ( CO_NAMESPACE , SECOND_NAMESPACE , THIRD_NAMESPACE ) ) ; applyBindings ( extensionContext , CO_NAMESPACE ) ; List < ClusterRoleBinding > clusterRoleBindingList = ClusterRoleBindingTemplates . clusterRoleBindingsForAllNamespaces ( CO_NAMESPACE ) ; clusterRoleBindingList . forEach ( clusterRoleBinding -> ClusterRoleBindingResource . clusterRoleBinding ( extensionContext , clusterRoleBinding ) ) ; resourceManager . createResource ( extensionContext , BundleResource . clusterOperator ( CO_NAMESPACE , "*" , Constants . RECONCILIATION_INTERVAL ) . build ( ) ) ; String previousNamespace = cluster . setNamespace ( THIRD_NAMESPACE ) ; resourceManager . createResource ( extensionContext , KafkaTemplates . kafkaEphemeral ( MAIN_NAMESPACE_CLUSTER_NAME , 1 , 1 ) . editSpec ( ) . editEntityOperator ( ) . editTopicOperator ( ) . withWatchedNamespace ( SECOND_NAMESPACE ) . endTopicOperator ( ) . editUserOperator ( ) . withWatchedNamespace ( SECOND_NAMESPACE ) . endUserOperator ( ) . endEntityOperator ( ) . endSpec ( ) . build ( ) ) ; cluster . setNamespace ( SECOND_NAMESPACE ) ; resourceManager . createResource ( extensionContext , KafkaTemplates . kafkaEphemeral ( SECOND_CLUSTER_NAME , 3 ) . build ( ) ) ; cluster . setNamespace ( previousNamespace ) ; }
public void test() { try { stm = c . createStatement ( ) ; ResultSet rs = stm . executeQuery ( "SELECT " + HM_COLUMN . getColumnsInOrder ( ) + " FROM runningJobsMonitor" + " WHERE jobId=" + jobId ) ; code_block = IfStatement ; } catch ( SQLException e ) { String message = "SQL error querying runningJobsMonitor" + "\n" + ExceptionUtils . getSQLExceptionCause ( e ) ; log . warn ( message , e ) ; throw new IOFailure ( message , e ) ; } finally { DBUtils . closeStatementIfOpen ( stm ) ; HarvestDBConnection . release ( c ) ; } }
@ Override public void run ( Timeout timeout ) throws Exception { String message = "Request (" + _lastRequestId + ") to server " + _server + " timed-out waiting for response. Closing the channel !!" ; LOGGER . error ( message ) ; Exception e = new Exception ( message ) ; _outstandingFuture . get ( ) . onError ( e ) ; close ( ) ; }
private void restartWithSystem ( InternalDistributedSystem newSystem , InternalCache newCache ) throws IOException { synchronized ( locatorLock ) code_block = "" ; internalDistributedSystem = newSystem ; internalCache = newCache ; logger . info ( "Locator restart: initializing TcpServer" ) ; code_block = TryStatement ;  code_block = IfStatement ; code_block = IfStatement ; logger . info ( "Locator restart: initializing JMX manager" ) ; startJmxManagerLocationService ( newCache ) ; endStartLocator ( internalDistributedSystem ) ; logger . info ( "Locator restart completed" ) ; restartHandlers . forEach ( handler -> handler . restartCompleted ( newSystem ) ) ; }
public void test() { if ( ! membershipLocator . isAlive ( ) ) { logger . info ( "Locator restart: starting TcpServer" ) ; startTcpServer ( ) ; } }
private void restartWithSystem ( InternalDistributedSystem newSystem , InternalCache newCache ) throws IOException { synchronized ( locatorLock ) code_block = "" ; internalDistributedSystem = newSystem ; internalCache = newCache ; logger . info ( "Locator restart: initializing TcpServer" ) ; code_block = TryStatement ;  code_block = IfStatement ; code_block = IfStatement ; logger . info ( "Locator restart: initializing JMX manager" ) ; startJmxManagerLocationService ( newCache ) ; endStartLocator ( internalDistributedSystem ) ; logger . info ( "Locator restart completed" ) ; restartHandlers . forEach ( handler -> handler . restartCompleted ( newSystem ) ) ; }
private void restartWithSystem ( InternalDistributedSystem newSystem , InternalCache newCache ) throws IOException { synchronized ( locatorLock ) code_block = "" ; internalDistributedSystem = newSystem ; internalCache = newCache ; logger . info ( "Locator restart: initializing TcpServer" ) ; code_block = TryStatement ;  code_block = IfStatement ; code_block = IfStatement ; logger . info ( "Locator restart: initializing JMX manager" ) ; startJmxManagerLocationService ( newCache ) ; endStartLocator ( internalDistributedSystem ) ; logger . info ( "Locator restart completed" ) ; restartHandlers . forEach ( handler -> handler . restartCompleted ( newSystem ) ) ; }
public void test() { try { log . info ( "Start plugin '{}'" , getPluginLabel ( pluginWrapper . getDescriptor ( ) ) ) ; Plugin plugin = pluginWrapper . getPlugin ( ) ; plugin . start ( ) ; pluginWrapper . setPluginState ( PluginState . STARTED ) ; code_block = IfStatement ; startedPlugins . add ( pluginWrapper ) ; firePluginStateEvent ( new PluginStateEvent ( this , pluginWrapper , pluginState ) ) ; } catch ( Throwable e ) { log . error ( "Error while starting plugins " + e . getMessage ( ) , e ) ; } }
public void test() { if ( channels . containsKey ( name ) ) { String id = channels . get ( name ) ; transmitWithResult ( PATH_ZAP + UrlEncoded . encodeString ( id ) ) . ifPresent ( document -> channel = name ) ; } else { logger . warn ( "Channel {} not found." , name ) ; } }
@ Test ( enabled = true ) public void testExecutePassingUniqueOptionToHillClimb ( ) { log . info ( "=== TEST for SOLUTION GENERATION of HILLCLIMB optimizer SINGLE ADP - STARTED ===" ) ; optimizer = new Optimizer ( TestConstants . NUM_PLANS_TO_GENERATE , SearchMethodName . HILLCLIMB ) ; executeAndSave ( appModel , suitableCloudOffer , SearchMethodName . HILLCLIMB ) ; log . info ( "=== TEST for SOLUTION GENERATION with POLICIES of HILLCLIMB optimizer SINGLE ADP FINISEHD ===" ) ; }
@ Test ( enabled = true ) public void testExecutePassingUniqueOptionToHillClimb ( ) { log . info ( "=== TEST for SOLUTION GENERATION of HILLCLIMB optimizer SINGLE ADP - STARTED ===" ) ; optimizer = new Optimizer ( TestConstants . NUM_PLANS_TO_GENERATE , SearchMethodName . HILLCLIMB ) ; executeAndSave ( appModel , suitableCloudOffer , SearchMethodName . HILLCLIMB ) ; log . info ( "=== TEST for SOLUTION GENERATION with POLICIES of HILLCLIMB optimizer SINGLE ADP FINISEHD ===" ) ; }
public CharSequence postProcess ( CharSequence object ) { log . debug ( "Altering '" , object , "'" ) ; return ( prefix + object ) ; }
@ RequestMapping ( value = "/clusters/" + CLUSTER_NAME_PATH_VARIABLE , method = RequestMethod . PUT ) public void updateCluster ( @ PathVariable String clusterName , @ RequestBody ClusterTbl cluster ) { logger . info ( "[Update Cluster]{},{}" , clusterName , cluster ) ; clusterService . updateCluster ( clusterName , cluster ) ; }
public void test() { try { log . info ( "Refreshing Planet subscriptions" ) ; FeedUpdater updater = new SingleThreadedFeedUpdater ( ) ; updater . updateSubscriptions ( ) ; } catch ( Exception e ) { log . error ( "ERROR refreshing planet" , e ) ; } finally { WebloggerFactory . getWeblogger ( ) . release ( ) ; } }
public void test() { try { log . info ( "Refreshing Planet subscriptions" ) ; FeedUpdater updater = new SingleThreadedFeedUpdater ( ) ; updater . updateSubscriptions ( ) ; } catch ( Exception e ) { log . error ( "ERROR refreshing planet" , e ) ; } finally { WebloggerFactory . getWeblogger ( ) . release ( ) ; } }
public void test() { try { dataObjectsId = this . extractContentsId ( bean , reqCtx ) ; dataObjectsId = this . executeFullTextSearch ( bean , dataObjectsId , reqCtx ) ; } catch ( Throwable t ) { _logger . error ( "Error extracting dataObjects id" , t ) ; throw new ApsSystemException ( "Error extracting dataObjects id" , t ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( InterruptedException e ) { log . warn ( "Oracle leadership watcher has been interrupted unexpectedly" ) ; } }
protected void activate ( final ComponentContext componentContext , final Map < String , Object > properties ) { logger . debug ( "Activating Cloud Subscriber Wire Component..." ) ; this . wireSupport = this . wireHelperService . newWireSupport ( this , ( ServiceReference < WireComponent > ) componentContext . getServiceReference ( ) ) ; this . options = new CloudSubscriberOptions ( properties ) ; logger . debug ( "Activating Cloud Subscriber Wire Component... Done" ) ; }
protected void activate ( final ComponentContext componentContext , final Map < String , Object > properties ) { logger . debug ( "Activating Cloud Subscriber Wire Component..." ) ; this . wireSupport = this . wireHelperService . newWireSupport ( this , ( ServiceReference < WireComponent > ) componentContext . getServiceReference ( ) ) ; this . options = new CloudSubscriberOptions ( properties ) ; logger . debug ( "Activating Cloud Subscriber Wire Component... Done" ) ; }
public void test() { try { checkNotNull ( url , "url" ) ; checkNotNull ( targetName , "targetName" ) ; JavaWebAppDriver driver = getDriver ( ) ; String deployedName = driver . deploy ( url , targetName ) ; Set < String > deployedWars = getAttribute ( DEPLOYED_WARS ) ; code_block = IfStatement ; deployedWars . add ( deployedName ) ; sensors ( ) . set ( DEPLOYED_WARS , deployedWars ) ; } catch ( RuntimeException e ) { LOG . warn ( "Error deploying '" + url + "' to " + targetName + " on " + toString ( ) + "; rethrowing..." , e ) ; throw Throwables . propagate ( e ) ; } }
public void attachClean ( MbBaust instance ) { log . debug ( "attaching clean MbBaust instance" ) ; code_block = TryStatement ;  }
public void test() { try { getSession ( ) . lock ( instance , LockMode . NONE ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { uploadRetryMap . remove ( identifier ) ; asyncWriteCache . remove ( fileName ) ; LOG . info ( "Async Upload Aborted. Dataidentifer [{}], file [{}] removed from AsyncCache." , identifier , file . getAbsolutePath ( ) ) ; } catch ( IOException ie ) { LOG . warn ( "Cannot remove pending file upload. Dataidentifer [ " + identifier + "], file [" + file . getAbsolutePath ( ) + "]" , ie ) ; } }
public void setWorkManager ( final WorkManager workManager ) { this . workManager = workManager ; logger . debug ( "WorkManager set." ) ; }
private void getAllStorageSystems ( ) { ResourceCollection < StorageSystem > storageSystems = this . storageSystemClient . getAll ( ) ; LOGGER . info ( "StorageSystems returned to client : " + storageSystems . toJsonString ( ) ) ; }
public void test() { try { java . util . List < com . liferay . journal . model . JournalArticle > returnValue = JournalArticleServiceUtil . getGroupArticles ( groupId , userId , rootFolderId , status , start , end , orderByComparator ) ; return com . liferay . journal . model . JournalArticleSoap . toSoapModels ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Successfully initialized index provider '{0}' in repository '{1}'" , provider . getName ( ) , repository . name ( ) ) ; } }
@ Override public void channelUnlinked ( ChannelUID channelUID ) { logger . debug ( "channelUnlinked: {}" , channelUID ) ; code_block = SwitchStatement ; }
public void test() { try { ChannelParams params = new ChannelParams ( channel ) ; code_block = IfStatement ; } catch ( ConversionException e ) { logger . warn ( "Channel param error, reason: {}." , e . getMessage ( ) , e ) ; } }
public String create ( LogicalInfrastructure logicalInfrastructure ) throws RestServiceException { LOGGER . debug ( "create a VCPENetwork: " + logicalInfrastructure ) ; String resourceId = vcpeNetworkService . createVCPENetwork ( OpennaasBeanUtils . getVCPENetwork ( logicalInfrastructure ) ) ; LOGGER . debug ( "Polling for building task to finish" ) ; code_block = WhileStatement ; LOGGER . debug ( "Retrieving build result" ) ; vcpeNetworkService . getBuildResult ( resourceId ) ; return resourceId ; }
public String create ( LogicalInfrastructure logicalInfrastructure ) throws RestServiceException { LOGGER . debug ( "create a VCPENetwork: " + logicalInfrastructure ) ; String resourceId = vcpeNetworkService . createVCPENetwork ( OpennaasBeanUtils . getVCPENetwork ( logicalInfrastructure ) ) ; LOGGER . debug ( "Polling for building task to finish" ) ; code_block = WhileStatement ; LOGGER . debug ( "Retrieving build result" ) ; vcpeNetworkService . getBuildResult ( resourceId ) ; return resourceId ; }
public String create ( LogicalInfrastructure logicalInfrastructure ) throws RestServiceException { LOGGER . debug ( "create a VCPENetwork: " + logicalInfrastructure ) ; String resourceId = vcpeNetworkService . createVCPENetwork ( OpennaasBeanUtils . getVCPENetwork ( logicalInfrastructure ) ) ; LOGGER . debug ( "Polling for building task to finish" ) ; code_block = WhileStatement ; LOGGER . debug ( "Retrieving build result" ) ; vcpeNetworkService . getBuildResult ( resourceId ) ; return resourceId ; }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { log . warn ( expPrefix + region . getName ( ) + InventoryConstants . ERROR_CAUSE + e . getMessage ( ) + "\"}" ) ; ErrorManageUtil . uploadError ( accountId , region . getName ( ) , "egressgateway" , e . getMessage ( ) ) ; } }
public void leave ( RoomParticipant user ) { checkClosed ( ) ; log . debug ( "PARTICIPANT {}: Leaving room {}" , user . getName ( ) , this . name ) ; this . removeParticipant ( user . getName ( ) ) ; user . close ( ) ; }
public void test() { if ( isInitialized ( ) ) { logger . warn ( msg , event . getCode ( ) ) ; } else { logger . debug ( msg , event . getCode ( ) ) ; } }
public void test() { if ( isInitialized ( ) ) { logger . warn ( msg , event . getCode ( ) ) ; } else { logger . debug ( msg , event . getCode ( ) ) ; } }
public void test() { switch ( eventValue ) { case EvdevLibrary . KeyEventValue . DOWN : String keyCode = channel . getUID ( ) . getIdWithoutGroup ( ) ; updateState ( keyChannel . getUID ( ) , new StringType ( keyCode ) ) ; updateState ( channel . getUID ( ) , OpenClosedType . CLOSED ) ; triggerChannel ( keyChannel . getUID ( ) , keyCode ) ; triggerChannel ( channel . getUID ( ) , CommonTriggerEvents . PRESSED ) ; updateState ( keyChannel . getUID ( ) , new StringType ( ) ) ; break ; case EvdevLibrary . KeyEventValue . UP : updateState ( channel . getUID ( ) , OpenClosedType . OPEN ) ; triggerChannel ( channel . getUID ( ) , CommonTriggerEvents . RELEASED ) ; break ; case EvdevLibrary . KeyEventValue . REPEAT : break ; default : logger . debug ( "Unexpected event value for channel {}: {}" , channel , eventValue ) ; break ; } }
public void test() { if ( ! scheduledChecks . contains ( scheduledKey ) ) { checkIntegrationStatus ( integrationDeployment ) ; } else { LOG . debug ( "A check for IntegrationDeployment {} is already configured with key {}" , id , scheduledKey ) ; } }
public void test() { if ( integrationDeployment != null ) { String scheduledKey = getIntegrationMarkerKey ( integrationDeployment ) ; LOG . debug ( "Check if IntegrationStatus {} is already in progress for key: {} (keys: {})" , id , scheduledKey , scheduledChecks ) ; code_block = IfStatement ; } else { LOG . debug ( "No IntegrationDeployment with id: {}" , id ) ; } }
public void test() { if ( ! FSUtil . isFileExist ( hdfsPath ) ) { LOGGER . info ( "Predicate path: " + hdfsPath + " doesn't exist." ) ; return false ; } }
public void addNetwork ( QuantumModel quantumModel , Network network ) throws QuantumException { log . debug ( "Adding network " + network . getName ( ) + " to Quantum model." ) ; List < Network > networks = quantumModel . getNetworks ( ) ; if ( networks . contains ( network ) ) throw new QuantumException ( "Network  " + network . getName ( ) + " already exists in Quantum Model." ) ; quantumModel . getNetworks ( ) . add ( network ) ; log . debug ( "Network " + network . getName ( ) + " added to Quantum model." ) ; }
public void addNetwork ( QuantumModel quantumModel , Network network ) throws QuantumException { log . debug ( "Adding network " + network . getName ( ) + " to Quantum model." ) ; List < Network > networks = quantumModel . getNetworks ( ) ; if ( networks . contains ( network ) ) throw new QuantumException ( "Network  " + network . getName ( ) + " already exists in Quantum Model." ) ; quantumModel . getNetworks ( ) . add ( network ) ; log . debug ( "Network " + network . getName ( ) + " added to Quantum model." ) ; }
public void test() { try { com . liferay . commerce . model . CommerceOrder returnValue = CommerceOrderServiceUtil . executeWorkflowTransition ( commerceOrderId , workflowTaskId , transitionName , comment ) ; return com . liferay . commerce . model . CommerceOrderSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try { archiveFile . createNewFile ( ) ; } catch ( IOException e ) { LOG . error ( "Error creating a tar file" ) ; } }
private PolicyFinder createPolicyFinder ( ) { LOGGER . debug ( "XACML policies will be looked for in the following location(s): {}" , xacmlPolicyDirectories ) ; PolicyFinder policyFinder = new PolicyFinder ( ) ; PollingPolicyFinderModule policyFinderModule = new PollingPolicyFinderModule ( xacmlPolicyDirectories , defaultPollingIntervalInSeconds , securityLogger ) ; policyFinderModule . start ( ) ; Set < PolicyFinderModule > policyFinderModules = new HashSet < > ( 1 ) ; policyFinderModules . add ( policyFinderModule ) ; policyFinder . setModules ( policyFinderModules ) ; return policyFinder ; }
public void test() { try { DocFlavor flavor = new DocFlavor ( "application/vnd.cups-raster" , "[B" ) ; code_block = ForStatement ; log . error ( "No printer found" ) ; return false ; } catch ( PrintException e ) { log . error ( "print" , e ) ; } }
public void test() { try { ObjectMapper mapper = new ObjectMapper ( ) ; orgLocationList = mapper . readValue ( orgLocation , List . class ) ; } catch ( Exception e ) { logger . info ( actorMessage . getRequestContext ( ) , "Exception occurred while converting orgLocation to List<Map<String,String>>." ) ; } }
public void test() { if ( _log . isWarnEnabled ( ) ) { _log . warn ( portalException , portalException ) ; } }
@ Override public List < LwM2mObject > findLwM2mObject ( TenantId tenantId , String sortOrder , String sortProperty , String [ ] objectIds ) { log . trace ( "Executing findByTenantId [{}]" , tenantId ) ; validateId ( tenantId , INCORRECT_TENANT_ID + tenantId ) ; List < TbResource > resources = resourceService . findTenantResourcesByResourceTypeAndObjectIds ( tenantId , ResourceType . LWM2M_MODEL , objectIds ) ; return resources . stream ( ) . flatMap ( s -> Stream . ofNullable ( toLwM2mObject ( s , false ) ) ) . sorted ( getComparator ( sortProperty , sortOrder ) ) . collect ( Collectors . toList ( ) ) ; }
public void test() { if ( ! entry . isDirectory ( ) ) { final RequestMetaData metaData = new RequestMetaData ( entry . getFile ( ) . getLength ( ) , 2L , name ) ; final RequestIdentifier identifier = new RequestIdentifier ( uri ) ; FatFileIdentificationRequest req = new FatFileIdentificationRequest ( metaData , identifier , getTmpDir ( ) ) ; ByteBuffer buffer = ByteBuffer . allocate ( ( int ) entry . getFile ( ) . getLength ( ) ) ; entry . getFile ( ) . read ( 0 , buffer ) ; buffer . flip ( ) ; expandContainer ( req , new ByteArrayInputStream ( buffer . array ( ) , buffer . position ( ) , buffer . limit ( ) ) , newPath ) ; } else { log . trace ( "processing directory : " + entry . getName ( ) ) ; } }
public void test() { if ( _log . isInfoEnabled ( ) ) { _log . info ( "...Setting executeStatusLogLevelInfo: " + executeStatusLogLevelInfo ) ; } }
public void test() { if ( pendingResp != null ) { CancelSmResp resp = pduDecomposer . cancelSmResp ( pdu ) ; pendingResp . done ( resp ) ; } else { logger . error ( NO_REQUEST_FIND_FOR_SEQUENCE_NUMBER + pduHeader . getSequenceNumber ( ) ) ; } }
public void test() { try ( Service service = new Service ( coreSettings ) ) { sendResponse ( service . execute ( serviceRequestFromHttpRequest ( request , requestType ) ) , response ) ; } catch ( Exception exc ) { LOGGER . error ( "" , exc ) ; sendResponse ( new ServiceResponse < > ( 500 , exc . getMessage ( ) ) , response ) ; } }
public static void main ( String [ ] args ) { long startTime = 0 ; long endTime = 0 ; long aggregatorStart = Calendar . getInstance ( ) . getTimeInMillis ( ) ; long longest = 0 ; code_block = IfStatement ; String longQuery = null ; log . info ( "Aggregator started." ) ; String cluster = System . getProperty ( "CLUSTER" ) ; code_block = IfStatement ; String queries = Aggregator . getContents ( new File ( System . getenv ( "CHUKWA_CONF_DIR" ) + File . separator + "aggregator.sql" ) ) ; String [ ] query = queries . split ( "\n" ) ; code_block = WhileStatement ; long aggregatorEnd = Calendar . getInstance ( ) . getTimeInMillis ( ) ; log . info ( "Longest running query: " + longQuery + " (" + ( double ) longest / 1000 + " seconds)" ) ; log . info ( "Total running time: (" + ( double ) ( aggregatorEnd - aggregatorStart ) / 1000 + " seconds)" ) ; log . info ( "Aggregator finished." ) ; }
public void test() { if ( query [ i ] . indexOf ( "" ) == 0 ) { log . debug ( "skipping: " + query [ i ] ) ; } else-if ( ! query [ i ] . equals ( "" ) ) { Aggregator dba = new Aggregator ( ) ; dba . setWriter ( new DatabaseWriter ( cluster ) ) ; long start = Calendar . getInstance ( ) . getTimeInMillis ( ) ; code_block = TryStatement ;  long end = Calendar . getInstance ( ) . getTimeInMillis ( ) ; long duration = end - start ; code_block = IfStatement ; } }
public void test() { try { code_block = IfStatement ; } catch ( Throwable e ) { log . error ( "Invalid query:" + query [ i ] ) ; } }
public static void main ( String [ ] args ) { long startTime = 0 ; long endTime = 0 ; long aggregatorStart = Calendar . getInstance ( ) . getTimeInMillis ( ) ; long longest = 0 ; code_block = IfStatement ; String longQuery = null ; log . info ( "Aggregator started." ) ; String cluster = System . getProperty ( "CLUSTER" ) ; code_block = IfStatement ; String queries = Aggregator . getContents ( new File ( System . getenv ( "CHUKWA_CONF_DIR" ) + File . separator + "aggregator.sql" ) ) ; String [ ] query = queries . split ( "\n" ) ; code_block = WhileStatement ; long aggregatorEnd = Calendar . getInstance ( ) . getTimeInMillis ( ) ; log . info ( "Longest running query: " + longQuery + " (" + ( double ) longest / 1000 + " seconds)" ) ; log . info ( "Total running time: (" + ( double ) ( aggregatorEnd - aggregatorStart ) / 1000 + " seconds)" ) ; log . info ( "Aggregator finished." ) ; }
public static void main ( String [ ] args ) { long startTime = 0 ; long endTime = 0 ; long aggregatorStart = Calendar . getInstance ( ) . getTimeInMillis ( ) ; long longest = 0 ; code_block = IfStatement ; String longQuery = null ; log . info ( "Aggregator started." ) ; String cluster = System . getProperty ( "CLUSTER" ) ; code_block = IfStatement ; String queries = Aggregator . getContents ( new File ( System . getenv ( "CHUKWA_CONF_DIR" ) + File . separator + "aggregator.sql" ) ) ; String [ ] query = queries . split ( "\n" ) ; code_block = WhileStatement ; long aggregatorEnd = Calendar . getInstance ( ) . getTimeInMillis ( ) ; log . info ( "Longest running query: " + longQuery + " (" + ( double ) longest / 1000 + " seconds)" ) ; log . info ( "Total running time: (" + ( double ) ( aggregatorEnd - aggregatorStart ) / 1000 + " seconds)" ) ; log . info ( "Aggregator finished." ) ; }
public static void main ( String [ ] args ) { long startTime = 0 ; long endTime = 0 ; long aggregatorStart = Calendar . getInstance ( ) . getTimeInMillis ( ) ; long longest = 0 ; code_block = IfStatement ; String longQuery = null ; log . info ( "Aggregator started." ) ; String cluster = System . getProperty ( "CLUSTER" ) ; code_block = IfStatement ; String queries = Aggregator . getContents ( new File ( System . getenv ( "CHUKWA_CONF_DIR" ) + File . separator + "aggregator.sql" ) ) ; String [ ] query = queries . split ( "\n" ) ; code_block = WhileStatement ; long aggregatorEnd = Calendar . getInstance ( ) . getTimeInMillis ( ) ; log . info ( "Longest running query: " + longQuery + " (" + ( double ) longest / 1000 + " seconds)" ) ; log . info ( "Total running time: (" + ( double ) ( aggregatorEnd - aggregatorStart ) / 1000 + " seconds)" ) ; log . info ( "Aggregator finished." ) ; }
public void test() { if ( replyTo != null ) { LOG . debug ( "Using JMSReplyTo destination: {}" , replyTo ) ; JmsMessageHelper . setJMSReplyTo ( answer , replyTo ) ; } else { LOG . trace ( "Not using JMSReplyTo" ) ; JmsMessageHelper . setJMSReplyTo ( answer , null ) ; } }
public void test() { if ( replyTo != null ) { LOG . debug ( "Using JMSReplyTo destination: {}" , replyTo ) ; JmsMessageHelper . setJMSReplyTo ( answer , replyTo ) ; } else { LOG . trace ( "Not using JMSReplyTo" ) ; JmsMessageHelper . setJMSReplyTo ( answer , null ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "getReferralsByFacility: clientId=" + clientId + ", of results=" + results . size ( ) ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( SegmentsEntryServiceUtil . class , "addSegmentsEntryClassPKs" , _addSegmentsEntryClassPKsParameterTypes2 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , segmentsEntryId , classPKs , serviceContext ) ; code_block = TryStatement ;  } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { return ( int ) index . count ( ) ; } catch ( SeriesServiceDatabaseException e ) { logger . error ( "Exception occured while counting series." , e ) ; throw new SeriesException ( e ) ; } }
public void test() { try ( FileInputStream fis = new FileInputStream ( configFile ) ) { Properties props = new Properties ( ) ; props . load ( fis ) ; code_block = ForStatement ; } catch ( IOException e ) { log . error ( "Problem reading config file for Godiva3 config.  Defaults will be used" , e ) ; } }
public void test() { if ( value instanceof String ) { code_block = TryStatement ;  } }
@ Override public RemoveRepositorySourceMirrorResult removeRepositorySourceMirror ( RemoveRepositorySourceMirrorRequest request ) { Preconditions . checkArgument ( null != request , "the request is required" ) ; Preconditions . checkArgument ( StringUtils . isNotBlank ( request . code ) , "the code is required on the request" ) ; final ObjectContext context = serverRuntime . newContext ( ) ; RepositorySourceMirror repositorySourceMirror = RepositorySourceMirror . tryGetByCode ( context , request . code ) . orElseThrow ( ( ) -> new ObjectNotFoundException ( RepositorySourceMirror . class . getSimpleName ( ) , request . code ) ) ; code_block = IfStatement ; repositorySourceMirror . getRepositorySource ( ) . getRepository ( ) . setModifyTimestamp ( ) ; context . deleteObject ( repositorySourceMirror ) ; context . commitChanges ( ) ; LOGGER . info ( "did remote the repository source mirror [{}]" , request . code ) ; return new RemoveRepositorySourceMirrorResult ( ) ; }
private void rejectPermission ( int personId ) { log . debug ( "Loading Person object from database" ) ; Person user = personDao . read ( personId ) ; log . debug ( "Loaded Person - user name = " + user . getUsername ( ) ) ; log . debug ( "Setting the authority to ROLE_READER" ) ; user . setAuthority ( "ROLE_READER" ) ; log . debug ( "Updating Person in database" ) ; personDao . update ( user ) ; String email = user . getEmail ( ) ; sendEmail ( email , false ) ; }
private void rejectPermission ( int personId ) { log . debug ( "Loading Person object from database" ) ; Person user = personDao . read ( personId ) ; log . debug ( "Loaded Person - user name = " + user . getUsername ( ) ) ; log . debug ( "Setting the authority to ROLE_READER" ) ; user . setAuthority ( "ROLE_READER" ) ; log . debug ( "Updating Person in database" ) ; personDao . update ( user ) ; String email = user . getEmail ( ) ; sendEmail ( email , false ) ; }
private void rejectPermission ( int personId ) { log . debug ( "Loading Person object from database" ) ; Person user = personDao . read ( personId ) ; log . debug ( "Loaded Person - user name = " + user . getUsername ( ) ) ; log . debug ( "Setting the authority to ROLE_READER" ) ; user . setAuthority ( "ROLE_READER" ) ; log . debug ( "Updating Person in database" ) ; personDao . update ( user ) ; String email = user . getEmail ( ) ; sendEmail ( email , false ) ; }
private void rejectPermission ( int personId ) { log . debug ( "Loading Person object from database" ) ; Person user = personDao . read ( personId ) ; log . debug ( "Loaded Person - user name = " + user . getUsername ( ) ) ; log . debug ( "Setting the authority to ROLE_READER" ) ; user . setAuthority ( "ROLE_READER" ) ; log . debug ( "Updating Person in database" ) ; personDao . update ( user ) ; String email = user . getEmail ( ) ; sendEmail ( email , false ) ; }
public void test() { try { stat = conn . prepareStatement ( DELETE_ITEM ) ; stat . setString ( 1 , InitializerManager . REPORT_CONFIG_ITEM ) ; stat . setString ( 2 , version ) ; stat . executeUpdate ( ) ; } catch ( Throwable t ) { _logger . error ( "Error deleting item" , t ) ; throw new RuntimeException ( "Error deleting item" , t ) ; } finally { closeDaoResources ( null , stat ) ; } }
@ Test public void parseTestAbbreviation_Array ( ) throws IOException { BsonValue c = new BsonBoolean ( true ) ; BsonDocument document = new BsonDocument ( ) . append ( "double" , new BsonDouble ( 12.3 ) ) . append ( "arrayInt" , new BsonArray ( Arrays . asList ( c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c ) ) ) ; BasicDBObject query = new BasicDBObject ( ) ; query . put ( "ComplexBson" , document ) ; logger . debug ( "document:{}" , document ) ; NormalizedBson stringStringValue = MongoUtil . parseBson ( new Object [ ] code_block = "" ; , true ) ; logger . debug ( "val:{}" , stringStringValue ) ; List list = objectMapper . readValue ( "[" + stringStringValue . getNormalizedBson ( ) + "]" , List . class ) ; Assert . assertEquals ( list . size ( ) , 1 ) ; Map < String , ? > query1Map = ( Map < String , ? > ) list . get ( 0 ) ; checkValue ( query1Map ) ; }
@ Test public void parseTestAbbreviation_Array ( ) throws IOException { BsonValue c = new BsonBoolean ( true ) ; BsonDocument document = new BsonDocument ( ) . append ( "double" , new BsonDouble ( 12.3 ) ) . append ( "arrayInt" , new BsonArray ( Arrays . asList ( c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c , c ) ) ) ; BasicDBObject query = new BasicDBObject ( ) ; query . put ( "ComplexBson" , document ) ; logger . debug ( "document:{}" , document ) ; NormalizedBson stringStringValue = MongoUtil . parseBson ( new Object [ ] code_block = "" ; , true ) ; logger . debug ( "val:{}" , stringStringValue ) ; List list = objectMapper . readValue ( "[" + stringStringValue . getNormalizedBson ( ) + "]" , List . class ) ; Assert . assertEquals ( list . size ( ) , 1 ) ; Map < String , ? > query1Map = ( Map < String , ? > ) list . get ( 0 ) ; checkValue ( query1Map ) ; }
public void test() { try { poolEntry . sendMessage ( message , recipients ) ; } catch ( RuntimeException | MessagingException e1 ) { LOG . error ( "Sending failed with the second try" , e1 ) ; throw e1 ; } }
@ Override public void addParentOfferings ( final String offering , final Collection < String > parentOfferings ) { CacheValidation . notNullOrEmpty ( OFFERING , offering ) ; CacheValidation . noNullOrEmptyValues ( PARENT_OFFERINGS , parentOfferings ) ; LOG . trace ( "Adding parentOfferings {} to offering {}" , parentOfferings , offering ) ; this . parentOfferingsForOfferings . computeIfAbsent ( offering , createSynchronizedSet ( ) ) . addAll ( parentOfferings ) ; parentOfferings . forEach ( parentOffering -> this . childOfferingsForOfferings . computeIfAbsent ( parentOffering , createSynchronizedSet ( ) ) . add ( offering ) ) ; }
@ Test public void alertDetailsMessageTime ( ) { alertingService . addAlertListener ( member , SEVERE ) ; logger . fatal ( alertMessage ) ; assertThat ( captureAlertDetails ( ) . getMsgTime ( ) ) . isNotNull ( ) ; }
public void test() { try { clientRequest . setHttpMethod ( getHttpMethod ( ) ) ; clientRequest . header ( "Content-Type" , getRequest ( ) . getContentType ( ) ) ; code_block = IfStatement ; JSONObject requestBody = getRequest ( ) . getJSONParameters ( ) ; clientRequest . body ( MediaType . APPLICATION_JSON , requestBody . toString ( 4 ) ) ; clientResponse = clientRequest . post ( String . class ) ; setResponse ( new PushErrorResponse ( clientResponse ) ) ; } catch ( Exception e ) { LOG . error ( e . getMessage ( ) , e ) ; } finally { closeConnection ( ) ; } }
public void test() { try ( Response response = make ( request ) ) { check ( response ) ; return RequestResponse . parseFromResponse ( response ) ; } catch ( VitamClientInternalException e ) { throw new AccessExternalClientServerException ( e ) ; } catch ( AdminExternalClientException e ) { LOGGER . error ( e ) ; return e . getVitamError ( ) ; } }
public void test() { if ( LOG . isWarnEnabled ( ) ) { LOG . warn ( e . getMessage ( ) ) ; } }
public void test() { if ( hadoopPolicyList . size ( ) == 0 ) { log . info ( "Ranger Plugin : Unable to find Ranger Hive policy " + rangerHivePolicyName + " ... Ignoring" ) ; } else-if ( hadoopPolicyList . size ( ) > 1 ) { throw new RuntimeException ( "Unable to find Hive unique policy." ) ; } else { code_block = ForStatement ; } }
public void test() { try { rangerRestClient . deletePolicy ( hadoopPolicy . getId ( ) ) ; } catch ( Exception e ) { log . error ( "Unable to delete policy" , e ) ; throw new RuntimeException ( "Unable to delete policy to " + rangerHivePolicyName , e ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { BufferedReader reader = new BufferedReader ( new InputStreamReader ( req . getInputStream ( ) , "UTF-8" ) ) ; String line ; StringBuilder builder = new StringBuilder ( ) ; code_block = WhileStatement ; reader . close ( ) ; LOG . debug ( "[RequestBody]  " + httpreq . getRequestURL ( ) ) ; LOG . debug ( "" + builder . toString ( ) ) ; } }
public void test() { if ( suspendMinorCompaction . compareAndSet ( true , false ) ) { LOG . info ( "{} Minor Compaction back to normal since bookie has enough space now." , Thread . currentThread ( ) . getName ( ) ) ; } }
@ Override protected void implCloseChannel ( ) throws IOException { log . debug ( "Emulating close file channel" ) ; }
public void test() { try { SessionObject sesObj = ( SessionObject ) request . getSession ( ) . getAttribute ( FdahpStudyDesignerConstants . SESSION_OBJECT ) ; int oldOrderNumber ; int newOrderNumber ; code_block = IfStatement ; jsonobject . put ( FdahpStudyDesignerConstants . MESSAGE , message ) ; response . setContentType ( FdahpStudyDesignerConstants . APPLICATION_JSON ) ; out = response . getWriter ( ) ; out . print ( jsonobject ) ; } catch ( Exception e ) { logger . error ( "StudyController - reOrderComprehensionTestQuestion - ERROR" , e ) ; } }
@ Test @ InRequestScope public void testExecuteToGetAll ( ) throws Exception { GetTransUnitList action = GetTransUnitList . newAction ( new GetTransUnitActionContext ( document ) ) ; prepareActionAndMockLocaleService ( action ) ; long startTime = System . nanoTime ( ) ; GetTransUnitListResult result = handler . execute ( action , null ) ; log . info ( "********** duration :{} second" , ( System . nanoTime ( ) - startTime ) / 1.0E9 ) ; log . info ( "result: {}" , result ) ; assertThat ( result . getDocumentId ( ) ) . isEqualTo ( document . getId ( ) ) ; assertThat ( result . getGotoRow ( ) ) . isEqualTo ( 0 ) ; assertThat ( getIntIds ( result . getUnits ( ) ) ) . contains ( 1 , 2 , 3 , 4 , 5 ) ; }
@ Test @ InRequestScope public void testExecuteToGetAll ( ) throws Exception { GetTransUnitList action = GetTransUnitList . newAction ( new GetTransUnitActionContext ( document ) ) ; prepareActionAndMockLocaleService ( action ) ; long startTime = System . nanoTime ( ) ; GetTransUnitListResult result = handler . execute ( action , null ) ; log . info ( "********** duration :{} second" , ( System . nanoTime ( ) - startTime ) / 1.0E9 ) ; log . info ( "result: {}" , result ) ; assertThat ( result . getDocumentId ( ) ) . isEqualTo ( document . getId ( ) ) ; assertThat ( result . getGotoRow ( ) ) . isEqualTo ( 0 ) ; assertThat ( getIntIds ( result . getUnits ( ) ) ) . contains ( 1 , 2 , 3 , 4 , 5 ) ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( _NO_SUCH_ENTITY_WITH_PRIMARY_KEY + primaryKey ) ; } }
private void testNoteCreate ( String noteName ) throws IOException { String jsonRequest = "{\"name\":\"" + noteName + "\"}" ; CloseableHttpResponse post = httpPost ( "/notebook/" , jsonRequest ) ; String postResponse = EntityUtils . toString ( post . getEntity ( ) , StandardCharsets . UTF_8 ) ; LOG . info ( "testNoteCreate \n" + postResponse ) ; assertThat ( "test note create method:" , post , isAllowed ( ) ) ; Map < String , Object > resp = gson . fromJson ( postResponse , new TypeToken < Map < String , Object > > ( ) code_block = "" ; . getType ( ) ) ; String newNoteId = ( String ) resp . get ( "body" ) ; LOG . info ( "newNoteId:=" + newNoteId ) ; Note newNote = TestUtils . getInstance ( Notebook . class ) . getNote ( newNoteId ) ; assertNotNull ( "Can not find new note by id" , newNote ) ; String newNoteName = newNote . getName ( ) ; LOG . info ( "new note name is: " + newNoteName ) ; code_block = IfStatement ; assertEquals ( "compare note name" , noteName , newNoteName ) ; TestUtils . getInstance ( Notebook . class ) . removeNote ( newNote , anonymous ) ; post . close ( ) ; }
private void testNoteCreate ( String noteName ) throws IOException { String jsonRequest = "{\"name\":\"" + noteName + "\"}" ; CloseableHttpResponse post = httpPost ( "/notebook/" , jsonRequest ) ; String postResponse = EntityUtils . toString ( post . getEntity ( ) , StandardCharsets . UTF_8 ) ; LOG . info ( "testNoteCreate \n" + postResponse ) ; assertThat ( "test note create method:" , post , isAllowed ( ) ) ; Map < String , Object > resp = gson . fromJson ( postResponse , new TypeToken < Map < String , Object > > ( ) code_block = "" ; . getType ( ) ) ; String newNoteId = ( String ) resp . get ( "body" ) ; LOG . info ( "newNoteId:=" + newNoteId ) ; Note newNote = TestUtils . getInstance ( Notebook . class ) . getNote ( newNoteId ) ; assertNotNull ( "Can not find new note by id" , newNote ) ; String newNoteName = newNote . getName ( ) ; LOG . info ( "new note name is: " + newNoteName ) ; code_block = IfStatement ; assertEquals ( "compare note name" , noteName , newNoteName ) ; TestUtils . getInstance ( Notebook . class ) . removeNote ( newNote , anonymous ) ; post . close ( ) ; }
private void testNoteCreate ( String noteName ) throws IOException { String jsonRequest = "{\"name\":\"" + noteName + "\"}" ; CloseableHttpResponse post = httpPost ( "/notebook/" , jsonRequest ) ; String postResponse = EntityUtils . toString ( post . getEntity ( ) , StandardCharsets . UTF_8 ) ; LOG . info ( "testNoteCreate \n" + postResponse ) ; assertThat ( "test note create method:" , post , isAllowed ( ) ) ; Map < String , Object > resp = gson . fromJson ( postResponse , new TypeToken < Map < String , Object > > ( ) code_block = "" ; . getType ( ) ) ; String newNoteId = ( String ) resp . get ( "body" ) ; LOG . info ( "newNoteId:=" + newNoteId ) ; Note newNote = TestUtils . getInstance ( Notebook . class ) . getNote ( newNoteId ) ; assertNotNull ( "Can not find new note by id" , newNote ) ; String newNoteName = newNote . getName ( ) ; LOG . info ( "new note name is: " + newNoteName ) ; code_block = IfStatement ; assertEquals ( "compare note name" , noteName , newNoteName ) ; TestUtils . getInstance ( Notebook . class ) . removeNote ( newNote , anonymous ) ; post . close ( ) ; }
public void test() { try { @ SuppressWarnings ( "unchecked" ) LayoutBuilder builder = ( LayoutBuilder ) LoaderUtil . newInstanceOf ( plugin . getPluginClass ( ) ) ; return builder . parseLayout ( layoutElement , config ) ; } catch ( InstantiationException | IllegalAccessException | InvocationTargetException ex ) { LOGGER . warn ( "Unable to load plugin: {} due to: {}" , plugin . getKey ( ) , ex . getMessage ( ) ) ; } }
public void test() { try { List < Rule > rules = ruleRepository . findAll ( ) ; List < JobExecutionManager > jobs = jobRepository . findAll ( ) ; boolean rulesEnabled = false ; boolean jobsEnabled = false ; code_block = ForStatement ; code_block = ForStatement ; code_block = IfStatement ; code_block = IfStatement ; return status ; } catch ( Exception e ) { log . error ( "Error in fetching status of system" , e ) ; throw new PacManException ( "Error in fetching the status of system" ) ; } }
public void test() { if ( curr > segment . getDateRangeEnd ( ) && delta > segmentManager . cubeDuration ) { logger . debug ( "Make {} immutable because it lastUpdate[{}] exceed wait duration." , segment . getSegmentName ( ) , segment . getLastUpdateTime ( ) ) ; segmentManager . makeSegmentImmutable ( segment . getSegmentName ( ) ) ; } }
public void test() { if ( ! segments . isEmpty ( ) ) { logger . info ( "found cube {} segments:{} are immutable, retention policy is: {}" , cubeName , segments , retentionPolicyInfo . getName ( ) ) ; } else { continue ; } }
public void test() { try { Collection < StreamingCubeSegment > activeSegments = segmentManager . getActiveSegments ( ) ; code_block = ForStatement ; RetentionPolicyInfo retentionPolicyInfo = new RetentionPolicyInfo ( ) ; String policyName = cubeInstance . getConfig ( ) . getStreamingSegmentRetentionPolicy ( ) ; Map < String , String > policyProps = cubeInstance . getConfig ( ) . getStreamingSegmentRetentionPolicyProperties ( policyName ) ; retentionPolicyInfo . setName ( policyName ) ; retentionPolicyInfo . setProperties ( policyProps ) ; Collection < StreamingCubeSegment > segments = segmentManager . getRequireRemotePersistSegments ( ) ; code_block = IfStatement ; handleImmutableCubeSegments ( cubeName , segmentManager , segments , retentionPolicyInfo ) ; } catch ( Exception e ) { logger . error ( "error when handle cube:" + cubeName , e ) ; } }
public void test() { try { Files . copy ( secureStoragePathCopy , secureStoragePath , REPLACE_EXISTING ) ; } catch ( IOException e ) { LOG . error ( e . getMessage ( ) , e ) ; } }
@ Override public void onSessionInitiated ( final BindingAwareBroker . ProviderContext providerContext ) { LOG . info ( "Starting GbpIseAdapterProvider .." ) ; final EPPolicyTemplateProviderFacade templateProviderFacade = new EPPolicyTemplateProviderIseImpl ( ) ; epPolicyTemplateProviderRegistration = sxpEpProvider . getEPPolicyTemplateProviderRegistry ( ) . registerTemplateProvider ( templateProviderFacade ) ; final SgtInfoProcessor epgGenerator = new SgtToEpgGeneratorImpl ( dataBroker ) ; final SgtInfoProcessor templateGenerator = new SgtToEPTemplateGeneratorImpl ( dataBroker ) ; final GbpIseSgtHarvester gbpIseSgtHarvester = new GbpIseSgtHarvesterImpl ( epgGenerator , templateGenerator ) ; final GbpIseConfigListenerImpl gbpIseConfigListener = new GbpIseConfigListenerImpl ( dataBroker , gbpIseSgtHarvester , templateProviderFacade ) ; templateProviderFacade . setIseSgtHarvester ( gbpIseSgtHarvester ) ; final DataTreeIdentifier < IseSourceConfig > dataTreePath = new DataTreeIdentifier < > ( LogicalDatastoreType . CONFIGURATION , InstanceIdentifier . create ( GbpSxpIseAdapter . class ) . child ( IseSourceConfig . class ) ) ; registration = dataBroker . registerDataTreeChangeListener ( dataTreePath , gbpIseConfigListener ) ; LOG . info ( "Started" ) ; }
@ Override public void onSessionInitiated ( final BindingAwareBroker . ProviderContext providerContext ) { LOG . info ( "Starting GbpIseAdapterProvider .." ) ; final EPPolicyTemplateProviderFacade templateProviderFacade = new EPPolicyTemplateProviderIseImpl ( ) ; epPolicyTemplateProviderRegistration = sxpEpProvider . getEPPolicyTemplateProviderRegistry ( ) . registerTemplateProvider ( templateProviderFacade ) ; final SgtInfoProcessor epgGenerator = new SgtToEpgGeneratorImpl ( dataBroker ) ; final SgtInfoProcessor templateGenerator = new SgtToEPTemplateGeneratorImpl ( dataBroker ) ; final GbpIseSgtHarvester gbpIseSgtHarvester = new GbpIseSgtHarvesterImpl ( epgGenerator , templateGenerator ) ; final GbpIseConfigListenerImpl gbpIseConfigListener = new GbpIseConfigListenerImpl ( dataBroker , gbpIseSgtHarvester , templateProviderFacade ) ; templateProviderFacade . setIseSgtHarvester ( gbpIseSgtHarvester ) ; final DataTreeIdentifier < IseSourceConfig > dataTreePath = new DataTreeIdentifier < > ( LogicalDatastoreType . CONFIGURATION , InstanceIdentifier . create ( GbpSxpIseAdapter . class ) . child ( IseSourceConfig . class ) ) ; registration = dataBroker . registerDataTreeChangeListener ( dataTreePath , gbpIseConfigListener ) ; LOG . info ( "Started" ) ; }
public void test() { try ( Tx tx = StructrApp . getInstance ( ) . tx ( ) ) { size = page . getContent ( RenderContext . EditMode . RAW ) . length ( ) ; tx . success ( ) ; } catch ( FrameworkException fex ) { logger . error ( "Error while last modified date of " + this , fex ) ; } }
@ Test public void testAggregateProcessDefinitions ( ) throws Exception { String xml1 = read ( this . getClass ( ) . getResourceAsStream ( "/jaxb/process-def-1.xml" ) ) ; String xml2 = read ( this . getClass ( ) . getResourceAsStream ( "/jaxb/process-def-2.xml" ) ) ; JaxbXMLResponseAggregator aggregate = new JaxbXMLResponseAggregator ( ) ; List < String > data = new ArrayList < > ( ) ; data . add ( xml1 ) ; data . add ( xml2 ) ; String result = aggregate . aggregate ( data ) ; logger . debug ( result ) ; Document xml = toXml ( result ) ; assertNotNull ( xml ) ; NodeList processes = xml . getElementsByTagName ( "process-definitions" ) ; assertNotNull ( processes ) ; assertEquals ( 1 , processes . getLength ( ) ) ; NodeList processDefs = xml . getElementsByTagName ( "processes" ) ; assertNotNull ( processDefs ) ; assertEquals ( 5 , processDefs . getLength ( ) ) ; }
public void attachDirty ( StgSysExport instance ) { log . debug ( "attaching dirty StgSysExport instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { log . info ( "Entering:*********waitForElementPresent()******" ) ; this . by = getBySelector ( locator ) ; this . wait = new WebDriverWait ( driver , TIMEOUT ) ; log . info ( "Waiting:*************until the element is visible  ***********" ) ; } catch ( Exception sc ) { log . info ( "-----waitForElementPresent timeup------" ) ; sc . printStackTrace ( ) ; } }
public void test() { try { log . info ( "Entering:*********waitForElementPresent()******" ) ; this . by = getBySelector ( locator ) ; this . wait = new WebDriverWait ( driver , TIMEOUT ) ; log . info ( "Waiting:*************until the element is visible  ***********" ) ; } catch ( Exception sc ) { log . info ( "-----waitForElementPresent timeup------" ) ; sc . printStackTrace ( ) ; } }
public void test() { try { log . info ( "Entering:*********waitForElementPresent()******" ) ; this . by = getBySelector ( locator ) ; this . wait = new WebDriverWait ( driver , TIMEOUT ) ; log . info ( "Waiting:*************until the element is visible  ***********" ) ; } catch ( Exception sc ) { log . info ( "-----waitForElementPresent timeup------" ) ; sc . printStackTrace ( ) ; } }
public void test() { if ( baseURI != null && ! baseURI . isEmpty ( ) ) { update = "BASE <" + baseURI + "> \n" + update ; } }
public void test() { if ( baseURI != null && ! baseURI . isEmpty ( ) ) { update = "BASE <" + baseURI + "> \n" + update ; } }
public void test() { if ( baseURI != null && ! baseURI . isEmpty ( ) ) { update = "BASE <" + baseURI + "> \n" + update ; } }
public void test() { try { final Owner ownerObj = getOwner ( entity ) ; final EntityReference owner = ownerObj == null ? null : ownerObj . getUser ( ) ; final Collection < Collaborator > collaborators = getCollaborators ( entity ) ; final Set < DocumentReference > processedEntities = new HashSet < > ( ) ; final Queue < DocumentReference > entitiesToCheck = new LinkedList < > ( ) ; entitiesToCheck . add ( ( DocumentReference ) userOrGroup ) ; AccessLevel currentItemAccess ; DocumentReference currentItem ; final XWikiContext context = this . xcontextProvider . get ( ) ; final XWikiGroupService groupService = context . getWiki ( ) . getGroupService ( context ) ; code_block = WhileStatement ; } catch ( final XWikiException ex ) { this . logger . warn ( "Failed to compute access level for [{}] on [{}]: {}" , userOrGroup , entity . getId ( ) , ex . getMessage ( ) ) ; } }
byte [ ] readWithChecksum ( int length ) throws Exception { byte [ ] b = new byte [ length ] ; code_block = ForStatement ; int checksum = read ( false ) ; StringBuffer sb = new StringBuffer ( ) ; code_block = ForStatement ; sb . append ( String . format ( "%02x" , checksum & 0xff ) ) ; Logger . trace ( "< " + sb . toString ( ) ) ; return b ; }
public void test() { try { File inFile = new File ( logDir , strLogFileName ) ; HashMap hp = FileUtil . getRandomAccessFileView ( inFile , Integer . parseInt ( startLen ) , Integer . parseInt ( seek ) , 0 ) ; socketLogger . info ( "strFileView : " + hp . get ( "file_desc" ) ) ; outputObj . put ( ProtocolID . DX_EX_CODE , strDxExCode ) ; outputObj . put ( ProtocolID . RESULT_CODE , strSuccessCode ) ; outputObj . put ( ProtocolID . ERR_CODE , strErrCode ) ; outputObj . put ( ProtocolID . ERR_MSG , strErrMsg ) ; outputObj . put ( ProtocolID . RESULT_DATA , hp . get ( "file_desc" ) ) ; inFile = null ; send ( TotalLengthBit , outputObj . toString ( ) . getBytes ( ) ) ; } catch ( Exception e ) { errLogger . error ( "DxT031 {} " , e . toString ( ) ) ; outputObj . put ( ProtocolID . DX_EX_CODE , TranCodeType . DxT031 ) ; outputObj . put ( ProtocolID . RESULT_CODE , "1" ) ; outputObj . put ( ProtocolID . ERR_CODE , TranCodeType . DxT031 ) ; outputObj . put ( ProtocolID . ERR_MSG , "DxT031 Error [" + e . toString ( ) + "]" ) ; sendBuff = outputObj . toString ( ) . getBytes ( ) ; send ( 4 , sendBuff ) ; } finally { outputObj = null ; sendBuff = null ; } }
public void test() { try { BaseQueryMetric metric = rq . getMetric ( ) ; code_block = SwitchStatement ; if ( queryMetricsBean != null ) queryMetricsBean . updateMetric ( metric ) ; else log . error ( "QueryMetricsBean JNDI lookup returned null" ) ; } catch ( Exception e ) { log . error ( "Unable to record metrics to " + queryCall . methodType + " method: " + e . getLocalizedMessage ( ) , e ) ; } }
public void test() { if ( rq != null ) { BaseQueryLogic < ? > baseLogic = null ; QueryLogic < ? > logic = rq . getLogic ( ) ; code_block = IfStatement ; code_block = IfStatement ; } else { log . error ( "RunningQuery instance not in the cache!, queryId: " + queryCall . queryID ) ; } }
public void test() { if ( queryCache != null ) { RunningQuery rq = queryCache . get ( queryCall . queryID ) ; code_block = IfStatement ; } else { log . error ( "Query cache not injected! No metrics will be recorded for serialization times." ) ; } }
public void test() { try { doDeployTarballResource ( url , targetName ) ; } catch ( RuntimeException e ) { LOG . error ( "Error deploying '" + url + "' on " + toString ( ) + "; rethrowing..." , e ) ; throw Throwables . propagate ( e ) ; } }
public void delete ( StgMsUnjTxt persistentInstance ) { log . debug ( "deleting StgMsUnjTxt instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . delete ( persistentInstance ) ; log . debug ( "delete successful" ) ; } catch ( RuntimeException re ) { log . error ( "delete failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . delete ( persistentInstance ) ; log . debug ( "delete successful" ) ; } catch ( RuntimeException re ) { log . error ( "delete failed" , re ) ; throw re ; } }
public void test() { if ( context . getRetryCount ( ) > 0 ) { long mapExactMatches = mapExactMatches ( assetExtractionId , tmId , assetId ) ; logger . error ( "Assume concurrent modification happened, perform remapping: {}" , mapExactMatches ) ; } }
public void test() { if ( prevResult . equals ( streamTaskResult ) ) { log . info ( i + ": Got the same results for two Stream tasks runs" ) ; } else { log . error ( i + ": Did not get the same results for two Stream tasks runs" ) ; break ; } }
public void test() { if ( s instanceof FlowVariable ) { FlowVariable v = ( FlowVariable ) s ; NodeSettingsWO sub = stackSet . addNodeSettings ( "Variable_" + c ) ; sub . addString ( "type" , "variable" ) ; v . save ( sub ) ; } else-if ( s instanceof FlowLoopContext ) { code_block = IfStatement ; } else-if ( s instanceof InnerFlowLoopContext ) { NodeSettingsWO sub = stackSet . addNodeSettings ( "Loop_Execute_" + c ) ; sub . addString ( "type" , "loopcontext_execute" ) ; } else-if ( s instanceof FlowCaptureContext ) { code_block = IfStatement ; } else-if ( s instanceof FlowScopeContext ) { code_block = IfStatement ; } else { SAVE_LOGGER . error ( "Saving of flow objects of type \"" + s . getClass ( ) . getSimpleName ( ) + "\" not implemented" ) ; } }
public void test() { try { record = create . selectFrom ( Tables . PROJECT ) . where ( Tables . PROJECT . ID . equal ( idProject ) ) . fetchOne ( ) ; code_block = IfStatement ; } catch ( org . jooq . exception . DataAccessException sqex ) { LOG . error ( "Database error encountered {}" , sqex . getMessage ( ) ) ; return null ; } catch ( Exception ex ) { LOG . error ( "Unexpected exception retrieving a project record" ) ; LOG . error ( "Error Message: {}" , ex . getMessage ( ) ) ; return null ; } }
public void test() { try { record = create . selectFrom ( Tables . PROJECT ) . where ( Tables . PROJECT . ID . equal ( idProject ) ) . fetchOne ( ) ; code_block = IfStatement ; } catch ( org . jooq . exception . DataAccessException sqex ) { LOG . error ( "Database error encountered {}" , sqex . getMessage ( ) ) ; return null ; } catch ( Exception ex ) { LOG . error ( "Unexpected exception retrieving a project record" ) ; LOG . error ( "Error Message: {}" , ex . getMessage ( ) ) ; return null ; } }
public void test() { if ( minEvictableIdleTimeMillis < 1000 * 30 ) { LOG . error ( "minEvictableIdleTimeMillis should be greater than 30000" ) ; } }
public void test() { try { com . liferay . commerce . pricing . model . CommercePricingClassCPDefinitionRel returnValue = CommercePricingClassCPDefinitionRelServiceUtil . fetchCommercePricingClassCPDefinitionRel ( commercePricingClassId , cpDefinitionId ) ; return com . liferay . commerce . pricing . model . CommercePricingClassCPDefinitionRelSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try ( Connection connection = this . databaseSpring . connect ( ) ; PreparedStatement preStat = connection . prepareStatement ( query . toString ( ) ) ; ) { int i = 1 ; preStat . setString ( i ++ , testCaseCountryProperties . getTest ( ) ) ; preStat . setString ( i ++ , testCaseCountryProperties . getTestcase ( ) ) ; preStat . setBytes ( i ++ , testCaseCountryProperties . getProperty ( ) . getBytes ( "UTF-8" ) ) ; preStat . setString ( i ++ , testCaseCountryProperties . getType ( ) ) ; preStat . setString ( i ++ , testCaseCountryProperties . getDatabase ( ) ) ; preStat . setBytes ( i ++ , testCaseCountryProperties . getValue1 ( ) . getBytes ( "UTF-8" ) ) ; preStat . setBytes ( i ++ , testCaseCountryProperties . getValue2 ( ) . getBytes ( "UTF-8" ) ) ; preStat . setString ( i ++ , String . valueOf ( testCaseCountryProperties . getLength ( ) ) ) ; preStat . setString ( i ++ , String . valueOf ( testCaseCountryProperties . getRowLimit ( ) ) ) ; preStat . setString ( i ++ , testCaseCountryProperties . getNature ( ) ) ; code_block = TryStatement ;  } catch ( SQLException exception ) { LOG . error ( "Unable to execute query : " + exception . toString ( ) ) ; } catch ( UnsupportedEncodingException ex ) { LOG . error ( ex . toString ( ) ) ; } }
public void test() { if ( response . getStatus ( ) == HttpStatus . UNAUTHORIZED_401 ) { logger . trace ( "Re-Auth needed." ) ; httpClient . getAuthenticationStore ( ) . clearAuthenticationResults ( ) ; request = prepareSOAPRequest ( soapRequest ) . timeout ( SOAP_TIMEOUT , TimeUnit . SECONDS ) ; response = request . send ( ) ; } }
public void setup ( String [ ] names , String [ ] tablepaths , Schema [ ] schemas ) throws Exception { LOG . info ( "===================================================" ) ; LOG . info ( "Starting Test Cluster." ) ; LOG . info ( "===================================================" ) ; util = new TajoTestingCluster ( ) ; util . startMiniCluster ( 1 ) ; conf = util . getConfiguration ( ) ; client = util . newTajoClient ( ) ; FileSystem fs = util . getDefaultFileSystem ( ) ; Path rootDir = TajoConf . getWarehouseDir ( conf ) ; fs . mkdirs ( rootDir ) ; code_block = ForStatement ; LOG . info ( "===================================================" ) ; LOG . info ( "Test Cluster ready and test table created." ) ; LOG . info ( "===================================================" ) ; }
public void setup ( String [ ] names , String [ ] tablepaths , Schema [ ] schemas ) throws Exception { LOG . info ( "===================================================" ) ; LOG . info ( "Starting Test Cluster." ) ; LOG . info ( "===================================================" ) ; util = new TajoTestingCluster ( ) ; util . startMiniCluster ( 1 ) ; conf = util . getConfiguration ( ) ; client = util . newTajoClient ( ) ; FileSystem fs = util . getDefaultFileSystem ( ) ; Path rootDir = TajoConf . getWarehouseDir ( conf ) ; fs . mkdirs ( rootDir ) ; code_block = ForStatement ; LOG . info ( "===================================================" ) ; LOG . info ( "Test Cluster ready and test table created." ) ; LOG . info ( "===================================================" ) ; }
public void test() { try { entityMap = getEntityList ( extensionName , jobName , feedForms , processForms , config ) ; submitEntities ( extensionName , jobName , entityMap , config , request ) ; entityNameMap = getJobEntities ( metaStore . getExtensionJobDetails ( jobName ) ) ; scheduleEntities ( entityNameMap , request , coloExpr ) ; } catch ( FalconException | IOException | JAXBException e ) { LOG . error ( "Error while submitting extension job: " , e ) ; throw FalconWebException . newAPIException ( e , Response . Status . INTERNAL_SERVER_ERROR ) ; } }
@ Override public void configure ( JobConf job ) { log . info ( "Initialisation complete." ) ; }
public void test() { if ( attribute != null ) { LOG . warn ( "Wrong type of {} attribute. Expected {}, got {}" , PaxWebConstants . CONTEXT_PARAM_BUNDLE_CONTEXT , BundleContext . class . getName ( ) , attribute . getClass ( ) . getName ( ) ) ; } }
public void test() { if ( isRunning ( ) ) { LOG . error ( "Error resizing: " + e , e ) ; } else { if ( LOG . isDebugEnabled ( ) ) LOG . debug ( "Error resizing, but no longer running: " + e , e ) ; } }
public void test() { try { int currentSize = getCurrentSizeOperator ( ) . apply ( entity ) ; int desiredSize = Math . min ( max , Math . max ( min , currentSize ) ) ; code_block = IfStatement ; } catch ( Exception e ) { code_block = IfStatement ; } catch ( Throwable t ) { LOG . error ( "Error resizing: " + t , t ) ; throw Throwables . propagate ( t ) ; } }
public void test() { if ( containsAlias ( alias ) ) { TlsContext removeMe = tlsContexts . get ( alias ) ; inboundTlsContexts . remove ( removeMe ) ; outboundTlsContexts . remove ( removeMe ) ; tlsContexts . remove ( alias ) ; knownAliases . remove ( alias ) ; } else { LOGGER . debug ( "No context with alias " + alias + " found, nothing to remove" ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { log . error ( "Error while retrieving records for entity {}, row keys {}" , entityClass , rowIds ) ; throw new KunderaException ( e ) ; } }
public void test() { if ( ! instanceDiscoveryCompleted ) { code_block = IfStatement ; String msg = LogHelper . createMessage ( "Instance discovery was successful" , headers . get ( ClientDataHttpHeaders . CORRELATION_ID_HEADER_NAME ) ) ; log . info ( msg ) ; instanceDiscoveryCompleted = true ; } }
public void test() { try { code_block = IfStatement ; } catch ( PropertyNotSetException e ) { logger . error ( "PropertyNotSetException while deleting nominal label " + e . getMessage ( ) ) ; fail ( "tearDown failed" ) ; } }
public void test() { if ( _log . isWarnEnabled ( ) ) { _log . warn ( portalException , portalException ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( "Registering model listeners for broker " + _broker ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( "Considering virtualhostnode " + vhostNode ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( "Registered model listeners" ) ; } }
public void test() { if ( map . containsKey ( STRIMZI_DEFAULT_TLS_SIDECAR_KAFKA_IMAGE ) ) { log . warn ( "Kafka TLS sidecar container has been removed and the environment variable {} is not used anymore. " + "You can remove it from the Strimzi Cluster Operator deployment." , STRIMZI_DEFAULT_TLS_SIDECAR_KAFKA_IMAGE ) ; } }
@ Test public void testGetJSDateWithDate ( ) { GregorianCalendar calendar = new GregorianCalendar ( 2009 , 11 , 11 ) ; String expectedJavascript = "new Date(2009,11,11,0,0,0,0)" ; String generatedJavascript = DateHelper . getJSDate ( calendar . getTime ( ) ) . toString ( ) ; log . info ( expectedJavascript ) ; log . info ( generatedJavascript ) ; assertEquals ( generatedJavascript , expectedJavascript ) ; }
@ Test public void testGetJSDateWithDate ( ) { GregorianCalendar calendar = new GregorianCalendar ( 2009 , 11 , 11 ) ; String expectedJavascript = "new Date(2009,11,11,0,0,0,0)" ; String generatedJavascript = DateHelper . getJSDate ( calendar . getTime ( ) ) . toString ( ) ; log . info ( expectedJavascript ) ; log . info ( generatedJavascript ) ; assertEquals ( generatedJavascript , expectedJavascript ) ; }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Created FTPClient[connectTimeout: {}, soTimeout: {}, dataTimeout: {}, bufferSize: {}" + ", receiveDataSocketBufferSize: {}, sendDataSocketBufferSize: {}]: {}" , client . getConnectTimeout ( ) , getSoTimeout ( ) , dataTimeout , client . getBufferSize ( ) , client . getReceiveDataSocketBufferSize ( ) , client . getSendDataSocketBufferSize ( ) , client ) ; } }
public void test() { if ( LOG . isInfoEnabled ( ) ) { final String hostInfo = hostname == null ? "(no host locality info)" : "(on host '" + hostname + "')" ; LOG . info ( "Subtask {} {} is requesting a file source split" , subtask , hostInfo ) ; } }
public void test() { if ( nextSplit . isPresent ( ) ) { final FileSourceSplit split = nextSplit . get ( ) ; context . assignSplit ( split , subtask ) ; LOG . info ( "Assigned split to subtask {} : {}" , subtask , split ) ; } else { context . signalNoMoreSplits ( subtask ) ; LOG . info ( "No more splits available for subtask {}" , subtask ) ; } }
public void test() { if ( nextSplit . isPresent ( ) ) { final FileSourceSplit split = nextSplit . get ( ) ; context . assignSplit ( split , subtask ) ; LOG . info ( "Assigned split to subtask {} : {}" , subtask , split ) ; } else { context . signalNoMoreSplits ( subtask ) ; LOG . info ( "No more splits available for subtask {}" , subtask ) ; } }
public void test() { try { localAddress = InetAddress . getByName ( config . outboundIP ) ; logger . debug ( "Outbound local IP.\"{}\"" , localAddress ) ; return ; } catch ( UnknownHostException e ) { } }
public void test() { try { code_block = TryStatement ;  } catch ( FileNotFoundException e ) { Log . error ( "Input file was not found" , e ) ; return false ; } catch ( IOException ex ) { Log . error ( "IOException" , ex ) ; return false ; } }
public void test() { try { code_block = TryStatement ;  } catch ( FileNotFoundException e ) { Log . error ( "Input file was not found" , e ) ; return false ; } catch ( IOException ex ) { Log . error ( "IOException" , ex ) ; return false ; } }
public void test() { if ( statistics != null ) { statistics . onHit ( endpoint . getEndpointUri ( ) ) ; } }
public void test() { if ( statistics != null ) { statistics . onHit ( endpoint . getEndpointUri ( ) ) ; } }
protected void resetAuthentication ( ) { logger . trace ( "resetAuthentication() called." ) ; authenticationToken = emptyAuthenticationToken ; return ; }
public void test() { try { return terminalInfoAvp . getGrouped ( ) . getAvp ( Avp . SOFTWARE_VERSION ) != null ; } catch ( AvpDataException ex ) { logger . debug ( "Failure trying to obtain (Terminal-Information) Software-Version AVP value" , ex ) ; } }
public void test() { try { stmt . close ( ) ; } catch ( SQLException e ) { log . error ( "Error closing SQL statement" , e ) ; } }
public void test() { try { conn . close ( ) ; } catch ( SQLException e ) { log . error ( "Error closing SQL Connection" , e ) ; } }
private void establishServer ( ) throws TTransportException { logger . info ( "[{}] Cluster node {} begins to set up with {} mode" , getServerClientName ( ) , thisNode , ClusterDescriptor . getInstance ( ) . getConfig ( ) . isUseAsyncServer ( ) ? "Async" : "Sync" ) ; code_block = IfStatement ; clientService = Executors . newSingleThreadExecutor ( r -> new Thread ( r , getServerClientName ( ) ) ) ; clientService . submit ( ( ) -> poolServer . serve ( ) ) ; logger . info ( "[{}] Cluster node {} is up" , getServerClientName ( ) , thisNode ) ; }
public void sendSuccessEdgeNotification ( final MonitoredMessage trackMessage ) { LOG . debug ( "Inside Message Monitoring API sendSuccessEdgeNotification() method." ) ; final String subject = MessageMonitoringUtil . getSuccessfulMessageSubjectPrefix ( ) + trackMessage . getSubject ( ) ; final String emailText = MessageMonitoringUtil . getSuccessfulMessageEmailText ( ) + trackMessage . getRecipients ( ) ; final String postmasterEmailId = MessageMonitoringUtil . getDomainPostmasterEmailId ( ) + "@" + MessageMonitoringUtil . getDomainFromEmail ( trackMessage . getSenderemailid ( ) ) ; final DirectEdgeProxy proxy = MessageMonitoringUtil . getDirectEdgeProxy ( ) ; MimeMessage message = null ; String errorMsg = null ; code_block = TryStatement ;  LOG . debug ( "Exiting Message Monitoring API sendSuccessEdgeNotification() method." ) ; }
public void test() { try { message = MessageMonitoringUtil . createMimeMessage ( postmasterEmailId , subject , trackMessage . getSenderemailid ( ) , emailText , trackMessage . getMessageid ( ) ) ; proxy . provideAndRegisterDocumentSetB ( message ) ; getDirectEventLogger ( ) . log ( DirectEventType . DIRECT_EDGE_NOTIFICATION_SUCCESSFUL , message ) ; } catch ( final MessagingException ex ) { errorMsg = ex . getLocalizedMessage ( ) ; LOG . error ( errorMsg , ex ) ; } }
public void test() { if ( hbaseAdmin . tableExists ( htable ) ) { code_block = IfStatement ; hbaseAdmin . deleteTable ( htable ) ; logger . info ( "Deleted HBase table {}" , htable ) ; } else { logger . info ( "HBase table {} does not exist." , htable ) ; } }
public void test() { if ( hbaseAdmin . tableExists ( htable ) ) { code_block = IfStatement ; hbaseAdmin . deleteTable ( htable ) ; logger . info ( "Deleted HBase table {}" , htable ) ; } else { logger . info ( "HBase table {} does not exist." , htable ) ; } }
public void test() { try { code_block = ForStatement ; } catch ( Exception e ) { logger . error ( "Deleting HBase table failed" ) ; } finally { IOUtils . closeQuietly ( hbaseAdmin ) ; } }
@ Test public void testGetBridge ( ) { LOG . info ( "Hadoop version: " + org . apache . hadoop . util . VersionInfo . getVersion ( ) ) ; LOG . info ( "HBase version: " + org . apache . hadoop . hbase . util . VersionInfo . getVersion ( ) ) ; SchemaPlatformBridge bridge = SchemaPlatformBridge . get ( ) ; assertNotNull ( bridge ) ; LOG . info ( "Got platform bridge: " + bridge . getClass ( ) . getName ( ) ) ; }
@ Test public void testGetBridge ( ) { LOG . info ( "Hadoop version: " + org . apache . hadoop . util . VersionInfo . getVersion ( ) ) ; LOG . info ( "HBase version: " + org . apache . hadoop . hbase . util . VersionInfo . getVersion ( ) ) ; SchemaPlatformBridge bridge = SchemaPlatformBridge . get ( ) ; assertNotNull ( bridge ) ; LOG . info ( "Got platform bridge: " + bridge . getClass ( ) . getName ( ) ) ; }
@ Test public void testGetBridge ( ) { LOG . info ( "Hadoop version: " + org . apache . hadoop . util . VersionInfo . getVersion ( ) ) ; LOG . info ( "HBase version: " + org . apache . hadoop . hbase . util . VersionInfo . getVersion ( ) ) ; SchemaPlatformBridge bridge = SchemaPlatformBridge . get ( ) ; assertNotNull ( bridge ) ; LOG . info ( "Got platform bridge: " + bridge . getClass ( ) . getName ( ) ) ; }
@ Test public void testPutMissedDhtRequest_UnstableTopology ( ) throws Exception { blockRebalance = true ; ccfg = cacheConfiguration ( 1 , FULL_SYNC ) ; startServers ( 4 ) ; client = true ; Ignite client = startGrid ( 4 ) ; IgniteCache < Integer , Integer > nearCache = client . cache ( TEST_CACHE ) ; testSpi ( ignite ( 0 ) ) . blockMessages ( new IgniteBiPredicate < ClusterNode , Message > ( ) code_block = "" ; ) ; Integer key = primaryKey ( ignite ( 0 ) . cache ( TEST_CACHE ) ) ; log . info ( "Start put [key=" + key + ']' ) ; IgniteFuture < ? > fut = nearCache . putAsync ( key , key ) ; U . sleep ( 500 ) ; assertFalse ( fut . isDone ( ) ) ; stopGrid ( 0 ) ; fut . get ( ) ; checkData ( F . asMap ( key , key ) ) ; }
private ScheduledReporter createAndGetConfiguredCSVReporter ( String prefix , String csvDir ) throws IOException { File outputDir ; code_block = IfStatement ; FileUtils . forceMkdir ( outputDir ) ; LOG . info ( "Configuring stats with csv output to directory [{}]" , outputDir . getAbsolutePath ( ) ) ; return CsvReporter . forRegistry ( metrics ) . convertRatesTo ( TimeUnit . SECONDS ) . convertDurationsTo ( TimeUnit . MILLISECONDS ) . build ( outputDir ) ; }
public void test() { try { MethodKey methodKey = new MethodKey ( MBMessageServiceUtil . class , "restoreMessageAttachmentFromTrash" , _restoreMessageAttachmentFromTrashParameterTypes32 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , messageId , fileName ) ; code_block = TryStatement ;  } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
@ Override public void text ( String text ) { log . trace ( text ) ; here . addChild ( TreeNode . create_text ( text ) ) ; }
public void test() { if ( couple == null ) { logger . warn ( "Tried to report FP Complications of a non-existing EC, with submission: " + submission ) ; return ; } }
@ Override public Object visit ( PropertyName expression , Object data ) { LOGGER . trace ( "Visiting PropertyName expression" ) ; return data ; }
public void test() { try { logger . info ( "Starting warm up for new cache of site '{}'" , siteName ) ; stopWatch . start ( ) ; doCacheWarmUp ( tmpContext ) ; code_block = IfStatement ; stopWatch . stop ( ) ; logger . info ( "Warm up for new cache of site '{}' completed (switched with old cache) in {} secs" , siteName , stopWatch . getTime ( TimeUnit . SECONDS ) ) ; } catch ( Exception e ) { cacheService . removeScope ( tmpContext ) ; logger . error ( "Cache warm up failed" , e ) ; } }
public void test() { try { logger . info ( "Starting warm up for new cache of site '{}'" , siteName ) ; stopWatch . start ( ) ; doCacheWarmUp ( tmpContext ) ; code_block = IfStatement ; stopWatch . stop ( ) ; logger . info ( "Warm up for new cache of site '{}' completed (switched with old cache) in {} secs" , siteName , stopWatch . getTime ( TimeUnit . SECONDS ) ) ; } catch ( Exception e ) { cacheService . removeScope ( tmpContext ) ; logger . error ( "Cache warm up failed" , e ) ; } }
public void test() { try { logger . info ( "Starting warm up for new cache of site '{}'" , siteName ) ; stopWatch . start ( ) ; doCacheWarmUp ( tmpContext ) ; code_block = IfStatement ; stopWatch . stop ( ) ; logger . info ( "Warm up for new cache of site '{}' completed (switched with old cache) in {} secs" , siteName , stopWatch . getTime ( TimeUnit . SECONDS ) ) ; } catch ( Exception e ) { cacheService . removeScope ( tmpContext ) ; logger . error ( "Cache warm up failed" , e ) ; } }
public void test() { if ( switchCache ) { Context currentContext = siteContext . getContext ( ) ; long oldCacheVersion = currentContext . getCacheVersion ( ) ; long newCacheVersion = System . nanoTime ( ) ; Context tmpContext = currentContext . clone ( ) ; tmpContext . setCacheVersion ( newCacheVersion ) ; cacheService . addScope ( tmpContext ) ; code_block = TryStatement ;  } else { logger . info ( "Starting warm up for cache of site '{}'" , siteName ) ; stopWatch . start ( ) ; doCacheWarmUp ( siteContext . getContext ( ) ) ; stopWatch . stop ( ) ; logger . info ( "Warm up for cache of site '{}' completed in {} secs" , siteName , stopWatch . getTime ( TimeUnit . SECONDS ) ) ; } }
public void test() { if ( switchCache ) { Context currentContext = siteContext . getContext ( ) ; long oldCacheVersion = currentContext . getCacheVersion ( ) ; long newCacheVersion = System . nanoTime ( ) ; Context tmpContext = currentContext . clone ( ) ; tmpContext . setCacheVersion ( newCacheVersion ) ; cacheService . addScope ( tmpContext ) ; code_block = TryStatement ;  } else { logger . info ( "Starting warm up for cache of site '{}'" , siteName ) ; stopWatch . start ( ) ; doCacheWarmUp ( siteContext . getContext ( ) ) ; stopWatch . stop ( ) ; logger . info ( "Warm up for cache of site '{}' completed in {} secs" , siteName , stopWatch . getTime ( TimeUnit . SECONDS ) ) ; } }
public void test() { try { olf . raf . close ( ) ; } catch ( IOException e ) { logger . warn ( String . format ( "Failed to close file %s" , olf . f . getAbsolutePath ( ) ) , e ) ; } }
public void test() { try { InsightsAssessmentConfiguration assessmentConfig = populateAssessmentReportConfiguration ( assessmentReportJson ) ; assessmentReportId = reportConfigDAL . saveInsightsAssessmentConfig ( assessmentConfig ) ; return assessmentReportId ; } catch ( InsightsCustomException e ) { log . error ( "Error while saving the report .. {}" , e . getMessage ( ) ) ; throw new InsightsCustomException ( e . getMessage ( ) ) ; } }
public void test() { if ( ! isZip ( outputZipFile . getName ( ) ) ) { log . warn ( "No .zip suffix[%s], putting files from [%s] into it anyway." , outputZipFile , directory ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "created subreports thread executor " + threadExecutor + " to " + fillContext . getMasterFiller ( ) . getJasperReport ( ) . getName ( ) ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "operationComplete({}) available={}" , this , available ) ; } }
public void test() { if ( future . isWritten ( ) ) { int available ; synchronized ( availableCount ) code_block = "" ; code_block = IfStatement ; log . error ( "operationComplete({}) invalid available count: {}" , this , available ) ; } else { Throwable err = future . getException ( ) ; log . error ( "operationComplete({}) Error ({}) signalled: {}" , this , err . getClass ( ) . getSimpleName ( ) , err . getMessage ( ) ) ; } }
public void test() { if ( future . isDone ( ) ) { code_block = IfStatement ; } else { log . error ( "operationComplete({}) Incomplete future signalled: {}" , this , future ) ; } }
public void test() { try { close ( ) ; } catch ( IOException e ) { log . warn ( "operationComplete({}) unexpected ({}) due to close: {}" , this , e . getClass ( ) . getSimpleName ( ) , e . getMessage ( ) ) ; } }
public void test() { -> { logger . warn ( "No repo loaded warning." ) ; nc . addNotification ( "You need to load a repository before you can perform operations on it. Click on the plus sign in the upper left corner!" ) ; } }
@ Transactional public void updateUserInfo ( int userId , String name , String surname , int age , String userName , String password ) { logger . debug ( "Updating user with id : " + userId ) ; User user = this . entityManager . find ( User . class , userId ) ; user . setName ( name ) ; user . setSurname ( surname ) ; user . setAge ( age ) ; user . setUserName ( userName ) ; user . setPassword ( password ) ; }
public void test() { try { this . requiredTables = requiredTables ; code_block = IfStatement ; code_block = ForStatement ; return true ; } catch ( ConsistencyException cx ) { logger . debug ( "failed to reset" , cx ) ; return false ; } }
private TransactionalDataSource createShardDatasource ( Long shardId , DbVersion dbVersion ) { long start = System . currentTimeMillis ( ) ; waitAvailability ( ) ; ShardDataSourceCreateHelper shardDataSourceCreateHelper = new ShardDataSourceCreateHelper ( this , shardId ) . createUninitializedDataSource ( ) ; TransactionalDataSource shardDb = shardDataSourceCreateHelper . getShardDb ( ) ; shardDb . init ( dbVersion ) ; connectedShardDataSourceMap . put ( shardId , shardDb ) ; log . debug ( "new SHARD datasource'{}' is ADDED in {} ms" , shardDataSourceCreateHelper . getShardName ( ) , System . currentTimeMillis ( ) - start ) ; return shardDb ; }
public void test() { try { code_block = ForStatement ; } catch ( ParseException pe ) { log . error ( pe . getMessage ( ) ) ; } catch ( NumberFormatException nfe ) { log . error ( "Unable to numeric argument " + start + ": " + nfe . getMessage ( ) ) ; } }
public void test() { try { code_block = ForStatement ; } catch ( ParseException pe ) { log . error ( pe . getMessage ( ) ) ; } catch ( NumberFormatException nfe ) { log . error ( "Unable to numeric argument " + start + ": " + nfe . getMessage ( ) ) ; } }
@ NotNull public Multimap < Chromosome , CobaltCount > tumorOnly ( @ NotNull final String tumorBam ) throws IOException , ExecutionException , InterruptedException { final File tumorFile = new File ( tumorBam ) ; final String chromosomeLengthFileName = ChromosomeLengthFile . generateFilename ( mOutputDir , mTumorId ) ; final List < ChromosomeLength > lengths ; code_block = TryStatement ;  ChromosomeLengthFile . write ( chromosomeLengthFileName , lengths ) ; CB_LOGGER . info ( "Calculating Read Count from {}" , tumorFile . toString ( ) ) ; final List < Future < ChromosomeReadCount > > tumorFutures = createFutures ( mReaderFactory , tumorFile , lengths ) ; final Multimap < Chromosome , ReadCount > tumorCounts = fromFutures ( tumorFutures ) ; CB_LOGGER . info ( "Read Count Complete" ) ; return CobaltCountFactory . tumorOnly ( tumorCounts ) ; }
@ NotNull public Multimap < Chromosome , CobaltCount > tumorOnly ( @ NotNull final String tumorBam ) throws IOException , ExecutionException , InterruptedException { final File tumorFile = new File ( tumorBam ) ; final String chromosomeLengthFileName = ChromosomeLengthFile . generateFilename ( mOutputDir , mTumorId ) ; final List < ChromosomeLength > lengths ; code_block = TryStatement ;  ChromosomeLengthFile . write ( chromosomeLengthFileName , lengths ) ; CB_LOGGER . info ( "Calculating Read Count from {}" , tumorFile . toString ( ) ) ; final List < Future < ChromosomeReadCount > > tumorFutures = createFutures ( mReaderFactory , tumorFile , lengths ) ; final Multimap < Chromosome , ReadCount > tumorCounts = fromFutures ( tumorFutures ) ; CB_LOGGER . info ( "Read Count Complete" ) ; return CobaltCountFactory . tumorOnly ( tumorCounts ) ; }
@ Override public Double visit ( LessEqualFilter lessEqualFilter ) { int minBound = 9 - IntStream . rangeClosed ( 0 , 9 ) . filter ( i -> percentiles [ 9 - i ] <= lessEqualFilter . getValue ( ) . doubleValue ( ) ) . findFirst ( ) . orElse ( 0 ) ; final double result = ( ( double ) minBound + 1.0 ) / 10.0 ; log . debug ( "cacheKey:{} LessEqualsFilter: {} percentiles[{}] = {} multiplier: {}" , cacheKey , lessEqualFilter , minBound , percentiles [ minBound ] , result ) ; return result ; }
public void test() { try { log . debug ( "Stylesheet is " + xslFile ) ; InputStream xsl = this . getClass ( ) . getClassLoader ( ) . getResourceAsStream ( xslFile ) ; Transformer tx = TransformerFactory . newInstance ( ) . newTransformer ( new StreamSource ( xsl ) ) ; tx . transform ( xmlSource , new StreamResult ( out ) ) ; log . debug ( out . toString ( ) ) ; InputStream result = new ByteArrayInputStream ( out . toByteArray ( ) ) ; return result ; } catch ( TransformerConfigurationException tce ) { log . error ( "Failed to configure transformer" , tce ) ; throw new CalendarException ( "Failed to configure transformer" , tce ) ; } catch ( TransformerException txe ) { throw new CalendarException ( "Failed transformation" , txe ) ; } }
public void test() { try { log . debug ( "Stylesheet is " + xslFile ) ; InputStream xsl = this . getClass ( ) . getClassLoader ( ) . getResourceAsStream ( xslFile ) ; Transformer tx = TransformerFactory . newInstance ( ) . newTransformer ( new StreamSource ( xsl ) ) ; tx . transform ( xmlSource , new StreamResult ( out ) ) ; log . debug ( out . toString ( ) ) ; InputStream result = new ByteArrayInputStream ( out . toByteArray ( ) ) ; return result ; } catch ( TransformerConfigurationException tce ) { log . error ( "Failed to configure transformer" , tce ) ; throw new CalendarException ( "Failed to configure transformer" , tce ) ; } catch ( TransformerException txe ) { throw new CalendarException ( "Failed transformation" , txe ) ; } }
public void test() { try { log . debug ( "Stylesheet is " + xslFile ) ; InputStream xsl = this . getClass ( ) . getClassLoader ( ) . getResourceAsStream ( xslFile ) ; Transformer tx = TransformerFactory . newInstance ( ) . newTransformer ( new StreamSource ( xsl ) ) ; tx . transform ( xmlSource , new StreamResult ( out ) ) ; log . debug ( out . toString ( ) ) ; InputStream result = new ByteArrayInputStream ( out . toByteArray ( ) ) ; return result ; } catch ( TransformerConfigurationException tce ) { log . error ( "Failed to configure transformer" , tce ) ; throw new CalendarException ( "Failed to configure transformer" , tce ) ; } catch ( TransformerException txe ) { throw new CalendarException ( "Failed transformation" , txe ) ; } }
public void test() { if ( object . getStatus ( ) != ServerStatus . Follower ) { logger . info ( "Land[Follower] -> server mast be Follower, but ->" + object . getStatus ( ) ) ; return ; } }
public void test() { if ( ! followerTimer . get ( ) ) { return ; } }
public void test() { if ( tokensData . isEmpty ( ) ) { LOGGER . info ( "Source " + getSourceName ( ) + " provides no token data." ) ; return null ; } }
public void test() { if ( loggerIsEnabled ) { logger . trace ( "HttpSubjectSecurityFilter skipped because we are already allowed or logged in." ) ; } }
public void test() { if ( loggerIsEnabled ) { logger . trace ( "HttpSecurityStandardFilter skipped because no realm is configured." ) ; } }
@ Test public void testSpdyServerSessionHandlerGoAway ( ) { logger . info ( "Running: testSpdyServerSessionHandlerGoAway v3" ) ; testSpdySessionHandlerGoAway ( SpdyVersion . SPDY_3 , true ) ; logger . info ( "Running: testSpdyServerSessionHandlerGoAway v3.1" ) ; testSpdySessionHandlerGoAway ( SpdyVersion . SPDY_3_1 , true ) ; }
@ Test public void testSpdyServerSessionHandlerGoAway ( ) { logger . info ( "Running: testSpdyServerSessionHandlerGoAway v3" ) ; testSpdySessionHandlerGoAway ( SpdyVersion . SPDY_3 , true ) ; logger . info ( "Running: testSpdyServerSessionHandlerGoAway v3.1" ) ; testSpdySessionHandlerGoAway ( SpdyVersion . SPDY_3_1 , true ) ; }
public void test() { try { lm . prepareDAG ( new ThroughputCounterApp ( ) , conf ) ; LocalMode . Controller lc = lm . getController ( ) ; lc . run ( 20000 ) ; } catch ( Exception ex ) { logger . info ( ex . getMessage ( ) ) ; } }
public String getAttachmentContentById ( String containerId , Number taskId , Number attachmentId , String marshallingType ) { containerId = context . getContainerId ( containerId , new ByTaskIdContainerLocator ( taskId . longValue ( ) ) ) ; Object attachment = userTaskService . getAttachmentContentById ( containerId , taskId . longValue ( ) , attachmentId . longValue ( ) ) ; code_block = IfStatement ; logger . debug ( "About to marshal task attachment with id '{}' {}" , attachmentId , attachment ) ; String response = marshallerHelper . marshal ( containerId , marshallingType , attachment ) ; return response ; }
public void test() { if ( engine == null ) { Class < ? > defaultEngine = ProtobufRpcEngine . class ; Class < ? > impl = defaultEngine ; LOG . debug ( "Using " + impl . getName ( ) + " to " + protocol . getName ( ) ) ; engine = ( RpcEngine ) ReflectionUtils . newInstance ( impl , conf ) ; if ( protocol . isInterface ( ) ) PROXY_ENGINES . put ( Proxy . getProxyClass ( protocol . getClassLoader ( ) , protocol ) , engine ) ; PROTOCOL_ENGINES . put ( protocol , engine ) ; } }
public void test() { try { properties . load ( PropertiesUtil . class . getClassLoader ( ) . getResourceAsStream ( properties_file ) ) ; } catch ( IOException ex ) { logger . error ( "Unable to access " + properties_file , ex ) ; } }
public void test() { try { URI uri = new URI ( l ) ; return Optional . of ( new HTTPLink ( uri ) ) ; } catch ( URISyntaxException e ) { logger . warn ( "Link extractor ({}) returned invalid URI: {}" , name , e . getMessage ( ) ) ; return Optional . empty ( ) ; } }
@ Override public boolean restoreVMFromBackup ( VirtualMachine vm , Backup backup ) { s_logger . debug ( "Restoring vm " + vm . getUuid ( ) + "from backup " + backup . getUuid ( ) + " on the Dummy Backup Provider" ) ; return true ; }
@ Override public void downPhysicalInterface ( LogicalPort iface ) throws CapabilityException { log . info ( "Start of downPhysicalInterface call" ) ; iface . setOperationalStatus ( OperationalStatus . STOPPED ) ; IAction action = createActionAndCheckParams ( ChassisActionSet . CONFIGURESTATUS , iface ) ; queueAction ( action ) ; log . info ( "End of downPhysicalInterface call" ) ; }
@ Override public void downPhysicalInterface ( LogicalPort iface ) throws CapabilityException { log . info ( "Start of downPhysicalInterface call" ) ; iface . setOperationalStatus ( OperationalStatus . STOPPED ) ; IAction action = createActionAndCheckParams ( ChassisActionSet . CONFIGURESTATUS , iface ) ; queueAction ( action ) ; log . info ( "End of downPhysicalInterface call" ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "going " + MemberExpression . DIRECTION . DOWN + " by " + ( expression . isWildcard ( ) ? "wildcard" : "key: [" + expression . getObjectKey ( ) + "]" ) + " on " + jrJsonNode . getDataNode ( ) ) ; } }
public void test() { if ( log != null ) { log . warn ( "Detected failure in monitor accessing " + getUrl ( ) + ": " + problem ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( StringBundler . concat ( "Unable to locate status for background task " , backgroundTaskId , " to process " , message ) ) ; } }
public void test() { if ( backgroundTaskId != _backgroundTaskId ) { return ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "deleting account " + " csid=" + id ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "deleted account " + " csid=" + id ) ; } }
public void test() { if ( ! session . isPresent ( ) ) { logger . error ( "Session not found : " + sessionId ) ; } else { UUID userId = session . get ( ) . getUserId ( ) ; managerFactory . tournamentManager ( ) . joinTournament ( tournamentId , userId ) ; } }
public void test() { try { connection = createConnection ( fullUser , "wrongPassword" , null , false ) ; connection . start ( ) ; fail ( "Expected JMSException" ) ; } catch ( JMSSecurityException ex ) { instanceLog . debug ( "Failed to authenticate connection with incorrect password." ) ; } finally { code_block = IfStatement ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; } }
@ GetMapping @ Override public Collection < TenantDto > getAll ( @ RequestParam final Optional < String > criteria ) { LOGGER . debug ( "Get all criteria={}" , criteria ) ; RestUtils . checkCriteria ( criteria ) ; return internalTenantService . getAll ( criteria ) ; }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { try { code_block = SwitchStatement ; frequencyDetails . setRuns ( runDetailsBean ) ; anchorRunDetailsBean = this . getAcivetaskFrequencyAncorDetailsForManuallySchedule ( activeTask , anchorRunDetailsBean , session ) ; frequencyDetails . setAnchorRuns ( anchorRunDetailsBean ) ; } catch ( Exception e ) { LOGGER . error ( "ActivityMetaDataDao - getFrequencyRunsDetailsForActiveTasks() :: ERROR" , e ) ; } }
public void test() { try { LOG . debug ( "truncating Cassandra table {}" , mapping . getCoreName ( ) ) ; this . client . getSession ( ) . execute ( CassandraQueryFactory . getTruncateTableQuery ( mapping ) ) ; } catch ( Exception e ) { throw new GoraException ( e ) ; } }
public void test() { try ( Stream < UUID > jobs = mantaClient . getAllJobIds ( ) ) { List < UUID > found = jobs . filter ( id -> id . equals ( job1id ) || id . equals ( job2id ) ) . collect ( Collectors . toList ( ) ) ; Assert . assertEquals ( found . size ( ) , 2 , "We should have found both jobs" ) ; } catch ( AssertionError e ) { String msg = "Couldn't find job in job list, retry test a few times to verify" ; LOG . error ( msg , e ) ; throw new SkipException ( msg , e ) ; } }
private void prepareExtendedRandomLength ( ExtendedRandomExtensionMessage msg ) { msg . setExtendedRandomLength ( msg . getExtendedRandom ( ) . getValue ( ) . length ) ; LOGGER . debug ( "ExtendedRandomLength: " + msg . getExtendedRandomLength ( ) . getValue ( ) ) ; }
public void test() { try { iterator = sequence . iterate ( ) ; } catch ( final XPathException xpe ) { LOG . error ( "Unable to extract the underlying Sequence Iterator: {}. Falling back to EMPTY_ITERATOR" , xpe . getMessage ( ) , xpe ) ; iterator = SequenceIterator . EMPTY_ITERATOR ; } }
public void test() { try { RegressionModelPrediction p = null ; AutoMLConfig autoMLConfig = autoMlDAL . getMLConfigByUsecase ( usecaseName ) ; String deployedMojoName = autoMLConfig . getMojoDeployed ( ) ; String predectionColumn = autoMLConfig . getPredictionColumn ( ) ; String path = FileUtils . getTempDirectoryPath ( ) ; byte [ ] mojoData = autoMLConfig . getMojoDeployedZip ( ) ; File file = new File ( path + usecaseName + ".zip" ) ; FileUtils . writeByteArrayToFile ( file , mojoData ) ; EasyPredictModelWrapper model = new EasyPredictModelWrapper ( MojoModel . load ( new TmpMojoReaderBackend ( file ) ) ) ; log . debug ( "Worlflow Detail ====  Mojo {}  Loaded Successfully" , deployedMojoName ) ; Gson gson = new Gson ( ) ; Type type = new TypeToken < Map < String , String > > ( ) code_block = "" ; . getType ( ) ; code_block = ForStatement ; } catch ( Exception e ) { log . error ( e ) ; throw new PredictException ( "Something went wrong while executing regression prediction to " + usecaseName + " " + e . getMessage ( ) ) ; } }
public void test() { try { RegressionModelPrediction p = null ; AutoMLConfig autoMLConfig = autoMlDAL . getMLConfigByUsecase ( usecaseName ) ; String deployedMojoName = autoMLConfig . getMojoDeployed ( ) ; String predectionColumn = autoMLConfig . getPredictionColumn ( ) ; String path = FileUtils . getTempDirectoryPath ( ) ; byte [ ] mojoData = autoMLConfig . getMojoDeployedZip ( ) ; File file = new File ( path + usecaseName + ".zip" ) ; FileUtils . writeByteArrayToFile ( file , mojoData ) ; EasyPredictModelWrapper model = new EasyPredictModelWrapper ( MojoModel . load ( new TmpMojoReaderBackend ( file ) ) ) ; log . debug ( "Worlflow Detail ====  Mojo {}  Loaded Successfully" , deployedMojoName ) ; Gson gson = new Gson ( ) ; Type type = new TypeToken < Map < String , String > > ( ) code_block = "" ; . getType ( ) ; code_block = ForStatement ; } catch ( Exception e ) { log . error ( e ) ; throw new PredictException ( "Something went wrong while executing regression prediction to " + usecaseName + " " + e . getMessage ( ) ) ; } }
public void test() { if ( logger . isTraceEnabled ( LogMarker . DLS_VERBOSE ) ) { logger . trace ( LogMarker . DLS_VERBOSE , "[DLockGrantor.handleLockQuery] {}" , query ) ; } }
public void test() { if ( printTagSet ) { getLogger ( ) . info ( getTagset ( ) . toString ( ) ) ; } }
@ Override public List < Pipeline > createAllPossiblePipelines ( ) { logger . info ( "Get all modules from registry" ) ; @ SuppressWarnings ( "unused" ) List < Module > modules = moduleDAO . getModules ( ) ; return null ; }
private void startMiniTajoCluster ( File testBuildDir , final int numSlaves , boolean local ) throws Exception { TajoConf c = getConfiguration ( ) ; c . setVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . TAJO_MASTER_UMBILICAL_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . WORKER_PEER_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . WORKER_TEMPORAL_DIR , "file://" + testBuildDir . getAbsolutePath ( ) + "/tajo-localdir" ) ; c . setVar ( ConfVars . REST_SERVICE_ADDRESS , "localhost:0" ) ; code_block = IfStatement ; setupCatalogForTesting ( c , testBuildDir ) ; LOG . info ( "derby repository is set to " + conf . get ( CatalogConstants . CATALOG_URI ) ) ; tajoMaster = new TajoMaster ( ) ; tajoMaster . init ( c ) ; tajoMaster . start ( ) ; this . conf . setVar ( ConfVars . WORKER_PEER_RPC_ADDRESS , c . getVar ( ConfVars . WORKER_PEER_RPC_ADDRESS ) ) ; this . conf . setVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS , c . getVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS ) ) ; InetSocketAddress tajoMasterAddress = tajoMaster . getContext ( ) . getTajoMasterService ( ) . getBindAddress ( ) ; this . conf . setVar ( ConfVars . TAJO_MASTER_UMBILICAL_RPC_ADDRESS , NetUtils . getHostPortString ( tajoMasterAddress ) ) ; this . conf . setVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS , c . getVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS ) ) ; this . conf . setVar ( ConfVars . CATALOG_ADDRESS , c . getVar ( ConfVars . CATALOG_ADDRESS ) ) ; InetSocketAddress tajoRestAddress = tajoMaster . getContext ( ) . getRestServer ( ) . getBindAddress ( ) ; this . conf . setVar ( ConfVars . REST_SERVICE_ADDRESS , NetUtils . getHostPortString ( tajoRestAddress ) ) ; startTajoWorkers ( numSlaves ) ; isTajoClusterRunning = true ; LOG . info ( "Mini Tajo cluster is up" ) ; LOG . info ( "====================================================================================" ) ; LOG . info ( "=                           MiniTajoCluster starts up                              =" ) ; LOG . info ( "====================================================================================" ) ; LOG . info ( "= * Master Address: " + tajoMaster . getMasterName ( ) ) ; LOG . info ( "= * CatalogStore: " + tajoMaster . getCatalogServer ( ) . getStoreClassName ( ) ) ; LOG . info ( "------------------------------------------------------------------------------------" ) ; LOG . info ( "= * Warehouse Dir: " + TajoConf . getWarehouseDir ( c ) ) ; LOG . info ( "= * Worker Tmp Dir: " + c . getVar ( ConfVars . WORKER_TEMPORAL_DIR ) ) ; LOG . info ( "====================================================================================" ) ; }
private void startMiniTajoCluster ( File testBuildDir , final int numSlaves , boolean local ) throws Exception { TajoConf c = getConfiguration ( ) ; c . setVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . TAJO_MASTER_UMBILICAL_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . WORKER_PEER_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . WORKER_TEMPORAL_DIR , "file://" + testBuildDir . getAbsolutePath ( ) + "/tajo-localdir" ) ; c . setVar ( ConfVars . REST_SERVICE_ADDRESS , "localhost:0" ) ; code_block = IfStatement ; setupCatalogForTesting ( c , testBuildDir ) ; LOG . info ( "derby repository is set to " + conf . get ( CatalogConstants . CATALOG_URI ) ) ; tajoMaster = new TajoMaster ( ) ; tajoMaster . init ( c ) ; tajoMaster . start ( ) ; this . conf . setVar ( ConfVars . WORKER_PEER_RPC_ADDRESS , c . getVar ( ConfVars . WORKER_PEER_RPC_ADDRESS ) ) ; this . conf . setVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS , c . getVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS ) ) ; InetSocketAddress tajoMasterAddress = tajoMaster . getContext ( ) . getTajoMasterService ( ) . getBindAddress ( ) ; this . conf . setVar ( ConfVars . TAJO_MASTER_UMBILICAL_RPC_ADDRESS , NetUtils . getHostPortString ( tajoMasterAddress ) ) ; this . conf . setVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS , c . getVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS ) ) ; this . conf . setVar ( ConfVars . CATALOG_ADDRESS , c . getVar ( ConfVars . CATALOG_ADDRESS ) ) ; InetSocketAddress tajoRestAddress = tajoMaster . getContext ( ) . getRestServer ( ) . getBindAddress ( ) ; this . conf . setVar ( ConfVars . REST_SERVICE_ADDRESS , NetUtils . getHostPortString ( tajoRestAddress ) ) ; startTajoWorkers ( numSlaves ) ; isTajoClusterRunning = true ; LOG . info ( "Mini Tajo cluster is up" ) ; LOG . info ( "====================================================================================" ) ; LOG . info ( "=                           MiniTajoCluster starts up                              =" ) ; LOG . info ( "====================================================================================" ) ; LOG . info ( "= * Master Address: " + tajoMaster . getMasterName ( ) ) ; LOG . info ( "= * CatalogStore: " + tajoMaster . getCatalogServer ( ) . getStoreClassName ( ) ) ; LOG . info ( "------------------------------------------------------------------------------------" ) ; LOG . info ( "= * Warehouse Dir: " + TajoConf . getWarehouseDir ( c ) ) ; LOG . info ( "= * Worker Tmp Dir: " + c . getVar ( ConfVars . WORKER_TEMPORAL_DIR ) ) ; LOG . info ( "====================================================================================" ) ; }
private void startMiniTajoCluster ( File testBuildDir , final int numSlaves , boolean local ) throws Exception { TajoConf c = getConfiguration ( ) ; c . setVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . TAJO_MASTER_UMBILICAL_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . WORKER_PEER_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . WORKER_TEMPORAL_DIR , "file://" + testBuildDir . getAbsolutePath ( ) + "/tajo-localdir" ) ; c . setVar ( ConfVars . REST_SERVICE_ADDRESS , "localhost:0" ) ; code_block = IfStatement ; setupCatalogForTesting ( c , testBuildDir ) ; LOG . info ( "derby repository is set to " + conf . get ( CatalogConstants . CATALOG_URI ) ) ; tajoMaster = new TajoMaster ( ) ; tajoMaster . init ( c ) ; tajoMaster . start ( ) ; this . conf . setVar ( ConfVars . WORKER_PEER_RPC_ADDRESS , c . getVar ( ConfVars . WORKER_PEER_RPC_ADDRESS ) ) ; this . conf . setVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS , c . getVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS ) ) ; InetSocketAddress tajoMasterAddress = tajoMaster . getContext ( ) . getTajoMasterService ( ) . getBindAddress ( ) ; this . conf . setVar ( ConfVars . TAJO_MASTER_UMBILICAL_RPC_ADDRESS , NetUtils . getHostPortString ( tajoMasterAddress ) ) ; this . conf . setVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS , c . getVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS ) ) ; this . conf . setVar ( ConfVars . CATALOG_ADDRESS , c . getVar ( ConfVars . CATALOG_ADDRESS ) ) ; InetSocketAddress tajoRestAddress = tajoMaster . getContext ( ) . getRestServer ( ) . getBindAddress ( ) ; this . conf . setVar ( ConfVars . REST_SERVICE_ADDRESS , NetUtils . getHostPortString ( tajoRestAddress ) ) ; startTajoWorkers ( numSlaves ) ; isTajoClusterRunning = true ; LOG . info ( "Mini Tajo cluster is up" ) ; LOG . info ( "====================================================================================" ) ; LOG . info ( "=                           MiniTajoCluster starts up                              =" ) ; LOG . info ( "====================================================================================" ) ; LOG . info ( "= * Master Address: " + tajoMaster . getMasterName ( ) ) ; LOG . info ( "= * CatalogStore: " + tajoMaster . getCatalogServer ( ) . getStoreClassName ( ) ) ; LOG . info ( "------------------------------------------------------------------------------------" ) ; LOG . info ( "= * Warehouse Dir: " + TajoConf . getWarehouseDir ( c ) ) ; LOG . info ( "= * Worker Tmp Dir: " + c . getVar ( ConfVars . WORKER_TEMPORAL_DIR ) ) ; LOG . info ( "====================================================================================" ) ; }
private void startMiniTajoCluster ( File testBuildDir , final int numSlaves , boolean local ) throws Exception { TajoConf c = getConfiguration ( ) ; c . setVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . TAJO_MASTER_UMBILICAL_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . WORKER_PEER_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . WORKER_TEMPORAL_DIR , "file://" + testBuildDir . getAbsolutePath ( ) + "/tajo-localdir" ) ; c . setVar ( ConfVars . REST_SERVICE_ADDRESS , "localhost:0" ) ; code_block = IfStatement ; setupCatalogForTesting ( c , testBuildDir ) ; LOG . info ( "derby repository is set to " + conf . get ( CatalogConstants . CATALOG_URI ) ) ; tajoMaster = new TajoMaster ( ) ; tajoMaster . init ( c ) ; tajoMaster . start ( ) ; this . conf . setVar ( ConfVars . WORKER_PEER_RPC_ADDRESS , c . getVar ( ConfVars . WORKER_PEER_RPC_ADDRESS ) ) ; this . conf . setVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS , c . getVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS ) ) ; InetSocketAddress tajoMasterAddress = tajoMaster . getContext ( ) . getTajoMasterService ( ) . getBindAddress ( ) ; this . conf . setVar ( ConfVars . TAJO_MASTER_UMBILICAL_RPC_ADDRESS , NetUtils . getHostPortString ( tajoMasterAddress ) ) ; this . conf . setVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS , c . getVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS ) ) ; this . conf . setVar ( ConfVars . CATALOG_ADDRESS , c . getVar ( ConfVars . CATALOG_ADDRESS ) ) ; InetSocketAddress tajoRestAddress = tajoMaster . getContext ( ) . getRestServer ( ) . getBindAddress ( ) ; this . conf . setVar ( ConfVars . REST_SERVICE_ADDRESS , NetUtils . getHostPortString ( tajoRestAddress ) ) ; startTajoWorkers ( numSlaves ) ; isTajoClusterRunning = true ; LOG . info ( "Mini Tajo cluster is up" ) ; LOG . info ( "====================================================================================" ) ; LOG . info ( "=                           MiniTajoCluster starts up                              =" ) ; LOG . info ( "====================================================================================" ) ; LOG . info ( "= * Master Address: " + tajoMaster . getMasterName ( ) ) ; LOG . info ( "= * CatalogStore: " + tajoMaster . getCatalogServer ( ) . getStoreClassName ( ) ) ; LOG . info ( "------------------------------------------------------------------------------------" ) ; LOG . info ( "= * Warehouse Dir: " + TajoConf . getWarehouseDir ( c ) ) ; LOG . info ( "= * Worker Tmp Dir: " + c . getVar ( ConfVars . WORKER_TEMPORAL_DIR ) ) ; LOG . info ( "====================================================================================" ) ; }
private void startMiniTajoCluster ( File testBuildDir , final int numSlaves , boolean local ) throws Exception { TajoConf c = getConfiguration ( ) ; c . setVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . TAJO_MASTER_UMBILICAL_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . WORKER_PEER_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . WORKER_TEMPORAL_DIR , "file://" + testBuildDir . getAbsolutePath ( ) + "/tajo-localdir" ) ; c . setVar ( ConfVars . REST_SERVICE_ADDRESS , "localhost:0" ) ; code_block = IfStatement ; setupCatalogForTesting ( c , testBuildDir ) ; LOG . info ( "derby repository is set to " + conf . get ( CatalogConstants . CATALOG_URI ) ) ; tajoMaster = new TajoMaster ( ) ; tajoMaster . init ( c ) ; tajoMaster . start ( ) ; this . conf . setVar ( ConfVars . WORKER_PEER_RPC_ADDRESS , c . getVar ( ConfVars . WORKER_PEER_RPC_ADDRESS ) ) ; this . conf . setVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS , c . getVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS ) ) ; InetSocketAddress tajoMasterAddress = tajoMaster . getContext ( ) . getTajoMasterService ( ) . getBindAddress ( ) ; this . conf . setVar ( ConfVars . TAJO_MASTER_UMBILICAL_RPC_ADDRESS , NetUtils . getHostPortString ( tajoMasterAddress ) ) ; this . conf . setVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS , c . getVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS ) ) ; this . conf . setVar ( ConfVars . CATALOG_ADDRESS , c . getVar ( ConfVars . CATALOG_ADDRESS ) ) ; InetSocketAddress tajoRestAddress = tajoMaster . getContext ( ) . getRestServer ( ) . getBindAddress ( ) ; this . conf . setVar ( ConfVars . REST_SERVICE_ADDRESS , NetUtils . getHostPortString ( tajoRestAddress ) ) ; startTajoWorkers ( numSlaves ) ; isTajoClusterRunning = true ; LOG . info ( "Mini Tajo cluster is up" ) ; LOG . info ( "====================================================================================" ) ; LOG . info ( "=                           MiniTajoCluster starts up                              =" ) ; LOG . info ( "====================================================================================" ) ; LOG . info ( "= * Master Address: " + tajoMaster . getMasterName ( ) ) ; LOG . info ( "= * CatalogStore: " + tajoMaster . getCatalogServer ( ) . getStoreClassName ( ) ) ; LOG . info ( "------------------------------------------------------------------------------------" ) ; LOG . info ( "= * Warehouse Dir: " + TajoConf . getWarehouseDir ( c ) ) ; LOG . info ( "= * Worker Tmp Dir: " + c . getVar ( ConfVars . WORKER_TEMPORAL_DIR ) ) ; LOG . info ( "====================================================================================" ) ; }
private void startMiniTajoCluster ( File testBuildDir , final int numSlaves , boolean local ) throws Exception { TajoConf c = getConfiguration ( ) ; c . setVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . TAJO_MASTER_UMBILICAL_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . WORKER_PEER_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . WORKER_TEMPORAL_DIR , "file://" + testBuildDir . getAbsolutePath ( ) + "/tajo-localdir" ) ; c . setVar ( ConfVars . REST_SERVICE_ADDRESS , "localhost:0" ) ; code_block = IfStatement ; setupCatalogForTesting ( c , testBuildDir ) ; LOG . info ( "derby repository is set to " + conf . get ( CatalogConstants . CATALOG_URI ) ) ; tajoMaster = new TajoMaster ( ) ; tajoMaster . init ( c ) ; tajoMaster . start ( ) ; this . conf . setVar ( ConfVars . WORKER_PEER_RPC_ADDRESS , c . getVar ( ConfVars . WORKER_PEER_RPC_ADDRESS ) ) ; this . conf . setVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS , c . getVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS ) ) ; InetSocketAddress tajoMasterAddress = tajoMaster . getContext ( ) . getTajoMasterService ( ) . getBindAddress ( ) ; this . conf . setVar ( ConfVars . TAJO_MASTER_UMBILICAL_RPC_ADDRESS , NetUtils . getHostPortString ( tajoMasterAddress ) ) ; this . conf . setVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS , c . getVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS ) ) ; this . conf . setVar ( ConfVars . CATALOG_ADDRESS , c . getVar ( ConfVars . CATALOG_ADDRESS ) ) ; InetSocketAddress tajoRestAddress = tajoMaster . getContext ( ) . getRestServer ( ) . getBindAddress ( ) ; this . conf . setVar ( ConfVars . REST_SERVICE_ADDRESS , NetUtils . getHostPortString ( tajoRestAddress ) ) ; startTajoWorkers ( numSlaves ) ; isTajoClusterRunning = true ; LOG . info ( "Mini Tajo cluster is up" ) ; LOG . info ( "====================================================================================" ) ; LOG . info ( "=                           MiniTajoCluster starts up                              =" ) ; LOG . info ( "====================================================================================" ) ; LOG . info ( "= * Master Address: " + tajoMaster . getMasterName ( ) ) ; LOG . info ( "= * CatalogStore: " + tajoMaster . getCatalogServer ( ) . getStoreClassName ( ) ) ; LOG . info ( "------------------------------------------------------------------------------------" ) ; LOG . info ( "= * Warehouse Dir: " + TajoConf . getWarehouseDir ( c ) ) ; LOG . info ( "= * Worker Tmp Dir: " + c . getVar ( ConfVars . WORKER_TEMPORAL_DIR ) ) ; LOG . info ( "====================================================================================" ) ; }
private void startMiniTajoCluster ( File testBuildDir , final int numSlaves , boolean local ) throws Exception { TajoConf c = getConfiguration ( ) ; c . setVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . TAJO_MASTER_UMBILICAL_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . WORKER_PEER_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . WORKER_TEMPORAL_DIR , "file://" + testBuildDir . getAbsolutePath ( ) + "/tajo-localdir" ) ; c . setVar ( ConfVars . REST_SERVICE_ADDRESS , "localhost:0" ) ; code_block = IfStatement ; setupCatalogForTesting ( c , testBuildDir ) ; LOG . info ( "derby repository is set to " + conf . get ( CatalogConstants . CATALOG_URI ) ) ; tajoMaster = new TajoMaster ( ) ; tajoMaster . init ( c ) ; tajoMaster . start ( ) ; this . conf . setVar ( ConfVars . WORKER_PEER_RPC_ADDRESS , c . getVar ( ConfVars . WORKER_PEER_RPC_ADDRESS ) ) ; this . conf . setVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS , c . getVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS ) ) ; InetSocketAddress tajoMasterAddress = tajoMaster . getContext ( ) . getTajoMasterService ( ) . getBindAddress ( ) ; this . conf . setVar ( ConfVars . TAJO_MASTER_UMBILICAL_RPC_ADDRESS , NetUtils . getHostPortString ( tajoMasterAddress ) ) ; this . conf . setVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS , c . getVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS ) ) ; this . conf . setVar ( ConfVars . CATALOG_ADDRESS , c . getVar ( ConfVars . CATALOG_ADDRESS ) ) ; InetSocketAddress tajoRestAddress = tajoMaster . getContext ( ) . getRestServer ( ) . getBindAddress ( ) ; this . conf . setVar ( ConfVars . REST_SERVICE_ADDRESS , NetUtils . getHostPortString ( tajoRestAddress ) ) ; startTajoWorkers ( numSlaves ) ; isTajoClusterRunning = true ; LOG . info ( "Mini Tajo cluster is up" ) ; LOG . info ( "====================================================================================" ) ; LOG . info ( "=                           MiniTajoCluster starts up                              =" ) ; LOG . info ( "====================================================================================" ) ; LOG . info ( "= * Master Address: " + tajoMaster . getMasterName ( ) ) ; LOG . info ( "= * CatalogStore: " + tajoMaster . getCatalogServer ( ) . getStoreClassName ( ) ) ; LOG . info ( "------------------------------------------------------------------------------------" ) ; LOG . info ( "= * Warehouse Dir: " + TajoConf . getWarehouseDir ( c ) ) ; LOG . info ( "= * Worker Tmp Dir: " + c . getVar ( ConfVars . WORKER_TEMPORAL_DIR ) ) ; LOG . info ( "====================================================================================" ) ; }
private void startMiniTajoCluster ( File testBuildDir , final int numSlaves , boolean local ) throws Exception { TajoConf c = getConfiguration ( ) ; c . setVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . TAJO_MASTER_UMBILICAL_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . WORKER_PEER_RPC_ADDRESS , "localhost:0" ) ; c . setVar ( ConfVars . WORKER_TEMPORAL_DIR , "file://" + testBuildDir . getAbsolutePath ( ) + "/tajo-localdir" ) ; c . setVar ( ConfVars . REST_SERVICE_ADDRESS , "localhost:0" ) ; code_block = IfStatement ; setupCatalogForTesting ( c , testBuildDir ) ; LOG . info ( "derby repository is set to " + conf . get ( CatalogConstants . CATALOG_URI ) ) ; tajoMaster = new TajoMaster ( ) ; tajoMaster . init ( c ) ; tajoMaster . start ( ) ; this . conf . setVar ( ConfVars . WORKER_PEER_RPC_ADDRESS , c . getVar ( ConfVars . WORKER_PEER_RPC_ADDRESS ) ) ; this . conf . setVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS , c . getVar ( ConfVars . TAJO_MASTER_CLIENT_RPC_ADDRESS ) ) ; InetSocketAddress tajoMasterAddress = tajoMaster . getContext ( ) . getTajoMasterService ( ) . getBindAddress ( ) ; this . conf . setVar ( ConfVars . TAJO_MASTER_UMBILICAL_RPC_ADDRESS , NetUtils . getHostPortString ( tajoMasterAddress ) ) ; this . conf . setVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS , c . getVar ( ConfVars . RESOURCE_TRACKER_RPC_ADDRESS ) ) ; this . conf . setVar ( ConfVars . CATALOG_ADDRESS , c . getVar ( ConfVars . CATALOG_ADDRESS ) ) ; InetSocketAddress tajoRestAddress = tajoMaster . getContext ( ) . getRestServer ( ) . getBindAddress ( ) ; this . conf . setVar ( ConfVars . REST_SERVICE_ADDRESS , NetUtils . getHostPortString ( tajoRestAddress ) ) ; startTajoWorkers ( numSlaves ) ; isTajoClusterRunning = true ; LOG . info ( "Mini Tajo cluster is up" ) ; LOG . info ( "====================================================================================" ) ; LOG . info ( "=                           MiniTajoCluster starts up                              =" ) ; LOG . info ( "====================================================================================" ) ; LOG . info ( "= * Master Address: " + tajoMaster . getMasterName ( ) ) ; LOG . info ( "= * CatalogStore: " + tajoMaster . getCatalogServer ( ) . getStoreClassName ( ) ) ; LOG . info ( "------------------------------------------------------------------------------------" ) ; LOG . info ( "= * Warehouse Dir: " + TajoConf . getWarehouseDir ( c ) ) ; LOG . info ( "= * Worker Tmp Dir: " + c . getVar ( ConfVars . WORKER_TEMPORAL_DIR ) ) ; LOG . info ( "====================================================================================" ) ; }
public void test() { try { final Session session = relayClient . createConnection ( relayHost , relayPort , securedRelay ) ; final GatekeeperRelayRequest request = relayClient . sendAcknowledgementAndReturnRequest ( session , gaMsg ) ; final Object response = handleRequest ( request ) ; relayClient . sendResponse ( session , request , response ) ; } catch ( final JMSException | ArrowheadException ex ) { logger . debug ( "Error while communicating with an other gatekeeper: {}" , ex . getMessage ( ) ) ; logger . debug ( "Exception:" , ex ) ; } }
public void test() { try { final Session session = relayClient . createConnection ( relayHost , relayPort , securedRelay ) ; final GatekeeperRelayRequest request = relayClient . sendAcknowledgementAndReturnRequest ( session , gaMsg ) ; final Object response = handleRequest ( request ) ; relayClient . sendResponse ( session , request , response ) ; } catch ( final JMSException | ArrowheadException ex ) { logger . debug ( "Error while communicating with an other gatekeeper: {}" , ex . getMessage ( ) ) ; logger . debug ( "Exception:" , ex ) ; } }
public void test() { try { LOGGER . debug ( "Listing topics" ) ; ListTopicsOptions listOptions = new ListTopicsOptions ( ) . listInternal ( true ) ; return mapFuture ( adminClient . listTopics ( listOptions ) . names ( ) ) ; } catch ( Exception e ) { return Future . failedFuture ( e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( IOException e ) { LOGGER . error ( "Failed to get revision {} of note {}" , revId , noteId , e ) ; } }
@ Override public void restart ( long seekTo ) { logger . debug ( "RestartFreezePosition=" + seekTo ) ; lastCurrentPosition = 0 ; load ( videoUri , seekTo , playWhenPrepared ) ; }
public void test() { try { api . deleteTag ( tag . getUuid ( ) ) ; } catch ( Exception ex ) { logger . warn ( ex , ">> could not delete tag: %s" , tag ) ; } }
public void test() { try { _log . info ( "validating dates...." ) ; validateDates ( bundleStartDate , bundleEndDate ) ; _log . info ( "dates valid" ) ; } catch ( DateValidationException e ) { code_block = TryStatement ;  } }
public void test() { try { _log . info ( "validating dates...." ) ; validateDates ( bundleStartDate , bundleEndDate ) ; _log . info ( "dates valid" ) ; } catch ( DateValidationException e ) { code_block = TryStatement ;  } }
public void test() { if ( response == null ) { BundleBuildRequest buildRequest = new BundleBuildRequest ( ) ; buildRequest . setBundleDirectory ( bundleDirectory ) ; buildRequest . setBundleName ( bundleName ) ; buildRequest . setEmailAddress ( email ) ; buildRequest . setBundleStartDate ( bundleStartDate ) ; buildRequest . setBundleEndDate ( bundleEndDate ) ; buildRequest . setBundleComment ( bundleComment ) ; buildRequest . setArchiveFlag ( archive ) ; buildRequest . setConsolidateFlag ( consolidate ) ; buildRequest . setPredate ( predate ) ; String session = RequestContextHolder . currentRequestAttributes ( ) . getSessionId ( ) ; buildRequest . setSessionId ( session ) ; code_block = TryStatement ;  } else { _log . warn ( "something went wrong with validation" ) ; } }
public void test() { if ( attributeParser != null ) { boolean isAttributeRead = attributeParser . processAttribute ( currentNode . getLocalPart ( ) , attributeName . getLocalPart ( ) , attribute . getValue ( ) , attributeName . getNamespaceURI ( ) , attributeName . getPrefix ( ) , isLastAttribute , sbmlElements . peek ( ) ) ; code_block = IfStatement ; } else { logger . warn ( "Cannot find a parser for the " + attribute . getName ( ) . getNamespaceURI ( ) + " namespace" ) ; } }
public void test() { try ( Tx tx = StructrApp . getInstance ( securityContext ) . tx ( ) ) { boolean exists = ( getStructrUser ( string ) != null ) ; tx . success ( ) ; return exists ; } catch ( FrameworkException fex ) { logger . error ( "Unable to determine if user " + string + " exists" , fex ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( WikiNodeServiceUtil . class , "getNodes" , _getNodesParameterTypes5 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( java . util . List < com . liferay . wiki . model . WikiNode > ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
private void clearResourceDefinitions ( ) { log . debug ( "Clearing resource definitions" ) ; resourceDefinitionRepository . deleteAll ( ) ; }
public void test() { if ( this . username == null ) { LOG . info ( "Username was not supplied." ) ; } }
public void test() { if ( this . password == null ) { LOG . info ( "Password was not supplied." ) ; } }
public void test() { try { closeable . close ( ) ; } catch ( final Exception e ) { getLogger ( ) . warn ( "Failed to close SQL resource" , e ) ; } }
@ SuppressWarnings ( "unchecked" ) public ModuleList nextSubModuleSet ( ) throws IOException { RingFactory coeff = nextCoefficientRing ( ) ; logger . info ( "coeff = " + coeff . getClass ( ) . getSimpleName ( ) ) ; vars = nextVariableList ( ) ; logger . info ( "vars = " + Arrays . toString ( vars ) ) ; code_block = IfStatement ; tord = nextTermOrder ( ) ; logger . info ( "tord = " + tord ) ; initFactory ( coeff , parsedCoeff ) ; List < List < GenPolynomial > > m = null ; m = nextSubModuleList ( ) ; logger . info ( "m = " + m ) ; return new ModuleList ( pfac , m ) ; }
@ SuppressWarnings ( "unchecked" ) public ModuleList nextSubModuleSet ( ) throws IOException { RingFactory coeff = nextCoefficientRing ( ) ; logger . info ( "coeff = " + coeff . getClass ( ) . getSimpleName ( ) ) ; vars = nextVariableList ( ) ; logger . info ( "vars = " + Arrays . toString ( vars ) ) ; code_block = IfStatement ; tord = nextTermOrder ( ) ; logger . info ( "tord = " + tord ) ; initFactory ( coeff , parsedCoeff ) ; List < List < GenPolynomial > > m = null ; m = nextSubModuleList ( ) ; logger . info ( "m = " + m ) ; return new ModuleList ( pfac , m ) ; }
@ SuppressWarnings ( "unchecked" ) public ModuleList nextSubModuleSet ( ) throws IOException { RingFactory coeff = nextCoefficientRing ( ) ; logger . info ( "coeff = " + coeff . getClass ( ) . getSimpleName ( ) ) ; vars = nextVariableList ( ) ; logger . info ( "vars = " + Arrays . toString ( vars ) ) ; code_block = IfStatement ; tord = nextTermOrder ( ) ; logger . info ( "tord = " + tord ) ; initFactory ( coeff , parsedCoeff ) ; List < List < GenPolynomial > > m = null ; m = nextSubModuleList ( ) ; logger . info ( "m = " + m ) ; return new ModuleList ( pfac , m ) ; }
@ SuppressWarnings ( "unchecked" ) public ModuleList nextSubModuleSet ( ) throws IOException { RingFactory coeff = nextCoefficientRing ( ) ; logger . info ( "coeff = " + coeff . getClass ( ) . getSimpleName ( ) ) ; vars = nextVariableList ( ) ; logger . info ( "vars = " + Arrays . toString ( vars ) ) ; code_block = IfStatement ; tord = nextTermOrder ( ) ; logger . info ( "tord = " + tord ) ; initFactory ( coeff , parsedCoeff ) ; List < List < GenPolynomial > > m = null ; m = nextSubModuleList ( ) ; logger . info ( "m = " + m ) ; return new ModuleList ( pfac , m ) ; }
public void test() { if ( cmd != null ) { bridge . getStick ( ) . sendCommand ( cmd , Collections . singletonList ( channelId ) ) ; } else { logger . debug ( "Unhandled command {}." , command ) ; } }
public void test() { -> { final List < CompletableFuture < DeploymentTestResult > > futures = Lists . newArrayList ( ) ; code_block = ForStatement ; final List < DeploymentTestResult > results = futures . stream ( ) . map ( CompletableFuture :: join ) . collect ( Collectors . toList ( ) ) ; context . setDeploymentTestResults ( results ) ; logger . info ( "Job statistics: {}" , context . getDeploymentTest ( ) . getStatistics ( ) ) ; return null ; } }
public void test() { if ( znRecord == null ) { LOGGER . warn ( "Failed to find segment ZK metadata for segment: {}, table: {}" , segment , _offlineTableName ) ; return INVALID_END_TIME_MS ; } }
public void test() { if ( endTime > 0 ) { TimeUnit timeUnit = znRecord . getEnumField ( CommonConstants . Segment . TIME_UNIT , TimeUnit . class , TimeUnit . DAYS ) ; endTimeMs = timeUnit . toMillis ( endTime ) ; } else { LOGGER . warn ( "Failed to find valid end time for segment: {}, table: {}" , segment , _offlineTableName ) ; } }
public void test() { try { csrfOptions = Csrf . CsrfOptions . valueOf ( csrfProtection . toString ( ) ) ; } catch ( IllegalArgumentException illegalArgumentException ) { _log . error ( illegalArgumentException . getMessage ( ) , illegalArgumentException ) ; } }
public void test() { try { AuthTokenUtil . checkCSRFToken ( themeDisplay . getRequest ( ) , CsrfValidationInterceptor . class . getName ( ) ) ; proceed = true ; } catch ( PrincipalException principalException ) { _log . error ( "Invalid CSRF token" , principalException ) ; } }
public void test() { if ( args [ 0 ] instanceof ClientDataRequest ) { ClientDataRequest clientDataRequest = ( ClientDataRequest ) args [ 0 ] ; String method = StringUtil . toLowerCase ( clientDataRequest . getMethod ( ) ) ; code_block = IfStatement ; } else { _log . error ( "The first parameter of the method signature must be an " + "ActionRequest or ResourceRequest" ) ; } }
public void test() { if ( args . length == 2 ) { code_block = IfStatement ; } else { _log . error ( "The method signature must include (ActionRequest, " + "ActionResponse) or (ResourceRequest, ResourceResponse) " + "as parameters" ) ; } }
public void test() { if ( yamlConfigFile != null ) { YamlConfigFile yamlConfig = readYamlConfigFile ( yamlConfigFile ) ; checkYamlConfig ( yamlConfigFile , yamlConfig ) ; LOG . info ( "Using YAML configuration!" ) ; return ParametersHolder . createWithCmdLineAndYaml ( cli , yamlConfig , Command . RUN_JOB ) ; } else { LOG . info ( "Using CLI configuration!" ) ; return ParametersHolder . createWithCmdLine ( cli , Command . RUN_JOB ) ; } }
public void test() { if ( yamlConfigFile != null ) { YamlConfigFile yamlConfig = readYamlConfigFile ( yamlConfigFile ) ; checkYamlConfig ( yamlConfigFile , yamlConfig ) ; LOG . info ( "Using YAML configuration!" ) ; return ParametersHolder . createWithCmdLineAndYaml ( cli , yamlConfig , Command . RUN_JOB ) ; } else { LOG . info ( "Using CLI configuration!" ) ; return ParametersHolder . createWithCmdLine ( cli , Command . RUN_JOB ) ; } }
@ Override public Representation represent ( final Variant variant ) throws ResourceException { log . debug ( "Getting recommendations for user with OS ID: " + openSocialId ) ; List < Recommendation > recommendations ; code_block = IfStatement ; log . debug ( recommendations . size ( ) + " recommendations found" ) ; JSONObject json = new JSONObject ( ) ; JSONArray recos = new JSONArray ( ) ; code_block = IfStatement ; json . put ( RECOMMENDATIONS_KEY , recos ) ; Representation rep = new StringRepresentation ( json . toString ( ) , MediaType . APPLICATION_JSON ) ; rep . setExpirationDate ( new Date ( 0L ) ) ; return rep ; }
@ Override public Representation represent ( final Variant variant ) throws ResourceException { log . debug ( "Getting recommendations for user with OS ID: " + openSocialId ) ; List < Recommendation > recommendations ; code_block = IfStatement ; log . debug ( recommendations . size ( ) + " recommendations found" ) ; JSONObject json = new JSONObject ( ) ; JSONArray recos = new JSONArray ( ) ; code_block = IfStatement ; json . put ( RECOMMENDATIONS_KEY , recos ) ; Representation rep = new StringRepresentation ( json . toString ( ) , MediaType . APPLICATION_JSON ) ; rep . setExpirationDate ( new Date ( 0L ) ) ; return rep ; }
private ByteBuffer recommendProtocolVersion ( ) { logger . debug ( "Recommending to Peer {} that Protocol Version {} be used" , peerDescription , protocolVersion ) ; final ByteBuffer buffer = ByteBuffer . allocate ( 1 ) ; buffer . put ( ( byte ) protocolVersion ) ; buffer . rewind ( ) ; readTimeout = System . currentTimeMillis ( ) + timeoutMillis ; phase = TransactionPhase . RECEIVE_PROTOCOL_VERSION_ACKNOWLEDGMENT ; return buffer ; }
public void test() { if ( placeHolderData == null ) { log . debug ( "no env type {} are configured for generated artifacts" , envType ) ; throw new ByActionStatusComponentException ( ActionStatus . GENERAL_ERROR ) ; } }
public void test() { if ( placeHolderData == null ) { log . debug ( "no env type {} are configured for generated artifacts" , envType ) ; throw new ByActionStatusComponentException ( ActionStatus . GENERAL_ERROR ) ; } }
public void test() { if ( addHeatEnvArtifact . isRight ( ) ) { log . debug ( "failed to create heat env artifact on resource instance" ) ; throw new ByResponseFormatComponentException ( componentsUtils . getResponseFormatForResourceInstance ( componentsUtils . convertFromStorageResponseForResourceInstance ( addHeatEnvArtifact . right ( ) . value ( ) , false ) , "" , null ) ) ; } }
private static void createExperimentEntities ( ) { ExperimentTest experimentTest = new ExperimentTest ( ) ; CreateExperiment createExperimentRequest = experimentTest . getCreateExperimentRequest ( project . getId ( ) , "Experiment_1" ) ; CreateExperiment . Response createExperimentResponse = experimentServiceStub . createExperiment ( createExperimentRequest ) ; experiment = createExperimentResponse . getExperiment ( ) ; LOGGER . info ( "Experiment created successfully" ) ; assertEquals ( "Experiment name not match with expected Experiment name" , createExperimentRequest . getName ( ) , experiment . getName ( ) ) ; }
public void test() { try { LOG . info ( "Building suggester index for: " + suggester . getName ( ) ) ; final long startMillis = System . currentTimeMillis ( ) ; suggester . build ( core , newSearcher ) ; final long timeTakenMillis = System . currentTimeMillis ( ) - startMillis ; LOG . info ( "Built suggester " + suggester . getName ( ) + ", took " + timeTakenMillis + " ms" ) ; } catch ( Exception e ) { LOG . error ( "Exception in building suggester index for: " + suggester . getName ( ) , e ) ; } }
public void test() { try { LOG . info ( "Building suggester index for: " + suggester . getName ( ) ) ; final long startMillis = System . currentTimeMillis ( ) ; suggester . build ( core , newSearcher ) ; final long timeTakenMillis = System . currentTimeMillis ( ) - startMillis ; LOG . info ( "Built suggester " + suggester . getName ( ) + ", took " + timeTakenMillis + " ms" ) ; } catch ( Exception e ) { LOG . error ( "Exception in building suggester index for: " + suggester . getName ( ) , e ) ; } }
public void test() { try { LOG . info ( "Building suggester index for: " + suggester . getName ( ) ) ; final long startMillis = System . currentTimeMillis ( ) ; suggester . build ( core , newSearcher ) ; final long timeTakenMillis = System . currentTimeMillis ( ) - startMillis ; LOG . info ( "Built suggester " + suggester . getName ( ) + ", took " + timeTakenMillis + " ms" ) ; } catch ( Exception e ) { LOG . error ( "Exception in building suggester index for: " + suggester . getName ( ) , e ) ; } }
public void test() { switch ( error ) { case NO_ENTRY_FOR_PARTICIPANT : case NO_ENTRY_FOR_SELECTED_BACKENDS : logger . trace ( "DISCOVERY lookup {} for domains: {}, interface: {}, {}, gbids: {} returned DiscoveryError {}, continuing" , arbitrationCnt , domains , interfaceName , interfaceVersion , Arrays . toString ( gbids ) , error ) ; code_block = IfStatement ; break ; case UNKNOWN_GBID : case INVALID_GBID : case INTERNAL_ERROR : default : logger . trace ( "DISCOVERY lookup {} for domains: {}, interface: {}, {}, gbids: {} returned DiscoveryError {}, giving up" , arbitrationCnt , domains , interfaceName , interfaceVersion , Arrays . toString ( gbids ) , error ) ; arbitrationFailed ( new ApplicationException ( error ) ) ; break ; } }
public void test() { try { listener . gotFavorites ( statuses ) ; } catch ( Exception e ) { logger . warn ( "Exception at getFavorites" , e ) ; } }
public void test() { if ( LOGGER . isInfoEnabled ( ) ) { LOGGER . info ( "dispose: " + this . getId ( ) ) ; } }
private RestResponse < ObjectMap > getAcl ( ) throws CatalogException , ClientException { logger . debug ( "Get Acl" ) ; StudyCommandOptions . AclsCommandOptions c = studiesCommandOptions . aclsCommandOptions ; ObjectMap params = new ObjectMap ( ) ; params . putIfNotEmpty ( "member" , c . memberId ) ; return openCGAClient . getStudyClient ( ) . acl ( getSingleValidStudy ( c . study ) , params ) ; }
public void test() { try { uc . retrieve ( Imeji . adminUser . getEmail ( ) ) ; } catch ( NotFoundException e ) { LOGGER . info ( "!!! IMPORTANT !!! Create admin@imeji.org as system administrator with password admin. !!! CHANGE PASSWORD !!!" ) ; Imeji . adminUser = uc . create ( Imeji . adminUser , USER_TYPE . ADMIN ) ; LOGGER . info ( "Created admin user successfully:" + Imeji . adminUser . getEmail ( ) , e ) ; } }
public void test() { try { uc . retrieve ( Imeji . adminUser . getEmail ( ) ) ; } catch ( NotFoundException e ) { LOGGER . info ( "!!! IMPORTANT !!! Create admin@imeji.org as system administrator with password admin. !!! CHANGE PASSWORD !!!" ) ; Imeji . adminUser = uc . create ( Imeji . adminUser , USER_TYPE . ADMIN ) ; LOGGER . info ( "Created admin user successfully:" + Imeji . adminUser . getEmail ( ) , e ) ; } }
public void test() { for ( User admin : admins ) { LOGGER . info ( admin . getEmail ( ) + " is admin + (" + admin . getId ( ) + ")" ) ; } }
public void test() { try { adminUser = new User ( ) ; adminUser . setPerson ( ImejiFactory . newPerson ( "Admin" , "imeji" , "imeji community" ) ) ; adminUser . setEmail ( ADMIN_EMAIL_INIT ) ; adminUser . setEncryptedPassword ( StringHelper . convertToMD5 ( ADMIN_PASSWORD_INIT ) ) ; adminUser . getGrants ( ) . addAll ( AuthorizationPredefinedRoles . imejiAdministrator ( adminUser . getId ( ) . toString ( ) ) ) ; adminUser . setApiKey ( APIKeyAuthentication . generateKey ( adminUser . getId ( ) , Integer . MAX_VALUE ) ) ; UserController uc = new UserController ( Imeji . adminUser ) ; List < User > admins = uc . retrieveAllAdmins ( ) ; code_block = IfStatement ; } catch ( AlreadyExistsException e ) { LOGGER . warn ( Imeji . adminUser . getEmail ( ) + " already exists" , e ) ; } catch ( Exception e ) { code_block = IfStatement ; } }
public void test() { if ( e . getCause ( ) instanceof AlreadyExistsException ) { LOGGER . warn ( Imeji . adminUser . getEmail ( ) + " already exists" ) ; } else { throw new RuntimeException ( "Error initializing Admin user! " , e ) ; } }
@ PayloadRoot ( localPart = "FindDevicesWhichHaveNoOwnerRequest" , namespace = DEVICE_MANAGEMENT_NAMESPACE ) @ ResponsePayload public FindDevicesWhichHaveNoOwnerResponse findDevicesWhichHaveNoOwner ( @ OrganisationIdentification final String organisationIdentification , @ RequestPayload final FindDevicesWhichHaveNoOwnerRequest request ) throws OsgpException { LOGGER . info ( "Finding devices which have no owner for organisation: {}." , organisationIdentification ) ; final FindDevicesWhichHaveNoOwnerResponse response = new FindDevicesWhichHaveNoOwnerResponse ( ) ; code_block = TryStatement ;  return response ; }
public void test() { if ( keyFactory != null ) { KeyManager [ ] keyManager = keyFactory . getKeyManagers ( ) ; code_block = IfStatement ; } else { LOG . debug ( "Key Factory is null" ) ; } }
public void test() { try { KeyManagerFactory keyFactory = getInstance ( ) . keyFactory ; code_block = IfStatement ; } catch ( IllegalStateException e ) { LOG . error ( e . getLocalizedMessage ( ) , e ) ; } }
public void test() { try { monitorHeartbeats ( ) ; } catch ( final Exception e ) { clusterCoordinator . reportEvent ( null , Severity . ERROR , "Failed to process heartbeats from nodes due to " + e . toString ( ) ) ; logger . error ( "Failed to process heartbeats" , e ) ; } }
public void test() { try { Assert . assertEquals ( 100L , liveServer . getServer ( ) . getNetworkHealthCheck ( ) . getPeriod ( ) ) ; liveServer . getServer ( ) . getNetworkHealthCheck ( ) . setTimeUnit ( TimeUnit . MILLISECONDS ) ; Assert . assertFalse ( liveServer . getServer ( ) . getNetworkHealthCheck ( ) . check ( ) ) ; Wait . assertFalse ( liveServer :: isStarted ) ; liveServer . getServer ( ) . getNetworkHealthCheck ( ) . setIgnoreLoopback ( true ) . addAddress ( "127.0.0.1" ) ; Wait . assertTrue ( liveServer :: isStarted ) ; Assert . assertTrue ( component . isStarted ( ) ) ; } catch ( Throwable e ) { logger . warn ( e . getMessage ( ) , e ) ; throw e ; } finally { liveServer . getServer ( ) . stop ( ) ; backupServer . getServer ( ) . stop ( ) ; } }
public void test() { try { this . add ( new HostParser ( protocols , protocol ) . get ( url ) ) ; } catch ( HostParserException e ) { log . warn ( e ) ; } }
public void test() { if ( matches ) { String columnName = matcher . group ( 1 ) ; String value = matcher . group ( 2 ) ; String entityTypeId = tryGetEntityTypeName ( tableName ) . orElse ( null ) ; String attributeName = tryGetAttributeName ( tableName , columnName ) . orElse ( null ) ; return new DuplicateValueException ( entityTypeId , attributeName , value , sourceThrowable ) ; } else { LOG . error ( ERROR_TRANSLATING_POSTGRES_EXC_MSG , pSqlException ) ; throw new RuntimeException ( ERROR_TRANSLATING_EXCEPTION_MSG , pSqlException ) ; } }
public void test() { if ( StringUtils . isEmpty ( registration . getServiceId ( ) ) ) { log . warn ( "No service to register for nacos client..." ) ; return ; } }
public void test() { try { namingService . registerInstance ( serviceId , group , instance ) ; log . info ( "nacos registry, {} {} {}:{} register finished" , group , serviceId , instance . getIp ( ) , instance . getPort ( ) ) ; } catch ( Exception e ) { log . error ( "nacos registry, {} register failed...{}," , serviceId , registration . toString ( ) , e ) ; rethrowRuntimeException ( e ) ; } }
public void test() { try { count = c . readAndProcess ( ) ; } catch ( InterruptedException ieo ) { LOG . info ( getName ( ) + ": readAndProcess caught InterruptedException" , ieo ) ; throw ieo ; } catch ( Exception e ) { LOG . info ( getName ( ) + ": readAndProcess threw exception " + e + ". Count of bytes read: " + count , e ) ; count = - 1 ; } }
public void test() { try { itNetworkPage . setMessage ( Messages . ItNetworkConverterWizard_PageTitle , DialogPage . INFORMATION ) ; runConvertingInWizard ( ) ; CnAElementFactory . getInstance ( ) . reloadBpModelFromDatabase ( ) ; } catch ( InvocationTargetException | InterruptedException e ) { log . error ( "InvocationTargetException or InterruptedException during conversion" , e ) ; itNetworkPage . setMessage ( Messages . ItNetworkConverterWizard_ErrorInformation , DialogPage . ERROR ) ; return false ; } }
public void test() { try { Class . forName ( clazz ) ; providers . put ( name , new DriverInfo ( clazz , config ) ) ; } catch ( ClassNotFoundException e ) { logger . error ( "Failed to instantiate asr driver (className = " + clazz + ")" , e ) ; throw new IllegalArgumentException ( "Cannot register ASR driver " + clazz , e ) ; } }
public void test() { if ( enabled ) { LOGGER . debug ( "Enabling " + instance . target ( ) + " since the profile value does not match the active profile." ) ; } else { LOGGER . debug ( "Disabling " + instance . target ( ) + " since the profile value matches the active profile." ) ; } }
public void test() { if ( enabled ) { LOGGER . debug ( "Enabling " + instance . target ( ) + " since the profile value does not match the active profile." ) ; } else { LOGGER . debug ( "Disabling " + instance . target ( ) + " since the profile value matches the active profile." ) ; } }
public void onError ( final Throwable t ) { LOG . error ( "TestReadListener error" , t ) ; }
public void test() { try { listener . createdSavedSearch ( savedSearch ) ; } catch ( Exception e ) { logger . warn ( "Exception at createSavedSearch" , e ) ; } }
public void testTopic ( String prod_broker_url , String cons_broker_url ) throws Exception { int num_msg ; Connection conn ; Session sess ; String topic_name ; Destination cons_dest ; num_msg = 5 ; LOG . info ( "TESTING TOPICS " + prod_broker_url + " -> " + cons_broker_url + " (" + num_msg + " messages)" ) ; conn = createConnection ( cons_broker_url ) ; conn . start ( ) ; sess = conn . createSession ( false , Session . AUTO_ACKNOWLEDGE ) ; topic_name = "topotest2.perm.topic" ; LOG . trace ( "Removing existing Topic" ) ; removeTopic ( conn , topic_name ) ; LOG . trace ( "Creating Topic, " + topic_name ) ; cons_dest = sess . createTopic ( topic_name ) ; testOneDest ( conn , sess , cons_dest , num_msg ) ; removeTopic ( conn , topic_name ) ; sess . close ( ) ; conn . close ( ) ; }
public void testTopic ( String prod_broker_url , String cons_broker_url ) throws Exception { int num_msg ; Connection conn ; Session sess ; String topic_name ; Destination cons_dest ; num_msg = 5 ; LOG . info ( "TESTING TOPICS " + prod_broker_url + " -> " + cons_broker_url + " (" + num_msg + " messages)" ) ; conn = createConnection ( cons_broker_url ) ; conn . start ( ) ; sess = conn . createSession ( false , Session . AUTO_ACKNOWLEDGE ) ; topic_name = "topotest2.perm.topic" ; LOG . trace ( "Removing existing Topic" ) ; removeTopic ( conn , topic_name ) ; LOG . trace ( "Creating Topic, " + topic_name ) ; cons_dest = sess . createTopic ( topic_name ) ; testOneDest ( conn , sess , cons_dest , num_msg ) ; removeTopic ( conn , topic_name ) ; sess . close ( ) ; conn . close ( ) ; }
public void testTopic ( String prod_broker_url , String cons_broker_url ) throws Exception { int num_msg ; Connection conn ; Session sess ; String topic_name ; Destination cons_dest ; num_msg = 5 ; LOG . info ( "TESTING TOPICS " + prod_broker_url + " -> " + cons_broker_url + " (" + num_msg + " messages)" ) ; conn = createConnection ( cons_broker_url ) ; conn . start ( ) ; sess = conn . createSession ( false , Session . AUTO_ACKNOWLEDGE ) ; topic_name = "topotest2.perm.topic" ; LOG . trace ( "Removing existing Topic" ) ; removeTopic ( conn , topic_name ) ; LOG . trace ( "Creating Topic, " + topic_name ) ; cons_dest = sess . createTopic ( topic_name ) ; testOneDest ( conn , sess , cons_dest , num_msg ) ; removeTopic ( conn , topic_name ) ; sess . close ( ) ; conn . close ( ) ; }
public void test() { if ( states . peek ( ) != State . INITIAL ) { log . error ( "Unexpected end state: {}" , states . peek ( ) ) ; } }
private void addWithGroovyClassLoader ( int x , int y ) throws IllegalAccessException , InstantiationException , IOException { Class calcClass = loader . parseClass ( new File ( "src/main/groovy/com/baeldung/" , "CalcMath.groovy" ) ) ; GroovyObject calc = ( GroovyObject ) calcClass . newInstance ( ) ; Object result = calc . invokeMethod ( "calcSum" , new Object [ ] code_block = "" ; ) ; LOG . info ( "Result of CalcMath.calcSum() method is {}" , result ) ; }
public void test() { try { Map < String , Object > returnMap = new HashMap < > ( ) ; Page < OperationLog > page = PageHelper . startPage ( pageNo , pageSize ) ; operationLogDao . selectLogsByOperationGroupId ( groupId ) ; returnMap . put ( "logData" , page ) ; returnMap . put ( "totalCount" , page . getTotal ( ) ) ; returnMap . put ( "totalPage" , page . getPages ( ) ) ; return returnMap ; } catch ( Exception e ) { logger . error ( "query logs from db error" , e ) ; return null ; } }
public void persist ( RechteRolleBericht transientInstance ) { log . debug ( "persisting RechteRolleBericht instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . persist ( transientInstance ) ; log . debug ( "persist successful" ) ; } catch ( RuntimeException re ) { log . error ( "persist failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . persist ( transientInstance ) ; log . debug ( "persist successful" ) ; } catch ( RuntimeException re ) { log . error ( "persist failed" , re ) ; throw re ; } }
public RuleResult execute ( final Map < String , String > ruleParam , Map < String , String > resourceAttributes ) { logger . debug ( "========SSLCertificateExpiryRule started=========" ) ; Annotation annotation = null ; String validTo = null ; long expiredDuration ; long targetExpiredDuration ; String targetExpiryDurationInString = ruleParam . get ( PacmanRuleConstants . EXPIRED_DURATION ) ; String severity = ruleParam . get ( PacmanRuleConstants . SEVERITY ) ; String category = ruleParam . get ( PacmanRuleConstants . CATEGORY ) ; MDC . put ( "executionId" , ruleParam . get ( "executionId" ) ) ; MDC . put ( "ruleId" , ruleParam . get ( PacmanSdkConstants . RULE_ID ) ) ; List < LinkedHashMap < String , Object > > issueList = new ArrayList < > ( ) ; LinkedHashMap < String , Object > issue = new LinkedHashMap < > ( ) ; code_block = IfStatement ; code_block = IfStatement ; logger . debug ( "========SSLCertificateExpiryRule ended=========" ) ; return new RuleResult ( PacmanSdkConstants . STATUS_SUCCESS , PacmanRuleConstants . SUCCESS_MESSAGE ) ; }
public void test() { if ( ! PacmanUtils . doesAllHaveValue ( targetExpiryDurationInString , severity , category ) ) { logger . info ( PacmanRuleConstants . MISSING_CONFIGURATION ) ; throw new InvalidInputException ( PacmanRuleConstants . MISSING_CONFIGURATION ) ; } }
public void test() { if ( expiredDuration <= targetExpiredDuration ) { annotation = Annotation . buildAnnotation ( ruleParam , Annotation . Type . ISSUE ) ; annotation . put ( PacmanSdkConstants . DESCRIPTION , "SSL Expiry within " + targetExpiryDurationInString + " days found!!" ) ; annotation . put ( PacmanRuleConstants . SEVERITY , severity ) ; annotation . put ( PacmanRuleConstants . CATEGORY , category ) ; issue . put ( PacmanRuleConstants . VIOLATION_REASON , "SSL Expiry within " + targetExpiryDurationInString + " days found!!" ) ; issueList . add ( issue ) ; annotation . put ( "issueDetails" , issueList . toString ( ) ) ; logger . debug ( "========SSLCertificateExpiryRule ended with annotation {} : =========" , annotation ) ; return new RuleResult ( PacmanSdkConstants . STATUS_FAILURE , PacmanRuleConstants . FAILURE_MESSAGE , annotation ) ; } }
public void test() { if ( expiredDuration > 0 ) { code_block = IfStatement ; } else { logger . info ( "Elb with SSL validity not expired" ) ; } }
public RuleResult execute ( final Map < String , String > ruleParam , Map < String , String > resourceAttributes ) { logger . debug ( "========SSLCertificateExpiryRule started=========" ) ; Annotation annotation = null ; String validTo = null ; long expiredDuration ; long targetExpiredDuration ; String targetExpiryDurationInString = ruleParam . get ( PacmanRuleConstants . EXPIRED_DURATION ) ; String severity = ruleParam . get ( PacmanRuleConstants . SEVERITY ) ; String category = ruleParam . get ( PacmanRuleConstants . CATEGORY ) ; MDC . put ( "executionId" , ruleParam . get ( "executionId" ) ) ; MDC . put ( "ruleId" , ruleParam . get ( PacmanSdkConstants . RULE_ID ) ) ; List < LinkedHashMap < String , Object > > issueList = new ArrayList < > ( ) ; LinkedHashMap < String , Object > issue = new LinkedHashMap < > ( ) ; code_block = IfStatement ; code_block = IfStatement ; logger . debug ( "========SSLCertificateExpiryRule ended=========" ) ; return new RuleResult ( PacmanSdkConstants . STATUS_SUCCESS , PacmanRuleConstants . SUCCESS_MESSAGE ) ; }
private void moveConfirmationConfiguration ( ) { Map < String , EmailConfirmationConfiguration > attrsConfig = new HashMap < > ( ) ; Map < String , EmailConfirmationConfiguration > idsConfig = new HashMap < > ( ) ; List < GenericObjectBean > conConfs = genericObjectsDAO . getObjectsOfType ( "confirmationConfiguration" ) ; code_block = ForStatement ; updateAttributeTypes ( attrsConfig ) ; updateIdentityTypes ( idsConfig ) ; log . info ( "Removing all confirmationConfiguration objects" ) ; genericObjectsDAO . removeObjectsByType ( "confirmationConfiguration" ) ; }
public void test() { if ( maxIdleTime <= timeoutPeriod ) { logger . debug ( "Not timed out: " + maxIdleTime + " " + timeoutPeriod ) ; bRet = false ; } else { logger . debug ( "Timed out: " + maxIdleTime + " " + timeoutPeriod ) ; } }
public void test() { if ( maxIdleTime <= timeoutPeriod ) { logger . debug ( "Not timed out: " + maxIdleTime + " " + timeoutPeriod ) ; bRet = false ; } else { logger . debug ( "Timed out: " + maxIdleTime + " " + timeoutPeriod ) ; } }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "Resolved function from provided [routing-expression]  " + routingExpression ) ; } }
public void test() { try { session = hibernateTemplate . getSessionFactory ( ) . openSession ( ) ; comprehensionTestQuestionBo = ( ComprehensionTestQuestionBo ) session . get ( ComprehensionTestQuestionBo . class , questionId ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "StudyDAOImpl - getComprehensionTestQuestionById() - Error" , e ) ; } finally { code_block = IfStatement ; } }
public void test() { if ( toRecipients != null && toRecipients . size ( ) > 0 ) { logger . debug ( "sending email to-->" ) ; toRecipients . stream ( ) . forEach ( logger :: debug ) ; Map < String , Object > mailDetails = Maps . newHashMap ( ) ; mailDetails . put ( "attachmentUrl" , "" ) ; mailDetails . put ( "from" , CommonUtils . getPropValue ( PacmanSdkConstants . SEND_EMAIL_FROM ) ) ; mailDetails . put ( "mailBodyAsString" , formateCommonFixBody ( silentautoFixTrans , ruleParam , resourceOwner ) ) ; mailDetails . put ( "placeholderValues" , Maps . newHashMap ( ) ) ; mailDetails . put ( "subject" , emailSubject ) ; mailDetails . put ( "to" , toRecipients ) ; CommonUtils . doHttpPost ( CommonUtils . getPropValue ( PacmanSdkConstants . EMAIL_SERVICE_URL ) , gson . toJson ( mailDetails ) , new HashMap < > ( ) ) ; } }
public void test() { try { List < String > toRecipients = Lists . newArrayList ( ) ; String emailCCList = CommonUtils . getPropValue ( PacmanSdkConstants . SEND_EMAIL_CC_KEY ) ; toRecipients . addAll ( Arrays . asList ( emailCCList . split ( "\\s*,\\s*" ) ) ) ; String emailSubject = CommonUtils . getPropValue ( PacmanSdkConstants . SEND_EMAIL_FIX_SUBJECT_PREFIX + ruleParam . get ( PacmanSdkConstants . RULE_ID ) ) ; Gson gson = new GsonBuilder ( ) . disableHtmlEscaping ( ) . create ( ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "error sending email" , e ) ; } }
public void test() { if ( host . getType ( ) . equalsIgnoreCase ( Host . HostType . Esx . name ( ) ) ) { _log . info ( "Host " + host . getLabel ( ) + " is compatible for vCenter cluster operation due to type " + host . getType ( ) + " and OS version " + host . getOsVersion ( ) ) ; return true ; } else { _log . info ( "Host " + host . getLabel ( ) + " is not compatible for vCenter cluster operation due to type " + host . getType ( ) ) ; return false ; } }
public void test() { if ( host . getType ( ) . equalsIgnoreCase ( Host . HostType . Esx . name ( ) ) ) { _log . info ( "Host " + host . getLabel ( ) + " is compatible for vCenter cluster operation due to type " + host . getType ( ) + " and OS version " + host . getOsVersion ( ) ) ; return true ; } else { _log . info ( "Host " + host . getLabel ( ) + " is not compatible for vCenter cluster operation due to type " + host . getType ( ) ) ; return false ; } }
public void test() { if ( currentTerm != term ) { logger . info ( "Partition group {}/node {} receive vote response from {}, " + "current term is {}, term is {}" , topicPartitionGroup , localNode , node . getNodeId ( ) , currentTerm , term ) ; return ; } }
public void test() { try { code_block = IfStatement ; } catch ( Throwable t ) { logger . warn ( "Partition group {}/node {} handle vote response fail" , topicPartitionGroup , localNode ) ; } }
protected void onNodeDeletePost ( final String networkId , final Node node , final HashMap < String , Response > respList ) { log . debug ( "" ) ; }
public void test() { try { code_block = ForStatement ; checkRequestHasContentLengthOrChunkedEncoding ( request , "After filtering, the request has neither chunked encoding nor content length: " + request ) ; logger . debug ( "Sending request %s: %s" , request . hashCode ( ) , request . getRequestLine ( ) ) ; wirePayloadIfEnabled ( wire , request ) ; utils . logRequest ( headerLog , request , ">>" ) ; nativeRequest = convert ( request ) ; response = invoke ( nativeRequest ) ; logger . debug ( "Receiving response %s: %s" , request . hashCode ( ) , response . getStatusLine ( ) ) ; utils . logResponse ( headerLog , response , "<<" ) ; if ( response . getPayload ( ) != null && wire . enabled ( ) ) wire . input ( response ) ; nativeRequest = null ; int statusCode = response . getStatusCode ( ) ; code_block = IfStatement ; } catch ( Exception e ) { IOException ioe = getFirstThrowableOfType ( e , IOException . class ) ; code_block = IfStatement ; command . setException ( new HttpResponseException ( e . getMessage ( ) + " connecting to " + command . getCurrentRequest ( ) . getRequestLine ( ) , command , null , e ) ) ; break ; } finally { cleanup ( nativeRequest ) ; } }
public void test() { try { code_block = ForStatement ; checkRequestHasContentLengthOrChunkedEncoding ( request , "After filtering, the request has neither chunked encoding nor content length: " + request ) ; logger . debug ( "Sending request %s: %s" , request . hashCode ( ) , request . getRequestLine ( ) ) ; wirePayloadIfEnabled ( wire , request ) ; utils . logRequest ( headerLog , request , ">>" ) ; nativeRequest = convert ( request ) ; response = invoke ( nativeRequest ) ; logger . debug ( "Receiving response %s: %s" , request . hashCode ( ) , response . getStatusLine ( ) ) ; utils . logResponse ( headerLog , response , "<<" ) ; if ( response . getPayload ( ) != null && wire . enabled ( ) ) wire . input ( response ) ; nativeRequest = null ; int statusCode = response . getStatusCode ( ) ; code_block = IfStatement ; } catch ( Exception e ) { IOException ioe = getFirstThrowableOfType ( e , IOException . class ) ; code_block = IfStatement ; command . setException ( new HttpResponseException ( e . getMessage ( ) + " connecting to " + command . getCurrentRequest ( ) . getRequestLine ( ) , command , null , e ) ) ; break ; } finally { cleanup ( nativeRequest ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( _NO_SUCH_ENTITY_WITH_PRIMARY_KEY + primaryKey ) ; } }
@ Override public STypeIR caseAUnresolvedType ( AUnresolvedType node , IRInfo question ) throws AnalysisException { log . error ( "Found unresolved type in the VDM AST" ) ; return new AUnknownTypeIR ( ) ; }
public void test() { try ( ZipFile zip = new ZipFile ( ftpsFileDownloader . getLocalFilePath ( ) ) ) { processAltNamesFile ( zip , altNamesMap , dnNameMap ) ; processIdentifiersFile ( zip , identifiersMap ) ; processDeletedElementsFile ( zip , deletedElementsMap ) ; processInstitutions ( zip , altNamesMap , identifiersMap , dnNameMap ) ; processDeletedElements ( deletedElementsMap ) ; return true ; } catch ( Exception e ) { LOGGER . error ( "Error importing RINGGOLD data" , e ) ; return false ; } finally { LOGGER . warn ( "Ringgold import completed" ) ; } }
public void test() { try ( ZipFile zip = new ZipFile ( ftpsFileDownloader . getLocalFilePath ( ) ) ) { processAltNamesFile ( zip , altNamesMap , dnNameMap ) ; processIdentifiersFile ( zip , identifiersMap ) ; processDeletedElementsFile ( zip , deletedElementsMap ) ; processInstitutions ( zip , altNamesMap , identifiersMap , dnNameMap ) ; processDeletedElements ( deletedElementsMap ) ; return true ; } catch ( Exception e ) { LOGGER . error ( "Error importing RINGGOLD data" , e ) ; return false ; } finally { LOGGER . warn ( "Ringgold import completed" ) ; } }
public void test() { try { String url = this . getAvatarManager ( ) . getAvatarUrl ( this . getUsername ( ) ) ; code_block = IfStatement ; MimetypesFileTypeMap mimeTypesMap = new MimetypesFileTypeMap ( ) ; this . setMimeType ( mimeTypesMap . getContentType ( url ) ) ; File avatar = this . getAvatarManager ( ) . getAvatarResource ( this . getUsername ( ) ) ; code_block = IfStatement ; this . setInputStream ( new FileInputStream ( avatar ) ) ; } catch ( Throwable t ) { _logger . info ( "local avatar not available" , t ) ; return this . extractDefaultAvatarStream ( ) ; } }
public void test() { if ( lastActionTime == null ) { LOG . info ( "Nothing is materialized for this coord: {}" , coord . getId ( ) ) ; code_block = IfStatement ; } else { LOG . info ( "Actions have materialized for this coord: {}, last action {}" , coord . getId ( ) , SchemaHelper . formatDateUTC ( lastActionTime ) ) ; code_block = IfStatement ; change ( cluster , coord . getId ( ) , concurrency , endTime , null ) ; } }
public void test() { if ( endTime . compareTo ( coord . getStartTime ( ) ) <= 0 ) { LOG . info ( "Setting end time to START TIME {}" , SchemaHelper . formatDateUTC ( coord . getStartTime ( ) ) ) ; change ( cluster , coord . getId ( ) , concurrency , coord . getStartTime ( ) , null ) ; } else { LOG . info ( "Setting end time to START TIME {}" , SchemaHelper . formatDateUTC ( endTime ) ) ; change ( cluster , coord . getId ( ) , concurrency , endTime , null ) ; } }
public void test() { if ( endTime . compareTo ( coord . getStartTime ( ) ) <= 0 ) { LOG . info ( "Setting end time to START TIME {}" , SchemaHelper . formatDateUTC ( coord . getStartTime ( ) ) ) ; change ( cluster , coord . getId ( ) , concurrency , coord . getStartTime ( ) , null ) ; } else { LOG . info ( "Setting end time to START TIME {}" , SchemaHelper . formatDateUTC ( endTime ) ) ; change ( cluster , coord . getId ( ) , concurrency , endTime , null ) ; } }
public void test() { if ( ! endTime . after ( lastActionTime ) ) { Date pauseTime = DateUtil . offsetTime ( endTime , - 1 * 60 ) ; LOG . info ( "Setting pause time on coord: {} to {}" , coord . getId ( ) , SchemaHelper . formatDateUTC ( pauseTime ) ) ; change ( cluster , coord . getId ( ) , concurrency , null , SchemaHelper . formatDateUTC ( pauseTime ) ) ; } }
public void test() { try { logger . debug ( "Entered enforceCompareDatastreamChecksum" ) ; String target = Constants . ACTION . COMPARE_DATASTREAM_CHECKSUM . uri ; context . setActionAttributes ( null ) ; MultiValueMap < URI > resourceAttributes = new MultiValueMap < URI > ( ) ; URI name = null ; code_block = TryStatement ;  context . setResourceAttributes ( resourceAttributes ) ; xacmlPep . enforce ( context . getSubjectValue ( Constants . SUBJECT . LOGIN_ID . uri ) , target , Constants . ACTION . APIM . uri , pid , extractNamespace ( pid ) , context ) ; } finally { logger . debug ( "Exiting enforceCompareDatastreamChecksum" ) ; } }
public void test() { try { logger . debug ( "Entered enforceCompareDatastreamChecksum" ) ; String target = Constants . ACTION . COMPARE_DATASTREAM_CHECKSUM . uri ; context . setActionAttributes ( null ) ; MultiValueMap < URI > resourceAttributes = new MultiValueMap < URI > ( ) ; URI name = null ; code_block = TryStatement ;  context . setResourceAttributes ( resourceAttributes ) ; xacmlPep . enforce ( context . getSubjectValue ( Constants . SUBJECT . LOGIN_ID . uri ) , target , Constants . ACTION . APIM . uri , pid , extractNamespace ( pid ) , context ) ; } finally { logger . debug ( "Exiting enforceCompareDatastreamChecksum" ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( this . getClass ( ) . getName ( ) + "/handleRequest!" ) ; } }
public void test() { try { resAttr = ResourceAttributes . getResources ( parts ) ; code_block = IfStatement ; actions . put ( Constants . ACTION . ID . getURI ( ) , Constants . ACTION . GET_DATASTREAM . getStringAttribute ( ) ) ; actions . put ( Constants . ACTION . ID . getURI ( ) , Constants . ACTION . GET_DATASTREAM_DISSEMINATION . getStringAttribute ( ) ) ; actions . put ( Constants . ACTION . API . getURI ( ) , Constants . ACTION . APIA . getStringAttribute ( ) ) ; req = getContextHandler ( ) . buildRequest ( getSubjects ( request ) , actions , resAttr , getEnvironment ( request ) ) ; LogUtil . statLog ( request . getRemoteUser ( ) , Constants . ACTION . GET_DATASTREAM_DISSEMINATION . uri , parts [ 1 ] , parts [ 3 ] ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; throw new ServletException ( e . getMessage ( ) , e ) ; } }
@ Override public void delete ( String bucket ) { s3Client . deleteBucket ( bucket ) ; log . info ( "Request to delete " + bucket + " sent" ) ; s3Client . waiters ( ) . bucketNotExists ( ) . run ( new WaiterParameters ( new HeadBucketRequest ( bucket ) ) ) ; log . info ( "Bucket " + bucket + " is deleted" ) ; }
@ Override public void delete ( String bucket ) { s3Client . deleteBucket ( bucket ) ; log . info ( "Request to delete " + bucket + " sent" ) ; s3Client . waiters ( ) . bucketNotExists ( ) . run ( new WaiterParameters ( new HeadBucketRequest ( bucket ) ) ) ; log . info ( "Bucket " + bucket + " is deleted" ) ; }
private void validateFailInConfigWithExplicitTotalFlinkAndManagedMem ( final Configuration customConfig ) { log . info ( "Validating failing in configuration with explicit total flink and managed memory size." ) ; final Configuration config = configWithExplicitTotalFlinkAndManagedMem ( ) ; config . addAll ( customConfig ) ; validateFail ( config ) ; }
public void test() { try { Thread . sleep ( 100 ) ; } catch ( InterruptedException e ) { LOG . error ( "Exception in WebDriverManager while getWebDriver " , e ) ; } }
public void test() { try { tagXmlExecutor . awaitTermination ( 120 , TimeUnit . SECONDS ) ; code_block = WhileStatement ; } catch ( InterruptedException | ExecutionException e ) { log . error ( "Interrupted while waiting for XML tag threads to terminate - no datatags were added!" ) ; } }
public void test() { if ( doc != null ) { setMetaId ( doc . nextMetaId ( ) ) ; logger . debug ( format ( "Some annotations would get lost because there was no metaid defined on {0}. To avoid this, an automatic metaid ''{0}'' has been generated." , getElementName ( ) , getMetaId ( ) ) ) ; getAnnotation ( ) . setAbout ( '' + getMetaId ( ) ) ; } else { logger . warn ( format ( "Some annotations can get lost because no metaid is defined on {0}." , getElementName ( ) ) ) ; } }
public void test() { if ( isSetId ( ) ) { code_block = IfStatement ; } }
public void test() { if ( getRequest ( ) . isVerbose ( ) ) { this . logger . info ( "{} events were saved in the new store because they did not already exist" , eventsToSave . size ( ) ) ; } }
private int logRunningTime ( String operationName , SupplierWithException < Integer , IOException > supplier ) throws IOException { long startTimeMillis = System . currentTimeMillis ( ) ; int result = supplier . get ( ) ; LOG . info ( "Hive source({}}) {} use time: {} ms, result: {}" , tablePath , operationName , System . currentTimeMillis ( ) - startTimeMillis , result ) ; return result ; }
public void test() { try { Map < String , Object > parameters = new HashMap < > ( ) ; Map < String , Object > extraParameters = new HashMap < > ( ) ; parameters . put ( AssessmentReportAndWorkflowConstants . WORKFLOW_ID , workflowId ) ; extraParameters . put ( "MaxResults" , 5 ) ; List < Long > executionIds = executeQueryWithExtraParameter ( "select distinct executionId FROM InsightsWorkflowExecutionHistory EH WHERE EH.workflowConfig.workflowId = :workflowId ORDER BY executionId DESC" , Long . class , parameters , extraParameters ) ; parameters . clear ( ) ; extraParameters . clear ( ) ; extraParameters . put ( "executionIDs" , executionIds ) ; return executeQueryWithExtraParameter ( "FROM InsightsWorkflowExecutionHistory EH WHERE EH.executionId IN (:executionIDs) ORDER BY executionId DESC" , InsightsWorkflowExecutionHistory . class , parameters , extraParameters ) ; } catch ( Exception e ) { log . error ( e ) ; throw e ; } }
public void test() { try { start . await ( ) ; retrieves . addAll ( retrieve ( store ) ) ; done . countDown ( ) ; } catch ( IOException e ) { log . info ( "Exception in retrieve" , e ) ; } catch ( InterruptedException e ) { log . info ( "Interrupted in retrieve" , e ) ; } }
public void test() { try { start . await ( ) ; retrieves . addAll ( retrieve ( store ) ) ; done . countDown ( ) ; } catch ( IOException e ) { log . info ( "Exception in retrieve" , e ) ; } catch ( InterruptedException e ) { log . info ( "Interrupted in retrieve" , e ) ; } }
public void resume ( ) { LOGGER . info ( "Resuming consumer " + getFlowName ( ) + " [" + creationTimestamp + "]" ) ; code_block = IfStatement ; pauseHandler . resume ( ) ; setStatus ( EventConsumerStatus . EXECUTING ) ; }
private void handleRoomScene ( ChannelUID channelUID , OnOffType command ) { logger . debug ( "handleRoomScene called for channel: {}, command: {}" , channelUID , command ) ; int linkNum ; code_block = SwitchStatement ; int roomNum = ( thingID + 7 ) / 8 ; int param2 = ( ( roomNum * 6 ) - 3 ) + linkNum ; sendOmnilinkCommand ( OnOffType . ON . equals ( command ) ? CommandMessage . CMD_UNIT_UPB_LINK_ON : CommandMessage . CMD_UNIT_UPB_LINK_OFF , 0 , param2 ) ; }
public void test() { switch ( channelUID . getId ( ) ) { case "scene_a" : linkNum = 0 ; break ; case "scene_b" : linkNum = 1 ; break ; case "scene_c" : linkNum = 2 ; break ; case "scene_d" : linkNum = 3 ; break ; default : logger . warn ( "Unexpected UPB Room scene: {}" , channelUID ) ; return ; } }
public void test() { if ( group == null ) { logger . debug ( "getGroup (by id) group not found: " + identifier ) ; } else { logger . debug ( "getGroup (by id) found group: {} for id: {}" , group . getName ( ) , identifier ) ; } }
public void log ( UserData o ) { Node n = o . getNode ( ) ; StringBuilder sb = new StringBuilder ( ) ; sb . append ( n . getParent ( ) ) ; sb . append ( " parent->" ) ; sb . append ( "(" ) ; sb . append ( n ) ; sb . append ( ")->" ) ; List < Spatial > children = n . getChildren ( ) ; code_block = ForStatement ; log . info ( sb . toString ( ) ) ; }
public List findByExample ( MbMassPhase instance ) { log . debug ( "finding MbMassPhase instance by example" ) ; code_block = TryStatement ;  }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.MbMassPhase" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.MbMassPhase" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
private void testAttributes ( ) throws Exception { log . info ( "Testing attributes" ) ; Device device = findDeviceByName ( "Edge Device 1" ) ; testAttributesUpdatedMsg ( device ) ; testPostAttributesMsg ( device ) ; testAttributesDeleteMsg ( device ) ; log . info ( "Attributes tested successfully" ) ; }
private void testAttributes ( ) throws Exception { log . info ( "Testing attributes" ) ; Device device = findDeviceByName ( "Edge Device 1" ) ; testAttributesUpdatedMsg ( device ) ; testPostAttributesMsg ( device ) ; testAttributesDeleteMsg ( device ) ; log . info ( "Attributes tested successfully" ) ; }
public void test() { try { tis = TikaInputStream . get ( payload , md ) ; tikaType = pika . getDetector ( ) . detect ( tis , md ) . toString ( ) ; } catch ( Throwable e ) { log . error ( "Tika.detect failed:" + e . getMessage ( ) ) ; return MediaType . OCTET_STREAM . toString ( ) ; } finally { code_block = IfStatement ; } }
public void test() { try { parseThread . start ( ) ; parseThread . join ( this . parseTimeout ) ; parseThread . interrupt ( ) ; } catch ( OutOfMemoryError o ) { log . error ( "TikaExtractor.parse(): " + tikaType + " : " + o . getMessage ( ) ) ; } catch ( RuntimeException r ) { log . error ( "TikaExtractor.parse(): " + tikaType + " : " + r . getMessage ( ) ) ; } finally { code_block = IfStatement ; } }
public void test() { try { parseThread . start ( ) ; parseThread . join ( this . parseTimeout ) ; parseThread . interrupt ( ) ; } catch ( OutOfMemoryError o ) { log . error ( "TikaExtractor.parse(): " + tikaType + " : " + o . getMessage ( ) ) ; } catch ( RuntimeException r ) { log . error ( "TikaExtractor.parse(): " + tikaType + " : " + r . getMessage ( ) ) ; } finally { code_block = IfStatement ; } }
public void test() { try { md . set ( Metadata . CONTENT_TYPE , tikaType . toString ( ) ) ; pika . setRecursive ( ctx , false ) ; ch = new WriteOutContentHandler ( MAX_BUF ) ; InputStream tikainput = TikaInputStream . get ( payload , md ) ; ParseRunner runner = new ParseRunner ( pika , tikainput , ch , md , ctx ) ; Thread parseThread = new Thread ( runner , Long . toString ( System . currentTimeMillis ( ) ) ) ; parseThread . setDaemon ( true ) ; code_block = TryStatement ;  String extMimeType = md . get ( PreservationParser . EXT_MIME_TYPE ) ; if ( runner . complete && extMimeType != null ) tikaType = extMimeType ; } catch ( Throwable e ) { log . debug ( "Tika Exception: " + e . getMessage ( ) ) ; } }
@ Override public void upgrade ( ) { code_block = IfStatement ; final IndexManagementConfig indexManagementConfig = clusterConfigService . get ( IndexManagementConfig . class ) ; checkState ( indexManagementConfig != null , "Couldn't find index management configuration" ) ; final IndexSetConfig config = IndexSetConfig . builder ( ) . title ( "Default index set" ) . description ( "The Graylog default index set" ) . indexPrefix ( elasticsearchConfiguration . getIndexPrefix ( ) ) . shards ( elasticsearchConfiguration . getShards ( ) ) . replicas ( elasticsearchConfiguration . getReplicas ( ) ) . rotationStrategy ( getRotationStrategyConfig ( indexManagementConfig ) ) . retentionStrategy ( getRetentionStrategyConfig ( indexManagementConfig ) ) . creationDate ( ZonedDateTime . now ( ZoneOffset . UTC ) ) . indexAnalyzer ( elasticsearchConfiguration . getAnalyzer ( ) ) . indexTemplateName ( elasticsearchConfiguration . getTemplateName ( ) ) . indexOptimizationMaxNumSegments ( elasticsearchConfiguration . getIndexOptimizationMaxNumSegments ( ) ) . indexOptimizationDisabled ( elasticsearchConfiguration . isDisableIndexOptimization ( ) ) . build ( ) ; final IndexSetConfig savedConfig = indexSetService . save ( config ) ; clusterConfigService . write ( DefaultIndexSetConfig . create ( savedConfig . id ( ) ) ) ; clusterConfigService . write ( DefaultIndexSetCreated . create ( ) ) ; LOG . debug ( "Successfully created default index set: {}" , savedConfig ) ; }
public void test() { try { MainFrame frame = new MainFrame ( configuration ) ; frame . setVisible ( true ) ; Logger . info ( String . format ( "Bienvenue, Bienvenido, Willkommen, Hello, Namaskar, Welkom, Bonjour to OpenPnP version %s." , Main . getVersion ( ) ) ) ; configuration . getScripting ( ) . on ( "Startup" , null ) ; } catch ( Exception e ) { e . printStackTrace ( ) ; } }
public void test() { if ( ! dataList . isEmpty ( ) ) { String healthLabels = ":" + routingKey . replace ( "." , ":" ) ; boolean isRecordUpdate = createHealthNodes ( dbHandler , dataList , healthLabels ) ; log . debug ( "Webhook Health Record update status {} ==== " , isRecordUpdate ) ; code_block = IfStatement ; } else { log . error ( " Data List is empty for webhook health record " ) ; EngineStatusLogger . getInstance ( ) . createEngineStatusNode ( " Data List is empty for webhook health record: " + routingKey , PlatformServiceConstants . FAILURE ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( InsightsCustomException e ) { log . error ( e ) ; } }
public void test() { try { Enumeration < String > aliases = keystore . aliases ( ) ; code_block = WhileStatement ; } catch ( KeyStoreException e ) { LOGGER . warn ( String . format ( GENERIC_INSECURE_DEFAULTS_MSG , keystorePath ) , e ) ; } }
public void configureRequestLog ( ) { LogConfiguration lc = configuration . logging ( ) ; code_block = IfStatement ; File logDir = new File ( lc . getLogNCSADirectory ( ) ) ; code_block = IfStatement ; RequestLogWriter writer = new RequestLogWriter ( ) ; writer . setAppend ( lc . isLogNCSAAppend ( ) ) ; code_block = IfStatement ; writer . setFilenameDateFormat ( lc . getLogNCSAFilenameDateFormat ( ) ) ; writer . setRetainDays ( lc . getLogNCSARetainDays ( ) ) ; writer . setTimeZone ( lc . getLogNCSATimeZone ( ) ) ; CustomRequestLog requestLog = new CustomRequestLog ( writer , lc . isLogNCSAExtended ( ) ? CustomRequestLog . EXTENDED_NCSA_FORMAT : CustomRequestLog . EXTENDED_NCSA_FORMAT ) ; server . setRequestLog ( requestLog ) ; LOG . info ( "NCSARequestlogging is using directory {}" , lc . getLogNCSADirectory ( ) ) ; }
public void test() { try { code_block = IfStatement ; } catch ( JSONException | IOException | XMLStreamException e ) { log . error ( "BPMN Analytics Core - Date Vs Task Instance Count TaskLevelMonitoring error." , e ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Date Vs Task Instance Count Result:" + sortedResult ) ; } }
public void test() { try { m = new JSONObject ( msg . getText ( ) ) ; code_block = SwitchStatement ; } catch ( Exception e ) { log . error ( "Error while processing incoming message" , e ) ; } }
@ Override public void terminateAllResources ( final long duration , final TimeUnit unit ) throws TimeoutException , CloudProvisioningException { logger . info ( "Attempting to terminate all cloud resources (timeout set to " + duration + " " + unit + ")" ) ; final long endTime = System . currentTimeMillis ( ) + unit . toMillis ( duration ) ; code_block = TryStatement ;  }
@ Override public void setWorkingDir ( URL url ) throws IOException { String path = Utils . getWorkingDirectory ( ) . getCanonicalPath ( ) ; code_block = IfStatement ; String title ; code_block = IfStatement ; LOGGER . debug ( "Using album title '" + title + "'" ) ; title = Utils . filesystemSafe ( title ) ; path += title ; path = Utils . getOriginalDirectory ( path ) + File . separator ; this . workingDir = new File ( path ) ; code_block = IfStatement ; LOGGER . debug ( "Set working directory to: " + this . workingDir ) ; }
public synchronized void disconnect ( ) throws IOException { LOG . info ( "Disconnecting sessions" ) ; code_block = ForStatement ; fireServiceInactivated ( ) ; idleChecker . destroy ( ) ; }
public void test() { try { return STRICT_OBJECT_MAPPER . readValue ( jsonString , clazz ) ; } catch ( final Exception ex ) { logger . warn ( "Exception when de-serializing " + clazz + " with " + jsonString , ex ) ; } }
public void test() { if ( HttpBindManager . LOG_HTTPBIND_ENABLED . getValue ( ) ) { Log . info ( "HTTP ERR(" + session . getStreamID ( ) . getID ( ) + "): " + bindingError . getErrorType ( ) . getType ( ) + ", " + bindingError . getCondition ( ) + "." ) ; } }
public void test() { if ( bindingError . getErrorType ( ) == BoshBindingError . Type . terminate ) { Log . debug ( "Closing session due to error: {}. Affected session: {}" , bindingError , session ) ; session . close ( ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "{} onNext:{}" , listener , value ) ; } }
private static void filterObjectNames ( final Set < String > objectNames , final String [ ] includes , final String includePattern , final String [ ] excludes , final String excludePattern , final Log log ) throws MojoExecutionException { log . info ( "Looking for matching Object names..." ) ; final Set < String > includedNames = new HashSet < > ( ) ; code_block = IfStatement ; final Set < String > excludedNames = new HashSet < > ( ) ; code_block = IfStatement ; Pattern incPattern ; code_block = IfStatement ; Pattern excPattern ; code_block = IfStatement ; final Set < String > acceptedNames = new HashSet < > ( ) ; code_block = ForStatement ; objectNames . clear ( ) ; objectNames . addAll ( acceptedNames ) ; log . info ( String . format ( "Found %s matching Objects" , objectNames . size ( ) ) ) ; }
public void test() { if ( newState == BusState . CONNECTED ) { logger . info ( "the connection has resumed." ) ; } }
public void test() { if ( newState == BusState . CONNECTION_INTERRUPTED ) { logger . warn ( "the connection to the server has been interrupted ..." ) ; } }
public void test() { { log . trace ( "Started blockchain Constants" ) ; ResponseBuilder response = ResponseBuilder . startTiming ( ) ; BlockchainConstantsDto dto = serverInfoService . getBlockchainConstants ( ) ; log . trace ( "blockchain Constants result : {}" , dto ) ; return response . bind ( dto ) . build ( ) ; } }
public void test() { { log . trace ( "Started blockchain Constants" ) ; ResponseBuilder response = ResponseBuilder . startTiming ( ) ; BlockchainConstantsDto dto = serverInfoService . getBlockchainConstants ( ) ; log . trace ( "blockchain Constants result : {}" , dto ) ; return response . bind ( dto ) . build ( ) ; } }
public void test() { try { CommerceVirtualOrderItemContentDisplayContext commerceVirtualOrderItemContentDisplayContext = new CommerceVirtualOrderItemContentDisplayContext ( _commerceChannelLocalService , _commerceVirtualOrderItemLocalService , _cpDefinitionHelper , _commerceAccountHelper , _cpDefinitionVirtualSettingService , _cpInstanceHelper , _portal . getHttpServletRequest ( renderRequest ) ) ; renderRequest . setAttribute ( WebKeys . PORTLET_DISPLAY_CONTEXT , commerceVirtualOrderItemContentDisplayContext ) ; } catch ( PortalException portalException ) { _log . error ( portalException , portalException ) ; } }
@ Override public void close ( ) throws Exception { LOG . info ( "Closing SourceCoordinator for source {}." , operatorName ) ; code_block = TryStatement ;  LOG . info ( "Source coordinator for source {} closed." , operatorName ) ; }
public void test() { if ( ! reply . isSuccess ( ) ) { logger . warn ( String . format ( "update vms priority failed on host[%s],because %s" , inv . getUuid ( ) , reply . getError ( ) ) ) ; return ; } }
@ Override public void removeAllElectronContainers ( ) { logger . debug ( "Removing all electron containers" ) ; super . removeAllElectronContainers ( ) ; }
@ Before public void setupManager ( ) throws IOException , RepositoryException { String temp = System . getProperty ( "java.io.tmpdir" ) ; home = new File ( temp , getClass ( ) . getName ( ) ) ; delete ( home ) ; home . deleteOnExit ( ) ; code_block = IfStatement ; InputStream configStream = getClass ( ) . getResourceAsStream ( "repository.xml" ) ; RepositoryConfig config = RepositoryConfig . create ( configStream , home . getAbsolutePath ( ) ) ; repo = RepositoryImpl . create ( config ) ; logger . info ( "Initializer Jackrabbit Repository in: " + home . getAbsolutePath ( ) ) ; Credentials credentials = new SimpleCredentials ( "admin" , "admin" . toCharArray ( ) ) ; Session session = repo . login ( credentials ) ; NamespaceRegistry nr = session . getWorkspace ( ) . getNamespaceRegistry ( ) ; nr . registerNamespace ( "brix" , "http://brix-cms.googlecode.com" ) ; session . save ( ) ; manager = new LocalWorkspaceManager ( repo ) . initialize ( ) ; }
@ Override public PooledDataSourceFactory getDataSourceFactory ( DPCAttributionConfiguration configuration ) { logger . debug ( "Connecting to database {} at {}" , configuration . getDatabase ( ) . getDriverClass ( ) , configuration . getDatabase ( ) . getUrl ( ) ) ; return configuration . getDatabase ( ) ; }
public void test() { try { JSONObject json = this . toJSON ( ) ; code_block = IfStatement ; return null ; } catch ( Exception e ) { logger . error ( "event=cns_topic_delivery_policy_to_string" , e ) ; return null ; } }
public void test() { try { Response response = client . getResponse ( ) ; saveCookies ( exchange , client , cxfRsEndpoint . getCookieHandler ( ) ) ; code_block = IfStatement ; code_block = IfStatement ; LOG . trace ( "Response body = {}" , response ) ; exchange . getOut ( ) . getHeaders ( ) . putAll ( exchange . getIn ( ) . getHeaders ( ) ) ; final CxfRsBinding binding = cxfRsEndpoint . getBinding ( ) ; exchange . getOut ( ) . getHeaders ( ) . putAll ( binding . bindResponseHeadersToCamelHeaders ( response , exchange ) ) ; exchange . getOut ( ) . setBody ( binding . bindResponseToCamelBody ( body , exchange ) ) ; exchange . getOut ( ) . setHeader ( Exchange . HTTP_RESPONSE_CODE , response . getStatus ( ) ) ; } catch ( Exception exception ) { LOG . error ( "Error while processing response" , exception ) ; fail ( exception ) ; } finally { callback . done ( false ) ; } }
public void test() { try { gateway . setAiravataInternalGatewayId ( UUID . randomUUID ( ) . toString ( ) ) ; code_block = IfStatement ; } catch ( Exception ex ) { logger . error ( "Error adding gateway-profile, reason: " + ex . getMessage ( ) , ex ) ; TenantProfileServiceException exception = new TenantProfileServiceException ( ) ; exception . setMessage ( "Error adding gateway-profile, reason: " + ex . getMessage ( ) ) ; throw exception ; } }
@ RestAccessControl ( permission = Permission . MANAGE_PAGES ) @ RequestMapping ( value = "/pages/search" , method = RequestMethod . GET , produces = MediaType . APPLICATION_JSON_VALUE ) public ResponseEntity < PagedRestResponse < PageDto > > getPages ( @ ModelAttribute ( "user" ) UserDetails user , PageSearchRequest searchRequest ) { logger . debug ( "getting page list with request {}" , searchRequest ) ; this . getPageValidator ( ) . validateRestListRequest ( searchRequest , PageDto . class ) ; List < String > groups = this . getAuthorizationService ( ) . getAllowedGroupCodes ( user ) ; PagedMetadata < PageDto > result = this . getPageService ( ) . searchPages ( searchRequest , groups ) ; return new ResponseEntity < > ( new PagedRestResponse < > ( result ) , HttpStatus . OK ) ; }
public void test() { try { final FormBuilder formBuilder = new FormBuilder ( ) ; final CollectionComboBoxModel < String > toolModel = new CollectionComboBoxModel < > ( Arrays . asList ( MAVEN_TOOL , GRADLE_TOOL ) ) ; toolComboBox = new ComboBox < > ( toolModel ) ; formBuilder . addLabeledComponent ( "Tool:" , toolComboBox ) ; groupIdField = new JBTextField ( getCurrentOrDefaultValue ( groupId , "com.example" ) ) ; formBuilder . addLabeledComponent ( "Group:" , groupIdField ) ; artifactIdField = new JBTextField ( getCurrentOrDefaultValue ( artifactId , "azure-function-examples" ) ) ; formBuilder . addLabeledComponent ( "Artifact:" , artifactIdField ) ; versionField = new JBTextField ( getCurrentOrDefaultValue ( version , "1.0.0-SNAPSHOT" ) ) ; formBuilder . addLabeledComponent ( "Version:" , versionField ) ; packageNameField = new JBTextField ( getCurrentOrDefaultValue ( packageName , "org.example.functions" ) ) ; formBuilder . addLabeledComponent ( "Package name:" , packageNameField ) ; panel . add ( ScrollPaneFactory . createScrollPane ( formBuilder . getPanel ( ) , true ) , "North" ) ; } catch ( final RuntimeException e ) { LOGGER . error ( e . getLocalizedMessage ( ) , e ) ; throw e ; } }
public void test() { try { analyzerClass = Class . forName ( className ) ; } catch ( ClassNotFoundException e ) { log . warn ( className + " could not be found" , e ) ; return DEFAULT_ANALYZER ; } }
public void test() { if ( ! Analyzer . class . isAssignableFrom ( analyzerClass ) ) { log . warn ( className + " is not a Lucene Analyzer" ) ; return DEFAULT_ANALYZER ; } else-if ( JackrabbitAnalyzer . class . isAssignableFrom ( analyzerClass ) ) { log . warn ( className + " can not be used as a JackrabbitAnalyzer component" ) ; return DEFAULT_ANALYZER ; } }
public void test() { if ( ! Analyzer . class . isAssignableFrom ( analyzerClass ) ) { log . warn ( className + " is not a Lucene Analyzer" ) ; return DEFAULT_ANALYZER ; } else-if ( JackrabbitAnalyzer . class . isAssignableFrom ( analyzerClass ) ) { log . warn ( className + " can not be used as a JackrabbitAnalyzer component" ) ; return DEFAULT_ANALYZER ; } }
public void test() { try { rootData = jsonData . getJSONObject ( "photos" ) ; } catch ( JSONException innerE ) { LOGGER . error ( "Unable to find photos in response" ) ; break ; } }
public void test() { try { addURLToDownload ( getLargestImageURL ( data . getString ( "id" ) , apiKey ) ) ; } catch ( MalformedURLException e ) { LOGGER . error ( "Flickr MalformedURLException: " + e . getMessage ( ) ) ; } }
@ ParameterizedTest @ MethodSource ( "getJobs" ) public void testJobLifecycle ( JobInfo jobInfo , String operationName ) throws Exception { log . info ( "Testing Job lifecycle for {} of type {}" , jobInfo . getOperation ( ) , jobInfo . getContentType ( ) ) ; jobInfo = createJob ( jobInfo ) ; jobInfo = template ( ) . requestBody ( "direct:getJob" , jobInfo , JobInfo . class ) ; assertSame ( JobStateEnum . OPEN , jobInfo . getState ( ) , "Job should be OPEN" ) ; jobInfo = template ( ) . requestBody ( "direct:closeJob" , jobInfo , JobInfo . class ) ; assertSame ( JobStateEnum . CLOSED , jobInfo . getState ( ) , "Job should be CLOSED" ) ; jobInfo = template ( ) . requestBody ( "direct:abortJob" , jobInfo , JobInfo . class ) ; assertSame ( JobStateEnum . ABORTED , jobInfo . getState ( ) , "Job should be ABORTED" ) ; }
public void test() { if ( connectorOperatorEnabled ) { List < NetworkPolicyIngressRule > rules = new ArrayList < > ( 2 ) ; NetworkPolicyIngressRule restApiRule = new NetworkPolicyIngressRuleBuilder ( ) . addNewPort ( ) . withNewPort ( REST_API_PORT ) . withNewProtocol ( "TCP" ) . endPort ( ) . build ( ) ; List < NetworkPolicyPeer > peers = new ArrayList < > ( 2 ) ; NetworkPolicyPeer connectPeer = new NetworkPolicyPeerBuilder ( ) . withNewPodSelector ( ) . addToMatchLabels ( getSelectorLabels ( ) . toMap ( ) ) . endPodSelector ( ) . build ( ) ; peers . add ( connectPeer ) ; NetworkPolicyPeer clusterOperatorPeer = new NetworkPolicyPeerBuilder ( ) . withNewPodSelector ( ) . addToMatchLabels ( Labels . STRIMZI_KIND_LABEL , "cluster-operator" ) . endPodSelector ( ) . build ( ) ; ModelUtils . setClusterOperatorNetworkPolicyNamespaceSelector ( clusterOperatorPeer , namespace , operatorNamespace , operatorNamespaceLabels ) ; peers . add ( clusterOperatorPeer ) ; restApiRule . setFrom ( peers ) ; rules . add ( restApiRule ) ; code_block = IfStatement ; NetworkPolicy networkPolicy = new NetworkPolicyBuilder ( ) . withNewMetadata ( ) . withName ( name ) . withNamespace ( namespace ) . withLabels ( labels . toMap ( ) ) . withOwnerReferences ( createOwnerReference ( ) ) . endMetadata ( ) . withNewSpec ( ) . withNewPodSelector ( ) . addToMatchLabels ( getSelectorLabels ( ) . toMap ( ) ) . endPodSelector ( ) . withIngress ( rules ) . endSpec ( ) . build ( ) ; log . trace ( "Created network policy {}" , networkPolicy ) ; return networkPolicy ; } else { return null ; } }
public void test() { try { ctx . getClassLoader ( ) . loadClass ( "org.apache.logging.log4j.web.ServletRequestThreadContext" ) ; } catch ( final ClassNotFoundException e ) { log . debug ( "log4j-web not available, skipping MDC setup" ) ; return ; } }
@ Path ( "/list" ) @ GET @ Produces ( "application/json" ) public String getAllDisplays ( ) { _log . info ( "Starting getAllDisplays." ) ; SignCodeData data = null ; code_block = TryStatement ;  List < CCDestinationSignMessage > messages = data . getAllDisplays ( ) ; ModelCounterpartConverter < CCDestinationSignMessage , DestinationSign > tcipToJsonConverter = new SignMessageFromTcip ( ) ; List < DestinationSign > jsonSigns = new ArrayList < DestinationSign > ( ) ; code_block = ForStatement ; DestinationSignsMessage outputMessage = new DestinationSignsMessage ( ) ; outputMessage . setSigns ( jsonSigns ) ; outputMessage . setStatus ( "OK" ) ; String output = null ; code_block = TryStatement ;  _log . info ( "Returning Json from getAllDisplays." ) ; return output ; }
public void test() { try { data = getDataObject ( ) ; } catch ( IOException e ) { _log . error ( "getAllDisplays Failure:" , e ) ; throw new WebApplicationException ( e , Response . Status . INTERNAL_SERVER_ERROR ) ; } }
@ Path ( "/list" ) @ GET @ Produces ( "application/json" ) public String getAllDisplays ( ) { _log . info ( "Starting getAllDisplays." ) ; SignCodeData data = null ; code_block = TryStatement ;  List < CCDestinationSignMessage > messages = data . getAllDisplays ( ) ; ModelCounterpartConverter < CCDestinationSignMessage , DestinationSign > tcipToJsonConverter = new SignMessageFromTcip ( ) ; List < DestinationSign > jsonSigns = new ArrayList < DestinationSign > ( ) ; code_block = ForStatement ; DestinationSignsMessage outputMessage = new DestinationSignsMessage ( ) ; outputMessage . setSigns ( jsonSigns ) ; outputMessage . setStatus ( "OK" ) ; String output = null ; code_block = TryStatement ;  _log . info ( "Returning Json from getAllDisplays." ) ; return output ; }
@ PUT @ Path ( "/{id}" ) public Response updateDocument ( @ PathParam ( "id" ) String id , @ Context HttpHeaders headers , InputStream message ) { LOGGER . info ( "id={}" , id ) ; return Response . ok ( ) . build ( ) ; }
public void test() { try { String aS = ( String ) a . get ( "name" ) ; String bS = ( String ) b . get ( "name" ) ; return aS . compareToIgnoreCase ( bS ) ; } catch ( JSONException ex ) { LOG . error ( "JSON Error Exception" , ex ) ; return 1 ; } }
public void test() { try { FileOutputStream fileOut = new FileOutputStream ( path ) ; ObjectOutputStream out = new ObjectOutputStream ( fileOut ) ; out . writeObject ( obj ) ; out . close ( ) ; fileOut . close ( ) ; } catch ( Exception ex ) { LOGGER . warn ( "Failed to save HexviewerPlus settings file. Error:{}" , ex . toString ( ) ) ; return false ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( "Tracing: finish server span={}" , span ) ; } }
public void test() { if ( span != null ) { code_block = IfStatement ; SpanDecorator sd = getSpanDecorator ( route . getEndpoint ( ) ) ; sd . post ( span , exchange , route . getEndpoint ( ) ) ; finishSpan ( span ) ; ActiveSpanManager . deactivate ( exchange ) ; } else { LOG . warn ( "Tracing: could not find managed span for exchange={}" , exchange ) ; } }
public void test() { try { code_block = IfStatement ; SpanAdapter span = ActiveSpanManager . getSpan ( exchange ) ; code_block = IfStatement ; } catch ( Exception t ) { LOG . warn ( "Tracing: Failed to capture tracing data" , t ) ; } }
public void test() { if ( newDiskSpace < vdDataDisk . getCapacityInKB ( ) ) { logger . error ( "Cannot reduce size of data disk " + vdDataDisk . getDeviceInfo ( ) . getLabel ( ) ) ; logger . error ( "Current disk space: " + vdDataDisk . getCapacityInKB ( ) + " new disk space: " + newDiskSpace ) ; throw new Exception ( Messages . getAll ( "error_invalid_diskspacereduction" ) . get ( 0 ) . getText ( ) ) ; } else-if ( newDiskSpace > vdDataDisk . getCapacityInKB ( ) ) { vdDataDisk . setCapacityInKB ( newDiskSpace ) ; VirtualDeviceConfigSpec vmDeviceSpec = new VirtualDeviceConfigSpec ( ) ; vmDeviceSpec . setOperation ( VirtualDeviceConfigSpecOperation . EDIT ) ; vmDeviceSpec . setDevice ( vdDataDisk ) ; vmConfigSpec . getDeviceChange ( ) . add ( vmDeviceSpec ) ; } else { logger . debug ( "Data disk size has not been changed. " + newDiskSpace + " KB" ) ; } }
public void test() { if ( newDiskSpace < vdDataDisk . getCapacityInKB ( ) ) { logger . error ( "Cannot reduce size of data disk " + vdDataDisk . getDeviceInfo ( ) . getLabel ( ) ) ; logger . error ( "Current disk space: " + vdDataDisk . getCapacityInKB ( ) + " new disk space: " + newDiskSpace ) ; throw new Exception ( Messages . getAll ( "error_invalid_diskspacereduction" ) . get ( 0 ) . getText ( ) ) ; } else-if ( newDiskSpace > vdDataDisk . getCapacityInKB ( ) ) { vdDataDisk . setCapacityInKB ( newDiskSpace ) ; VirtualDeviceConfigSpec vmDeviceSpec = new VirtualDeviceConfigSpec ( ) ; vmDeviceSpec . setOperation ( VirtualDeviceConfigSpecOperation . EDIT ) ; vmDeviceSpec . setDevice ( vdDataDisk ) ; vmConfigSpec . getDeviceChange ( ) . add ( vmDeviceSpec ) ; } else { logger . debug ( "Data disk size has not been changed. " + newDiskSpace + " KB" ) ; } }
public void test() { if ( newDiskSpace < vdDataDisk . getCapacityInKB ( ) ) { logger . error ( "Cannot reduce size of data disk " + vdDataDisk . getDeviceInfo ( ) . getLabel ( ) ) ; logger . error ( "Current disk space: " + vdDataDisk . getCapacityInKB ( ) + " new disk space: " + newDiskSpace ) ; throw new Exception ( Messages . getAll ( "error_invalid_diskspacereduction" ) . get ( 0 ) . getText ( ) ) ; } else-if ( newDiskSpace > vdDataDisk . getCapacityInKB ( ) ) { vdDataDisk . setCapacityInKB ( newDiskSpace ) ; VirtualDeviceConfigSpec vmDeviceSpec = new VirtualDeviceConfigSpec ( ) ; vmDeviceSpec . setOperation ( VirtualDeviceConfigSpecOperation . EDIT ) ; vmDeviceSpec . setDevice ( vdDataDisk ) ; vmConfigSpec . getDeviceChange ( ) . add ( vmDeviceSpec ) ; } else { logger . debug ( "Data disk size has not been changed. " + newDiskSpace + " KB" ) ; } }
public void delete ( Secuserrole persistentInstance ) { logger . debug ( "deleting Secuserrole instance" ) ; Session session = getSession ( ) ; code_block = TryStatement ;  }
public void test() { try { session . delete ( persistentInstance ) ; logger . debug ( "delete successful" ) ; } catch ( RuntimeException re ) { logger . error ( "delete failed" , re ) ; throw re ; } finally { this . releaseSession ( session ) ; } }
public void test() { try { parserThread . interrupt ( ) ; parserThread . join ( ) ; } catch ( InterruptedException e ) { logger . debug ( "Interrupted in packet parser thread shutdown join." ) ; } }
public void test() { if ( ServiceResult . SUCCESS == response . getResult ( ) ) { LOGGER . info ( LOG_MSG_LOGIN_REQUEST , loginRequest . getEmail ( ) ) ; UI . getCurrent ( ) . getNavigator ( ) . navigateTo ( UserViews . USERHOME_VIEW_NAME ) ; } else { showNotification ( LOGIN_FAILED , response . getErrorMessage ( ) , Notification . Type . WARNING_MESSAGE ) ; LOGGER . info ( LOG_MSG_LOGIN_REQUEST_FAILURE , loginRequest . getEmail ( ) ) ; } }
public void test() { if ( ServiceResult . SUCCESS == response . getResult ( ) ) { LOGGER . info ( LOG_MSG_LOGIN_REQUEST , loginRequest . getEmail ( ) ) ; UI . getCurrent ( ) . getNavigator ( ) . navigateTo ( UserViews . USERHOME_VIEW_NAME ) ; } else { showNotification ( LOGIN_FAILED , response . getErrorMessage ( ) , Notification . Type . WARNING_MESSAGE ) ; LOGGER . info ( LOG_MSG_LOGIN_REQUEST_FAILURE , loginRequest . getEmail ( ) ) ; } }
@ Override public void initialize ( URI name , Configuration conf ) throws IOException { super . initialize ( name , conf ) ; this . bucket = name . getHost ( ) ; code_block = IfStatement ; this . store . initialize ( name , conf ) ; setConf ( conf ) ; this . uri = URI . create ( name . getScheme ( ) + "://" + name . getAuthority ( ) ) ; this . workingDir = new Path ( "/user" , System . getProperty ( "user.name" ) ) . makeQualified ( this . uri , this . getWorkingDirectory ( ) ) ; this . owner = getOwnerId ( ) ; this . group = getGroupId ( ) ; LOG . debug ( "owner:" + owner + ", group:" + group ) ; BufferPool . getInstance ( ) . initialize ( this . getConf ( ) ) ; int uploadThreadPoolSize = this . getConf ( ) . getInt ( CosNConfigKeys . UPLOAD_THREAD_POOL_SIZE_KEY , CosNConfigKeys . DEFAULT_UPLOAD_THREAD_POOL_SIZE ) ; int readAheadPoolSize = this . getConf ( ) . getInt ( CosNConfigKeys . READ_AHEAD_QUEUE_SIZE , CosNConfigKeys . DEFAULT_READ_AHEAD_QUEUE_SIZE ) ; int ioThreadPoolSize = uploadThreadPoolSize + readAheadPoolSize / 3 ; long threadKeepAlive = this . getConf ( ) . getLong ( CosNConfigKeys . THREAD_KEEP_ALIVE_TIME_KEY , CosNConfigKeys . DEFAULT_THREAD_KEEP_ALIVE_TIME ) ; this . boundedIOThreadPool = BlockingThreadPoolExecutorService . newInstance ( ioThreadPoolSize / 2 , ioThreadPoolSize , threadKeepAlive , TimeUnit . SECONDS , "cos-transfer-thread-pool" ) ; int copyThreadPoolSize = this . getConf ( ) . getInt ( CosNConfigKeys . COPY_THREAD_POOL_SIZE_KEY , CosNConfigKeys . DEFAULT_COPY_THREAD_POOL_SIZE ) ; this . boundedCopyThreadPool = BlockingThreadPoolExecutorService . newInstance ( CosNConfigKeys . DEFAULT_COPY_THREAD_POOL_SIZE , copyThreadPoolSize , 60L , TimeUnit . SECONDS , "cos-copy-thread-pool" ) ; }
public void test() { if ( varDecl != null ) { varDecl . setFinal ( true ) ; localDecls . add ( varDecl ) ; } else { log . error ( "Problems encountered when trying to construct local variable" ) ; } }
public void test() { try { balance = paymentService . getAccountBalance ( paymentheader . getBankaccount ( ) . getId ( ) . toString ( ) , formatter . format ( new Date ( ) ) , paymentheader . getPaymentAmount ( ) , paymentheader . getId ( ) , paymentheader . getBankaccount ( ) . getChartofaccounts ( ) . getId ( ) ) ; } catch ( final ParseException e ) { LOGGER . error ( "Error" + e . getMessage ( ) , e ) ; throw new ValidationException ( Arrays . asList ( new ValidationError ( "Error While formatting date" , "Error While formatting date" ) ) ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "No measurement schedule for: " + key ) ; } }
public void openOpenshiftPage ( ) throws Exception { log . info ( "Opening openshift web page on route {}" , ocRoute ) ; selenium . getDriver ( ) . get ( ocRoute ) ; code_block = IfStatement ; code_block = IfStatement ; }
public void test() { try { logout ( ) ; } catch ( Exception ex ) { log . info ( "User is not logged" ) ; } }
public void test() { try { processVirtualFile ( vfile , stopped , localProcessor ) ; } catch ( ProcessCanceledException | IndexNotReadyException e ) { throw e ; } catch ( Throwable e ) { LOG . error ( "Error during processing of: " + vfile . getName ( ) , e ) ; throw e ; } }
public void test() { try { LOG . debug ( "Loading shared default stream instance" ) ; defaultStream = service . load ( Stream . DEFAULT_STREAM_ID ) ; } catch ( NotFoundException ignored ) { code_block = IfStatement ; i ++ ; Uninterruptibles . sleepUninterruptibly ( 500 , TimeUnit . MILLISECONDS ) ; } }
public void test() { if ( defaultStream != null ) { return defaultStream ; } }
public void test() { if ( bridge . getThing ( ) . getStatus ( ) != ThingStatus . ONLINE ) { logger . debug ( "Evohome Gateway not online, scanning postponed" ) ; return ; } }
@ Test @ Deprecated public void testAddLegacyLocationDefinition ( ) { Map < String , String > expectedConfig = ImmutableMap . of ( "identity" , "bob" , "credential" , "CR3dential" ) ; ClientResponse response = client ( ) . resource ( "/v1/locations" ) . type ( MediaType . APPLICATION_JSON_TYPE ) . post ( ClientResponse . class , new org . apache . brooklyn . rest . domain . LocationSpec ( legacyLocationName , "aws-ec2:us-east-1" , expectedConfig ) ) ; URI addedLegacyLocationUri = response . getLocation ( ) ; log . info ( "added legacy, at: " + addedLegacyLocationUri ) ; LocationSummary location = client ( ) . resource ( response . getLocation ( ) ) . get ( LocationSummary . class ) ; log . info ( " contents: " + location ) ; assertEquals ( location . getSpec ( ) , "brooklyn.catalog:" + legacyLocationName + ":" + legacyLocationVersion ) ; assertTrue ( addedLegacyLocationUri . toString ( ) . startsWith ( "/v1/locations/" ) ) ; JcloudsLocation l = ( JcloudsLocation ) getManagementContext ( ) . getLocationRegistry ( ) . resolve ( legacyLocationName ) ; Assert . assertEquals ( l . getProvider ( ) , "aws-ec2" ) ; Assert . assertEquals ( l . getRegion ( ) , "us-east-1" ) ; Assert . assertEquals ( l . getIdentity ( ) , "bob" ) ; Assert . assertEquals ( l . getCredential ( ) , "CR3dential" ) ; }
@ Test @ Deprecated public void testAddLegacyLocationDefinition ( ) { Map < String , String > expectedConfig = ImmutableMap . of ( "identity" , "bob" , "credential" , "CR3dential" ) ; ClientResponse response = client ( ) . resource ( "/v1/locations" ) . type ( MediaType . APPLICATION_JSON_TYPE ) . post ( ClientResponse . class , new org . apache . brooklyn . rest . domain . LocationSpec ( legacyLocationName , "aws-ec2:us-east-1" , expectedConfig ) ) ; URI addedLegacyLocationUri = response . getLocation ( ) ; log . info ( "added legacy, at: " + addedLegacyLocationUri ) ; LocationSummary location = client ( ) . resource ( response . getLocation ( ) ) . get ( LocationSummary . class ) ; log . info ( " contents: " + location ) ; assertEquals ( location . getSpec ( ) , "brooklyn.catalog:" + legacyLocationName + ":" + legacyLocationVersion ) ; assertTrue ( addedLegacyLocationUri . toString ( ) . startsWith ( "/v1/locations/" ) ) ; JcloudsLocation l = ( JcloudsLocation ) getManagementContext ( ) . getLocationRegistry ( ) . resolve ( legacyLocationName ) ; Assert . assertEquals ( l . getProvider ( ) , "aws-ec2" ) ; Assert . assertEquals ( l . getRegion ( ) , "us-east-1" ) ; Assert . assertEquals ( l . getIdentity ( ) , "bob" ) ; Assert . assertEquals ( l . getCredential ( ) , "CR3dential" ) ; }
public void test() { try { raf . close ( ) ; } catch ( IOException e ) { LOG . error ( e ) ; } }
@ Override public void run ( ) { logger . info ( "Starting edge labeling..." ) ; Map < String , String > map = new HashMap < String , String > ( ) ; int processedRels = 0 ; Transaction tx = graphDb . beginTx ( ) ; ResourceIterable < Relationship > rels = graphDb . getAllRelationships ( ) ; code_block = ForStatement ; logger . info ( processedRels + " relations labeled." ) ; tx . success ( ) ; tx . close ( ) ; }
@ Override public void run ( ) { logger . info ( "Starting edge labeling..." ) ; Map < String , String > map = new HashMap < String , String > ( ) ; int processedRels = 0 ; Transaction tx = graphDb . beginTx ( ) ; ResourceIterable < Relationship > rels = graphDb . getAllRelationships ( ) ; code_block = ForStatement ; logger . info ( processedRels + " relations labeled." ) ; tx . success ( ) ; tx . close ( ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "validate() action=" + action . name ( ) ) ; } }
@ Test public void testCollectionToPrimitiveArrayConversion ( ) throws Exception { List < Integer > list = new ArrayList < > ( ) ; list . add ( 5 ) ; list . add ( 6 ) ; Integer [ ] integerArray = converter . convertTo ( Integer [ ] . class , list ) ; assertEquals ( 2 , integerArray . length , "Integer[] length" ) ; int [ ] intArray = converter . convertTo ( int [ ] . class , list ) ; assertEquals ( 2 , intArray . length , "int[] length" ) ; long [ ] longArray = converter . convertTo ( long [ ] . class , intArray ) ; assertEquals ( 2 , longArray . length , "long[] length" ) ; List < ? > resultList = converter . convertTo ( List . class , intArray ) ; assertEquals ( 2 , resultList . size ( ) , "List size" ) ; LOG . debug ( "From primitive type array we've created the list: " + resultList ) ; }
public void attachDirty ( TmpRepZobbaumas instance ) { log . debug ( "attaching dirty TmpRepZobbaumas instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "getPath({}): {}" , uri , p ) ; } }
@ Override public GeoEvent process ( GeoEvent ge ) throws Exception { LOG . info ( "VisibilityProcessor.process starts................." ) ; double radius ; code_block = IfStatement ; srIn = ge . getGeometry ( ) . getSpatialReference ( ) ; code_block = IfStatement ; double elevation ; code_block = IfStatement ; LOG . info ( "Calling ConstructVisibilityRest................." ) ; GeoEvent outGeo = ConstructVisibilityRest ( ge , gp , is , radius , radiusUnit , elevation , units_elev , procwkid ) ; LOG . info ( "VisibilityProcessor.process ends................." ) ; return outGeo ; }
public void test() { if ( bulkResponse . hasFailures ( ) ) { LOGGER . error ( "Bulk response error={}." , bulkResponse . buildFailureMessage ( ) ) ; } }
public void test() { if ( openSettingsButton . isElementPresent ( MINIMAL_TIMEOUT ) ) { openSettingsButton . clickIfPresent ( DELAY ) ; String currentAndroidVersion = IDriverPool . getDefaultDevice ( ) . getOsVersion ( ) ; LOGGER . info ( "currentAndroidVersion=" + currentAndroidVersion ) ; code_block = IfStatement ; getDriver ( ) . navigate ( ) . back ( ) ; } }
private static void tryAssertion ( RegressionEnvironment env , RegressionPath path , String epl ) { env . compileDeploy ( epl , path ) . addListener ( "s0" ) ; log . info ( "Querying" ) ; long startTime = System . currentTimeMillis ( ) ; code_block = ForStatement ; log . info ( "Done Querying" ) ; long endTime = System . currentTimeMillis ( ) ; log . info ( "delta=" + ( endTime - startTime ) ) ; assertTrue ( ( endTime - startTime ) < 500 ) ; env . undeployModuleContaining ( "s0" ) ; }
private static void tryAssertion ( RegressionEnvironment env , RegressionPath path , String epl ) { env . compileDeploy ( epl , path ) . addListener ( "s0" ) ; log . info ( "Querying" ) ; long startTime = System . currentTimeMillis ( ) ; code_block = ForStatement ; log . info ( "Done Querying" ) ; long endTime = System . currentTimeMillis ( ) ; log . info ( "delta=" + ( endTime - startTime ) ) ; assertTrue ( ( endTime - startTime ) < 500 ) ; env . undeployModuleContaining ( "s0" ) ; }
@ Deactivate protected void deactivate ( ComponentContext context ) { logger . debug ( "deactivating..." ) ; }
public void test() { try { Group group = GroupLocalServiceUtil . getGroup ( groupId ) ; code_block = IfStatement ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; } }
@ Test public void testGetOrder ( ) throws Exception { orderService . setupDummyOrders ( ) ; String response = template . requestBodyAndHeader ( "restlet:http://localhost:8080/orders/{id}?restletMethod=GET" , null , "id" , "1" , String . class ) ; log . info ( "Response: {}" , response ) ; }
public void test() { try { logger . info ( " stop the canal client" ) ; clientTest . stop ( ) ; } catch ( Throwable e ) { logger . warn ( "something goes wrong when stopping canal:" , e ) ; } finally { logger . info ( " canal client is down." ) ; } }
public void test() { try { logger . info ( " stop the canal client" ) ; clientTest . stop ( ) ; } catch ( Throwable e ) { logger . warn ( "something goes wrong when stopping canal:" , e ) ; } finally { logger . info ( " canal client is down." ) ; } }
public void test() { try { logger . info ( " stop the canal client" ) ; clientTest . stop ( ) ; } catch ( Throwable e ) { logger . warn ( "something goes wrong when stopping canal:" , e ) ; } finally { logger . info ( " canal client is down." ) ; } }
public void test() { if ( ServletContextInitializerBeans . logger . isDebugEnabled ( ) ) { ServletContextInitializerBeans . logger . debug ( "Created " + type . getSimpleName ( ) + " initializer for bean '" + beanName + "'; order=" + order + ", resource=" + getResourceDescription ( beanName , beanFactory ) ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( tableTag + " lang sys table: " + langSysTag ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( tableTag + " lang sys table reorder table: " + lo ) ; log . debug ( tableTag + " lang sys table required feature index: " + rf ) ; log . debug ( tableTag + " lang sys table non-required feature count: " + nf ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( tableTag + " lang sys table reorder table: " + lo ) ; log . debug ( tableTag + " lang sys table required feature index: " + rf ) ; log . debug ( tableTag + " lang sys table non-required feature count: " + nf ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( tableTag + " lang sys table reorder table: " + lo ) ; log . debug ( tableTag + " lang sys table required feature index: " + rf ) ; log . debug ( tableTag + " lang sys table non-required feature count: " + nf ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( tableTag + " lang sys table non-required feature index: " + fi ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( getTestBanner ( testName , CLASS_NAME ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( testName + ": knownItemResourceId=" + newID + " inAuthority=" + vcsid ) ; } }
public void test() { if ( Files . exists ( path ) ) { privateLogger . debug ( "Loading configuration from '{}'" , path . toAbsolutePath ( ) ) ; this . configuration = Configuration . load ( new FileInputStream ( path . toFile ( ) ) ) ; } else { privateLogger . debug ( "Loading configuration from JAR file" ) ; this . configuration = Configuration . load ( Configuration . class . getClassLoader ( ) . getResourceAsStream ( "configuration.yml" ) ) ; } }
public void test() { if ( Files . exists ( path ) ) { privateLogger . debug ( "Loading configuration from '{}'" , path . toAbsolutePath ( ) ) ; this . configuration = Configuration . load ( new FileInputStream ( path . toFile ( ) ) ) ; } else { privateLogger . debug ( "Loading configuration from JAR file" ) ; this . configuration = Configuration . load ( Configuration . class . getClassLoader ( ) . getResourceAsStream ( "configuration.yml" ) ) ; } }
public void test() { try { result . addProperty ( "id" , project . getId ( ) ) ; result . addProperty ( "name" , project . getName ( ) ) ; result . addProperty ( "description" , project . getDescription ( ) ) ; result . addProperty ( "description-html" , project . getDescriptionHtml ( ) ) ; result . addProperty ( "type" , project . getType ( ) . name ( ) ) ; result . addProperty ( "board" , project . getBoard ( ) ) ; result . addProperty ( "private" , project . getPrivate ( ) ) ; result . addProperty ( "shared" , project . getShared ( ) ) ; result . addProperty ( "modified" , DateConversion . toDateTimeString ( project . getModified ( ) . getTime ( ) ) ) ; result . addProperty ( "settings" , project . getSettings ( ) ) ; } catch ( Exception ex ) { LOG . error ( "Exception {} detected." , ex . toString ( ) ) ; } }
public void test() { if ( ! isRunAllowed ( ) ) { exchange . setException ( new RejectedExecutionException ( "Run is not allowed" ) ) ; callback . done ( true ) ; return true ; } }
public void test() { try { final File dir = Parameters . getStorageDirectory ( getApplication ( ) ) ; code_block = IfStatement ; final File lastShutdownFile = new File ( dir , "last_shutdown.html" ) ; code_block = TryStatement ;  } catch ( final IOException e ) { LOG . warn ( "exception while writing the last shutdown report" , e ) ; } }
public void test() { try { genResult . get ( ) ; } catch ( ExecutionException ex ) { LOGGER . warn ( "Errored as expected" , ex ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( this + " informing " + listener + " about node down = " + nodeId ) ; } }
public void test() { if ( deleteSegment ( entry . getKey ( ) , segment ) ) { count = count - 1 ; executeHook ( afterDeleted , segment ) ; LOG . info ( "remove expired segment success. segment: {}" , segment ) ; } else { LOG . warn ( "remove expired segment failed. segment: {}" , segment ) ; return ; } }
public void test() { if ( deleteSegment ( entry . getKey ( ) , segment ) ) { count = count - 1 ; executeHook ( afterDeleted , segment ) ; LOG . info ( "remove expired segment success. segment: {}" , segment ) ; } else { LOG . warn ( "remove expired segment failed. segment: {}" , segment ) ; return ; } }
public static boolean download ( URL url , File destFolder , String filename ) throws IOException { File destinationFile = new File ( destFolder . getPath ( ) , filename ) ; LOG . info ( "Plugin download started" ) ; LOG . info ( "Source folder: \"{}\"" , url ) ; LOG . info ( "Destination folder: \"{}\"" , destinationFile ) ; URLConnection urlc = url . openConnection ( ) ; boolean result = false ; code_block = TryStatement ;  LOG . info ( "Plugin download completed" ) ; return result ; }
public static boolean download ( URL url , File destFolder , String filename ) throws IOException { File destinationFile = new File ( destFolder . getPath ( ) , filename ) ; LOG . info ( "Plugin download started" ) ; LOG . info ( "Source folder: \"{}\"" , url ) ; LOG . info ( "Destination folder: \"{}\"" , destinationFile ) ; URLConnection urlc = url . openConnection ( ) ; boolean result = false ; code_block = TryStatement ;  LOG . info ( "Plugin download completed" ) ; return result ; }
public static boolean download ( URL url , File destFolder , String filename ) throws IOException { File destinationFile = new File ( destFolder . getPath ( ) , filename ) ; LOG . info ( "Plugin download started" ) ; LOG . info ( "Source folder: \"{}\"" , url ) ; LOG . info ( "Destination folder: \"{}\"" , destinationFile ) ; URLConnection urlc = url . openConnection ( ) ; boolean result = false ; code_block = TryStatement ;  LOG . info ( "Plugin download completed" ) ; return result ; }
public static boolean download ( URL url , File destFolder , String filename ) throws IOException { File destinationFile = new File ( destFolder . getPath ( ) , filename ) ; LOG . info ( "Plugin download started" ) ; LOG . info ( "Source folder: \"{}\"" , url ) ; LOG . info ( "Destination folder: \"{}\"" , destinationFile ) ; URLConnection urlc = url . openConnection ( ) ; boolean result = false ; code_block = TryStatement ;  LOG . info ( "Plugin download completed" ) ; return result ; }
public void test() { if ( this . methodName != null ) { Assert . hasText ( this . methodName , "methodName must not be empty." ) ; } else { logger . debug ( "No method name provided, CrudRepository.saveAll will be used." ) ; } }
public void test() { if ( RocksDBMemoryControllerUtils . validateArenaBlockSize ( arenaBlockSize , mutableLimit ) ) { return true ; } else { LOG . warn ( "RocksDBStateBackend performance will be poor because of the current Flink memory configuration! " + "RocksDB will flush memtable constantly, causing high IO and CPU. " + "Typically the easiest fix is to increase task manager managed memory size. " + "If running locally, see the parameter taskmanager.memory.managed.size. " + "Details: arenaBlockSize {} > mutableLimit {} (writeBufferSize = {}, arenaBlockSizeConfigured = {}," + " defaultArenaBlockSize = {}, writeBufferManagerCapacity = {})" , arenaBlockSize , mutableLimit , writeBufferSize , arenaBlockSizeConfigured , defaultArenaBlockSize , writeBufferManagerCapacity ) ; return false ; } }
public void test() { while ( ! dcc . isStopped ( ) ) { log . info ( "Waiting on the Camel Context to stop" ) ; } }
@ Override public void tearDown ( ) throws Exception { super . tearDown ( ) ; DefaultCamelContext dcc = ( DefaultCamelContext ) context ; code_block = WhileStatement ; log . info ( "Closing JMS Session" ) ; code_block = IfStatement ; log . info ( "Closing JMS Connection" ) ; code_block = IfStatement ; log . info ( "Stopping the ActiveMQ Broker" ) ; code_block = IfStatement ; }
public void test() { try { String response = formServiceBase . getFormDisplayTask ( containerId , taskId , user , language , filter , formType ) ; code_block = IfStatement ; logger . debug ( "Returning OK response with content '{}'" , response ) ; return createResponse ( response , variant , Response . Status . OK , conversationIdHeader ) ; } catch ( PermissionDeniedException e ) { return permissionDenied ( MessageFormat . format ( TASK_PERMISSION_ERROR , taskId ) , variant , conversationIdHeader ) ; } catch ( TaskNotFoundException e ) { return notFound ( MessageFormat . format ( TASK_INSTANCE_NOT_FOUND , taskId ) , variant , conversationIdHeader ) ; } catch ( DeploymentNotFoundException e ) { return notFound ( MessageFormat . format ( CONTAINER_NOT_FOUND , containerId ) , variant , conversationIdHeader ) ; } catch ( IllegalStateException e ) { return notFound ( "Form for task id " + taskId + " not found" , variant , conversationIdHeader ) ; } catch ( Exception e ) { logger . error ( "Unexpected error during processing {}" , e . getMessage ( ) , e ) ; return internalServerError ( errorMessage ( e ) , variant , conversationIdHeader ) ; } }
public void test() { try { String response = formServiceBase . getFormDisplayTask ( containerId , taskId , user , language , filter , formType ) ; code_block = IfStatement ; logger . debug ( "Returning OK response with content '{}'" , response ) ; return createResponse ( response , variant , Response . Status . OK , conversationIdHeader ) ; } catch ( PermissionDeniedException e ) { return permissionDenied ( MessageFormat . format ( TASK_PERMISSION_ERROR , taskId ) , variant , conversationIdHeader ) ; } catch ( TaskNotFoundException e ) { return notFound ( MessageFormat . format ( TASK_INSTANCE_NOT_FOUND , taskId ) , variant , conversationIdHeader ) ; } catch ( DeploymentNotFoundException e ) { return notFound ( MessageFormat . format ( CONTAINER_NOT_FOUND , containerId ) , variant , conversationIdHeader ) ; } catch ( IllegalStateException e ) { return notFound ( "Form for task id " + taskId + " not found" , variant , conversationIdHeader ) ; } catch ( Exception e ) { logger . error ( "Unexpected error during processing {}" , e . getMessage ( ) , e ) ; return internalServerError ( errorMessage ( e ) , variant , conversationIdHeader ) ; } }
public void doSomething ( ) { log . info ( "Information message text" ) ; log . getName ( ) ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( ioException , ioException ) ; } }
public void test() { try { bookie . suspendProcessing ( ) ; code_block = IfStatement ; l . await ( ) ; bookie . resumeProcessing ( ) ; } catch ( Exception e ) { LOG . error ( "Error suspending bookie" , e ) ; } }
public void test() { try ( FileOutputStream fout = new FileOutputStream ( dir . get ( ) ) ) { allAccountBalancesBuilder . build ( ) . writeTo ( fout ) ; } catch ( IOException e ) { log . error ( String . format ( "Could not export to '%s'!" , dir ) , e ) ; return ; } }
public void test() { if ( now - getLastLogin ( ) < MIN_TIME_BEFORE_RELOGIN ) { LOG . warn ( "Not attempting to re-login since the last re-login was " + "attempted less than {} seconds before." , ( MIN_TIME_BEFORE_RELOGIN / 1000 ) ) ; return false ; } }
public void test() { try { metaConnection . disconnect ( ) ; } catch ( IOException e ) { logger . error ( "ERROR  disconnect meta connection for address:{}" , metaConnection . getConnector ( ) . getAddress ( ) , e ) ; } }
public void test() { try { store . close ( ) ; } catch ( SailException e ) { logger . error ( e . toString ( ) , e ) ; } finally { FileUtil . deltree ( dataDir ) ; dataDir = null ; store = null ; disk = null ; } }
@ Test public void testBench ( ) { logger . info ( "System benchmark result = {}" , SystemUtil . bench ( ) ) ; }
public void test() { if ( ( totalSkip / 100 ) % 250 == 0 ) { log . info ( MoreObjects . toStringHelper ( Thread . currentThread ( ) . getName ( ) + " bundle skip log" ) . add ( "thread-skip" , totalSkip ) . add ( "file-skip" , bundlesSkipped ) . add ( "file-to-skip" , read ) . add ( "global-skip-estimate" , bundlesSkipped + globalBundleSkip . count ( ) ) . toString ( ) ) ; localBundleSkip . set ( totalSkip ) ; globalBundleSkip . inc ( bundlesSkipped ) ; bundlesSkipped = 0 ; } }
public void start ( final boolean exitOnOSGiShutDown ) throws InstantiationException , IllegalAccessException , ClassNotFoundException , BundleException , IOException , InterruptedException { printBanner ( ) ; logger . info ( "----------------- Initialising and Starting the OSGi Framework -----------------" ) ; logger . info ( "FrameworkFactory Class: {}" , factoryClass ) ; logger . info ( "" ) ; FrameworkFactory factory = ( FrameworkFactory ) Class . forName ( factoryClass ) . newInstance ( ) ; framework = factory . newFramework ( frameworkProperties ) ; framework . init ( ) ; logger . info ( "The OSGi framework has been initialised" ) ; BundleContext context = framework . getBundleContext ( ) ; List < Bundle > bundles = new ArrayList < > ( ) ; int startLevel = 1 ; code_block = ForStatement ; startBundles ( bundles ) ; code_block = TryStatement ;  addShutdownHook ( ) ; addCleanupOnExit ( exitOnOSGiShutDown ) ; }
public void start ( final boolean exitOnOSGiShutDown ) throws InstantiationException , IllegalAccessException , ClassNotFoundException , BundleException , IOException , InterruptedException { printBanner ( ) ; logger . info ( "----------------- Initialising and Starting the OSGi Framework -----------------" ) ; logger . info ( "FrameworkFactory Class: {}" , factoryClass ) ; logger . info ( "" ) ; FrameworkFactory factory = ( FrameworkFactory ) Class . forName ( factoryClass ) . newInstance ( ) ; framework = factory . newFramework ( frameworkProperties ) ; framework . init ( ) ; logger . info ( "The OSGi framework has been initialised" ) ; BundleContext context = framework . getBundleContext ( ) ; List < Bundle > bundles = new ArrayList < > ( ) ; int startLevel = 1 ; code_block = ForStatement ; startBundles ( bundles ) ; code_block = TryStatement ;  addShutdownHook ( ) ; addCleanupOnExit ( exitOnOSGiShutDown ) ; }
public void start ( final boolean exitOnOSGiShutDown ) throws InstantiationException , IllegalAccessException , ClassNotFoundException , BundleException , IOException , InterruptedException { printBanner ( ) ; logger . info ( "----------------- Initialising and Starting the OSGi Framework -----------------" ) ; logger . info ( "FrameworkFactory Class: {}" , factoryClass ) ; logger . info ( "" ) ; FrameworkFactory factory = ( FrameworkFactory ) Class . forName ( factoryClass ) . newInstance ( ) ; framework = factory . newFramework ( frameworkProperties ) ; framework . init ( ) ; logger . info ( "The OSGi framework has been initialised" ) ; BundleContext context = framework . getBundleContext ( ) ; List < Bundle > bundles = new ArrayList < > ( ) ; int startLevel = 1 ; code_block = ForStatement ; startBundles ( bundles ) ; code_block = TryStatement ;  addShutdownHook ( ) ; addCleanupOnExit ( exitOnOSGiShutDown ) ; }
public void test() { try { framework . start ( ) ; logger . info ( "The OSGi framework has been started" ) ; logger . info ( "" ) ; } catch ( BundleException e ) { logger . error ( "An error occurred when starting the OSGi framework: {}" , e . getMessage ( ) , e ) ; } }
public void test() { try { framework . start ( ) ; logger . info ( "The OSGi framework has been started" ) ; logger . info ( "" ) ; } catch ( BundleException e ) { logger . error ( "An error occurred when starting the OSGi framework: {}" , e . getMessage ( ) , e ) ; } }
protected void localPerform ( ) throws IOException { getLogger ( ) . info ( "Starting to execute LosePacketsCommandAction" ) ; ServerName server = PolicyBasedChaosMonkey . selectRandomItem ( getCurrentServers ( ) ) ; String hostname = server . getHostname ( ) ; code_block = TryStatement ;  getLogger ( ) . info ( "Finished to execute LosePacketsCommandAction" ) ; }
public void test() { try { clusterManager . execSudoWithRetries ( hostname , timeout , getCommand ( ADD ) ) ; Thread . sleep ( duration ) ; } catch ( InterruptedException e ) { getLogger ( ) . debug ( "Failed to run the command for the full duration" , e ) ; } finally { clusterManager . execSudoWithRetries ( hostname , timeout , getCommand ( DELETE ) ) ; } }
protected void localPerform ( ) throws IOException { getLogger ( ) . info ( "Starting to execute LosePacketsCommandAction" ) ; ServerName server = PolicyBasedChaosMonkey . selectRandomItem ( getCurrentServers ( ) ) ; String hostname = server . getHostname ( ) ; code_block = TryStatement ;  getLogger ( ) . info ( "Finished to execute LosePacketsCommandAction" ) ; }
public void test() { if ( streamChangedJobsMap . containsKey ( jobName ) ) { log . warn ( "the DB have duplicated jobName({})" , jobName ) ; } else { streamChangedJobsMap . put ( jobName , streamChangedJob ) ; } }
public void test() { try { guid = new GUIDImpl ( properties . getProperty ( FIELDS . BASE32 . name ( ) ) ) ; guid2 = new GUIDImpl ( properties . getProperty ( FIELDS . BASE16 . name ( ) ) ) ; } catch ( final InvalidGuidOperationException e ) { LOGGER . error ( ResourcesPublicUtilTest . SHOULD_NOT_HAVE_AN_EXCEPTION , e ) ; fail ( ResourcesPublicUtilTest . SHOULD_NOT_HAVE_AN_EXCEPTION ) ; return ; } }
public void test() { if ( ExceptionUtils . containsInterruptedException ( t ) ) { Thread . currentThread ( ) . interrupt ( ) ; } else { LOG . error ( "{}: Failed to create checkpoint" , mMaster . getName ( ) , t ) ; } }
public void test() { try { UfsJournalCheckpointWriter journalWriter = mJournal . getCheckpointWriter ( nextSequenceNumber ) ; code_block = TryStatement ;  LOG . info ( "{}: Finished checkpoint [sequence number {}]." , mMaster . getName ( ) , nextSequenceNumber ) ; mNextSequenceNumberToCheckpoint = nextSequenceNumber ; } catch ( IOException e ) { LOG . error ( "{}: Failed to checkpoint." , mMaster . getName ( ) , e ) ; } }
public void test() { if ( LOGGER . isInfoEnabled ( ) ) { LOGGER . info ( "Undeploying library {}.{}" , dataverseName , libraryName ) ; } }
public void test() { try { log . debug ( "Waiting for interrupt" ) ; Thread . sleep ( 10000000 ) ; ModelNode result = new ModelNode ( ) ; result . get ( "testing" ) . set ( operation . get ( "test" ) ) ; return result ; } catch ( InterruptedException e ) { interrupted . countDown ( ) ; throw new RuntimeException ( e ) ; } }
public void test() { try { authorizer . replaceAllUsers ( snapshot . getUserMap ( ) ) ; } catch ( AuthException e ) { logger . error ( "{}:replace users failed" , metaGroupMember . getName ( ) , e ) ; } }
public void test() { if ( value instanceof String && StringHelper . hasStartToken ( ( String ) value , "simple" ) ) { LOG . warn ( "Simple expression: {} detected in header: {} of type String. This feature has been removed (see CAMEL-6748)." , value , Exchange . FILE_NAME ) ; } }
public void test() { try { int removedOldFilesCount = fileService . deleteModifiedBefore ( expirationTime ) ; LOGGER . info ( CleanUpJob . LOG_MARKER , format ( Messages . DELETED_FILES_0 , removedOldFilesCount ) ) ; } catch ( FileStorageException e ) { throw new SLException ( e , Messages . COULD_NOT_DELETE_FILES_MODIFIED_BEFORE_0 , expirationTime ) ; } }
@ Override public void onError ( JoynrRuntimeException error ) { logger . info ( name . getMethodName ( ) + " - callback - got unexpected exception" ) ; subscribeAttributeEnumerationCallbackResult = false ; subscribeAttributeEnumerationCallbackDone = true ; }
public void test() { try { latch = new CountDownLatch ( 3 ) ; log . info ( "Sending session opened  event: {}" , sessionOpenedEvent ) ; service . notifyEvent ( sessionOpenedEvent ) ; log . info ( "Sending session restored event: {}" , sessionRestored ) ; service . notifyEvent ( sessionRestored ) ; latch . await ( 5 , TimeUnit . SECONDS ) ; assertTrue ( genericEventListener . processed ) ; assertTrue ( sessionOpenedListener . processed ) ; assertTrue ( sessionRestoredListener . processed ) ; resetSessionListeners ( ) ; LensEvent genEvent = new LensEvent ( now ) code_block = "" ; ; latch = new CountDownLatch ( 2 ) ; log . info ( "Sending generic event {}" , genEvent . getEventId ( ) ) ; service . notifyEvent ( genEvent ) ; latch . await ( 5 , TimeUnit . SECONDS ) ; assertTrue ( genericEventListener . processed ) ; resetSessionListeners ( ) ; latch = new CountDownLatch ( 3 ) ; log . info ( "Sending session closed event {}" , sessionClosedEvent ) ; service . notifyEvent ( sessionClosedEvent ) ; log . info ( "Sending session expired event {}" , sessionExpired ) ; service . notifyEvent ( sessionExpired ) ; latch . await ( 5 , TimeUnit . SECONDS ) ; assertTrue ( sessionClosedListener . processed ) ; assertTrue ( sessionExpiredListner . processed ) ; assertFalse ( sessionOpenedListener . processed ) ; assertFalse ( sessionRestoredListener . processed ) ; } catch ( LensException e ) { fail ( e . getMessage ( ) ) ; } }
public void test() { try { latch = new CountDownLatch ( 3 ) ; log . info ( "Sending session opened  event: {}" , sessionOpenedEvent ) ; service . notifyEvent ( sessionOpenedEvent ) ; log . info ( "Sending session restored event: {}" , sessionRestored ) ; service . notifyEvent ( sessionRestored ) ; latch . await ( 5 , TimeUnit . SECONDS ) ; assertTrue ( genericEventListener . processed ) ; assertTrue ( sessionOpenedListener . processed ) ; assertTrue ( sessionRestoredListener . processed ) ; resetSessionListeners ( ) ; LensEvent genEvent = new LensEvent ( now ) code_block = "" ; ; latch = new CountDownLatch ( 2 ) ; log . info ( "Sending generic event {}" , genEvent . getEventId ( ) ) ; service . notifyEvent ( genEvent ) ; latch . await ( 5 , TimeUnit . SECONDS ) ; assertTrue ( genericEventListener . processed ) ; resetSessionListeners ( ) ; latch = new CountDownLatch ( 3 ) ; log . info ( "Sending session closed event {}" , sessionClosedEvent ) ; service . notifyEvent ( sessionClosedEvent ) ; log . info ( "Sending session expired event {}" , sessionExpired ) ; service . notifyEvent ( sessionExpired ) ; latch . await ( 5 , TimeUnit . SECONDS ) ; assertTrue ( sessionClosedListener . processed ) ; assertTrue ( sessionExpiredListner . processed ) ; assertFalse ( sessionOpenedListener . processed ) ; assertFalse ( sessionRestoredListener . processed ) ; } catch ( LensException e ) { fail ( e . getMessage ( ) ) ; } }
public void test() { try { latch = new CountDownLatch ( 3 ) ; log . info ( "Sending session opened  event: {}" , sessionOpenedEvent ) ; service . notifyEvent ( sessionOpenedEvent ) ; log . info ( "Sending session restored event: {}" , sessionRestored ) ; service . notifyEvent ( sessionRestored ) ; latch . await ( 5 , TimeUnit . SECONDS ) ; assertTrue ( genericEventListener . processed ) ; assertTrue ( sessionOpenedListener . processed ) ; assertTrue ( sessionRestoredListener . processed ) ; resetSessionListeners ( ) ; LensEvent genEvent = new LensEvent ( now ) code_block = "" ; ; latch = new CountDownLatch ( 2 ) ; log . info ( "Sending generic event {}" , genEvent . getEventId ( ) ) ; service . notifyEvent ( genEvent ) ; latch . await ( 5 , TimeUnit . SECONDS ) ; assertTrue ( genericEventListener . processed ) ; resetSessionListeners ( ) ; latch = new CountDownLatch ( 3 ) ; log . info ( "Sending session closed event {}" , sessionClosedEvent ) ; service . notifyEvent ( sessionClosedEvent ) ; log . info ( "Sending session expired event {}" , sessionExpired ) ; service . notifyEvent ( sessionExpired ) ; latch . await ( 5 , TimeUnit . SECONDS ) ; assertTrue ( sessionClosedListener . processed ) ; assertTrue ( sessionExpiredListner . processed ) ; assertFalse ( sessionOpenedListener . processed ) ; assertFalse ( sessionRestoredListener . processed ) ; } catch ( LensException e ) { fail ( e . getMessage ( ) ) ; } }
public void test() { try { latch = new CountDownLatch ( 3 ) ; log . info ( "Sending session opened  event: {}" , sessionOpenedEvent ) ; service . notifyEvent ( sessionOpenedEvent ) ; log . info ( "Sending session restored event: {}" , sessionRestored ) ; service . notifyEvent ( sessionRestored ) ; latch . await ( 5 , TimeUnit . SECONDS ) ; assertTrue ( genericEventListener . processed ) ; assertTrue ( sessionOpenedListener . processed ) ; assertTrue ( sessionRestoredListener . processed ) ; resetSessionListeners ( ) ; LensEvent genEvent = new LensEvent ( now ) code_block = "" ; ; latch = new CountDownLatch ( 2 ) ; log . info ( "Sending generic event {}" , genEvent . getEventId ( ) ) ; service . notifyEvent ( genEvent ) ; latch . await ( 5 , TimeUnit . SECONDS ) ; assertTrue ( genericEventListener . processed ) ; resetSessionListeners ( ) ; latch = new CountDownLatch ( 3 ) ; log . info ( "Sending session closed event {}" , sessionClosedEvent ) ; service . notifyEvent ( sessionClosedEvent ) ; log . info ( "Sending session expired event {}" , sessionExpired ) ; service . notifyEvent ( sessionExpired ) ; latch . await ( 5 , TimeUnit . SECONDS ) ; assertTrue ( sessionClosedListener . processed ) ; assertTrue ( sessionExpiredListner . processed ) ; assertFalse ( sessionOpenedListener . processed ) ; assertFalse ( sessionRestoredListener . processed ) ; } catch ( LensException e ) { fail ( e . getMessage ( ) ) ; } }
public void test() { try { latch = new CountDownLatch ( 3 ) ; log . info ( "Sending session opened  event: {}" , sessionOpenedEvent ) ; service . notifyEvent ( sessionOpenedEvent ) ; log . info ( "Sending session restored event: {}" , sessionRestored ) ; service . notifyEvent ( sessionRestored ) ; latch . await ( 5 , TimeUnit . SECONDS ) ; assertTrue ( genericEventListener . processed ) ; assertTrue ( sessionOpenedListener . processed ) ; assertTrue ( sessionRestoredListener . processed ) ; resetSessionListeners ( ) ; LensEvent genEvent = new LensEvent ( now ) code_block = "" ; ; latch = new CountDownLatch ( 2 ) ; log . info ( "Sending generic event {}" , genEvent . getEventId ( ) ) ; service . notifyEvent ( genEvent ) ; latch . await ( 5 , TimeUnit . SECONDS ) ; assertTrue ( genericEventListener . processed ) ; resetSessionListeners ( ) ; latch = new CountDownLatch ( 3 ) ; log . info ( "Sending session closed event {}" , sessionClosedEvent ) ; service . notifyEvent ( sessionClosedEvent ) ; log . info ( "Sending session expired event {}" , sessionExpired ) ; service . notifyEvent ( sessionExpired ) ; latch . await ( 5 , TimeUnit . SECONDS ) ; assertTrue ( sessionClosedListener . processed ) ; assertTrue ( sessionExpiredListner . processed ) ; assertFalse ( sessionOpenedListener . processed ) ; assertFalse ( sessionRestoredListener . processed ) ; } catch ( LensException e ) { fail ( e . getMessage ( ) ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( ThemeServiceUtil . class , "getWARThemes" , _getWARThemesParameterTypes1 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . portal . kernel . json . JSONArray ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( android ) { logger . debug ( "Platform: Android" ) ; } }
public void test() { while ( sentQueue . peek ( ) != null && sentQueue . peek ( ) . getFrmNum ( ) != ackNum ) { AshFrameData ackedFrame = sentQueue . poll ( ) ; logger . debug ( "ASH: Frame acked and removed {}" , ackedFrame ) ; } }
public void test() { try { Group group = getGroup ( ) ; code_block = IfStatement ; return group . getPrivateLayoutsPageCount ( ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; } }
public void test() { try { Runtime . start ( "gui" , "SwingGui" ) ; OpenCV cv = ( OpenCV ) Runtime . start ( "cv" , "OpenCV" ) ; OpenCVFilterAddMask mask = new OpenCVFilterAddMask ( "mask" ) ; cv . addFilter ( mask ) ; mask . test ( ) ; boolean done = true ; code_block = IfStatement ; cv . capture ( "src\\test\\resources\\OpenCV\\multipleFaces.jpg" ) ; cv . addFilter ( mask ) ; cv . capture ( ) ; } catch ( Exception e ) { log . error ( "main threw" , e ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Will sleep for {} millisec" , failoverPolicy . sleepBetweenHostsMilli ) ; } }
public void test() { try { Thread . sleep ( failoverPolicy . sleepBetweenHostsMilli ) ; } catch ( InterruptedException e ) { log . warn ( "Sleep between hosts interrupted" , e ) ; } }
@ Override public void configure ( Context context ) { super . configure ( context ) ; serializerType = context . getString ( "serializer" , "TEXT" ) ; useRawLocalFileSystem = context . getBoolean ( "hdfs.useRawLocalFileSystem" , false ) ; serializerContext = new Context ( context . getSubProperties ( EventSerializer . CTX_PREFIX ) ) ; logger . info ( "Serializer = " + serializerType + ", UseRawLocalFileSystem = " + useRawLocalFileSystem ) ; }
public void test() { if ( match + miss >= debug ) { log . warn ( "query[" + debugKey + "]: match=" + match + " miss=" + miss ) ; match = 0 ; miss = 0 ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( exception , exception ) ; } }
Source loadXml ( String theSchemaName ) { String pathToBase = myCtx . getVersion ( ) . getPathToSchemaDefinitions ( ) + '/' + theSchemaName ; ourLog . debug ( "Going to load resource: {}" , pathToBase ) ; String contents = ClasspathUtil . loadResource ( pathToBase , ClasspathUtil . withBom ( ) ) ; return new StreamSource ( new StringReader ( contents ) , null ) ; }
public void test() { if ( rng != null && seed != null ) { log . warn ( "getSplits() was called more than once and the 'seed' is set, " + "this can lead to no-repeatable behavior" ) ; } }
public static void waitForServiceDeletion ( String serviceName ) { LOGGER . info ( "Waiting for Service {} deletion in namespace {}" , serviceName , kubeClient ( ) . getNamespace ( ) ) ; TestUtils . waitFor ( "Service " + serviceName + " to be deleted" , Constants . POLL_INTERVAL_FOR_RESOURCE_READINESS , DELETION_TIMEOUT , ( ) -> kubeClient ( ) . getService ( serviceName ) == null ) ; LOGGER . info ( "Service {} in namespace {} was deleted" , serviceName , kubeClient ( ) . getNamespace ( ) ) ; }
public static void waitForServiceDeletion ( String serviceName ) { LOGGER . info ( "Waiting for Service {} deletion in namespace {}" , serviceName , kubeClient ( ) . getNamespace ( ) ) ; TestUtils . waitFor ( "Service " + serviceName + " to be deleted" , Constants . POLL_INTERVAL_FOR_RESOURCE_READINESS , DELETION_TIMEOUT , ( ) -> kubeClient ( ) . getService ( serviceName ) == null ) ; LOGGER . info ( "Service {} in namespace {} was deleted" , serviceName , kubeClient ( ) . getNamespace ( ) ) ; }
public void test() { try { code_block = TryStatement ;  } catch ( Exception e ) { LOGGER . warn ( "saveDataHostIndexToZk err:" , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "storing response message {}" , message . toStringForDebug ( false ) ) ; } }
public void test() { if ( LOGGER . isInfoEnabled ( ) ) { LOGGER . info ( "Ordered Scan:" ) ; } }
public void test() { if ( LOGGER . isInfoEnabled ( ) ) { LOGGER . info ( rec ) ; } }
public void test() { try { backend . sendCode ( settings . emailSecurityCodeMsgTemplate , false ) ; return true ; } catch ( Exception e ) { log . warn ( "Credential reset notification failed" , e ) ; NotificationPopup . showError ( msg . getMessage ( "error" ) , msg . getMessage ( "CredentialReset.resetNotPossible" ) ) ; onCancel ( ) ; return false ; } }
public void test() { if ( StatusUtils . isResourceV1alpha1 ( connect ) ) { log . warn ( "{}: The resource needs to be upgraded from version {} to 'v1beta1' to use the status field" , reconciliation , connect . getApiVersion ( ) ) ; updateStatusPromise . complete ( ) ; } else { KafkaConnectS2IStatus currentStatus = connect . getStatus ( ) ; StatusDiff ksDiff = new StatusDiff ( currentStatus , desiredStatus ) ; code_block = IfStatement ; } }
public void test() { if ( updateRes . succeeded ( ) ) { log . debug ( "{}: Completed status update" , reconciliation ) ; updateStatusPromise . complete ( ) ; } else { log . error ( "{}: Failed to update status" , reconciliation , updateRes . cause ( ) ) ; updateStatusPromise . fail ( updateRes . cause ( ) ) ; } }
public void test() { if ( updateRes . succeeded ( ) ) { log . debug ( "{}: Completed status update" , reconciliation ) ; updateStatusPromise . complete ( ) ; } else { log . error ( "{}: Failed to update status" , reconciliation , updateRes . cause ( ) ) ; updateStatusPromise . fail ( updateRes . cause ( ) ) ; } }
public void test() { if ( getRes . succeeded ( ) ) { KafkaConnectS2I connect = getRes . result ( ) ; code_block = IfStatement ; } else { log . error ( "{}: Failed to get the current Kafka ConnectS2I resource and its status" , reconciliation , getRes . cause ( ) ) ; updateStatusPromise . fail ( getRes . cause ( ) ) ; } }
@ Override public void setAVUMetadata ( final String userName , final AvuData avuData ) throws DataNotFoundException , JargonException { log . info ( "setAVUMetadata()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "set avu metadata for user: {}" , avuData ) ; log . info ( "userName {}" , userName ) ; code_block = IfStatement ; final ModAvuMetadataInp modifyAvuMetadataInp = ModAvuMetadataInp . instanceForSetUserMetadata ( userName , avuData ) ; log . debug ( "sending avu request" ) ; code_block = TryStatement ;  log . debug ( "metadata added" ) ; }
@ Override public void setAVUMetadata ( final String userName , final AvuData avuData ) throws DataNotFoundException , JargonException { log . info ( "setAVUMetadata()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "set avu metadata for user: {}" , avuData ) ; log . info ( "userName {}" , userName ) ; code_block = IfStatement ; final ModAvuMetadataInp modifyAvuMetadataInp = ModAvuMetadataInp . instanceForSetUserMetadata ( userName , avuData ) ; log . debug ( "sending avu request" ) ; code_block = TryStatement ;  log . debug ( "metadata added" ) ; }
@ Override public void setAVUMetadata ( final String userName , final AvuData avuData ) throws DataNotFoundException , JargonException { log . info ( "setAVUMetadata()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "set avu metadata for user: {}" , avuData ) ; log . info ( "userName {}" , userName ) ; code_block = IfStatement ; final ModAvuMetadataInp modifyAvuMetadataInp = ModAvuMetadataInp . instanceForSetUserMetadata ( userName , avuData ) ; log . debug ( "sending avu request" ) ; code_block = TryStatement ;  log . debug ( "metadata added" ) ; }
@ Override public void setAVUMetadata ( final String userName , final AvuData avuData ) throws DataNotFoundException , JargonException { log . info ( "setAVUMetadata()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "set avu metadata for user: {}" , avuData ) ; log . info ( "userName {}" , userName ) ; code_block = IfStatement ; final ModAvuMetadataInp modifyAvuMetadataInp = ModAvuMetadataInp . instanceForSetUserMetadata ( userName , avuData ) ; log . debug ( "sending avu request" ) ; code_block = TryStatement ;  log . debug ( "metadata added" ) ; }
public void test() { try { getIRODSProtocol ( ) . irodsFunction ( modifyAvuMetadataInp ) ; } catch ( JargonException je ) { code_block = IfStatement ; log . error ( "jargon exception adding AVU metadata" , je ) ; throw je ; } }
@ Override public void setAVUMetadata ( final String userName , final AvuData avuData ) throws DataNotFoundException , JargonException { log . info ( "setAVUMetadata()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "set avu metadata for user: {}" , avuData ) ; log . info ( "userName {}" , userName ) ; code_block = IfStatement ; final ModAvuMetadataInp modifyAvuMetadataInp = ModAvuMetadataInp . instanceForSetUserMetadata ( userName , avuData ) ; log . debug ( "sending avu request" ) ; code_block = TryStatement ;  log . debug ( "metadata added" ) ; }
public void test() { if ( resourcePath . size ( ) != 2 || reqPayload . getBody ( ) == null || reqPayload . getBody ( ) . length == 0 || ! resourcePath . get ( 0 ) . equals ( KEYSTORES ) ) { logger . error ( NONE_RESOURCE_FOUND_MESSAGE ) ; throw new KuraException ( KuraErrorCode . BAD_REQUEST ) ; } }
public void test() { if ( _log . isInfoEnabled ( ) ) { _log . info ( "Binding " + filterParser ) ; } }
public void writeDecimalType ( String path , DecimalType value ) throws OwException { OwserverPacket requestPacket = new OwserverPacket ( OwserverMessageType . WRITE , path ) ; requestPacket . appendPayload ( String . valueOf ( value ) ) ; OwserverPacket returnPacket = request ( requestPacket ) ; logger . trace ( "wrote: {}, got: {} " , requestPacket , returnPacket ) ; }
public void test() { if ( connectionErrorCounter > CONNECTION_MAX_RETRY ) { logger . debug ( "OW connection state: set to failed as max retries exceeded." ) ; owserverConnectionState = OwserverConnectionState . FAILED ; tryingConnectionRecovery = false ; thingHandlerCallback . reportConnectionState ( owserverConnectionState ) ; } else-if ( ! tryingConnectionRecovery ) { thingHandlerCallback . reportConnectionState ( owserverConnectionState ) ; } }
public void test() { try ( final BufferedReader br = new BufferedReader ( new InputStreamReader ( new FileInputStream ( validationRuleFile ) , StandardCharsets . UTF_8 ) ) ) { processAssertions ( br ) ; } catch ( final Exception ex ) { log . error ( "Error creating Drools base message validator." , ex ) ; } }
public void test() { try { msg . setField ( 0 , identifier ) ; } catch ( Exception ex ) { LOG . debug ( "Failed to set 'identifier': " + identifier ) ; } }
public void test() { try { msg . setField ( 1 , infoID ) ; } catch ( Exception ex ) { LOG . debug ( "Failed to set 'infoID': " + infoID ) ; } }
@ Override public UserDetails loadUserDetails ( PreAuthenticatedAuthenticationToken token ) throws UsernameNotFoundException { ApplicationUser user = ( ApplicationUser ) token . getPrincipal ( ) ; Set < GrantedAuthority > authorities = new HashSet < > ( ) ; authorities . addAll ( securityHelper . mapRolesToFunctions ( user . getRoles ( ) ) ) ; authorities . addAll ( securityHelper . getUnrestrictedFunctions ( ) ) ; SecurityUserWrapper result = new SecurityUserWrapper ( user . getUserId ( ) , "N/A" , true , true , true , true , authorities , user ) ; LOGGER . debug ( "Loaded User: " + result ) ; return result ; }
public void test() { try { logger . debug ( "Retrieving response from server." ) ; final String serverResponse = retrieveResponseFromServer ( new URL ( validationUrl ) , ticket ) ; code_block = IfStatement ; logger . debug ( "Server response: {}" , serverResponse ) ; return parseResponseFromServer ( serverResponse ) ; } catch ( final MalformedURLException e ) { throw new TicketValidationException ( e ) ; } }
public void test() { try { logger . debug ( "Retrieving response from server." ) ; final String serverResponse = retrieveResponseFromServer ( new URL ( validationUrl ) , ticket ) ; code_block = IfStatement ; logger . debug ( "Server response: {}" , serverResponse ) ; return parseResponseFromServer ( serverResponse ) ; } catch ( final MalformedURLException e ) { throw new TicketValidationException ( e ) ; } }
public void test() { try { configStore . initiateUpdate ( newEntity ) ; obtainEntityLocks ( oldEntity , "update" , tokenList ) ; configStore . update ( newEntity . getEntityType ( ) , newEntity ) ; } catch ( Throwable e ) { LOG . error ( "Update failed" , e ) ; throw FalconWebException . newAPIException ( e ) ; } finally { ConfigurationStore . get ( ) . cleanupUpdateInit ( ) ; releaseEntityLocks ( oldEntity . getName ( ) , tokenList ) ; } }
public void test() { try { code_block = SwitchStatement ; } catch ( NumberFormatException | XPathException e ) { LOG . trace ( "Cannot convert field {} to type {}. Content was: {}" , fieldName , Type . getTypeName ( type ) , content ) ; } }
@ Override public void doLoadShedding ( ) { long overloadThreshold = this . getLoadBalancerBrokerOverloadedThresholdPercentage ( ) ; long comfortLoadLevel = this . getLoadBalancerBrokerComfortLoadThresholdPercentage ( ) ; log . info ( "Running load shedding task as leader broker, overload threshold {}, comfort loadlevel {}" , overloadThreshold , comfortLoadLevel ) ; Map < ResourceUnit , String > namespaceBundlesToBeUnloaded = new HashMap < > ( ) ; synchronized ( currentLoadReports ) code_block = "" ; unloadNamespacesFromOverLoadedBrokers ( namespaceBundlesToBeUnloaded ) ; }
public void test() { if ( bundleStats == null ) { log . warn ( "Null bundle stats for bundle {}" , lr . getName ( ) ) ; continue ; } }
public void test() { if ( bundleStats . size ( ) == 1 ) { String bundleName = lr . getBundleStats ( ) . keySet ( ) . iterator ( ) . next ( ) ; log . warn ( "HIGH USAGE WARNING : Sole namespace bundle {} is overloading broker {}. " + "No Load Shedding will be done on this broker" , bundleName , overloadedRU . getResourceId ( ) ) ; continue ; } }
public void test() { if ( isBrokerAvailableForRebalancing ( bundleStat . getKey ( ) , comfortLoadLevel ) ) { log . info ( "Namespace bundle {} will be unloaded from overloaded broker {}," + " bundle stats (topics: {}, producers {}, " + "consumers {}, bandwidthIn {}, bandwidthOut {})" , bundleName , overloadedRU . getResourceId ( ) , stats . topics , stats . producerCount , stats . consumerCount , stats . msgThroughputIn , stats . msgThroughputOut ) ; namespaceBundlesToBeUnloaded . put ( overloadedRU , bundleName ) ; } else { log . info ( "Unable to shed load from broker {}, no brokers with enough capacity available " + "for re-balancing {}" , overloadedRU . getResourceId ( ) , bundleName ) ; } }
public void test() { if ( isBrokerAvailableForRebalancing ( bundleStat . getKey ( ) , comfortLoadLevel ) ) { log . info ( "Namespace bundle {} will be unloaded from overloaded broker {}," + " bundle stats (topics: {}, producers {}, " + "consumers {}, bandwidthIn {}, bandwidthOut {})" , bundleName , overloadedRU . getResourceId ( ) , stats . topics , stats . producerCount , stats . consumerCount , stats . msgThroughputIn , stats . msgThroughputOut ) ; namespaceBundlesToBeUnloaded . put ( overloadedRU , bundleName ) ; } else { log . info ( "Unable to shed load from broker {}, no brokers with enough capacity available " + "for re-balancing {}" , overloadedRU . getResourceId ( ) , bundleName ) ; } }
public CreateProjectPage enterProjectId ( String projectId ) { log . info ( "Enter project ID {}" , projectId ) ; enterText ( readyElement ( idField ) , projectId ) ; return new CreateProjectPage ( getDriver ( ) ) ; }
public void test() { try { Object value = BeanUtil . pojo . getProperty ( bean , param ) ; beanValue = _converter . toDoubleValue ( value , defaultValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; } }
public boolean isDirectory ( ) { logger . trace ( "[{}] isDirectory() -> {}" , name , isDirectory ) ; return isDirectory ; }
protected Response makeBearerError ( BearerTokenError error ) { String header = error . toWWWAuthenticateHeader ( ) ; log . debug ( "Retuning OAuth bearer error response: " + header ) ; return toResponse ( Response . status ( error . getHTTPStatusCode ( ) ) . header ( "WWW-Authenticate" , header ) ) ; }
public void test() { if ( "export" . equals ( mode ) ) { doExport ( parameters ) ; } else-if ( "import" . equals ( mode ) ) { doImport ( parameters ) ; } else { logger . warn ( "Unsupported mode '{}'" , mode ) ; } }
public void test() { if ( systemUnit == null ) { LOGGER . warn ( "Unit field points to a null value: {}" , field ) ; } else-if ( systemUnit . isCompatible ( unit ) ) { return dimension ; } }
public void test() { try { systemUnit = ( Unit < ? > ) field . get ( null ) ; code_block = IfStatement ; } catch ( IllegalArgumentException | IllegalAccessException e ) { LOGGER . error ( "The unit field '{}' seems to be not accessible" , field , e ) ; } }
public void test() { if ( genericType instanceof ParameterizedType ) { String dimension = ( ( Class < ? > ) ( ( ParameterizedType ) genericType ) . getActualTypeArguments ( ) [ 0 ] ) . getSimpleName ( ) ; Unit < ? > systemUnit ; code_block = TryStatement ;  } else { LOGGER . warn ( "There is a unit field defined which has no generic type parametrization: {}" , field ) ; } }
private void getServerProfileCompliancePreview ( ) { ServerProfile serverProfile = this . serverProfileClient . getByName ( SERVER_PROFILE_NAME ) . get ( 0 ) ; ServerProfileCompliancePreview compliance = serverProfileClient . getCompliancePreview ( serverProfile . getResourceId ( ) ) ; LOGGER . info ( "ServerProfileCompliancePreview object returned to client : " + JsonPrettyPrinter . print ( compliance ) ) ; }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { for ( Exchange exchange : list ) { Entry entry = exchange . getIn ( ) . getBody ( Entry . class ) ; assertNotNull ( entry , "No entry found for exchange: " + exchange ) ; String expectedTitle = expectedTitles [ counter ] ; String title = entry . getTitle ( ) ; assertEquals ( expectedTitle , title , "Title of message " + counter ) ; LOG . debug ( "<<<< {}" , entry ) ; counter ++ ; } }
public void test() { try { ActiveTransactionsRecord myRecord = this . activeTxRecord ; code_block = IfStatement ; } catch ( SQLException sqlex ) { logger . warn ( "SqlException: " + sqlex . getMessage ( ) ) ; throw new CommitException ( ) ; } catch ( LookupException le ) { throw new Error ( "Error while obtaining database connection" , le ) ; } }
public void test() { if ( TransactionChangeLogs . updateFromTxLogsOnDatabase ( pb , myRecord , true ) != myRecord ) { code_block = IfStatement ; } }
public void test() { try { String jsonString = query ( sparqlQueryString ) ; rs = SparqlQuery . convertJSONtoResultSet ( jsonString ) ; } catch ( Exception e ) { logger . warn ( e . getMessage ( ) ) ; } }
public void test() { try { action . call ( ) ; } catch ( Exception ex ) { LOGGER . error ( "**** Error!! Error running privileged action." , ex ) ; } }
public void test() { try { final File testFile = new File ( datenDownload . arr [ DatenDownload . DOWNLOAD_ZIEL_PFAD_DATEINAME ] ) ; code_block = IfStatement ; } catch ( Exception ex ) { logger . error ( "Fehler beim Ermitteln der DateigrÃ¶Ãe: {}" , datenDownload . arr [ DatenDownload . DOWNLOAD_ZIEL_PFAD_DATEINAME ] ) ; } }
public void test() { if ( this . cacheAssertions ) { logger . debug ( "Caching assertion for principal {}" , this . assertion . getPrincipal ( ) ) ; ASSERTION_CACHE . put ( this . ticket , this . assertion ) ; } }
public void attachClean ( StgMbCss instance ) { log . debug ( "attaching clean StgMbCss instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . lock ( instance , LockMode . NONE ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { if ( ! addrs . equals ( this . nameSrvAddr ) ) { log . info ( "name server address changed, old: {} new: {}" , this . nameSrvAddr , addrs ) ; this . updateNameServerAddressList ( addrs ) ; this . nameSrvAddr = addrs ; return nameSrvAddr ; } }
public void test() { try { String addrs = this . topAddressing . fetchNSAddr ( ) ; code_block = IfStatement ; } catch ( Exception e ) { log . error ( "fetchNameServerAddr Exception" , e ) ; } }
public void test() { try { setupHelper . cleanUp ( ) ; } catch ( RuntimeException | IOException e ) { log . debug ( "Expected error while shutting down Hibernate Search, caused by the deletion of an index" , e ) ; } }
protected void createOCFS2Sr ( StorageFilerTO pool ) throws XmlRpcException { OvmStoragePool . Details d = new OvmStoragePool . Details ( ) ; d . path = pool . getPath ( ) ; d . type = OvmStoragePool . OCFS2 ; d . uuid = pool . getUuid ( ) ; OvmStoragePool . create ( _conn , d ) ; s_logger . debug ( String . format ( "Created SR (mount point:%1$s)" , d . path ) ) ; }
public void test() { try { shareStudy = dao . findByID ( shareStudyId ) ; } catch ( Exception exception ) { LOG . error ( "Cannot find ShareStudy by shareStudyID " + shareStudyId ) ; LOG . error ( exception . getMessage ( ) ) ; } }
public void test() { try { shareStudy = dao . findByID ( shareStudyId ) ; } catch ( Exception exception ) { LOG . error ( "Cannot find ShareStudy by shareStudyID " + shareStudyId ) ; LOG . error ( exception . getMessage ( ) ) ; } }
public void test() { try { bytesReceived = offset ; connectionFactory = initializeConnectionFactory ( brokerURL ) ; connection = connectionFactory . createConnection ( ) ; log . info ( "Starting JMS adaptor: " + adaptorID + " started on brokerURL=" + brokerURL + ", topic=" + topic + ", selector=" + selector + ", offset =" + bytesReceived ) ; code_block = IfStatement ; connection . start ( ) ; } catch ( Exception e ) { throw new AdaptorException ( e ) ; } }
public void test() { if ( flinkPod . getPodWithoutMainContainer ( ) . getSpec ( ) . getRestartPolicy ( ) != null ) { logger . info ( "The restart policy of JobManager pod will be overwritten to 'always' " + "since it is controlled by the Kubernetes deployment." ) ; } }
@ Test public void testPostSettingExpectsOK ( ) throws ParseException , IOException { String key = "example_key" ; String value = "example_value" ; JSONObject actual = ( JSONObject ) parser . parse ( given ( ) . formParam ( "key" , key ) . formParam ( "value" , value ) . expect ( ) . statusCode ( HttpStatus . SC_OK ) . contentType ( ContentType . JSON ) . body ( "key" , equalTo ( key ) ) . body ( "value" , equalTo ( value ) ) . when ( ) . post ( rt . host ( "setting" ) ) . asString ( ) ) ; logger . info ( actual . toJSONString ( ) ) ; }
public void test() { try { boot . group ( connectGroup ) . channelFactory ( NioUdtProvider . BYTE_CONNECTOR ) . handler ( new ChannelInitializer < UdtChannel > ( ) code_block = "" ; ) ; channel = boot . connect ( address ) . sync ( ) . channel ( ) ; isRunning = true ; log . info ( "Client ready." ) ; waitForRunning ( false ) ; log . info ( "Client closing..." ) ; channel . close ( ) . sync ( ) ; isShutdown = true ; log . info ( "Client is done." ) ; } catch ( final Throwable e ) { log . error ( "Client failed." , e ) ; } finally { connectGroup . shutdownGracefully ( ) . syncUninterruptibly ( ) ; } }
public void test() { try { boot . group ( connectGroup ) . channelFactory ( NioUdtProvider . BYTE_CONNECTOR ) . handler ( new ChannelInitializer < UdtChannel > ( ) code_block = "" ; ) ; channel = boot . connect ( address ) . sync ( ) . channel ( ) ; isRunning = true ; log . info ( "Client ready." ) ; waitForRunning ( false ) ; log . info ( "Client closing..." ) ; channel . close ( ) . sync ( ) ; isShutdown = true ; log . info ( "Client is done." ) ; } catch ( final Throwable e ) { log . error ( "Client failed." , e ) ; } finally { connectGroup . shutdownGracefully ( ) . syncUninterruptibly ( ) ; } }
public void test() { try { boot . group ( connectGroup ) . channelFactory ( NioUdtProvider . BYTE_CONNECTOR ) . handler ( new ChannelInitializer < UdtChannel > ( ) code_block = "" ; ) ; channel = boot . connect ( address ) . sync ( ) . channel ( ) ; isRunning = true ; log . info ( "Client ready." ) ; waitForRunning ( false ) ; log . info ( "Client closing..." ) ; channel . close ( ) . sync ( ) ; isShutdown = true ; log . info ( "Client is done." ) ; } catch ( final Throwable e ) { log . error ( "Client failed." , e ) ; } finally { connectGroup . shutdownGracefully ( ) . syncUninterruptibly ( ) ; } }
public void test() { try { kylinConfig . getCliCommandExecutor ( ) . execute ( yarnCmd ) ; } catch ( Exception ex ) { logger . warn ( "Failed to get yarn logs. " , ex ) ; } }
public void test() { if ( shouldDoLogCollection ( applicationId , kylinConfig ) ) { File destFile = new File ( destDir , applicationId + ".log" ) ; String yarnCmd = "yarn logs -applicationId " + applicationId + " > " + destFile . getAbsolutePath ( ) ; logger . info ( yarnCmd ) ; code_block = TryStatement ;  } else { logger . info ( "Skip this application {}." , applicationId ) ; } }
@ Test public void test19AnonUpdate ( ) { LOGGER . info ( "  test19AnonUpdate" ) ; Thing thing = THINGS . get ( 0 ) . withOnlyId ( ) ; thing . setDescription ( "Anon Updated Thing made by Admin." ) ; code_block = TryStatement ;  EntityUtils . filterAndCheck ( serviceRead . things ( ) , "" , THINGS ) ; }
public void test() { try { serviceAnon . update ( thing ) ; Assert . fail ( ANON_SHOULD_NOT_BE_ABLE_TO_UPDATE ) ; } catch ( NotAuthorizedException ex ) { LOGGER . trace ( "This should happen." , ex ) ; } catch ( ServiceFailureException ex ) { Assert . fail ( "Expected NotAuthorizedException, got " + ex ) ; } }
public void test() { try { delete ( uuid ) ; data . setUUID ( uuid ) ; add ( data ) ; return data ; } catch ( Exception e ) { LOG . error ( Freedomotic . getStackTraceInfo ( e ) ) ; return null ; } }
void updateCategory ( String categoryUrl , String parentUrl , String name ) throws IOException { log . info ( format ( "Updating category %s to parent %s and name %s" , categoryUrl , parentUrl , name ) ) ; Put request = new Put ( categoryUrl , credentials ) ; request . setAccept ( APPLICATION_JSON ) ; request . addString ( "parent" , parentUrl ) ; request . addString ( "name" , name ) ; String result = request . executeAsString ( ) ; if ( request . isUnAuthorized ( ) ) throw new UnAuthorizedException ( "Not authorized to update category " + name , categoryUrl ) ; if ( request . isForbidden ( ) ) throw new ForbiddenException ( "Forbidden to update category " + name , categoryUrl ) ; if ( request . isNotFound ( ) ) throw new NotFoundException ( "Category not found" , categoryUrl ) ; if ( request . isBadRequest ( ) ) throw new NotOwnerException ( "Not owner of category to update" , categoryUrl ) ; if ( request . isPreconditionFailed ( ) ) throw new DuplicateNameException ( "Category " + name + " already exists" , categoryUrl ) ; if ( ! request . isSuccessful ( ) ) throw new IOException ( "PUT on " + categoryUrl + " with payload " + parentUrl + "/" + name + " not successful: " + result ) ; }
public void test() { if ( logger . isTraceEnabled ( LogMarker . PERSIST_RECOVERY_VERBOSE ) ) { logger . trace ( LogMarker . PERSIST_RECOVERY_VERBOSE , "bad disk region id!" ) ; } else { throw new IllegalStateException ( "bad disk region id" ) ; } }
@ Override public boolean insert ( PostgresPersistenceManager < J > pm , FeatureOfInterest foi ) throws IncompleteEntityException { Map < Field , Object > insert = new HashMap < > ( ) ; insert . put ( table . colName , foi . getName ( ) ) ; insert . put ( table . colDescription , foi . getDescription ( ) ) ; insert . put ( table . colProperties , new JsonValue ( foi . getProperties ( ) ) ) ; String encodingType = foi . getEncodingType ( ) ; insert . put ( table . colEncodingType , encodingType ) ; EntityFactories . insertGeometry ( insert , table . colFeature , table . colGeom , encodingType , foi . getFeature ( ) ) ; entityFactories . insertUserDefinedId ( pm , insert , table . getId ( ) , foi ) ; DSLContext dslContext = pm . getDslContext ( ) ; Record1 < J > result = dslContext . insertInto ( table ) . set ( insert ) . returningResult ( table . getId ( ) ) . fetchOne ( ) ; J generatedId = result . component1 ( ) ; LOGGER . debug ( "Inserted FeatureOfInterest. Created id = {}." , generatedId ) ; foi . setId ( entityFactories . idFromObject ( generatedId ) ) ; return true ; }
@ Test ( timeout = 60 * 1000 ) public void automaticSplitWith250Same ( ) throws Exception { log . info ( "Automatic with 250 with same prefix" ) ; final String tableName = getUniqueNames ( 1 ) [ 0 ] ; code_block = TryStatement ;  }
public void test() { if ( isTrace ) { StringBuilder txt = new StringBuilder ( ) ; txt . append ( "Window ID: " ) . append ( windowId ) . append ( ", Removed values from old map:  " ) . append ( removed ) . append ( ", set " ) . append ( ctr ) . append ( " values." ) ; LOGGER . debug ( txt . toString ( ) ) ; } }
@ Override protected void decode ( ChannelHandlerContext ctx , DatagramPacket msg , List < Object > out ) { LOG . debug ( "OFDatagramPacketFramer" ) ; MessageConsumer consumer = UdpConnectionMap . getMessageConsumer ( msg . sender ( ) ) ; code_block = IfStatement ; ByteBuf bb = msg . content ( ) ; int readableBytes = bb . readableBytes ( ) ; code_block = IfStatement ; int length = bb . getUnsignedShort ( bb . readerIndex ( ) + LENGTH_INDEX_IN_HEADER ) ; LOG . debug ( "length of actual message: {}" , length ) ; code_block = IfStatement ; LOG . debug ( "OF Protocol message received, type:{}" , bb . getByte ( bb . readerIndex ( ) + 1 ) ) ; byte version = bb . readByte ( ) ; code_block = IfStatement ; bb . skipBytes ( bb . readableBytes ( ) ) ; }
public void test() { if ( consumer == null ) { ConnectionFacade connectionFacade = adapterFactory . createConnectionFacade ( ctx . channel ( ) , msg . sender ( ) , false , channelOutboundQueueSize ) ; connectionHandler . onSwitchConnected ( connectionFacade ) ; connectionFacade . checkListeners ( ) ; UdpConnectionMap . addConnection ( msg . sender ( ) , connectionFacade ) ; } }
@ Override protected void decode ( ChannelHandlerContext ctx , DatagramPacket msg , List < Object > out ) { LOG . debug ( "OFDatagramPacketFramer" ) ; MessageConsumer consumer = UdpConnectionMap . getMessageConsumer ( msg . sender ( ) ) ; code_block = IfStatement ; ByteBuf bb = msg . content ( ) ; int readableBytes = bb . readableBytes ( ) ; code_block = IfStatement ; int length = bb . getUnsignedShort ( bb . readerIndex ( ) + LENGTH_INDEX_IN_HEADER ) ; LOG . debug ( "length of actual message: {}" , length ) ; code_block = IfStatement ; LOG . debug ( "OF Protocol message received, type:{}" , bb . getByte ( bb . readerIndex ( ) + 1 ) ) ; byte version = bb . readByte ( ) ; code_block = IfStatement ; bb . skipBytes ( bb . readableBytes ( ) ) ; }
public void test() { if ( version == EncodeConstants . OF13_VERSION_ID || version == EncodeConstants . OF10_VERSION_ID ) { LOG . debug ( "detected version: {}" , version ) ; ByteBuf messageBuffer = bb . slice ( ) ; out . add ( new VersionMessageUdpWrapper ( version , messageBuffer , msg . sender ( ) ) ) ; messageBuffer . retain ( ) ; } else { LOG . warn ( "detected version: {} - currently not supported" , version ) ; } }
public void test() { if ( ! func . getActivationPolicy ( ) . getStatus ( ) ) { logger . debug ( func . getIdentifier ( ) + " is not activated" ) ; return null ; } }
@ Override public Date getDateValue ( TimeUnitValueFunctionality func , Date currentDate , BusinessErrorCode errorCode ) { code_block = IfStatement ; logger . debug ( func . getIdentifier ( ) + " is activated" ) ; Calendar calendar = getCalendarWithoutTime ( timeService . dateNow ( ) ) ; calendar . add ( func . toCalendarValue ( ) , func . getValue ( ) ) ; Date defaultDate = calendar . getTime ( ) ; code_block = IfStatement ; logger . debug ( func . getIdentifier ( ) + " has a delegation policy" ) ; Date now = getCalendarWithoutTime ( timeService . dateNow ( ) ) . getTime ( ) ; code_block = IfStatement ; Calendar c = new GregorianCalendar ( ) ; c . setTime ( now ) ; c . add ( func . toCalendarMaxValue ( ) , func . getMaxValue ( ) ) ; Date maxDate = getCalendarWithoutTime ( c . getTime ( ) ) . getTime ( ) ; code_block = IfStatement ; return currentDate ; }
public void test() { if ( currentDate . before ( now ) || currentDate . after ( maxDate ) ) { String errorMessage = buildErrorMessage ( func , dateFormat . format ( currentDate ) , dateFormat . format ( now ) , dateFormat . format ( maxDate ) ) ; logger . warn ( errorMessage ) ; throw new BusinessException ( errorCode , errorMessage ) ; } }
public void test() { try { Release release = client . getReleaseById ( id , user ) ; putDirectlyLinkedReleaseRelationsInRequest ( request , release ) ; } catch ( TException e ) { log . error ( "Error getting projects!" , e ) ; throw new PortletException ( "cannot get projects" , e ) ; } }
public GetPQValuesResponse dequeueGetPQValuesResponse ( final String correlationUid ) throws OsgpException { LOGGER . debug ( "dequeueGetPQValuesResponse called with correlation uid {}" , correlationUid ) ; return ( GetPQValuesResponse ) this . processResponse ( correlationUid ) ; }
public void test() { try { EventSearchQuery query = new EventSearchQuery ( securityService . getOrganization ( ) . getId ( ) , securityService . getUser ( ) ) ; query . withSeriesId ( seriesId ) ; SearchResult < Event > result = searchIndex . getByQuery ( query ) ; elementsCount = result . getHitCount ( ) ; } catch ( SearchIndexException e ) { logger . warn ( "Could not perform search query" , e ) ; throw new WebApplicationException ( Status . INTERNAL_SERVER_ERROR ) ; } }
public void test() { if ( LOGGER . isInfoEnabled ( ) ) { String generatorMessages = generators . stream ( ) . map ( Generator :: name ) . collect ( Collectors . joining ( ", " ) ) ; LOGGER . info ( "Generator discovery performed, found [{}]" , generatorMessages ) ; } }
public void test() { if ( version != null ) { LOG . debug ( "Bitfinex websocket API version: {}." , version . intValue ( ) ) ; } }
public void test() { if ( message . get ( STATUS ) . textValue ( ) . equals ( BitfinexAuthRequestStatus . FAILED . name ( ) ) ) { LOG . error ( "Authentication error: {}" , message . get ( MESSAGE ) ) ; } }
public void test() { if ( message . get ( STATUS ) . textValue ( ) . equals ( BitfinexAuthRequestStatus . OK . name ( ) ) ) { LOG . info ( "Authenticated successfully" ) ; } }
public void test() { try { String subscriptionUniqueId = getSubscriptionUniqueId ( channel , pair ) ; subscribedChannels . put ( channelId , subscriptionUniqueId ) ; LOG . debug ( "Register channel {}: {}" , subscriptionUniqueId , channelId ) ; } catch ( Exception e ) { LOG . error ( e . getMessage ( ) ) ; } }
public void test() { try { String subscriptionUniqueId = getSubscriptionUniqueId ( channel , pair ) ; subscribedChannels . put ( channelId , subscriptionUniqueId ) ; LOG . debug ( "Register channel {}: {}" , subscriptionUniqueId , channelId ) ; } catch ( Exception e ) { LOG . error ( e . getMessage ( ) ) ; } }
public void test() { if ( message . get ( "code" ) . asInt ( ) == SUBSCRIPTION_FAILED ) { LOG . error ( "Error with message: " + message . get ( "symbol" ) + " " + message . get ( "msg" ) ) ; return ; } }
public void test() { if ( message . get ( "code" ) . asInt ( ) == SUBSCRIPTION_DUP ) { LOG . warn ( "Already subscribed: " + message . toString ( ) ) ; return ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Attempting to acquire leader lease..." ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "The tryAcquireOrRenew result is {}" , success ) ; } }
public void test() { try { Boolean success = future . get ( retryPeriodMillis , TimeUnit . MILLISECONDS ) ; code_block = IfStatement ; acquired . set ( success ) ; } catch ( CancellationException e ) { log . info ( "Processing tryAcquireOrRenew successfully canceled" ) ; } catch ( Throwable t ) { this . exceptionHandler . accept ( t ) ; future . cancel ( true ) ; } finally { maybeReportTransition ( ) ; } }
public void test() { try { code_block = WhileStatement ; } catch ( InterruptedException e ) { log . error ( "LeaderElection acquire loop gets interrupted" , e ) ; return false ; } finally { scheduledFuture . cancel ( true ) ; } }
public void test() { if ( titles . size ( ) > 1 ) { log . warn ( "got multiple titles for document " + id + ", storing first title only" ) ; } }
public void test() { try { writer = new FileWriter ( file ) ; CSVSaveService . saveCSVStats ( getAllDataAsTable ( model , FORMATS , getLabels ( COLUMNS ) ) , writer , saveHeaders . isSelected ( ) ) ; } catch ( IOException e ) { log . warn ( e . getMessage ( ) ) ; } finally { code_block = TryStatement ;  } }
public void test() { try { code_block = IfStatement ; } catch ( IOException ex ) { log . warn ( "There was problem closing file stream" , ex ) ; } }
public void test() { try { type = DataUtilities . createType ( "testCityData" , CITY_ATTRIBUTE + ":String," + STATE_ATTRIBUTE + ":String," + POPULATION_ATTRIBUTE + ":Double," + LAND_AREA_ATTRIBUTE + ":Double," + GEOMETRY_ATTRIBUTE + ":Geometry" ) ; } catch ( final SchemaException e ) { LOGGER . error ( "Unable to create SimpleFeatureType" , e ) ; } }
public void test() { if ( configurableComponent == null ) { logger . warn ( "Unable to get Processor of type {}; its default properties will be fingerprinted instead of being ignored." , className ) ; } }
@ Test public void shouldReturnNotActiveOnUnknownToken ( ) throws Exception { TokensManagement tokensManagement = new MockTokensMan ( ) ; TokenIntrospectionResource tested = createIntrospectionResource ( tokensManagement ) ; setupInvocationContext ( 111 ) ; Response r = tested . introspectToken ( "UNKNOWN-TOKEN" ) ; assertEquals ( HTTPResponse . SC_OK , r . getStatus ( ) ) ; JSONObject parsed = ( JSONObject ) JSONValue . parse ( ( r . getEntity ( ) . toString ( ) ) ) ; log . info ( "{}" , parsed ) ; assertThat ( parsed . getAsString ( "active" ) ) . isEqualTo ( "false" ) ; assertThat ( parsed . size ( ) ) . isEqualTo ( 1 ) ; }
public void test() { try { initialContext = new InitialContext ( ) ; customLogLocation = ( String ) initialContext . lookup ( this . logConfig ) ; code_block = IfStatement ; } catch ( final NamingException | FileNotFoundException | JoranException e ) { this . logger . info ( "Failed to initialize logging using {} or {}" , this . logConfig , defaultLogLocation , e ) ; throw new ServletException ( e ) ; } }
public void test() { if ( line . contains ( "FAILING CHECKS" ) ) { readingFailedChecks = true ; readingMonitorResults = false ; } else-if ( line . contains ( "MONITOR RESULTS" ) ) { readingFailedChecks = false ; readingMonitorResults = true ; } else-if ( readingFailedChecks && ! readingMonitorResults ) { failingChecks . addAll ( getFailingChecks ( line ) ) ; } else-if ( ! readingFailedChecks && readingMonitorResults ) { monitorResults . append ( line ) ; } else { s_logger . error ( "Unexpected lines reached while parsing health check response. Skipping line:- " + line ) ; } }
public void test() { if ( ! killMetaRs && targetServer . equals ( metaServer ) ) { getLogger ( ) . info ( "Not killing server because it holds hbase:meta." ) ; } else { killRs ( targetServer ) ; killedServers . add ( targetServer ) ; } }
public void test() { try { q = this . qm . createQuery ( "select doc.name from Document doc, doc.object(PhenoTips.LabeledIdentifierClass) obj " + "where obj.label = :label and obj.value = :value" , Query . XWQL ) ; q . bindValue ( KEY_LABEL , label ) ; q . bindValue ( KEY_VALUE , id ) ; return q . execute ( ) ; } catch ( QueryException ex ) { this . logger . warn ( "Failed to query patient documents with label [{}] and corresponding external ID [{}]: {}" , label , id , ex . getMessage ( ) , ex ) ; throw new QueryException ( ex . getMessage ( ) , q , ex ) ; } }
public void test() { if ( ! executorService . awaitTermination ( secondsToWaitOnShutdown , TimeUnit . SECONDS ) ) { log . warn ( "Timeout during shutdown of async job executor. " + "The current running jobs could not end within " + secondsToWaitOnShutdown + " seconds after shutdown operation." ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( InterruptedException e ) { log . warn ( "Interrupted while shutting down the async job executor. " , e ) ; } }
public void test() { if ( ! started ) { log . info ( "Loading JGroups form: " + org . jgroups . Version . class . getProtectionDomain ( ) . getCodeSource ( ) . getLocation ( ) ) ; log . info ( "JGroups version: " + org . jgroups . Version . printDescription ( ) ) ; code_block = TryStatement ;  localAddr = ch . getAddress ( ) ; started = true ; } }
public void test() { try ( PubsubClient pubsubClient = pubsubFactory . newClient ( timestampAttribute , idAttribute , options . as ( PubsubOptions . class ) ) ) { SubscriptionPath subscriptionPath = pubsubClient . createRandomSubscription ( projectPath , topicPath , DEAULT_ACK_TIMEOUT_SEC ) ; LOG . warn ( "Created subscription {} to topic {}." + " Note this subscription WILL NOT be deleted when the pipeline terminates" , subscriptionPath , topic ) ; return subscriptionPath ; } }
private void scheduleConnectionMonitorJob ( ) { logger . debug ( "Starting connection monitor job for thing {} at IP {}" , thingID ( ) , commandConnection . getIP ( ) ) ; connectionMonitorJob = scheduler . scheduleWithFixedDelay ( connectionMonitorRunnable , CONNECTION_MONITOR_START_DELAY , CONNECTION_MONITOR_FREQUENCY , TimeUnit . SECONDS ) ; }
public void test() { if ( backfillAttempts > 10 ) { LOG . info ( "Skipping attempt to back-fill plugin metadata after 10 failures." ) ; return ; } }
public void test() { try { this . getPluginCounts ( namespace , apps ) ; } catch ( IOException e ) { updateFailed = true ; LOG . warn ( "Failed to write plugin metadata updates for namespace '{}': {}" , namespace , e ) ; } }
@ Test public void testSubmitForSettlementWithId ( ) throws Exception { assertNotNull ( this . gateway , "BraintreeGateway can't be null" ) ; final Result < Transaction > createResult = requestBody ( "direct://SALE" , new TransactionRequest ( ) . amount ( new BigDecimal ( "100.03" ) ) . paymentMethodNonce ( "fake-valid-nonce" ) . options ( ) . submitForSettlement ( false ) . done ( ) , Result . class ) ; LOG . info ( "Result message: {}" , createResult . getMessage ( ) ) ; assertNotNull ( createResult , "sale result" ) ; assertTrue ( createResult . isSuccess ( ) ) ; LOG . info ( "Transaction done - id={}" , createResult . getTarget ( ) . getId ( ) ) ; this . transactionIds . add ( createResult . getTarget ( ) . getId ( ) ) ; final Result < Transaction > result = requestBody ( "direct://SUBMITFORSETTLEMENT_WITH_ID" , createResult . getTarget ( ) . getId ( ) , Result . class ) ; assertNotNull ( result , "Submit For Settlement result" ) ; LOG . debug ( "Transaction submitted for settlement - id={}" , result . getTarget ( ) . getId ( ) ) ; }
@ Test public void testSubmitForSettlementWithId ( ) throws Exception { assertNotNull ( this . gateway , "BraintreeGateway can't be null" ) ; final Result < Transaction > createResult = requestBody ( "direct://SALE" , new TransactionRequest ( ) . amount ( new BigDecimal ( "100.03" ) ) . paymentMethodNonce ( "fake-valid-nonce" ) . options ( ) . submitForSettlement ( false ) . done ( ) , Result . class ) ; LOG . info ( "Result message: {}" , createResult . getMessage ( ) ) ; assertNotNull ( createResult , "sale result" ) ; assertTrue ( createResult . isSuccess ( ) ) ; LOG . info ( "Transaction done - id={}" , createResult . getTarget ( ) . getId ( ) ) ; this . transactionIds . add ( createResult . getTarget ( ) . getId ( ) ) ; final Result < Transaction > result = requestBody ( "direct://SUBMITFORSETTLEMENT_WITH_ID" , createResult . getTarget ( ) . getId ( ) , Result . class ) ; assertNotNull ( result , "Submit For Settlement result" ) ; LOG . debug ( "Transaction submitted for settlement - id={}" , result . getTarget ( ) . getId ( ) ) ; }
@ Test public void testSubmitForSettlementWithId ( ) throws Exception { assertNotNull ( this . gateway , "BraintreeGateway can't be null" ) ; final Result < Transaction > createResult = requestBody ( "direct://SALE" , new TransactionRequest ( ) . amount ( new BigDecimal ( "100.03" ) ) . paymentMethodNonce ( "fake-valid-nonce" ) . options ( ) . submitForSettlement ( false ) . done ( ) , Result . class ) ; LOG . info ( "Result message: {}" , createResult . getMessage ( ) ) ; assertNotNull ( createResult , "sale result" ) ; assertTrue ( createResult . isSuccess ( ) ) ; LOG . info ( "Transaction done - id={}" , createResult . getTarget ( ) . getId ( ) ) ; this . transactionIds . add ( createResult . getTarget ( ) . getId ( ) ) ; final Result < Transaction > result = requestBody ( "direct://SUBMITFORSETTLEMENT_WITH_ID" , createResult . getTarget ( ) . getId ( ) , Result . class ) ; assertNotNull ( result , "Submit For Settlement result" ) ; LOG . debug ( "Transaction submitted for settlement - id={}" , result . getTarget ( ) . getId ( ) ) ; }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Restored universe: " + Thread . currentThread ( ) ) ; } }
public void test() { switch ( device . getStatus ( ) ) { case JNA . CLibrary . TELLSTICK_TURNON : st = OnOffType . ON ; break ; case JNA . CLibrary . TELLSTICK_TURNOFF : st = OnOffType . OFF ; break ; case JNA . CLibrary . TELLSTICK_DIM : BigDecimal dimValue = new BigDecimal ( device . getData ( ) ) ; code_block = IfStatement ; break ; default : logger . warn ( "Could not handle {} for {}" , device . getStatus ( ) , device ) ; } }
public void test() { for ( ProcessDefinition pd : processDefinitionPage . getContent ( ) ) { logger . info ( "\t > Process definition: " + pd ) ; } }
public void test() { try { Method fhLimitMethod = sunBeanClass . getMethod ( "getMaxFileDescriptorCount" ) ; Object result = fhLimitMethod . invoke ( ManagementFactory . getOperatingSystemMXBean ( ) ) ; return ( Long ) result ; } catch ( Throwable t ) { LOG . warn ( "Unexpected error when accessing file handle limit" , t ) ; return - 1L ; } }
public void test() { if ( s_logger . isInfoEnabled ( ) ) { s_logger . info ( "Executing resource DestroyCommand to evict template from storage pool: " + getHumanReadableBytesJson ( _gson . toJson ( cmd ) ) ) ; } }
public void test() { if ( s_logger . isInfoEnabled ( ) ) { s_logger . info ( "Destroy template volume " + vol . getPath ( ) ) ; } }
public void test() { if ( s_logger . isInfoEnabled ( ) ) { s_logger . info ( "Template volume " + vol . getPath ( ) + " is not found, no need to delete." ) ; } }
public void test() { if ( e instanceof RemoteException ) { s_logger . warn ( "Encounter remote exception to vCenter, invalidate VMware session context" ) ; invalidateServiceContext ( null ) ; } }
public void test() { try { VmwareContext context = getServiceContext ( null ) ; VmwareHypervisorHost hyperHost = getHyperHost ( context , null ) ; VolumeTO vol = cmd . getVolume ( ) ; VirtualMachineMO vmMo = findVmOnDatacenter ( context , hyperHost , vol ) ; code_block = IfStatement ; return new Answer ( cmd , true , "Success" ) ; } catch ( Throwable e ) { code_block = IfStatement ; String msg = "DestroyCommand failed due to " + VmwareHelper . getExceptionMessage ( e ) ; s_logger . error ( msg , e ) ; return new Answer ( cmd , false , msg ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "{}: Sending error message header type: {} transaction: {}" , servConn . getName ( ) , messageType , origMsg . getTransactionId ( ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "{}: Sending error message chunk: {}" , servConn . getName ( ) , message ) ; } }
public void test() { try { String mpId = mp . getIdentifier ( ) . toString ( ) ; Snapshot snapshot = getSnapshot ( mpId ) ; MediaPackage archivedMp = snapshot . getMediaPackage ( ) ; removeLivePublicationChannel ( archivedMp ) ; logger . debug ( "Removed live pub channel from archived media package {}" , mp ) ; snapshotVersionCache . put ( mpId , assetManager . takeSnapshot ( archivedMp ) . getVersion ( ) ) ; } catch ( LiveScheduleException e ) { } }
public void test() { try { StoragePath metadataPath = DefaultStoragePath . parse ( repIterator . next ( ) . getStoragePath ( ) , RodaConstants . STORAGE_DIRECTORY_METADATA , RodaConstants . STORAGE_DIRECTORY_PRESERVATION ) ; code_block = IfStatement ; } catch ( RODAException e ) { LOGGER . error ( "Could not list resources under representation data directory" , e ) ; } }
public void test() { try { StoragePath repPath = DefaultStoragePath . parse ( aipIteratorSub . next ( ) . getStoragePath ( ) , RodaConstants . STORAGE_DIRECTORY_REPRESENTATIONS ) ; code_block = IfStatement ; } catch ( RODAException e ) { LOGGER . error ( "Could not list resources under AIP" , e ) ; } }
public void test() { if ( context . isRestored ( ) ) { code_block = IfStatement ; } else { LOG . info ( "No restore state for FlinkKinesisConsumer." ) ; } }
public void test() { if ( getGeometryHandler ( ) . isSpatialDatasource ( ) ) { c . add ( SpatialRestrictions . filter ( DataEntity . PROPERTY_GEOMETRY_ENTITY , ( ( GetObservationRequest ) request ) . getSpatialFilter ( ) . getOperator ( ) , getGeometryHandler ( ) . switchCoordinateAxisFromToDatasourceIfNeeded ( ( ( GetObservationRequest ) request ) . getSpatialFilter ( ) . getGeometry ( ) ) ) ) ; logArgs . append ( ", spatialFilter" ) ; } else { LOGGER . warn ( "Spatial filtering for lat/lon is not yet implemented!" ) ; } }
@ Override public void onContainerStarted ( ContainerId containerId , Map < String , ByteBuffer > allServiceResponse ) { LOG . trace ( "NM: Container started: " + containerId ) ; controller . containerStarted ( containerId ) ; }
@ Override public void execute ( Runnable command ) { LOG . info ( "Adding command {}" , command ) ; this . runnables . add ( command ) ; }
public void test() { try { code_block = IfStatement ; return null ; } catch ( Exception ex ) { log . error ( "Failed to prepare JSON from ImportPersonConfig: '{}'" , oxTrustImportPersonConfiguration , ex ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "==> RangerKeyStoreProvider.getFromJceks()" ) ; } }
public void test() { if ( xaDBPassword != null && ! xaDBPassword . trim ( ) . isEmpty ( ) && ! xaDBPassword . trim ( ) . equalsIgnoreCase ( "none" ) ) { conf . set ( key , xaDBPassword ) ; } else { logger . info ( "Credential keystore password not applied for KMS; clear text password shall be applicable" ) ; } }
@ Override public void query ( String deviceId , String startTs , String endTs , String notification , String sortField , String sortOrderSt , Integer take , Integer skip , @ Suspended final AsyncResponse asyncResponse ) { logger . debug ( "Device notification query requested for device {}" , deviceId ) ; final Date timestampSt = TimestampQueryParamParser . parse ( startTs ) ; final Date timestampEnd = TimestampQueryParamParser . parse ( endTs ) ; DeviceVO byIdWithPermissionsCheck = deviceService . findById ( deviceId ) ; code_block = IfStatement ; }
public void test() { if ( ! hooked . isEmpty ( ) ) { final StringBuilder string = new StringBuilder ( ) ; code_block = ForStatement ; final String plugins = string . substring ( 0 , string . length ( ) - 2 ) ; LOG . info ( null , "Hooked into " + plugins + "!" ) ; } }
public void test() { try { retryTimer . newTimeout ( timerTask , 1000 , TimeUnit . MILLISECONDS ) ; } catch ( RejectedExecutionException e ) { logger . debug ( "retry fail {}" , e . getCause ( ) , e ) ; } }
public void attachClean ( StgG20SclMapping instance ) { log . debug ( "attaching clean StgG20SclMapping instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . lock ( instance , LockMode . NONE ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void passivateObject ( LdapConnection connection ) { log . debug ( "passivate connection: {}" , connection ) ; }
public void test() { if ( body != null ) { result = policyChecker . checkIncomingPolicy ( body , assertion ) ; } else { LOG . warn ( "Admin Dist request body was null" ) ; } }
public void test() { try ( InputStream input = CamelKafkaConnectorCatalog . class . getResourceAsStream ( "/" + DESCRIPTORS_DIR + "/" + CONNECTORS_PROPERTIES ) ) { BufferedReader reader = new BufferedReader ( new InputStreamReader ( input ) ) ; code_block = WhileStatement ; } catch ( FileNotFoundException e ) { LOG . error ( "Cannot find file: {}" , e . getMessage ( ) , e ) ; } catch ( IOException e ) { LOG . error ( "IO Exception: {}" , e . getMessage ( ) , e ) ; } }
public void test() { try ( InputStream input = CamelKafkaConnectorCatalog . class . getResourceAsStream ( "/" + DESCRIPTORS_DIR + "/" + CONNECTORS_PROPERTIES ) ) { BufferedReader reader = new BufferedReader ( new InputStreamReader ( input ) ) ; code_block = WhileStatement ; } catch ( FileNotFoundException e ) { LOG . error ( "Cannot find file: {}" , e . getMessage ( ) , e ) ; } catch ( IOException e ) { LOG . error ( "IO Exception: {}" , e . getMessage ( ) , e ) ; } }
public void testCreateTemporaryQueueThenCreateAQueueFromItsName ( ) throws Exception { Session session = connection . createSession ( false , Session . AUTO_ACKNOWLEDGE ) ; Queue tempQueue = session . createTemporaryQueue ( ) ; String name = tempQueue . getQueueName ( ) ; LOG . info ( "Created queue named: " + name ) ; Queue createdQueue = session . createQueue ( name ) ; assertEquals ( "created queue not equal to temporary queue" , tempQueue , createdQueue ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Processing of event listener yielded." ) ; } }
public void test() { try { executor . execute ( this ) ; code_block = IfStatement ; } catch ( RejectedExecutionException e ) { logger . info ( "Processing of event listener could not yield. Executor refused the task." ) ; return false ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "http response:\n" + response ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( ade . getMessage ( ) ) ; } }
private static void move ( InterpretationPipelineOptions options , String from , String to ) { String targetPath = options . getTargetPath ( ) ; String deletePath = PathBuilder . buildPath ( targetPath , to , options . getDatasetId ( ) + "_*" ) . toString ( ) ; log . info ( "Deleting avro files {}" , deletePath ) ; FsUtils . deleteByPattern ( options . getHdfsSiteConfig ( ) , options . getCoreSiteConfig ( ) , targetPath , deletePath ) ; String filter = PathBuilder . buildFilePathViewUsingInputPath ( options , from , "*.avro" ) ; String movePath = PathBuilder . buildPath ( targetPath , to ) . toString ( ) ; log . info ( "Moving files with pattern {} to {}" , filter , movePath ) ; FsUtils . moveDirectory ( options . getHdfsSiteConfig ( ) , options . getCoreSiteConfig ( ) , movePath , filter ) ; log . info ( "Files moved to {} directory" , movePath ) ; }
private static void move ( InterpretationPipelineOptions options , String from , String to ) { String targetPath = options . getTargetPath ( ) ; String deletePath = PathBuilder . buildPath ( targetPath , to , options . getDatasetId ( ) + "_*" ) . toString ( ) ; log . info ( "Deleting avro files {}" , deletePath ) ; FsUtils . deleteByPattern ( options . getHdfsSiteConfig ( ) , options . getCoreSiteConfig ( ) , targetPath , deletePath ) ; String filter = PathBuilder . buildFilePathViewUsingInputPath ( options , from , "*.avro" ) ; String movePath = PathBuilder . buildPath ( targetPath , to ) . toString ( ) ; log . info ( "Moving files with pattern {} to {}" , filter , movePath ) ; FsUtils . moveDirectory ( options . getHdfsSiteConfig ( ) , options . getCoreSiteConfig ( ) , movePath , filter ) ; log . info ( "Files moved to {} directory" , movePath ) ; }
private static void move ( InterpretationPipelineOptions options , String from , String to ) { String targetPath = options . getTargetPath ( ) ; String deletePath = PathBuilder . buildPath ( targetPath , to , options . getDatasetId ( ) + "_*" ) . toString ( ) ; log . info ( "Deleting avro files {}" , deletePath ) ; FsUtils . deleteByPattern ( options . getHdfsSiteConfig ( ) , options . getCoreSiteConfig ( ) , targetPath , deletePath ) ; String filter = PathBuilder . buildFilePathViewUsingInputPath ( options , from , "*.avro" ) ; String movePath = PathBuilder . buildPath ( targetPath , to ) . toString ( ) ; log . info ( "Moving files with pattern {} to {}" , filter , movePath ) ; FsUtils . moveDirectory ( options . getHdfsSiteConfig ( ) , options . getCoreSiteConfig ( ) , movePath , filter ) ; log . info ( "Files moved to {} directory" , movePath ) ; }
public void test() { try { JAXBContext context = JAXBContext . newInstance ( ActivityStreamInfo . class ) ; Unmarshaller unmarshaller = context . createUnmarshaller ( ) ; ByteArrayInputStream is = new ByteArrayInputStream ( xml . getBytes ( StandardCharsets . UTF_8 ) ) ; bodyObject = ( ActivityStreamInfo ) unmarshaller . unmarshal ( is ) ; } catch ( Throwable t ) { _logger . error ( "Error unmarshalling activity stream info config. xml: {}" , xml , t ) ; throw new ApsSystemException ( "Error unmarshalling activity stream info config" , t ) ; } }
public void init ( ) { LOG . info ( "==> RangerChainedPlugin.init(" + serviceType + ", " + serviceName + ")" ) ; this . plugin . init ( ) ; LOG . info ( "<== RangerChainedPlugin.init(" + serviceType + ", " + serviceName + ")" ) ; }
public void init ( ) { LOG . info ( "==> RangerChainedPlugin.init(" + serviceType + ", " + serviceName + ")" ) ; this . plugin . init ( ) ; LOG . info ( "<== RangerChainedPlugin.init(" + serviceType + ", " + serviceName + ")" ) ; }
public void test() { try { queue . push ( message ) ; } catch ( MessageQueueOverflowException e ) { LOG . error ( "Message queue overflow, dropping message: " + e . getMessage ( ) ) ; } }
private MessageEvent unexpectedError ( Exception exception ) { LOG . error ( "Unable to execute query : " + exception . toString ( ) ) ; MessageEvent msg = new MessageEvent ( MessageEventEnum . DATA_OPERATION_ERROR_UNEXPECTED ) ; msg . setDescription ( msg . getDescription ( ) . replace ( "%DESCRIPTION%" , exception . toString ( ) ) ) ; return msg ; }
@ Override public void executeImpl ( DelegateExecution execution ) throws Exception { String contentTypeString = activitiHelper . getRequiredExpressionVariableAsString ( contentType , execution , "ContentType" ) . trim ( ) ; String requestString = activitiHelper . getRequiredExpressionVariableAsString ( businessObjectDataStorageFilesCreateRequest , execution , "BusinessObjectDataCreateRequest" ) . trim ( ) ; BusinessObjectDataStorageFilesCreateRequest request = getRequestObject ( contentTypeString , requestString , BusinessObjectDataStorageFilesCreateRequest . class ) ; LOGGER . info ( "inside4 AddBusinessObjectDataStorageFiles" ) ; BusinessObjectDataStorageFilesCreateResponse businessObjectDataStorageFilesCreateResponse = businessObjectDataStorageFileService . createBusinessObjectDataStorageFiles ( request ) ; setJsonResponseAsWorkflowVariable ( businessObjectDataStorageFilesCreateResponse , execution ) ; }
@ Override public void clearCache ( ) { LOGGER . debug ( "Invalidate cache." ) ; aclClassCache . invalidateAll ( ) ; }
public List findByExample ( StgMUmsetzStatTxt instance ) { log . debug ( "finding StgMUmsetzStatTxt instance by example" ) ; code_block = TryStatement ;  }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.StgMUmsetzStatTxt" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.StgMUmsetzStatTxt" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
public void test() { for ( Integer bookieIndex : bookieIndexesToRereplicate ) { BookieId bookie = ensemble . get ( bookieIndex ) ; bookiesToExclude . add ( bookie ) ; } }
public void test() { if ( fs . exists ( path ) == false ) { logger . debug ( "Ignoring non-existent path assuming replay : {}" , path ) ; return ; } }
public void test() { if ( fs . exists ( path ) ) { PutObjectResult result = s3client . putObject ( request ) ; logger . debug ( "File {} Uploaded at {}" , keyName , result . getETag ( ) ) ; } }
public void test() { try { Path path = new Path ( outputMetaData . getPath ( ) ) ; code_block = IfStatement ; FSDataInputStream fsinput = fs . open ( path ) ; ObjectMetadata omd = new ObjectMetadata ( ) ; omd . setContentLength ( outputMetaData . getSize ( ) ) ; String keyName = directoryName + Path . SEPARATOR + outputMetaData . getFileName ( ) ; PutObjectRequest request = new PutObjectRequest ( bucketName , keyName , fsinput , omd ) ; code_block = IfStatement ; code_block = IfStatement ; } catch ( FileNotFoundException e ) { logger . debug ( "Ignoring non-existent path assuming replay : {}" , outputMetaData . getPath ( ) ) ; } catch ( IOException e ) { logger . error ( "Unable to create Stream: {}" , e . getMessage ( ) ) ; } }
public void test() { try { Path path = new Path ( outputMetaData . getPath ( ) ) ; code_block = IfStatement ; FSDataInputStream fsinput = fs . open ( path ) ; ObjectMetadata omd = new ObjectMetadata ( ) ; omd . setContentLength ( outputMetaData . getSize ( ) ) ; String keyName = directoryName + Path . SEPARATOR + outputMetaData . getFileName ( ) ; PutObjectRequest request = new PutObjectRequest ( bucketName , keyName , fsinput , omd ) ; code_block = IfStatement ; code_block = IfStatement ; } catch ( FileNotFoundException e ) { logger . debug ( "Ignoring non-existent path assuming replay : {}" , outputMetaData . getPath ( ) ) ; } catch ( IOException e ) { logger . error ( "Unable to create Stream: {}" , e . getMessage ( ) ) ; } }
public void test() { try { systemJobManager . submit ( indexSetCleanupJobFactory . create ( indexSet ) ) ; } catch ( SystemJobConcurrencyException e ) { LOG . error ( "Error running system job" , e ) ; } }
@ SuppressWarnings ( "unused" ) private void testScanRaw ( String msg ) throws IOException { long t = System . currentTimeMillis ( ) ; IGTScanner scan = simpleStore . scan ( new GTScanRequestBuilder ( ) . setInfo ( info ) . setRanges ( null ) . setDimensions ( null ) . setFilterPushDown ( null ) . createGTScanRequest ( ) ) ; ResultScanner innerScanner = ( ( SimpleHBaseStore . Reader ) scan ) . getHBaseScanner ( ) ; int count = 0 ; code_block = ForStatement ; scan . close ( ) ; t = System . currentTimeMillis ( ) - t ; logger . info ( msg + ", " + count + " rows, " + speed ( t ) + "K row/sec" ) ; }
public void onGroupRenamed ( UDGroup group ) { Isy99iFrame . writeAreaLog ( Isy99iUtilities . getDateTime ( ) + ": Scene: " + group . address + " was removed by someone or something!" ) ; Freedomotic . logger . info ( "Scene: " + group . address + " was renamed to " + group . name ) ; }
@ RequestMapping ( "/experimentoverview" ) public @ ResponseBody String experimentoverview ( @ RequestParam ( value = "experimentType" ) String experimentType , @ RequestParam ( value = "matching" ) String matchingString ) { LOGGER . debug ( "Got request on /experimentoverview(experimentType={}, matching={}" , experimentType , matchingString ) ; Matching matching = MainController . getMatching ( matchingString ) ; ExperimentType eType = ExperimentType . valueOf ( experimentType ) ; String annotatorNames [ ] = loadAnnotators ( eType ) ; String datasetNames [ ] = loadDatasets ( eType ) ; double results [ ] [ ] = loadLatestResults ( eType , matching , annotatorNames , datasetNames ) ; double correlations [ ] [ ] = calculateCorrelations ( results , datasetNames ) ; return generateJson ( results , correlations , annotatorNames , datasetNames ) ; }
public void test() { try { code_block = ForStatement ; addRemoveInternetScsiTargetsToAllHosts ( false , targetsToRemove , hosts ) ; rescanAllHosts ( hosts , true , false ) ; } catch ( Exception ex ) { s_logger . warn ( ex . getMessage ( ) ) ; } }
public void test() { -> { log . warn ( "No pong from server " + remoteNodeId + " - will consider it dead" ) ; close ( ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( DDMTemplateServiceUtil . class , "getTemplatesByClassPK" , _getTemplatesByClassPKParameterTypes16 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , companyId , groupId , classPK , resourceClassNameId , status ) ; Object returnObj = null ; code_block = TryStatement ;  return ( java . util . List < com . liferay . dynamic . data . mapping . model . DDMTemplate > ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { prepare ( 2 ) ; FakeAllocatableAction instance0 = new FakeAllocatableAction ( fao , 0 ) ; FakeAllocatableAction instance1 = new FakeAllocatableAction ( fao , 1 ) ; instance0 . assignResource ( rs ) ; instance1 . assignResource ( rs ) ; instance0 . tryToLaunch ( ) ; error ( instance0 ) ; checkExecutions ( new int [ ] code_block = "" ; ) ; checkErrors ( new int [ ] code_block = "" ; ) ; checkFailed ( new int [ ] code_block = "" ; ) ; checkCancelled ( new int [ ] code_block = "" ; ) ; instance0 . assignResource ( rs ) ; instance0 . tryToLaunch ( ) ; error ( instance0 ) ; checkExecutions ( new int [ ] code_block = "" ; ) ; checkErrors ( new int [ ] code_block = "" ; ) ; checkFailed ( new int [ ] code_block = "" ; ) ; checkCancelled ( new int [ ] code_block = "" ; ) ; } catch ( Throwable e ) { LOGGER . error ( e ) ; fail ( e . getMessage ( ) ) ; } }
public void test() { try ( final LockedDocument lockedDoc = broker . getXMLResource ( getTarget ( ) , Lock . LockMode . WRITE_LOCK ) ) { final DocumentImpl doc = lockedDoc . getDocument ( ) ; final Permission permission = doc . getPermissions ( ) ; PermissionFactory . chown ( broker , permission , Optional . ofNullable ( getOwner ( ) ) , Optional . ofNullable ( getGroup ( ) ) ) ; PermissionFactory . chmod ( broker , permission , Optional . of ( getMode ( ) ) , Optional . ofNullable ( permission instanceof ACLPermission ? getAces ( ) : null ) ) ; broker . storeXMLResource ( transaction , doc ) ; } catch ( final PermissionDeniedException e ) { final String msg = "ERROR: Failed to set permissions on Document '" + getTarget ( ) + "'." ; LOG . error ( msg , e ) ; getListener ( ) . warn ( msg ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( String . format ( "Calculate checksum with header %s" , header ) ) ; } }
public void test() { if ( configuration . getApi ( ) . isDisableApiServer ( ) ) { logger . info ( "Not starting the API..." ) ; return ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Output from Server .... \n" ) ; } }
public void test() { try { transaction . rollback ( ) ; } catch ( RuntimeException ex ) { LOG . error ( "Transaction rollback failed: " + ex . getLocalizedMessage ( ) ) ; LOG . debug ( "Exception follows." , ex ) ; } finally { transaction . close ( ) ; this . transaction = null ; } }
public void test() { for ( ContainerBlockInfo containerBlockInfo : containers ) { containerBlockInfos = new BlockDeletingTask ( containerBlockInfo . containerData , TASK_PRIORITY_DEFAULT , containerBlockInfo . numBlocksToDelete ) ; queue . add ( containerBlockInfos ) ; totalBlocks += containerBlockInfo . numBlocksToDelete ; } }
public void test() { try { containers = chooseContainerForBlockDeletion ( blockLimitPerInterval , containerDeletionPolicy ) ; BlockDeletingTask containerBlockInfos = null ; long totalBlocks = 0 ; code_block = ForStatement ; code_block = IfStatement ; } catch ( StorageContainerException e ) { LOG . warn ( "Failed to initiate block deleting tasks, " + "caused by unable to get containers info. " + "Retry in next interval. " , e ) ; } catch ( Exception e ) { code_block = IfStatement ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Unexpected error occurs during deleting blocks." , e ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( sqlException , sqlException ) ; } }
public void test() { if ( indexTaskSpecs . isEmpty ( ) ) { log . warn ( "Can't find segments from inputSpec[%s], nothing to do." , ioConfig . getInputSpec ( ) ) ; return TaskStatus . failure ( getId ( ) ) ; } else { registerResourceCloserOnAbnormalExit ( currentSubTaskHolder ) ; final int totalNumSpecs = indexTaskSpecs . size ( ) ; log . info ( "Generated [%d] compaction task specs" , totalNumSpecs ) ; int failCnt = 0 ; code_block = ForStatement ; log . info ( "Run [%d] specs, [%d] succeeded, [%d] failed" , totalNumSpecs , totalNumSpecs - failCnt , failCnt ) ; return failCnt == 0 ? TaskStatus . success ( getId ( ) ) : TaskStatus . failure ( getId ( ) ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { failCnt ++ ; log . warn ( e , "Failed to run indexSpec: [%s].\nTrying the next indexSpec." , json ) ; } }
public void test() { try { FileUtils . deleteDirectory ( parent ) ; LOG . info ( "Deleted directory: " + parent . getAbsolutePath ( ) ) ; } catch ( IOException e ) { LOG . error ( "Failed to delete directory " + parent . getAbsolutePath ( ) + ": " + e . getMessage ( ) , e ) ; } }
public void test() { if ( log != null && log . isInfoEnabled ( ) ) { log . info ( "Rollover segment [" + idx + " to " + res . getSegmentId ( ) + "], recordType=" + ( rec == null ? null : rec . type ( ) ) ) ; } }
public void test() { try { LOG . info ( "Monitoring thread(s) !!" ) ; future . get ( ) ; } catch ( ExecutionException ex ) { LOG . error ( "Monitor noticed one or more threads failed. Requesting graceful shutdown of other threads" , ex ) ; error = true ; } catch ( InterruptedException ie ) { LOG . error ( "Got interrupted Monitoring threads" , ie ) ; error = true ; } finally { shutdown = true ; code_block = IfStatement ; shutdown ( false ) ; } }
public void test() { try { LOG . info ( "Monitoring thread(s) !!" ) ; future . get ( ) ; } catch ( ExecutionException ex ) { LOG . error ( "Monitor noticed one or more threads failed. Requesting graceful shutdown of other threads" , ex ) ; error = true ; } catch ( InterruptedException ie ) { LOG . error ( "Got interrupted Monitoring threads" , ie ) ; error = true ; } finally { shutdown = true ; code_block = IfStatement ; shutdown ( false ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( String . format ( "==> RangerServiceDefValidator.isValueUnique(%s, %s, %s, %s, %s)" , value , alreadySeen , valueName , collectionName , failures ) ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( String . format ( "==> RangerServiceDefValidator.isValueUnique(%s, %s, %s, %s, %s): %s" , value , alreadySeen , valueName , collectionName , failures , valid ) ) ; } }
public void test() { if ( logger . isFineEnabled ( ) ) { logger . info ( leavingMember + " will be removed from " + changes ) ; } else { logger . info ( leavingMember + " will be removed from " + leavingGroupIds ) ; } }
public void test() { if ( logger . isFineEnabled ( ) ) { logger . info ( leavingMember + " will be removed from " + changes ) ; } else { logger . info ( leavingMember + " will be removed from " + leavingGroupIds ) ; } }
@ Before public void setUp ( ) throws Exception { IOHelper . deleteFile ( schedulerStoreDir ) ; LOG . info ( "Test Dir = {}" , schedulerStoreDir ) ; createBroker ( ) ; broker . start ( ) ; broker . waitUntilStarted ( ) ; schedulerStore = ( JobSchedulerStoreImpl ) broker . getJobSchedulerStore ( ) ; }
public void test() { try { server . start ( ) ; server . join ( ) ; } catch ( Exception e ) { logger . error ( "Failed to start application" , e ) ; } }
public void test() { try { OAuth2RestOperations restTemplate = this . restTemplate ; code_block = IfStatement ; OAuth2AccessToken existingToken = restTemplate . getOAuth2ClientContext ( ) . getAccessToken ( ) ; code_block = IfStatement ; return restTemplate . getForEntity ( path , Map . class ) . getBody ( ) ; } catch ( Exception ex ) { this . logger . info ( "Could not fetch user details: " + ex . getClass ( ) + ", " + ex . getMessage ( ) ) ; return Collections . < String , Object > singletonMap ( "error" , "Could not fetch user details" ) ; } }
@ Override public boolean isReadOnly ( ) { logger . info ( "x" ) ; return false ; }
public void test() { try { System . setProperty ( "groovy.root" , new File ( this . environmentProvider . get ( ) . getPermanentDirectory ( ) , "cache/groovy" ) . getAbsolutePath ( ) ) ; } catch ( Exception e ) { this . logger . debug ( "No registered environment, keep default Groovy setup" , e ) ; } }
@ Override public void refresh ( Collection < Refreshable > alreadyRefreshed ) { log . debug ( "Reloading..." ) ; reload ( ) ; }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { LOG . error ( "init schedule task failed !" ) ; throw new RuntimeException ( e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( DLFileEntryServiceUtil . class , "deleteFileVersion" , _deleteFileVersionParameterTypes10 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , fileEntryId , version ) ; code_block = TryStatement ;  } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( count != contentRows . size ( ) ) { String errmsg = String . format ( "Wrong number of contents migrated. Expected: %s, Inserted: %s" , contentRows . size ( ) , count ) ; this . logger . error ( errmsg ) ; throw new DatabaseException ( errmsg ) ; } }
public void test() { if ( currentStateEnum == ZoneStateEnum . ENROLLED ) { logger . debug ( "{}: IAS CIE is already enrolled" , iasZoneCluster . getZigBeeAddress ( ) ) ; return ; } }
public void test() { if ( currentState != null ) { ZoneStateEnum currentStateEnum = ZoneStateEnum . getByValue ( currentState ) ; logger . debug ( "{}: IAS CIE state is currently {}[{}]" , iasZoneCluster . getZigBeeAddress ( ) , currentStateEnum , currentState ) ; code_block = IfStatement ; } else { logger . debug ( "{}: IAS CIE failed to get state" , iasZoneCluster . getZigBeeAddress ( ) ) ; } }
private void initialise ( ) { Integer currentState = ( Integer ) iasZoneCluster . getAttribute ( ZclIasZoneCluster . ATTR_ZONESTATE ) . readValue ( Long . MAX_VALUE ) ; code_block = IfStatement ; ZclAttribute cieAddressAttribute = iasZoneCluster . getAttribute ( ZclIasZoneCluster . ATTR_IASCIEADDRESS ) ; IeeeAddress currentIeeeAddress = ( IeeeAddress ) cieAddressAttribute . readValue ( 0 ) ; logger . debug ( "{}: IAS CIE address is currently {}" , iasZoneCluster . getZigBeeAddress ( ) , currentIeeeAddress ) ; code_block = IfStatement ; Integer currentZone = ( Integer ) iasZoneCluster . getAttribute ( ZclIasZoneCluster . ATTR_ZONEID ) . readValue ( 0 ) ; code_block = IfStatement ; zoneType = ( Integer ) iasZoneCluster . getAttribute ( ZclIasZoneCluster . ATTR_ZONETYPE ) . readValue ( Long . MAX_VALUE ) ; code_block = IfStatement ; final Runnable runnableTask = new AutoEnrollmentTask ( ) ; autoEnrollmentTask = networkManager . scheduleTask ( runnableTask , autoEnrollDelay ) ; }
public void test() { if ( ieeeAddress . equals ( currentIeeeAddress ) ) { logger . debug ( "{}: IAS CIE address is confirmed {}" , iasZoneCluster . getZigBeeAddress ( ) , currentIeeeAddress ) ; } else { logger . warn ( "{}: IAS CIE address is NOT confirmed {}" , iasZoneCluster . getZigBeeAddress ( ) , currentIeeeAddress ) ; } }
public void test() { if ( ieeeAddress . equals ( currentIeeeAddress ) ) { logger . debug ( "{}: IAS CIE address is confirmed {}" , iasZoneCluster . getZigBeeAddress ( ) , currentIeeeAddress ) ; } else { logger . warn ( "{}: IAS CIE address is NOT confirmed {}" , iasZoneCluster . getZigBeeAddress ( ) , currentIeeeAddress ) ; } }
public void test() { if ( currentZone == null ) { logger . debug ( "{}: IAS CIE zone ID request failed" , iasZoneCluster . getZigBeeAddress ( ) ) ; } else { logger . debug ( "{}: IAS CIE zone ID is currently {}" , iasZoneCluster . getZigBeeAddress ( ) , currentZone ) ; } }
public void test() { if ( currentZone == null ) { logger . debug ( "{}: IAS CIE zone ID request failed" , iasZoneCluster . getZigBeeAddress ( ) ) ; } else { logger . debug ( "{}: IAS CIE zone ID is currently {}" , iasZoneCluster . getZigBeeAddress ( ) , currentZone ) ; } }
public void test() { if ( zoneType == null ) { logger . debug ( "{}: IAS CIE zone type request failed" , iasZoneCluster . getZigBeeAddress ( ) ) ; } else { logger . debug ( "{}: IAS CIE zone type is 0x{}, {}" , iasZoneCluster . getZigBeeAddress ( ) , String . format ( "%04X" , zoneType ) , ZoneTypeEnum . getByValue ( zoneType ) ) ; } }
public void test() { if ( zoneType == null ) { logger . debug ( "{}: IAS CIE zone type request failed" , iasZoneCluster . getZigBeeAddress ( ) ) ; } else { logger . debug ( "{}: IAS CIE zone type is 0x{}, {}" , iasZoneCluster . getZigBeeAddress ( ) , String . format ( "%04X" , zoneType ) , ZoneTypeEnum . getByValue ( zoneType ) ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Transport not those allowed: {}" , allowedTransports ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Transport not those allowed: {}" , allowedTransports ) ; } }
public void test() { try { Map < String , Object > parameters = new HashMap < > ( ) ; parameters . put ( USECASE , usecase ) ; AutoMLConfig result = getSingleResult ( "FROM AutoMLConfig AC WHERE AC.usecase = :usecase" , AutoMLConfig . class , parameters ) ; return result . getPredictionColumn ( ) ; } catch ( Exception e ) { log . error ( e . getMessage ( ) ) ; throw e ; } }
public void test() { try { socket . close ( ) ; } catch ( Throwable t ) { logger . warn ( "Failed to close a socket." , t ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( exception , exception ) ; } }
public void test() { if ( curIndex > previousIndex ) { String text = _html . substring ( previousIndex , curIndex ) ; sb . append ( text ) ; } }
public void test() { if ( _log . isWarnEnabled ( ) ) { _log . warn ( "Unable to find image source " + text ) ; } }
public void test() { if ( _log . isWarnEnabled ( ) ) { _log . warn ( "Unable to obtain image URL from file entry " + imageFileEntry . getFileEntryId ( ) , portalException ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( _NO_SUCH_ENTITY_WITH_PRIMARY_KEY + primaryKey ) ; } }
public void test() { try { result = deleteCiscoNexusVSM ( cmd . getCiscoNexusVSMDeviceId ( ) ) ; } catch ( ResourceInUseException e ) { s_logger . info ( "VSM could not be deleted" ) ; throw new CloudRuntimeException ( "Failed to delete specified VSM" ) ; } }
protected synchronized void activate ( final Map < String , Object > properties , ComponentContext componentContext ) { logger . debug ( "Activating Regex Filter..." ) ; this . filter = String . valueOf ( properties . getOrDefault ( REGEX_PROP , "" ) ) ; this . componentPid = String . valueOf ( properties . get ( KURA_SERVICE_PID ) ) ; this . filterType = getType ( properties ) ; this . wireSupport = this . wireHelperService . newWireSupport ( this , ( ServiceReference < WireComponent > ) componentContext . getServiceReference ( ) ) ; logger . debug ( "Activating Regex Filter... Done" ) ; }
protected synchronized void activate ( final Map < String , Object > properties , ComponentContext componentContext ) { logger . debug ( "Activating Regex Filter..." ) ; this . filter = String . valueOf ( properties . getOrDefault ( REGEX_PROP , "" ) ) ; this . componentPid = String . valueOf ( properties . get ( KURA_SERVICE_PID ) ) ; this . filterType = getType ( properties ) ; this . wireSupport = this . wireHelperService . newWireSupport ( this , ( ServiceReference < WireComponent > ) componentContext . getServiceReference ( ) ) ; logger . debug ( "Activating Regex Filter... Done" ) ; }
public void test() { try { createTestNodes ( TestOne . class , 100 ) ; app . command ( SyncCommand . class ) . execute ( toMap ( "mode" , "export" , "file" , EXPORT_FILENAME ) ) ; final Path exportFile = Paths . get ( EXPORT_FILENAME ) ; assertTrue ( "Export file doesn't exist!" , Files . exists ( exportFile ) ) ; cleanDatabaseAndSchema ( ) ; app . command ( SyncCommand . class ) . execute ( toMap ( "mode" , "import" , "file" , EXPORT_FILENAME ) ) ; code_block = TryStatement ;  Files . delete ( exportFile ) ; } catch ( Exception ex ) { logger . warn ( "" , ex ) ; fail ( "Unexpected exception." ) ; } }
public void run ( ) { EntityManager em = MCREntityManagerProvider . getEntityManagerFactory ( ) . createEntityManager ( ) ; EntityTransaction transaction = em . getTransaction ( ) ; LOGGER . info ( "MCRJob is Checked for dead Entries" ) ; transaction . begin ( ) ; StringBuilder sb = new StringBuilder ( "FROM MCRJob WHERE " ) ; code_block = IfStatement ; sb . append ( " status='" + MCRJobStatus . PROCESSING + "' ORDER BY id ASC" ) ; TypedQuery < MCRJob > query = em . createQuery ( sb . toString ( ) , MCRJob . class ) ; long current = new Date ( System . currentTimeMillis ( ) ) . getTime ( ) / 60000 ; boolean reset = query . getResultList ( ) . stream ( ) . map ( job code_block = LoopStatement ; ) . reduce ( Boolean :: logicalOr ) . orElse ( false ) ; code_block = TryStatement ;  code_block = IfStatement ; em . close ( ) ; LOGGER . info ( "MCRJob checking is done" ) ; }
public void test() { if ( current - start >= maxTimeDiff ) { LOGGER . debug ( "->Resetting too long in queue" ) ; job . setStatus ( MCRJobStatus . NEW ) ; job . setStart ( null ) ; ret = true ; } else { LOGGER . debug ( "->ok" ) ; } }
public void test() { if ( current - start >= maxTimeDiff ) { LOGGER . debug ( "->Resetting too long in queue" ) ; job . setStatus ( MCRJobStatus . NEW ) ; job . setStart ( null ) ; ret = true ; } else { LOGGER . debug ( "->ok" ) ; } }
public void run ( ) { EntityManager em = MCREntityManagerProvider . getEntityManagerFactory ( ) . createEntityManager ( ) ; EntityTransaction transaction = em . getTransaction ( ) ; LOGGER . info ( "MCRJob is Checked for dead Entries" ) ; transaction . begin ( ) ; StringBuilder sb = new StringBuilder ( "FROM MCRJob WHERE " ) ; code_block = IfStatement ; sb . append ( " status='" + MCRJobStatus . PROCESSING + "' ORDER BY id ASC" ) ; TypedQuery < MCRJob > query = em . createQuery ( sb . toString ( ) , MCRJob . class ) ; long current = new Date ( System . currentTimeMillis ( ) ) . getTime ( ) / 60000 ; boolean reset = query . getResultList ( ) . stream ( ) . map ( job code_block = LoopStatement ; ) . reduce ( Boolean :: logicalOr ) . orElse ( false ) ; code_block = TryStatement ;  code_block = IfStatement ; em . close ( ) ; LOGGER . info ( "MCRJob checking is done" ) ; }
public void test() { if ( to_save ) { log . debug ( "CVE data of bug [" + _b . getBugId ( ) + "] changed, triggering update of local database" ) ; this . customSave ( _b , false ) ; update_happened = true ; } else { log . debug ( "CVE data of bug [" + _b . getBugId ( ) + "] did not change, no update of local database needed" ) ; } }
public void test() { if ( to_save ) { log . debug ( "CVE data of bug [" + _b . getBugId ( ) + "] changed, triggering update of local database" ) ; this . customSave ( _b , false ) ; update_happened = true ; } else { log . debug ( "CVE data of bug [" + _b . getBugId ( ) + "] did not change, no update of local database needed" ) ; } }
public void test() { try { String cve_id = Cve . extractCveIdentifier ( _b . getBugId ( ) ) ; if ( cve_id == null ) cve_id = Cve . extractCveIdentifier ( _b . getBugIdAlt ( ) ) ; final Cve cve = CveReader2 . read ( cve_id ) ; code_block = IfStatement ; } catch ( CacheException e ) { log . error ( "Cache exception when refreshing CVE data of bug [" + _b . getBugId ( ) + "]: " + e . getMessage ( ) ) ; } catch ( PersistenceException e ) { log . error ( "Cannot save bug [" + _b . getBugId ( ) + "] with refreshed CVE data: " + e . getMessage ( ) ) ; } }
public void test() { try { String cve_id = Cve . extractCveIdentifier ( _b . getBugId ( ) ) ; if ( cve_id == null ) cve_id = Cve . extractCveIdentifier ( _b . getBugIdAlt ( ) ) ; final Cve cve = CveReader2 . read ( cve_id ) ; code_block = IfStatement ; } catch ( CacheException e ) { log . error ( "Cache exception when refreshing CVE data of bug [" + _b . getBugId ( ) + "]: " + e . getMessage ( ) ) ; } catch ( PersistenceException e ) { log . error ( "Cannot save bug [" + _b . getBugId ( ) + "] with refreshed CVE data: " + e . getMessage ( ) ) ; } }
public void test() { if ( _force || this . needsCveData ( _b ) ) { code_block = TryStatement ;  } else { log . debug ( "Bug [" + _b . getBugId ( ) + "] does not need a refresh of cached CVE data" ) ; } }
public void test() { try { InternetAddress address = new InternetAddress ( email , true ) ; address . setPersonal ( udr . getUser ( ) . getDisplayName ( ) , Charsets . UTF_8 . name ( ) ) ; return address ; } catch ( UnsupportedEncodingException | AddressException e ) { logger . warn ( "Cannot set internet address" , e ) ; throw Throwables . propagate ( e ) ; } }
public void test() { try { code_block = ForStatement ; } catch ( Exception e ) { LOG . error ( "sync db to tasks error" , e ) ; } }
public void test() { try ( final ManagedLock < ReentrantLock > dbLock = lockManager . acquireBtreeReadLock ( index . db . getLockName ( ) ) ) { final SearchCallback cb = new SearchCallback ( contextId , query , ngram , docs , contextSet , context , result , axis == NodeSet . ANCESTOR ) ; final int op = query . codePointCount ( 0 , query . length ( ) ) < getN ( ) ? IndexQuery . TRUNC_RIGHT : IndexQuery . EQ ; index . db . query ( new IndexQuery ( op , key ) , cb ) ; } catch ( final LockException e ) { LOG . warn ( "Failed to acquire lock to '{}'" , FileUtils . fileName ( index . db . getFile ( ) ) , e ) ; } catch ( final IOException | BTreeException e ) { LOG . error ( "{} in '{}'" , e . getMessage ( ) , FileUtils . fileName ( index . db . getFile ( ) ) , e ) ; } }
public void test() { try ( final ManagedLock < ReentrantLock > dbLock = lockManager . acquireBtreeReadLock ( index . db . getLockName ( ) ) ) { final SearchCallback cb = new SearchCallback ( contextId , query , ngram , docs , contextSet , context , result , axis == NodeSet . ANCESTOR ) ; final int op = query . codePointCount ( 0 , query . length ( ) ) < getN ( ) ? IndexQuery . TRUNC_RIGHT : IndexQuery . EQ ; index . db . query ( new IndexQuery ( op , key ) , cb ) ; } catch ( final LockException e ) { LOG . warn ( "Failed to acquire lock to '{}'" , FileUtils . fileName ( index . db . getFile ( ) ) , e ) ; } catch ( final IOException | BTreeException e ) { LOG . error ( "{} in '{}'" , e . getMessage ( ) , FileUtils . fileName ( index . db . getFile ( ) ) , e ) ; } }
public void test() { if ( attribute . needsBase64Encoding ( ) ) { LOG . trace ( "Found binary attribute: " + attributeName + ". Is defined in LDAP config: " + getOperationService ( ) . isBinaryAttribute ( attributeName ) ) ; } }
public void test() { try { inetAddress = InetAddress . getByAddress ( finalmask ) ; } catch ( UnknownHostException e ) { LOG . error ( "Failed to convert the Ipv6 subnetmask from integer to mask value " , e ) ; return null ; } }
@ Override public void onError ( Throwable throwable ) { logger . error ( "Error occurred during Application Key Mappings discovery" , throwable ) ; XdsSchedulerManager . getInstance ( ) . startApplicationKeyMappingDiscoveryScheduling ( ) ; nack ( throwable ) ; }
public void test() { try { return resultSet . getTimestamp ( columnIndex ) ; } catch ( SQLException e ) { LOGGER . error ( "error trying to deserialize column at index: " + columnIndex ) ; LOGGER . error ( "raw value:" + resultSet . getObject ( columnIndex ) ) ; throw ( e ) ; } }
public void test() { try { txn . start ( ) ; String sql = UPDATE_USER_STATS ; PreparedStatement pstmt = null ; pstmt = txn . prepareAutoCloseStatement ( sql ) ; code_block = ForStatement ; pstmt . executeBatch ( ) ; txn . commit ( ) ; } catch ( Exception ex ) { txn . rollback ( ) ; s_logger . error ( "error updating user stats to cloud_usage db" , ex ) ; throw new CloudRuntimeException ( ex . getMessage ( ) ) ; } }
public void test() { if ( LOGGER . isTraceEnabled ( ) ) { LOGGER . trace ( "Versioning updated metacards: {}" , updateResponse . getUpdatedMetacards ( ) ) ; } }
public void test() { if ( LOGGER . isTraceEnabled ( ) ) { LOGGER . trace ( "Successfully created metacard versions under ids: {}" , response . getCreatedMetacards ( ) . stream ( ) . map ( Metacard :: getId ) . collect ( TO_A_STRING ) ) ; } }
public void test() { if ( null == database ) { log . warn ( String . format ( "Missing database %s" , database ) ) ; return super . verify ( host , key ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( IOException e ) { log . error ( String . format ( "Failure verifying host key entry %s. %s" , entry , e . getMessage ( ) ) ) ; return false ; } }
public MbSchicht merge ( MbSchicht detachedInstance ) { log . debug ( "merging MbSchicht instance" ) ; code_block = TryStatement ;  }
public void test() { try { MbSchicht result = ( MbSchicht ) sessionFactory . getCurrentSession ( ) . merge ( detachedInstance ) ; log . debug ( "merge successful" ) ; return result ; } catch ( RuntimeException re ) { log . error ( "merge failed" , re ) ; throw re ; } }
public void test() { try { MbSchicht result = ( MbSchicht ) sessionFactory . getCurrentSession ( ) . merge ( detachedInstance ) ; log . debug ( "merge successful" ) ; return result ; } catch ( RuntimeException re ) { log . error ( "merge failed" , re ) ; throw re ; } }
public void test() { try { tState = TableState . valueOf ( sState ) ; } catch ( IllegalArgumentException e ) { log . error ( "Unrecognized state for table with tableId={}: {}" , tableId , sState ) ; } }
@ Test public void test ( ) throws IOException { log . debug ( "Client started" ) ; JsonRpcClient client = createJsonRpcClient ( "/jsonrpc" ) ; Params params = new Params ( ) ; params . param1 = "Value1" ; params . param2 = "Value2" ; Params result = client . sendRequest ( "echo" , params , Params . class ) ; log . debug ( "Response:" + result ) ; Assert . assertEquals ( params . param1 , result . param1 ) ; Assert . assertEquals ( params . param2 , result . param2 ) ; client . close ( ) ; log . debug ( "Client finished" ) ; }
@ Test public void test ( ) throws IOException { log . debug ( "Client started" ) ; JsonRpcClient client = createJsonRpcClient ( "/jsonrpc" ) ; Params params = new Params ( ) ; params . param1 = "Value1" ; params . param2 = "Value2" ; Params result = client . sendRequest ( "echo" , params , Params . class ) ; log . debug ( "Response:" + result ) ; Assert . assertEquals ( params . param1 , result . param1 ) ; Assert . assertEquals ( params . param2 , result . param2 ) ; client . close ( ) ; log . debug ( "Client finished" ) ; }
@ Test public void test ( ) throws IOException { log . debug ( "Client started" ) ; JsonRpcClient client = createJsonRpcClient ( "/jsonrpc" ) ; Params params = new Params ( ) ; params . param1 = "Value1" ; params . param2 = "Value2" ; Params result = client . sendRequest ( "echo" , params , Params . class ) ; log . debug ( "Response:" + result ) ; Assert . assertEquals ( params . param1 , result . param1 ) ; Assert . assertEquals ( params . param2 , result . param2 ) ; client . close ( ) ; log . debug ( "Client finished" ) ; }
public ItemsVO createNewEntryInItemsTable ( ItemsVO vo ) { logger . debug ( "JDBC::createNewEntryInItemsTable" ) ; long timerStart = System . currentTimeMillis ( ) ; Long i = conf . getDBDAO ( ) . doCreateNewEntryInItemsTable ( vo ) ; vo . setItemid ( i . intValue ( ) ) ; logTime ( "doCreateNewEntryInItemsTable" , timerStart , System . currentTimeMillis ( ) ) ; return vo ; }
public void test() { if ( file . exists ( ) && ! file . delete ( ) ) { LOGGER . warn ( String . format ( Locale . ROOT , "Could not delete %s." , file ) ) ; } }
public void test() { if ( Validator . isNull ( userId ) ) { return ; } }
public void test() { if ( ! enableContentRecommendation || ( _asahInterestTermProvider == null ) ) { return ; } }
public void test() { if ( interestTerms . length == 0 ) { return ; } }
public int doMove ( PrintStream resultOs , PrintStream logOs ) throws Exception { code_block = ForStatement ; int failedMoves = ReplicaMove . failedMoves ( allMoves ) ; LOG . info ( "{} : failed with {} moves" , this , failedMoves ) ; return failedMoves ; }
public void test() { try { runnable . run ( ) ; } catch ( Throwable e ) { log . error ( "error executing shutdown hook" , e ) ; } }
public void test() { if ( trace ) { log . trace ( "{} {} <- {}" , impl . uniqueId ( ) , key , prev + delta ) ; } }
public void test() { try { this . client . getSetSchedule ( request ) ; } catch ( final SoapFaultClientException ex ) { LOGGER . info ( "Received a SOAP fault on setSchedule" ) ; final String faultString = ex . getFaultStringOrReason ( ) ; code_block = IfStatement ; ScenarioContext . current ( ) . put ( PlatformKeys . RESPONSE , ex ) ; return null ; } }
public void test() { { String name = Thread . currentThread ( ) . getName ( ) ; logger . debug ( "sleep:{}" , name ) ; Thread . sleep ( 10000 ) ; logger . debug ( "messageReceived thread-{} message:" , Thread . currentThread ( ) . getName ( ) ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( PortalException portalException ) { _log . error ( "Unable to check is back order allowed" , portalException ) ; } }
@ AfterClass public static void reportTestFinish ( ) { stopwatch . stop ( ) ; LOGGER . warn ( "-----------------------------------------" ) ; LOGGER . warn ( "*                                       *" ) ; LOGGER . warn ( "* FINISHED GeoWaveSparkSQLIT        *" ) ; LOGGER . warn ( "*         " + stopwatch . getTimeString ( ) + " elapsed.             *" ) ; LOGGER . warn ( "*                                       *" ) ; LOGGER . warn ( "-----------------------------------------" ) ; }
public void test() { try { response . sendRedirect ( StringExtensions . combinePath ( request . getContextPath ( ) , redirectTo ) ) ; } catch ( Exception e ) { log . error ( "failed to redirect request to {}" , redirectTo , e ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( XmlUtils . marshaltoString ( wmlPackage . getMainDocumentPart ( ) . getJaxbElement ( ) , true , true ) ) ; } }
public void test() { if ( configuration . getPowerStateChanging ( ) == PowerStateChanging . ALWAYS_OFF && ( powerState != OnOffType . OFF ) ) { logger . debug ( "Correcting power state of {} ({}) to off" , deviceType , macAddress ) ; handleOnOffCommand ( OnOffType . OFF ) ; } else-if ( configuration . getPowerStateChanging ( ) == PowerStateChanging . ALWAYS_ON && ( powerState != OnOffType . ON ) ) { logger . debug ( "Correcting power state of {} ({}) to on" , deviceType , macAddress ) ; handleOnOffCommand ( OnOffType . ON ) ; } }
public void test() { if ( configuration . getPowerStateChanging ( ) == PowerStateChanging . ALWAYS_OFF && ( powerState != OnOffType . OFF ) ) { logger . debug ( "Correcting power state of {} ({}) to off" , deviceType , macAddress ) ; handleOnOffCommand ( OnOffType . OFF ) ; } else-if ( configuration . getPowerStateChanging ( ) == PowerStateChanging . ALWAYS_ON && ( powerState != OnOffType . ON ) ) { logger . debug ( "Correcting power state of {} ({}) to on" , deviceType , macAddress ) ; handleOnOffCommand ( OnOffType . ON ) ; } }
public void test() { try { vendorNames = thriftClients . makeVendorClient ( ) . getAllVendorNames ( ) ; } catch ( TException e ) { log . error ( "Problem retrieving all the Vendor names" , e ) ; vendorNames = Collections . emptySet ( ) ; } }
public void test() { if ( e instanceof AuthorizationException ) { throw new AccessDeniedException ( "Access denied by Wicket's security layer" , e ) ; } }
private boolean isDuplicatedSlot ( TaskSlot taskSlot , JobID jobId , ResourceProfile resourceProfile , int index ) { LOG . info ( "Slot with allocationId {} already exist, with resource profile {}, job id {} and index {}. The required index is {}." , taskSlot . getAllocationId ( ) , taskSlot . getResourceProfile ( ) , taskSlot . getJobId ( ) , taskSlot . getIndex ( ) , index ) ; return taskSlot . getJobId ( ) . equals ( jobId ) && taskSlot . getResourceProfile ( ) . equals ( resourceProfile ) && ( isDynamicIndex ( index ) || taskSlot . getIndex ( ) == index ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Request matched by universal pattern '/**'" ) ; } }
public void test() { if ( ! Objects . equals ( original . getMetadata ( ) , addressSpace . getMetadata ( ) ) ) { log . debug ( "Meta changed from {} to {}" , original . getMetadata ( ) , addressSpace . getMetadata ( ) ) ; changed = true ; } }
public void test() { if ( ! Objects . equals ( original . getSpec ( ) , addressSpace . getSpec ( ) ) ) { log . debug ( "Spec changed from {} to {}" , original . getSpec ( ) , addressSpace . getSpec ( ) ) ; changed = true ; } }
public void test() { if ( ! Objects . equals ( original . getStatus ( ) , addressSpace . getStatus ( ) ) ) { log . debug ( "Status changed from {} to {}" , original . getStatus ( ) , addressSpace . getStatus ( ) ) ; changed = true ; } }
public void test() { try { applicationRoot = VFS . getManager ( ) . resolveFile ( applicationDirectoryPath ) ; code_block = IfStatement ; } catch ( FileSystemException e ) { logger . error ( "Configured application directory " + applicationDirectoryPath + " is not valid" , e ) ; } }
public void initialize ( ) { String applicationDirectoryPath = getApplicationDirectoryPath ( ) ; if ( applicationDirectoryPath != null ) code_block = TryStatement ;  code_block = IfStatement ; logger . info ( "Application directory: {}" , applicationRoot ) ; code_block = TryStatement ;  String actionsDirectory = getConfiguration ( ) . getString ( "portofino.actions.path" , "actions" ) ; codeBase = initApplicationRoot ( actionsDirectory ) ; logger . info ( "Application initialized." ) ; }
public void initialize ( ) { String applicationDirectoryPath = getApplicationDirectoryPath ( ) ; if ( applicationDirectoryPath != null ) code_block = TryStatement ;  code_block = IfStatement ; logger . info ( "Application directory: {}" , applicationRoot ) ; code_block = TryStatement ;  String actionsDirectory = getConfiguration ( ) . getString ( "portofino.actions.path" , "actions" ) ; codeBase = initApplicationRoot ( actionsDirectory ) ; logger . info ( "Application initialized." ) ; }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceTierPriceEntryServiceUtil . class , "getCommerceTierPriceEntry" , _getCommerceTierPriceEntryParameterTypes11 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , commerceTierPriceEntryId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . commerce . price . list . model . CommerceTierPriceEntry ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( composedMessage == null ) { log . error ( "No message object of type org.projectforge.mail.Mail given. E-Mail not sent." ) ; return false ; } }
public void test() { if ( cf == null || ! cf . isEmailEnabled ( ) ) { log . error ( "No e-mail host configured. E-Mail not sent: " + composedMessage . toString ( ) ) ; return false ; } }
public void test() { try { SAMLBindings binding = isGet ? SAMLBindings . HTTP_REDIRECT : SAMLBindings . HTTP_POST ; LogoutResponseDocument respDoc = LogoutResponseDocument . Factory . parse ( samlResponse ) ; SAMLVerifiableElement verifiableMessage = binding == SAMLBindings . HTTP_REDIRECT ? new RedirectedMessage ( httpReq . getQueryString ( ) ) : new XMLExpandedMessage ( respDoc , respDoc . getLogoutResponse ( ) ) ; SAMLMessage < LogoutResponseDocument > responseMessage = new SAMLMessage < > ( verifiableMessage , relayState , binding , respDoc ) ; logoutProcessor . handleAsyncLogoutResponse ( responseMessage , httpResp ) ; } catch ( XmlException e ) { log . warn ( "Got an invalid SAML Single Logout response (XML is broken)" , e ) ; httpResp . sendError ( HttpServletResponse . SC_BAD_REQUEST , "Invalid SLO response (XML is malformed)" ) ; } catch ( EopException e ) { } }
public void test() { try { Version esVersion = Version . fromString ( client . info ( RequestOptions . DEFAULT ) . getVersion ( ) . getNumber ( ) ) ; code_block = IfStatement ; String esVersionCompatibilityWarn = String . format ( "ES version(%s) is not compatible with the recommendation(%s)" , esVersion . toString ( ) , RECOMMENDED_ES_VERSION . toString ( ) ) ; LOGGER . warn ( esVersionCompatibilityWarn ) ; return CheckResult . builder ( ) . checkName ( checkName ( ) ) . resultType ( ResultType . BAD ) . description ( esVersionCompatibilityWarn ) . build ( ) ; } catch ( IOException e ) { LOGGER . error ( VERSION_CHECKING_ERROR_MESSAGE , e ) ; return CheckResult . builder ( ) . checkName ( checkName ( ) ) . resultType ( ResultType . BAD ) . description ( VERSION_CHECKING_ERROR_MESSAGE + ": " + e . getMessage ( ) ) . build ( ) ; } }
public void test() { try { Version esVersion = Version . fromString ( client . info ( RequestOptions . DEFAULT ) . getVersion ( ) . getNumber ( ) ) ; code_block = IfStatement ; String esVersionCompatibilityWarn = String . format ( "ES version(%s) is not compatible with the recommendation(%s)" , esVersion . toString ( ) , RECOMMENDED_ES_VERSION . toString ( ) ) ; LOGGER . warn ( esVersionCompatibilityWarn ) ; return CheckResult . builder ( ) . checkName ( checkName ( ) ) . resultType ( ResultType . BAD ) . description ( esVersionCompatibilityWarn ) . build ( ) ; } catch ( IOException e ) { LOGGER . error ( VERSION_CHECKING_ERROR_MESSAGE , e ) ; return CheckResult . builder ( ) . checkName ( checkName ( ) ) . resultType ( ResultType . BAD ) . description ( VERSION_CHECKING_ERROR_MESSAGE + ": " + e . getMessage ( ) ) . build ( ) ; } }
public void test() { try { Cache cache = this . getCache ( ) ; this . releaseCachedObjects ( cache ) ; Map < String , Group > groups = groupDAO . loadGroups ( ) ; this . insertObjectsOnCache ( cache , groups ) ; } catch ( Throwable t ) { _logger . error ( "Error loading groups" , t ) ; throw new ApsSystemException ( "Error loading groups" , t ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Synchronizing the project routers" ) ; } }
public void test() { try { synchronizeProjectRoutes ( ) ; } catch ( Exception e ) { log . error ( "Error while handling synchronizing projects during topology update event." , e ) ; handler . fail ( 400 , "Could not initialize projects." ) ; } }
private QoSInterDirectPingMeasurementResponseDTO getInterDirectPingMeasurementFromQoSMonitor ( final CloudSystemFormDTO request ) { logger . debug ( "getInterDirectPingMeasurementFromQoSMonitor started..." ) ; final QoSInterDirectPingMeasurementResponseDTO measurement = orchestratorDriver . getInterDirectPingMeasurement ( request ) ; code_block = IfStatement ; return measurement ; }
public void test() { if ( uncastResult == null ) { return null ; } }
@ Override public void transform ( Message message , DataType from , DataType to ) throws Exception { assertEquals ( "name=XOrder" , message . getBody ( ) ) ; LOG . info ( "Bean: Other -> XOrder" ) ; message . setBody ( new XOrder ( ) ) ; }
@ Test public void testHandleGetRequest ( ) { String jsonResponse = JsonLoader . loadJson ( DomainHelper . getRestUrlV2 ( ) + "/content/words" ) ; logger . info ( "jsonResponse: " + jsonResponse ) ; JSONArray wordsJSONArray = new JSONArray ( jsonResponse ) ; logger . info ( "wordsJSONArray.length(): " + wordsJSONArray . length ( ) ) ; assertThat ( wordsJSONArray . length ( ) > 0 , is ( true ) ) ; JSONObject wordJsonObject = wordsJSONArray . getJSONObject ( 0 ) ; assertThat ( wordJsonObject . getLong ( "id" ) , not ( nullValue ( ) ) ) ; assertThat ( wordJsonObject . getString ( "text" ) , not ( nullValue ( ) ) ) ; }
@ Test public void testHandleGetRequest ( ) { String jsonResponse = JsonLoader . loadJson ( DomainHelper . getRestUrlV2 ( ) + "/content/words" ) ; logger . info ( "jsonResponse: " + jsonResponse ) ; JSONArray wordsJSONArray = new JSONArray ( jsonResponse ) ; logger . info ( "wordsJSONArray.length(): " + wordsJSONArray . length ( ) ) ; assertThat ( wordsJSONArray . length ( ) > 0 , is ( true ) ) ; JSONObject wordJsonObject = wordsJSONArray . getJSONObject ( 0 ) ; assertThat ( wordJsonObject . getLong ( "id" ) , not ( nullValue ( ) ) ) ; assertThat ( wordJsonObject . getString ( "text" ) , not ( nullValue ( ) ) ) ; }
protected < T > ServiceResponse < T > makeBackwardCompatibleHttpPostRequestAndCreateServiceResponse ( String uri , String body , Class < T > resultType ) { logger . debug ( "About to send POST request to '{}' with payload '{}'" , uri , body ) ; KieServerHttpRequest request = newRequest ( uri ) . body ( body ) . post ( ) ; KieServerHttpResponse response = request . response ( ) ; owner . setConversationId ( response . header ( KieServerConstants . KIE_CONVERSATION_ID_TYPE_HEADER ) ) ; code_block = IfStatement ; }
public void test() { if ( pin == null ) { log . error ( "pin definition null" ) ; return null ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( String . format ( "Removing property: '%s' from scope: '%s'" , matchedValue , getScopeName ( ) ) ) ; } }
public void test() { if ( depositSet . get ( uuid ) . contains ( CleanupDepositJob . class . getName ( ) ) ) { depositStatusFactory . setState ( uuid , DepositState . finished ) ; } else { LOG . info ( "Skipping resumption of deposit {} because it already is in the queue" , uuid ) ; } }
public void test() { if ( depositSet . get ( uuid ) . contains ( CleanupDepositJob . class . getName ( ) ) ) { depositStatusFactory . setState ( uuid , DepositState . finished ) ; } else { LOG . debug ( "Skipping resumption of queued deposit {} because it already is in the queue" , uuid ) ; } }
public void test() { if ( publisher == null ) { logger . error ( "Cannot publish the failure event as analytics publisher is null." ) ; return ; } }
public void test() { try { dataCollector . collectData ( ) ; logger . debug ( "Analytics event for failure event is published." ) ; } catch ( AnalyticsException e ) { logger . error ( "Error while publishing the analytics event. " , e ) ; } }
public void test() { try { IsotopicDistribution lCalc = new IsotopicDistribution ( 60 , 13 , 86 , 13 , 2 ) ; Assert . assertEquals ( 0.39350045799282984 , lCalc . getPercMax ( ) [ 2 ] , 0 ) ; Assert . assertEquals ( 0.16628993915006032 , lCalc . getPercTot ( ) [ 2 ] , 0 ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; fail ( e . getMessage ( ) ) ; } }
public void test() { if ( ! destMd5 . equals ( srcMd5 ) ) { logger . debug ( String . format ( "file MD5 changed, src[%s, md5:%s] dest[%s, md5, %s]" , sourceFilePath , srcMd5 , destFilePath , destMd5 ) ) ; return true ; } }
public void test() { { LOGGER . debug ( "check exist accessContract={}" , accessContractDto ) ; final boolean exist = securityProfileExternalService . check ( accessContractDto ) ; return RestUtils . buildBooleanResponse ( exist ) ; } }
public void test() { try { loggingCallback . awaitCompletion ( 3 , TimeUnit . SECONDS ) ; } catch ( InterruptedException e ) { log . error ( e . getMessage ( ) ) ; Thread . currentThread ( ) . interrupt ( ) ; } }
@ Override public void trace ( Marker marker , String msg ) { traceMessages . add ( new LogMessage ( marker , msg , null ) ) ; logger . trace ( marker , msg ) ; }
public void test() { try { loader = loaderClass . newInstance ( ) ; } catch ( Exception e ) { LOGGER . error ( "Failed to create " + Conf . class . getPackage ( ) . getName ( ) + " to " + loaderClass . getName ( ) , e ) ; throw new RuntimeException ( "Failed to create " + Conf . class . getPackage ( ) . getName ( ) + " to " + loaderClass . getName ( ) , e ) ; } }
public void test() { try ( Session session = modelDBHibernateUtil . getSessionFactory ( ) . openSession ( ) ) { ExperimentRunEntity experimentRunEntity = session . load ( ExperimentRunEntity . class , experimentRunId , LockMode . PESSIMISTIC_WRITE ) ; experimentRunEntity . setParent_id ( parentExperimentRunId ) ; long currentTimestamp = Calendar . getInstance ( ) . getTimeInMillis ( ) ; experimentRunEntity . setDate_updated ( currentTimestamp ) ; Transaction transaction = session . beginTransaction ( ) ; session . update ( experimentRunEntity ) ; transaction . commit ( ) ; LOGGER . debug ( "ExperimentRun parentId updated successfully" ) ; } catch ( Exception ex ) { code_block = IfStatement ; } }
public void test() { try { code_block = ForStatement ; prepareFinalExtractedDataList ( responceDataList , responceFinalDataList , responceData ) ; } catch ( Exception e ) { LOG . error ( " Error while parsing dynamic response template" , e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( FieldException e ) { logger . debug ( "bad field handling link records {}" , e . getMessage ( ) ) ; } catch ( IOException e ) { logger . debug ( "got IO exception handling link records {}" , e . getMessage ( ) ) ; } catch ( IllegalStateException e ) { logger . debug ( "got exception requesting link records {}" , e . getMessage ( ) ) ; } catch ( InvalidMessageTypeException e ) { logger . warn ( "invalid message " , e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( FieldException e ) { logger . debug ( "bad field handling link records {}" , e . getMessage ( ) ) ; } catch ( IOException e ) { logger . debug ( "got IO exception handling link records {}" , e . getMessage ( ) ) ; } catch ( IllegalStateException e ) { logger . debug ( "got exception requesting link records {}" , e . getMessage ( ) ) ; } catch ( InvalidMessageTypeException e ) { logger . warn ( "invalid message " , e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( FieldException e ) { logger . debug ( "bad field handling link records {}" , e . getMessage ( ) ) ; } catch ( IOException e ) { logger . debug ( "got IO exception handling link records {}" , e . getMessage ( ) ) ; } catch ( IllegalStateException e ) { logger . debug ( "got exception requesting link records {}" , e . getMessage ( ) ) ; } catch ( InvalidMessageTypeException e ) { logger . warn ( "invalid message " , e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( FieldException e ) { logger . debug ( "bad field handling link records {}" , e . getMessage ( ) ) ; } catch ( IOException e ) { logger . debug ( "got IO exception handling link records {}" , e . getMessage ( ) ) ; } catch ( IllegalStateException e ) { logger . debug ( "got exception requesting link records {}" , e . getMessage ( ) ) ; } catch ( InvalidMessageTypeException e ) { logger . warn ( "invalid message " , e ) ; } }
private void reloadAllDataModel ( ) throws IOException { ResourceStore store = getStore ( ) ; logger . debug ( "Reloading DataModel from folder " + store . getReadableResourcePath ( ResourceStore . DATA_MODEL_DESC_RESOURCE_ROOT ) ) ; dataModelDescMap . clear ( ) ; List < String > paths = store . collectResourceRecursively ( ResourceStore . DATA_MODEL_DESC_RESOURCE_ROOT , MetadataConstants . FILE_SURFIX ) ; code_block = ForStatement ; logger . debug ( "Loaded " + dataModelDescMap . size ( ) + " DataModel(s)" ) ; }
public void test() { try { reloadDataModelDescAt ( path ) ; } catch ( IllegalStateException e ) { logger . error ( "Error to load DataModel at " + path , e ) ; continue ; } }
public void test() { if ( log . isTraceEnable ( ) ) { log . warn ( this , "JsHelper eval Exception: script=" + script + ", StrategyDesc=" + stra . getDesc ( ) + ", StrategyName=" + stra . getName ( ) ) ; } }
public void test() { if ( log . isDebugEnable ( ) ) { log . debug ( this , "Judgement: FireSize=" + map . size ( ) + ", conditions=" + JSONHelper . toString ( crs ) ) ; } }
public void test() { if ( "AWS" . equalsIgnoreCase ( detectInstance ) ) { logger . info ( "Detect instance set to AWS, trying to determine AWS instance ID" ) ; result = getLocalInstanceId ( "AWS" , "http://169.254.169.254/latest/meta-data/instance-id" , null ) ; code_block = IfStatement ; } else-if ( "GCE" . equalsIgnoreCase ( detectInstance ) ) { logger . info ( "Detect instance set to GCE, trying to determine GCE instance ID" ) ; result = getLocalInstanceId ( "GCE" , "http://metadata/computeMetadata/v1/instance/id" , ImmutableMap . of ( "X-Google-Metadata-Request" , "True" ) ) ; code_block = IfStatement ; } else { result = null ; logger . info ( "No source instance ID passed, and not set to detect, sending metrics without an instance ID" ) ; } }
public void test() { if ( result != null ) { } else { logger . info ( "Unable to detect AWS instance ID for this machine, sending metrics without an instance ID" ) ; } }
public void test() { if ( "AWS" . equalsIgnoreCase ( detectInstance ) ) { logger . info ( "Detect instance set to AWS, trying to determine AWS instance ID" ) ; result = getLocalInstanceId ( "AWS" , "http://169.254.169.254/latest/meta-data/instance-id" , null ) ; code_block = IfStatement ; } else-if ( "GCE" . equalsIgnoreCase ( detectInstance ) ) { logger . info ( "Detect instance set to GCE, trying to determine GCE instance ID" ) ; result = getLocalInstanceId ( "GCE" , "http://metadata/computeMetadata/v1/instance/id" , ImmutableMap . of ( "X-Google-Metadata-Request" , "True" ) ) ; code_block = IfStatement ; } else { result = null ; logger . info ( "No source instance ID passed, and not set to detect, sending metrics without an instance ID" ) ; } }
public void test() { if ( result == null ) { logger . info ( "Unable to detect GCE instance ID for this machine, sending metrics without an instance ID" ) ; } }
public void test() { if ( "AWS" . equalsIgnoreCase ( detectInstance ) ) { logger . info ( "Detect instance set to AWS, trying to determine AWS instance ID" ) ; result = getLocalInstanceId ( "AWS" , "http://169.254.169.254/latest/meta-data/instance-id" , null ) ; code_block = IfStatement ; } else-if ( "GCE" . equalsIgnoreCase ( detectInstance ) ) { logger . info ( "Detect instance set to GCE, trying to determine GCE instance ID" ) ; result = getLocalInstanceId ( "GCE" , "http://metadata/computeMetadata/v1/instance/id" , ImmutableMap . of ( "X-Google-Metadata-Request" , "True" ) ) ; code_block = IfStatement ; } else { result = null ; logger . info ( "No source instance ID passed, and not set to detect, sending metrics without an instance ID" ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( _NO_SUCH_ENTITY_WITH_PRIMARY_KEY + primaryKey ) ; } }
public void test() { try { ByteArrayOutputStream out = new ByteArrayOutputStream ( ) ; ObjectOutputStream objectOut = new ObjectOutputStream ( out ) ; objectOut . writeObject ( elements . toArray ( new Object [ elements . size ( ) ] ) ) ; transfer . doJavaToNative ( out . toByteArray ( ) , transferData ) ; } catch ( IOException e ) { LOG . error ( "Error while serializing object for dnd" , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . error ( "" , e ) ; } }
public void test() { if ( CollectionUtils . isNotEmpty ( entities ) ) { AtlasEntityWithExtInfo getByGuidResponse = atlasClientV2 . getEntityByGuid ( entities . get ( 0 ) . getGuid ( ) ) ; ret = getByGuidResponse ; LOG . info ( "Updated {} entity: name={}, guid={} " , ret . getEntity ( ) . getTypeName ( ) , ret . getEntity ( ) . getAttribute ( ATTRIBUTE_QUALIFIED_NAME ) , ret . getEntity ( ) . getGuid ( ) ) ; } else { LOG . info ( "Entity: name={} " , entity . toString ( ) + " not updated as it is unchanged from what is in Atlas" ) ; ret = entity ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( SiteNavigationMenuServiceUtil . class , "addSiteNavigationMenu" , _addSiteNavigationMenuParameterTypes0 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , name , type , auto , serviceContext ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . site . navigation . model . SiteNavigationMenu ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public List < AttendeeRequest > getAttendeeRequests ( Profile user , boolean includeAckd ) { log . info ( "getAttendeeRequests: " + user . getName ( ) ) ; List < AttendeeRequest > list = new ArrayList < > ( ) ; code_block = IfStatement ; log . info ( "getAttendeeRequests: found requests: " + list . size ( ) ) ; return list ; }
public void test() { try { CQSHandler . deleteMessage ( queueUrl , receiptHandle ) ; logger . debug ( "event=deleting_publish_job_from_cqs message_id=" + message . getMessageId ( ) + " queue_url=" + queueUrl + " receipt_handle=" + receiptHandle ) ; } catch ( Exception ex ) { logger . error ( "event=failed_to_kill_message" , ex ) ; } }
public void test() { try { Object [ ] array = ( Object [ ] ) pgArray . getArray ( ) ; List < String > ltrees = new ArrayList < > ( array . length ) ; code_block = ForStatement ; r . deliver ( ltrees ) ; } catch ( SQLException e ) { logger . error ( "Failed to parse PgArray: " + pgArray , e ) ; } }
@ PostConstruct void init ( ) throws IOException { LOGGER . trace ( "Velocity engine initializing..." ) ; final Properties properties = new Properties ( ) ; final var velocityLog = fedoraPropsConfig . getVelocityLog ( ) . toString ( ) ; LOGGER . debug ( "Setting Velocity runtime log: {}" , velocityLog ) ; properties . setProperty ( "runtime.log" , velocityLog ) ; final URL propertiesUrl = getClass ( ) . getResource ( velocityPropertiesLocation ) ; LOGGER . debug ( "Using Velocity configuration from {}" , propertiesUrl ) ; code_block = TryStatement ;  velocity . init ( properties ) ; LOGGER . trace ( "Velocity engine initialized." ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "æ§è¡è®¾ç½®:response.setCharacterEncoding(\"" + encoding + "\"" ) ; } }
public void test() { try { return Files . walk ( derivRoot ) . map ( MCRPath :: toMCRPath ) . filter ( p -> ! Files . isDirectory ( p ) ) . filter ( p -> ! p . equals ( derivRoot ) ) ; } catch ( IOException e ) { LOGGER . error ( "I/O error while access the starting file of derivate {}!" , derivateId , e ) ; } catch ( SecurityException s ) { LOGGER . error ( "No access to starting file of derivate {}!" , derivateId , s ) ; } }
public void test() { try { return Files . walk ( derivRoot ) . map ( MCRPath :: toMCRPath ) . filter ( p -> ! Files . isDirectory ( p ) ) . filter ( p -> ! p . equals ( derivRoot ) ) ; } catch ( IOException e ) { LOGGER . error ( "I/O error while access the starting file of derivate {}!" , derivateId , e ) ; } catch ( SecurityException s ) { LOGGER . error ( "No access to starting file of derivate {}!" , derivateId , s ) ; } }
public void test() { try { deliverBatch ( batch ) ; } catch ( Throwable e ) { logger . error ( "Failed to delivery updates" , e ) ; } }
public void test() { if ( convertedModelFs . exists ( convertedModelPath ) ) { LOG . warn ( "output directory " + convertedModelDir + " already exists" ) ; convertedModelFs . delete ( convertedModelPath , true ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Shutting down scheduler thread pool" ) ; } }
public void test() { if ( registration . isLIMSAdmin ( ) ) { Log . debug ( "Skipping permissions admin on Experiment object " + swAccession ) ; return true ; } }
public void test() { if ( ! consideredBefore ) { considered . add ( this . getSwAccession ( ) ) ; Log . debug ( "Checking permissions for experiment object " + swAccession ) ; } else { Log . debug ( "Skipping permissions for experiment object " + swAccession + " , checked before" ) ; return true ; } }
public void test() { if ( ! consideredBefore ) { considered . add ( this . getSwAccession ( ) ) ; Log . debug ( "Checking permissions for experiment object " + swAccession ) ; } else { Log . debug ( "Skipping permissions for experiment object " + swAccession + " , checked before" ) ; return true ; } }
public void test() { if ( registration . equals ( this . owner ) || registration . isLIMSAdmin ( ) ) { LOGGER . warn ( "Modifying Orphan Experiment: " + this . getName ( ) ) ; hasPermission = true ; } else-if ( owner == null ) { LOGGER . warn ( "Experiment has no owner! Modifying Orphan Experiment: " + this . getName ( ) ) ; hasPermission = true ; } else { LOGGER . warn ( "Not modifying Orphan Experiment: " + this . getName ( ) ) ; hasPermission = false ; } }
public void test() { if ( registration . equals ( this . owner ) || registration . isLIMSAdmin ( ) ) { LOGGER . warn ( "Modifying Orphan Experiment: " + this . getName ( ) ) ; hasPermission = true ; } else-if ( owner == null ) { LOGGER . warn ( "Experiment has no owner! Modifying Orphan Experiment: " + this . getName ( ) ) ; hasPermission = true ; } else { LOGGER . warn ( "Not modifying Orphan Experiment: " + this . getName ( ) ) ; hasPermission = false ; } }
public void test() { if ( registration . equals ( this . owner ) || registration . isLIMSAdmin ( ) ) { LOGGER . warn ( "Modifying Orphan Experiment: " + this . getName ( ) ) ; hasPermission = true ; } else-if ( owner == null ) { LOGGER . warn ( "Experiment has no owner! Modifying Orphan Experiment: " + this . getName ( ) ) ; hasPermission = true ; } else { LOGGER . warn ( "Not modifying Orphan Experiment: " + this . getName ( ) ) ; hasPermission = false ; } }
public void test() { try { sysTable . startBackupExclusiveOperation ( ) ; deleteSessionStarted = true ; } catch ( IOException e ) { LOG . warn ( "You can not run delete command while active backup session is in progress. \n" + "If there is no active backup session running, run backup repair utility to " + "restore \nbackup system integrity." ) ; return - 1 ; } }
public void test() { if ( ! BackupSystemTable . snapshotExists ( conn ) ) { BackupSystemTable . snapshot ( conn ) ; } else { LOG . warn ( "Backup system table snapshot exists" ) ; } }
public void test() { if ( BackupSystemTable . snapshotExists ( conn ) ) { BackupSystemTable . restoreFromSnapshot ( conn ) ; BackupSystemTable . deleteSnapshot ( conn ) ; LOG . error ( "Delete operation failed, please run backup repair utility to restore " + "backup system integrity" , e ) ; throw e ; } else { LOG . warn ( "Delete operation succeeded, there were some errors: " , e ) ; } }
public void test() { if ( BackupSystemTable . snapshotExists ( conn ) ) { BackupSystemTable . restoreFromSnapshot ( conn ) ; BackupSystemTable . deleteSnapshot ( conn ) ; LOG . error ( "Delete operation failed, please run backup repair utility to restore " + "backup system integrity" , e ) ; throw e ; } else { LOG . warn ( "Delete operation succeeded, there were some errors: " , e ) ; } }
private void loadWeightDataForOneDay ( UpdateInfo updateInfo , String formattedDate ) throws Exception { String json = getWeightData ( updateInfo , formattedDate ) ; String fatJson = getBodyFatData ( updateInfo , formattedDate ) ; JSONObject jsonWeight = JSONObject . fromObject ( json ) ; JSONObject jsonFat = JSONObject . fromObject ( fatJson ) ; json = mergeWeightInfos ( jsonWeight , jsonFat ) ; logger . info ( "guestId=" + updateInfo . getGuestId ( ) + " connector=fitbit action=loadWeightDataForOneDay json=" + json ) ; apiDataService . eraseApiData ( updateInfo . apiKey , weightOT , Arrays . asList ( formattedDate ) ) ; code_block = IfStatement ; }
public void test() { try { responseJson = PacHttpUtils . doHttpPost ( urlToQuery . toString ( ) , requestBody . toString ( ) ) ; } catch ( Exception e ) { LOGGER . error ( Constants . ERROR_UNIQUEHOST , e ) ; } }
public void test() { for ( String duplicatedVariableId : duplicatedVariableIds ) { List < Variable > duplicatedVariables = variablesById . get ( duplicatedVariableId ) ; LOGGER . warn ( "The variable '{}' appears more than once: {}." , duplicatedVariableId , duplicatedVariables ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( principalException , principalException ) ; } }
@ GetMapping ( "/hystrix" ) public Object hystrixPluginFallback ( ) { log . error ( "the default fallback for hystrix" ) ; return DefaultShenyuEntity . error ( ShenyuResultEnum . HYSTRIX_PLUGIN_FALLBACK . getCode ( ) , ShenyuResultEnum . HYSTRIX_PLUGIN_FALLBACK . getMsg ( ) , null ) ; }
public void test() { try { newAssignment . findOrCreateContainer ( AssignmentType . F_POLICY_RULE ) ; assignmentType . setPolicyRule ( new PolicyRuleType ( ) ) ; } catch ( SchemaException e ) { LOGGER . error ( "Cannot create policy rule assignment: {}" , e . getMessage ( ) , e ) ; getSession ( ) . error ( "Cannot create policyRule assignment." ) ; target . add ( getPageBase ( ) . getFeedbackPanel ( ) ) ; return ; } }
protected ProcessBuilder createProcessBuilder ( List < String > command , EnvironmentDescriptor env ) { ProcessBuilder processBuilder = new ProcessBuilder ( command ) ; log . debug ( "Building Process for command: {}" , ( ) -> String . join ( " " , processBuilder . command ( ) ) ) ; processBuilder . directory ( new File ( env . getWorkingDirectory ( ) ) ) ; processBuilder . environment ( ) . putAll ( env . getParameters ( ) ) ; processBuilder . redirectErrorStream ( true ) ; return processBuilder ; }
public void test() { try { MethodKey methodKey = new MethodKey ( AssetTagServiceUtil . class , "getGroupTagsCount" , _getGroupTagsCountParameterTypes6 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( ( Integer ) returnObj ) . intValue ( ) ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( ! md5 . equals ( e . getAttributeValue ( "md5" ) ) ) { LOGGER . warn ( "Fixed MD5 of {} to {}" , path , md5 ) ; e . setAttribute ( "md5" , md5 ) ; } }
private List < Recommendation > createMetroPointRecommendations ( VirtualArray srcVarray , List < VirtualArray > tgtVarrays , VirtualPool srcVpool , VirtualArray haVarray , VirtualPool haVpool , Project project , VirtualPoolCapabilityValuesWrapper capabilities , List < StoragePool > candidatePrimaryPools , List < StoragePool > candidateSecondaryPools , Volume vpoolChangeVolume ) { List < Recommendation > recommendations = new ArrayList < Recommendation > ( ) ; RPProtectionRecommendation rpProtectionRecommendaton = null ; Map < VirtualArray , List < StoragePool > > tgtVarrayStoragePoolsMap = getVplexTargetMatchingPools ( tgtVarrays , srcVpool , project , capabilities , vpoolChangeVolume ) ; rpProtectionRecommendaton = createRPProtectionRecommendationForMetroPoint ( srcVarray , tgtVarrays , srcVpool , haVarray , haVpool , capabilities , candidatePrimaryPools , candidateSecondaryPools , tgtVarrayStoragePoolsMap , vpoolChangeVolume , project ) ; _log . info ( String . format ( "Produced %s recommendations for MetroPoint placement." , rpProtectionRecommendaton . getResourceCount ( ) ) ) ; recommendations . add ( rpProtectionRecommendaton ) ; return recommendations ; }
public void test() { if ( logger . isTraceEnabled ( LogMarker . SERIALIZER_VERBOSE ) ) { logger . trace ( LogMarker . SERIALIZER_VERBOSE , "Writing File {}" , file ) ; } }
public void test() { try { SessionState ss = ( ( JetspeedRunData ) rundata ) . getPortletSessionState ( peid ) ; ControllerState state = ( ControllerState ) ss . getAttribute ( "state" ) ; state . recycle ( ) ; ss . removeAttribute ( "state" ) ; ss . clear ( ) ; } catch ( Exception e ) { log . warn ( e . getMessage ( ) , e ) ; } }
public void test() { if ( submittedRequest == null ) { submitTMRequest ( action ) ; } else { Log . debug ( "blocking TM request until outstanding request returns" ) ; } }
@ Override protected void doGet ( @ Nullable HttpServletRequest req , @ Nullable HttpServletResponse resp ) throws ServletException , IOException { logger . debug ( "Spotify auth callback servlet received GET request {}." , req . getRequestURI ( ) ) ; final String servletBaseURL = req . getRequestURL ( ) . toString ( ) ; final Map < String , String > replaceMap = new HashMap < > ( ) ; handleSpotifyRedirect ( replaceMap , servletBaseURL , req . getQueryString ( ) ) ; resp . setContentType ( CONTENT_TYPE ) ; replaceMap . put ( KEY_REDIRECT_URI , servletBaseURL ) ; replaceMap . put ( KEY_PLAYERS , formatPlayers ( playerTemplate , servletBaseURL ) ) ; resp . getWriter ( ) . append ( replaceKeysFromMap ( indexTemplate , replaceMap ) ) ; resp . getWriter ( ) . close ( ) ; }
public void test() { try { logger . info ( "connecting to %s:%s ..." , ips . get ( index ) , ports . get ( index ) ) ; URL url = buildUrl ( endpoint , param , ips . get ( index ) , ports . get ( index ) ) ; HttpURLConnection conn = openConnection ( url ) ; conn . setConnectTimeout ( DEFAULT_CONNECTION_TIMEOUT ) ; code_block = IfStatement ; conn . setRequestMethod ( method ) ; conn . setUseCaches ( false ) ; conn . setDoInput ( true ) ; code_block = ForStatement ; } catch ( URISyntaxException | IOException | InterruptedException e ) { logger . error ( e ) ; } }
public void test() { try { code_block = IfStatement ; int code = conn . getResponseCode ( ) ; code_block = IfStatement ; logger . error ( "code: %d, msg: %s" , code , conn . getResponseMessage ( ) ) ; code_block = IfStatement ; break ; } catch ( IOException e ) { logger . error ( "Error when connecting to %s:%s:\n%s" , ips . get ( index ) , ports . get ( index ) , e ) ; } }
public void test() { try { logger . info ( "connecting to %s:%s ..." , ips . get ( index ) , ports . get ( index ) ) ; URL url = buildUrl ( endpoint , param , ips . get ( index ) , ports . get ( index ) ) ; HttpURLConnection conn = openConnection ( url ) ; conn . setConnectTimeout ( DEFAULT_CONNECTION_TIMEOUT ) ; code_block = IfStatement ; conn . setRequestMethod ( method ) ; conn . setUseCaches ( false ) ; conn . setDoInput ( true ) ; code_block = ForStatement ; } catch ( URISyntaxException | IOException | InterruptedException e ) { logger . error ( e ) ; } }
@ Override public void perform ( ) throws Exception { getLogger ( ) . info ( "Performing action: Restart active master" ) ; ServerName master = cluster . getClusterMetrics ( ) . getMasterName ( ) ; restartMaster ( master , sleepTime ) ; }
public void test() { try { cleanUpExpiredTransactions ( ) ; } catch ( Throwable t ) { log . error ( t , "Unexpected exception while cleaning up expired transactions" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "A new run info set " + runInfo ) ; } }
public void test() { try { LOG . debug ( "compiling stylesheet {}" , stylesheet ) ; final Templates template = factory . newTemplates ( new StreamSource ( stylesheet ) ) ; handler = factory . newTransformerHandler ( template ) ; } catch ( final TransformerConfigurationException e ) { throw new TriggerException ( e . getMessage ( ) , e ) ; } }
public void test() { if ( application . isBundled ( ) ) { String redirectPath = application . getBaseUrl ( ) + "/" + pageName ; log . info ( String . format ( "Redirecting to bundled app: %s" , redirectPath ) ) ; response . sendRedirect ( redirectPath ) ; return ; } }
public void test() { if ( application . getActivities ( ) != null && application . getActivities ( ) . getDhis ( ) != null && "*" . equals ( application . getActivities ( ) . getDhis ( ) . getHref ( ) ) ) { String contextPath = ContextUtils . getContextPath ( request ) ; log . debug ( String . format ( "Manifest context path: '%s'" , contextPath ) ) ; application . getActivities ( ) . getDhis ( ) . setHref ( contextPath ) ; } }
public void test() { if ( nextLoadTime > 0 ) { LOG . info ( "Lookup join cache has expired after {} minute(s), reloading" , reloadInterval . toMinutes ( ) ) ; } else { LOG . info ( "Populating lookup join cache" ) ; } }
public void test() { if ( nextLoadTime > 0 ) { LOG . info ( "Lookup join cache has expired after {} minute(s), reloading" , reloadInterval . toMinutes ( ) ) ; } else { LOG . info ( "Populating lookup join cache" ) ; } }
public void test() { try { long count = 0 ; GenericRowData reuse = new GenericRowData ( rowType . getFieldCount ( ) ) ; partitionReader . open ( partitionFetcher . fetch ( fetcherContext ) ) ; RowData row ; code_block = WhileStatement ; partitionReader . close ( ) ; nextLoadTime = System . currentTimeMillis ( ) + reloadInterval . toMillis ( ) ; LOG . info ( "Loaded {} row(s) into lookup join cache" , count ) ; return ; } catch ( Exception e ) { code_block = IfStatement ; numRetry ++ ; long toSleep = numRetry * RETRY_INTERVAL . toMillis ( ) ; LOG . warn ( String . format ( "Failed to load table into cache, will retry in %d seconds" , toSleep / 1000 ) , e ) ; code_block = TryStatement ;  } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "MANAGER[" + name + "] SESSION[" + branchSession + "] " + LogOperation . BRANCH_REMOVE ) ; } }
public void test() { if ( ! namedclsDir . exists ( ) ) { _log . info ( "*No namedcls directory so skip it: namedclsPath=" + namedclsPath ) ; return ; } }
public void test() { if ( fileLocation == null ) { log . debug ( "User connection configuration file not found" ) ; return props ; } }
@ Test public void testFilter ( ) { String xml = "<wfs:GetFeature " + "service=\"WFS\" " + "version=\"1.1.0\" " + "outputFormat=\"gml32\" " + "xmlns:ogc=\"http://www.opengis.net/ogc\" " + "xmlns:wfs=\"http://www.opengis.net/wfs\" " + "xmlns:gml=\"http://www.opengis.net/gml/3.2\" " + "xmlns:wml2dr=\"http://www.opengis.net/waterml/DR/2.0\" " + "xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" " + "xsi:schemaLocation=\"" + "http://www.opengis.net/wfs http://schemas.opengis.net/wfs/1.1.0/wfs.xsd" + "\"" + ">" + "<wfs:Query typeName=\"wml2dr:MeasurementTimeseriesDomainRange\">" + "    <ogc:Filter>" + "        <ogc:PropertyIsLike wildCard=\"*\" singleChar=\"\" escapeChar=\"\\\">" + "            <ogc:PropertyName>wml2dr:MeasurementTimeseriesDomainRange/gml:rangeSet/gml:QuantityList</ogc:PropertyName>" + "            <ogc:Literal>*16.2*</ogc:Literal>" + "        </ogc:PropertyIsLike>" + "    </ogc:Filter>" + "</wfs:Query> " + "</wfs:GetFeature>" ; validate ( xml ) ; Document doc = postAsDOM ( "wfs" , xml ) ; LOGGER . info ( "WFS filter GetFeature response:\n" + prettyString ( doc ) ) ; assertEquals ( "wfs:FeatureCollection" , doc . getDocumentElement ( ) . getNodeName ( ) ) ; assertXpathEvaluatesTo ( "1" , "/wfs:FeatureCollection/@numberReturned" , doc ) ; assertXpathCount ( 1 , "//wml2dr:MeasurementTimeseriesDomainRange" , doc ) ; assertXpathEvaluatesTo ( "ID2" , "//wml2dr:MeasurementTimeseriesDomainRange/@gml:id" , doc ) ; assertXpathEvaluatesTo ( "tpl." + "ID2" , "//wml2dr:MeasurementTimeseriesDomainRange[@gml:id='" + "ID2" + "']/gml:domainSet/wml2dr:TimePositionList/@gml:id" , doc ) ; assertXpathEvaluatesTo ( "16.2" , "//wml2dr:MeasurementTimeseriesDomainRange[@gml:id='" + "ID2" + "']/gml:rangeSet/gml:QuantityList" , doc ) ; assertXpathEvaluatesTo ( "degC" , "//wml2dr:MeasurementTimeseriesDomainRange[@gml:id='" + "ID2" + "']/gml:rangeSet/gml:QuantityList/@uom" , doc ) ; assertXpathEvaluatesTo ( "string" , "//wml2dr:MeasurementTimeseriesDomainRange[@gml:id='" + "ID2" + "']/gml:coverageFunction/gml:MappingRule" , doc ) ; assertXpathEvaluatesTo ( "http://ns.bgs.ac.uk/thesaurus/lithostratigraphy" , "//wml2dr:MeasurementTimeseriesDomainRange[@gml:id='" + "ID2" + "']/gmlcov:rangeType/@xlink:href" , doc ) ; assertXpathEvaluatesTo ( "1949-05-01T00:00:00Z" , "//wml2dr:MeasurementTimeseriesDomainRange[@gml:id='" + "ID2" + "']/gml:domainSet/wml2dr:TimePositionList/wml2dr:timePositionList" , doc ) ; }
protected final boolean isLayerizedNetwork ( String networkId ) { log . debug ( "" ) ; String connType = getConnectionType ( networkId ) ; code_block = IfStatement ; return false ; }
public void test() { try { code_block = IfStatement ; } catch ( SQLException e ) { log . warn ( "Rollback failed" , e ) ; } }
public static String runAndWaitArray ( final String [ ] command ) { log . trace ( "Running command on the shell: {}" , Arrays . toString ( command ) ) ; String result = runAndWaitNoLog ( command ) ; log . trace ( "Result:" + result ) ; return result ; }
public static String runAndWaitArray ( final String [ ] command ) { log . trace ( "Running command on the shell: {}" , Arrays . toString ( command ) ) ; String result = runAndWaitNoLog ( command ) ; log . trace ( "Result:" + result ) ; return result ; }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceOrderItemServiceUtil . class , "updateCommerceOrderItemUnitPrice" , _updateCommerceOrderItemUnitPriceParameterTypes29 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , commerceOrderItemId , quantity , unitPrice ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . commerce . model . CommerceOrderItem ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( ! type . equals ( DATE ) && ! type . equals ( TIMESTAMP ) ) { log . warn ( "Temporal column type of table ID %s set incorrectly to %s" , tableId , type ) ; return false ; } }
private void handleControlMessage ( OdeControlData controlData ) { String infoMsg = controlData . toJson ( false ) ; logger . info ( infoMsg ) ; EventLogger . logger . info ( infoMsg ) ; }
public List < Template > findAllTemplates ( ) { log . debug ( "findAllTemplates()" ) ; return templateRepository . findAll ( ) ; }
public void test() { try { KieServerConfigDto serverConfigDto = kieConfigService . getConfig ( configCode ) ; KieBpmConfig bpmConfig = kieConfigService . buildConfig ( serverConfigDto ) ; cases = caseManager . getCasesDefinitions ( bpmConfig , deploymentUnit ) ; } catch ( Exception e ) { logger . error ( "Failed to fetch container ids " , e ) ; } }
@ Test public void stringImageInputStream ( ) { LOGGER . info ( "Testing capabilities of StringImageInputStreamSpi" ) ; final String inURLToFile = TestData . getResource ( this , "a.txt" ) . toString ( ) ; ImageInputStream instream ; code_block = TryStatement ;  Assert . assertNotNull ( "Unable to get an StringImageInputStreamSpi from a URL pointing to a File" , instream ) ; code_block = TryStatement ;  Assert . assertNotNull ( "Unable to get an URLImageInputStreamSpi from a URL pointing to an http page" , instream ) ; code_block = TryStatement ;  LOGGER . info ( "Testing capabilities of URLImageInputStreamSpi: SUCCESS!!!" ) ; }
public void test() { if ( startupLogger != null ) { startupLogger . info ( "shutting down..." ) ; } }
public void test() { if ( ! thread . isDaemon ( ) && thread != Thread . currentThread ( ) && stackTrace . length != 0 ) { startupLogger . info ( "Found non-daemon thread after shutdown: {}\n    {}" , thread . getName ( ) , Joiner . on ( "\n    " ) . join ( stackTrace ) ) ; } }
public void test() { if ( startupLogger == null ) { t . printStackTrace ( ) ; } else { startupLogger . error ( "error during shutdown: {}" , t . getMessage ( ) , t ) ; } }
@ Override public synchronized void start ( ) { Assert . state ( m_worker == null , "The fiber has already run or is running" ) ; m_worker = new Thread ( this , getName ( ) ) ; m_worker . start ( ) ; m_status = STARTING ; LOG . info ( "start: scheduler started" ) ; }
@ Override public boolean visitEnter ( final HierComposite node ) { log . info ( "visitEnter()" ) ; code_block = IfStatement ; log . debug ( "checking control rod" ) ; aborted = checkControlRod ( node ) ; code_block = IfStatement ; log . info ( "obtaining metadata for:{}" , node ) ; code_block = TryStatement ;  }
public void test() { try { List < MetaDataAndDomainData > metadata = collectionAO . findMetadataValuesForCollection ( node . getAbsolutePath ( ) , 0 ) ; metadataRollup . getMetadata ( ) . push ( metadata ) ; log . info ( "pushed metadata in the stack...now filter and then delegate to visitEnterWithMetadata() in the impl class to make any determinations" ) ; code_block = IfStatement ; boolean shortCircuit = visitEnterWithMetadata ( node , metadataRollup ) ; return shortCircuit ; } catch ( JargonException | JargonQueryException e ) { log . error ( "error in obtaining metadata" , e ) ; throw new JargonRuntimeException ( "error getting metadata" , e ) ; } }
public void test() { try { List < MetaDataAndDomainData > metadata = collectionAO . findMetadataValuesForCollection ( node . getAbsolutePath ( ) , 0 ) ; metadataRollup . getMetadata ( ) . push ( metadata ) ; log . info ( "pushed metadata in the stack...now filter and then delegate to visitEnterWithMetadata() in the impl class to make any determinations" ) ; code_block = IfStatement ; boolean shortCircuit = visitEnterWithMetadata ( node , metadataRollup ) ; return shortCircuit ; } catch ( JargonException | JargonQueryException e ) { log . error ( "error in obtaining metadata" , e ) ; throw new JargonRuntimeException ( "error getting metadata" , e ) ; } }
public void test() { if ( entrySize > maxSaneEntrySize ) { LOG . warn ( "Sanity check failed for entry size of " + entrySize + " at location " + pos + " in " + entryLogId ) ; } }
public void test() { try { mu = mp . getUsage ( ) ; } catch ( IllegalArgumentException ex ) { continue ; } catch ( InternalError ie ) { s . close ( ) ; it . remove ( ) ; reInitPools = true ; logger . warn ( "Accessing MemoryPool '{}' threw an Internal Error: {}" , mp . getName ( ) , ie . getMessage ( ) ) ; continue ; } }
public void test() { if ( totalItemRead < expectedNumberOfDocuments ) { logger . info ( "Total item read {} from {} is less than {}, retrying reads" , totalItemRead , this . client . getReadEndpoint ( ) , expectedNumberOfDocuments ) ; code_block = TryStatement ;  continue ; } else { logger . info ( "Read {} items from {}" , totalItemRead , this . client . getReadEndpoint ( ) ) ; break ; } }
public void test() { try { repositoryInfo = m_access . describeRepository ( context ) ; code_block = IfStatement ; } catch ( AuthzException ae ) { throw ae ; } catch ( Throwable th ) { String msg = "Error describing repository" ; logger . error ( msg , th ) ; throw new GeneralException ( msg , th ) ; } }
public void test() { try { loggedInUserId = AuthHelper . getGuestId ( ) ; accessAllowed = isOwnerOrAdmin ( uid ) ; code_block = IfStatement ; } catch ( Exception e ) { LOG . error ( "BodyTrackController.getFluxtreamCapturePhoto(): Exception while trying to check authorization." , e ) ; } }
public void test() { try { photo = photoFetchStrategy . getPhoto ( ) ; } catch ( Exception e ) { final String message = "Exception while trying to get photo [" + photoFetchStrategy . getPhotoIdentifier ( ) + "]" ; LOG . error ( "BodyTrackController.getFluxtreamCapturePhoto(): " + message , e ) ; return jsonResponseHelper . internalServerError ( message ) ; } }
public void test() { if ( photo == null ) { final String message = "Photo [" + photoFetchStrategy . getPhotoIdentifier ( ) + "] requested by user [" + loggedInUserId + "] not found" ; LOG . error ( "BodyTrackController.getFluxtreamCapturePhoto(): " + message ) ; return jsonResponseHelper . notFound ( message ) ; } }
public void test() { if ( ! accessAllowed ) { final TrustedBuddy trustedBuddy = buddiesService . getTrustedBuddy ( loggedInUserId , uid ) ; code_block = IfStatement ; } }
public void test() { try { queue . poll ( ) . run ( ) ; } catch ( Throwable e ) { log . error ( e , "Task failed" ) ; } }
public void test() { if ( resultI . getSuperTypes ( ) . isEmpty ( ) ) { log . warn ( "Unable to find catalog item for type " + typeName + ". There is an existing catalog item with ID " + resultI . getId ( ) + " but it doesn't define a class type." ) ; return null ; } }
public void test() { try { jobInstance = jobService . getJobInstance ( jobId ) ; } catch ( Exception e ) { logger . error ( e . getLocalizedMessage ( ) , e ) ; throw new InternalErrorException ( e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( DLFolderServiceUtil . class , "getFoldersAndFileEntriesAndFileShortcutsCount" , _getFoldersAndFileEntriesAndFileShortcutsCountParameterTypes23 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , folderId , mimeTypes , includeMountFolders , queryDefinition ) ; Object returnObj = null ; code_block = TryStatement ;  return ( ( Integer ) returnObj ) . intValue ( ) ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
private void writeSerializedPublicKeyLength ( SrpClientKeyExchangeMessage msg ) { appendInt ( msg . getPublicKeyLength ( ) . getValue ( ) , HandshakeByteLength . SRP_PUBLICKEY_LENGTH ) ; LOGGER . debug ( "SerializedPublicKexLength: " + msg . getPublicKeyLength ( ) . getValue ( ) ) ; }
@ Override public void destroyNode ( String id ) { VirtualGuest guest = getNode ( id ) ; if ( guest == null ) return ; if ( guest . getBillingItemId ( ) == - 1 ) throw new IllegalStateException ( String . format ( "no billing item for guest(%s) so we cannot cancel the order" , id ) ) ; logger . debug ( ">> canceling service for guest(%s) billingItem(%s)" , id , guest . getBillingItemId ( ) ) ; client . getVirtualGuestClient ( ) . cancelService ( guest . getBillingItemId ( ) ) ; }
@ Override public void debug ( String msg , Throwable thrown ) { logger . debug ( msg , thrown ) ; }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Append record id = " + id + " recordType = " + recordType ) ; } }
public void test() { if ( null != currentContent ) { code_block = IfStatement ; currentContent . setLastEditor ( this . getCurrentUser ( ) . getUsername ( ) ) ; code_block = IfStatement ; String sessionParamName = ContentActionConstants . SESSION_PARAM_NAME_CURRENT_CONTENT_PREXIX + this . getContentOnSessionMarker ( ) ; this . getRequest ( ) . getSession ( ) . removeAttribute ( sessionParamName ) ; _logger . info ( "Salvato contenuto " + currentContent . getId ( ) + " - Descrizione: '" + currentContent . getDescription ( ) + "' - Utente: " + this . getCurrentUser ( ) . getUsername ( ) ) ; } else { _logger . error ( "Tentativo Salvataggio/approvazione contenuto NULLO - Utente: " + this . getCurrentUser ( ) . getUsername ( ) ) ; } }
public void test() { try { Content currentContent = this . getContent ( ) ; code_block = IfStatement ; } catch ( Exception e ) { _logger . error ( "Error extracting saving content" , e ) ; return FAILURE ; } }
@ Override @ Transactional ( readOnly = false ) public void setTimeZone ( long guestId , String date , String timeZone ) { logger . warn ( "component=metadata action=setTimeZone message=attempt to set timezone" ) ; }
protected void optimizeFile ( ) { final ElapsedTimer timer = new ElapsedTimer ( ) ; timesOptimized ++ ; log . info ( "{0}: Beginning Optimization {1}" , logCacheName , timesOptimized ) ; IndexedDiskElementDescriptor [ ] defragList = null ; storageLock . writeLock ( ) . lock ( ) ; code_block = TryStatement ;  long expectedNextPos = defragFile ( defragList , 0 ) ; storageLock . writeLock ( ) . lock ( ) ; code_block = TryStatement ;  log . info ( "{0}: Finished {1}, Optimization took {2}" , logCacheName , timesOptimized , timer . getElapsedTimeString ( ) ) ; }
public void test() { try { code_block = IfStatement ; dataFile . truncate ( expectedNextPos ) ; } catch ( final IOException e ) { log . error ( "{0}: Error optimizing queued puts." , logCacheName , e ) ; } }
protected void optimizeFile ( ) { final ElapsedTimer timer = new ElapsedTimer ( ) ; timesOptimized ++ ; log . info ( "{0}: Beginning Optimization {1}" , logCacheName , timesOptimized ) ; IndexedDiskElementDescriptor [ ] defragList = null ; storageLock . writeLock ( ) . lock ( ) ; code_block = TryStatement ;  long expectedNextPos = defragFile ( defragList , 0 ) ; storageLock . writeLock ( ) . lock ( ) ; code_block = TryStatement ;  log . info ( "{0}: Finished {1}, Optimization took {2}" , logCacheName , timesOptimized , timer . getElapsedTimeString ( ) ) ; }
@ Override public void removeItemMetadata ( String name ) { logger . debug ( "Removing all metadata for item {}" , name ) ; getAll ( ) . stream ( ) . filter ( MetadataPredicates . ofItem ( name ) ) . map ( Metadata :: getUID ) . forEach ( this :: remove ) ; }
public void test() { if ( compssHome == null || compssHome . isEmpty ( ) ) { LOGGER . warn ( "WARN: COMPSS_HOME not defined, no adaptors loaded." ) ; return ; } }
public void test() { try { Classpath . loadPath ( compssHome + ADAPTORS_REL_PATH , LOGGER ) ; } catch ( FileNotFoundException ex ) { LOGGER . warn ( "WARN_MSG = [Adaptors folder not defined, no adaptors loaded.]" ) ; } }
public void test() { try { view . callModelChanged ( ) ; } catch ( Exception e ) { String msg = "View [" + view . getViewName ( ) + "] caused an error while displaying new contents: " + e . getMessage ( ) ; setWarningMessage ( msg ) ; m_logger . debug ( msg , e ) ; } }
public void test() { try { session . getWorkspace ( ) . copy ( srcPath , destPath ) ; return session . getNode ( destPath ) ; } catch ( AccessDeniedException e ) { log . debug ( "Access denied" , e ) ; throw new AccessControlException ( e . getMessage ( ) ) ; } catch ( RepositoryException e ) { throw new MetadataRepositoryException ( "Failed to copy source path: " + srcPath + " to destination path: " + destPath , e ) ; } }
public void test() { if ( IS_CLOSED_UPDATER . get ( this ) == TRUE ) { log . warn ( "[{}] Dispatcher is already closed. Closing consumer {}" , name , consumer ) ; consumer . disconnect ( ) ; return ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "PortletRequestDispatcher requested for path: " + path + " at context: " + app . getContextPath ( ) ) ; } }
public void test() { if ( LOG . isInfoEnabled ( ) ) { LOG . info ( "Failed to retrieve PortletRequestDispatcher: path name must begin with a slash '/'." ) ; } }
public void test() { if ( LOG . isInfoEnabled ( ) ) { LOG . info ( "No matching request dispatcher found for: " + path ) ; } }
public void test() { if ( LOG . isInfoEnabled ( ) ) { LOG . info ( "Failed to retrieve PortletRequestDispatcher: " + ex . getMessage ( ) ) ; } }
public void test() { if ( ! response . getType ( ) . equals ( ServiceResponse . ResponseType . SUCCESS ) ) { logger . debug ( "Scanner (scan now) failed on server instance {} due to {}" , container . getUrl ( ) , response . getMsg ( ) ) ; container . setStatus ( KieContainerStatus . FAILED ) ; } }
@ Override public void onReceive ( Double doubleOut , String stringOut ) { logger . info ( name . getMethodName ( ) + " - callback - got broadcast" ) ; code_block = IfStatement ; subscribeBroadcastWithMultiplePrimitiveParametersCallbackDone = true ; }
public void test() { if ( ! stringOut . equals ( "boom" ) || ! IltUtil . cmpDouble ( doubleOut , 1.1d ) ) { logger . info ( name . getMethodName ( ) + " - callback - invalid content" ) ; subscribeBroadcastWithMultiplePrimitiveParametersCallbackResult = false ; } else { logger . info ( name . getMethodName ( ) + " - callback - content OK" ) ; subscribeBroadcastWithMultiplePrimitiveParametersCallbackResult = true ; } }
public void test() { if ( ! stringOut . equals ( "boom" ) || ! IltUtil . cmpDouble ( doubleOut , 1.1d ) ) { logger . info ( name . getMethodName ( ) + " - callback - invalid content" ) ; subscribeBroadcastWithMultiplePrimitiveParametersCallbackResult = false ; } else { logger . info ( name . getMethodName ( ) + " - callback - content OK" ) ; subscribeBroadcastWithMultiplePrimitiveParametersCallbackResult = true ; } }
public void test() { if ( ! rs . getOwner ( ) . equals ( auth . getName ( ) ) ) { logger . warn ( "Unauthorized resource set request from bad user; expected " + rs . getOwner ( ) + " got " + auth . getName ( ) ) ; m . addAttribute ( HttpCodeView . CODE , HttpStatus . FORBIDDEN ) ; return HttpCodeView . VIEWNAME ; } }
public void test() { if ( VeluxBindingConstants . SUPPORTED_THINGS_BINDING . contains ( thingTypeUID ) ) { resultHandler = createBindingHandler ( thing ) ; } else-if ( VeluxBindingConstants . SUPPORTED_THINGS_BRIDGE . contains ( thingTypeUID ) ) { resultHandler = createBridgeHandler ( thing ) ; } else-if ( VeluxBindingConstants . SUPPORTED_THINGS_ITEMS . contains ( thingTypeUID ) ) { resultHandler = createThingHandler ( thing ) ; } else { logger . warn ( "createHandler({}) failed: ThingHandler not found for {}." , thingTypeUID , thing . getLabel ( ) ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceOrderServiceUtil . class , "updateCommerceOrder" , _updateCommerceOrderParameterTypes37 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , externalReferenceCode , commerceOrderId , billingAddressId , shippingAddressId , commercePaymentMethodKey , commerceShippingMethodId , shippingOptionName , purchaseOrderNumber , subtotal , shippingAmount , total , subtotalWithTaxAmount , shippingWithTaxAmount , totalWithTaxAmount , advanceStatus , commerceContext ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . commerce . model . CommerceOrder ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { return listListenableFuture . get ( timeout , TimeUnit . MILLISECONDS ) ; } catch ( InterruptedException e ) { Thread . currentThread ( ) . interrupt ( ) ; } catch ( ExecutionException e ) { logger . info ( "ExecutionError {} {}" , name , e . getMessage ( ) ) ; } catch ( TimeoutException e ) { logger . info ( "Timeout {} {}" , name , e . getMessage ( ) ) ; } }
public void test() { try { return listListenableFuture . get ( timeout , TimeUnit . MILLISECONDS ) ; } catch ( InterruptedException e ) { Thread . currentThread ( ) . interrupt ( ) ; } catch ( ExecutionException e ) { logger . info ( "ExecutionError {} {}" , name , e . getMessage ( ) ) ; } catch ( TimeoutException e ) { logger . info ( "Timeout {} {}" , name , e . getMessage ( ) ) ; } }
public void test() { try { clientStack . push ( syncClientFactory . getSyncClient ( node , this ) ) ; NodeStatusManager . getINSTANCE ( ) . activate ( node ) ; } catch ( TTransportException e ) { logger . error ( "Cannot open transport for client {}" , node , e ) ; nodeClientNumMap . computeIfPresent ( clusterNode , ( n , oldValue ) -> oldValue - 1 ) ; NodeStatusManager . getINSTANCE ( ) . deactivate ( node ) ; } }
public void test() { if ( connectionExecutor . getQueue ( ) . size ( ) > queuewarninglimit ) { logger . warn ( new IllegalThreadStateException ( "connectionordered channel handler `queue size: " + connectionExecutor . getQueue ( ) . size ( ) + " exceed the warning limit number :" + queuewarninglimit ) ) ; } }
public void test() { try { task . run ( ) ; } catch ( Throwable t ) { log . error ( "Exception while executing runnable " + task , t ) ; Throwables . throwIfUnchecked ( t ) ; throw new AssertionError ( t ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "adding a user:{}" , user ) ; } }
@ Override public User addUser ( final User user ) throws JargonException , DuplicateDataException { code_block = IfStatement ; code_block = IfStatement ; updatePreChecks ( user ) ; GeneralAdminInp adminPI = GeneralAdminInp . instanceForAddUser ( user ) ; log . debug ( "executing admin PI" ) ; code_block = TryStatement ;  log . debug ( "user added, now process other fields" ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; return findByName ( user . getName ( ) ) ; }
public void test() { try { getIRODSProtocol ( ) . irodsFunction ( adminPI ) ; } catch ( DuplicateDataException dde ) { throw dde ; } catch ( NoMoreRulesException nmr ) { log . warn ( "no more rules exception caught, will throw as duplicate data for backwards compatibility" , nmr ) ; throw new DuplicateDataException ( "no more rules interpereted as duplicate data exception for backwards compatibility" ) ; } }
@ Override public User addUser ( final User user ) throws JargonException , DuplicateDataException { code_block = IfStatement ; code_block = IfStatement ; updatePreChecks ( user ) ; GeneralAdminInp adminPI = GeneralAdminInp . instanceForAddUser ( user ) ; log . debug ( "executing admin PI" ) ; code_block = TryStatement ;  log . debug ( "user added, now process other fields" ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; return findByName ( user . getName ( ) ) ; }
public void test() { switch ( messageType ) { case CONNECT : case CONNACK : case PINGREQ : case PINGRESP : case DISCONNECT : LOG . info ( "{} {} <{}>" , direction , messageType , clientID ) ; break ; case SUBSCRIBE : MqttSubscribeMessage subscribe = ( MqttSubscribeMessage ) msg ; LOG . info ( "{} SUBSCRIBE <{}> to topics {}" , direction , clientID , subscribe . payload ( ) . topicSubscriptions ( ) ) ; break ; case UNSUBSCRIBE : MqttUnsubscribeMessage unsubscribe = ( MqttUnsubscribeMessage ) msg ; LOG . info ( "{} UNSUBSCRIBE <{}> to topics <{}>" , direction , clientID , unsubscribe . payload ( ) . topics ( ) ) ; break ; case PUBLISH : MqttPublishMessage publish = ( MqttPublishMessage ) msg ; LOG . info ( "{} PUBLISH <{}> to topics <{}>" , direction , clientID , publish . variableHeader ( ) . topicName ( ) ) ; break ; case PUBREC : case PUBCOMP : case PUBREL : case PUBACK : case UNSUBACK : LOG . info ( "{} {} <{}> packetID <{}>" , direction , messageType , clientID , messageId ( msg ) ) ; break ; case SUBACK : MqttSubAckMessage suback = ( MqttSubAckMessage ) msg ; final List < Integer > grantedQoSLevels = suback . payload ( ) . grantedQoSLevels ( ) ; LOG . info ( "{} SUBACK <{}> packetID <{}>, grantedQoses {}" , direction , clientID , messageId ( msg ) , grantedQoSLevels ) ; break ; } }
public void test() { switch ( messageType ) { case CONNECT : case CONNACK : case PINGREQ : case PINGRESP : case DISCONNECT : LOG . info ( "{} {} <{}>" , direction , messageType , clientID ) ; break ; case SUBSCRIBE : MqttSubscribeMessage subscribe = ( MqttSubscribeMessage ) msg ; LOG . info ( "{} SUBSCRIBE <{}> to topics {}" , direction , clientID , subscribe . payload ( ) . topicSubscriptions ( ) ) ; break ; case UNSUBSCRIBE : MqttUnsubscribeMessage unsubscribe = ( MqttUnsubscribeMessage ) msg ; LOG . info ( "{} UNSUBSCRIBE <{}> to topics <{}>" , direction , clientID , unsubscribe . payload ( ) . topics ( ) ) ; break ; case PUBLISH : MqttPublishMessage publish = ( MqttPublishMessage ) msg ; LOG . info ( "{} PUBLISH <{}> to topics <{}>" , direction , clientID , publish . variableHeader ( ) . topicName ( ) ) ; break ; case PUBREC : case PUBCOMP : case PUBREL : case PUBACK : case UNSUBACK : LOG . info ( "{} {} <{}> packetID <{}>" , direction , messageType , clientID , messageId ( msg ) ) ; break ; case SUBACK : MqttSubAckMessage suback = ( MqttSubAckMessage ) msg ; final List < Integer > grantedQoSLevels = suback . payload ( ) . grantedQoSLevels ( ) ; LOG . info ( "{} SUBACK <{}> packetID <{}>, grantedQoses {}" , direction , clientID , messageId ( msg ) , grantedQoSLevels ) ; break ; } }
public void test() { switch ( messageType ) { case CONNECT : case CONNACK : case PINGREQ : case PINGRESP : case DISCONNECT : LOG . info ( "{} {} <{}>" , direction , messageType , clientID ) ; break ; case SUBSCRIBE : MqttSubscribeMessage subscribe = ( MqttSubscribeMessage ) msg ; LOG . info ( "{} SUBSCRIBE <{}> to topics {}" , direction , clientID , subscribe . payload ( ) . topicSubscriptions ( ) ) ; break ; case UNSUBSCRIBE : MqttUnsubscribeMessage unsubscribe = ( MqttUnsubscribeMessage ) msg ; LOG . info ( "{} UNSUBSCRIBE <{}> to topics <{}>" , direction , clientID , unsubscribe . payload ( ) . topics ( ) ) ; break ; case PUBLISH : MqttPublishMessage publish = ( MqttPublishMessage ) msg ; LOG . info ( "{} PUBLISH <{}> to topics <{}>" , direction , clientID , publish . variableHeader ( ) . topicName ( ) ) ; break ; case PUBREC : case PUBCOMP : case PUBREL : case PUBACK : case UNSUBACK : LOG . info ( "{} {} <{}> packetID <{}>" , direction , messageType , clientID , messageId ( msg ) ) ; break ; case SUBACK : MqttSubAckMessage suback = ( MqttSubAckMessage ) msg ; final List < Integer > grantedQoSLevels = suback . payload ( ) . grantedQoSLevels ( ) ; LOG . info ( "{} SUBACK <{}> packetID <{}>, grantedQoses {}" , direction , clientID , messageId ( msg ) , grantedQoSLevels ) ; break ; } }
public void test() { switch ( messageType ) { case CONNECT : case CONNACK : case PINGREQ : case PINGRESP : case DISCONNECT : LOG . info ( "{} {} <{}>" , direction , messageType , clientID ) ; break ; case SUBSCRIBE : MqttSubscribeMessage subscribe = ( MqttSubscribeMessage ) msg ; LOG . info ( "{} SUBSCRIBE <{}> to topics {}" , direction , clientID , subscribe . payload ( ) . topicSubscriptions ( ) ) ; break ; case UNSUBSCRIBE : MqttUnsubscribeMessage unsubscribe = ( MqttUnsubscribeMessage ) msg ; LOG . info ( "{} UNSUBSCRIBE <{}> to topics <{}>" , direction , clientID , unsubscribe . payload ( ) . topics ( ) ) ; break ; case PUBLISH : MqttPublishMessage publish = ( MqttPublishMessage ) msg ; LOG . info ( "{} PUBLISH <{}> to topics <{}>" , direction , clientID , publish . variableHeader ( ) . topicName ( ) ) ; break ; case PUBREC : case PUBCOMP : case PUBREL : case PUBACK : case UNSUBACK : LOG . info ( "{} {} <{}> packetID <{}>" , direction , messageType , clientID , messageId ( msg ) ) ; break ; case SUBACK : MqttSubAckMessage suback = ( MqttSubAckMessage ) msg ; final List < Integer > grantedQoSLevels = suback . payload ( ) . grantedQoSLevels ( ) ; LOG . info ( "{} SUBACK <{}> packetID <{}>, grantedQoses {}" , direction , clientID , messageId ( msg ) , grantedQoSLevels ) ; break ; } }
public void test() { switch ( messageType ) { case CONNECT : case CONNACK : case PINGREQ : case PINGRESP : case DISCONNECT : LOG . info ( "{} {} <{}>" , direction , messageType , clientID ) ; break ; case SUBSCRIBE : MqttSubscribeMessage subscribe = ( MqttSubscribeMessage ) msg ; LOG . info ( "{} SUBSCRIBE <{}> to topics {}" , direction , clientID , subscribe . payload ( ) . topicSubscriptions ( ) ) ; break ; case UNSUBSCRIBE : MqttUnsubscribeMessage unsubscribe = ( MqttUnsubscribeMessage ) msg ; LOG . info ( "{} UNSUBSCRIBE <{}> to topics <{}>" , direction , clientID , unsubscribe . payload ( ) . topics ( ) ) ; break ; case PUBLISH : MqttPublishMessage publish = ( MqttPublishMessage ) msg ; LOG . info ( "{} PUBLISH <{}> to topics <{}>" , direction , clientID , publish . variableHeader ( ) . topicName ( ) ) ; break ; case PUBREC : case PUBCOMP : case PUBREL : case PUBACK : case UNSUBACK : LOG . info ( "{} {} <{}> packetID <{}>" , direction , messageType , clientID , messageId ( msg ) ) ; break ; case SUBACK : MqttSubAckMessage suback = ( MqttSubAckMessage ) msg ; final List < Integer > grantedQoSLevels = suback . payload ( ) . grantedQoSLevels ( ) ; LOG . info ( "{} SUBACK <{}> packetID <{}>, grantedQoses {}" , direction , clientID , messageId ( msg ) , grantedQoSLevels ) ; break ; } }
public void test() { switch ( messageType ) { case CONNECT : case CONNACK : case PINGREQ : case PINGRESP : case DISCONNECT : LOG . info ( "{} {} <{}>" , direction , messageType , clientID ) ; break ; case SUBSCRIBE : MqttSubscribeMessage subscribe = ( MqttSubscribeMessage ) msg ; LOG . info ( "{} SUBSCRIBE <{}> to topics {}" , direction , clientID , subscribe . payload ( ) . topicSubscriptions ( ) ) ; break ; case UNSUBSCRIBE : MqttUnsubscribeMessage unsubscribe = ( MqttUnsubscribeMessage ) msg ; LOG . info ( "{} UNSUBSCRIBE <{}> to topics <{}>" , direction , clientID , unsubscribe . payload ( ) . topics ( ) ) ; break ; case PUBLISH : MqttPublishMessage publish = ( MqttPublishMessage ) msg ; LOG . info ( "{} PUBLISH <{}> to topics <{}>" , direction , clientID , publish . variableHeader ( ) . topicName ( ) ) ; break ; case PUBREC : case PUBCOMP : case PUBREL : case PUBACK : case UNSUBACK : LOG . info ( "{} {} <{}> packetID <{}>" , direction , messageType , clientID , messageId ( msg ) ) ; break ; case SUBACK : MqttSubAckMessage suback = ( MqttSubAckMessage ) msg ; final List < Integer > grantedQoSLevels = suback . payload ( ) . grantedQoSLevels ( ) ; LOG . info ( "{} SUBACK <{}> packetID <{}>, grantedQoses {}" , direction , clientID , messageId ( msg ) , grantedQoSLevels ) ; break ; } }
public void test() { try { return ( T ) server . getAttribute ( objectName , attributeName ) ; } catch ( MBeanException | AttributeNotFoundException | InstanceNotFoundException | ReflectionException e ) { LOG . warn ( "Could not read attribute {}." , attributeName , e ) ; return errorValue ; } }
public void test() { try { String sessionId = state . transfer ( cmd ) ; events . contextChanged ( ) ; loadSession ( sessionId , ! cmd . getPlayback ( ) . getIsPaused ( ) , true ) ; } catch ( IOException | MercuryClient . MercuryException ex ) { LOGGER . error ( "Failed loading context!" , ex ) ; panicState ( null ) ; } catch ( AbsSpotifyContext . UnsupportedContextException ex ) { LOGGER . error ( "Cannot play context!" , ex ) ; panicState ( null ) ; } }
public void test() { try { String sessionId = state . transfer ( cmd ) ; events . contextChanged ( ) ; loadSession ( sessionId , ! cmd . getPlayback ( ) . getIsPaused ( ) , true ) ; } catch ( IOException | MercuryClient . MercuryException ex ) { LOGGER . error ( "Failed loading context!" , ex ) ; panicState ( null ) ; } catch ( AbsSpotifyContext . UnsupportedContextException ex ) { LOGGER . error ( "Cannot play context!" , ex ) ; panicState ( null ) ; } }
public void test() { if ( ! result . add ( alias ) && log . isWarnEnabled ( ) ) { log . warn ( "Duplicate MassTruncate RestException type: " + alias ) ; } }
public void test() { try { rules = metaStore . getRuleInfo ( ) ; } catch ( MetaStoreException e ) { LOG . error ( "Can not load rules from database:\n" + e . getMessage ( ) ) ; } }
@ Override public void init ( ) throws IOException { LOG . info ( "Initializing ..." ) ; List < RuleInfo > rules = null ; code_block = TryStatement ;  code_block = ForStatement ; LOG . info ( "Initialized. Totally " + rules . size ( ) + " rules loaded from DataBase." ) ; code_block = IfStatement ; }
public void test() { for ( RuleInfo info : rules ) { LOG . debug ( "\t" + info ) ; } }
@ Override protected void decode ( ChannelHandlerContext chc , DatagramPacket msg , List < Object > list ) { ByteBuf bb = msg . content ( ) ; code_block = IfStatement ; int length = bb . getUnsignedShort ( bb . readerIndex ( ) + LENGTH_INDEX_IN_HEADER ) ; code_block = IfStatement ; LOG . debug ( "OF Protocol message received, type:{}" , bb . getByte ( bb . readerIndex ( ) + 1 ) ) ; ByteBuf messageBuffer = bb . slice ( bb . readerIndex ( ) , length ) ; list . add ( messageBuffer ) ; messageBuffer . retain ( ) ; bb . skipBytes ( length ) ; }
public void test() { try { AuthzCache . getInstance ( ) . clear ( ) ; } catch ( ApplicationSettingsException e ) { logger . error ( e . getMessage ( ) , e ) ; throw new AiravataSecurityException ( "Error in obtaining the authorization cache instance." ) ; } }
private void copyTemplateFromSecondaryToPrimary ( VmwareHypervisorHost hyperHost , DatastoreMO datastoreMo , String secondaryStorageUrl , String templatePathAtSecondaryStorage , String templateName , String templateUuid , String nfsVersion ) throws Exception { s_logger . info ( "Executing copyTemplateFromSecondaryToPrimary. secondaryStorage: " + secondaryStorageUrl + ", templatePathAtSecondaryStorage: " + templatePathAtSecondaryStorage + ", templateName: " + templateName ) ; String secondaryMountPoint = _mountService . getMountPoint ( secondaryStorageUrl , nfsVersion ) ; s_logger . info ( "Secondary storage mount point: " + secondaryMountPoint ) ; String srcOVAFileName = secondaryMountPoint + "/" + templatePathAtSecondaryStorage + templateName + "." + ImageFormat . OVA . getFileExtension ( ) ; String srcFileName = getOVFFilePath ( srcOVAFileName ) ; code_block = IfStatement ; srcFileName = getOVFFilePath ( srcOVAFileName ) ; code_block = IfStatement ; String vmName = templateUuid ; hyperHost . importVmFromOVF ( srcFileName , vmName , datastoreMo , "thin" , null ) ; VirtualMachineMO vmMo = hyperHost . findVmOnHyperHost ( vmName ) ; code_block = IfStatement ; code_block = IfStatement ; }
public void test() { if ( e . getMessage ( ) . contains ( "quota_calculated" ) ) { s_logger . warn ( "cloud_usage.cloud_usage table already has a column called quota_calculated" ) ; } else { throw new CloudRuntimeException ( "Unable to create column quota_calculated in table cloud_usage.cloud_usage" , e ) ; } }
public void test() { if ( databaseHistory . skipUnparseableDdlStatements ( ) ) { LOGGER . warn ( "Ignoring unparsable DDL statement '{}': {}" , ddlText , e ) ; streamingMetrics . incrementWarningCount ( ) ; streamingMetrics . incrementUnparsableDdlCount ( ) ; } else { throw e ; } }
public void test() { switch ( event . type ( ) ) { case CREATE_TABLE : changeEvents . add ( createTableEvent ( ( TableCreatedEvent ) event ) ) ; break ; case ALTER_TABLE : changeEvents . add ( alterTableEvent ( ( TableAlteredEvent ) event ) ) ; break ; case DROP_TABLE : changeEvents . add ( dropTableEvent ( tableBefore , ( TableDroppedEvent ) event ) ) ; break ; default : LOGGER . info ( "Skipped DDL event type {}: {}" , event . type ( ) , ddlText ) ; break ; } }
@ Override public void dispose ( ) { logger . debug ( "WeMoMakerHandler disposed." ) ; ScheduledFuture < ? > job = refreshJob ; code_block = IfStatement ; refreshJob = null ; }
public void test() { try { exchange . respond ( coapTransportAdaptor . convertToPublish ( isConRequest ( ) , msg ) ) ; } catch ( AdaptorException e ) { log . trace ( "Failed to reply due to error" , e ) ; exchange . respond ( CoAP . ResponseCode . INTERNAL_SERVER_ERROR ) ; } }
public void loadSettings ( @ NotNull InputStream in , @ NotNull String systemId ) throws ConfigurationException , IOException { VaultSettings settings = new VaultSettings ( ) ; settings . load ( in ) ; setSettings ( settings ) ; log . trace ( "Loaded settings from {}." , systemId ) ; }
public void test() { try { log . debug ( "Link release [" + releaseId + "] to project [" + projectId + "]" ) ; ProjectService . Iface client = thriftClients . makeProjectClient ( ) ; Project project = client . getProjectByIdForEdit ( projectId , user ) ; project . putToReleaseIdToUsage ( releaseId , new ProjectReleaseRelationship ( ReleaseRelationship . CONTAINED , MainlineState . OPEN ) ) ; client . updateProject ( project , user ) ; JSONObject jsonObject = JSONFactoryUtil . createJSONObject ( ) ; jsonObject . put ( "success" , true ) ; jsonObject . put ( "releaseId" , releaseId ) ; jsonObject . put ( "projectId" , projectId ) ; writeJSON ( request , response , jsonObject ) ; } catch ( TException exception ) { log . error ( "Cannot link release [" + releaseId + "] to project [" + projectId + "]." ) ; response . setProperty ( ResourceResponse . HTTP_STATUS_CODE , "500" ) ; } }
public void test() { try { log . debug ( "Link release [" + releaseId + "] to project [" + projectId + "]" ) ; ProjectService . Iface client = thriftClients . makeProjectClient ( ) ; Project project = client . getProjectByIdForEdit ( projectId , user ) ; project . putToReleaseIdToUsage ( releaseId , new ProjectReleaseRelationship ( ReleaseRelationship . CONTAINED , MainlineState . OPEN ) ) ; client . updateProject ( project , user ) ; JSONObject jsonObject = JSONFactoryUtil . createJSONObject ( ) ; jsonObject . put ( "success" , true ) ; jsonObject . put ( "releaseId" , releaseId ) ; jsonObject . put ( "projectId" , projectId ) ; writeJSON ( request , response , jsonObject ) ; } catch ( TException exception ) { log . error ( "Cannot link release [" + releaseId + "] to project [" + projectId + "]." ) ; response . setProperty ( ResourceResponse . HTTP_STATUS_CODE , "500" ) ; } }
public void test() { try { this . sleep ( 2000 ) ; } catch ( InterruptedException e ) { logger . error ( "VM status updator is interrupted." ) ; return ; } }
public void test() { if ( ! oldFile . delete ( ) ) { logger . warn ( "Failed to delete temp file, marked for deletion when VM closes " + oldFile . getAbsolutePath ( ) ) ; oldFile . deleteOnExit ( ) ; } }
public void test() { try { updatePKICerts ( certs , entityId , IDP_META_CERT ) ; } catch ( EngineException e ) { log . error ( "Adding remote SPs certs to local certs store failed, " + "skipping IdP: " + entityId , e ) ; continue ; } }
public void test() { try { transformURLMethod . invoke ( configurationFileInstaller , file ) ; } catch ( InvocationTargetException invocationTargetException ) { _log . error ( "Unable to install " + file , invocationTargetException ) ; } }
public void test() { try { workflowProcessing . publishMessageToNextInMQ ( requestMessage ) ; } catch ( WorkflowTaskInitializationException e ) { log . debug ( " Worlflow Detail  ====  workflow failed to retry and will be picked up in next retry schedule  {} " , lastCompletedTaskExecution ) ; InsightsStatusProvider . getInstance ( ) . createInsightStatusNode ( "In WorkflowRetryExecutor,retryWorkflowWithCompletedTask failed due to exception. Last completed execution: " + lastCompletedTaskExecution , PlatformServiceConstants . FAILURE ) ; } }
public void test() { switch ( level ) { case TRACE : logger . trace ( marker , message , exception ) ; break ; case DEBUG : logger . debug ( marker , message , exception ) ; break ; case INFO : logger . info ( marker , message , exception ) ; break ; case WARN : logger . warn ( marker , message , exception ) ; break ; case ERROR : logger . error ( marker , message , exception ) ; break ; } }
public void test() { switch ( level ) { case TRACE : logger . trace ( marker , message , exception ) ; break ; case DEBUG : logger . debug ( marker , message , exception ) ; break ; case INFO : logger . info ( marker , message , exception ) ; break ; case WARN : logger . warn ( marker , message , exception ) ; break ; case ERROR : logger . error ( marker , message , exception ) ; break ; } }
public void test() { switch ( level ) { case TRACE : logger . trace ( marker , message , exception ) ; break ; case DEBUG : logger . debug ( marker , message , exception ) ; break ; case INFO : logger . info ( marker , message , exception ) ; break ; case WARN : logger . warn ( marker , message , exception ) ; break ; case ERROR : logger . error ( marker , message , exception ) ; break ; } }
public void test() { switch ( level ) { case TRACE : logger . trace ( marker , message , exception ) ; break ; case DEBUG : logger . debug ( marker , message , exception ) ; break ; case INFO : logger . info ( marker , message , exception ) ; break ; case WARN : logger . warn ( marker , message , exception ) ; break ; case ERROR : logger . error ( marker , message , exception ) ; break ; } }
public void test() { switch ( level ) { case TRACE : logger . trace ( marker , message , exception ) ; break ; case DEBUG : logger . debug ( marker , message , exception ) ; break ; case INFO : logger . info ( marker , message , exception ) ; break ; case WARN : logger . warn ( marker , message , exception ) ; break ; case ERROR : logger . error ( marker , message , exception ) ; break ; } }
public void loginEvent ( final User user , final String surrogateIdentifier , final String ip , final String errorMessage ) { LOGGER . info ( "Login statut: {} / user: {} - {} / surrogate: {} / IP: {} / errorMessage: {}" , errorMessage != null ? StatusCode . KO : StatusCode . OK , user . getIdentifier ( ) , user . getEmail ( ) , surrogateIdentifier , ip , errorMessage ) ; }
public void test() { try { roleDescriptor = provider . getRole ( entityID , roleName , supportedProtocol ) ; code_block = IfStatement ; } catch ( MetadataProviderException e ) { log . warn ( "Error retrieving metadata from provider of type {}, proceeding to next provider" , provider . getClass ( ) . getName ( ) , e ) ; continue ; } }
public void test() { try { activeListeners . inc ( ) ; filebeatServer . listen ( ) ; } catch ( InterruptedException e ) { logger . info ( "Filebeat server on port " + port + " shut down" ) ; } catch ( Exception e ) { code_block = IfStatement ; } finally { activeListeners . dec ( ) ; } }
public void test() { try { mimeType = new MimeType ( mimeTypeString ) ; } catch ( MimeTypeParseException e ) { LOGGER . debug ( "Failed to parse mimetype: " + mimeTypeString , e ) ; } }
public void test() { try { result = type . cast ( new BinaryContentImpl ( exchange . getOut ( ) . getBody ( InputStream . class ) , mimeType ) ) ; } catch ( ClassCastException e ) { LOGGER . debug ( "Failed to create BinaryContent" , e ) ; } }
public void test() { if ( profileWithActions != null ) { List < Action > actionList = profileWithActions . getAction ( ) ; code_block = ForStatement ; } else { log . error ( "Could not find profile " + profileRef . getName ( ) + " of user " + getAuthService ( ) . getUsername ( ) ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( SegmentsEntryServiceUtil . class , "getSegmentsEntry" , _getSegmentsEntryParameterTypes8 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , segmentsEntryId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . segments . model . SegmentsEntry ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void fixed ( String what , String reason ) { LOG . warn ( FIXED_REASON , what , reason ) ; }
public void test() { if ( LOGGER . isInfoEnabled ( ) ) { LOGGER . info ( "Starting CC in " + ccRoot . getAbsolutePath ( ) ) ; } }
public void test() { try { MDC . put ( RootContext . MDC_KEY_XID , globalSession . getXid ( ) ) ; handler . handle ( globalSession ) ; } catch ( Throwable th ) { LOGGER . error ( "handle global session failed: {}" , globalSession . getXid ( ) , th ) ; } finally { MDC . remove ( RootContext . MDC_KEY_XID ) ; } }
@ Override public MCCIIN000002UV01 processPatientDiscoveryAsyncReq ( PRPAIN201305UV02 message , AssertionType assertion , NhinTargetCommunitiesType target ) { LOG . debug ( "Begin processPatientDiscoveryAsyncReq" ) ; MCCIIN000002UV01 response = new MCCIIN000002UV01 ( ) ; code_block = TryStatement ;  LOG . debug ( "End processPatientDiscoveryAsyncReq" ) ; return response ; }
@ Override public MCCIIN000002UV01 processPatientDiscoveryAsyncReq ( PRPAIN201305UV02 message , AssertionType assertion , NhinTargetCommunitiesType target ) { LOG . debug ( "Begin processPatientDiscoveryAsyncReq" ) ; MCCIIN000002UV01 response = new MCCIIN000002UV01 ( ) ; code_block = TryStatement ;  LOG . debug ( "End processPatientDiscoveryAsyncReq" ) ; return response ; }
public void test() { if ( result . succeeded ( ) ) { log . debug ( "got connection 1st tme" ) ; testContext . assertEquals ( 1 , pool . connCount ( ) ) ; pool . getConnection ( "hostname" , result2 code_block = LoopStatement ; ) ; testContext . assertFalse ( haveGotConnection . get ( ) , "got a connection on the 2nd try already" ) ; testContext . assertEquals ( 1 , pool . connCount ( ) ) ; log . debug ( "didn't get a connection 2nd time yet" ) ; result . result ( ) . returnToPool ( ) ; vertx . setTimer ( 1000 , v code_block = LoopStatement ; ) ; } else { log . info ( result . cause ( ) ) ; testContext . fail ( result . cause ( ) ) ; } }
public void test() { if ( result2 . succeeded ( ) ) { haveGotConnection . set ( true ) ; log . debug ( "got connection 2nd time" ) ; testContext . assertEquals ( 1 , pool . connCount ( ) ) ; result2 . result ( ) . returnToPool ( ) ; pool . close ( v code_block = LoopStatement ; ) ; } else { log . info ( result2 . cause ( ) ) ; testContext . fail ( result2 . cause ( ) ) ; } }
public void test() { if ( result2 . succeeded ( ) ) { haveGotConnection . set ( true ) ; log . debug ( "got connection 2nd time" ) ; testContext . assertEquals ( 1 , pool . connCount ( ) ) ; result2 . result ( ) . returnToPool ( ) ; pool . close ( v code_block = LoopStatement ; ) ; } else { log . info ( result2 . cause ( ) ) ; testContext . fail ( result2 . cause ( ) ) ; } }
public void test() { if ( result . succeeded ( ) ) { log . debug ( "got connection 1st tme" ) ; testContext . assertEquals ( 1 , pool . connCount ( ) ) ; pool . getConnection ( "hostname" , result2 code_block = LoopStatement ; ) ; testContext . assertFalse ( haveGotConnection . get ( ) , "got a connection on the 2nd try already" ) ; testContext . assertEquals ( 1 , pool . connCount ( ) ) ; log . debug ( "didn't get a connection 2nd time yet" ) ; result . result ( ) . returnToPool ( ) ; vertx . setTimer ( 1000 , v code_block = LoopStatement ; ) ; } else { log . info ( result . cause ( ) ) ; testContext . fail ( result . cause ( ) ) ; } }
public void test() { if ( result . succeeded ( ) ) { log . debug ( "got connection 1st tme" ) ; testContext . assertEquals ( 1 , pool . connCount ( ) ) ; pool . getConnection ( "hostname" , result2 code_block = LoopStatement ; ) ; testContext . assertFalse ( haveGotConnection . get ( ) , "got a connection on the 2nd try already" ) ; testContext . assertEquals ( 1 , pool . connCount ( ) ) ; log . debug ( "didn't get a connection 2nd time yet" ) ; result . result ( ) . returnToPool ( ) ; vertx . setTimer ( 1000 , v code_block = LoopStatement ; ) ; } else { log . info ( result . cause ( ) ) ; testContext . fail ( result . cause ( ) ) ; } }
@ Override public List < String > listZoneNames ( ) throws JargonException { log . info ( "listZoneNames()" ) ; IRODSGenQueryExecutor irodsGenQueryExecutor = new IRODSGenQueryExecutorImpl ( getIRODSSession ( ) , getIRODSAccount ( ) ) ; IRODSGenQueryBuilder builder = new IRODSGenQueryBuilder ( true , null ) ; IRODSQueryResultSet resultSet ; code_block = TryStatement ;  List < String > zones = new ArrayList < String > ( ) ; String zone ; code_block = ForStatement ; return zones ; }
public void test() { if ( StringUtils . isBlank ( identifier ) ) { log . warn ( "Missing key MountFSClassName" ) ; continue ; } }
public void test() { try { code_block = ForStatement ; } catch ( IllegalArgumentException e ) { logger . error ( "Error during prepare composite key, Caused by {}." , e ) ; throw new PersistenceException ( e ) ; } catch ( IllegalAccessException e ) { logger . error ( e . getMessage ( ) ) ; } }
@ GET @ Path ( "/{id}" ) public Response getRepairSchedule ( @ PathParam ( "id" ) UUID repairScheduleId ) { LOG . debug ( "get repair_schedule called with: id = {}" , repairScheduleId ) ; Optional < RepairSchedule > repairSchedule = context . storage . getRepairSchedule ( repairScheduleId ) ; code_block = IfStatement ; }
public void test() { try { IoTDBConfigCheck . getInstance ( ) . checkConfig ( ) ; } catch ( IOException e ) { logger . error ( "meet error when doing start checking" , e ) ; } }
public void test() { if ( reader . hasNext ( ) ) { return reader . next ( ) . getFieldValues ( ) ; } else { LOGGER . warn ( "Unable to find data ID '" + StringUtils . stringFromBinary ( dataId ) + " (hex:" + ByteArrayUtils . getHexString ( dataId ) + ")' with adapter ID " + adapterId + " in data table" ) ; } }
public void test() { try ( final RowReader < GeoWaveRow > reader = getRowReader ( operations , adapterStore , mappingStore , internalAdapterStore , fieldSubsets , aggregation , additionalAuthorizations , adapterId , dataId ) ) { code_block = IfStatement ; } catch ( final Exception e ) { LOGGER . warn ( "Unable to close reader" , e ) ; } }
@ Test public void checkOverlaps ( ) throws IOException { final StringWriter writer = new StringWriter ( ) ; subject . checkOverlaps ( writer ) ; final String output = writer . getBuffer ( ) . toString ( ) ; LOGGER . debug ( "overlaps:\n{}" , output ) ; final List < String > overlaps = Splitter . on ( "\n" ) . splitToList ( output ) ; assertThat ( overlaps , hasSize ( 11 ) ) ; assertThat ( overlaps , containsInAnyOrder ( "GRS1	GRS2	aut-num     	AS1-AS2" , "GRS1	GRS2	inetnum     	10.0.0.0-10.0.0.0" , "GRS1	GRS2	inetnum     	192.0.0.1-192.0.0.2" , "GRS1	GRS2	inet6num    	::/0" , "GRS1	GRS3	aut-num     	AS1-AS1" , "GRS1	GRS3	inetnum     	193.0.0.10-193.0.0.11" , "GRS1	GRS3	inet6num    	::/0" , "GRS2	GRS3	aut-num     	AS10-AS10" , "GRS2	GRS3	aut-num     	AS1-AS1" , "GRS2	GRS3	inet6num    	::/0" , "" ) ) ; }
void debug ( ) { LOG . debug ( "TPnodeEquipments" ) ; nodeEquipments . forEach ( this :: debug ) ; }
public void test() { try { return Integer . valueOf ( strValue . trim ( ) ) ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; return defaultValue ; } }
public void test() { try { Set < ContainerReplica > replicas = containerManager . getContainerReplicas ( ContainerID . valueOf ( containerID ) ) ; LOG . info ( "Container {} has {} replicas on {}" , containerID , replicas . size ( ) , replicas . stream ( ) . map ( ContainerReplica :: getDatanodeDetails ) . map ( DatanodeDetails :: getUuidString ) . sorted ( ) . collect ( toList ( ) ) ) ; return replicas . size ( ) ; } catch ( ContainerNotFoundException e ) { LOG . warn ( "Container {} not found" , containerID ) ; return 0 ; } }
public void test() { if ( branchUuid == null ) { log . warn ( "Parent edge from child {} to parent {} does not have a branch uuid. Skipping this edge." , vertex . getProperty ( "uuid" ) , branchUuid ) ; continue ; } }
public void test() { try { clientPool . invalidateObject ( client ) ; } catch ( Exception e ) { logger . warn ( "exception occurred during releasing thrift client" , e ) ; } }
public void test() { try { LOG . info ( "Starting " + getName ( ) ) ; doRunServer ( ) ; synchronized ( this ) code_block = "" ; } catch ( final Throwable e ) { LOG . error ( getName ( ) + " died with exception." , e ) ; } }
@ Override public void handleDelivery ( String consumerTag , Envelope envelope , AMQP . BasicProperties properties , byte [ ] body ) throws IOException { LOGGER . info ( "AMQP.BasicProperties: {}" , properties ) ; receivedHeaders . putAll ( properties . getHeaders ( ) ) ; received . add ( new String ( body ) ) ; }
public void test() { try { future . get ( DEFAULT_EXTRACTOR_JOB_TIME , TimeUnit . SECONDS ) ; } catch ( Exception e ) { LOG . error ( "Error while waiting for extractor job to be finished" , e ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( e , e ) ; } }
public void test() { if ( query . getResults ( ) != null && ! query . getResults ( ) . isEmpty ( ) ) { code_block = TryStatement ;  } else-if ( tracker . getException ( ) != null ) { LOGGER . error ( tracker . getException ( ) , tracker . getException ( ) ) ; } }
public void test() { try { monitorRequestPending . set ( false ) ; code_block = IfStatement ; final HashMap < String , InterfaceState > newInterfaceStatuses = new HashMap < > ( ) ; logger . debug ( "tracked modems: {}" , modems . keySet ( ) ) ; code_block = ForStatement ; checkStatusChange ( this . interfaceStatuses , newInterfaceStatuses ) ; this . interfaceStatuses = newInterfaceStatuses ; } catch ( Exception ex ) { logger . error ( "Unexpected error during monitoring" , ex ) ; } }
public void test() { if ( h . failed ( ) ) { log . warn ( "Failed to close the pool" , h . cause ( ) ) ; } }
public void test() { if ( f . delete ( ) ) { LOGGER . info ( "[DeleteFileRequest] File " + filePath + " deleted." ) ; } else { LOGGER . error ( "[DeleteFileRequest] Error on deleting file " + filePath ) ; } }
public void test() { if ( f . delete ( ) ) { LOGGER . info ( "[DeleteFileRequest] File " + filePath + " deleted." ) ; } else { LOGGER . error ( "[DeleteFileRequest] Error on deleting file " + filePath ) ; } }
public void test() { if ( log . isDebugEnabled ( ) && ( t != null ) ) { log . warn ( message , o1 , o2 , o3 , o4 , o5 , o6 , o7 , t ) ; } else { log . warn ( message , o1 , o2 , o3 , o4 , o5 , o6 , o7 ) ; } }
public void test() { if ( log . isDebugEnabled ( ) && ( t != null ) ) { log . warn ( message , o1 , o2 , o3 , o4 , o5 , o6 , o7 , t ) ; } else { log . warn ( message , o1 , o2 , o3 , o4 , o5 , o6 , o7 ) ; } }
@ Bean ( name = BEAN_NAME_EMBEDDED_MYSQL , destroyMethod = "stop" ) public MySQLContainer mysql ( ConfigurableEnvironment environment , MySQLProperties properties ) { log . info ( "Starting mysql server. Docker image: {}" , properties . dockerImage ) ; MySQLContainer mysql = new MySQLContainer < > ( properties . dockerImage ) . withEnv ( "MYSQL_ALLOW_EMPTY_PASSWORD" , "yes" ) . withUsername ( properties . getUser ( ) ) . withDatabaseName ( properties . getDatabase ( ) ) . withPassword ( properties . getPassword ( ) ) . withCommand ( "--character-set-server=" + properties . getEncoding ( ) , "--collation-server=" + properties . getCollation ( ) ) . withExposedPorts ( properties . port ) . withCreateContainerCmdModifier ( cmd -> cmd . getHostConfig ( ) . withCapAdd ( Capability . NET_ADMIN ) ) . withInitScript ( properties . initScriptPath ) ; mysql = ( MySQLContainer ) configureCommonsAndStart ( mysql , properties , log ) ; registerMySQLEnvironment ( mysql , environment , properties ) ; return mysql ; }
public void test() { if ( oldScheduler != null ) { boolean stopped = oldScheduler . cancel ( true ) ; logger . debug ( "Stopped polling: {}" , stopped ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Detect meters from {} objects" , telegram . getCosemObjects ( ) . size ( ) ) ; } }
public void test() { try { CRS_BY_SRS_ID . put ( WGS84_SRS_ID , WGS84 ) ; CoordinateReferenceSystem webMercatorCrs = CRS . decode ( SpatialReferenceSystem . WEB_MERCATOR_SRS_ID ) ; CRS_BY_SRS_ID . put ( SpatialReferenceSystem . WEB_MERCATOR_SRS_ID , webMercatorCrs ) ; getOrCreateTransform ( SpatialReferenceSystem . WGS84_SRS_ID , SpatialReferenceSystem . WEB_MERCATOR_SRS_ID ) ; getOrCreateTransform ( SpatialReferenceSystem . WEB_MERCATOR_SRS_ID , SpatialReferenceSystem . WGS84_SRS_ID ) ; } catch ( Exception e ) { LOG . error ( "Error while initializing CoordinateOperations" , e ) ; throw new RuntimeException ( e ) ; } }
public void test() { if ( ! assertSuccessCreationKerberosPrincipal ( "principal/localhost@DOMAIN.COM" ) ) { LOG . warn ( "Could not create a SASL client with valid Kerberos credential" ) ; } }
public void test() { try { server . shutdown ( ) . awaitTermination ( 10L , TimeUnit . SECONDS ) ; } catch ( InterruptedException ex ) { LOG . warn ( "{} couldn't be stopped gracefully" , getClass ( ) . getSimpleName ( ) ) ; } }
public void delete ( StgG20Sys persistentInstance ) { log . debug ( "deleting StgG20Sys instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . delete ( persistentInstance ) ; log . debug ( "delete successful" ) ; } catch ( RuntimeException re ) { log . error ( "delete failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . delete ( persistentInstance ) ; log . debug ( "delete successful" ) ; } catch ( RuntimeException re ) { log . error ( "delete failed" , re ) ; throw re ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "[BEGIN] " + proxy . getClass ( ) + " : " + method . getName ( ) ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "[END] " + proxy . getClass ( ) + " : " + method . getName ( ) ) ; } }
public void test() { if ( logger . isInfoEnabled ( ) ) { StringBuilder builder = new StringBuilder ( ) ; builder . append ( "error. [object]" ) . append ( proxy . getClass ( ) . toString ( ) ) . append ( "[proceed]" ) . append ( proceed . getName ( ) ) ; code_block = IfStatement ; builder . append ( "\n[throwable.getMessage()] " ) . append ( throwable . getMessage ( ) ) ; logger . info ( builder . toString ( ) ) ; } }
public void test() { if ( e . getCause ( ) != null ) { logger . info ( "  <Cause>" , e . getCause ( ) ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( _NO_SUCH_ENTITY_WITH_PRIMARY_KEY + primaryKey ) ; } }
private boolean getBooleanValueFromYaml ( String option ) { String stringValue = yamlStringConfigs . get ( option ) ; boolean result = stringValue != null && Boolean . valueOf ( stringValue ) . equals ( Boolean . TRUE ) ; LOG . debug ( "Found config value {} for key {} " + "from YAML configuration." , result , option ) ; return result ; }
public void test() { if ( ex == null ) { LOG . info ( "Updated ZK to point ledger fragments" + " from old bookies to new bookies: {}" , oldBookie2NewBookie ) ; ensembleUpdatedCb . processResult ( BKException . Code . OK , null , null ) ; } else { LOG . error ( "Error updating ledger config metadata for ledgerId {}" , lh . getId ( ) , ex ) ; ensembleUpdatedCb . processResult ( BKException . getExceptionCode ( ex , BKException . Code . UnexpectedConditionException ) , null , null ) ; } }
public void test() { if ( ex == null ) { LOG . info ( "Updated ZK to point ledger fragments" + " from old bookies to new bookies: {}" , oldBookie2NewBookie ) ; ensembleUpdatedCb . processResult ( BKException . Code . OK , null , null ) ; } else { LOG . error ( "Error updating ledger config metadata for ledgerId {}" , lh . getId ( ) , ex ) ; ensembleUpdatedCb . processResult ( BKException . getExceptionCode ( ex , BKException . Code . UnexpectedConditionException ) , null , null ) ; } }
public void test() { -> { logger . warn ( "Git error on branch notification" ) ; notificationPaneController . addNotification ( String . format ( "Sorry, there was a git error on branch %s." , branch . getRefName ( ) ) ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( KBFolderServiceUtil . class , "fetchFirstChildKBFolder" , _fetchFirstChildKBFolderParameterTypes2 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , kbFolderId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . knowledge . base . model . KBFolder ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { Bootstrap b = new Bootstrap ( ) ; b . group ( group ) . channel ( NioSocketChannel . class ) . handler ( clientInitializer ) ; b . connect ( host , port ) . sync ( ) ; synchronized ( scenarioHandler ) code_block = "" ; } catch ( Exception ex ) { LOG . error ( ex . getMessage ( ) , ex ) ; } finally { LOG . debug ( "shutting down" ) ; code_block = TryStatement ;  } }
public void test() { try { group . shutdownGracefully ( ) . get ( ) ; LOG . debug ( "shutdown succesful" ) ; } catch ( InterruptedException | ExecutionException e ) { LOG . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Characteristic {} from {} has been read - value {}" , characteristicUUID , address , HexUtils . bytesToHex ( data ) ) ; } }
public void test() { try { ThemeDisplay clonedThemeDisplay = ( ThemeDisplay ) themeDisplay . clone ( ) ; clonedThemeDisplay . setScopeGroupId ( _journalArticle . getGroupId ( ) ) ; return Optional . ofNullable ( _assetDisplayPageFriendlyURLProvider . getFriendlyURL ( JournalArticle . class . getName ( ) , _journalArticle . getResourcePrimKey ( ) , locale , clonedThemeDisplay ) ) . map ( url code_block = LoopStatement ; ) . orElse ( StringPool . BLANK ) ; } catch ( CloneNotSupportedException | PortalException exception ) { _log . error ( exception , exception ) ; return StringPool . BLANK ; } }
public void test() { try { cache . clearAllExpired ( ) ; } catch ( Throwable e ) { LOG . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( sessionMd != null ) { processInWebSocketService ( sessionMd . sessionRef , SessionEvent . onError ( tError ) ) ; } else { log . trace ( "[{}] Failed to find session" , session . getId ( ) ) ; } }
public void test() { try { tuningObject = context . getBean ( tuningClass ) ; logger . info ( "find singleton tuning Object from IOC" ) ; } catch ( NoSuchBeanDefinitionException e ) { logger . info ( "can not find singleton  tuning Object from IOC" ) ; } }
public void test() { try { Class < ? > tuningClass = this . getClass ( ) . getClassLoader ( ) . loadClass ( PersistenceConf . TUNING_CLASS . getValue ( ) ) ; ApplicationContext context = DataWorkCloudApplication . getApplicationContext ( ) ; code_block = IfStatement ; code_block = IfStatement ; tuningMethod = tuningClass . getMethod ( PersistenceConf . TUNING_METHOD . getValue ( ) , Object . class ) ; tuningIsOpen = true ; } catch ( ClassNotFoundException | InstantiationException | IllegalAccessException | NoSuchMethodException e ) { logger . warn ( "can not load tuning class,tuning is close" , e ) ; } finally { isInited = true ; } }
@ Test public void h_getObservationTest ( ) { LOGGER . info ( "Get Observation from ExperimentRun test start................................" ) ; g_logObservationsTest ( ) ; GetObservations getObservationRequest = GetObservations . newBuilder ( ) . setId ( experimentRun . getId ( ) ) . setObservationKey ( "Google developer Observation artifact" ) . build ( ) ; GetObservations . Response response = experimentRunServiceStub . getObservations ( getObservationRequest ) ; LOGGER . info ( "GetObservations Response : " + response . getObservationsCount ( ) ) ; code_block = ForStatement ; LOGGER . info ( "Get Observation from ExperimentRun tags test stop................................" ) ; }
@ Test public void h_getObservationTest ( ) { LOGGER . info ( "Get Observation from ExperimentRun test start................................" ) ; g_logObservationsTest ( ) ; GetObservations getObservationRequest = GetObservations . newBuilder ( ) . setId ( experimentRun . getId ( ) ) . setObservationKey ( "Google developer Observation artifact" ) . build ( ) ; GetObservations . Response response = experimentRunServiceStub . getObservations ( getObservationRequest ) ; LOGGER . info ( "GetObservations Response : " + response . getObservationsCount ( ) ) ; code_block = ForStatement ; LOGGER . info ( "Get Observation from ExperimentRun tags test stop................................" ) ; }
public void test() { if ( _log . isDebugEnabled ( ) && ( phase == _PHASE_BEFORE ) ) { _log . debug ( "The method " + aopMethodInvocation + " must have at least one parameter" ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) && ( phase == _PHASE_BEFORE ) ) { _log . debug ( "The first parameter of " + aopMethodInvocation + " must implement ClassedModel" ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) && ( phase == _PHASE_BEFORE ) ) { _log . debug ( "The first parameter of " + aopMethodInvocation + " must be a long" ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { StringBundler sb = new StringBundler ( 4 ) ; sb . append ( "If send is true, the first parameter of " ) ; sb . append ( aopMethodInvocation ) ; sb . append ( " must implement AuditedModel, GroupedModel, or " ) ; sb . append ( "StagedModel" ) ; _log . debug ( sb . toString ( ) ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "Beginning paged ldap search with " + resultsPerPage + " results per page.  Filter: " + inEncodedFilter + " using dc: " + ( ( LdapContextSource ) inLdapTemplate . getContextSource ( ) ) . getUrls ( ) [ 0 ] + " using baseLdapPath: " + ( ( LdapContextSource ) inLdapTemplate . getContextSource ( ) ) . getBaseLdapPathAsString ( ) ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "Paged ldap search complete with " + inHandler . getList ( ) . size ( ) + " results retrieved" ) ; } }
public void test() { try { res = file . getChannel ( ) . tryLock ( ) ; } catch ( OverlappingFileLockException oe ) { file . close ( ) ; return null ; } catch ( IOException e ) { LOGGER . error ( "Cannot create lock on " + lockF , e ) ; file . close ( ) ; throw e ; } }
public void test() { try { Thread . sleep ( 500 ) ; } catch ( Exception e ) { LOG . error ( "Interrupted BSP thread." , e ) ; } }
public void test() { try { parsingBuffer . position ( 0 ) ; scanMessageData ( parsingBuffer ) ; lazyDecodeApplicationProperties ( parsingBuffer ) ; parsingBuffer = null ; } catch ( RuntimeException expected ) { logger . debug ( "The buffer for AMQP Large Mesasge was probably not complete, so an exception eventually would be expected" , expected ) ; } }
public void test() { if ( clone == e && LOG . isDebugEnabled ( ) ) { LOG . debug ( "Failed to clone the AstElement. Returning same reference; the client may fail. {} - {}" , e . getClass ( ) . getName ( ) , e ) ; } }
public void test() { try { Prevention prev = PreventionData . getLocalandRemotePreventions ( loggedInInfo , Integer . parseInt ( demo ) ) ; pf . getMessages ( prev ) ; @ SuppressWarnings ( "unchecked" ) Map < String , Object > m = prev . getWarningMsgs ( ) ; @ SuppressWarnings ( "rawtypes" ) Set set = m . entrySet ( ) ; @ SuppressWarnings ( "rawtypes" ) Iterator i = set . iterator ( ) ; String k = "" ; code_block = IfStatement ; code_block = WhileStatement ; dataCache . put ( demo , ret ) ; } catch ( Exception e ) { ret = "" ; logger . error ( "Error" , e ) ; } }
public void test() { try { connStr = configuration . getConnectionString ( Settings . KEYS . DB_CONNECTION_STRING , Settings . KEYS . DB_FILE_NAME ) ; } catch ( IOException ex ) { LOGGER . debug ( "Unable to get connectionn string" , ex ) ; return false ; } }
private void prepareCcsProtocolType ( ChangeCipherSpecMessage msg ) { msg . setCcsProtocolType ( CCS_PROTOCOL_TYPE ) ; LOGGER . debug ( "CCSProtocollType: " + msg . getCcsProtocolType ( ) . getValue ( ) ) ; }
@ Test public void testEvent ( ) throws EventDeliveryException , MQBrokerException , MQClientException , InterruptedException , UnsupportedEncodingException , RemotingException { DefaultMQProducer producer = new DefaultMQProducer ( producerGroup ) ; producer . setNamesrvAddr ( nameServer ) ; String sendMsg = "\"Hello Flume\"" + "," + DateFormatUtils . format ( new Date ( ) , "yyyy-MM-dd hh:mm:ss" ) ; producer . start ( ) ; Message msg = new Message ( TOPIC_DEFAULT , tag , sendMsg . getBytes ( "UTF-8" ) ) ; SendResult sendResult = producer . send ( msg ) ; Context context = new Context ( ) ; context . put ( NAME_SERVER_CONFIG , nameServer ) ; context . put ( TAG_CONFIG , tag ) ; Channel channel = new MemoryChannel ( ) ; Configurables . configure ( channel , context ) ; List < Channel > channels = new ArrayList < > ( ) ; channels . add ( channel ) ; ChannelSelector channelSelector = new ReplicatingChannelSelector ( ) ; channelSelector . setChannels ( channels ) ; ChannelProcessor channelProcessor = new ChannelProcessor ( channelSelector ) ; RocketMQSource source = new RocketMQSource ( ) ; source . setChannelProcessor ( channelProcessor ) ; Configurables . configure ( source , context ) ; source . start ( ) ; Thread . sleep ( 2000 ) ; sendMsg = "\"Hello Flume\"" + "," + DateFormatUtils . format ( new Date ( ) , "yyyy-MM-dd hh:mm:ss" ) ; msg = new Message ( TOPIC_DEFAULT , tag , sendMsg . getBytes ( "UTF-8" ) ) ; sendResult = producer . send ( msg ) ; log . info ( "publish message : {}, sendResult:{}" , sendMsg , sendResult ) ; PollableSource . Status status = source . process ( ) ; code_block = IfStatement ; Thread . sleep ( 1000 ) ; producer . shutdown ( ) ; source . stop ( ) ; Transaction transaction = channel . getTransaction ( ) ; transaction . begin ( ) ; Event event = channel . take ( ) ; code_block = IfStatement ; byte [ ] body = event . getBody ( ) ; String receiveMsg = new String ( body , "UTF-8" ) ; log . info ( "receive message : {}" , receiveMsg ) ; assertEquals ( sendMsg , receiveMsg ) ; }
@ Test public void testEvent ( ) throws EventDeliveryException , MQBrokerException , MQClientException , InterruptedException , UnsupportedEncodingException , RemotingException { DefaultMQProducer producer = new DefaultMQProducer ( producerGroup ) ; producer . setNamesrvAddr ( nameServer ) ; String sendMsg = "\"Hello Flume\"" + "," + DateFormatUtils . format ( new Date ( ) , "yyyy-MM-dd hh:mm:ss" ) ; producer . start ( ) ; Message msg = new Message ( TOPIC_DEFAULT , tag , sendMsg . getBytes ( "UTF-8" ) ) ; SendResult sendResult = producer . send ( msg ) ; Context context = new Context ( ) ; context . put ( NAME_SERVER_CONFIG , nameServer ) ; context . put ( TAG_CONFIG , tag ) ; Channel channel = new MemoryChannel ( ) ; Configurables . configure ( channel , context ) ; List < Channel > channels = new ArrayList < > ( ) ; channels . add ( channel ) ; ChannelSelector channelSelector = new ReplicatingChannelSelector ( ) ; channelSelector . setChannels ( channels ) ; ChannelProcessor channelProcessor = new ChannelProcessor ( channelSelector ) ; RocketMQSource source = new RocketMQSource ( ) ; source . setChannelProcessor ( channelProcessor ) ; Configurables . configure ( source , context ) ; source . start ( ) ; Thread . sleep ( 2000 ) ; sendMsg = "\"Hello Flume\"" + "," + DateFormatUtils . format ( new Date ( ) , "yyyy-MM-dd hh:mm:ss" ) ; msg = new Message ( TOPIC_DEFAULT , tag , sendMsg . getBytes ( "UTF-8" ) ) ; sendResult = producer . send ( msg ) ; log . info ( "publish message : {}, sendResult:{}" , sendMsg , sendResult ) ; PollableSource . Status status = source . process ( ) ; code_block = IfStatement ; Thread . sleep ( 1000 ) ; producer . shutdown ( ) ; source . stop ( ) ; Transaction transaction = channel . getTransaction ( ) ; transaction . begin ( ) ; Event event = channel . take ( ) ; code_block = IfStatement ; byte [ ] body = event . getBody ( ) ; String receiveMsg = new String ( body , "UTF-8" ) ; log . info ( "receive message : {}" , receiveMsg ) ; assertEquals ( sendMsg , receiveMsg ) ; }
public void test() { try { LoadTestDataSimpleResponseMessageType response = ( LoadTestDataSimpleResponseMessageType ) invokeClientPort ( AdminWSConstants . ADMIN_LTD_DELETEDOCUMENT , request ) ; logDebug ( AdminWSConstants . ADMIN_LTD_DELETEDOCUMENT , response . isStatus ( ) , response . getMessage ( ) ) ; return response . isStatus ( ) ; } catch ( Exception e ) { LOG . error ( "error during delete document: {}" , e . getLocalizedMessage ( ) , e ) ; } }
@ Given ( "^Stop Event Broker$" ) public void stop ( ) { code_block = IfStatement ; logger . info ( "Stopping Event Broker instance ..." ) ; code_block = TryStatement ;  code_block = IfStatement ; logger . info ( "Stopping Event Broker instance ... done!" ) ; }
public void test() { try ( final Suppressed < RuntimeException > s = Suppressed . withRuntimeException ( ) ) { closables . values ( ) . stream ( ) . flatMap ( values -> values . stream ( ) ) . forEach ( s :: closeSuppressed ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Failed to stop Event Broker" , e ) ; } }
@ Given ( "^Stop Event Broker$" ) public void stop ( ) { code_block = IfStatement ; logger . info ( "Stopping Event Broker instance ..." ) ; code_block = TryStatement ;  code_block = IfStatement ; logger . info ( "Stopping Event Broker instance ... done!" ) ; }
@ SuppressWarnings ( "ConstantConditions" ) private void scheduleRenewal ( Reactor reactor ) { int sasTokenRenewalPeriod = this . amqpsSessionHandler . getDeviceClientConfig ( ) . getSasTokenAuthentication ( ) . getMillisecondsBeforeProactiveRenewal ( ) ; log . trace ( "Scheduling proactive sas token renewal for device {} in {} milliseconds" , this . amqpsSessionHandler . getDeviceId ( ) , sasTokenRenewalPeriod ) ; reactor . schedule ( sasTokenRenewalPeriod , this ) ; }
public void test() { try { doDispose ( ) ; } catch ( Exception e ) { logger . error ( "Failed to cleanly dispose: " + e . getMessage ( ) , e ) ; } }
public static QLog initial ( String [ ] args , int type ) { loggerType = type ; QConfig . cfg ( type ) . prepareCLI ( args ) ; final QLog log = LogerHolder . INSTANCE ; About . load ( ) ; QLog . l ( ) . logger . info ( "\"QSystem " + About . ver + "\"!  date: " + About . date ) ; QLog . l ( ) . logger . info ( "START LOGER. Logger: " + QLog . l ( ) . logger ( ) . getName ( ) ) ; code_block = IfStatement ; QLog . l ( ) . logger . info ( "Mode: " + ( QConfig . cfg ( ) . isDebug ( ) ? "KEY_DEBUG" : ( QConfig . cfg ( ) . isDemo ( ) ? "KEY_DEMO" : "FULL" ) ) ) ; QLog . l ( ) . logger . info ( "Plugins: " + ( QConfig . cfg ( ) . isNoPlugins ( ) ? "NO" : "YES" ) ) ; code_block = IfStatement ; return log ; }
public static QLog initial ( String [ ] args , int type ) { loggerType = type ; QConfig . cfg ( type ) . prepareCLI ( args ) ; final QLog log = LogerHolder . INSTANCE ; About . load ( ) ; QLog . l ( ) . logger . info ( "\"QSystem " + About . ver + "\"!  date: " + About . date ) ; QLog . l ( ) . logger . info ( "START LOGER. Logger: " + QLog . l ( ) . logger ( ) . getName ( ) ) ; code_block = IfStatement ; QLog . l ( ) . logger . info ( "Mode: " + ( QConfig . cfg ( ) . isDebug ( ) ? "KEY_DEBUG" : ( QConfig . cfg ( ) . isDemo ( ) ? "KEY_DEMO" : "FULL" ) ) ) ; QLog . l ( ) . logger . info ( "Plugins: " + ( QConfig . cfg ( ) . isNoPlugins ( ) ? "NO" : "YES" ) ) ; code_block = IfStatement ; return log ; }
public static QLog initial ( String [ ] args , int type ) { loggerType = type ; QConfig . cfg ( type ) . prepareCLI ( args ) ; final QLog log = LogerHolder . INSTANCE ; About . load ( ) ; QLog . l ( ) . logger . info ( "\"QSystem " + About . ver + "\"!  date: " + About . date ) ; QLog . l ( ) . logger . info ( "START LOGER. Logger: " + QLog . l ( ) . logger ( ) . getName ( ) ) ; code_block = IfStatement ; QLog . l ( ) . logger . info ( "Mode: " + ( QConfig . cfg ( ) . isDebug ( ) ? "KEY_DEBUG" : ( QConfig . cfg ( ) . isDemo ( ) ? "KEY_DEMO" : "FULL" ) ) ) ; QLog . l ( ) . logger . info ( "Plugins: " + ( QConfig . cfg ( ) . isNoPlugins ( ) ? "NO" : "YES" ) ) ; code_block = IfStatement ; return log ; }
public static QLog initial ( String [ ] args , int type ) { loggerType = type ; QConfig . cfg ( type ) . prepareCLI ( args ) ; final QLog log = LogerHolder . INSTANCE ; About . load ( ) ; QLog . l ( ) . logger . info ( "\"QSystem " + About . ver + "\"!  date: " + About . date ) ; QLog . l ( ) . logger . info ( "START LOGER. Logger: " + QLog . l ( ) . logger ( ) . getName ( ) ) ; code_block = IfStatement ; QLog . l ( ) . logger . info ( "Mode: " + ( QConfig . cfg ( ) . isDebug ( ) ? "KEY_DEBUG" : ( QConfig . cfg ( ) . isDemo ( ) ? "KEY_DEMO" : "FULL" ) ) ) ; QLog . l ( ) . logger . info ( "Plugins: " + ( QConfig . cfg ( ) . isNoPlugins ( ) ? "NO" : "YES" ) ) ; code_block = IfStatement ; return log ; }
public void test() { if ( QConfig . cfg ( ) . isUbtnStart ( ) ) { QLog . l ( ) . logger . info ( "Auto start: YES" ) ; } }
@ Test void withHeaderLoggerShouldBeUsedAsHeader ( ) throws IOException { Path targetFile = dir . homePath ( ) . resolve ( "debug.log" ) ; Path targetFile1 = dir . homePath ( ) . resolve ( "debug.log.1" ) ; ctx = LogConfig . createBuilder ( fs , targetFile , Level . INFO ) . withRotation ( 30 , 2 ) . withHeaderLogger ( log code_block = LoopStatement ; , "org.neo4j.HeaderClassName" ) . build ( ) ; assertThat ( fs . fileExists ( targetFile ) ) . isEqualTo ( true ) ; Logger logger = ctx . getLogger ( "className" ) ; logger . warn ( "Long line that will get next message to be written to next file" ) ; logger . warn ( "test2" ) ; assertThat ( fs . fileExists ( targetFile ) ) . isEqualTo ( true ) ; assertThat ( fs . fileExists ( targetFile1 ) ) . isEqualTo ( true ) ; assertThat ( Files . readString ( targetFile1 ) ) . matches ( DATE_PATTERN + format ( " %-5s \\[className] Long line that will get next message to be written to next file%n" , Level . WARN ) ) ; assertThat ( Files . readString ( targetFile ) ) . matches ( format ( DATE_PATTERN + " %-5s \\[o\\.n\\.HeaderClassName] My Header%n" + DATE_PATTERN + " %-5s \\[o\\.n\\.HeaderClassName] In Two Lines%n" + DATE_PATTERN + " %-5s \\[className] test2%n" , Level . WARN , Level . WARN , Level . WARN ) ) ; }
@ Test void withHeaderLoggerShouldBeUsedAsHeader ( ) throws IOException { Path targetFile = dir . homePath ( ) . resolve ( "debug.log" ) ; Path targetFile1 = dir . homePath ( ) . resolve ( "debug.log.1" ) ; ctx = LogConfig . createBuilder ( fs , targetFile , Level . INFO ) . withRotation ( 30 , 2 ) . withHeaderLogger ( log code_block = LoopStatement ; , "org.neo4j.HeaderClassName" ) . build ( ) ; assertThat ( fs . fileExists ( targetFile ) ) . isEqualTo ( true ) ; Logger logger = ctx . getLogger ( "className" ) ; logger . warn ( "Long line that will get next message to be written to next file" ) ; logger . warn ( "test2" ) ; assertThat ( fs . fileExists ( targetFile ) ) . isEqualTo ( true ) ; assertThat ( fs . fileExists ( targetFile1 ) ) . isEqualTo ( true ) ; assertThat ( Files . readString ( targetFile1 ) ) . matches ( DATE_PATTERN + format ( " %-5s \\[className] Long line that will get next message to be written to next file%n" , Level . WARN ) ) ; assertThat ( Files . readString ( targetFile ) ) . matches ( format ( DATE_PATTERN + " %-5s \\[o\\.n\\.HeaderClassName] My Header%n" + DATE_PATTERN + " %-5s \\[o\\.n\\.HeaderClassName] In Two Lines%n" + DATE_PATTERN + " %-5s \\[className] test2%n" , Level . WARN , Level . WARN , Level . WARN ) ) ; }
public void test() { if ( isDebugEnabled_DLS ) { logger . trace ( LogMarker . DLS_VERBOSE , "getLockTokensForRecovery is skipping {}" , token ) ; } }
@ Override public void addParentProcedure ( String procedure , String parentProcedure ) { CacheValidation . notNullOrEmpty ( PROCEDURE , procedure ) ; CacheValidation . notNullOrEmpty ( PARENT_PROCEDURE , parentProcedure ) ; LOG . trace ( "Adding parentProcedure {} to procedure {}" , parentProcedure , procedure ) ; this . parentProceduresForProcedures . computeIfAbsent ( procedure , createSynchronizedSet ( ) ) . add ( parentProcedure ) ; this . childProceduresForProcedures . computeIfAbsent ( parentProcedure , createSynchronizedSet ( ) ) . add ( procedure ) ; }
public void test() { try { String content = objectMapper . writeValueAsString ( result ) ; output . write ( content ) ; } catch ( JsonProcessingException e ) { LOG . error ( "Failed to process ValidationResult as JSON" , e ) ; } }
public void test() { try { LOG . debug ( "Getting info for task(id={})" , taskAssignmentId ) ; code_block = IfStatement ; BoxTaskAssignment taskAssignment = new BoxTaskAssignment ( boxConnection , taskAssignmentId ) ; return taskAssignment . getInfo ( ) ; } catch ( BoxAPIException e ) { throw new RuntimeException ( String . format ( "Box API returned the error code %d%n%n%s" , e . getResponseCode ( ) , e . getResponse ( ) ) , e ) ; } }
public void test() { try { doSwitch ( current ) ; } catch ( Throwable t ) { String msg = "ClusterConfig changed listener error" ; LOGGER . error ( msg , t ) ; throw new DalRuntimeException ( msg , t ) ; } }
public void test() { { LOGGER . debug ( "Get {}" , id ) ; ParameterChecker . checkParameter ( "Identifier is mandatory : " , id ) ; return customerExternalService . getOne ( id ) ; } }
@ Test public void geodeLoggerDebugNotLoggedAfterRestoringLogLevelToDefault ( ) { when ( config . getLogLevel ( ) ) . thenReturn ( FINE . intLevel ( ) ) ; configuration . configChanged ( ) ; geodeConsoleAppender . clearLogEvents ( ) ; when ( config . getLogLevel ( ) ) . thenReturn ( CONFIG . intLevel ( ) ) ; configuration . configChanged ( ) ; geodeLogger . debug ( logMessage ) ; assertThat ( geodeConsoleAppender . getLogEvents ( ) ) . isEmpty ( ) ; }
@ RestAccessControl ( permission = Permission . MANAGE_USERS ) @ RequestMapping ( value = "/{target:.+}" , method = RequestMethod . DELETE , produces = MediaType . APPLICATION_JSON_VALUE ) public ResponseEntity < SimpleRestResponse < Map > > deleteUser ( @ ModelAttribute ( "user" ) UserDetails user , @ PathVariable String target , BindingResult bindingResult ) throws ApsSystemException { logger . debug ( "deleting {}" , target ) ; code_block = IfStatement ; code_block = IfStatement ; this . getUserService ( ) . removeUser ( target ) ; Map < String , String > result = new HashMap < > ( ) ; result . put ( "code" , target ) ; return new ResponseEntity < > ( new SimpleRestResponse < > ( result ) , HttpStatus . OK ) ; }
public void test() { if ( isTimeOut ( config . getResourceStoreReconnectTimeoutMs ( ) ) ) { logger . error ( "Reconnect to resource store timeout, abandoning..." , ex ) ; return false ; } }
public void test() { if ( config . isResourceStoreReconnectEnabled ( ) && store . isUnreachableException ( ex ) ) { code_block = IfStatement ; long waitMs = getSleepTimeMs ( ) ; long seconds = waitMs / 1000 ; logger . info ( "Will try to re-connect after {} seconds." , seconds ) ; code_block = TryStatement ;  increaseRetryCount ( ) ; return true ; } }
public void test() { if ( infoRequest . hasAdditionalParameters ( ) ) { infoRequest . parseAdditionalParameter ( parameters ) ; } else-if ( parameters . size ( ) < 1 ) { LOGGER . warn ( "You have provided additional parameters that are not necessary for the information parameter {}." , infoRequest . getInfoParameter ( ) ) ; } }
@ Override public void open ( ) throws NuvoException { logger . warn ( "Nuvo binding incorrectly configured. Please configure for Serial or IP over serial connection" ) ; setConnected ( false ) ; }
public void test() { try { code_block = IfStatement ; } catch ( SecurityException se ) { LOG . warn ( "addSdsets tenant={} name [{}] caught SecurityException={}" , getTenant ( ) , sd . getName ( ) + se ) ; } }
public void test() { if ( factory == null || ! isSipEnabled ( ) || ! r . isSipEnabled ( ) ) { log . warn ( "Asterisk is not configured or denied in room {}" , r . getId ( ) ) ; return Optional . empty ( ) ; } }
public void test() { try { doClean ( ) ; } catch ( Exception e ) { Loggers . SRV_LOG . error ( "Clean {} fail. " , getType ( ) , e ) ; } }
public void test() { if ( printDoc ) { LOGGER . info ( prettyString ( doc ) ) ; } }
public void test() { try { run0 ( ) ; } catch ( Exception e ) { LOGGER . error ( "Failed to terminate workers" , e ) ; } }
public void execute ( DelegateExecution execution ) { log . info ( "Entering DelegateExpressionBean.execute()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "Leaving DelegateExpressionBean.execute()" ) ; }
public void execute ( DelegateExecution execution ) { log . info ( "Entering DelegateExpressionBean.execute()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "Leaving DelegateExpressionBean.execute()" ) ; }
public static void printSizeStats ( String type , double density , long count , long totalBytes ) { LOG . info ( StringUtils . format ( " type = %s, density = %06.5f, count = %5d, average byte size = %5d" , type , density , count , totalBytes / count ) ) ; }
public void test() { switch ( p . getAction ( ) ) { case 1 : logger . trace ( "Pump command (ack): {}: " , p ) ; break ; case 4 : logger . trace ( "Turn pump control panel (ack) {}: {} - {}" , p . getSource ( ) , p . getByte ( PentairPacket . STARTOFDATA ) , p ) ; break ; case 5 : logger . trace ( "Set pump mode (ack) {}: {} - {}" , p . getSource ( ) , p . getByte ( PentairPacket . STARTOFDATA ) , p ) ; break ; case 6 : logger . trace ( "Set run mode (ack) {}: {} - {}" , p . getSource ( ) , p . getByte ( PentairPacket . STARTOFDATA ) , p ) ; break ; case 7 : code_block = IfStatement ; logger . debug ( "Pump status: {}" , p ) ; PentairPacketPumpStatus ppsOld = ppscur ; ppscur = new PentairPacketPumpStatus ( p ) ; updateChannel ( INTELLIFLO_RUN , ppsOld ) ; updateChannel ( INTELLIFLO_MODE , ppsOld ) ; updateChannel ( INTELLIFLO_DRIVESTATE , ppsOld ) ; updateChannel ( INTELLIFLO_POWER , ppsOld ) ; updateChannel ( INTELLIFLO_RPM , ppsOld ) ; updateChannel ( INTELLIFLO_PPC , ppsOld ) ; updateChannel ( INTELLIFLO_ERROR , ppsOld ) ; updateChannel ( INTELLIFLO_TIMER , ppsOld ) ; break ; default : logger . debug ( "Unhandled Intelliflo command: {}" , p . toString ( ) ) ; break ; } }
public void test() { switch ( p . getAction ( ) ) { case 1 : logger . trace ( "Pump command (ack): {}: " , p ) ; break ; case 4 : logger . trace ( "Turn pump control panel (ack) {}: {} - {}" , p . getSource ( ) , p . getByte ( PentairPacket . STARTOFDATA ) , p ) ; break ; case 5 : logger . trace ( "Set pump mode (ack) {}: {} - {}" , p . getSource ( ) , p . getByte ( PentairPacket . STARTOFDATA ) , p ) ; break ; case 6 : logger . trace ( "Set run mode (ack) {}: {} - {}" , p . getSource ( ) , p . getByte ( PentairPacket . STARTOFDATA ) , p ) ; break ; case 7 : code_block = IfStatement ; logger . debug ( "Pump status: {}" , p ) ; PentairPacketPumpStatus ppsOld = ppscur ; ppscur = new PentairPacketPumpStatus ( p ) ; updateChannel ( INTELLIFLO_RUN , ppsOld ) ; updateChannel ( INTELLIFLO_MODE , ppsOld ) ; updateChannel ( INTELLIFLO_DRIVESTATE , ppsOld ) ; updateChannel ( INTELLIFLO_POWER , ppsOld ) ; updateChannel ( INTELLIFLO_RPM , ppsOld ) ; updateChannel ( INTELLIFLO_PPC , ppsOld ) ; updateChannel ( INTELLIFLO_ERROR , ppsOld ) ; updateChannel ( INTELLIFLO_TIMER , ppsOld ) ; break ; default : logger . debug ( "Unhandled Intelliflo command: {}" , p . toString ( ) ) ; break ; } }
public void test() { switch ( p . getAction ( ) ) { case 1 : logger . trace ( "Pump command (ack): {}: " , p ) ; break ; case 4 : logger . trace ( "Turn pump control panel (ack) {}: {} - {}" , p . getSource ( ) , p . getByte ( PentairPacket . STARTOFDATA ) , p ) ; break ; case 5 : logger . trace ( "Set pump mode (ack) {}: {} - {}" , p . getSource ( ) , p . getByte ( PentairPacket . STARTOFDATA ) , p ) ; break ; case 6 : logger . trace ( "Set run mode (ack) {}: {} - {}" , p . getSource ( ) , p . getByte ( PentairPacket . STARTOFDATA ) , p ) ; break ; case 7 : code_block = IfStatement ; logger . debug ( "Pump status: {}" , p ) ; PentairPacketPumpStatus ppsOld = ppscur ; ppscur = new PentairPacketPumpStatus ( p ) ; updateChannel ( INTELLIFLO_RUN , ppsOld ) ; updateChannel ( INTELLIFLO_MODE , ppsOld ) ; updateChannel ( INTELLIFLO_DRIVESTATE , ppsOld ) ; updateChannel ( INTELLIFLO_POWER , ppsOld ) ; updateChannel ( INTELLIFLO_RPM , ppsOld ) ; updateChannel ( INTELLIFLO_PPC , ppsOld ) ; updateChannel ( INTELLIFLO_ERROR , ppsOld ) ; updateChannel ( INTELLIFLO_TIMER , ppsOld ) ; break ; default : logger . debug ( "Unhandled Intelliflo command: {}" , p . toString ( ) ) ; break ; } }
public void test() { switch ( p . getAction ( ) ) { case 1 : logger . trace ( "Pump command (ack): {}: " , p ) ; break ; case 4 : logger . trace ( "Turn pump control panel (ack) {}: {} - {}" , p . getSource ( ) , p . getByte ( PentairPacket . STARTOFDATA ) , p ) ; break ; case 5 : logger . trace ( "Set pump mode (ack) {}: {} - {}" , p . getSource ( ) , p . getByte ( PentairPacket . STARTOFDATA ) , p ) ; break ; case 6 : logger . trace ( "Set run mode (ack) {}: {} - {}" , p . getSource ( ) , p . getByte ( PentairPacket . STARTOFDATA ) , p ) ; break ; case 7 : code_block = IfStatement ; logger . debug ( "Pump status: {}" , p ) ; PentairPacketPumpStatus ppsOld = ppscur ; ppscur = new PentairPacketPumpStatus ( p ) ; updateChannel ( INTELLIFLO_RUN , ppsOld ) ; updateChannel ( INTELLIFLO_MODE , ppsOld ) ; updateChannel ( INTELLIFLO_DRIVESTATE , ppsOld ) ; updateChannel ( INTELLIFLO_POWER , ppsOld ) ; updateChannel ( INTELLIFLO_RPM , ppsOld ) ; updateChannel ( INTELLIFLO_PPC , ppsOld ) ; updateChannel ( INTELLIFLO_ERROR , ppsOld ) ; updateChannel ( INTELLIFLO_TIMER , ppsOld ) ; break ; default : logger . debug ( "Unhandled Intelliflo command: {}" , p . toString ( ) ) ; break ; } }
public void test() { if ( p . getLength ( ) != 15 ) { logger . debug ( "Expected length of 15: {}" , p ) ; return ; } }
public void test() { switch ( p . getAction ( ) ) { case 1 : logger . trace ( "Pump command (ack): {}: " , p ) ; break ; case 4 : logger . trace ( "Turn pump control panel (ack) {}: {} - {}" , p . getSource ( ) , p . getByte ( PentairPacket . STARTOFDATA ) , p ) ; break ; case 5 : logger . trace ( "Set pump mode (ack) {}: {} - {}" , p . getSource ( ) , p . getByte ( PentairPacket . STARTOFDATA ) , p ) ; break ; case 6 : logger . trace ( "Set run mode (ack) {}: {} - {}" , p . getSource ( ) , p . getByte ( PentairPacket . STARTOFDATA ) , p ) ; break ; case 7 : code_block = IfStatement ; logger . debug ( "Pump status: {}" , p ) ; PentairPacketPumpStatus ppsOld = ppscur ; ppscur = new PentairPacketPumpStatus ( p ) ; updateChannel ( INTELLIFLO_RUN , ppsOld ) ; updateChannel ( INTELLIFLO_MODE , ppsOld ) ; updateChannel ( INTELLIFLO_DRIVESTATE , ppsOld ) ; updateChannel ( INTELLIFLO_POWER , ppsOld ) ; updateChannel ( INTELLIFLO_RPM , ppsOld ) ; updateChannel ( INTELLIFLO_PPC , ppsOld ) ; updateChannel ( INTELLIFLO_ERROR , ppsOld ) ; updateChannel ( INTELLIFLO_TIMER , ppsOld ) ; break ; default : logger . debug ( "Unhandled Intelliflo command: {}" , p . toString ( ) ) ; break ; } }
public void test() { switch ( p . getAction ( ) ) { case 1 : logger . trace ( "Pump command (ack): {}: " , p ) ; break ; case 4 : logger . trace ( "Turn pump control panel (ack) {}: {} - {}" , p . getSource ( ) , p . getByte ( PentairPacket . STARTOFDATA ) , p ) ; break ; case 5 : logger . trace ( "Set pump mode (ack) {}: {} - {}" , p . getSource ( ) , p . getByte ( PentairPacket . STARTOFDATA ) , p ) ; break ; case 6 : logger . trace ( "Set run mode (ack) {}: {} - {}" , p . getSource ( ) , p . getByte ( PentairPacket . STARTOFDATA ) , p ) ; break ; case 7 : code_block = IfStatement ; logger . debug ( "Pump status: {}" , p ) ; PentairPacketPumpStatus ppsOld = ppscur ; ppscur = new PentairPacketPumpStatus ( p ) ; updateChannel ( INTELLIFLO_RUN , ppsOld ) ; updateChannel ( INTELLIFLO_MODE , ppsOld ) ; updateChannel ( INTELLIFLO_DRIVESTATE , ppsOld ) ; updateChannel ( INTELLIFLO_POWER , ppsOld ) ; updateChannel ( INTELLIFLO_RPM , ppsOld ) ; updateChannel ( INTELLIFLO_PPC , ppsOld ) ; updateChannel ( INTELLIFLO_ERROR , ppsOld ) ; updateChannel ( INTELLIFLO_TIMER , ppsOld ) ; break ; default : logger . debug ( "Unhandled Intelliflo command: {}" , p . toString ( ) ) ; break ; } }
public void setBundleContext ( BundleContext bundleContext ) throws Exception { logger . debug ( "Registering OSGIObjectFactory" ) ; code_block = IfStatement ; objectFactory = new OSGIObjectFactory ( bundleContext ) ; PentahoSystem . registerObjectFactory ( objectFactory ) ; PentahoSystem . setBundleContext ( bundleContext ) ; logger . debug ( "OSGIObjectFactory installed" ) ; }
public void setBundleContext ( BundleContext bundleContext ) throws Exception { logger . debug ( "Registering OSGIObjectFactory" ) ; code_block = IfStatement ; objectFactory = new OSGIObjectFactory ( bundleContext ) ; PentahoSystem . registerObjectFactory ( objectFactory ) ; PentahoSystem . setBundleContext ( bundleContext ) ; logger . debug ( "OSGIObjectFactory installed" ) ; }
public void test() { try { return authenticate ( em , request ) ; } catch ( SecurityException e ) { log . debug ( "Authentication failed" , e ) ; throw new AuthenticationFailedException ( ) ; } finally { em . close ( ) ; } }
public void test() { if ( needToPrintStackTrace ( ) ) { logger . warn ( STACK_TRACE_LOGGER_PREFIX + format , buildNewArgs ( args , cause ) ) ; } else { logger . warn ( format , args ) ; } }
public void test() { if ( needToPrintStackTrace ( ) ) { logger . warn ( STACK_TRACE_LOGGER_PREFIX + format , buildNewArgs ( args , cause ) ) ; } else { logger . warn ( format , args ) ; } }
@ Override public void runJob ( Path input , Path output , BayesParameters params ) throws IOException { Configurable client = new JobClient ( ) ; JobConf conf = new JobConf ( BayesWeightSummerDriver . class ) ; conf . setJobName ( "TfIdf Driver running over input: " + input ) ; conf . setOutputKeyClass ( StringTuple . class ) ; conf . setOutputValueClass ( DoubleWritable . class ) ; FileInputFormat . addInputPath ( conf , new Path ( output , "trainer-termDocCount" ) ) ; FileInputFormat . addInputPath ( conf , new Path ( output , "trainer-wordFreq" ) ) ; FileInputFormat . addInputPath ( conf , new Path ( output , "trainer-featureCount" ) ) ; Path outPath = new Path ( output , "trainer-tfIdf" ) ; FileOutputFormat . setOutputPath ( conf , outPath ) ; conf . setJarByClass ( BayesTfIdfDriver . class ) ; conf . setMapperClass ( BayesTfIdfMapper . class ) ; conf . setInputFormat ( SequenceFileInputFormat . class ) ; conf . setCombinerClass ( BayesTfIdfReducer . class ) ; conf . setReducerClass ( BayesTfIdfReducer . class ) ; conf . setOutputFormat ( BayesTfIdfOutputFormat . class ) ; conf . set ( "io.serializations" , "org.apache.hadoop.io.serializer.JavaSerialization," + "org.apache.hadoop.io.serializer.WritableSerialization" ) ; HadoopUtil . delete ( conf , outPath ) ; Path interimFile = new Path ( output , "trainer-docCount/part-*" ) ; Map < String , Double > labelDocumentCounts = SequenceFileModelReader . readLabelDocumentCounts ( interimFile , conf ) ; DefaultStringifier < Map < String , Double > > mapStringifier = new DefaultStringifier < Map < String , Double > > ( conf , GenericsUtil . getClass ( labelDocumentCounts ) ) ; String labelDocumentCountString = mapStringifier . toString ( labelDocumentCounts ) ; log . info ( "Counts of documents in Each Label" ) ; Map < String , Double > c = mapStringifier . fromString ( labelDocumentCountString ) ; log . info ( "{}" , c ) ; conf . set ( "cnaivebayes.labelDocumentCounts" , labelDocumentCountString ) ; log . info ( params . print ( ) ) ; conf . set ( "bayes.parameters" , params . toString ( ) ) ; client . setConf ( conf ) ; JobClient . runJob ( conf ) ; }
@ Override public void runJob ( Path input , Path output , BayesParameters params ) throws IOException { Configurable client = new JobClient ( ) ; JobConf conf = new JobConf ( BayesWeightSummerDriver . class ) ; conf . setJobName ( "TfIdf Driver running over input: " + input ) ; conf . setOutputKeyClass ( StringTuple . class ) ; conf . setOutputValueClass ( DoubleWritable . class ) ; FileInputFormat . addInputPath ( conf , new Path ( output , "trainer-termDocCount" ) ) ; FileInputFormat . addInputPath ( conf , new Path ( output , "trainer-wordFreq" ) ) ; FileInputFormat . addInputPath ( conf , new Path ( output , "trainer-featureCount" ) ) ; Path outPath = new Path ( output , "trainer-tfIdf" ) ; FileOutputFormat . setOutputPath ( conf , outPath ) ; conf . setJarByClass ( BayesTfIdfDriver . class ) ; conf . setMapperClass ( BayesTfIdfMapper . class ) ; conf . setInputFormat ( SequenceFileInputFormat . class ) ; conf . setCombinerClass ( BayesTfIdfReducer . class ) ; conf . setReducerClass ( BayesTfIdfReducer . class ) ; conf . setOutputFormat ( BayesTfIdfOutputFormat . class ) ; conf . set ( "io.serializations" , "org.apache.hadoop.io.serializer.JavaSerialization," + "org.apache.hadoop.io.serializer.WritableSerialization" ) ; HadoopUtil . delete ( conf , outPath ) ; Path interimFile = new Path ( output , "trainer-docCount/part-*" ) ; Map < String , Double > labelDocumentCounts = SequenceFileModelReader . readLabelDocumentCounts ( interimFile , conf ) ; DefaultStringifier < Map < String , Double > > mapStringifier = new DefaultStringifier < Map < String , Double > > ( conf , GenericsUtil . getClass ( labelDocumentCounts ) ) ; String labelDocumentCountString = mapStringifier . toString ( labelDocumentCounts ) ; log . info ( "Counts of documents in Each Label" ) ; Map < String , Double > c = mapStringifier . fromString ( labelDocumentCountString ) ; log . info ( "{}" , c ) ; conf . set ( "cnaivebayes.labelDocumentCounts" , labelDocumentCountString ) ; log . info ( params . print ( ) ) ; conf . set ( "bayes.parameters" , params . toString ( ) ) ; client . setConf ( conf ) ; JobClient . runJob ( conf ) ; }
@ Override public void runJob ( Path input , Path output , BayesParameters params ) throws IOException { Configurable client = new JobClient ( ) ; JobConf conf = new JobConf ( BayesWeightSummerDriver . class ) ; conf . setJobName ( "TfIdf Driver running over input: " + input ) ; conf . setOutputKeyClass ( StringTuple . class ) ; conf . setOutputValueClass ( DoubleWritable . class ) ; FileInputFormat . addInputPath ( conf , new Path ( output , "trainer-termDocCount" ) ) ; FileInputFormat . addInputPath ( conf , new Path ( output , "trainer-wordFreq" ) ) ; FileInputFormat . addInputPath ( conf , new Path ( output , "trainer-featureCount" ) ) ; Path outPath = new Path ( output , "trainer-tfIdf" ) ; FileOutputFormat . setOutputPath ( conf , outPath ) ; conf . setJarByClass ( BayesTfIdfDriver . class ) ; conf . setMapperClass ( BayesTfIdfMapper . class ) ; conf . setInputFormat ( SequenceFileInputFormat . class ) ; conf . setCombinerClass ( BayesTfIdfReducer . class ) ; conf . setReducerClass ( BayesTfIdfReducer . class ) ; conf . setOutputFormat ( BayesTfIdfOutputFormat . class ) ; conf . set ( "io.serializations" , "org.apache.hadoop.io.serializer.JavaSerialization," + "org.apache.hadoop.io.serializer.WritableSerialization" ) ; HadoopUtil . delete ( conf , outPath ) ; Path interimFile = new Path ( output , "trainer-docCount/part-*" ) ; Map < String , Double > labelDocumentCounts = SequenceFileModelReader . readLabelDocumentCounts ( interimFile , conf ) ; DefaultStringifier < Map < String , Double > > mapStringifier = new DefaultStringifier < Map < String , Double > > ( conf , GenericsUtil . getClass ( labelDocumentCounts ) ) ; String labelDocumentCountString = mapStringifier . toString ( labelDocumentCounts ) ; log . info ( "Counts of documents in Each Label" ) ; Map < String , Double > c = mapStringifier . fromString ( labelDocumentCountString ) ; log . info ( "{}" , c ) ; conf . set ( "cnaivebayes.labelDocumentCounts" , labelDocumentCountString ) ; log . info ( params . print ( ) ) ; conf . set ( "bayes.parameters" , params . toString ( ) ) ; client . setConf ( conf ) ; JobClient . runJob ( conf ) ; }
public void test() { for ( String message : exceptionMessages ) { LOG . warn ( message ) ; } }
public void test() { if ( ! exceptionMessages . isEmpty ( ) ) { code_block = ForStatement ; } else { LOG . info ( "Path " + uri + " was successfully loaded." ) ; } }
public void test() { if ( user != null && ! user . isClosed ( ) ) { log . warn ( "Transport error" , exception ) ; } }
public void test() { try ( final Tx tx = app . tx ( ) ) { final File file = app . get ( File . class , objectId ) ; code_block = IfStatement ; tx . success ( ) ; } catch ( Throwable t ) { logger . warn ( "" , t ) ; } }
public void test() { try { Optional < List < Field > > fields = StatUtil . getFieldsWithManualHistogram ( statContainer , type ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Error occurred while trying to calculate statistics for Index: {}, Type: {}" , index , type ) ; logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( AssetVocabularyServiceUtil . class , "addVocabulary" , _addVocabularyParameterTypes1 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , title , titleMap , descriptionMap , settings , serviceContext ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . asset . kernel . model . AssetVocabulary ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( cellBaseDataResultList . get ( i ) . getResults ( ) != null && cellBaseDataResultList . get ( i ) . getResults ( ) . size ( ) > 0 ) { code_block = IfStatement ; } else { logger . warn ( "Empty result to '{}'" , cellBaseDataResultList . get ( i ) . getId ( ) ) ; } }
public void test() { if ( command instanceof RefreshType ) { logger . debug ( "Refresh for Trigger group not implemented" ) ; } else { code_block = IfStatement ; } }
public void test() { switch ( reorderCode . toLowerCase ( ) ) { case "default" : return Collator . ReorderCodes . DEFAULT ; case "none" : return Collator . ReorderCodes . NONE ; case "others" : return Collator . ReorderCodes . OTHERS ; case "space" : return Collator . ReorderCodes . SPACE ; case "first" : return Collator . ReorderCodes . FIRST ; case "punctuation" : return Collator . ReorderCodes . PUNCTUATION ; case "symbol" : return Collator . ReorderCodes . SYMBOL ; case "currency" : return Collator . ReorderCodes . CURRENCY ; case "digit" : return Collator . ReorderCodes . DIGIT ; default : logger . warn ( "eXist-db does not support the collation reorderCode: {}" , reorderCode ) ; return - 1 ; } }
public void test() { if ( this . spoutMap . get ( id ) == null || override ) { this . spoutMap . put ( spout . getId ( ) , spout ) ; } else { LOG . warn ( "Ignoring attempt to create spout '{}' with override == false." , id ) ; } }
public void test() { try { result . append ( runTransaction ( clientSession -> privateUpdate ( clientSession , family , parameters , variableSetList , queryOptions ) ) ) ; } catch ( CatalogDBException | CatalogParameterException | CatalogAuthorizationException e ) { logger . error ( "Could not update family {}: {}" , family . getId ( ) , e . getMessage ( ) , e ) ; result . getEvents ( ) . add ( new Event ( Event . Type . ERROR , family . getId ( ) , e . getMessage ( ) ) ) ; result . setNumMatches ( result . getNumMatches ( ) + 1 ) ; } }
public Set < State > processEvent ( Event event , String stateMachineInstanceId ) { StateMachine stateMachine = null ; stateMachine = stateMachinesDAO . findById ( stateMachineInstanceId ) ; code_block = IfStatement ; Context context = new RAMContext ( System . currentTimeMillis ( ) , null , stateMachine ) ; final Set < State > dependantStates = context . getDependantStates ( event . getName ( ) ) ; logger . debug ( "These states {} depend on event {}" , dependantStates , event . getName ( ) ) ; Set < State > executableStates = getExecutableStates ( dependantStates , stateMachine . getId ( ) ) ; logger . debug ( "These states {} are now unblocked after event {}" , executableStates , event . getName ( ) ) ; executeStates ( stateMachine , executableStates , event , false ) ; return executableStates ; }
public Set < State > processEvent ( Event event , String stateMachineInstanceId ) { StateMachine stateMachine = null ; stateMachine = stateMachinesDAO . findById ( stateMachineInstanceId ) ; code_block = IfStatement ; Context context = new RAMContext ( System . currentTimeMillis ( ) , null , stateMachine ) ; final Set < State > dependantStates = context . getDependantStates ( event . getName ( ) ) ; logger . debug ( "These states {} depend on event {}" , dependantStates , event . getName ( ) ) ; Set < State > executableStates = getExecutableStates ( dependantStates , stateMachine . getId ( ) ) ; logger . debug ( "These states {} are now unblocked after event {}" , executableStates , event . getName ( ) ) ; executeStates ( stateMachine , executableStates , event , false ) ; return executableStates ; }
public void test() { if ( realmInRawPrincipal ) { identity = rawPrincipal ; logger . debug ( "Realm was specified in principal {}, default realm was not added to the identity being authenticated" , rawPrincipal ) ; } else-if ( StringUtils . isNotBlank ( defaultRealm ) ) { identity = StringUtils . joinWith ( "@" , rawPrincipal , defaultRealm ) ; logger . debug ( "Realm was not specified in principal {}, default realm {} was added to the identity being authenticated" , rawPrincipal , defaultRealm ) ; } else { identity = rawPrincipal ; logger . debug ( "Realm was not specified in principal {}, default realm is blank and was not added to the identity being authenticated" , rawPrincipal ) ; } }
public void test() { if ( realmInRawPrincipal ) { identity = rawPrincipal ; logger . debug ( "Realm was specified in principal {}, default realm was not added to the identity being authenticated" , rawPrincipal ) ; } else-if ( StringUtils . isNotBlank ( defaultRealm ) ) { identity = StringUtils . joinWith ( "@" , rawPrincipal , defaultRealm ) ; logger . debug ( "Realm was not specified in principal {}, default realm {} was added to the identity being authenticated" , rawPrincipal , defaultRealm ) ; } else { identity = rawPrincipal ; logger . debug ( "Realm was not specified in principal {}, default realm is blank and was not added to the identity being authenticated" , rawPrincipal ) ; } }
public void test() { if ( realmInRawPrincipal ) { identity = rawPrincipal ; logger . debug ( "Realm was specified in principal {}, default realm was not added to the identity being authenticated" , rawPrincipal ) ; } else-if ( StringUtils . isNotBlank ( defaultRealm ) ) { identity = StringUtils . joinWith ( "@" , rawPrincipal , defaultRealm ) ; logger . debug ( "Realm was not specified in principal {}, default realm {} was added to the identity being authenticated" , rawPrincipal , defaultRealm ) ; } else { identity = rawPrincipal ; logger . debug ( "Realm was not specified in principal {}, default realm is blank and was not added to the identity being authenticated" , rawPrincipal ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Created authentication token " + token . toString ( ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Ran provider.authenticate(token) and returned authentication to " + "principal={} with name={} and isAuthenticated={}" , authentication . getPrincipal ( ) , authentication . getName ( ) , authentication . isAuthenticated ( ) ) ; } }
@ Override public final void onCachePeriodChanged ( final long period ) { LOG . info ( "onCachePeriodChanged with value {} has been triggered!" , period ) ; cacheSchedulerHelper . scheduleWithPeriod ( period ) ; cacheStrategy . clear ( ) ; }
@ Test public void listAllPreparedQueriesByUser ( ) throws Exception { String user = "diff" , pass = "diff" ; String session1 = sHelper . openSession ( user , pass , lens . getCurrentDB ( ) ) ; QueryPrepareHandle queryPrepareHandle1 = qHelper . submitPreparedQuery ( QueryInventory . QUERY ) ; Assert . assertNotEquals ( queryPrepareHandle1 , null , "Query Execute Failed" ) ; logger . info ( "PREPARE QUERY HANDLE : " + queryPrepareHandle1 ) ; QueryPrepareHandle queryPrepareHandle2 = qHelper . submitPreparedQuery ( QueryInventory . QUERY , null , session1 ) ; Assert . assertNotEquals ( queryPrepareHandle2 , null , "Query Execute Failed" ) ; logger . info ( "PREPARE QUERY HANDLE : " + queryPrepareHandle2 ) ; List < QueryPrepareHandle > list = qHelper . getPreparedQueryHandleList ( null , lens . getUserName ( ) ) ; Assert . assertTrue ( list . contains ( queryPrepareHandle1 ) , "List of All QueryPreparedHandle By user failed" ) ; list = qHelper . getPreparedQueryHandleList ( null , user ) ; Assert . assertTrue ( list . contains ( queryPrepareHandle2 ) , "List of All QueryPreparedHandle By user failed" ) ; list = qHelper . getPreparedQueryHandleList ( null , "all" ) ; Assert . assertTrue ( list . contains ( queryPrepareHandle1 ) , "List of All QueryPreparedHandle by 'all' user failed" ) ; Assert . assertTrue ( list . contains ( queryPrepareHandle2 ) , "List of All QueryPreparedHandle by 'all' user failed" ) ; }
@ Test public void listAllPreparedQueriesByUser ( ) throws Exception { String user = "diff" , pass = "diff" ; String session1 = sHelper . openSession ( user , pass , lens . getCurrentDB ( ) ) ; QueryPrepareHandle queryPrepareHandle1 = qHelper . submitPreparedQuery ( QueryInventory . QUERY ) ; Assert . assertNotEquals ( queryPrepareHandle1 , null , "Query Execute Failed" ) ; logger . info ( "PREPARE QUERY HANDLE : " + queryPrepareHandle1 ) ; QueryPrepareHandle queryPrepareHandle2 = qHelper . submitPreparedQuery ( QueryInventory . QUERY , null , session1 ) ; Assert . assertNotEquals ( queryPrepareHandle2 , null , "Query Execute Failed" ) ; logger . info ( "PREPARE QUERY HANDLE : " + queryPrepareHandle2 ) ; List < QueryPrepareHandle > list = qHelper . getPreparedQueryHandleList ( null , lens . getUserName ( ) ) ; Assert . assertTrue ( list . contains ( queryPrepareHandle1 ) , "List of All QueryPreparedHandle By user failed" ) ; list = qHelper . getPreparedQueryHandleList ( null , user ) ; Assert . assertTrue ( list . contains ( queryPrepareHandle2 ) , "List of All QueryPreparedHandle By user failed" ) ; list = qHelper . getPreparedQueryHandleList ( null , "all" ) ; Assert . assertTrue ( list . contains ( queryPrepareHandle1 ) , "List of All QueryPreparedHandle by 'all' user failed" ) ; Assert . assertTrue ( list . contains ( queryPrepareHandle2 ) , "List of All QueryPreparedHandle by 'all' user failed" ) ; }
public void test() { try { tempDir = FileTools . createDirectoryWithUniqueName ( new File ( this . bundleDir ) , "wokflow_zip_temp" ) ; } catch ( Exception e ) { Log . error ( "Problem creating a temp directory to use for zipping workflow " + e . getMessage ( ) ) ; ret . setExitStatus ( ReturnValue . FAILURE ) ; return ret ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "==> RangerAccessControlEnforcer.isAccessAllowed(" + path + ", " + access + ", " + context . user + ")" ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "<== RangerAccessControlEnforcer.isAccessAllowed(" + path + ", " + access + ", " + context . user + "): " + ret ) ; } }
public void test() { try { DatabaseMetaData metaData = connection . getMetaData ( ) ; String dbProduct = metaData . getDatabaseProductName ( ) ; dialect = identifyDialect ( dbProduct ) ; code_block = IfStatement ; code_block = IfStatement ; } catch ( Exception e ) { logger . debug ( "Unable to read JDBC metadata." , e ) ; } }
public void delete ( FilterCol persistentInstance ) { log . debug ( "deleting FilterCol instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . delete ( persistentInstance ) ; log . debug ( "delete successful" ) ; } catch ( RuntimeException re ) { log . error ( "delete failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . delete ( persistentInstance ) ; log . debug ( "delete successful" ) ; } catch ( RuntimeException re ) { log . error ( "delete failed" , re ) ; throw re ; } }
public List findByExample ( NZobSb instance ) { log . debug ( "finding NZobSb instance by example" ) ; code_block = TryStatement ;  }
public void test() { try { List results = getSession ( ) . createCriteria ( "sernet.gs.reveng.NZobSb" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
public void test() { try { List results = getSession ( ) . createCriteria ( "sernet.gs.reveng.NZobSb" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
public HostnameVerifier verifierFor ( String hostname ) { code_block = IfStatement ; final String tHostname = hostname . endsWith ( "." ) ? hostname . substring ( 0 , hostname . length ( ) - 1 ) : hostname ; log . debug ( "configuring DefaultHostnameVerifier to expect hostname={}" , tHostname ) ; return new HostnameVerifier ( ) code_block = "" ; ; }
public void test() { try { code_block = ForStatement ; } catch ( Exception e ) { logger . error ( "Failed retrieving features: {}" , e . getMessage ( ) , debugException ( e ) ) ; } }
public void test() { try { LOGGER . debug ( "Loading users information from the external UserSystemService" ) ; final List < User > userList = userSystemService . findAllUsers ( ) ; final int userListSize = userList != null ? userList . size ( ) : 0 ; LOGGER . debug ( "Users information was loaded successful: {} users were returned from external system, next synchronization will be in a period of {}" , userListSize , usersSyncInterval ) ; nextUsersSyncTime = calculateNextUsersSyncTime ( ) ; return Pair . of ( true , userList ) ; } catch ( Exception e ) { final String msg = String . format ( "An error was produced during users information loading from the external UserSystem repository." + " Tasks status will still be updated and users synchronization next attempt will be in a period of %s, error: %s" , syncInterval , e . getMessage ( ) ) ; LOGGER . warn ( msg ) ; LOGGER . debug ( msg , e ) ; return Pair . of ( false , Collections . emptyList ( ) ) ; } }
public void test() { try { LOGGER . debug ( "Loading users information from the external UserSystemService" ) ; final List < User > userList = userSystemService . findAllUsers ( ) ; final int userListSize = userList != null ? userList . size ( ) : 0 ; LOGGER . debug ( "Users information was loaded successful: {} users were returned from external system, next synchronization will be in a period of {}" , userListSize , usersSyncInterval ) ; nextUsersSyncTime = calculateNextUsersSyncTime ( ) ; return Pair . of ( true , userList ) ; } catch ( Exception e ) { final String msg = String . format ( "An error was produced during users information loading from the external UserSystem repository." + " Tasks status will still be updated and users synchronization next attempt will be in a period of %s, error: %s" , syncInterval , e . getMessage ( ) ) ; LOGGER . warn ( msg ) ; LOGGER . debug ( msg , e ) ; return Pair . of ( false , Collections . emptyList ( ) ) ; } }
public void test() { try { LOGGER . debug ( "Loading users information from the external UserSystemService" ) ; final List < User > userList = userSystemService . findAllUsers ( ) ; final int userListSize = userList != null ? userList . size ( ) : 0 ; LOGGER . debug ( "Users information was loaded successful: {} users were returned from external system, next synchronization will be in a period of {}" , userListSize , usersSyncInterval ) ; nextUsersSyncTime = calculateNextUsersSyncTime ( ) ; return Pair . of ( true , userList ) ; } catch ( Exception e ) { final String msg = String . format ( "An error was produced during users information loading from the external UserSystem repository." + " Tasks status will still be updated and users synchronization next attempt will be in a period of %s, error: %s" , syncInterval , e . getMessage ( ) ) ; LOGGER . warn ( msg ) ; LOGGER . debug ( msg , e ) ; return Pair . of ( false , Collections . emptyList ( ) ) ; } }
public void test() { try { LOGGER . debug ( "Loading users information from the external UserSystemService" ) ; final List < User > userList = userSystemService . findAllUsers ( ) ; final int userListSize = userList != null ? userList . size ( ) : 0 ; LOGGER . debug ( "Users information was loaded successful: {} users were returned from external system, next synchronization will be in a period of {}" , userListSize , usersSyncInterval ) ; nextUsersSyncTime = calculateNextUsersSyncTime ( ) ; return Pair . of ( true , userList ) ; } catch ( Exception e ) { final String msg = String . format ( "An error was produced during users information loading from the external UserSystem repository." + " Tasks status will still be updated and users synchronization next attempt will be in a period of %s, error: %s" , syncInterval , e . getMessage ( ) ) ; LOGGER . warn ( msg ) ; LOGGER . debug ( msg , e ) ; return Pair . of ( false , Collections . emptyList ( ) ) ; } }
@ Override public void releaseAddress ( String ip ) { log . warn ( "Not implemented: DockerIaas.releaseAddress()" ) ; }
public void test() { try { URIResolver uriResolver = new URIResolver ( ) ; uriResolver . resolve ( "" , wsdlLocation , this . getClass ( ) ) ; wsdlURL = uriResolver . isResolved ( ) ? uriResolver . getURL ( ) : new URL ( wsdlLocation ) ; service = AccessController . doPrivileged ( ( PrivilegedAction < Service > ) ( ) -> Service . create ( wsdlURL , QName . valueOf ( serviceName ) ) ) ; auditRemoteConnection ( wsdlURL ) ; } catch ( Exception e ) { LOGGER . info ( "Unable to create service from WSDL location. Set log level for \"org.codice.ddf.security.claims.attributequery.common\" to DEBUG for more information." ) ; LOGGER . debug ( "Unable to create service from WSDL location." , e ) ; } }
public void test() { try { URIResolver uriResolver = new URIResolver ( ) ; uriResolver . resolve ( "" , wsdlLocation , this . getClass ( ) ) ; wsdlURL = uriResolver . isResolved ( ) ? uriResolver . getURL ( ) : new URL ( wsdlLocation ) ; service = AccessController . doPrivileged ( ( PrivilegedAction < Service > ) ( ) -> Service . create ( wsdlURL , QName . valueOf ( serviceName ) ) ) ; auditRemoteConnection ( wsdlURL ) ; } catch ( Exception e ) { LOGGER . info ( "Unable to create service from WSDL location. Set log level for \"org.codice.ddf.security.claims.attributequery.common\" to DEBUG for more information." ) ; LOGGER . debug ( "Unable to create service from WSDL location." , e ) ; } }
@ ParallelTest void testMirrorMakerLogSetting ( ExtensionContext extensionContext ) { String clusterName = mapWithClusterNames . get ( extensionContext . getDisplayName ( ) ) ; String mirrorMakerName = clusterName + "-mirror-maker" ; resourceManager . createResource ( extensionContext , KafkaMirrorMakerTemplates . kafkaMirrorMaker ( mirrorMakerName , LOG_SETTING_CLUSTER_NAME , GC_LOGGING_SET_NAME , "my-group" , 1 , false ) . editMetadata ( ) . withNamespace ( NAMESPACE ) . endMetadata ( ) . editSpec ( ) . withNewInlineLogging ( ) . withLoggers ( MIRROR_MAKER_LOGGERS ) . endInlineLogging ( ) . withNewJvmOptions ( ) . withGcLoggingEnabled ( true ) . endJvmOptions ( ) . endSpec ( ) . build ( ) ) ; String mmDepName = KafkaMirrorMakerResources . deploymentName ( mirrorMakerName ) ; Map < String , String > mmPods = DeploymentUtils . depSnapshot ( NAMESPACE , mmDepName ) ; String mirrorMakerMap = KafkaMirrorMakerResources . metricsAndLogConfigMapName ( mirrorMakerName ) ; LOGGER . info ( "Checking if MirrorMaker has log level set properly" ) ; assertThat ( "KafkaMirrorMaker's log level is set properly" , checkLoggersLevel ( NAMESPACE , MIRROR_MAKER_LOGGERS , mirrorMakerMap ) , is ( true ) ) ; assertThat ( "Mirror-maker GC logging is enabled" , checkGcLoggingDeployments ( NAMESPACE , mmDepName ) , is ( true ) ) ; KafkaMirrorMakerResource . replaceMirrorMakerResourceInSpecificNamespace ( mirrorMakerName , mm -> mm . getSpec ( ) . setJvmOptions ( JVM_OPTIONS ) , NAMESPACE ) ; DeploymentUtils . waitTillDepHasRolled ( NAMESPACE , mmDepName , 1 , mmPods ) ; assertThat ( "Mirror-maker GC logging is disabled" , checkGcLoggingDeployments ( NAMESPACE , mmDepName ) , is ( false ) ) ; kubectlGetStrimziUntilOperationIsSuccessful ( NAMESPACE , mirrorMakerName ) ; checkContainersHaveProcessOneAsTini ( NAMESPACE , mirrorMakerName ) ; }
protected void logException ( Exception e , String message ) { LOGGER . error ( message , e ) ; getProcessLogger ( ) . error ( message , e ) ; }
public void test() { if ( response . getStatus ( ) != ClientResponse . Status . OK . getStatusCode ( ) ) { _log . error ( "Failed to reload authN providers on endpoint {} response {}" , endpoint , response . toString ( ) ) ; } }
public void test() { try { ClientResponse response = authSvcItr . post ( _URI_RELOAD , null ) ; code_block = IfStatement ; } catch ( Exception e ) { _log . error ( "Caught exception trying to reload an authsvc on {} continuing" , endpoint , e ) ; } }
public void test() { try { AuthSvcInternalApiClientIterator authSvcItr = new AuthSvcInternalApiClientIterator ( _authSvcEndPointLocator , _coordinator ) ; code_block = WhileStatement ; } catch ( CoordinatorException e ) { _log . error ( "Caught coordinator exception trying to find an authsvc endpoint" , e ) ; } }
@ Override public void add ( IRingSet ringSet ) { logger . debug ( "Adding ring set: " , ringSet ) ; super . add ( ringSet ) ; }
public void test() { try { conn = dataSource . getConnection ( ) ; stmt = conn . prepareStatement ( getDiffsAffectedByUserSQL , ResultSet . TYPE_FORWARD_ONLY , ResultSet . CONCUR_READ_ONLY ) ; stmt . setFetchDirection ( ResultSet . FETCH_FORWARD ) ; stmt . setFetchSize ( getFetchSize ( ) ) ; stmt . setLong ( 1 , userID ) ; log . debug ( "Executing SQL query: {}" , getDiffsAffectedByUserSQL ) ; rs = stmt . executeQuery ( ) ; code_block = WhileStatement ; } catch ( SQLException sqle ) { log . warn ( "Exception while removing item diff" , sqle ) ; throw new TasteException ( sqle ) ; } finally { IOUtils . quietClose ( rs , stmt , conn ) ; } }
public void test() { try { size = byteSource . size ( ) ; } catch ( IOException e ) { LOGGER . debug ( "Problem determining size of resource; defaulting to {}." , size , e ) ; } }
@ Override @ Deprecated @ SuppressFBWarnings ( "SLF4J_SIGN_ONLY_FORMAT" ) public void onLinkRemoved ( final LinkRemoved notification ) { LOG . debug ( "-------------------------------------------" ) ; LOG . debug ( "LinkRemoved notification   ........" ) ; LOG . debug ( "-------------------------------------------" ) ; }
public void test() { if ( ClassUtils . isAssignable ( functionClass , TestFunction . class ) ) { logger . debug ( "Found evaluator {} for TestFunction {}" , clazz . getCanonicalName ( ) , functionClass . getCanonicalName ( ) ) ; testFunctionEvaluators . put ( ( Class < ? extends TestFunction > ) functionClass , clazz ) ; } else-if ( ClassUtils . isAssignable ( functionClass , NodeTest . class ) ) { logger . debug ( "Found evaluator {} for NodeTest {}" , clazz . getCanonicalName ( ) , functionClass . getCanonicalName ( ) ) ; testEvaluators . put ( ( Class < ? extends NodeTest > ) functionClass , clazz ) ; } else-if ( ClassUtils . isAssignable ( functionClass , SelectorFunction . class ) ) { logger . debug ( "Found evaluator {} for NodeFunction {}" , clazz . getCanonicalName ( ) , functionClass . getCanonicalName ( ) ) ; functionEvaluators . put ( ( Class < ? extends SelectorFunction > ) functionClass , clazz ) ; } else { logger . debug ( "Found evaluator {} for NodeSelector {}" , clazz . getCanonicalName ( ) , functionClass . getCanonicalName ( ) ) ; defaultEvaluators . put ( ( Class < ? extends NodeSelector > ) functionClass , clazz ) ; } }
public void test() { if ( ClassUtils . isAssignable ( functionClass , TestFunction . class ) ) { logger . debug ( "Found evaluator {} for TestFunction {}" , clazz . getCanonicalName ( ) , functionClass . getCanonicalName ( ) ) ; testFunctionEvaluators . put ( ( Class < ? extends TestFunction > ) functionClass , clazz ) ; } else-if ( ClassUtils . isAssignable ( functionClass , NodeTest . class ) ) { logger . debug ( "Found evaluator {} for NodeTest {}" , clazz . getCanonicalName ( ) , functionClass . getCanonicalName ( ) ) ; testEvaluators . put ( ( Class < ? extends NodeTest > ) functionClass , clazz ) ; } else-if ( ClassUtils . isAssignable ( functionClass , SelectorFunction . class ) ) { logger . debug ( "Found evaluator {} for NodeFunction {}" , clazz . getCanonicalName ( ) , functionClass . getCanonicalName ( ) ) ; functionEvaluators . put ( ( Class < ? extends SelectorFunction > ) functionClass , clazz ) ; } else { logger . debug ( "Found evaluator {} for NodeSelector {}" , clazz . getCanonicalName ( ) , functionClass . getCanonicalName ( ) ) ; defaultEvaluators . put ( ( Class < ? extends NodeSelector > ) functionClass , clazz ) ; } }
public void test() { if ( ClassUtils . isAssignable ( functionClass , TestFunction . class ) ) { logger . debug ( "Found evaluator {} for TestFunction {}" , clazz . getCanonicalName ( ) , functionClass . getCanonicalName ( ) ) ; testFunctionEvaluators . put ( ( Class < ? extends TestFunction > ) functionClass , clazz ) ; } else-if ( ClassUtils . isAssignable ( functionClass , NodeTest . class ) ) { logger . debug ( "Found evaluator {} for NodeTest {}" , clazz . getCanonicalName ( ) , functionClass . getCanonicalName ( ) ) ; testEvaluators . put ( ( Class < ? extends NodeTest > ) functionClass , clazz ) ; } else-if ( ClassUtils . isAssignable ( functionClass , SelectorFunction . class ) ) { logger . debug ( "Found evaluator {} for NodeFunction {}" , clazz . getCanonicalName ( ) , functionClass . getCanonicalName ( ) ) ; functionEvaluators . put ( ( Class < ? extends SelectorFunction > ) functionClass , clazz ) ; } else { logger . debug ( "Found evaluator {} for NodeSelector {}" , clazz . getCanonicalName ( ) , functionClass . getCanonicalName ( ) ) ; defaultEvaluators . put ( ( Class < ? extends NodeSelector > ) functionClass , clazz ) ; } }
public void test() { if ( ClassUtils . isAssignable ( functionClass , TestFunction . class ) ) { logger . debug ( "Found evaluator {} for TestFunction {}" , clazz . getCanonicalName ( ) , functionClass . getCanonicalName ( ) ) ; testFunctionEvaluators . put ( ( Class < ? extends TestFunction > ) functionClass , clazz ) ; } else-if ( ClassUtils . isAssignable ( functionClass , NodeTest . class ) ) { logger . debug ( "Found evaluator {} for NodeTest {}" , clazz . getCanonicalName ( ) , functionClass . getCanonicalName ( ) ) ; testEvaluators . put ( ( Class < ? extends NodeTest > ) functionClass , clazz ) ; } else-if ( ClassUtils . isAssignable ( functionClass , SelectorFunction . class ) ) { logger . debug ( "Found evaluator {} for NodeFunction {}" , clazz . getCanonicalName ( ) , functionClass . getCanonicalName ( ) ) ; functionEvaluators . put ( ( Class < ? extends SelectorFunction > ) functionClass , clazz ) ; } else { logger . debug ( "Found evaluator {} for NodeSelector {}" , clazz . getCanonicalName ( ) , functionClass . getCanonicalName ( ) ) ; defaultEvaluators . put ( ( Class < ? extends NodeSelector > ) functionClass , clazz ) ; } }
public void test() { if ( multiSearchResponse . isError ) { logger . error ( "failed to execute multiSearch: {}" , multiSearchResponse ) ; return false ; } }
@ Test public void testContextualQueryNullMetadata ( ) throws Exception { String methodName = "testContextualQueryNullMetadata" ; LOGGER . debug ( "***************  START: {}  *****************" , methodName ) ; String searchPhrase = "serengeti event" ; MockQuery query = new MockQuery ( ) ; query . addContextualFilter ( searchPhrase , null ) ; SubscriptionFilterVisitor visitor = new SubscriptionFilterVisitor ( ) ; Predicate predicate = ( Predicate ) query . getFilter ( ) . accept ( visitor , null ) ; MetacardImpl metacard = new MetacardImpl ( ) ; metacard . setId ( "ABC123" ) ; metacard . setMetadata ( TestDataLibrary . getCatAndDogEntry ( ) ) ; HashMap < String , Object > properties = new HashMap < > ( ) ; properties . put ( PubSubConstants . HEADER_ID_KEY , metacard . getId ( ) ) ; properties . put ( PubSubConstants . HEADER_ENTRY_KEY , metacard ) ; properties . put ( PubSubConstants . HEADER_OPERATION_KEY , PubSubConstants . CREATE ) ; Event testEvent = new Event ( "topic" , properties ) ; assertFalse ( predicate . matches ( testEvent ) ) ; LOGGER . debug ( "***************  END: {}  *****************" , methodName ) ; }
@ Test public void testContextualQueryNullMetadata ( ) throws Exception { String methodName = "testContextualQueryNullMetadata" ; LOGGER . debug ( "***************  START: {}  *****************" , methodName ) ; String searchPhrase = "serengeti event" ; MockQuery query = new MockQuery ( ) ; query . addContextualFilter ( searchPhrase , null ) ; SubscriptionFilterVisitor visitor = new SubscriptionFilterVisitor ( ) ; Predicate predicate = ( Predicate ) query . getFilter ( ) . accept ( visitor , null ) ; MetacardImpl metacard = new MetacardImpl ( ) ; metacard . setId ( "ABC123" ) ; metacard . setMetadata ( TestDataLibrary . getCatAndDogEntry ( ) ) ; HashMap < String , Object > properties = new HashMap < > ( ) ; properties . put ( PubSubConstants . HEADER_ID_KEY , metacard . getId ( ) ) ; properties . put ( PubSubConstants . HEADER_ENTRY_KEY , metacard ) ; properties . put ( PubSubConstants . HEADER_OPERATION_KEY , PubSubConstants . CREATE ) ; Event testEvent = new Event ( "topic" , properties ) ; assertFalse ( predicate . matches ( testEvent ) ) ; LOGGER . debug ( "***************  END: {}  *****************" , methodName ) ; }
public void test() { try { return Integer . parseInt ( value ) ; } catch ( Exception e ) { LOG . warn ( "could not get table column view index for [{}]. Loaded value '{}'" , col . getClass ( ) . getName ( ) , value , e ) ; } }
public void test() { -> { log . debug ( "to be introspected: {}" , type ) ; } }
public void test() { try { LOG . debug ( "Merging XML based CamelContext with Spring Boot configuration properties" ) ; CamelAutoConfiguration . doConfigureCamelContext ( applicationContext , camelContext , config ) ; } catch ( Exception e ) { throw RuntimeCamelException . wrapRuntimeCamelException ( e ) ; } }
public void test() { if ( ep == null ) { String errMsg = "No remote endpoint to send command, check if host or ssvm is down?" ; s_logger . error ( errMsg ) ; return null ; } }
public void test() { if ( tip != null ) { tip . reportProgress ( taskStatus ) ; return true ; } else { LOG . warn ( "Progress from unknown child task: " + taskId ) ; return false ; } }
private static String convertPKCS1ToPKCS8 ( String pkcs1 ) throws IOException { String uuid = UUID . randomUUID ( ) . toString ( ) ; File pkcs1File = new File ( "keys/" + uuid + "/pkcs1.key" ) ; File pkcs8File = new File ( "keys/" + uuid + "/pkcs8.key" ) ; String transformCmd = "openssl rsa  -RSAPublicKey_in -in " + pkcs1File . toPath ( ) + " -out " + pkcs8File . toPath ( ) ; logger . info ( "exec script : {}" , transformCmd ) ; FileUtils . writeStringToFile ( pkcs1File , pkcs1 , "UTF-8" ) ; Process p = Runtime . getRuntime ( ) . exec ( transformCmd ) ; BufferedReader stdError = new BufferedReader ( new InputStreamReader ( p . getErrorStream ( ) ) ) ; String s ; code_block = WhileStatement ; return FileUtils . readFileToString ( pkcs8File , "UTF-8" ) ; }
public void test() { if ( logger . isInfoEnabled ( ) ) { StringBuilder sqlCmds = new StringBuilder ( 128 ) ; Iterator < String > iter = tcConn . getDDLConverter ( ) . getDDL ( spec ) . iterator ( ) ; code_block = WhileStatement ; logger . info ( "Creating new table '{}' with command(s): {}" , spec . getName ( ) , sqlCmds . toString ( ) ) ; } }
public void test() { try ( Connection con = getDatasource ( ) . getConnection ( ) ; ) { String query1 = "UPDATE vcenter SET url = ?, userid = ?, password = ? WHERE tkey = ?" ; code_block = TryStatement ;  } catch ( SQLException e ) { logger . error ( "Failed to save controller configuration" , e ) ; throw new Exception ( Messages . get ( locale , "error_db_save_conf" ) ) ; } }
@ Override public void onNext ( DiscoveryResponse response ) { logger . debug ( "Received API discovery response " + response ) ; XdsSchedulerManager . getInstance ( ) . stopAPIDiscoveryScheduling ( ) ; latestReceived = response ; code_block = TryStatement ;  }
public void test() { if ( reply . isSuccess ( ) ) { logger . debug ( String . format ( "host[uuid:%s] load successfully" , cmsg . getHostUuid ( ) ) ) ; } else-if ( reply . isCanceled ( ) ) { logger . warn ( String . format ( "canceled connect kvm host[uuid:%s], because it connecting now" , cmsg . getHostUuid ( ) ) ) ; } else { logger . warn ( String . format ( "failed to load host[uuid:%s], %s" , cmsg . getHostUuid ( ) , reply . getError ( ) ) ) ; } }
public void test() { if ( reply . isSuccess ( ) ) { logger . debug ( String . format ( "host[uuid:%s] load successfully" , cmsg . getHostUuid ( ) ) ) ; } else-if ( reply . isCanceled ( ) ) { logger . warn ( String . format ( "canceled connect kvm host[uuid:%s], because it connecting now" , cmsg . getHostUuid ( ) ) ) ; } else { logger . warn ( String . format ( "failed to load host[uuid:%s], %s" , cmsg . getHostUuid ( ) , reply . getError ( ) ) ) ; } }
public void test() { if ( reply . isSuccess ( ) ) { logger . debug ( String . format ( "host[uuid:%s] load successfully" , cmsg . getHostUuid ( ) ) ) ; } else-if ( reply . isCanceled ( ) ) { logger . warn ( String . format ( "canceled connect kvm host[uuid:%s], because it connecting now" , cmsg . getHostUuid ( ) ) ) ; } else { logger . warn ( String . format ( "failed to load host[uuid:%s], %s" , cmsg . getHostUuid ( ) , reply . getError ( ) ) ) ; } }
@ Override public List < String > getAll ( BatchRunContext batchRunContext ) { logger . info ( getClass ( ) . toString ( ) + " job starting ..." ) ; SystemAccount actor = getSystemAccount ( ) ; List < String > allShares = service . findAllExpiredEntries ( actor , actor ) ; logger . info ( allShares . size ( ) + " anonymous share(s) have been found to be deleted" ) ; return allShares ; }
public void sleepUntilNextRetry ( ) throws InterruptedException { int attempts = getAttemptTimes ( ) ; long sleepTime = getBackoffTime ( ) ; LOG . trace ( "Sleeping {} ms before retry {}..." , sleepTime , attempts ) ; retryConfig . getTimeUnit ( ) . sleep ( sleepTime ) ; useRetry ( ) ; }
public void test() { if ( log . isTraceEnabled ( ) ) { log . debug ( "WebSocket data: {}" , frame ) ; } }
public void test() { try { sendException ( new WebSocketException ( WSConstants . NO_REQUEST_ID , "Internal Server Error" ) ) ; } catch ( Exception e2 ) { log . warn ( "Could not inform client of earlier Internal Server Error due to additional exception " + e2 , e2 ) ; } }
public void test() { try { volumeCountList . add ( nodeStateManager . getNode ( dn ) . getHealthyVolumeCount ( ) ) ; } catch ( NodeNotFoundException e ) { LOG . warn ( "Cannot generate NodeStat, datanode {} not found." , dn . getUuid ( ) ) ; } }
public void test() { try { MimeMessage mimeMessage = mailSender . createMimeMessage ( ) ; MimeMessageHelper helper = new MimeMessageHelper ( mimeMessage , true ) ; helper . setSubject ( subject ) ; helper . setText ( text , true ) ; helper . setTo ( receiver ) ; helper . setFrom ( SENDER ) ; mailSender . send ( mimeMessage ) ; log . info ( "é®ä»¶åéæå" ) ; } catch ( Exception e ) { log . error ( e . getMessage ( ) ) ; } }
public void test() { try { MimeMessage mimeMessage = mailSender . createMimeMessage ( ) ; MimeMessageHelper helper = new MimeMessageHelper ( mimeMessage , true ) ; helper . setSubject ( subject ) ; helper . setText ( text , true ) ; helper . setTo ( receiver ) ; helper . setFrom ( SENDER ) ; mailSender . send ( mimeMessage ) ; log . info ( "é®ä»¶åéæå" ) ; } catch ( Exception e ) { log . error ( e . getMessage ( ) ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Deleting node {" + getUuid ( ) + "}" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Deleting node {" + getUuid ( ) + "} vertex." ) ; } }
@ Override public synchronized void stopScan ( ) { code_block = IfStatement ; logger . info ( "Stopping EnOcean discovery scan" ) ; bridgeHandler . stopDiscovery ( ) ; super . stopScan ( ) ; }
public void test() { try { final Object value1 = BeanHelper . getNestedProperty ( o1 , prop ) ; final Object value2 = BeanHelper . getNestedProperty ( o2 , prop ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; } catch ( final Exception ex ) { log . error ( "Exception while comparing values of property '" + prop + "': " + ex . getMessage ( ) ) ; return 0 ; } }
public void test() { try { Objects . requireNonNull ( session ) . getRemote ( ) . sendPing ( ByteBuffer . wrap ( PING_PAYLOAD ) ) ; } catch ( IOException e ) { LOG . warn ( "Cannot send ping message over web-socket session {}." , session , e ) ; } }
public void test() { try { fw = new FileWriter ( file ) ; IOUtils . write ( result . getResults ( ) , fw ) ; } catch ( IOException e ) { logger . error ( "Error" , e ) ; } finally { IOUtils . closeQuietly ( fw ) ; } }
public void test() { try { getSession ( ) . getRemote ( ) . sendString ( resultMessage ) ; } catch ( IOException e ) { logger . error ( "Failed to send a message back to remote." , e ) ; } }
public void test() { if ( ! recycles . isEmpty ( ) ) { logger . debug ( String . format ( "transfer %s trash to recycles from %s: %s" , recycles . size ( ) , type , recycles ) ) ; } }
public void test() { try { log . debug ( "Calling ExampleActionSetService" ) ; return ( IActionSet ) getServiceFromRegistry ( context , createFilterExampleActionSet ( name , version ) ) ; } catch ( InvalidSyntaxException e ) { throw new ActivatorException ( e ) ; } }
public void test() { if ( assertion != null && assertion . getUserInfo ( ) != null && assertion . getUserInfo ( ) . getRoleCoded ( ) != null ) { value = assertion . getUserInfo ( ) . getRoleCoded ( ) . getCodeSystemName ( ) ; } else { LOG . warn ( "Unable to find user role in SAML, will not be included in message to policy engine" ) ; } }
public void test() { if ( ActiveMQRALogger . LOGGER . isTraceEnabled ( ) ) { ActiveMQRALogger . LOGGER . trace ( "getClientID()" ) ; } }
private void verifyServiceInstances ( final Service service ) throws Exception { final Instances instances = graphql . instances ( new InstancesQuery ( ) . serviceId ( service . getKey ( ) ) . start ( startTime ) . end ( now ( ) ) ) ; LOGGER . info ( "instances: {}" , instances ) ; load ( "expected/profile/instances.yml" ) . as ( InstancesMatcher . class ) . verify ( instances ) ; }
public void test() { try { code_block = IfStatement ; } catch ( IOException e ) { log . warn ( "exception closing inputstream" , e ) ; } }
@ Test public void shouldDecodeNRowResponseSmallyChunked ( ) throws Exception { String response = Resources . read ( "chunked.json" , this . getClass ( ) ) ; String [ ] chunks = new String [ ] code_block = "" ; ; StringBuilder sb = new StringBuilder ( "Chunks:" ) ; code_block = ForStatement ; LOGGER . info ( sb . toString ( ) ) ; shouldDecodeChunked ( true , chunks ) ; }
@ Override public void init ( ServletConfig servletConfig ) throws ServletException { super . init ( servletConfig ) ; log . info ( "Initializing PageServlet" ) ; this . excludeOwnerPages = WebloggerConfig . getBooleanProperty ( "cache.excludeOwnerEditPages" ) ; this . weblogPageCache = WeblogPageCache . getInstance ( ) ; this . siteWideCache = SiteWideCache . getInstance ( ) ; this . processReferrers = WebloggerConfig . getBooleanProperty ( "site.bannedwordslist.enable.referrers" ) ; log . info ( "Referrer spam check enabled = " + this . processReferrers ) ; String robotPatternStr = WebloggerConfig . getProperty ( "referrer.robotCheck.userAgentPattern" ) ; code_block = IfStatement ; themeReload = WebloggerConfig . getBooleanProperty ( "themes.reload.mode" ) ; }
@ Override public void init ( ServletConfig servletConfig ) throws ServletException { super . init ( servletConfig ) ; log . info ( "Initializing PageServlet" ) ; this . excludeOwnerPages = WebloggerConfig . getBooleanProperty ( "cache.excludeOwnerEditPages" ) ; this . weblogPageCache = WeblogPageCache . getInstance ( ) ; this . siteWideCache = SiteWideCache . getInstance ( ) ; this . processReferrers = WebloggerConfig . getBooleanProperty ( "site.bannedwordslist.enable.referrers" ) ; log . info ( "Referrer spam check enabled = " + this . processReferrers ) ; String robotPatternStr = WebloggerConfig . getProperty ( "referrer.robotCheck.userAgentPattern" ) ; code_block = IfStatement ; themeReload = WebloggerConfig . getBooleanProperty ( "themes.reload.mode" ) ; }
public void test() { try { robotPattern = Pattern . compile ( robotPatternStr ) ; } catch ( Exception e ) { log . error ( "Error parsing referrer.robotCheck.userAgentPattern value '" + robotPatternStr + "'.  Robots will not be filtered. " , e ) ; } }
public void test() { try { ContainerQuota containerQuota = containerHost . getQuota ( ) ; code_block = IfStatement ; } catch ( Exception e ) { LOG . error ( "Error getting container quota: {}" , e . getMessage ( ) ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( sb . toString ( ) ) ; } }
public void test() { try { code_block = ForStatement ; } catch ( Exception ex ) { log . error ( "Error in thread " + Thread . currentThread ( ) . getId ( ) , ex ) ; return null ; } }
public void test() { try { code_block = IfStatement ; } catch ( IllegalArgumentException e ) { LOGGER . error ( String . format ( "Unable to compare Kubernetes version for cluster version : %s with %s" , version . getName ( ) , KubernetesClusterService . MIN_KUBERNETES_VERSION_HA_SUPPORT ) , e ) ; } }
private void stop ( boolean swallowException ) { log . trace ( "Stopping the Connect group member." ) ; AtomicReference < Throwable > firstException = new AtomicReference < > ( ) ; this . stopped = true ; Utils . closeQuietly ( coordinator , "coordinator" , firstException ) ; Utils . closeQuietly ( metrics , "consumer metrics" , firstException ) ; Utils . closeQuietly ( client , "consumer network client" , firstException ) ; AppInfoParser . unregisterAppInfo ( JMX_PREFIX , clientId , metrics ) ; if ( firstException . get ( ) != null && ! swallowException ) throw new KafkaException ( "Failed to stop the Connect group member" , firstException . get ( ) ) ; else log . debug ( "The Connect group member has stopped." ) ; }
public void test() { try { Transformer trans = TransformerFactory . newInstance ( ) . newTransformer ( ) ; trans . setOutputProperty ( OutputKeys . INDENT , "yes" ) ; trans . setOutputProperty ( OutputKeys . OMIT_XML_DECLARATION , "yes" ) ; trans . setOutputProperty ( OutputKeys . METHOD , "html" ) ; trans . transform ( src , res ) ; tags = sw . toString ( ) . replaceAll ( ROOT_ELEMENT_REGEX , "" ) ; } catch ( Exception e ) { StringBuilder txt = new StringBuilder ( ) ; txt . append ( "Error converting tags to string. Exception: " ) ; txt . append ( e . toString ( ) ) ; LOG . warn ( txt . toString ( ) ) ; } }
public void test() { if ( isTrace ) { StringBuilder sb = new StringBuilder ( ) ; sb . append ( "returning tags: " ) ; sb . append ( ( tags . length ( ) > 0 ) ? "\n" + tags : "" ) ; LOG . trace ( sb . toString ( ) ) ; } }
public void test() { try { lockListener . lostLock ( ) ; } catch ( Exception e ) { LOGGER . warn ( "On notify lost lock" , e ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Sending additional invalidation for requestors if necessary." ) ; } }
public void test() { { checkRights ( getRights ( ) . canAccessPasswordReset ( ) ) ; getUserService ( ) . requestPasswordReset ( resetPassword . getUser ( ) ) ; LOG . info ( "Successfully processed password reset request for user {}" , resetPassword . getUser ( ) ) ; return Response . noContent ( ) . build ( ) ; } }
@ PayloadRoot ( localPart = "SwitchConfigurationAsyncRequest" , namespace = NAMESPACE ) @ ResponsePayload public SwitchConfigurationResponse getSwitchConfigurationResponse ( @ OrganisationIdentification final String organisationIdentification , @ RequestPayload final SwitchConfigurationAsyncRequest request ) throws OsgpException { LOGGER . info ( "Switch Configuration Async Request received from organisation: {} for device: {}." , organisationIdentification , request . getAsyncRequest ( ) . getDeviceId ( ) ) ; final SwitchConfigurationResponse response = new SwitchConfigurationResponse ( ) ; code_block = TryStatement ;  return response ; }
public void test() { if ( message != null ) { response . setResult ( OsgpResultType . fromValue ( message . getResult ( ) . getValue ( ) ) ) ; } else { LOGGER . debug ( "Get Configuration data is null" ) ; } }
public void test() { try { executorCursor ( cursor ) ; } catch ( Exception e ) { logger . error ( "replicaSet:{} shutdown....." , replicaSetConfig , e ) ; } finally { code_block = IfStatement ; replicaSet . shutdown ( ) ; } }
public void test() { try { CProductVersionConfiguration cProductVersionConfiguration = ConfigurationProviderUtil . getConfiguration ( CProductVersionConfiguration . class , new SystemSettingsLocator ( CProductVersionConfiguration . class . getName ( ) ) ) ; code_block = IfStatement ; } catch ( PortalException portalException ) { _log . error ( portalException , portalException ) ; } }
public void test() { if ( log . isDebugEnable ( ) ) { log . debug ( this , "Send SMS END: phone=" + phoneNum + ",code=" + code + ",desc=" + desc ) ; } }
public void test() { try { ObjStat objStat = irodsFileSystemAO . getObjStat ( getAbsolutePath ( ) ) ; lastMod = objStat . getModifiedAt ( ) . getTime ( ) ; } catch ( FileNotFoundException e ) { log . warn ( "file not found exception, return 0L" , e ) ; } catch ( JargonException e ) { log . error ( "jargon exception, rethrow as unchecked" , e ) ; throw new JargonRuntimeException ( e ) ; } }
public void test() { try { ObjStat objStat = irodsFileSystemAO . getObjStat ( getAbsolutePath ( ) ) ; lastMod = objStat . getModifiedAt ( ) . getTime ( ) ; } catch ( FileNotFoundException e ) { log . warn ( "file not found exception, return 0L" , e ) ; } catch ( JargonException e ) { log . error ( "jargon exception, rethrow as unchecked" , e ) ; throw new JargonRuntimeException ( e ) ; } }
public void test() { if ( returnValue instanceof HasTraceEntryMixin ) { TraceEntry traceEntry = ( ( HasTraceEntryMixin ) httpURLConnection ) . glowroot$getTraceEntry ( ) ; ( ( HasTraceEntryMixin ) returnValue ) . glowroot$setTraceEntry ( traceEntry ) ; } else-if ( returnValue != null && ! outputStreamIssueAlreadyLogged . getAndSet ( true ) ) { logger . info ( "found non-instrumented http url connection output stream, please" + " report to the Glowroot project: {}" , returnValue . getClass ( ) . getName ( ) ) ; } }
public void test() { if ( LOGGER . isErrorEnabled ( ) ) { LOGGER . error ( "Exception while trying to get attachment version !" , e ) ; } }
public void test() { try { latch . countDown ( ) ; code_block = ForStatement ; } catch ( Exception e ) { LOG . error ( "Caught exception while checking all ledgers" , e ) ; exceptionCaught . set ( true ) ; } }
public void test() { if ( isDebug ) { logger . debug ( "Set header {}={}" , name , value ) ; } }
@ Test public void testEntityConnections ( ) throws Exception { EntityManager em = app . getEntityManager ( ) ; final UUID applicationId = app . getId ( ) ; assertNotNull ( em ) ; logger . info ( "\n\nCreating Cat entity A with name of Dylan\n" ) ; Map < String , Object > properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Dylan" ) ; Entity catA = em . create ( "cat" , properties ) ; assertNotNull ( catA ) ; logger . info ( "\n\nEntity A created with id " + catA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nLooking up cat with id " + catA . getUuid ( ) + "\n" ) ; Entity cat = em . get ( catA ) ; assertNotNull ( cat ) ; logger . info ( "\n\nFound entity " + cat . getUuid ( ) + " of type " + cat . getType ( ) + " with name " + cat . getProperty ( "name" ) + "\n" ) ; logger . info ( "\n\nCreating cat entity B with name of Nico\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Nico" ) ; Entity catB = em . create ( "cat" , properties ) ; assertNotNull ( catB ) ; logger . info ( "\n\nEntity B created with id " + catB . getUuid ( ) + "\n" ) ; logger . info ( "\n\nCreating award entity with name of 'best cat'\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Best Cat Ever" ) ; Entity awardA = em . create ( "award" , properties ) ; assertNotNull ( awardA ) ; logger . info ( "\n\nEntity created with id " + awardA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nConnecting " + catA . getUuid ( ) + " \"likes\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( catA , "likes" , catB ) ; logger . info ( "\n\nConnecting " + awardA . getUuid ( ) + " \"awarded\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( awardA , "awarded" , catB ) ; app . waitForQueueDrainAndRefreshIndex ( ) ; logger . info ( "Find all connections for cat A: " + catA . getUuid ( ) ) ; testEntityConnections ( applicationId , catA . getUuid ( ) , "likes" , "cat" , 1 ) ; logger . info ( "Find all connections for award A: " + awardA . getUuid ( ) ) ; testEntityConnections ( applicationId , awardA . getUuid ( ) , "awarded" , "award" , 1 ) ; logger . info ( "\n\nConnecting " + awardA . getUuid ( ) + " \"awarded\" " + catA . getUuid ( ) + "\n" ) ; em . createConnection ( awardA , "awarded" , catA ) ; app . waitForQueueDrainAndRefreshIndex ( ) ; testApplicationCollections ( applicationId , "cats" , 2 ) ; testApplicationCollections ( applicationId , "awards" , 1 ) ; logger . info ( "\n\nSearching Award A for recipients with the name Dylan\n" ) ; }
@ Test public void testEntityConnections ( ) throws Exception { EntityManager em = app . getEntityManager ( ) ; final UUID applicationId = app . getId ( ) ; assertNotNull ( em ) ; logger . info ( "\n\nCreating Cat entity A with name of Dylan\n" ) ; Map < String , Object > properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Dylan" ) ; Entity catA = em . create ( "cat" , properties ) ; assertNotNull ( catA ) ; logger . info ( "\n\nEntity A created with id " + catA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nLooking up cat with id " + catA . getUuid ( ) + "\n" ) ; Entity cat = em . get ( catA ) ; assertNotNull ( cat ) ; logger . info ( "\n\nFound entity " + cat . getUuid ( ) + " of type " + cat . getType ( ) + " with name " + cat . getProperty ( "name" ) + "\n" ) ; logger . info ( "\n\nCreating cat entity B with name of Nico\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Nico" ) ; Entity catB = em . create ( "cat" , properties ) ; assertNotNull ( catB ) ; logger . info ( "\n\nEntity B created with id " + catB . getUuid ( ) + "\n" ) ; logger . info ( "\n\nCreating award entity with name of 'best cat'\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Best Cat Ever" ) ; Entity awardA = em . create ( "award" , properties ) ; assertNotNull ( awardA ) ; logger . info ( "\n\nEntity created with id " + awardA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nConnecting " + catA . getUuid ( ) + " \"likes\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( catA , "likes" , catB ) ; logger . info ( "\n\nConnecting " + awardA . getUuid ( ) + " \"awarded\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( awardA , "awarded" , catB ) ; app . waitForQueueDrainAndRefreshIndex ( ) ; logger . info ( "Find all connections for cat A: " + catA . getUuid ( ) ) ; testEntityConnections ( applicationId , catA . getUuid ( ) , "likes" , "cat" , 1 ) ; logger . info ( "Find all connections for award A: " + awardA . getUuid ( ) ) ; testEntityConnections ( applicationId , awardA . getUuid ( ) , "awarded" , "award" , 1 ) ; logger . info ( "\n\nConnecting " + awardA . getUuid ( ) + " \"awarded\" " + catA . getUuid ( ) + "\n" ) ; em . createConnection ( awardA , "awarded" , catA ) ; app . waitForQueueDrainAndRefreshIndex ( ) ; testApplicationCollections ( applicationId , "cats" , 2 ) ; testApplicationCollections ( applicationId , "awards" , 1 ) ; logger . info ( "\n\nSearching Award A for recipients with the name Dylan\n" ) ; }
@ Test public void testEntityConnections ( ) throws Exception { EntityManager em = app . getEntityManager ( ) ; final UUID applicationId = app . getId ( ) ; assertNotNull ( em ) ; logger . info ( "\n\nCreating Cat entity A with name of Dylan\n" ) ; Map < String , Object > properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Dylan" ) ; Entity catA = em . create ( "cat" , properties ) ; assertNotNull ( catA ) ; logger . info ( "\n\nEntity A created with id " + catA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nLooking up cat with id " + catA . getUuid ( ) + "\n" ) ; Entity cat = em . get ( catA ) ; assertNotNull ( cat ) ; logger . info ( "\n\nFound entity " + cat . getUuid ( ) + " of type " + cat . getType ( ) + " with name " + cat . getProperty ( "name" ) + "\n" ) ; logger . info ( "\n\nCreating cat entity B with name of Nico\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Nico" ) ; Entity catB = em . create ( "cat" , properties ) ; assertNotNull ( catB ) ; logger . info ( "\n\nEntity B created with id " + catB . getUuid ( ) + "\n" ) ; logger . info ( "\n\nCreating award entity with name of 'best cat'\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Best Cat Ever" ) ; Entity awardA = em . create ( "award" , properties ) ; assertNotNull ( awardA ) ; logger . info ( "\n\nEntity created with id " + awardA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nConnecting " + catA . getUuid ( ) + " \"likes\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( catA , "likes" , catB ) ; logger . info ( "\n\nConnecting " + awardA . getUuid ( ) + " \"awarded\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( awardA , "awarded" , catB ) ; app . waitForQueueDrainAndRefreshIndex ( ) ; logger . info ( "Find all connections for cat A: " + catA . getUuid ( ) ) ; testEntityConnections ( applicationId , catA . getUuid ( ) , "likes" , "cat" , 1 ) ; logger . info ( "Find all connections for award A: " + awardA . getUuid ( ) ) ; testEntityConnections ( applicationId , awardA . getUuid ( ) , "awarded" , "award" , 1 ) ; logger . info ( "\n\nConnecting " + awardA . getUuid ( ) + " \"awarded\" " + catA . getUuid ( ) + "\n" ) ; em . createConnection ( awardA , "awarded" , catA ) ; app . waitForQueueDrainAndRefreshIndex ( ) ; testApplicationCollections ( applicationId , "cats" , 2 ) ; testApplicationCollections ( applicationId , "awards" , 1 ) ; logger . info ( "\n\nSearching Award A for recipients with the name Dylan\n" ) ; }
@ Test public void testEntityConnections ( ) throws Exception { EntityManager em = app . getEntityManager ( ) ; final UUID applicationId = app . getId ( ) ; assertNotNull ( em ) ; logger . info ( "\n\nCreating Cat entity A with name of Dylan\n" ) ; Map < String , Object > properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Dylan" ) ; Entity catA = em . create ( "cat" , properties ) ; assertNotNull ( catA ) ; logger . info ( "\n\nEntity A created with id " + catA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nLooking up cat with id " + catA . getUuid ( ) + "\n" ) ; Entity cat = em . get ( catA ) ; assertNotNull ( cat ) ; logger . info ( "\n\nFound entity " + cat . getUuid ( ) + " of type " + cat . getType ( ) + " with name " + cat . getProperty ( "name" ) + "\n" ) ; logger . info ( "\n\nCreating cat entity B with name of Nico\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Nico" ) ; Entity catB = em . create ( "cat" , properties ) ; assertNotNull ( catB ) ; logger . info ( "\n\nEntity B created with id " + catB . getUuid ( ) + "\n" ) ; logger . info ( "\n\nCreating award entity with name of 'best cat'\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Best Cat Ever" ) ; Entity awardA = em . create ( "award" , properties ) ; assertNotNull ( awardA ) ; logger . info ( "\n\nEntity created with id " + awardA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nConnecting " + catA . getUuid ( ) + " \"likes\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( catA , "likes" , catB ) ; logger . info ( "\n\nConnecting " + awardA . getUuid ( ) + " \"awarded\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( awardA , "awarded" , catB ) ; app . waitForQueueDrainAndRefreshIndex ( ) ; logger . info ( "Find all connections for cat A: " + catA . getUuid ( ) ) ; testEntityConnections ( applicationId , catA . getUuid ( ) , "likes" , "cat" , 1 ) ; logger . info ( "Find all connections for award A: " + awardA . getUuid ( ) ) ; testEntityConnections ( applicationId , awardA . getUuid ( ) , "awarded" , "award" , 1 ) ; logger . info ( "\n\nConnecting " + awardA . getUuid ( ) + " \"awarded\" " + catA . getUuid ( ) + "\n" ) ; em . createConnection ( awardA , "awarded" , catA ) ; app . waitForQueueDrainAndRefreshIndex ( ) ; testApplicationCollections ( applicationId , "cats" , 2 ) ; testApplicationCollections ( applicationId , "awards" , 1 ) ; logger . info ( "\n\nSearching Award A for recipients with the name Dylan\n" ) ; }
@ Test public void testEntityConnections ( ) throws Exception { EntityManager em = app . getEntityManager ( ) ; final UUID applicationId = app . getId ( ) ; assertNotNull ( em ) ; logger . info ( "\n\nCreating Cat entity A with name of Dylan\n" ) ; Map < String , Object > properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Dylan" ) ; Entity catA = em . create ( "cat" , properties ) ; assertNotNull ( catA ) ; logger . info ( "\n\nEntity A created with id " + catA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nLooking up cat with id " + catA . getUuid ( ) + "\n" ) ; Entity cat = em . get ( catA ) ; assertNotNull ( cat ) ; logger . info ( "\n\nFound entity " + cat . getUuid ( ) + " of type " + cat . getType ( ) + " with name " + cat . getProperty ( "name" ) + "\n" ) ; logger . info ( "\n\nCreating cat entity B with name of Nico\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Nico" ) ; Entity catB = em . create ( "cat" , properties ) ; assertNotNull ( catB ) ; logger . info ( "\n\nEntity B created with id " + catB . getUuid ( ) + "\n" ) ; logger . info ( "\n\nCreating award entity with name of 'best cat'\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Best Cat Ever" ) ; Entity awardA = em . create ( "award" , properties ) ; assertNotNull ( awardA ) ; logger . info ( "\n\nEntity created with id " + awardA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nConnecting " + catA . getUuid ( ) + " \"likes\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( catA , "likes" , catB ) ; logger . info ( "\n\nConnecting " + awardA . getUuid ( ) + " \"awarded\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( awardA , "awarded" , catB ) ; app . waitForQueueDrainAndRefreshIndex ( ) ; logger . info ( "Find all connections for cat A: " + catA . getUuid ( ) ) ; testEntityConnections ( applicationId , catA . getUuid ( ) , "likes" , "cat" , 1 ) ; logger . info ( "Find all connections for award A: " + awardA . getUuid ( ) ) ; testEntityConnections ( applicationId , awardA . getUuid ( ) , "awarded" , "award" , 1 ) ; logger . info ( "\n\nConnecting " + awardA . getUuid ( ) + " \"awarded\" " + catA . getUuid ( ) + "\n" ) ; em . createConnection ( awardA , "awarded" , catA ) ; app . waitForQueueDrainAndRefreshIndex ( ) ; testApplicationCollections ( applicationId , "cats" , 2 ) ; testApplicationCollections ( applicationId , "awards" , 1 ) ; logger . info ( "\n\nSearching Award A for recipients with the name Dylan\n" ) ; }
@ Test public void testEntityConnections ( ) throws Exception { EntityManager em = app . getEntityManager ( ) ; final UUID applicationId = app . getId ( ) ; assertNotNull ( em ) ; logger . info ( "\n\nCreating Cat entity A with name of Dylan\n" ) ; Map < String , Object > properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Dylan" ) ; Entity catA = em . create ( "cat" , properties ) ; assertNotNull ( catA ) ; logger . info ( "\n\nEntity A created with id " + catA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nLooking up cat with id " + catA . getUuid ( ) + "\n" ) ; Entity cat = em . get ( catA ) ; assertNotNull ( cat ) ; logger . info ( "\n\nFound entity " + cat . getUuid ( ) + " of type " + cat . getType ( ) + " with name " + cat . getProperty ( "name" ) + "\n" ) ; logger . info ( "\n\nCreating cat entity B with name of Nico\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Nico" ) ; Entity catB = em . create ( "cat" , properties ) ; assertNotNull ( catB ) ; logger . info ( "\n\nEntity B created with id " + catB . getUuid ( ) + "\n" ) ; logger . info ( "\n\nCreating award entity with name of 'best cat'\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Best Cat Ever" ) ; Entity awardA = em . create ( "award" , properties ) ; assertNotNull ( awardA ) ; logger . info ( "\n\nEntity created with id " + awardA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nConnecting " + catA . getUuid ( ) + " \"likes\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( catA , "likes" , catB ) ; logger . info ( "\n\nConnecting " + awardA . getUuid ( ) + " \"awarded\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( awardA , "awarded" , catB ) ; app . waitForQueueDrainAndRefreshIndex ( ) ; logger . info ( "Find all connections for cat A: " + catA . getUuid ( ) ) ; testEntityConnections ( applicationId , catA . getUuid ( ) , "likes" , "cat" , 1 ) ; logger . info ( "Find all connections for award A: " + awardA . getUuid ( ) ) ; testEntityConnections ( applicationId , awardA . getUuid ( ) , "awarded" , "award" , 1 ) ; logger . info ( "\n\nConnecting " + awardA . getUuid ( ) + " \"awarded\" " + catA . getUuid ( ) + "\n" ) ; em . createConnection ( awardA , "awarded" , catA ) ; app . waitForQueueDrainAndRefreshIndex ( ) ; testApplicationCollections ( applicationId , "cats" , 2 ) ; testApplicationCollections ( applicationId , "awards" , 1 ) ; logger . info ( "\n\nSearching Award A for recipients with the name Dylan\n" ) ; }
@ Test public void testEntityConnections ( ) throws Exception { EntityManager em = app . getEntityManager ( ) ; final UUID applicationId = app . getId ( ) ; assertNotNull ( em ) ; logger . info ( "\n\nCreating Cat entity A with name of Dylan\n" ) ; Map < String , Object > properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Dylan" ) ; Entity catA = em . create ( "cat" , properties ) ; assertNotNull ( catA ) ; logger . info ( "\n\nEntity A created with id " + catA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nLooking up cat with id " + catA . getUuid ( ) + "\n" ) ; Entity cat = em . get ( catA ) ; assertNotNull ( cat ) ; logger . info ( "\n\nFound entity " + cat . getUuid ( ) + " of type " + cat . getType ( ) + " with name " + cat . getProperty ( "name" ) + "\n" ) ; logger . info ( "\n\nCreating cat entity B with name of Nico\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Nico" ) ; Entity catB = em . create ( "cat" , properties ) ; assertNotNull ( catB ) ; logger . info ( "\n\nEntity B created with id " + catB . getUuid ( ) + "\n" ) ; logger . info ( "\n\nCreating award entity with name of 'best cat'\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Best Cat Ever" ) ; Entity awardA = em . create ( "award" , properties ) ; assertNotNull ( awardA ) ; logger . info ( "\n\nEntity created with id " + awardA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nConnecting " + catA . getUuid ( ) + " \"likes\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( catA , "likes" , catB ) ; logger . info ( "\n\nConnecting " + awardA . getUuid ( ) + " \"awarded\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( awardA , "awarded" , catB ) ; app . waitForQueueDrainAndRefreshIndex ( ) ; logger . info ( "Find all connections for cat A: " + catA . getUuid ( ) ) ; testEntityConnections ( applicationId , catA . getUuid ( ) , "likes" , "cat" , 1 ) ; logger . info ( "Find all connections for award A: " + awardA . getUuid ( ) ) ; testEntityConnections ( applicationId , awardA . getUuid ( ) , "awarded" , "award" , 1 ) ; logger . info ( "\n\nConnecting " + awardA . getUuid ( ) + " \"awarded\" " + catA . getUuid ( ) + "\n" ) ; em . createConnection ( awardA , "awarded" , catA ) ; app . waitForQueueDrainAndRefreshIndex ( ) ; testApplicationCollections ( applicationId , "cats" , 2 ) ; testApplicationCollections ( applicationId , "awards" , 1 ) ; logger . info ( "\n\nSearching Award A for recipients with the name Dylan\n" ) ; }
@ Test public void testEntityConnections ( ) throws Exception { EntityManager em = app . getEntityManager ( ) ; final UUID applicationId = app . getId ( ) ; assertNotNull ( em ) ; logger . info ( "\n\nCreating Cat entity A with name of Dylan\n" ) ; Map < String , Object > properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Dylan" ) ; Entity catA = em . create ( "cat" , properties ) ; assertNotNull ( catA ) ; logger . info ( "\n\nEntity A created with id " + catA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nLooking up cat with id " + catA . getUuid ( ) + "\n" ) ; Entity cat = em . get ( catA ) ; assertNotNull ( cat ) ; logger . info ( "\n\nFound entity " + cat . getUuid ( ) + " of type " + cat . getType ( ) + " with name " + cat . getProperty ( "name" ) + "\n" ) ; logger . info ( "\n\nCreating cat entity B with name of Nico\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Nico" ) ; Entity catB = em . create ( "cat" , properties ) ; assertNotNull ( catB ) ; logger . info ( "\n\nEntity B created with id " + catB . getUuid ( ) + "\n" ) ; logger . info ( "\n\nCreating award entity with name of 'best cat'\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Best Cat Ever" ) ; Entity awardA = em . create ( "award" , properties ) ; assertNotNull ( awardA ) ; logger . info ( "\n\nEntity created with id " + awardA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nConnecting " + catA . getUuid ( ) + " \"likes\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( catA , "likes" , catB ) ; logger . info ( "\n\nConnecting " + awardA . getUuid ( ) + " \"awarded\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( awardA , "awarded" , catB ) ; app . waitForQueueDrainAndRefreshIndex ( ) ; logger . info ( "Find all connections for cat A: " + catA . getUuid ( ) ) ; testEntityConnections ( applicationId , catA . getUuid ( ) , "likes" , "cat" , 1 ) ; logger . info ( "Find all connections for award A: " + awardA . getUuid ( ) ) ; testEntityConnections ( applicationId , awardA . getUuid ( ) , "awarded" , "award" , 1 ) ; logger . info ( "\n\nConnecting " + awardA . getUuid ( ) + " \"awarded\" " + catA . getUuid ( ) + "\n" ) ; em . createConnection ( awardA , "awarded" , catA ) ; app . waitForQueueDrainAndRefreshIndex ( ) ; testApplicationCollections ( applicationId , "cats" , 2 ) ; testApplicationCollections ( applicationId , "awards" , 1 ) ; logger . info ( "\n\nSearching Award A for recipients with the name Dylan\n" ) ; }
@ Test public void testEntityConnections ( ) throws Exception { EntityManager em = app . getEntityManager ( ) ; final UUID applicationId = app . getId ( ) ; assertNotNull ( em ) ; logger . info ( "\n\nCreating Cat entity A with name of Dylan\n" ) ; Map < String , Object > properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Dylan" ) ; Entity catA = em . create ( "cat" , properties ) ; assertNotNull ( catA ) ; logger . info ( "\n\nEntity A created with id " + catA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nLooking up cat with id " + catA . getUuid ( ) + "\n" ) ; Entity cat = em . get ( catA ) ; assertNotNull ( cat ) ; logger . info ( "\n\nFound entity " + cat . getUuid ( ) + " of type " + cat . getType ( ) + " with name " + cat . getProperty ( "name" ) + "\n" ) ; logger . info ( "\n\nCreating cat entity B with name of Nico\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Nico" ) ; Entity catB = em . create ( "cat" , properties ) ; assertNotNull ( catB ) ; logger . info ( "\n\nEntity B created with id " + catB . getUuid ( ) + "\n" ) ; logger . info ( "\n\nCreating award entity with name of 'best cat'\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Best Cat Ever" ) ; Entity awardA = em . create ( "award" , properties ) ; assertNotNull ( awardA ) ; logger . info ( "\n\nEntity created with id " + awardA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nConnecting " + catA . getUuid ( ) + " \"likes\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( catA , "likes" , catB ) ; logger . info ( "\n\nConnecting " + awardA . getUuid ( ) + " \"awarded\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( awardA , "awarded" , catB ) ; app . waitForQueueDrainAndRefreshIndex ( ) ; logger . info ( "Find all connections for cat A: " + catA . getUuid ( ) ) ; testEntityConnections ( applicationId , catA . getUuid ( ) , "likes" , "cat" , 1 ) ; logger . info ( "Find all connections for award A: " + awardA . getUuid ( ) ) ; testEntityConnections ( applicationId , awardA . getUuid ( ) , "awarded" , "award" , 1 ) ; logger . info ( "\n\nConnecting " + awardA . getUuid ( ) + " \"awarded\" " + catA . getUuid ( ) + "\n" ) ; em . createConnection ( awardA , "awarded" , catA ) ; app . waitForQueueDrainAndRefreshIndex ( ) ; testApplicationCollections ( applicationId , "cats" , 2 ) ; testApplicationCollections ( applicationId , "awards" , 1 ) ; logger . info ( "\n\nSearching Award A for recipients with the name Dylan\n" ) ; }
@ Test public void testEntityConnections ( ) throws Exception { EntityManager em = app . getEntityManager ( ) ; final UUID applicationId = app . getId ( ) ; assertNotNull ( em ) ; logger . info ( "\n\nCreating Cat entity A with name of Dylan\n" ) ; Map < String , Object > properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Dylan" ) ; Entity catA = em . create ( "cat" , properties ) ; assertNotNull ( catA ) ; logger . info ( "\n\nEntity A created with id " + catA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nLooking up cat with id " + catA . getUuid ( ) + "\n" ) ; Entity cat = em . get ( catA ) ; assertNotNull ( cat ) ; logger . info ( "\n\nFound entity " + cat . getUuid ( ) + " of type " + cat . getType ( ) + " with name " + cat . getProperty ( "name" ) + "\n" ) ; logger . info ( "\n\nCreating cat entity B with name of Nico\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Nico" ) ; Entity catB = em . create ( "cat" , properties ) ; assertNotNull ( catB ) ; logger . info ( "\n\nEntity B created with id " + catB . getUuid ( ) + "\n" ) ; logger . info ( "\n\nCreating award entity with name of 'best cat'\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Best Cat Ever" ) ; Entity awardA = em . create ( "award" , properties ) ; assertNotNull ( awardA ) ; logger . info ( "\n\nEntity created with id " + awardA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nConnecting " + catA . getUuid ( ) + " \"likes\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( catA , "likes" , catB ) ; logger . info ( "\n\nConnecting " + awardA . getUuid ( ) + " \"awarded\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( awardA , "awarded" , catB ) ; app . waitForQueueDrainAndRefreshIndex ( ) ; logger . info ( "Find all connections for cat A: " + catA . getUuid ( ) ) ; testEntityConnections ( applicationId , catA . getUuid ( ) , "likes" , "cat" , 1 ) ; logger . info ( "Find all connections for award A: " + awardA . getUuid ( ) ) ; testEntityConnections ( applicationId , awardA . getUuid ( ) , "awarded" , "award" , 1 ) ; logger . info ( "\n\nConnecting " + awardA . getUuid ( ) + " \"awarded\" " + catA . getUuid ( ) + "\n" ) ; em . createConnection ( awardA , "awarded" , catA ) ; app . waitForQueueDrainAndRefreshIndex ( ) ; testApplicationCollections ( applicationId , "cats" , 2 ) ; testApplicationCollections ( applicationId , "awards" , 1 ) ; logger . info ( "\n\nSearching Award A for recipients with the name Dylan\n" ) ; }
@ Test public void testEntityConnections ( ) throws Exception { EntityManager em = app . getEntityManager ( ) ; final UUID applicationId = app . getId ( ) ; assertNotNull ( em ) ; logger . info ( "\n\nCreating Cat entity A with name of Dylan\n" ) ; Map < String , Object > properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Dylan" ) ; Entity catA = em . create ( "cat" , properties ) ; assertNotNull ( catA ) ; logger . info ( "\n\nEntity A created with id " + catA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nLooking up cat with id " + catA . getUuid ( ) + "\n" ) ; Entity cat = em . get ( catA ) ; assertNotNull ( cat ) ; logger . info ( "\n\nFound entity " + cat . getUuid ( ) + " of type " + cat . getType ( ) + " with name " + cat . getProperty ( "name" ) + "\n" ) ; logger . info ( "\n\nCreating cat entity B with name of Nico\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Nico" ) ; Entity catB = em . create ( "cat" , properties ) ; assertNotNull ( catB ) ; logger . info ( "\n\nEntity B created with id " + catB . getUuid ( ) + "\n" ) ; logger . info ( "\n\nCreating award entity with name of 'best cat'\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Best Cat Ever" ) ; Entity awardA = em . create ( "award" , properties ) ; assertNotNull ( awardA ) ; logger . info ( "\n\nEntity created with id " + awardA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nConnecting " + catA . getUuid ( ) + " \"likes\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( catA , "likes" , catB ) ; logger . info ( "\n\nConnecting " + awardA . getUuid ( ) + " \"awarded\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( awardA , "awarded" , catB ) ; app . waitForQueueDrainAndRefreshIndex ( ) ; logger . info ( "Find all connections for cat A: " + catA . getUuid ( ) ) ; testEntityConnections ( applicationId , catA . getUuid ( ) , "likes" , "cat" , 1 ) ; logger . info ( "Find all connections for award A: " + awardA . getUuid ( ) ) ; testEntityConnections ( applicationId , awardA . getUuid ( ) , "awarded" , "award" , 1 ) ; logger . info ( "\n\nConnecting " + awardA . getUuid ( ) + " \"awarded\" " + catA . getUuid ( ) + "\n" ) ; em . createConnection ( awardA , "awarded" , catA ) ; app . waitForQueueDrainAndRefreshIndex ( ) ; testApplicationCollections ( applicationId , "cats" , 2 ) ; testApplicationCollections ( applicationId , "awards" , 1 ) ; logger . info ( "\n\nSearching Award A for recipients with the name Dylan\n" ) ; }
@ Test public void testEntityConnections ( ) throws Exception { EntityManager em = app . getEntityManager ( ) ; final UUID applicationId = app . getId ( ) ; assertNotNull ( em ) ; logger . info ( "\n\nCreating Cat entity A with name of Dylan\n" ) ; Map < String , Object > properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Dylan" ) ; Entity catA = em . create ( "cat" , properties ) ; assertNotNull ( catA ) ; logger . info ( "\n\nEntity A created with id " + catA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nLooking up cat with id " + catA . getUuid ( ) + "\n" ) ; Entity cat = em . get ( catA ) ; assertNotNull ( cat ) ; logger . info ( "\n\nFound entity " + cat . getUuid ( ) + " of type " + cat . getType ( ) + " with name " + cat . getProperty ( "name" ) + "\n" ) ; logger . info ( "\n\nCreating cat entity B with name of Nico\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Nico" ) ; Entity catB = em . create ( "cat" , properties ) ; assertNotNull ( catB ) ; logger . info ( "\n\nEntity B created with id " + catB . getUuid ( ) + "\n" ) ; logger . info ( "\n\nCreating award entity with name of 'best cat'\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Best Cat Ever" ) ; Entity awardA = em . create ( "award" , properties ) ; assertNotNull ( awardA ) ; logger . info ( "\n\nEntity created with id " + awardA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nConnecting " + catA . getUuid ( ) + " \"likes\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( catA , "likes" , catB ) ; logger . info ( "\n\nConnecting " + awardA . getUuid ( ) + " \"awarded\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( awardA , "awarded" , catB ) ; app . waitForQueueDrainAndRefreshIndex ( ) ; logger . info ( "Find all connections for cat A: " + catA . getUuid ( ) ) ; testEntityConnections ( applicationId , catA . getUuid ( ) , "likes" , "cat" , 1 ) ; logger . info ( "Find all connections for award A: " + awardA . getUuid ( ) ) ; testEntityConnections ( applicationId , awardA . getUuid ( ) , "awarded" , "award" , 1 ) ; logger . info ( "\n\nConnecting " + awardA . getUuid ( ) + " \"awarded\" " + catA . getUuid ( ) + "\n" ) ; em . createConnection ( awardA , "awarded" , catA ) ; app . waitForQueueDrainAndRefreshIndex ( ) ; testApplicationCollections ( applicationId , "cats" , 2 ) ; testApplicationCollections ( applicationId , "awards" , 1 ) ; logger . info ( "\n\nSearching Award A for recipients with the name Dylan\n" ) ; }
@ Test public void testEntityConnections ( ) throws Exception { EntityManager em = app . getEntityManager ( ) ; final UUID applicationId = app . getId ( ) ; assertNotNull ( em ) ; logger . info ( "\n\nCreating Cat entity A with name of Dylan\n" ) ; Map < String , Object > properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Dylan" ) ; Entity catA = em . create ( "cat" , properties ) ; assertNotNull ( catA ) ; logger . info ( "\n\nEntity A created with id " + catA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nLooking up cat with id " + catA . getUuid ( ) + "\n" ) ; Entity cat = em . get ( catA ) ; assertNotNull ( cat ) ; logger . info ( "\n\nFound entity " + cat . getUuid ( ) + " of type " + cat . getType ( ) + " with name " + cat . getProperty ( "name" ) + "\n" ) ; logger . info ( "\n\nCreating cat entity B with name of Nico\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Nico" ) ; Entity catB = em . create ( "cat" , properties ) ; assertNotNull ( catB ) ; logger . info ( "\n\nEntity B created with id " + catB . getUuid ( ) + "\n" ) ; logger . info ( "\n\nCreating award entity with name of 'best cat'\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Best Cat Ever" ) ; Entity awardA = em . create ( "award" , properties ) ; assertNotNull ( awardA ) ; logger . info ( "\n\nEntity created with id " + awardA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nConnecting " + catA . getUuid ( ) + " \"likes\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( catA , "likes" , catB ) ; logger . info ( "\n\nConnecting " + awardA . getUuid ( ) + " \"awarded\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( awardA , "awarded" , catB ) ; app . waitForQueueDrainAndRefreshIndex ( ) ; logger . info ( "Find all connections for cat A: " + catA . getUuid ( ) ) ; testEntityConnections ( applicationId , catA . getUuid ( ) , "likes" , "cat" , 1 ) ; logger . info ( "Find all connections for award A: " + awardA . getUuid ( ) ) ; testEntityConnections ( applicationId , awardA . getUuid ( ) , "awarded" , "award" , 1 ) ; logger . info ( "\n\nConnecting " + awardA . getUuid ( ) + " \"awarded\" " + catA . getUuid ( ) + "\n" ) ; em . createConnection ( awardA , "awarded" , catA ) ; app . waitForQueueDrainAndRefreshIndex ( ) ; testApplicationCollections ( applicationId , "cats" , 2 ) ; testApplicationCollections ( applicationId , "awards" , 1 ) ; logger . info ( "\n\nSearching Award A for recipients with the name Dylan\n" ) ; }
@ Test public void testEntityConnections ( ) throws Exception { EntityManager em = app . getEntityManager ( ) ; final UUID applicationId = app . getId ( ) ; assertNotNull ( em ) ; logger . info ( "\n\nCreating Cat entity A with name of Dylan\n" ) ; Map < String , Object > properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Dylan" ) ; Entity catA = em . create ( "cat" , properties ) ; assertNotNull ( catA ) ; logger . info ( "\n\nEntity A created with id " + catA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nLooking up cat with id " + catA . getUuid ( ) + "\n" ) ; Entity cat = em . get ( catA ) ; assertNotNull ( cat ) ; logger . info ( "\n\nFound entity " + cat . getUuid ( ) + " of type " + cat . getType ( ) + " with name " + cat . getProperty ( "name" ) + "\n" ) ; logger . info ( "\n\nCreating cat entity B with name of Nico\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Nico" ) ; Entity catB = em . create ( "cat" , properties ) ; assertNotNull ( catB ) ; logger . info ( "\n\nEntity B created with id " + catB . getUuid ( ) + "\n" ) ; logger . info ( "\n\nCreating award entity with name of 'best cat'\n" ) ; properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "name" , "Best Cat Ever" ) ; Entity awardA = em . create ( "award" , properties ) ; assertNotNull ( awardA ) ; logger . info ( "\n\nEntity created with id " + awardA . getUuid ( ) + "\n" ) ; logger . info ( "\n\nConnecting " + catA . getUuid ( ) + " \"likes\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( catA , "likes" , catB ) ; logger . info ( "\n\nConnecting " + awardA . getUuid ( ) + " \"awarded\" " + catB . getUuid ( ) + "\n" ) ; em . createConnection ( awardA , "awarded" , catB ) ; app . waitForQueueDrainAndRefreshIndex ( ) ; logger . info ( "Find all connections for cat A: " + catA . getUuid ( ) ) ; testEntityConnections ( applicationId , catA . getUuid ( ) , "likes" , "cat" , 1 ) ; logger . info ( "Find all connections for award A: " + awardA . getUuid ( ) ) ; testEntityConnections ( applicationId , awardA . getUuid ( ) , "awarded" , "award" , 1 ) ; logger . info ( "\n\nConnecting " + awardA . getUuid ( ) + " \"awarded\" " + catA . getUuid ( ) + "\n" ) ; em . createConnection ( awardA , "awarded" , catA ) ; app . waitForQueueDrainAndRefreshIndex ( ) ; testApplicationCollections ( applicationId , "cats" , 2 ) ; testApplicationCollections ( applicationId , "awards" , 1 ) ; logger . info ( "\n\nSearching Award A for recipients with the name Dylan\n" ) ; }
public void test() { try { dAdminMgr . assignUser ( userRole ) ; } catch ( SecurityException se ) { LOG . warn ( "addUserAdminRoles tenant={} userId={} role name={} caught SecurityException={}" , getTenant ( ) , userRole . getUserId ( ) , userRole . getName ( ) , se ) ; } }
@ BeforeEach void logTest ( TestInfo testInfo ) { log . info ( "Executing: {}" , testInfo . getDisplayName ( ) ) ; }
public void test() { try { resetCoprocessor ( tableName , hbaseAdmin , hdfsCoprocessorJar ) ; processed . add ( tableName ) ; } catch ( IOException ex ) { logger . error ( "Error processing " + tableName , ex ) ; } }
public void test() { try { log . info ( "Run started" ) ; addCategories ( ) ; code_block = ForStatement ; log . info ( "Run successful" ) ; } catch ( Exception e ) { log . error ( "Cannot process run" , e ) ; } }
public void test() { if ( ! type . isInstance ( session ) ) { LOG . debug ( "Session not of the expected type [session={}, expectedType={}]" , session , type ) ; return null ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( advice ) ; } }
public void test() { if ( s_logger . isInfoEnabled ( ) ) { s_logger . info ( "Executing resource DeleteStoragePoolCommand: " + _gson . toJson ( cmd ) ) ; } }
public void test() { if ( e instanceof RemoteException ) { s_logger . warn ( "Encounter remote exception to vCenter, invalidate VMware session context" ) ; invalidateServiceContext ( ) ; } }
@ Bean ( name = "domainDistributionAutomationInboundKafkaRequestsMessageListenerContainer" ) public DefaultMessageListenerContainer messageListenerContainer ( @ Qualifier ( "domainDistributionAutomationInboundKafkaRequestsMessageListener" ) final MessageListener messageListener ) { LOGGER . info ( "Initializing domainDistributionAutomationInboundKafkaRequestsMessageListenerContainer bean." ) ; return this . jmsConfigurationFactory . initMessageListenerContainer ( messageListener ) ; }
public void test() { if ( e instanceof InterpreterRPCException ) { result . ex = ( InterpreterRPCException ) e ; result . setExIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof InterpreterRPCException ) { result . ex = ( InterpreterRPCException ) e ; result . setExIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof InterpreterRPCException ) { result . ex = ( InterpreterRPCException ) e ; result . setExIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { try { fcall . sendResponse ( fb , msg , msgType , seqid ) ; } catch ( java . lang . Exception ex ) { _LOGGER . error ( "Exception writing to internal frame buffer" , ex ) ; fb . close ( ) ; } }
@ Override public void onPendingFailure ( ProviderException cause ) { LOG . trace ( "TX:{} has failed a acknowledge." , getTransactionId ( ) ) ; participants . put ( envelope . getConsumerId ( ) , envelope . getConsumerId ( ) ) ; }
public void test() { try { LOGGER . warn ( "Waiting 1s for taskThread to be set..." ) ; Thread . sleep ( 1000 ) ; } catch ( InterruptedException e ) { LOGGER . warn ( e . toString ( ) , e ) ; Thread . currentThread ( ) . interrupt ( ) ; return false ; } }
@ Override public DexOrder findCounterOffer ( DexOrder createdOrder ) { log . debug ( "DexMatcherServiceImpl:findCounterOffer()" ) ; OrderType counterOrderType = createdOrder . getType ( ) . isBuy ( ) ? OrderType . SELL : OrderType . BUY ; String orderBy = createdOrder . getType ( ) . isSell ( ) ? "DESC" : "ASC" ; Integer currentTime = timeService . getEpochTime ( ) ; BigDecimal offerAmount = new BigDecimal ( createdOrder . getOrderAmount ( ) ) ; Integer pairCurrency = DexCurrency . getValue ( createdOrder . getPairCurrency ( ) ) ; BigDecimal pairRate = new BigDecimal ( EthUtil . ethToGwei ( createdOrder . getPairRate ( ) ) ) ; log . debug ( "Dumping arguments: type: {}, currentTime: {}, offerAmount: {}, offerCurrency: {}, pairRate: {}, order: {}" , counterOrderType , currentTime , offerAmount , pairCurrency , pairRate , orderBy ) ; DexOrderDBMatchingRequest dexOrderDBMatchingRequest = new DexOrderDBMatchingRequest ( counterOrderType , currentTime , 0 , offerAmount , pairCurrency . intValue ( ) , pairRate , orderBy ) ; List < DexOrder > orders = dexMatchingService . getOffersForMatching ( dexOrderDBMatchingRequest , orderBy ) ; log . debug ( "offers found: {}" , orders . size ( ) ) ; List < DexOrder > filteredOrders = orders . stream ( ) . filter ( order -> ! order . getAccountId ( ) . equals ( createdOrder . getAccountId ( ) ) ) . collect ( Collectors . toList ( ) ) ; code_block = ForStatement ; return null ; }
@ Override public DexOrder findCounterOffer ( DexOrder createdOrder ) { log . debug ( "DexMatcherServiceImpl:findCounterOffer()" ) ; OrderType counterOrderType = createdOrder . getType ( ) . isBuy ( ) ? OrderType . SELL : OrderType . BUY ; String orderBy = createdOrder . getType ( ) . isSell ( ) ? "DESC" : "ASC" ; Integer currentTime = timeService . getEpochTime ( ) ; BigDecimal offerAmount = new BigDecimal ( createdOrder . getOrderAmount ( ) ) ; Integer pairCurrency = DexCurrency . getValue ( createdOrder . getPairCurrency ( ) ) ; BigDecimal pairRate = new BigDecimal ( EthUtil . ethToGwei ( createdOrder . getPairRate ( ) ) ) ; log . debug ( "Dumping arguments: type: {}, currentTime: {}, offerAmount: {}, offerCurrency: {}, pairRate: {}, order: {}" , counterOrderType , currentTime , offerAmount , pairCurrency , pairRate , orderBy ) ; DexOrderDBMatchingRequest dexOrderDBMatchingRequest = new DexOrderDBMatchingRequest ( counterOrderType , currentTime , 0 , offerAmount , pairCurrency . intValue ( ) , pairRate , orderBy ) ; List < DexOrder > orders = dexMatchingService . getOffersForMatching ( dexOrderDBMatchingRequest , orderBy ) ; log . debug ( "offers found: {}" , orders . size ( ) ) ; List < DexOrder > filteredOrders = orders . stream ( ) . filter ( order -> ! order . getAccountId ( ) . equals ( createdOrder . getAccountId ( ) ) ) . collect ( Collectors . toList ( ) ) ; code_block = ForStatement ; return null ; }
@ Override public DexOrder findCounterOffer ( DexOrder createdOrder ) { log . debug ( "DexMatcherServiceImpl:findCounterOffer()" ) ; OrderType counterOrderType = createdOrder . getType ( ) . isBuy ( ) ? OrderType . SELL : OrderType . BUY ; String orderBy = createdOrder . getType ( ) . isSell ( ) ? "DESC" : "ASC" ; Integer currentTime = timeService . getEpochTime ( ) ; BigDecimal offerAmount = new BigDecimal ( createdOrder . getOrderAmount ( ) ) ; Integer pairCurrency = DexCurrency . getValue ( createdOrder . getPairCurrency ( ) ) ; BigDecimal pairRate = new BigDecimal ( EthUtil . ethToGwei ( createdOrder . getPairRate ( ) ) ) ; log . debug ( "Dumping arguments: type: {}, currentTime: {}, offerAmount: {}, offerCurrency: {}, pairRate: {}, order: {}" , counterOrderType , currentTime , offerAmount , pairCurrency , pairRate , orderBy ) ; DexOrderDBMatchingRequest dexOrderDBMatchingRequest = new DexOrderDBMatchingRequest ( counterOrderType , currentTime , 0 , offerAmount , pairCurrency . intValue ( ) , pairRate , orderBy ) ; List < DexOrder > orders = dexMatchingService . getOffersForMatching ( dexOrderDBMatchingRequest , orderBy ) ; log . debug ( "offers found: {}" , orders . size ( ) ) ; List < DexOrder > filteredOrders = orders . stream ( ) . filter ( order -> ! order . getAccountId ( ) . equals ( createdOrder . getAccountId ( ) ) ) . collect ( Collectors . toList ( ) ) ; code_block = ForStatement ; return null ; }
public void test() { try { code_block = IfStatement ; } catch ( Exception ex ) { log . debug ( "Validation error: {}" , ex . toString ( ) ) ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( threadName + ": starting" ) ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( threadName + ": stopped" ) ; } }
public void test() { try { ExecStatus . valueOf ( execution . getStatus ( ) ) ; } catch ( IllegalArgumentException e ) { LOG . error ( "Invalid execution status '" + execution . getStatus ( ) + '\'' , e ) ; isValid = false ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Failed to parse {}" , pager . requestedTimeRange , e ) ; } }
public void test() { try { application . stop ( ) ; } catch ( final Exception e ) { LOGGER . error ( e ) ; } }
public void test() { try { return ll . findIUS ( "/" + swAccession ) ; } catch ( IOException | JAXBException ex ) { Log . error ( ex ) ; } }
public void test() { { log . debug ( "getHistory:  fsym: {}, resolution: {}, to: {}, from: {}" , symbol , resolution , to , from ) ; code_block = IfStatement ; TradingDataOutput tradingDataOutput = tradingViewService . getUpdatedDataForIntervalFromOffers ( symbol , resolution , to , from ) ; return Response . ok ( new TradingDataOutputToDtoConverter ( ) . apply ( tradingDataOutput ) ) . build ( ) ; } }
public void test() { if ( to <= 1569369600 ) { log . debug ( "flushing: " ) ; TradingDataOutput tdo = new TradingDataOutput ( ) ; tdo . setC ( null ) ; tdo . setH ( null ) ; tdo . setL ( null ) ; tdo . setO ( null ) ; tdo . setT ( null ) ; tdo . setV ( null ) ; tdo . setNextTime ( null ) ; tdo . setS ( "no_data" ) ; return Response . ok ( converter . apply ( tdo ) ) . build ( ) ; } }
public void test() { try { int returnValue = CommerceWishListItemServiceUtil . getCommerceWishListItemByContainsCProductCount ( commerceWishListId , cProductId ) ; return returnValue ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try { return Resource . deserializeObject ( buffer ) ; } catch ( Exception e ) { LOGGER . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( Files . exists ( path ) && Files . isReadable ( path ) && ! Files . isDirectory ( path ) ) { LOGGER . info ( "Found " + type + " file: " + path . toAbsolutePath ( ) ) ; return Optional . of ( ConfigSources . file ( path ) . build ( ) ) ; } }
public void test() { try { getEntityManager ( ) . remove ( modelFamily ) ; } catch ( HibernateException he ) { logger . error ( "Hibernate exception on deleting ModelFamily using ID=" + modelFamilyID + he . getStackTrace ( ) ) ; return false ; } }
@ VisibleForTesting protected void scheduledTask ( ) { logger . info ( "[scheduledTask] start retrieving info" ) ; int totalCount , successCount , failCount ; List < EventModel > events = alertEventService . getLastHourAlertEvent ( ) ; totalCount = events . size ( ) ; Pair < Integer , Integer > successAndFail = statistics ( events ) ; successCount = successAndFail . getKey ( ) ; failCount = successAndFail . getValue ( ) ; logger . info ( "[scheduledTask] scheduled report, total email count: {}" , totalCount ) ; EventMonitor . DEFAULT . logEvent ( EMAIL_SERVICE_CAT_TYPE , "total sent out" , totalCount ) ; EventMonitor . DEFAULT . logEvent ( EMAIL_SERVICE_CAT_TYPE , "success sent out" , successCount ) ; EventMonitor . DEFAULT . logEvent ( EMAIL_SERVICE_CAT_TYPE , "fail sent out" , failCount ) ; }
@ VisibleForTesting protected void scheduledTask ( ) { logger . info ( "[scheduledTask] start retrieving info" ) ; int totalCount , successCount , failCount ; List < EventModel > events = alertEventService . getLastHourAlertEvent ( ) ; totalCount = events . size ( ) ; Pair < Integer , Integer > successAndFail = statistics ( events ) ; successCount = successAndFail . getKey ( ) ; failCount = successAndFail . getValue ( ) ; logger . info ( "[scheduledTask] scheduled report, total email count: {}" , totalCount ) ; EventMonitor . DEFAULT . logEvent ( EMAIL_SERVICE_CAT_TYPE , "total sent out" , totalCount ) ; EventMonitor . DEFAULT . logEvent ( EMAIL_SERVICE_CAT_TYPE , "success sent out" , successCount ) ; EventMonitor . DEFAULT . logEvent ( EMAIL_SERVICE_CAT_TYPE , "fail sent out" , failCount ) ; }
public void test() { if ( ! destination . exists ( ) || isDifferent ( bytes ) ) { code_block = TryStatement ;  log . info ( destination + " created" ) ; } else { log . info ( destination + " didn't change, skip rewriting" ) ; } }
public void test() { if ( ! destination . exists ( ) || isDifferent ( bytes ) ) { code_block = TryStatement ;  log . info ( destination + " created" ) ; } else { log . info ( destination + " didn't change, skip rewriting" ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Removing " + localJobFile + " and " + localJarFile + " getJobFile = " + profile . getJobFile ( ) ) ; } }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; FileSystem fs = FileSystem . get ( conf ) ; fs . delete ( new Path ( profile . getJobFile ( ) ) . getParent ( ) , true ) ; } catch ( IOException e ) { LOG . info ( "Error cleaning up " + profile . getJobID ( ) + ": " + e ) ; } }
public void test() { if ( t instanceof GemFireSecurityException ) { securityLogger . error ( message , t ) ; } else { logger . error ( message , t ) ; } }
public void test() { if ( t instanceof GemFireSecurityException ) { securityLogger . error ( message , t ) ; } else { logger . error ( message , t ) ; } }
@ Test public void markers ( ) { Logger log = LogManager . getLogger ( Log4j2NativeApiTest . class ) ; Marker m1 = MarkerManager . getMarker ( "m1" ) ; m1 . addParents ( MarkerManager . getMarker ( "p1" ) , MarkerManager . getMarker ( "p2" ) ) ; log . info ( m1 , "markers - INFO" ) ; }
public void test() { { echo 'yes' }" . getBytes ( "UTF-8" ) ; GHContentUpdateResponse updateResponse = helper . getGithubRepository ( ) . createContent ( content , "Jenkinsfile" , "Jenkinsfile" , "main" ) ; helper . getGithubRepository ( ) . createRef ( "refs/heads/branch1" , updateResponse . getCommit ( ) . getSHA1 ( ) ) ; logger . info ( "Created master and branch1 branches in " + helper . getGithubRepository ( ) . getFullName ( ) ) ; helper . getGithubRepository ( ) . createContent ( "hi there" , "newfile" , "newfile" , "branch1" ) ; creationPage . createPipeline ( helper . getAccessToken ( ) , helper . getOrganizationOrUsername ( ) , helper . getActualRepositoryName ( ) ) ; } }
private List < OmObservation > querySeriesObservation ( GetObservationRequest request , Session session ) throws OwsExceptionReport , ConverterException { code_block = IfStatement ; Locale requestedLocale = getRequestedLocale ( request ) ; String pdf = getProcedureDescriptionFormat ( request . getResponseFormat ( ) ) ; final long start = System . currentTimeMillis ( ) ; List < String > features = request . getFeatureIdentifiers ( ) ; Collection < DataEntity < ? > > seriesObservations = Lists . newArrayList ( ) ; AbstractSeriesDAO seriesDAO = daoFactory . getSeriesDAO ( ) ; code_block = ForStatement ; final List < OmObservation > result = new LinkedList < > ( ) ; code_block = IfStatement ; LOGGER . debug ( LOG_TIME_TO_QUERY , System . currentTimeMillis ( ) - start ) ; toSosObservation ( new ArrayList < > ( seriesObservations ) , request , requestedLocale , pdf , observationCreatorContext , session ) . forEachRemaining ( result :: add ) ; return result ; }
public void test() { try { List < EnvironmentLogic > del = new ArrayList < > ( environments ) ; code_block = ForStatement ; } catch ( Exception e ) { LOG . error ( Freedomotic . getStackTraceInfo ( e ) ) ; } finally { environments . clear ( ) ; } }
public void test() { try { Bootstrap b = new Bootstrap ( ) ; b . group ( group ) . channel ( NioDatagramChannel . class ) . option ( ChannelOption . SO_BROADCAST , false ) . handler ( clientInitializer ) ; b . connect ( host , port ) . sync ( ) ; synchronized ( scenarioHandler ) code_block = "" ; } catch ( Exception ex ) { LOG . error ( ex . getMessage ( ) , ex ) ; } finally { LOG . debug ( "shutting down" ) ; code_block = TryStatement ;  } }
public void test() { try { group . shutdownGracefully ( ) . get ( ) ; LOG . debug ( "shutdown succesful" ) ; } catch ( InterruptedException | ExecutionException e ) { LOG . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { code_block = IfStatement ; HelixCustomCodeRunner customCodeRunner = new HelixCustomCodeRunner ( manager , ZK_ADDR ) ; customCodeRunner . invoke ( _callback ) . on ( ChangeType . LIVE_INSTANCE ) . usingLeaderStandbyModel ( "TestParticLeader" ) . start ( ) ; } catch ( Exception e ) { LOG . error ( "Exception do pre-connect job" , e ) ; } }
public void test() { { log . info ( Color . GREEN + "Shape_7 : invalid column shape_dist_traveled" + Color . NORMAL ) ; Context context = new Context ( ) ; CheckPointReport result = verifyValidation ( log , context , "shape_7" , GTFS_1_GTFS_Common_16 , SEVERITY . ERROR , RESULT . NOK , true ) ; Assert . assertEquals ( result . getCheckPointErrorCount ( ) , 1 , "detail count" ) ; code_block = ForStatement ; } }
public void test() { if ( IS_DEBUG ) { LOG . debug ( "Checksum encoding : {}" , Strings . dumpBytes ( buffer . array ( ) ) ) ; LOG . debug ( "Checksum initial value : {}" , this ) ; } }
public void test() { try { PortalServiceUtil . testAutoSyncHibernateSessionStateOnTxCreation ( ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( startTime == 0 ) { log . error ( "setEndTime must be called after setStartTime" , new Throwable ( INVALID_CALL_SEQUENCE_MSG ) ) ; } else { elapsedTime = endTime - startTime - idleTime ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Parsing json array : {}" , value ) ; } }
public void test() { try { o = clazz . newInstance ( ) ; return o ; } catch ( InstantiationException ie ) { log . warn ( "Instantiation exception,caused by :" + ie . getMessage ( ) ) ; return null ; } catch ( IllegalAccessException iae ) { log . warn ( "Illegal access exception,caused by :" + iae . getMessage ( ) ) ; return null ; } }
public void test() { try { o = clazz . newInstance ( ) ; return o ; } catch ( InstantiationException ie ) { log . warn ( "Instantiation exception,caused by :" + ie . getMessage ( ) ) ; return null ; } catch ( IllegalAccessException iae ) { log . warn ( "Illegal access exception,caused by :" + iae . getMessage ( ) ) ; return null ; } }
protected void configureChannels ( ) { Channel channel ; ChannelTypeUID channelTypeUID ; ChannelUID channelUID ; logger . debug ( "Configuring channels for keypad {}" , integrationId ) ; List < Channel > channelList = new ArrayList < > ( ) ; List < Channel > existingChannels = getThing ( ) . getChannels ( ) ; code_block = IfStatement ; ThingBuilder thingBuilder = editThing ( ) ; code_block = ForStatement ; code_block = ForStatement ; code_block = ForStatement ; thingBuilder . withChannels ( channelList ) ; updateThing ( thingBuilder . build ( ) ) ; logger . debug ( "Done configuring channels for keypad {}" , integrationId ) ; }
public void test() { try { LOG . debug ( "Policy Check Succeeded" ) ; getDocSubmissionUtils ( ) . convertDataToFileLocationIfEnabled ( body ) ; response = sendToAdapter ( body , assertion ) ; } catch ( LargePayloadException lpe ) { LOG . error ( "Failed to retrieve payload document. " + lpe . getLocalizedMessage ( ) , lpe ) ; response = MessageGeneratorUtils . getInstance ( ) . createXDRAckWithRegistryErrorResponse ( ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Closing handles for LedgerRange: {}" , range ) ; } }
protected ProjectOverviewDTO getProjectOverview ( UIContext uiContext ) { StopWatch watch = new StopWatch ( ) ; ProjectOverviewDTO projectOverview = new ProjectOverviewDTO ( ) ; File rootFolder = getSelectionFolder ( uiContext ) ; code_block = IfStatement ; log . info ( "getProjectOverview took " + watch . taken ( ) ) ; return projectOverview ; }
public void test() { { LOGGER . info ( "Retrieving next {} accounts." , NUMBER_OF_RESULTS ) ; managedCustomerPage = managedCustomerService . get ( selector ) ; addClientCustomerIds ( managedCustomerPage , clientCustomerIdsSet ) ; LOGGER . info ( "{} accounts retrieved." , clientCustomerIdsSet . size ( ) ) ; offset += NUMBER_OF_RESULTS ; selector = builder . increaseOffsetBy ( NUMBER_OF_RESULTS ) . build ( ) ; } }
public void test() { try { publish . payload ( delivery . message ( ) . getBodyAs ( Buffer . class ) ) ; } catch ( FilterException e ) { log . error ( e , "Internal Server Error: Could not covert message body to a Buffer" ) ; } }
private static List < URI > resolve ( final String srvName , final String protocol , final String domain , final DnsSrvResolver resolver ) { final String name ; code_block = SwitchStatement ; final List < LookupResult > lookupResults = resolver . resolve ( name ) ; final ImmutableList . Builder < URI > endpoints = ImmutableList . builder ( ) ; code_block = ForStatement ; final ImmutableList < URI > uris = endpoints . build ( ) ; log . info ( "Resolved {} to {}" , name , uris ) ; return uris ; }
public void test() { if ( ! reallyEnded ) { LOG . warn ( "Persistence tasks took too long to terminate, when stopping persistence, although pending changes were persisted (ignoring): " + scheduledTask ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { if ( e instanceof org . apache . accumulo . core . clientImpl . thrift . ThriftNotActiveServiceException ) { result . tnase = ( org . apache . accumulo . core . clientImpl . thrift . ThriftNotActiveServiceException ) e ; result . setTnaseIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof org . apache . accumulo . core . clientImpl . thrift . ThriftNotActiveServiceException ) { result . tnase = ( org . apache . accumulo . core . clientImpl . thrift . ThriftNotActiveServiceException ) e ; result . setTnaseIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof org . apache . accumulo . core . clientImpl . thrift . ThriftNotActiveServiceException ) { result . tnase = ( org . apache . accumulo . core . clientImpl . thrift . ThriftNotActiveServiceException ) e ; result . setTnaseIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { try { fcall . sendResponse ( fb , msg , msgType , seqid ) ; } catch ( java . lang . Exception ex ) { _LOGGER . error ( "Exception writing to internal frame buffer" , ex ) ; fb . close ( ) ; } }
public void test() { try { Optional . ofNullable ( descriptor . getCharset ( ) ) . map ( Charset :: forName ) . ifPresent ( currentlyBuildMimePart :: charset ) ; } catch ( Exception e ) { LOGGER . info ( "Failed parsing charset" , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "DistTXPrecommitMessage.operateOnTx: Tx {} with Secondaries List {}" , txId , this . secondaryTransactionalOperations ) ; } }
public void test() { if ( ! txStateProxy . isDistTx ( ) || ! txStateProxy . isTxStateProxy ( ) || txStateProxy . isCreatedOnDistTxCoordinator ( ) ) { throw new UnsupportedOperationInTransactionException ( String . format ( "Expected %s during a distributed transaction but got %s" , "DistTXStateProxyImplOnDatanode" , txStateProxy . getClass ( ) . getSimpleName ( ) ) ) ; } }
public void test() { if ( ! txStateProxy . isDistTx ( ) || ! txStateProxy . isTxStateProxy ( ) || txStateProxy . isCreatedOnDistTxCoordinator ( ) ) { throw new UnsupportedOperationInTransactionException ( String . format ( "Expected %s during a distributed transaction but got %s" , "DistTXStateProxyImplOnDatanode" , txStateProxy . getClass ( ) . getSimpleName ( ) ) ) ; } }
public void test() { if ( result . isRight ( ) ) { log . error ( EcompLoggerErrorCode . DATA_ERROR , null , "GetToscaElement" , result . right ( ) . value ( ) . name ( ) + "for component with id {}" , componentId ) ; continue ; } }
public void test() { if ( ! accepted ) { _log . error ( "Local alert buffer full!  Clearing!  Dropping " + alert . getId ( ) + " record" ) ; } }
@ Override public void onPing ( String ping ) { log . info ( "Server: {} - onPing ping={}" , server , ping ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "{}: Send heartbeat to {} followers" , memberName , nodes . size ( ) - 1 ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { LOGGER . debug ( "Failed to load the file [" + configurationLocation + "] using direct " + "file access. The error was [" + e . getMessage ( ) + "]. Trying to load it " + "as a resource using the Servlet Context..." ) ; } }
public void test() { if ( context != null ) { InputStream xwikicfgis = context . getResourceAsStream ( configurationLocation ) ; LOGGER . debug ( "Failed to load the file [" + configurationLocation + "] as a resource " + "using the Servlet Context. Trying to load it as classpath resource..." ) ; code_block = IfStatement ; } else { LOGGER . debug ( "No Servlet Context available. Trying to load it as classpath resource..." ) ; } }
public void test() { try { InputStream stdout = process . getInputStream ( ) ; synchronized ( stdout ) code_block = "" ; } catch ( IOException e ) { LOG . warn ( String . format ( "Error while closing the input stream of process %s: %s" , process , e . getMessage ( ) ) ) ; } }
@ Override public void deleteAVUMetadata ( final String resourceName , final AvuData avuData ) throws InvalidResourceException , JargonException { code_block = IfStatement ; code_block = IfStatement ; log . info ( "delete avu metadata from resource: {}" , resourceName ) ; log . info ( "avu: {}" , avuData ) ; final ModAvuMetadataInp modifyAvuMetadataInp = ModAvuMetadataInp . instanceForDeleteResourceMetadata ( resourceName , avuData ) ; log . debug ( "sending avu request" ) ; code_block = TryStatement ;  log . debug ( "metadata removed" ) ; }
@ Override public void deleteAVUMetadata ( final String resourceName , final AvuData avuData ) throws InvalidResourceException , JargonException { code_block = IfStatement ; code_block = IfStatement ; log . info ( "delete avu metadata from resource: {}" , resourceName ) ; log . info ( "avu: {}" , avuData ) ; final ModAvuMetadataInp modifyAvuMetadataInp = ModAvuMetadataInp . instanceForDeleteResourceMetadata ( resourceName , avuData ) ; log . debug ( "sending avu request" ) ; code_block = TryStatement ;  log . debug ( "metadata removed" ) ; }
@ Override public void deleteAVUMetadata ( final String resourceName , final AvuData avuData ) throws InvalidResourceException , JargonException { code_block = IfStatement ; code_block = IfStatement ; log . info ( "delete avu metadata from resource: {}" , resourceName ) ; log . info ( "avu: {}" , avuData ) ; final ModAvuMetadataInp modifyAvuMetadataInp = ModAvuMetadataInp . instanceForDeleteResourceMetadata ( resourceName , avuData ) ; log . debug ( "sending avu request" ) ; code_block = TryStatement ;  log . debug ( "metadata removed" ) ; }
public void test() { try { getIRODSProtocol ( ) . irodsFunction ( modifyAvuMetadataInp ) ; } catch ( JargonException je ) { code_block = IfStatement ; log . error ( "jargon exception removing AVU metadata" , je ) ; throw je ; } }
@ Override public void deleteAVUMetadata ( final String resourceName , final AvuData avuData ) throws InvalidResourceException , JargonException { code_block = IfStatement ; code_block = IfStatement ; log . info ( "delete avu metadata from resource: {}" , resourceName ) ; log . info ( "avu: {}" , avuData ) ; final ModAvuMetadataInp modifyAvuMetadataInp = ModAvuMetadataInp . instanceForDeleteResourceMetadata ( resourceName , avuData ) ; log . debug ( "sending avu request" ) ; code_block = TryStatement ;  log . debug ( "metadata removed" ) ; }
@ Override public void warn ( String string , Object o ) { logger . warn ( string , o ) ; }
@ Override public Response toResponse ( ContextInferrenceFailedException exception ) { LOG . warn ( "Failed Context Inferrence" ) ; Object entity = Collections . EMPTY_LIST ; code_block = IfStatement ; return Response . status ( Status . NOT_FOUND ) . entity ( entity ) . header ( "TotalCount" , 0 ) . build ( ) ; }
@ Override public void initialize ( ) { final DSMRDeviceConfiguration deviceConfig = getConfigAs ( DSMRDeviceConfiguration . class ) ; code_block = IfStatement ; logger . trace ( "Using configuration {}" , deviceConfig ) ; updateStatus ( ThingStatus . UNKNOWN ) ; receivedTimeoutNanos = TimeUnit . SECONDS . toNanos ( deviceConfig . receivedTimeout ) ; final DSMRDevice dsmrDevice = createDevice ( deviceConfig ) ; resetLastReceivedState ( ) ; this . dsmrDevice = dsmrDevice ; dsmrDeviceRunnable = new DSMRDeviceRunnable ( dsmrDevice , this ) ; dsmrDeviceThread = new Thread ( dsmrDeviceRunnable ) ; dsmrDeviceThread . setName ( "OH-binding-" + getThing ( ) . getUID ( ) ) ; dsmrDeviceThread . setDaemon ( true ) ; dsmrDeviceThread . start ( ) ; watchdog = scheduler . scheduleWithFixedDelay ( this :: alive , receivedTimeoutNanos , receivedTimeoutNanos , TimeUnit . NANOSECONDS ) ; }
public void test() { try { code_block = IfStatement ; } catch ( InvocationTargetException e ) { logger . error ( "Recursive problem with weighting model named: " + name , e ) ; } catch ( Exception e ) { logger . error ( "Problem with weighting model named: " + name , e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( InvocationTargetException e ) { logger . error ( "Recursive problem with weighting model named: " + name , e ) ; } catch ( Exception e ) { logger . error ( "Problem with weighting model named: " + name , e ) ; } }
@ Override public Void call ( ) throws Exception { code_block = IfStatement ; ManagedTask task = tasks . get ( id ) ; code_block = IfStatement ; TaskController controller = null ; String failure = null ; code_block = TryStatement ;  code_block = IfStatement ; task = new ManagedTask ( id , originalSpec , spec , controller , TaskStateType . PENDING ) ; tasks . put ( id , task ) ; long delayMs = task . startDelayMs ( time . milliseconds ( ) ) ; task . startFuture = scheduler . schedule ( executor , new RunTask ( task ) , delayMs ) ; log . info ( "Created a new task {} with spec {}, scheduled to start {} ms from now." , id , spec , delayMs ) ; return null ; }
public void test() { try { federationBatchInfoService . applyRetentionPolicy ( retentionDays ) ; logger . debug ( "Retention policy applied successfully." ) ; } catch ( Exception e ) { logger . error ( "Application of retention policy failed." , e ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { try { return objMapper . writeValueAsString ( tuple ) ; } catch ( JsonProcessingException e ) { logger . error ( "Error while converting tuple {} {}" , tuple , e ) ; } }
public void test() { try { adminMgr . deassignUser ( userRole ) ; } catch ( SecurityException se ) { LOG . warn ( "delUserRoles tenant={} userId={} roleName={} caught SecurityException={}" , getTenant ( ) , userRole . getUserId ( ) , userRole . getName ( ) , se ) ; } }
public void test() { if ( currentElement == null ) { LOG . error ( "currentElement == null" ) ; } else { pendingAttrs . add ( new PendingAttr ( attr , path , conf ) ) ; } }
public void test() { try { StringWriter writer = new StringWriter ( ) ; mapper . writeValue ( writer , answer ) ; return writer . toString ( ) ; } catch ( IOException e ) { LOG . warn ( "Failed to marshal the events: " + e , e ) ; throw new IOException ( e . getMessage ( ) ) ; } }
public void test() { if ( isDebugEnabled ) { logger . debug ( "{}: Conflating {} at queue index={} queue size={} head={} tail={}" , this , object , tailKey , size ( ) , this . headKey , tailKey ) ; } }
public void test() { if ( latestIndexesForRegion == null ) { latestIndexesForRegion = new HashMap < > ( ) ; this . indexes . put ( rName , latestIndexesForRegion ) ; } }
public void test() { if ( latestIndexesForRegion == null ) { latestIndexesForRegion = new HashMap < > ( ) ; this . indexes . put ( rName , latestIndexesForRegion ) ; } }
public void test() { if ( latestIndexesForRegion == null ) { latestIndexesForRegion = new HashMap < > ( ) ; this . indexes . put ( rName , latestIndexesForRegion ) ; } }
public void test() { if ( latestIndexesForRegion == null ) { latestIndexesForRegion = new HashMap < > ( ) ; this . indexes . put ( rName , latestIndexesForRegion ) ; } }
public void test() { if ( isDebugEnabled ) { logger . debug ( "{}: Not conflating {} queue size: {} head={} tail={}" , this , object , size ( ) , this . headKey , tailKey ) ; } }
public void test() { try { createResponse = provider . create ( createRequest ) ; createdMetacards = createResponse . getCreatedMetacards ( ) ; } catch ( IngestException e ) { printErrorMessage ( String . format ( "Received error while ingesting: %s%n" , e . getMessage ( ) ) ) ; LOGGER . debug ( "Error during ingest. Attempting to ingest batch individually." ) ; return ingestSingly ( provider , metacards ) ; } catch ( SourceUnavailableException e ) { printErrorMessage ( String . format ( "Received error while ingesting: %s%n" , e . getMessage ( ) ) ) ; LOGGER . debug ( "Error during ingest:" , e ) ; return createdMetacards ; } catch ( Exception e ) { printErrorMessage ( String . format ( "Unexpected Exception received while ingesting: %s%n" , e . getMessage ( ) ) ) ; LOGGER . debug ( "Unexpected Exception during ingest:" , e ) ; return createdMetacards ; } }
public void test() { try { createResponse = provider . create ( createRequest ) ; createdMetacards = createResponse . getCreatedMetacards ( ) ; } catch ( IngestException e ) { printErrorMessage ( String . format ( "Received error while ingesting: %s%n" , e . getMessage ( ) ) ) ; LOGGER . debug ( "Error during ingest. Attempting to ingest batch individually." ) ; return ingestSingly ( provider , metacards ) ; } catch ( SourceUnavailableException e ) { printErrorMessage ( String . format ( "Received error while ingesting: %s%n" , e . getMessage ( ) ) ) ; LOGGER . debug ( "Error during ingest:" , e ) ; return createdMetacards ; } catch ( Exception e ) { printErrorMessage ( String . format ( "Unexpected Exception received while ingesting: %s%n" , e . getMessage ( ) ) ) ; LOGGER . debug ( "Unexpected Exception during ingest:" , e ) ; return createdMetacards ; } }
public void test() { try { createResponse = provider . create ( createRequest ) ; createdMetacards = createResponse . getCreatedMetacards ( ) ; } catch ( IngestException e ) { printErrorMessage ( String . format ( "Received error while ingesting: %s%n" , e . getMessage ( ) ) ) ; LOGGER . debug ( "Error during ingest. Attempting to ingest batch individually." ) ; return ingestSingly ( provider , metacards ) ; } catch ( SourceUnavailableException e ) { printErrorMessage ( String . format ( "Received error while ingesting: %s%n" , e . getMessage ( ) ) ) ; LOGGER . debug ( "Error during ingest:" , e ) ; return createdMetacards ; } catch ( Exception e ) { printErrorMessage ( String . format ( "Unexpected Exception received while ingesting: %s%n" , e . getMessage ( ) ) ) ; LOGGER . debug ( "Unexpected Exception during ingest:" , e ) ; return createdMetacards ; } }
public void test() { try { log . info ( "Trying to resend the add command [" + adaptorName + "][" + offset + "][" + params + "] [" + numRetries + "]" ) ; addByName ( null , adaptorName , type , params , offset , numRetries , retryInterval ) ; } catch ( Exception e ) { log . warn ( "Exception in AddAdaptorTask.run" , e ) ; e . printStackTrace ( ) ; } }
public void test() { try { log . info ( "Trying to resend the add command [" + adaptorName + "][" + offset + "][" + params + "] [" + numRetries + "]" ) ; addByName ( null , adaptorName , type , params , offset , numRetries , retryInterval ) ; } catch ( Exception e ) { log . warn ( "Exception in AddAdaptorTask.run" , e ) ; e . printStackTrace ( ) ; } }
public void test() { if ( s_logger . isInfoEnabled ( ) ) { s_logger . info ( "Executing resource PingTestCommand: " + _gson . toJson ( cmd ) ) ; } }
public void test() { try { Pair < Boolean , String > result = SshHelper . sshExecute ( controlIp , DefaultDomRSshPort , "root" , getSystemVmKeyFile ( ) , null , "/bin/ping" + args ) ; if ( result . first ( ) ) return new Answer ( cmd ) ; } catch ( Exception e ) { s_logger . error ( "Unable to execute ping command on DomR (" + controlIp + "), domR may not be ready yet. failure due to " + VmwareHelper . getExceptionMessage ( e ) , e ) ; } }
public void test() { try { HostMO hostMo = ( HostMO ) hyperHost ; ClusterMO clusterMo = new ClusterMO ( context , hostMo . getHyperHostCluster ( ) ) ; VmwareManager mgr = context . getStockObject ( VmwareManager . CONTEXT_STOCK_NAME ) ; List < Pair < ManagedObjectReference , String > > hosts = clusterMo . getClusterHosts ( ) ; code_block = ForStatement ; } catch ( Exception e ) { s_logger . error ( "Unable to execute ping command on host (" + cmd . getComputingHostIp ( ) + "). failure due to " + VmwareHelper . getExceptionMessage ( e ) , e ) ; } }
@ Override public void reduce ( Text key , Iterable < BytesWritable > values , Reducer < Text , BytesWritable , Text , Text > . Context context ) throws IOException , InterruptedException { log . info ( "starting reduce, key: " + key . toString ( ) ) ; long startTime = new Date ( ) . getTime ( ) ; Configuration conf = context . getConfiguration ( ) ; initialMaxDocsSetSize = conf . getInt ( "INITIAL_MAX_DOCS_SET_SIZE" , initialMaxDocsSetSize ) ; process ( key , context , values , initialMaxDocsSetSize ) ; log . info ( "time [msec]: " + ( new Date ( ) . getTime ( ) - startTime ) ) ; }
@ Override public void reduce ( Text key , Iterable < BytesWritable > values , Reducer < Text , BytesWritable , Text , Text > . Context context ) throws IOException , InterruptedException { log . info ( "starting reduce, key: " + key . toString ( ) ) ; long startTime = new Date ( ) . getTime ( ) ; Configuration conf = context . getConfiguration ( ) ; initialMaxDocsSetSize = conf . getInt ( "INITIAL_MAX_DOCS_SET_SIZE" , initialMaxDocsSetSize ) ; process ( key , context , values , initialMaxDocsSetSize ) ; log . info ( "time [msec]: " + ( new Date ( ) . getTime ( ) - startTime ) ) ; }
public void test() { switch ( reusableLog . getLogType ( ) ) { case LogType . UPDATE : case LogType . ENTITY_COMMIT : case LogType . FILTER : logManager . log ( reusableLog ) ; break ; case LogType . JOB_COMMIT : case LogType . ABORT : RemoteLogRecord jobTerminationLog = new RemoteLogRecord ( ) ; TransactionUtil . formJobTerminateLogRecord ( jobTerminationLog , reusableLog . getTxnId ( ) , reusableLog . getLogType ( ) == LogType . JOB_COMMIT ) ; jobTerminationLog . setRequester ( this ) ; jobTerminationLog . setReplicationWorker ( worker ) ; jobTerminationLog . setLogSource ( LogSource . REMOTE ) ; logManager . log ( jobTerminationLog ) ; break ; case LogType . FLUSH : RemoteLogRecord flushLog = new RemoteLogRecord ( ) ; TransactionUtil . formFlushLogRecord ( flushLog , reusableLog . getDatasetId ( ) , reusableLog . getResourcePartition ( ) , reusableLog . getFlushingComponentMinId ( ) , reusableLog . getFlushingComponentMaxId ( ) , null ) ; flushLog . setRequester ( this ) ; flushLog . setLogSource ( LogSource . REMOTE ) ; flushLog . setMasterLsn ( reusableLog . getLSN ( ) ) ; logManager . log ( flushLog ) ; break ; default : LOGGER . error ( ( ) -> "Unsupported LogType: " + reusableLog . getLogType ( ) ) ; } }
public void test() { if ( timestamp > 0 ) { LOG . debug ( "Grok parser validated message: {}" , message ) ; return true ; } }
public void test() { try { code_block = IfStatement ; } catch ( IOException ex ) { LOGGER . error ( "Error writing HTTP result" , ex ) ; httpResponse . setStatus ( 500 ) ; } }
public void test() { try { callback . onTick ( System . currentTimeMillis ( ) ) ; } catch ( Exception e ) { LOG . error ( "Timer thread caught, ignored an exception" , e ) ; } }
public void test() { try { ObjectNode node = MAPPER . createObjectNode ( ) ; node . put ( MESSAGE_ATTRIBUTE , body . toString ( ) ) ; String newBody = MAPPER . writerWithDefaultPrettyPrinter ( ) . writeValueAsString ( node ) ; exchange . getIn ( ) . setBody ( newBody ) ; LOGGER . debug ( "New Exchange In Body: {}" , newBody ) ; } catch ( Exception ex ) { throw new IllegalStateException ( "Failed to post-process the TemplateStep's message into JSON" , ex ) ; } }
public void test() { try { SearchTemplateResponse response = getClient ( ) . searchTemplate ( searchTemplateRequest , RequestOptions . DEFAULT ) ; SearchResponse searchResponse = response . getResponse ( ) ; List terms = ( ( ParsedTerms ) searchResponse . getAggregations ( ) . asMap ( ) . get ( "client_versions" ) ) . getBuckets ( ) ; List < String > results = ( List < String > ) terms . stream ( ) . map ( o -> ( ( ParsedTerms . ParsedBucket ) o ) . getKey ( ) ) . collect ( Collectors . toList ( ) ) ; handler . handle ( AsyncResultImpl . create ( results ) ) ; } catch ( IOException e ) { LOGGER . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { log . trace ( "Invoking synchronization.onAfterRoute: {} with {}" , synchronization , exchange ) ; ( ( SynchronizationRouteAware ) synchronization ) . onAfterRoute ( route , exchange ) ; } catch ( Throwable e ) { log . warn ( "Exception occurred during onAfterRoute. This exception will be ignored." , e ) ; } }
public void test() { try { keyspace . prepareColumnMutation ( CassandraModel . CF_METRICS_METADATA , locator , metaKey ) . putValue ( metaValue , StringMetadataSerializer . get ( ) , null ) . execute ( ) ; } catch ( ConnectionException e ) { Instrumentation . markWriteError ( e ) ; log . error ( "Error writing Metadata Value" , e ) ; throw e ; } finally { ctx . stop ( ) ; } }
public void test() { if ( logNum == 0 && t != null ) { LOG . error ( "" , t ) ; } }
public void test() { if ( maxLogNum . intValue ( ) < 0 || currentLogNum . intValue ( ) < maxLogNum . intValue ( ) ) { LOG . error ( "èæ°æ®: {}%n" , this . formatDirty ( dirtyRecord , t , errorMessage ) ) ; } }
public void test() { try { String request = params . getMandatoryString ( "request" ) ; dispatchWmsRequest ( request , params , httpServletRequest , httpServletResponse , catalogue ) ; } catch ( EdalException wmse ) { boolean v130 ; code_block = TryStatement ;  handleWmsException ( wmse , httpServletResponse , v130 ) ; } catch ( SocketException se ) { } catch ( IOException ioe ) { code_block = IfStatement ; throw ioe ; } catch ( Exception e ) { log . error ( "Problem with GET request" , e ) ; e . printStackTrace ( ) ; throw new IOException ( e ) ; } }
@ Override public SchemaIdVersion addSchemaVersion ( SchemaMetadata schemaMetadata , SchemaVersion schemaVersion , boolean disableCanonicalCheck ) throws InvalidSchemaException , IncompatibleSchemaException , SchemaNotFoundException , SchemaBranchNotFoundException { LOG . info ( "---------------1 addSchemaVersion {} {}" , schemaMetadata , schemaVersion ) ; return schemaVersionLifecycleManager . addSchemaVersion ( SchemaBranch . MASTER_BRANCH , schemaMetadata , schemaVersion , x -> registerSchemaMetadata ( x ) , disableCanonicalCheck ) ; }
public void test() { try { LOG . info ( "Attempting to declare TX:[{}]" , txId ) ; coordinator . declare ( txId , request ) ; } catch ( Exception e ) { request . onFailure ( e ) ; } }
public void test() { for ( final EndpointDescription desc : availableEndpoints ) { logger . debug ( "{}: {}" , message , desc . getEndpointUrl ( ) ) ; } }
public void test() { for ( final EndpointDescription desc : availableEndpoints ) { logger . trace ( "{}: {}" , message , desc ) ; } }
public void test() { if ( matcher . find ( ) ) { String csrfTokenString = matcher . group ( 1 ) ; logger . debug ( "CSRF token from login html: {}" , csrfTokenString ) ; return new DefaultCsrfToken ( CSRF_HEADER_NAME , CSRF_PARAM_NAME , csrfTokenString ) ; } else { throw new SessionAuthenticationException ( "Could not find CSRF_TOKEN variable on login page" ) ; } }
private void connectionUnreachable ( String webSocketServerUrl ) { log . warn ( "WebSocket server is unreachable. Possible VPN/Network issues, will retry in: " + reconnectDelay + " milliseconds." ) ; retryConnection ( webSocketServerUrl ) ; }
public void test() { if ( activeThrottles . isEmpty ( ) ) { log . warn ( "CryptoTransfer has no throttle buckets, fee multiplier will remain at one!" ) ; } }
public void test() { if ( this . ldapContextFactory == null ) { LOGGER . debug ( "No LdapContextFactory specified - creating a default instance." ) ; DefaultLdapContextFactory defaultFactory = new DefaultLdapContextFactory ( ) ; defaultFactory . setPrincipalSuffix ( this . principalSuffix ) ; defaultFactory . setSearchBase ( this . searchBase ) ; defaultFactory . setUrl ( this . url ) ; defaultFactory . setSystemUsername ( this . systemUsername ) ; defaultFactory . setSystemPassword ( getSystemPassword ( ) ) ; this . ldapContextFactory = defaultFactory ; } }
public void test() { try { Context ctx = cache . getJNDIContext ( ) ; ds = ( DataSource ) ctx . lookup ( "java:/XAPooledDataSource" ) ; } catch ( NamingException e ) { logger . debug ( "Naming Exception caught in lookup: " + e ) ; fail ( "failed in naming lookup: " , e ) ; return ; } catch ( Exception e ) { logger . debug ( "Exception caught during naming lookup: " + e ) ; fail ( "failed in naming lookup: " , e ) ; return ; } }
public void test() { try { Context ctx = cache . getJNDIContext ( ) ; ds = ( DataSource ) ctx . lookup ( "java:/XAPooledDataSource" ) ; } catch ( NamingException e ) { logger . debug ( "Naming Exception caught in lookup: " + e ) ; fail ( "failed in naming lookup: " , e ) ; return ; } catch ( Exception e ) { logger . debug ( "Exception caught during naming lookup: " + e ) ; fail ( "failed in naming lookup: " , e ) ; return ; } }
public void test() { try { code_block = ForStatement ; } catch ( SQLException e ) { logger . debug ( "Success SQLException caught in runTest1: " + e ) ; fail ( "runTest1 SQL Exception caught: " , e ) ; } catch ( Exception e ) { logger . debug ( "Exception caught in runTest1: " + e ) ; fail ( "Exception caught in runTest1: " , e ) ; e . printStackTrace ( ) ; } }
public void test() { try { code_block = ForStatement ; } catch ( SQLException e ) { logger . debug ( "Success SQLException caught in runTest1: " + e ) ; fail ( "runTest1 SQL Exception caught: " , e ) ; } catch ( Exception e ) { logger . debug ( "Exception caught in runTest1: " + e ) ; fail ( "Exception caught in runTest1: " , e ) ; e . printStackTrace ( ) ; } }
public void test() { if ( subscriber . isUnsubscribed ( ) ) { log . debug ( "unsubscribed" ) ; } else { log . debug ( "onCompleted" ) ; subscriber . onCompleted ( ) ; } }
public void test() { if ( subscriber . isUnsubscribed ( ) ) { log . debug ( "unsubscribed" ) ; } else { log . debug ( "onCompleted" ) ; subscriber . onCompleted ( ) ; } }
@ Override public void onAuthenticationSuccess ( HttpServletRequest request , HttpServletResponse response , Authentication authentication ) throws IOException , ServletException { logger . debug ( "onAuthenticationSuccess() started" ) ; String userId = authentication . getName ( ) ; Cookie newCookie = basicLoginService . createNewCookie ( userId ) ; response . addCookie ( newCookie ) ; response . setStatus ( HttpStatus . OK . value ( ) ) ; response . setContentType ( MediaType . APPLICATION_JSON_VALUE ) ; response . sendRedirect ( BasicLoginConstants . URI_MAIN ) ; return ; }
public void test() { try { lm . prepareDAG ( new FilterClassifierApp ( ) , conf ) ; LocalMode . Controller lc = lm . getController ( ) ; lc . run ( 20000 ) ; } catch ( Exception ex ) { logger . info ( ex . getMessage ( ) ) ; } }
public void test() { if ( primaryEmail == null ) { LOGGER . info ( "No primary email for orcid: " + orcid ) ; return ; } }
public void test() { if ( ! notifications . isEmpty ( ) ) { LOGGER . info ( "Found {} messages to send for orcid: {}" , notifications . size ( ) , orcid ) ; EmailMessage digestMessage ; code_block = IfStatement ; digestMessage . setFrom ( DIGEST_FROM_ADDRESS ) ; digestMessage . setTo ( primaryEmail . getEmail ( ) ) ; boolean successfullySent = mailGunManager . sendEmail ( digestMessage . getFrom ( ) , digestMessage . getTo ( ) , digestMessage . getSubject ( ) , digestMessage . getBodyText ( ) , digestMessage . getBodyHtml ( ) ) ; code_block = IfStatement ; } }
public void test() { try { Float emailFrequencyDays = null ; Date recordActiveDate = null ; recordActiveDate = ( Date ) element [ 1 ] ; List < Notification > notifications = notificationManager . findNotificationsToSend ( orcid , emailFrequencyDays , recordActiveDate ) ; EmailEntity primaryEmail = emailDao . findPrimaryEmail ( orcid ) ; code_block = IfStatement ; code_block = IfStatement ; } catch ( RuntimeException e ) { LOGGER . warn ( "Problem sending email message to user: " + orcid , e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( KBArticleServiceUtil . class , "fetchKBArticleByUrlTitle" , _fetchKBArticleByUrlTitleParameterTypes8 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , kbFolderId , urlTitle ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . knowledge . base . model . KBArticle ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { UserWorkspace userWorkspace = getUserWorkspace ( ) ; RulesProject selectedProject = userWorkspace . getProject ( repositoryId , currentProjectName , false ) ; AProjectResource projectResource ; code_block = IfStatement ; file = File . createTempFile ( "export-" , "-file" ) ; IOUtils . copyAndClose ( projectResource . getContent ( ) , new FileOutputStream ( file ) ) ; addCookie ( cookieName , "success" , - 1 ) ; final FacesContext facesContext = FacesContext . getCurrentInstance ( ) ; HttpServletResponse response = ( HttpServletResponse ) WebStudioUtils . getExternalContext ( ) . getResponse ( ) ; ExportFile . writeOutContent ( response , file , getFileName ( ) ) ; facesContext . responseComplete ( ) ; } catch ( Exception e ) { String msg = "Failed to export file version. " ; LOG . error ( msg , e ) ; addCookie ( cookieName , msg + e . getMessage ( ) , - 1 ) ; } finally { FileUtils . deleteQuietly ( file ) ; } }
public void test() { try { boolean result = keeperService . isKeeper ( hostPort ) ; return GenericRetMessage . createGenericRetMessage ( result ) ; } catch ( Exception e ) { logger . error ( "[isKeeper]{}" , e ) ; return RetMessage . createFailMessage ( e . getMessage ( ) ) ; } }
@ Override public HttpService addingService ( ServiceReference < HttpService > serviceRef ) { logger . info ( "registering servlet" ) ; httpService = super . addingService ( serviceRef ) ; HttpContext httpContext = new CustomHttpContext ( context . getBundle ( ) ) ; registerServlet ( httpContext ) ; registerResources ( httpContext ) ; return httpService ; }
public void test() { try { availableClientQueue . add ( client ) ; } catch ( IllegalStateException ise ) { log . warn ( "Capacity hit adding client back to queue. Closing extra" ) ; client . close ( ) ; } }
public void test() { try { Gson gson = new Gson ( ) ; Set < String > setElementIds = gson . fromJson ( elementId , new TypeToken < Set < String > > ( ) code_block = "" ; . getType ( ) ) ; MediaPackage mediapackage = MediaPackageParser . getFromXml ( mediaPackageXml ) ; result = service . distributeSync ( channelId , mediapackage , setElementIds , checkAvailability ) ; } catch ( IllegalArgumentException e ) { logger . debug ( "Unable to distribute element: {}" , e . getMessage ( ) ) ; return status ( Status . BAD_REQUEST ) . build ( ) ; } catch ( Exception e ) { logger . warn ( "Unable to distribute media package {}, element {} to aws s3 channel: {}" , mediaPackageXml , elementId , e ) ; return Response . serverError ( ) . status ( Status . INTERNAL_SERVER_ERROR ) . build ( ) ; } }
public void test() { try { Gson gson = new Gson ( ) ; Set < String > setElementIds = gson . fromJson ( elementId , new TypeToken < Set < String > > ( ) code_block = "" ; . getType ( ) ) ; MediaPackage mediapackage = MediaPackageParser . getFromXml ( mediaPackageXml ) ; result = service . distributeSync ( channelId , mediapackage , setElementIds , checkAvailability ) ; } catch ( IllegalArgumentException e ) { logger . debug ( "Unable to distribute element: {}" , e . getMessage ( ) ) ; return status ( Status . BAD_REQUEST ) . build ( ) ; } catch ( Exception e ) { logger . warn ( "Unable to distribute media package {}, element {} to aws s3 channel: {}" , mediaPackageXml , elementId , e ) ; return Response . serverError ( ) . status ( Status . INTERNAL_SERVER_ERROR ) . build ( ) ; } }
public void test() { if ( ! result . isPresent ( ) && log . isDebugEnabled ( ) ) { log . debug ( "Task not found matching " + matcher + "; contender names were " + taskNames ) ; } }
public void test() { try { String sp = getSpName ( delegate ) ; code_block = IfStatement ; } catch ( MetadataProviderException e ) { log . error ( "Unable to get IDP alias for:" + delegate , e ) ; } }
@ Test public void parseBsonArrayWithValues ( ) throws IOException { BsonValue a = new BsonString ( "stest" ) ; BsonValue b = new BsonDouble ( 111 ) ; BsonValue c = new BsonBoolean ( true ) ; BsonDocument document = new BsonDocument ( ) . append ( "int32" , new BsonInt32 ( 12 ) ) . append ( "int64" , new BsonInt64 ( 77L ) ) . append ( "bo\"olean" , new BsonBoolean ( true ) ) . append ( "date" , new BsonDateTime ( new Date ( ) . getTime ( ) ) ) . append ( "double" , new BsonDouble ( 12.3 ) ) . append ( "string" , new BsonString ( "pinpoint" ) ) . append ( "objectId" , new BsonObjectId ( new ObjectId ( ) ) ) . append ( "code" , new BsonJavaScript ( "int i = 10;" ) ) . append ( "codeWithScope" , new BsonJavaScriptWithScope ( "int x = y" , new BsonDocument ( "y" , new BsonInt32 ( 1 ) ) ) ) . append ( "regex" , new BsonRegularExpression ( "^test.*regex.*xyz$" , "big" ) ) . append ( "symbol" , new BsonSymbol ( "wow" ) ) . append ( "timestamp" , new BsonTimestamp ( 0x12345678 , 5 ) ) . append ( "undefined" , new BsonUndefined ( ) ) . append ( "binary1" , new BsonBinary ( new byte [ ] code_block = "" ; ) ) . append ( "oldBinary" , new BsonBinary ( BsonBinarySubType . OLD_BINARY , new byte [ ] code_block = "" ; ) ) . append ( "arrayInt" , new BsonArray ( Arrays . asList ( a , b , c , new BsonInt32 ( 7 ) ) ) ) . append ( "document" , new BsonDocument ( "a" , new BsonInt32 ( 77 ) ) ) . append ( "dbPointer" , new BsonDbPointer ( "db.coll" , new ObjectId ( ) ) ) . append ( "null" , new BsonNull ( ) ) . append ( "decimal128" , new BsonDecimal128 ( new Decimal128 ( 55 ) ) ) ; BasicDBObject query = new BasicDBObject ( ) ; query . put ( "ComplexBson" , document ) ; logger . debug ( "document:{}" , document ) ; NormalizedBson stringStringValue = MongoUtil . parseBson ( new Object [ ] code_block = "" ; , true ) ; logger . debug ( "val:{}" , stringStringValue ) ; List list = objectMapper . readValue ( "[" + stringStringValue . getNormalizedBson ( ) + "]" , List . class ) ; Assert . assertEquals ( list . size ( ) , 1 ) ; Map < String , ? > query1Map = ( Map < String , ? > ) list . get ( 0 ) ; checkValue ( query1Map ) ; }
@ Test public void parseBsonArrayWithValues ( ) throws IOException { BsonValue a = new BsonString ( "stest" ) ; BsonValue b = new BsonDouble ( 111 ) ; BsonValue c = new BsonBoolean ( true ) ; BsonDocument document = new BsonDocument ( ) . append ( "int32" , new BsonInt32 ( 12 ) ) . append ( "int64" , new BsonInt64 ( 77L ) ) . append ( "bo\"olean" , new BsonBoolean ( true ) ) . append ( "date" , new BsonDateTime ( new Date ( ) . getTime ( ) ) ) . append ( "double" , new BsonDouble ( 12.3 ) ) . append ( "string" , new BsonString ( "pinpoint" ) ) . append ( "objectId" , new BsonObjectId ( new ObjectId ( ) ) ) . append ( "code" , new BsonJavaScript ( "int i = 10;" ) ) . append ( "codeWithScope" , new BsonJavaScriptWithScope ( "int x = y" , new BsonDocument ( "y" , new BsonInt32 ( 1 ) ) ) ) . append ( "regex" , new BsonRegularExpression ( "^test.*regex.*xyz$" , "big" ) ) . append ( "symbol" , new BsonSymbol ( "wow" ) ) . append ( "timestamp" , new BsonTimestamp ( 0x12345678 , 5 ) ) . append ( "undefined" , new BsonUndefined ( ) ) . append ( "binary1" , new BsonBinary ( new byte [ ] code_block = "" ; ) ) . append ( "oldBinary" , new BsonBinary ( BsonBinarySubType . OLD_BINARY , new byte [ ] code_block = "" ; ) ) . append ( "arrayInt" , new BsonArray ( Arrays . asList ( a , b , c , new BsonInt32 ( 7 ) ) ) ) . append ( "document" , new BsonDocument ( "a" , new BsonInt32 ( 77 ) ) ) . append ( "dbPointer" , new BsonDbPointer ( "db.coll" , new ObjectId ( ) ) ) . append ( "null" , new BsonNull ( ) ) . append ( "decimal128" , new BsonDecimal128 ( new Decimal128 ( 55 ) ) ) ; BasicDBObject query = new BasicDBObject ( ) ; query . put ( "ComplexBson" , document ) ; logger . debug ( "document:{}" , document ) ; NormalizedBson stringStringValue = MongoUtil . parseBson ( new Object [ ] code_block = "" ; , true ) ; logger . debug ( "val:{}" , stringStringValue ) ; List list = objectMapper . readValue ( "[" + stringStringValue . getNormalizedBson ( ) + "]" , List . class ) ; Assert . assertEquals ( list . size ( ) , 1 ) ; Map < String , ? > query1Map = ( Map < String , ? > ) list . get ( 0 ) ; checkValue ( query1Map ) ; }
public void test() { if ( LOGGER . isInfoEnabled ( ) ) { LOGGER . info ( format ( "%s '%s' never started, nothing to dispose of" , capitalize ( artifactType ) , getArtifactName ( ) ) ) ; } }
public void test() { try { stop ( ) ; } catch ( DeploymentStopException e ) { LOGGER . error ( format ( "Error stopping %s '%s'" , artifactType , getArtifactName ( ) ) , e ) ; } }
@ Override public Alert findAlertByPrimaryKey ( BigInteger id ) { requireNotDisposed ( ) ; requireArgument ( id != null && id . compareTo ( ZERO ) > 0 , "ID must be a positive non-zero value." ) ; EntityManager em = _emProvider . get ( ) ; em . getEntityManagerFactory ( ) . getCache ( ) . evictAll ( ) ; Alert result = Alert . findByPrimaryKey ( em , id , Alert . class ) ; _logger . debug ( "Query for alert having id {} resulted in : {}" , id , result ) ; return result ; }
public void test() { try { g . removeStatements ( xDMA , uri , null ) ; } catch ( NoSuchElementException x ) { LOGGER . trace ( "" , x ) ; } }
public void test() { try { XURI uri = getDataIdURI ( dataId ) ; code_block = TryStatement ;  g . addStatement ( xDMA , uri , Literal . create ( UNO . defaultContext , dataValue ) ) ; } catch ( Exception e ) { LOGGER . error ( L . m ( "Kann RDF-Metadatum zur DataID '%1' nicht setzen." , dataId ) , e ) ; } }
public void test() { if ( properties . containsKey ( propertyKey ) ) { log . warn ( "Duplicate property values for [" + propertyKey + "]." ) ; } }
public void test() { try { List < String > allowedGroupCodes = this . getAllowedGroupCodes ( ) ; result = this . getPageManager ( ) . searchPages ( this . getPageCodeToken ( ) , allowedGroupCodes ) ; } catch ( Throwable t ) { _logger . error ( "Error on searching pages" , t ) ; throw new RuntimeException ( "Error on searching pages" , t ) ; } }
public void test() { try ( EntityManagerContainer emc = EntityManagerContainerFactory . instance ( ) . create ( ) ) { ids = attendanceDetailMobileService . listAllAnalyseWithStatus ( emc , 0 ) ; } catch ( Exception e ) { logger . error ( new QueryMobileDetailWithStatusException ( 0 ) ) ; } }
public void test() { try { attendanceDetailMobileAnalyseServiceAdv . analyseAttendanceDetailMobile ( id , false ) ; } catch ( Exception e ) { logger . error ( new AnalyseMobileDetailByIdException ( id ) ) ; } }
public void test() { if ( ! res ) { logger . error ( "ConsistencyWriter: Write barrier has not been met for global strong request. SelectedGlobalCommittedLsn: {}" , request . requestContext . globalCommittedSelectedLSN ) ; return Mono . error ( new GoneException ( RMResources . GlobalStrongWriteBarrierNotMet ) ) ; } }
public void test() { if ( ReplicatedResourceClient . isGlobalStrongEnabled ( ) && this . isGlobalStrongRequest ( request , response ) ) { Utils . ValueHolder < Long > lsn = Utils . ValueHolder . initialize ( - 1L ) ; Utils . ValueHolder < Long > globalCommittedLsn = Utils . ValueHolder . initialize ( - 1L ) ; getLsnAndGlobalCommittedLsn ( response , lsn , globalCommittedLsn ) ; code_block = IfStatement ; request . requestContext . globalStrongWriteResponse = response ; request . requestContext . globalCommittedSelectedLSN = lsn . v ; request . requestContext . forceRefreshAddressCache = false ; logger . debug ( "ConsistencyWriter: globalCommittedLsn {}, lsn {}" , globalCommittedLsn , lsn ) ; code_block = IfStatement ; } else { return Mono . just ( response ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceOrderServiceUtil . class , "updateCustomFields" , _updateCustomFieldsParameterTypes44 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , commerceOrderId , serviceContext ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . commerce . model . CommerceOrder ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
@ Override public void analyse ( String source , ArchiveRecordHeader header , InputStream tikainput , SolrRecord solr ) { final String url = Normalisation . sanitiseWARCHeaderValue ( header . getUrl ( ) ) ; log . debug ( "Analysing " + url ) ; final long start = System . nanoTime ( ) ; code_block = TryStatement ;  Instrument . timeRel ( "WARCPayloadAnalyzers.analyzetotal" , "WARCPayloadAnalyzers.analyzetikasolrextract" , start ) ; }
public void test() { try { code_block = IfStatement ; } catch ( Exception i ) { log . error ( i + ": " + i . getMessage ( ) + ";tika; " + url + "@" + header . getOffset ( ) ) ; } }
public void test() { try { UnitOfWorkHelper . doneSynchronizations ( correlatedExchange , correlatedExchange . adapt ( ExtendedExchange . class ) . handoverCompletions ( ) , LOG ) ; LOG . debug ( "Record with claim check number: {} committed." , claimCheck ) ; } catch ( Throwable t ) { LOG . error ( "Exception during Unit Of Work completion: {} caused by: {}" , t . getMessage ( ) , t . getCause ( ) ) ; throw new RuntimeException ( t ) ; } finally { exchangesWaitingForAck [ claimCheck ] = null ; freeSlots . add ( claimCheck ) ; LOG . debug ( "Claim check number: {} freed." , claimCheck ) ; } }
public void test() { if ( cachedRowSet . size ( ) == 0 ) { return Optional . empty ( ) ; } else-if ( cachedRowSet . size ( ) > 1 ) { log . info ( MessageFormat . format ( "Found multiple implementations for {0}. Returning first implementation" , executionRequestLabelKey . toString ( ) ) ) ; } }
public void test() { if ( ! match . isBoolean ( ) ) { log . error ( "matching returned error. (Should never happen): {}" , match . getMessage ( ) ) ; return policyRetrievalResult . withError ( ) ; } }
public void test() { try { druidDataSource . close ( ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { running = false ; logger . info ( " stop the canal client adapters" ) ; code_block = IfStatement ; code_block = ForStatement ; DatasourceConfig . DATA_SOURCES . clear ( ) ; } catch ( Throwable e ) { logger . warn ( " something goes wrong when stopping canal client adapters:" , e ) ; } finally { logger . info ( " canal client adapters are down." ) ; } }
public void test() { try { running = false ; logger . info ( " stop the canal client adapters" ) ; code_block = IfStatement ; code_block = ForStatement ; DatasourceConfig . DATA_SOURCES . clear ( ) ; } catch ( Throwable e ) { logger . warn ( " something goes wrong when stopping canal client adapters:" , e ) ; } finally { logger . info ( " canal client adapters are down." ) ; } }
private void createNetModeTransaction ( ) { logger . debug ( "createNetModeTransaction" ) ; final List < ByteString > argList = new ArrayList < > ( ) ; Lifecycle . InstallChaincodeArgs installChaincodeArgs = Lifecycle . InstallChaincodeArgs . newBuilder ( ) . setChaincodeInstallPackage ( ByteString . copyFrom ( chaincodeBytes ) ) . build ( ) ; argList . add ( ByteString . copyFromUtf8 ( action ) ) ; argList . add ( ByteString . copyFrom ( installChaincodeArgs . toByteArray ( ) ) ) ; args ( argList ) ; }
@ Pollable ( expectedSubTaskNumber = 3 , message = "Importing file: {fileName}" ) private void importFile ( DropImporter dropImporter , DropExporter dropExporter , @ MsgArg ( name = "fileName" , accessor = "getName" ) DropFile dropFile , TMTextUnitVariant . Status importStatus , @ ParentTask PollableTask parentTask , @ InjectCurrentTask PollableTask currentTask ) throws DropImporterException , DropExporterException , ImportDropException { logger . debug ( "Import file: {}" , dropFile . getName ( ) ) ; downloadDropFileContent ( dropImporter , dropFile , currentTask ) ; UpdateTMWithXLIFFResult updateReport = updateTMWithLocalizedXLIFF ( dropFile , importStatus , currentTask ) ; exportImportedFile ( dropExporter , dropFile , updateReport . getXliffContent ( ) , updateReport . getComment ( ) , currentTask ) ; }
@ Test public void testGetRepositoryPath ( ) { mLog . info ( "testGetRepositoryPath.." ) ; cmisObject . getProperties ( ) . getProperties ( ) . add ( new PropertyId ( PropertiesBase . OBJECTID , "repository" ) ) ; cmisObject . getProperties ( ) . getProperties ( ) . add ( new PropertyString ( PropertiesBase . OBJECTTYPEID , CmisObject . OBJECT_TYPE_FOLDER ) ) ; String path = navService . getRepositoryPath ( cmisObject ) ; assertTrue ( "repository" . equals ( path ) ) ; cmisObject . setProperties ( new CmisProperties ( ) ) ; cmisObject . getProperties ( ) . getProperties ( ) . add ( new PropertyId ( PropertiesBase . OBJECTID , "/admin/pat" ) ) ; cmisObject . getProperties ( ) . getProperties ( ) . add ( new PropertyString ( PropertiesBase . OBJECTTYPEID , null ) ) ; path = navService . getRepositoryPath ( cmisObject ) ; assertTrue ( "Repository path as expected" , "/admin" . equals ( path ) ) ; cmisObject . setProperties ( new CmisProperties ( ) ) ; cmisObject . getProperties ( ) . getProperties ( ) . add ( new PropertyId ( PropertiesBase . OBJECTID , "repository" ) ) ; cmisObject . getProperties ( ) . getProperties ( ) . add ( new PropertyString ( PropertiesBase . OBJECTTYPEID , null ) ) ; path = navService . getRepositoryPath ( cmisObject ) ; assertTrue ( "Repository path as expected" , "" . equals ( path ) ) ; }
static void abort ( AppContext context , RepairSegment segment , JmxProxy jmxConnection ) { postpone ( context , segment , context . storage . getRepairUnit ( segment . getRepairUnitId ( ) ) ) ; LOG . info ( "Aborting repair on segment with id {} on coordinator {}" , segment . getId ( ) , segment . getCoordinatorHost ( ) ) ; String metric = MetricRegistry . name ( SegmentRunner . class , "abort" , Optional . ofNullable ( segment . getCoordinatorHost ( ) ) . orElse ( "null" ) . replace ( '.' , '-' ) ) ; context . metricRegistry . counter ( metric ) . inc ( ) ; jmxConnection . cancelAllRepairs ( ) ; }
public void test() { try { String protocol = url . getProtocol ( ) ; String path = url . getPath ( ) ; code_block = IfStatement ; } catch ( Throwable t ) { LOGGER . error ( t . getMessage ( ) , t ) ; } }
public void test() { try { context . sendCeaMessage ( resultCode , message ( event ) , null ) ; switchToNextState ( OKAY ) ; } catch ( Exception e ) { logger . debug ( "Can not send CEA" , e ) ; doDisconnect ( ) ; doEndConnection ( ) ; } }
public void test() { switch ( type ( event ) ) { case DISCONNECT_EVENT : setTimer ( 0 ) ; doEndConnection ( ) ; break ; case TIMEOUT_EVENT : doDisconnect ( ) ; doEndConnection ( ) ; break ; case STOP_EVENT : setTimer ( 0 ) ; doDisconnect ( ) ; switchToNextState ( DOWN ) ; break ; case CEA_EVENT : setTimer ( 0 ) ; code_block = IfStatement ; break ; case CER_EVENT : int resultCode = context . processCerMessage ( key ( event ) , message ( event ) ) ; code_block = IfStatement ; break ; case SEND_MSG_EVENT : throw new IllegalStateException ( "Connection is down" ) ; default : logger . debug ( "Unknown event type {} in state {}" , type ( event ) , state ) ; return false ; } }
public FilterSuchenFelderTxt findById ( sernet . gs . reveng . FilterSuchenFelderTxtId id ) { log . debug ( "getting FilterSuchenFelderTxt instance with id: " + id ) ; code_block = TryStatement ;  }
public void test() { if ( instance == null ) { log . debug ( "get successful, no instance found" ) ; } else { log . debug ( "get successful, instance found" ) ; } }
public void test() { if ( instance == null ) { log . debug ( "get successful, no instance found" ) ; } else { log . debug ( "get successful, instance found" ) ; } }
public void test() { try { FilterSuchenFelderTxt instance = ( FilterSuchenFelderTxt ) sessionFactory . getCurrentSession ( ) . get ( "sernet.gs.reveng.FilterSuchenFelderTxt" , id ) ; code_block = IfStatement ; return instance ; } catch ( RuntimeException re ) { log . error ( "get failed" , re ) ; throw re ; } }
public void test() { try { com . liferay . segments . model . SegmentsExperimentRel returnValue = SegmentsExperimentRelServiceUtil . updateSegmentsExperimentRel ( segmentsExperimentRelId , split ) ; return com . liferay . segments . model . SegmentsExperimentRelSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try { assertFalse ( running ) ; running = true ; code_block = TryStatement ;  } catch ( Throwable e ) { log . error ( "Test failed" , e ) ; addException ( e ) ; } }
public void test() { if ( actionMap . get ( "type" ) . equalsIgnoreCase ( FloodlightOFAction . TYPE_OUTPUT . toLowerCase ( ) ) ) { FloodlightOFAction action = new FloodlightOFAction ( ) ; action . setType ( FloodlightOFAction . TYPE_OUTPUT . toLowerCase ( ) ) ; action . setValue ( actionMap . get ( "port" ) ) ; actions . add ( action ) ; } else { log . info ( "Property type unknown: " + actionMap . get ( "type" ) ) ; } }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "JspPlugin config={}" , config ) ; } }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "JspPlugin disabled" ) ; } }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "Adding Jasper 2 JSP Engine(Tomcat, Jetty, JBoss)." ) ; } }
public void test() { if ( Logger . logLevel <= ILogger . INFO ) { Logger . logger . info ( Logger . MISC_LOG + id + ": " + message ) ; } }
public void test() { if ( p instanceof SpecParameterIncludingDefinitionForInheritance ) { p = ( ( SpecParameterIncludingDefinitionForInheritance < ? > ) p ) . resolveWithAncestor ( existingP ) ; } else { log . warn ( "Found non-definitional spec parameter: " + p + " adding to " + spec ) ; } }
@ Override public void draggedImage_movedTo ( NSImage image , NSPoint point ) { log . trace ( "draggedImage_movedTo" ) ; }
public void startTimedScheduler ( ) { logger . info ( "Scheduling report aggregator job..." ) ; Date startTime = DateTimeUtil . now ( ) . plusMinutes ( START_DELAY_IN_MINUTES ) . toDate ( ) ; MotechEvent event = new MotechEvent ( SUBJECT ) ; RepeatingSchedulableJob job = new RepeatingSchedulableJob ( event , startTime , null , REPEAT_INTERVAL_IN_MINUTES * MILLIS_PER_MINUTE ) ; schedulerService . safeScheduleRepeatingJob ( job ) ; }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Not found portalUser with username " + principal . getUserName ( ) + ". Redirecting to registration form" ) ; } }
public void test() { if ( tivoConfigData . isKeepConnActive ( ) ) { refreshJob = scheduler . schedule ( runnable , INIT_POLLING_DELAY_S , TimeUnit . SECONDS ) ; logger . debug ( "Status collection '{}' will start in '{}' seconds." , getThing ( ) . getUID ( ) , INIT_POLLING_DELAY_S ) ; } else-if ( tivoConfigData . doPollChanges ( ) ) { refreshJob = scheduler . scheduleWithFixedDelay ( runnable , INIT_POLLING_DELAY_S , tivoConfigData . getPollInterval ( ) , TimeUnit . SECONDS ) ; logger . debug ( "Status polling '{}' will start in '{}' seconds." , getThing ( ) . getUID ( ) , INIT_POLLING_DELAY_S ) ; } else { tivoConnection . ifPresent ( connection code_block = LoopStatement ; ) ; } }
public void test() { if ( tivoConfigData . isKeepConnActive ( ) ) { refreshJob = scheduler . schedule ( runnable , INIT_POLLING_DELAY_S , TimeUnit . SECONDS ) ; logger . debug ( "Status collection '{}' will start in '{}' seconds." , getThing ( ) . getUID ( ) , INIT_POLLING_DELAY_S ) ; } else-if ( tivoConfigData . doPollChanges ( ) ) { refreshJob = scheduler . scheduleWithFixedDelay ( runnable , INIT_POLLING_DELAY_S , tivoConfigData . getPollInterval ( ) , TimeUnit . SECONDS ) ; logger . debug ( "Status polling '{}' will start in '{}' seconds." , getThing ( ) . getUID ( ) , INIT_POLLING_DELAY_S ) ; } else { tivoConnection . ifPresent ( connection code_block = LoopStatement ; ) ; } }
public void test() { if ( systemPropertyName . isPresent ( ) && getProperty ( systemPropertyName . get ( ) ) != null ) { enabled = getBoolean ( systemPropertyName . get ( ) ) ; LOGGER . debug ( "Setting feature {} = {} for artifact [{}] because of System Property '{}'" , feature , enabled , artifactName , systemPropertyName ) ; } else { enabled = featurePredicate . test ( featureContext ) ; LOGGER . debug ( "Setting feature {} = {} for artifact [{}]" , feature , enabled , artifactName ) ; } }
public void test() { if ( systemPropertyName . isPresent ( ) && getProperty ( systemPropertyName . get ( ) ) != null ) { enabled = getBoolean ( systemPropertyName . get ( ) ) ; LOGGER . debug ( "Setting feature {} = {} for artifact [{}] because of System Property '{}'" , feature , enabled , artifactName , systemPropertyName ) ; } else { enabled = featurePredicate . test ( featureContext ) ; LOGGER . debug ( "Setting feature {} = {} for artifact [{}]" , feature , enabled , artifactName ) ; } }
public void test() { try { xacmlPdp = new XacmlPdp ( dirPath , parser , environmentAttributes , securityLogger ) ; } catch ( PdpException e ) { LOGGER . warn ( "Unable to create XACML PDP." , e ) ; } }
@ Override public void warning ( final String msg ) { log . warn ( msg ) ; }
public void test() { try { fail ( vo , e . getErrorCode ( ) ) ; } catch ( Throwable t ) { logger . warn ( String . format ( "Something seriously wrong happened when roll back" ) , t ) ; } }
public void test() { try { flow . process ( ctx , this ) ; } catch ( WorkFlowException e ) { code_block = TryStatement ;  } catch ( Throwable t ) { logger . warn ( String . format ( "workflow[%s] in chain[%s] failed because of an unhandle exception" , flow . getName ( ) , getName ( ) ) , t ) ; ErrorCode err = inerr ( t . getMessage ( ) ) ; code_block = TryStatement ;  } }
public void test() { try { fail ( vo , err ) ; } catch ( Throwable t1 ) { logger . warn ( String . format ( "Something seriously wrong happened when roll back" ) , t1 ) ; } }
@ Override public void onError ( final String reason ) { logger . warn ( "Error on '{}' delivery: {}" , variant . getType ( ) . getTypeName ( ) , reason ) ; PrometheusExporter . instance ( ) . increaseTotalPushRequestsFail ( ) ; pushMessageMetricsService . appendError ( pushMessageInformation , variant , reason ) ; }
public void test() { try { fileContainer . setBytes ( mpf . getBytes ( ) ) ; createFolderIfNotExists ( ) ; FileCopyUtils . copy ( mpf . getBytes ( ) , new BufferedOutputStream ( new FileOutputStream ( path + mpf . getOriginalFilename ( ) ) ) ) ; } catch ( IOException e ) { logger . error ( "Error during file upload" , e ) ; fileContainer . setError ( e . getMessage ( ) ) ; } }
public void test() { for ( ImageResizeMethod method : ImageResizeMethod . values ( ) ) { code_block = IfStatement ; log . info ( "Trying {}" , method ) ; Nd4j . getRandom ( ) . setSeed ( 12345 ) ; SameDiff sd = SameDiff . create ( ) ; boolean preserveAspectRatio = true ; boolean antialias = true ; SDVariable inputImage = sd . var ( Nd4j . rand ( DataType . FLOAT , 1 , 5 , 5 , 3 ) ) ; long [ ] expectedShape = new long [ ] code_block = "" ; ; SDVariable requestedSize = sd . constant ( Nd4j . createFromArray ( new long [ ] code_block = "" ; ) ) ; Function < INDArray , String > checkFunction = in code_block = LoopStatement ; ; SDVariable out = new ImageResize ( sd , inputImage , requestedSize , preserveAspectRatio , antialias , method ) . outputVariable ( ) . std ( true ) ; String err = OpValidation . validate ( new TestCase ( sd ) . gradientCheck ( false ) . expected ( "image_resize" , checkFunction ) ) ; assertNull ( err ) ; } }
@ Override public Number getAggregated ( ) { code_block = IfStatement ; Number result = currentValue - previousValue ; log . debug ( "getAggregated() = {}" , result ) ; return result ; }
public void test() { try { docModel . setProperty ( IRelationsManager . SERVICE_COMMONPART_NAME , targetField , newRefName ) ; repoSession . saveDocument ( docModel ) ; } catch ( ClientException e ) { logger . error ( String . format ( "Could not update field '%s' with updated refName '%s' for relations record CSID=%s" , targetField , newRefName , docModel . getName ( ) ) ) ; } }
public void test() { try { boolean morePages = true ; code_block = WhileStatement ; } catch ( Exception e ) { logger . error ( "Internal error updating the ref names in relations: " + e . getLocalizedMessage ( ) ) ; logger . debug ( Tools . errorToString ( e , true ) ) ; throw e ; } }
public void test() { try { repoSession . save ( ) ; } catch ( ClientException e ) { logger . error ( "Could not flush results of relation-refName payload updates to Nuxeo repository" ) ; } }
public void test() { if ( ! connection . isConnected ( ) ) { connection . connect ( timeout ) ; connection . login ( username , password , resource , timeout ) ; connection . addStanzaListener ( new RayoMessageListener ( "offer" ) code_block = "" ; ) ; connection . addStanzaListener ( new RayoMessageListener ( "end" ) code_block = "" ; ) ; broadcastAvailability ( ) ; TimerTask pingTask = new TimerTask ( ) code_block = "" ; ; pingTimer = new Timer ( ) ; pingTimer . schedule ( pingTask , 5000 , 30000 ) ; connection . addStanzaListener ( new RayoMessageListener ( "ping" ) code_block = "" ; ) ; } else { logger . error ( "Trying to connect while the old XMPP connection is active. Please, disconnect first" ) ; } }
public void test() { if ( dryrun ) { logger . info ( "skipping cycle " + cycle + " because dryrun is set to true" ) ; return ; } else { logger . info ( "running cycle " + cycle + " because dryrun is set to false" ) ; } }
public void test() { if ( dryrun ) { logger . info ( "skipping cycle " + cycle + " because dryrun is set to true" ) ; return ; } else { logger . info ( "running cycle " + cycle + " because dryrun is set to false" ) ; } }
private void downloadEnsemblData ( Path geneFolder ) throws IOException , InterruptedException { logger . info ( "Downloading gene Ensembl data (gtf, pep, cdna, motifs) ..." ) ; List < String > downloadedUrls = new ArrayList < > ( 4 ) ; String ensemblHost = ensemblHostUrl + "/" + ensemblRelease ; code_block = IfStatement ; String bacteriaCollectionPath = "" ; code_block = IfStatement ; String version = ensemblRelease . split ( "-" ) [ 1 ] ; String url = ensemblHost + "/gtf/" + bacteriaCollectionPath + speciesShortName + "/*" + version + ".gtf.gz" ; String fileName = geneFolder . resolve ( speciesShortName + ".gtf.gz" ) . toString ( ) ; downloadFile ( url , fileName ) ; downloadedUrls . add ( url ) ; url = ensemblHost + "/fasta/" + bacteriaCollectionPath + speciesShortName + "/pep/*.pep.all.fa.gz" ; fileName = geneFolder . resolve ( speciesShortName + ".pep.all.fa.gz" ) . toString ( ) ; downloadFile ( url , fileName ) ; downloadedUrls . add ( url ) ; url = ensemblHost + "/fasta/" + bacteriaCollectionPath + speciesShortName + "/cdna/*.cdna.all.fa.gz" ; fileName = geneFolder . resolve ( speciesShortName + ".cdna.all.fa.gz" ) . toString ( ) ; downloadFile ( url , fileName ) ; downloadedUrls . add ( url ) ; saveVersionData ( EtlCommons . GENE_DATA , ENSEMBL_NAME , ensemblVersion , getTimeStamp ( ) , downloadedUrls , buildFolder . resolve ( "ensemblCoreVersion.json" ) ) ; }
public void test() { try { long resultCode = answer . getResultCodeAvp ( ) . getUnsigned32 ( ) ; code_block = IfStatement ; code_block = IfStatement ; deliverRoAnswer ( ( RoCreditControlRequest ) localEvent . getRequest ( ) , ( RoCreditControlAnswer ) localEvent . getAnswer ( ) ) ; } catch ( AvpDataException e ) { logger . debug ( "Failure handling received answer event" , e ) ; setState ( ClientRoSessionState . IDLE , false ) ; } }
@ Override protected void shutDown ( ) throws Exception { LOG . debug ( "Shutting down journal!" ) ; shuttingDown = true ; offsetFlusherFuture . cancel ( false ) ; logRetentionFuture . cancel ( false ) ; checkpointFlusherFuture . cancel ( false ) ; dirtyLogFlushFuture . cancel ( false ) ; kafkaScheduler . shutdown ( ) ; logManager . shutdown ( ) ; offsetFlusher . run ( ) ; teardownLogMetrics ( ) ; }
@ Test void testWithoutFields ( ) throws Exception { Logger logger = lc . getLogger ( getClass ( ) ) ; logger . info ( LOG_MESSAGE ) ; assertThat ( GelfTestSender . getMessages ( ) ) . hasSize ( 1 ) ; GelfMessage gelfMessage = GelfTestSender . getMessages ( ) . get ( 0 ) ; String myMdc = gelfMessage . getField ( MDC_MY_MDC ) ; assertThat ( myMdc ) . isNull ( ) ; }
public void test() { if ( eofAckCount >= EOF_ACK_LIMIT ) { log . warn ( "EOF_ACK_LIMIT reached" ) ; eofSenderFuture . cancel ( false ) ; } else { log . info ( "CFDP sending EOF" ) ; EofPacket eof = new EofPacket ( ConditionCode . NO_ERROR , checksum , fileSize , null , directiveHeader ) ; simulator . transmitCfdp ( eof ) ; } }
public void test() { try { LocalDate lastUpdatedDate = LocalDate . parse ( lastUpdated , formatter ) . atStartOfDay ( ) . toLocalDate ( ) ; return today . isAfter ( lastUpdatedDate ) ; } catch ( DateTimeParseException e ) { StartLog . debug ( e ) ; return false ; } }
protected void printOperationInfo ( Logger logger ) { StringBuilder strb = new StringBuilder ( 35 ) ; strb . append ( "[" ) . append ( dateFormat . format ( new Date ( getPreExecutionTime ( ) ) ) ) . append ( " - " ) . append ( dateFormat . format ( new Date ( getPostExecutionTime ( ) ) ) ) . append ( " (" ) . append ( getPostExecutionTime ( ) - getPreExecutionTime ( ) ) . append ( "ms)]" ) ; strb . append ( " " ) . append ( getOperation ( ) . getClass ( ) . getSimpleName ( ) ) . append ( " " ) ; code_block = IfStatement ; logger . info ( strb . toString ( ) ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( String . format ( "Run with feature %s" , feature ) ) ; } }
public void test() { if ( domainGroup == null ) { LOG . info ( "Domain group not found. Nothing to do." ) ; return ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( measurement . toString ( ) ) ; } }
public void test() { switch ( level ) { case DEBUG : code_block = IfStatement ; break ; case ERROR : log . error ( measurement . toString ( ) ) ; break ; case FATAL : log . fatal ( measurement . toString ( ) ) ; break ; case INFO : code_block = IfStatement ; break ; case TRACE : code_block = IfStatement ; break ; case WARN : log . warn ( measurement . toString ( ) ) ; break ; } }
public void test() { switch ( level ) { case DEBUG : code_block = IfStatement ; break ; case ERROR : log . error ( measurement . toString ( ) ) ; break ; case FATAL : log . fatal ( measurement . toString ( ) ) ; break ; case INFO : code_block = IfStatement ; break ; case TRACE : code_block = IfStatement ; break ; case WARN : log . warn ( measurement . toString ( ) ) ; break ; } }
public void test() { if ( log . isInfoEnabled ( ) ) { log . info ( measurement . toString ( ) ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( measurement . toString ( ) ) ; } }
public void test() { switch ( level ) { case DEBUG : code_block = IfStatement ; break ; case ERROR : log . error ( measurement . toString ( ) ) ; break ; case FATAL : log . fatal ( measurement . toString ( ) ) ; break ; case INFO : code_block = IfStatement ; break ; case TRACE : code_block = IfStatement ; break ; case WARN : log . warn ( measurement . toString ( ) ) ; break ; } }
public void test() { if ( logger . isWarnEnabled ( ) ) { logger . warn ( "@EnableAutoConfiguration was declared on a class " + "in the default package. Automatic @Repository and " + "@Entity scanning is not enabled." ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { String packageNames = StringUtils . collectionToCommaDelimitedString ( this . packages ) ; logger . debug ( "@EnableAutoConfiguration was declared on a class " + "in the package '" + packageNames + "'. Automatic @Repository and @Entity scanning is " + "enabled." ) ; } }
public void test() { try { defaultArticleURL = _portal . getControlPanelFullURL ( article . getGroupId ( ) , portletId , null ) ; } catch ( PortalException portalException ) { _log . error ( portalException , portalException ) ; } }
public void test() { try { AssetRendererFactory < JournalArticle > assetRendererFactory = AssetRendererFactoryRegistryUtil . getAssetRendererFactoryByClass ( JournalArticle . class ) ; AssetRenderer < JournalArticle > assetRenderer = assetRendererFactory . getAssetRenderer ( article , AssetRendererFactory . TYPE_LATEST_APPROVED ) ; return assetRenderer . getURLViewInContext ( liferayPortletRequest , null , defaultArticleURL ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception ex ) { _logger . warn ( "Could not enqueue scheduled job. " + ex . getMessage ( ) ) ; _auditService . createAudit ( "Could not enqueue scheduled job. " + ex . getMessage ( ) , JPAEntity . class . cast ( job ) ) ; } }
public void test() { try { List < Organization > organizations = user . getOrganizations ( ) ; code_block = IfStatement ; } catch ( PortalException | SystemException e ) { log . error ( "Error getting department" , e ) ; } }
public void test() { try { XarInstalledExtension xarInstalledExtension = addCacheXarExtension ( localExtension ) ; code_block = IfStatement ; } catch ( Exception e ) { this . logger . error ( "Failed to parse extension [{}]" , localExtension . getId ( ) , e ) ; continue ; } }
@ Override protected void connectionLog ( String message ) { connectionLogger . debug ( message ) ; }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Proxy handler " + this + ": connect.strategy=" + connectStrategy + "." ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( sb . toString ( ) ) ; } }
public void test() { else-if ( Math . abs ( newStart - read . getAlignmentStart ( ) ) > MAX_POS_MOVE_ALLOWED ) { logger . debug ( String . format ( "Attempting to realign read %s at %d more than %d bases to %d." , read . getReadName ( ) , read . getAlignmentStart ( ) , MAX_POS_MOVE_ALLOWED , newStart ) ) ; return false ; } }
@ Override public synchronized V put ( K key , V value ) { clearGCCollected ( ) ; logger . debug ( "Putting {} on cache. size: {}" , key , size ( ) ) ; map . put ( key , new SoftValue < K , V > ( value , key , queue ) ) ; return value ; }
public void test() { if ( deviceManagementProviderService == null ) { String msg = "Device Management service has not initialized." ; log . error ( msg ) ; throw new IllegalStateException ( msg ) ; } }
@ PatchMapping ( value = CommonConstants . PATH_ID ) @ ApiOperation ( value = "Update partially provider" ) @ ResponseStatus ( HttpStatus . OK ) public IdentityProviderDto patchProvider ( final @ RequestBody Map < String , Object > provider , final @ PathVariable String id ) { LOGGER . debug ( "Update partially provider id={} with partialDto={}" , id , provider ) ; ParameterChecker . checkParameter ( "Parameters are mandatory : " , provider , id ) ; return service . patch ( buildUiHttpContext ( ) , provider , null , null , id , ProviderPatchType . JSON ) ; }
public void test() { if ( droppingSinks . contains ( entry . getKey ( ) ) ) { log . warn ( "Skipping push of currently-dropping sink[%s]" , entry . getKey ( ) ) ; continue ; } }
public void test() { if ( dataSegment != null ) { dataSegments . add ( dataSegment ) ; } else { log . warn ( "mergeAndPush[%s] returned null, skipping." , entry . getKey ( ) ) ; } }
public void test() { if ( m_nodeLogger . isDebugEnabled ( ) ) { FormattingTuple ft = MessageFormatter . arrayFormat ( format , arguments ) ; m_nodeLogger . debug ( ft . getMessage ( ) , ft . getThrowable ( ) ) ; } }
@ Transactional ( rollbackFor = ArrowheadException . class ) public void logIntraMeasurementDetailsToDB ( final QoSIntraPingMeasurementLog measurementLogSaved , final List < IcmpPingResponse > responseList , final ZonedDateTime aroundNow ) { logger . debug ( "logIntraMeasurementDetailsToDB started..." ) ; code_block = IfStatement ; final List < QoSIntraPingMeasurementLogDetails > measurementLogDetailsList = new ArrayList < > ( responseList . size ( ) ) ; int measurementSequenece = 0 ; code_block = ForStatement ; code_block = TryStatement ;  }
public void test() { try { qoSIntraPingMeasurementLogDetailsRepository . saveAll ( measurementLogDetailsList ) ; qoSIntraPingMeasurementLogDetailsRepository . flush ( ) ; } catch ( final Exception ex ) { logger . debug ( ex . getMessage ( ) , ex ) ; throw new ArrowheadException ( CoreCommonConstants . DATABASE_OPERATION_EXCEPTION_MSG ) ; } }
@ Test public void testObjectMethodNotifications ( ) throws Exception { LOGGER . info ( "Running TestManagementNotifications.testIngest..." ) ; String pid = apim . ingest ( TypeUtility . convertBytesToDataHandler ( demo998FOXMLObjectXML ) , FOXML1_1 . uri , "ingesting new foxml object" ) ; assertNotNull ( pid ) ; checkNotification ( pid , "ingest" ) ; LOGGER . info ( "Running TestManagementNotifications.testModifyObject..." ) ; String modifyResult = apim . modifyObject ( pid , "I" , "Updated Object Label" , null , "Changed state to inactive and updated label" ) ; assertNotNull ( modifyResult ) ; checkNotification ( pid , "modifyObject" ) ; LOGGER . info ( "Running TestManagementNotifications.testAddRelationship..." ) ; boolean addRelResult = apim . addRelationship ( pid , "rel:isRelatedTo" , "demo:5" , false , null ) ; assertTrue ( addRelResult ) ; checkNotification ( pid , "addRelationship" ) ; LOGGER . info ( "Running TestManagementNotifications.testAddRelationship..." ) ; addRelResult = apim . addRelationship ( PID . toURI ( pid ) , "rel:isRelatedTo" , "demo:6" , false , null ) ; assertTrue ( addRelResult ) ; checkNotification ( pid , "addRelationship" ) ; LOGGER . info ( "Running TestManagementNotifications.testAddRelationship..." ) ; addRelResult = apim . addRelationship ( PID . toURI ( pid ) + "/DS1" , "rel:isRelatedTo" , "demo:7" , false , null ) ; assertTrue ( addRelResult ) ; checkNotification ( pid , "addRelationship" ) ; LOGGER . info ( "Running TestManagementNotifications.testPurgeRelationship..." ) ; boolean purgeRelResult = apim . purgeRelationship ( pid , "rel:isRelatedTo" , "demo:5" , false , null ) ; assertTrue ( purgeRelResult ) ; checkNotification ( pid , "purgeRelationship" ) ; LOGGER . info ( "Running TestManagementNotifications.testPurgeRelationship..." ) ; purgeRelResult = apim . purgeRelationship ( PID . toURI ( pid ) , "rel:isRelatedTo" , "demo:6" , false , null ) ; assertTrue ( purgeRelResult ) ; checkNotification ( pid , "purgeRelationship" ) ; LOGGER . info ( "Running TestManagementNotifications.testPurgeRelationship..." ) ; purgeRelResult = apim . purgeRelationship ( PID . toURI ( pid ) + "/DS1" , "rel:isRelatedTo" , "demo:7" , false , null ) ; assertTrue ( purgeRelResult ) ; checkNotification ( pid , "purgeRelationship" ) ; LOGGER . info ( "Running TestManagementNotifications.testPurgeObject..." ) ; String purgeResult = apim . purgeObject ( pid , "Purging object " + pid , false ) ; assertNotNull ( purgeResult ) ; checkNotification ( pid , "purgeObject" ) ; }
@ Test public void testObjectMethodNotifications ( ) throws Exception { LOGGER . info ( "Running TestManagementNotifications.testIngest..." ) ; String pid = apim . ingest ( TypeUtility . convertBytesToDataHandler ( demo998FOXMLObjectXML ) , FOXML1_1 . uri , "ingesting new foxml object" ) ; assertNotNull ( pid ) ; checkNotification ( pid , "ingest" ) ; LOGGER . info ( "Running TestManagementNotifications.testModifyObject..." ) ; String modifyResult = apim . modifyObject ( pid , "I" , "Updated Object Label" , null , "Changed state to inactive and updated label" ) ; assertNotNull ( modifyResult ) ; checkNotification ( pid , "modifyObject" ) ; LOGGER . info ( "Running TestManagementNotifications.testAddRelationship..." ) ; boolean addRelResult = apim . addRelationship ( pid , "rel:isRelatedTo" , "demo:5" , false , null ) ; assertTrue ( addRelResult ) ; checkNotification ( pid , "addRelationship" ) ; LOGGER . info ( "Running TestManagementNotifications.testAddRelationship..." ) ; addRelResult = apim . addRelationship ( PID . toURI ( pid ) , "rel:isRelatedTo" , "demo:6" , false , null ) ; assertTrue ( addRelResult ) ; checkNotification ( pid , "addRelationship" ) ; LOGGER . info ( "Running TestManagementNotifications.testAddRelationship..." ) ; addRelResult = apim . addRelationship ( PID . toURI ( pid ) + "/DS1" , "rel:isRelatedTo" , "demo:7" , false , null ) ; assertTrue ( addRelResult ) ; checkNotification ( pid , "addRelationship" ) ; LOGGER . info ( "Running TestManagementNotifications.testPurgeRelationship..." ) ; boolean purgeRelResult = apim . purgeRelationship ( pid , "rel:isRelatedTo" , "demo:5" , false , null ) ; assertTrue ( purgeRelResult ) ; checkNotification ( pid , "purgeRelationship" ) ; LOGGER . info ( "Running TestManagementNotifications.testPurgeRelationship..." ) ; purgeRelResult = apim . purgeRelationship ( PID . toURI ( pid ) , "rel:isRelatedTo" , "demo:6" , false , null ) ; assertTrue ( purgeRelResult ) ; checkNotification ( pid , "purgeRelationship" ) ; LOGGER . info ( "Running TestManagementNotifications.testPurgeRelationship..." ) ; purgeRelResult = apim . purgeRelationship ( PID . toURI ( pid ) + "/DS1" , "rel:isRelatedTo" , "demo:7" , false , null ) ; assertTrue ( purgeRelResult ) ; checkNotification ( pid , "purgeRelationship" ) ; LOGGER . info ( "Running TestManagementNotifications.testPurgeObject..." ) ; String purgeResult = apim . purgeObject ( pid , "Purging object " + pid , false ) ; assertNotNull ( purgeResult ) ; checkNotification ( pid , "purgeObject" ) ; }
@ Test public void testObjectMethodNotifications ( ) throws Exception { LOGGER . info ( "Running TestManagementNotifications.testIngest..." ) ; String pid = apim . ingest ( TypeUtility . convertBytesToDataHandler ( demo998FOXMLObjectXML ) , FOXML1_1 . uri , "ingesting new foxml object" ) ; assertNotNull ( pid ) ; checkNotification ( pid , "ingest" ) ; LOGGER . info ( "Running TestManagementNotifications.testModifyObject..." ) ; String modifyResult = apim . modifyObject ( pid , "I" , "Updated Object Label" , null , "Changed state to inactive and updated label" ) ; assertNotNull ( modifyResult ) ; checkNotification ( pid , "modifyObject" ) ; LOGGER . info ( "Running TestManagementNotifications.testAddRelationship..." ) ; boolean addRelResult = apim . addRelationship ( pid , "rel:isRelatedTo" , "demo:5" , false , null ) ; assertTrue ( addRelResult ) ; checkNotification ( pid , "addRelationship" ) ; LOGGER . info ( "Running TestManagementNotifications.testAddRelationship..." ) ; addRelResult = apim . addRelationship ( PID . toURI ( pid ) , "rel:isRelatedTo" , "demo:6" , false , null ) ; assertTrue ( addRelResult ) ; checkNotification ( pid , "addRelationship" ) ; LOGGER . info ( "Running TestManagementNotifications.testAddRelationship..." ) ; addRelResult = apim . addRelationship ( PID . toURI ( pid ) + "/DS1" , "rel:isRelatedTo" , "demo:7" , false , null ) ; assertTrue ( addRelResult ) ; checkNotification ( pid , "addRelationship" ) ; LOGGER . info ( "Running TestManagementNotifications.testPurgeRelationship..." ) ; boolean purgeRelResult = apim . purgeRelationship ( pid , "rel:isRelatedTo" , "demo:5" , false , null ) ; assertTrue ( purgeRelResult ) ; checkNotification ( pid , "purgeRelationship" ) ; LOGGER . info ( "Running TestManagementNotifications.testPurgeRelationship..." ) ; purgeRelResult = apim . purgeRelationship ( PID . toURI ( pid ) , "rel:isRelatedTo" , "demo:6" , false , null ) ; assertTrue ( purgeRelResult ) ; checkNotification ( pid , "purgeRelationship" ) ; LOGGER . info ( "Running TestManagementNotifications.testPurgeRelationship..." ) ; purgeRelResult = apim . purgeRelationship ( PID . toURI ( pid ) + "/DS1" , "rel:isRelatedTo" , "demo:7" , false , null ) ; assertTrue ( purgeRelResult ) ; checkNotification ( pid , "purgeRelationship" ) ; LOGGER . info ( "Running TestManagementNotifications.testPurgeObject..." ) ; String purgeResult = apim . purgeObject ( pid , "Purging object " + pid , false ) ; assertNotNull ( purgeResult ) ; checkNotification ( pid , "purgeObject" ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Calling authorization advice before " + method . getName ( ) ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { User user = Context . getAuthenticatedUser ( ) ; log . debug ( "User " + user ) ; code_block = IfStatement ; } }
public void test() { if ( user != null ) { log . debug ( "has roles " + user . getAllRoles ( ) ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "User has privilege " + privilege + "? " + Context . hasPrivilege ( privilege ) ) ; } }
protected void singleOrderBySameRangeScanGreater ( IoHelper io ) throws Exception { io . doSetup ( ) ; int size = 20 ; int queryLimit = 10 ; int startValue = 9 ; long start = System . currentTimeMillis ( ) ; logger . info ( "Writing {} entities." , size ) ; List < String > expected = new ArrayList < String > ( size ) ; code_block = ForStatement ; app . waitForQueueDrainAndRefreshIndex ( ) ; Thread . sleep ( 500 ) ; long stop = System . currentTimeMillis ( ) ; logger . info ( "Writes took {} ms" , stop - start ) ; Query query = Query . fromQL ( "select * where index >= " + startValue + " order by index desc" ) ; query . setLimit ( queryLimit ) ; int count = 0 ; start = System . currentTimeMillis ( ) ; Results results ; do code_block = "" ; while ( results . hasCursor ( ) ) ; assertEquals ( expected . size ( ) - startValue , count ) ; stop = System . currentTimeMillis ( ) ; logger . info ( "Query took {} ms to return {} entities" , stop - start , count ) ; }
protected void singleOrderBySameRangeScanGreater ( IoHelper io ) throws Exception { io . doSetup ( ) ; int size = 20 ; int queryLimit = 10 ; int startValue = 9 ; long start = System . currentTimeMillis ( ) ; logger . info ( "Writing {} entities." , size ) ; List < String > expected = new ArrayList < String > ( size ) ; code_block = ForStatement ; app . waitForQueueDrainAndRefreshIndex ( ) ; Thread . sleep ( 500 ) ; long stop = System . currentTimeMillis ( ) ; logger . info ( "Writes took {} ms" , stop - start ) ; Query query = Query . fromQL ( "select * where index >= " + startValue + " order by index desc" ) ; query . setLimit ( queryLimit ) ; int count = 0 ; start = System . currentTimeMillis ( ) ; Results results ; do code_block = "" ; while ( results . hasCursor ( ) ) ; assertEquals ( expected . size ( ) - startValue , count ) ; stop = System . currentTimeMillis ( ) ; logger . info ( "Query took {} ms to return {} entities" , stop - start , count ) ; }
protected void singleOrderBySameRangeScanGreater ( IoHelper io ) throws Exception { io . doSetup ( ) ; int size = 20 ; int queryLimit = 10 ; int startValue = 9 ; long start = System . currentTimeMillis ( ) ; logger . info ( "Writing {} entities." , size ) ; List < String > expected = new ArrayList < String > ( size ) ; code_block = ForStatement ; app . waitForQueueDrainAndRefreshIndex ( ) ; Thread . sleep ( 500 ) ; long stop = System . currentTimeMillis ( ) ; logger . info ( "Writes took {} ms" , stop - start ) ; Query query = Query . fromQL ( "select * where index >= " + startValue + " order by index desc" ) ; query . setLimit ( queryLimit ) ; int count = 0 ; start = System . currentTimeMillis ( ) ; Results results ; do code_block = "" ; while ( results . hasCursor ( ) ) ; assertEquals ( expected . size ( ) - startValue , count ) ; stop = System . currentTimeMillis ( ) ; logger . info ( "Query took {} ms to return {} entities" , stop - start , count ) ; }
public void test() { try { conn = dataSource . getConnection ( ) ; stmt = conn . prepareStatement ( getUserSQL , ResultSet . TYPE_FORWARD_ONLY , ResultSet . CONCUR_READ_ONLY ) ; stmt . setFetchDirection ( ResultSet . FETCH_FORWARD ) ; stmt . setFetchSize ( getFetchSize ( ) ) ; setLongParameter ( stmt , 1 , userID ) ; log . debug ( "Executing SQL query: {}" , getUserSQL ) ; rs = stmt . executeQuery ( ) ; FastIDSet result = new FastIDSet ( ) ; code_block = WhileStatement ; code_block = IfStatement ; return result ; } catch ( SQLException sqle ) { log . warn ( "Exception while retrieving item s" , sqle ) ; throw new TasteException ( sqle ) ; } finally { IOUtils . quietClose ( rs , stmt , conn ) ; } }
public void test() { try { conn = dataSource . getConnection ( ) ; stmt = conn . prepareStatement ( getUserSQL , ResultSet . TYPE_FORWARD_ONLY , ResultSet . CONCUR_READ_ONLY ) ; stmt . setFetchDirection ( ResultSet . FETCH_FORWARD ) ; stmt . setFetchSize ( getFetchSize ( ) ) ; setLongParameter ( stmt , 1 , userID ) ; log . debug ( "Executing SQL query: {}" , getUserSQL ) ; rs = stmt . executeQuery ( ) ; FastIDSet result = new FastIDSet ( ) ; code_block = WhileStatement ; code_block = IfStatement ; return result ; } catch ( SQLException sqle ) { log . warn ( "Exception while retrieving item s" , sqle ) ; throw new TasteException ( sqle ) ; } finally { IOUtils . quietClose ( rs , stmt , conn ) ; } }
public void test() { try { Field [ ] fields = o . getClass ( ) . getDeclaredFields ( ) ; code_block = ForStatement ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { code_block = WhileStatement ; } catch ( InterruptedException e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { logSafe ( "Stopping store {}-{}..." , topic , partitionGroup ) ; logSafe ( "Waiting for flush finished {}-{}..." , topic , partitionGroup ) ; code_block = TryStatement ;  stopWriteThread ( ) ; logSafe ( "Stopping flush thread {}-{}..." , topic , partitionGroup ) ; stopFlushThread ( ) ; flushCheckpoint ( ) ; code_block = IfStatement ; System . out . println ( "Store stopped. " + base . getAbsolutePath ( ) ) ; logSafe ( "Store stopped {}-{}." , topic , partitionGroup ) ; } catch ( Throwable t ) { logger . error ( t . getMessage ( ) , t ) ; } }
@ GET @ Produces ( MediaType . APPLICATION_XML ) public Response getProviders ( ) { logger . debug ( "StartOf getProviders - REQUEST for /providers" ) ; ProviderHelper providerRestService = getProviderHelper ( ) ; String serializedProviders = null ; code_block = TryStatement ;  logger . debug ( "EndOf getTemplates" ) ; return buildResponse ( 200 , serializedProviders ) ; }
public void test() { try { serializedProviders = providerRestService . getProviders ( ) ; } catch ( HelperException e ) { logger . info ( "getTemplates exception:" + e . getMessage ( ) ) ; return buildResponse ( e ) ; } }
@ GET @ Produces ( MediaType . APPLICATION_XML ) public Response getProviders ( ) { logger . debug ( "StartOf getProviders - REQUEST for /providers" ) ; ProviderHelper providerRestService = getProviderHelper ( ) ; String serializedProviders = null ; code_block = TryStatement ;  logger . debug ( "EndOf getTemplates" ) ; return buildResponse ( 200 , serializedProviders ) ; }
public String getApiKey ( ) { log . info ( "Query API Key" ) ; return readyElement ( apiKeyLabel ) . getAttribute ( "value" ) ; }
public void test() { if ( myPayload . getTimeBudget ( ) . isExpired ( ) ) { StringBuilder error = new StringBuilder ( 150 ) ; error . append ( "Timed out waiting to " ) ; int numServicesWaiting = 0 ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; error . append ( numServicesWaiting > 1 ? "responses" : "response" ) . append ( " from Server: " ) . append ( mySource . getName ( ) ) ; error . append ( ".  Consider increasing timeout settings in Edit->Settings->Servers." ) ; LOGGER . warn ( error . toString ( ) ) ; sendResponseAndCleanup ( error . toString ( ) ) ; } else { myResponseTimer . setInitialDelay ( myPayload . getTimeBudget ( ) . getRemainingMilliseconds ( ) ) ; myResponseTimer . start ( ) ; } }
@ Override public void delete ( ConnectionParameterKey metadataKey ) { LOGGER . trace ( MessageFormat . format ( "Deleting Connection {0}." , metadataKey . toString ( ) ) ) ; code_block = IfStatement ; String deleteStatement = deleteStatement ( metadataKey ) ; getMetadataRepository ( ) . executeUpdate ( deleteStatement ) ; }
public void test() { try ( Session session = modelDBHibernateUtil . getSessionFactory ( ) . openSession ( ) ) { Query < ? > query = session . createQuery ( GET_PROJECT_EXPERIMENTS_COUNT_HQL ) ; query . setParameterList ( ModelDBConstants . PROJECT_IDS , projectIds ) ; Long count = ( Long ) query . uniqueResult ( ) ; LOGGER . debug ( "Experiment Count : {}" , count ) ; return count ; } catch ( Exception ex ) { code_block = IfStatement ; } }
public void test() { try { records = this . getActionLogDAO ( ) . getActionRecords ( searchBean ) ; } catch ( Throwable t ) { _logger . error ( "Error loading actionlogger records" , t ) ; throw new ApsSystemException ( "Error loading actionlogger records" , t ) ; } }
@ Before public void setUp ( ) { LOG . debug ( "Starting..." ) ; testPath = "file:///tmp/flume-test." + Calendar . getInstance ( ) . getTimeInMillis ( ) + "." + Thread . currentThread ( ) . getId ( ) ; sink = new HDFSEventSink ( ) ; sink . setName ( "HDFSEventSink-" + UUID . randomUUID ( ) . toString ( ) ) ; dirCleanup ( ) ; }
public void test() { if ( null != exchangeFound ) { LOG . info ( "saveExchange--updated-exchangeFound" ) ; exchangeFound . setName ( exchangeUpdate . getName ( ) ) ; exchangeFound . setDisabled ( exchangeUpdate . isDisabled ( ) ) ; exchangeFound . setKey ( exchangeUpdate . getKey ( ) ) ; exchangeFound . setTLSVersions ( exchangeUpdate . getTLSVersions ( ) ) ; exchangeFound . setType ( exchangeUpdate . getType ( ) ) ; exchangeFound . setUrl ( exchangeUpdate . getUrl ( ) ) ; exchangeFound . setSniName ( exchangeUpdate . getSniName ( ) ) ; exchangeFound . setCertificateAlias ( exchangeUpdate . getCertificateAlias ( ) ) ; } else { LOG . info ( "saveExchange--adding-exchangeUpdate" ) ; exchanges . add ( exchangeUpdate ) ; } }
public void test() { try { code_block = IfStatement ; List < ExchangeType > exchanges = ExchangeManagerHelper . getAllExchanges ( exInfo , true ) ; String nameLookup = originalExchangeName != null ? originalExchangeName : exchangeUpdate . getName ( ) ; ExchangeType exchangeFound = ExchangeManagerHelper . findExchangeTypeBy ( exchanges , nameLookup ) ; code_block = IfStatement ; saveExchangeInfo ( ) ; bSave = true ; } catch ( ExchangeManagerException e ) { LOG . error ( "unable to save-exchangeType: {}" , e . getLocalizedMessage ( ) , e ) ; } }
@ ActivityStreamAuditable @ RestAccessControl ( permission = Permission . MANAGE_PAGES ) @ RequestMapping ( value = "/pages/{pageCode}" , method = RequestMethod . DELETE , produces = MediaType . APPLICATION_JSON_VALUE ) public ResponseEntity < SimpleRestResponse < ? > > deletePage ( @ ModelAttribute ( "user" ) UserDetails user , @ PathVariable String pageCode ) throws ApsSystemException { logger . debug ( "deleting {}" , pageCode ) ; code_block = IfStatement ; DataBinder binder = new DataBinder ( pageCode ) ; BindingResult bindingResult = binder . getBindingResult ( ) ; code_block = IfStatement ; getPageValidator ( ) . validateOnlinePage ( pageCode , bindingResult ) ; code_block = IfStatement ; getPageValidator ( ) . validateChildren ( pageCode , bindingResult ) ; code_block = IfStatement ; this . getPageService ( ) . removePage ( pageCode ) ; Map < String , String > payload = new HashMap < > ( ) ; payload . put ( "code" , pageCode ) ; return new ResponseEntity < > ( new SimpleRestResponse < > ( payload ) , HttpStatus . OK ) ; }
private void testBookieRecoveryToRandomBookies ( boolean async , int numBookiesToKill ) throws Exception { int numLedgers = 3 ; List < LedgerHandle > lhs = createLedgers ( numLedgers , 3 , 3 ) ; int numMsgs = 10 ; writeEntriestoLedgers ( numMsgs , 0 , lhs ) ; LOG . info ( "Finished writing all ledger entries so shutdown {} bookies." , numBookiesToKill ) ; Set < BookieSocketAddress > bookiesSrc = new HashSet < BookieSocketAddress > ( ) ; code_block = ForStatement ; code_block = ForStatement ; writeEntriestoLedgers ( numMsgs , 10 , lhs ) ; LOG . info ( "Now recover the data on the killed bookie (" + bookiesSrc + ") and replicate it to a random available one" ) ; code_block = IfStatement ; verifyLedgerMetadata ( lhs , bookiesSrc ) ; verifyRecoveredLedgers ( lhs , 2 * numMsgs - 1 ) ; }
private void testBookieRecoveryToRandomBookies ( boolean async , int numBookiesToKill ) throws Exception { int numLedgers = 3 ; List < LedgerHandle > lhs = createLedgers ( numLedgers , 3 , 3 ) ; int numMsgs = 10 ; writeEntriestoLedgers ( numMsgs , 0 , lhs ) ; LOG . info ( "Finished writing all ledger entries so shutdown {} bookies." , numBookiesToKill ) ; Set < BookieSocketAddress > bookiesSrc = new HashSet < BookieSocketAddress > ( ) ; code_block = ForStatement ; code_block = ForStatement ; writeEntriestoLedgers ( numMsgs , 10 , lhs ) ; LOG . info ( "Now recover the data on the killed bookie (" + bookiesSrc + ") and replicate it to a random available one" ) ; code_block = IfStatement ; verifyLedgerMetadata ( lhs , bookiesSrc ) ; verifyRecoveredLedgers ( lhs , 2 * numMsgs - 1 ) ; }
public void test() { try { renderEditViewForId ( request , response , id ) ; } catch ( TException e ) { log . error ( "Thrift error" , e ) ; } }
public void test() { if ( networkModel == null ) { log . warn ( "Failed to obtain network statistics. Model is not loaded yet." ) ; } else { code_block = IfStatement ; } }
public void test() { if ( networkModel . getTopology ( ) == null ) { log . warn ( "Failed to obtain network statistics. Topology is not loaded yet." ) ; } else { code_block = ForStatement ; code_block = IfStatement ; } }
public void test() { try { SwitchPortStatistics switchPortStatistics = getSwitchPortStatisticsForNetworkElement ( ne , networkModel ) ; netStats . addPortSwitchStatistic ( ne . getId ( ) , switchPortStatistics ) ; } catch ( Exception e ) { log . warn ( "Failed to obtain port statistics for network element" + ne . getId ( ) + ". Skipping it." , e ) ; } }
public void test() { if ( netStats . getSwitchStatistics ( ) . isEmpty ( ) ) { log . error ( "No matching port statistics" ) ; } }
@ Override public void init ( ) { logger . debug ( "Initializing MQTT skeleton (ownGbid={}) ..." , ownGbid ) ; messageRouter . registerMessageProcessedListener ( this ) ; mqttClient . setMessageListener ( this ) ; mqttClient . start ( ) ; mqttClientFactory . createSender ( ownGbid ) . start ( ) ; subscribe ( ) ; }
public void test() { try { LOGGER . debug ( "Creating JAXB context with context path: {}." , contextPath ) ; jaxbContext = JAXBContext . newInstance ( contextPath , CswJAXBElementProvider . class . getClassLoader ( ) ) ; } catch ( JAXBException e ) { LOGGER . info ( "Unable to create JAXB context using contextPath: {}." , contextPath , e ) ; } }
public void test() { try { String agentDaemonQueueName = ApplicationConfigProvider . getInstance ( ) . getAgentDetails ( ) . getAgentPkgQueue ( ) ; code_block = IfStatement ; agentConfigDAL . updateAgentRunningStatus ( agentId , AGENTACTION . valueOf ( action ) ) ; } catch ( Exception e ) { log . error ( "Error while agent {} " , action , e ) ; throw new InsightsCustomException ( e . toString ( ) ) ; } }
public void test() { try ( final Writer writer = new BufferedWriter ( new OutputStreamWriter ( out . stream ( ) , Constants . CHARSET_UTF_8 ) ) ) { writeCall . accept ( writer ) ; writer . flush ( ) ; } catch ( final Exception e ) { logger . warn ( "Failed to write {} to response." , id , e ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Retrieving business entities from UDDI using find_business web service call." ) ; } }
public void test() { try { UDDIFindBusinessProxyObjectFactory uddiFactory = new UDDIFindBusinessProxyObjectFactory ( ) ; UDDIFindBusinessProxy uddiProxy = uddiFactory . getUDDIBusinessInfoProxy ( ) ; businessList = uddiProxy . findBusinessesFromUDDI ( exchange ) ; removeIgnoredBusinesses ( businessList ) ; } catch ( UDDIFindBusinessException e ) { String sErrorMessage = "Failed to call 'find_business' web service on the NHIN UDDI server.  Error: " + e . getMessage ( ) ; LOG . error ( sErrorMessage , e ) ; throw new UDDIAccessorException ( sErrorMessage , e ) ; } }
public void test() { try { logger . debug ( "validating widget {} for page {}" , widget . getCode ( ) , page . getCode ( ) ) ; WidgetValidatorCmsHelper . validateTitle ( widget , getLangManager ( ) , bindingResult ) ; WidgetValidatorCmsHelper . validateLink ( widget , getLangManager ( ) , getPageManager ( ) , bindingResult ) ; this . validateContentType ( widget , bindingResult ) ; this . validateFilters ( widget , bindingResult ) ; this . validateContentModel ( widget , bindingResult ) ; } catch ( Throwable e ) { logger . error ( "error in validate wiget {} in page {}" , widget . getCode ( ) , page . getCode ( ) ) ; throw new RestServerError ( "error in widget config validation" , e ) ; } }
public void test() { try { logger . debug ( "validating widget {} for page {}" , widget . getCode ( ) , page . getCode ( ) ) ; WidgetValidatorCmsHelper . validateTitle ( widget , getLangManager ( ) , bindingResult ) ; WidgetValidatorCmsHelper . validateLink ( widget , getLangManager ( ) , getPageManager ( ) , bindingResult ) ; this . validateContentType ( widget , bindingResult ) ; this . validateFilters ( widget , bindingResult ) ; this . validateContentModel ( widget , bindingResult ) ; } catch ( Throwable e ) { logger . error ( "error in validate wiget {} in page {}" , widget . getCode ( ) , page . getCode ( ) ) ; throw new RestServerError ( "error in widget config validation" , e ) ; } }
public void test() { if ( Message . RESERVED_FIELDS . contains ( csfr . key ( ) ) && ! Message . RESERVED_SETTABLE_FIELDS . contains ( csfr . key ( ) ) ) { final String message = "Cannot add static field. Field [" + csfr . key ( ) + "] is reserved." ; LOG . error ( message ) ; throw new BadRequestException ( message ) ; } }
public void test() { { checkPermission ( RestPermissions . INPUTS_EDIT , inputId ) ; final MessageInput input = persistedInputs . get ( inputId ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; input . addStaticField ( csfr . key ( ) , csfr . value ( ) ) ; final Input mongoInput = inputService . find ( input . getPersistId ( ) ) ; inputService . addStaticField ( mongoInput , csfr . key ( ) , csfr . value ( ) ) ; final String msg = "Added static field [" + csfr . key ( ) + "] to input <" + inputId + ">." ; LOG . info ( msg ) ; activityWriter . write ( new Activity ( msg , StaticFieldsResource . class ) ) ; final URI inputUri = getUriBuilderToSelf ( ) . path ( InputsResource . class ) . path ( "{inputId}" ) . build ( mongoInput . getId ( ) ) ; return Response . created ( inputUri ) . build ( ) ; } }
public void test() { if ( provider instanceof ModifiableServerProvider ) { ( ( ModifiableServerProvider < StreamingServer > ) provider ) . addServer ( myServer ) ; } else { LOGGER . warn ( "Could not find a streaming server provider." ) ; } }
public void test() { try { aquisitionDate = sdf . parse ( input . get ( "acquisitionDate" ) ) ; featureBuilder . add ( aquisitionDate ) ; } catch ( final ParseException e ) { LOGGER . warn ( "Unable to parse aquisition date" , e ) ; featureBuilder . add ( null ) ; } }
public void test() { if ( userToken . expiresWithin ( EXPIRY_THRESHOLD_MINUTES ) ) { logger . debug ( "User already has an expired token (or at least a " + "token that expires within the next " + EXPIRY_THRESHOLD_MINUTES + " minutes). This token " + "will be deleted." ) ; dao . delete ( userToken ) ; } else { logger . debug ( "Returning existing token for user '" + user . getAccountName ( ) + "'" ) ; return userToken ; } }
protected E getValidTokenForUser ( User user , Integer expirationTimeInMinutes ) throws NoSuchMethodException , SecurityException , InstantiationException , IllegalAccessException , IllegalArgumentException , InvocationTargetException { E userToken = findByUser ( user ) ; code_block = IfStatement ; userToken = buildConcreteInstance ( user , expirationTimeInMinutes ) ; dao . saveOrUpdate ( userToken ) ; final String tokenType = userToken . getClass ( ) . getSimpleName ( ) ; logger . debug ( "Successfully created a user token of type '" + tokenType + "' for user '" + user . getAccountName ( ) + "'" ) ; return userToken ; }
private void scheduledLogStatus ( ) { log . debug ( printStatus ( ) ) ; scheduleRunAsync ( this :: scheduledLogStatus , STATUS_LOG_INTERVAL_MS , TimeUnit . MILLISECONDS ) ; }
public void test() { try { String port = "COM10" ; LoggingFactory . init ( Level . INFO ) ; MrlComm arduino = ( MrlComm ) Runtime . start ( "arduino" , "MrlComm" ) ; Servo servo01 = ( Servo ) Runtime . start ( "servo01" , "Servo" ) ; } catch ( Exception e ) { log . error ( "main threw" , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( exception , ex ) ; } }
public void test() { for ( Map < String , Object > location : locations ) { Entity entity = em . create ( "store" , location ) ; assertNotNull ( entity ) ; logger . debug ( "Entity {} created" , entity . getProperty ( "name" ) ) ; } }
public void test() { try { response = getIRODSProtocol ( ) . irodsFunction ( specificQueryInp ) ; } catch ( DataNotFoundException e ) { log . debug ( "no results from iRODS, return as an empty result set" ) ; return new SpecificQueryResultSet ( specificQuery , specificQueryDefinition . getColumnNames ( ) ) ; } }
private SpecificQueryResultSet queryOnAliasGivenDefinition ( final SpecificQuery specificQuery , final int maxRows , final SpecificQueryDefinition specificQueryDefinition , final int userDefinedOffset ) throws JargonException { SpecificQueryInp specificQueryInp = SpecificQueryInp . instance ( specificQuery . getArguments ( ) , specificQuery . getQueryString ( ) , maxRows , specificQuery . getContinuationValue ( ) , specificQuery . getZoneHint ( ) ) ; Tag response = null ; code_block = TryStatement ;  int continuation = QueryResultProcessingUtils . getContinuationValue ( response ) ; boolean hasMoreRecords = false ; code_block = IfStatement ; List < IRODSQueryResultRow > resultRows = QueryResultProcessingUtils . translateResponseIntoResultSet ( response , specificQueryDefinition . getColumnNames ( ) , continuation , userDefinedOffset ) ; SpecificQueryResultSet results = new SpecificQueryResultSet ( specificQuery , resultRows , specificQueryDefinition . getColumnNames ( ) , hasMoreRecords , continuation ) ; log . debug ( "doing a close for this page..." ) ; closeResultSet ( results ) ; return results ; }
public void test() { for ( int a = 2 ; ; a ++ ) { LOG . info ( "Test a=" + a ) ; BigInteger aPow = BigInteger . valueOf ( a ) . pow ( m ) ; long nthRoot = Roots . ithRoot ( aPow , n ) [ 0 ] . longValue ( ) ; long bMin = Math . max ( 0 , nthRoot - b_COUNT ) ; long bMax = nthRoot + b_COUNT ; code_block = ForStatement ; code_block = ForStatement ; code_block = ForStatement ; } }
public void test() { try { code_block = ForStatement ; } catch ( Exception e ) { LOG . error ( "Error while substituting variables" , e ) ; } }
public void test() { try { DLAppServiceUtil . checkOutFileEntry ( fileEntryId , serviceContext ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Dis-enrolling sampledevice device : " + deviceId ) ; } }
public void test() { try { DeviceTypeDAO . rollbackTransaction ( ) ; } catch ( DeviceMgtPluginException iotDAOEx ) { String msg = "Error occurred while roll back the device dis enrol transaction :" + deviceId . toString ( ) ; log . warn ( msg , iotDAOEx ) ; } }
public void test() { try { code_block = IfStatement ; DeviceTypeDAO . beginTransaction ( ) ; status = deviceTypeDAO . getDeviceTypeDAO ( ) . deleteDevice ( deviceId . getId ( ) ) ; DeviceTypeDAO . commitTransaction ( ) ; } catch ( DeviceMgtPluginException e ) { code_block = TryStatement ;  String msg = "Error while removing the sampledevice device : " + deviceId . getId ( ) ; log . error ( msg , e ) ; throw new DeviceManagementException ( msg , e ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Outbound Websocket frame is throttled. " + ctx . channel ( ) . toString ( ) ) ; } }
public void test() { try { invokable . triggerCheckpointAsync ( checkpointMetaData , checkpointOptions ) ; } catch ( RejectedExecutionException ex ) { LOG . debug ( "Triggering checkpoint {} for {} ({}) was rejected by the mailbox" , checkpointID , taskNameWithSubtask , executionId ) ; } catch ( Throwable t ) { code_block = IfStatement ; } }
public void test() { if ( getExecutionState ( ) == ExecutionState . RUNNING ) { failExternally ( new Exception ( "Error while triggering checkpoint " + checkpointID + " to " + taskNameWithSubtask , t ) ) ; } else { LOG . debug ( "Encountered error while triggering checkpoint {} to " + "{} ({}) while being not in state running." , checkpointID , taskNameWithSubtask , executionId , t ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( IngestException e ) { INGEST_LOGGER . error ( "Error updating metacards for CatalogStore {}" , store . getId ( ) , e ) ; exceptions . add ( new ProcessingDetailsImpl ( store . getId ( ) , e ) ) ; } }
public void test() { if ( clientHeartbeat == null ) { return super . sendHeartBeat ( ) ; } }
@ Override public Map < String , Object > getServiceBindingParameters ( UUID guid ) { logger . debug ( Messages . GETTING_PARAMETERS_OF_SERVICE_BINDING_0 , guid ) ; return delegate . getServiceBindingParameters ( guid ) ; }
public void test() { if ( length != null ) { LOG . debug ( "Removing file [{0}] with length [{1}]." , name , length ) ; long blocks = length / _blockSize ; _store . delete ( new BytesRef ( name + LENGTH ) ) ; _store . delete ( new BytesRef ( name + LASTMOD ) ) ; code_block = ForStatement ; writeFileNamesAndSync ( ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( illegalArgumentException , illegalArgumentException ) ; } }
public void test() { try { java . util . List < com . liferay . blogs . model . BlogsEntry > returnValue = BlogsEntryServiceUtil . getGroupEntries ( groupId , status , max ) ; return com . liferay . blogs . model . BlogsEntrySoap . toSoapModels ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
void writeSolrConfiguration ( String core ) { File configDir = Paths . get ( this . dataDirectory . getAbsolutePath ( ) , core , "conf" ) . toFile ( ) ; boolean directoriesMade = configDir . mkdirs ( ) ; LOGGER . debug ( "Solr Config directories made?  {}" , directoriesMade ) ; code_block = ForStatement ; }
public void test() { try ( InputStream inputStream = ConfigurationFileProxy . class . getClassLoader ( ) . getResourceAsStream ( "solr/conf/" + filename ) ; FileOutputStream outputStream = new FileOutputStream ( currentFile ) ) { long byteCount = IOUtils . copyLarge ( inputStream , outputStream ) ; LOGGER . debug ( "Wrote out {} bytes for [{}]." , byteCount , filename ) ; } catch ( IOException e ) { LOGGER . warn ( "Unable to copy Solr configuration file: " + filename , e ) ; } }
public void test() { try ( InputStream inputStream = ConfigurationFileProxy . class . getClassLoader ( ) . getResourceAsStream ( "solr/conf/" + filename ) ; FileOutputStream outputStream = new FileOutputStream ( currentFile ) ) { long byteCount = IOUtils . copyLarge ( inputStream , outputStream ) ; LOGGER . debug ( "Wrote out {} bytes for [{}]." , byteCount , filename ) ; } catch ( IOException e ) { LOGGER . warn ( "Unable to copy Solr configuration file: " + filename , e ) ; } }
public void test() { if ( searchService == null ) { logger . warn ( "{} denies to handle request for {} due to missing search service" , this , query ) ; return false ; } else-if ( feedURI == null ) { logger . warn ( "{} denies to handle request for {} since no uri is defined" , this , query ) ; return false ; } else-if ( query . length == 0 ) { logger . debug ( "{} denies to handle unknown request" , this ) ; return false ; } }
public void test() { if ( searchService == null ) { logger . warn ( "{} denies to handle request for {} due to missing search service" , this , query ) ; return false ; } else-if ( feedURI == null ) { logger . warn ( "{} denies to handle request for {} since no uri is defined" , this , query ) ; return false ; } else-if ( query . length == 0 ) { logger . debug ( "{} denies to handle unknown request" , this ) ; return false ; } }
public void test() { if ( searchService == null ) { logger . warn ( "{} denies to handle request for {} due to missing search service" , this , query ) ; return false ; } else-if ( feedURI == null ) { logger . warn ( "{} denies to handle request for {} since no uri is defined" , this , query ) ; return false ; } else-if ( query . length == 0 ) { logger . debug ( "{} denies to handle unknown request" , this ) ; return false ; } }
public void test() { try { Connection conn = null ; code_block = TryStatement ;  code_block = IfStatement ; } catch ( SQLException e ) { logger . info ( "GET JDBC User FAIL. " + e . getMessage ( ) ) ; } }
public void test() { if ( logger . isDebugable ( ) ) { logger . debug ( "Install JDBC Proxy END: " + targetServer + "@" + user + "," + rc , null ) ; } }
public void test() { if ( batchTimeoutSecs <= 0 || batchTimeoutSecs > maxBatchTimeout ) { LOG . debug ( "The configured batch timeout '{}' for sensor type '{}' is <=0 or > the maximum allowable batch timeout '{}'. Setting the batch timeout to the maximum allowable." , batchTimeoutSecs , sensorType , maxBatchTimeout ) ; batchTimeoutSecs = maxBatchTimeout ; } }
@ Test ( timeOut = 10_000 ) public void testZkClientLosingSession ( ) throws Exception { long sessionId = zkClient . getZookeeperClient ( ) . getZooKeeper ( ) . getSessionId ( ) ; byte [ ] sessionPasswd = zkClient . getZookeeperClient ( ) . getZooKeeper ( ) . getSessionPasswd ( ) ; ZooKeeper zk = new ZooKeeper ( ZK_CLUSTER , 1000 , null , sessionId , sessionPasswd ) ; zk . close ( ) ; LOG . info ( "ZKClient session closed" ) ; long previousMaxTimestamp = INITIAL_MAX_TS_VALUE ; code_block = ForStatement ; assertEquals ( storage . getMaxTimestamp ( ) , 1_000_000 * ITERATION_COUNT ) ; }
public void test() { try { byte [ ] utf8 = str . getBytes ( "UTF8" ) ; byte [ ] enc = ecipher . doFinal ( utf8 ) ; return Base64 . encodeBase64String ( enc ) ; } catch ( UnsupportedEncodingException e ) { LOGGER . trace ( "DesEncrypter unsupported encoding exception" , e ) ; throw new ApplicationException ( "DesEncrypter failed - UnsupportedEncodingException " , e ) ; } catch ( GeneralSecurityException e ) { LOGGER . trace ( "DesEncrypter encryption failed" , e ) ; throw new ApplicationException ( "DesEncrypter encryption failed - GeneralSecurityException" , e ) ; } }
public void test() { try { byte [ ] utf8 = str . getBytes ( "UTF8" ) ; byte [ ] enc = ecipher . doFinal ( utf8 ) ; return Base64 . encodeBase64String ( enc ) ; } catch ( UnsupportedEncodingException e ) { LOGGER . trace ( "DesEncrypter unsupported encoding exception" , e ) ; throw new ApplicationException ( "DesEncrypter failed - UnsupportedEncodingException " , e ) ; } catch ( GeneralSecurityException e ) { LOGGER . trace ( "DesEncrypter encryption failed" , e ) ; throw new ApplicationException ( "DesEncrypter encryption failed - GeneralSecurityException" , e ) ; } }
private BuildSetTask buildProjectsAndWaitForUpdates ( BuildConfigurationSet buildConfigurationSet , BuildCoordinator buildCoordinator , int nStatusUpdates , Consumer < BuildStatusChangedEvent > onStatusUpdate , List < BuildStatusChangedEvent > receivedStatuses , List < BuildSetStatusChangedEvent > receivedSetStatuses ) throws InterruptedException , CoreException { Consumer < BuildStatusChangedEvent > onStatusUpdateInternal = ( statusUpdate ) code_block = LoopStatement ; ; final Semaphore semaphore = registerReleaseListenersAndAcquireSemaphore ( onStatusUpdateInternal , nStatusUpdates ) ; final Semaphore buildSetSemaphore = registerBuildSetListeners ( receivedSetStatuses , BUILD_SET_STATUS_UPDATES ) ; BuildOptions buildOptions = new BuildOptions ( ) ; buildOptions . setRebuildMode ( RebuildMode . FORCE ) ; BuildSetTask buildSetTask = buildCoordinator . build ( buildConfigurationSet , MockUser . newTestUser ( 1 ) , buildOptions ) ; assertBuildStartedSuccessfully ( buildSetTask ) ; log . info ( "Waiting to receive all {} status updates..." , nStatusUpdates ) ; waitForStatusUpdates ( nStatusUpdates , semaphore , "" ) ; log . debug ( "All status updates should be received. Semaphore has {} free entries." , semaphore . availablePermits ( ) ) ; log . info ( "Waiting to receive all {} build set status updates..." , BUILD_SET_STATUS_UPDATES ) ; waitForStatusUpdates ( BUILD_SET_STATUS_UPDATES , buildSetSemaphore , "build set task: " + buildSetTask ) ; log . debug ( "All status updates should be received. Semaphore has {} free entries." , semaphore . availablePermits ( ) ) ; return buildSetTask ; }
private BuildSetTask buildProjectsAndWaitForUpdates ( BuildConfigurationSet buildConfigurationSet , BuildCoordinator buildCoordinator , int nStatusUpdates , Consumer < BuildStatusChangedEvent > onStatusUpdate , List < BuildStatusChangedEvent > receivedStatuses , List < BuildSetStatusChangedEvent > receivedSetStatuses ) throws InterruptedException , CoreException { Consumer < BuildStatusChangedEvent > onStatusUpdateInternal = ( statusUpdate ) code_block = LoopStatement ; ; final Semaphore semaphore = registerReleaseListenersAndAcquireSemaphore ( onStatusUpdateInternal , nStatusUpdates ) ; final Semaphore buildSetSemaphore = registerBuildSetListeners ( receivedSetStatuses , BUILD_SET_STATUS_UPDATES ) ; BuildOptions buildOptions = new BuildOptions ( ) ; buildOptions . setRebuildMode ( RebuildMode . FORCE ) ; BuildSetTask buildSetTask = buildCoordinator . build ( buildConfigurationSet , MockUser . newTestUser ( 1 ) , buildOptions ) ; assertBuildStartedSuccessfully ( buildSetTask ) ; log . info ( "Waiting to receive all {} status updates..." , nStatusUpdates ) ; waitForStatusUpdates ( nStatusUpdates , semaphore , "" ) ; log . debug ( "All status updates should be received. Semaphore has {} free entries." , semaphore . availablePermits ( ) ) ; log . info ( "Waiting to receive all {} build set status updates..." , BUILD_SET_STATUS_UPDATES ) ; waitForStatusUpdates ( BUILD_SET_STATUS_UPDATES , buildSetSemaphore , "build set task: " + buildSetTask ) ; log . debug ( "All status updates should be received. Semaphore has {} free entries." , semaphore . availablePermits ( ) ) ; return buildSetTask ; }
public void test() { if ( ! getOpts ( ) . isDryRun ( ) ) { log . info ( "Writing source file for document {}" , doc . getName ( ) ) ; strat . writeSrcFile ( doc ) ; } else { log . info ( "Writing source file for document {} (skipped due to dry run)" , doc . getName ( ) ) ; } }
public void test() { if ( ! getOpts ( ) . isDryRun ( ) ) { log . info ( "Writing source file for document {}" , doc . getName ( ) ) ; strat . writeSrcFile ( doc ) ; } else { log . info ( "Writing source file for document {} (skipped due to dry run)" , doc . getName ( ) ) ; } }
public void test() { try { signature = generateSignature ( msg , signHashAlgo ) ; } catch ( CryptoException E ) { LOGGER . warn ( "Could not generate Signature! Using empty one instead!" , E ) ; } }
public void test() { try ( Connection con = getDatabaseManager ( ) . getDataSource ( ) . getConnection ( ) ; PreparedStatement pstmt = con . prepareStatement ( "SELECT * FROM dex_offer AS offer where latest = true " + "AND offer.status = 0 AND offer.finish_time < ?" ) ) { int i = 0 ; pstmt . setLong ( ++ i , currentTime ) ; DbIterator < DexOrder > orders = getManyBy ( con , pstmt , true ) ; return CollectionUtil . toList ( orders ) ; } catch ( SQLException ex ) { log . error ( ex . getMessage ( ) , ex ) ; } }
private RepairRun buildRepairRunFromRow ( Row repairRunResult , UUID id ) { LOG . trace ( "buildRepairRunFromRow {} / {}" , id , repairRunResult ) ; Date startTime = repairRunResult . getTimestamp ( "start_time" ) ; Date pauseTime = repairRunResult . getTimestamp ( "pause_time" ) ; Date endTime = repairRunResult . getTimestamp ( "end_time" ) ; return RepairRun . builder ( repairRunResult . getString ( "cluster_name" ) , repairRunResult . getUUID ( "repair_unit_id" ) ) . creationTime ( new DateTime ( repairRunResult . getTimestamp ( "creation_time" ) ) ) . intensity ( repairRunResult . getDouble ( "intensity" ) ) . segmentCount ( repairRunResult . getInt ( "segment_count" ) ) . repairParallelism ( RepairParallelism . fromName ( repairRunResult . getString ( "repair_parallelism" ) ) ) . cause ( repairRunResult . getString ( "cause" ) ) . owner ( repairRunResult . getString ( "owner" ) ) . startTime ( null != startTime ? new DateTime ( startTime ) : null ) . pauseTime ( null != pauseTime ? new DateTime ( pauseTime ) : null ) . endTime ( null != endTime ? new DateTime ( endTime ) : null ) . lastEvent ( repairRunResult . getString ( "last_event" ) ) . runState ( RunState . valueOf ( repairRunResult . getString ( "state" ) ) ) . tables ( repairRunResult . getSet ( "tables" , String . class ) ) . build ( id ) ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( exception , exception ) ; } }
public void test() { { validateBusinessObjectDefinitionTagKey ( businessObjectDefinitionTagKey ) ; BusinessObjectDefinitionTagEntity businessObjectDefinitionTagEntity = getBusinessObjectDefinitionTagEntity ( businessObjectDefinitionTagKey ) ; businessObjectDefinitionTagDao . delete ( businessObjectDefinitionTagEntity ) ; BusinessObjectDefinitionEntity businessObjectDefinitionEntity = businessObjectDefinitionDaoHelper . getBusinessObjectDefinitionEntity ( businessObjectDefinitionTagKey . getBusinessObjectDefinitionKey ( ) ) ; LOGGER . info ( "Modify the business object definition in the search index associated with the business object definition tag being deleted." + " tagTypeCode=\"{}\", tagCode=\"{}\", businessObjectDefinitionId=\"{}\", searchIndexUpdateType=\"{}\"" , businessObjectDefinitionTagEntity . getTag ( ) . getTagType ( ) . getCode ( ) , businessObjectDefinitionTagEntity . getTag ( ) . getTagCode ( ) , businessObjectDefinitionEntity . getId ( ) , SEARCH_INDEX_UPDATE_TYPE_UPDATE ) ; searchIndexUpdateHelper . modifyBusinessObjectDefinitionInSearchIndex ( businessObjectDefinitionEntity , SEARCH_INDEX_UPDATE_TYPE_UPDATE ) ; return createBusinessObjectDefinitionTagFromEntity ( businessObjectDefinitionTagEntity ) ; } }
public void test() { try { String url = conf . get ( DBConfiguration . URL_PROPERTY ) ; code_block = TryStatement ;  Properties properties = ConnectionConfig . getConnectionArguments ( conf . get ( DBUtils . CONNECTION_ARGUMENTS ) , conf . get ( DBConfiguration . USERNAME_PROPERTY ) , conf . get ( DBConfiguration . PASSWORD_PROPERTY ) ) ; connection = DriverManager . getConnection ( url , properties ) ; boolean autoCommitEnabled = conf . getBoolean ( AUTO_COMMIT_ENABLED , false ) ; code_block = IfStatement ; String level = conf . get ( TransactionIsolationLevel . CONF_KEY ) ; LOG . debug ( "Transaction isolation level: {}" , level ) ; connection . setTransactionIsolation ( TransactionIsolationLevel . getLevel ( level ) ) ; } catch ( Exception e ) { throw Throwables . propagate ( e ) ; } }
public void test() { if ( _orphaned . size ( ) > 0 && logger . isInfoEnabled ( ) ) { logger . info ( "orphan = " + _orphaned ) ; } }
public void test() { if ( WARNED_READ_ONLY_ATTRIBUTES . add ( attribute . getName ( ) ) ) { LOG . warn ( message + " (future messages for this sensor logged at trace)" ) ; } else-if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( message ) ; } }
public void test() { if ( WARNED_READ_ONLY_ATTRIBUTES . add ( attribute . getName ( ) ) ) { LOG . warn ( message + " (future messages for this sensor logged at trace)" ) ; } else-if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( message ) ; } }
public void test() { try { MigrationEvent other = migrationEventDao . buildMigrationEvent ( migrationEvent . getMigrationEventId ( ) ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "[refresh]" , e ) ; } }
public void test() { try { out = new PrintWriter ( new FileWriter ( file , true ) ) ; } catch ( IOException e ) { out = new PrintWriter ( new NullWriter ( ) ) ; LOG . error ( "Error while creating process log" , e ) ; } }
public List < DataSourceColumnDto > findAllDataFileColumns ( final Long dataFileId ) { log . debug ( "findAllDataFileColumns() - dataFileId: {}" , dataFileId ) ; final List < DataSourceColumn > result = new ArrayList < > ( ) ; final DataFile dataFile = dataFileService . find ( dataFileId ) ; code_block = ForStatement ; return dataSourceColumnToDataSourceColumnDtoConverter . convertToList ( result ) ; }
public void test() { try { BigDecimal state = new BigDecimal ( parameters [ 1 ] ) ; updateState ( CHANNEL_SWITCH , state . compareTo ( BigDecimal . ZERO ) == 0 ? OnOffType . OFF : OnOffType . ON ) ; } catch ( NumberFormatException e ) { logger . warn ( "Unable to parse update {} {} from CCO {}" , type , Arrays . asList ( parameters ) , integrationId ) ; return ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( _NO_SUCH_ENTITY_WITH_PRIMARY_KEY + primaryKey ) ; } }
@ Override public void status ( WorkloadStatusSnapshot status , RecentThroughputAndDuration recentThroughputAndDuration , long completionTimeAsMilli ) { String statusString ; statusString = ( detailedStatus ) ? formatWithCt ( status . operationCount ( ) , status . runDurationAsMilli ( ) , status . durationSinceLastMeasurementAsMilli ( ) , status . throughput ( ) , recentThroughputAndDuration . throughput ( ) , recentThroughputAndDuration . duration ( ) , completionTimeAsMilli ) : formatWithoutCt ( status . operationCount ( ) , status . runDurationAsMilli ( ) , status . durationSinceLastMeasurementAsMilli ( ) , status . throughput ( ) , recentThroughputAndDuration . throughput ( ) , recentThroughputAndDuration . duration ( ) ) ; logger . info ( statusString ) ; }
public void test() { if ( logger . isLoggable ( Level . INFO ) ) { logger . info ( "[uploadInternal] - received request to upload file " + name ) ; } }
public void test() { if ( logger . isLoggable ( Level . INFO ) ) { logger . info ( "[uploadInternal] - successfuly uploaded file " + name + " [upload key = " + uploadKey + "]" ) ; } }
public void test() { try { DriverManager . deregisterDriver ( driver ) ; } catch ( SQLException e ) { log . info ( "Failed to de-registering driver {}" , driver ) ; } }
@ Path ( "/getAllShouldSucceed" ) @ POST public void getAllShouldSucceed ( ) { LOG . debug ( "Calling OpenstackSwiftContainerResource.getAllShouldSucceed()" ) ; String uri = String . format ( URI_FORMAT , OpenstackConstants . GET_ALL ) ; SwiftContainer [ ] containers = template . requestBody ( uri , null , SwiftContainer [ ] . class ) ; assertNotNull ( containers ) ; assertEquals ( 2 , containers . length ) ; assertEquals ( 100 , containers [ 0 ] . getTotalSize ( ) ) ; assertEquals ( "Test" , containers [ 0 ] . getName ( ) ) ; assertEquals ( "marktwain" , containers [ 1 ] . getName ( ) ) ; }
public void test() { try { String response = send ( params , CMBProperties . getInstance ( ) . getCQSServiceUrl ( ) ) ; return CqsStressTester . deserialize ( response , "QueueUrl" ) . trim ( ) ; } catch ( Exception e ) { logger . error ( "Action=CreateQueue status=error exception=" , e ) ; return null ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "insert HostApplicationMap, host:{}, app:{},SType:{},parentApp:{},parentAppSType{}" , host , bindApplicationName , bindServiceType , parentApplicationName , parentServiceType ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "Iterator not complete and there are results object is null, advancing" ) ; } }
private void getServerProfileTemplateById ( ) { ServerProfileTemplate template = this . serverProfileTemplateClient . getByName ( SERVER_PROFILE_TEMPLATE_NAME ) . get ( 0 ) ; template = serverProfileTemplateClient . getById ( template . getResourceId ( ) ) ; LOGGER . info ( "Server Profile Template object returned to client: {}" , template . toJsonString ( ) ) ; }
public void test() { if ( flowConfigurationFile == null ) { logger . debug ( "Flow Configuration file was null" ) ; return null ; } }
public void test() { if ( ! Files . exists ( flowPath ) || Files . size ( flowPath ) == 0 ) { logger . warn ( "Flow Configuration does not exist or was empty" ) ; return null ; } }
public void test() { try { code_block = IfStatement ; } catch ( IOException e ) { logger . error ( "An error occurred determining the size of the Flow Configuration file" ) ; return null ; } }
protected void generatePojo ( TableDefinition table ) { JavaWriter out = newJavaWriter ( getFile ( table , Mode . POJO ) ) ; log . info ( "Generating POJO" , out . file ( ) . getName ( ) ) ; generatePojo ( table , out ) ; closeJavaWriter ( out ) ; }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( r . toString ( ) ) ; log . trace ( Hexdump . toHexString ( this . sbuf , 4 , size ) ) ; } }
public void test() { try { CompletableFuture < List < IType > > future = subtypesCache . get ( searchParams , ( ) -> client . javaSubTypes ( searchParams ) . handle ( ( results , exception ) -> results . stream ( ) . map ( e -> detailed ? toType ( e . getRight ( ) ) : toTypeFromDescriptor ( e . getLeft ( ) ) ) . collect ( Collectors . toList ( ) ) ) ) ; return Mono . fromFuture ( future ) . flatMapMany ( results -> Flux . fromIterable ( results ) ) ; } catch ( ExecutionException e ) { log . error ( "{}" , e ) ; return Flux . empty ( ) ; } }
@ Override public void execute ( ) throws Exception { logger . debug ( "Executing job command line" ) ; String subCommandString = getParsedSubCommand ( jobCommandOptions . jCommander ) ; configure ( ) ; code_block = SwitchStatement ; }
public void test() { switch ( subCommandString ) { case "secondary-index" : secondaryIndex ( ) ; break ; default : logger . error ( "Subcommand not valid" ) ; break ; } }
public void test() { try { final Cipher cipher = cipherFactory . initCipher ( Cipher . ENCRYPT_MODE ) ; writeEncryptionParametersToStream ( os , cipher ) ; @ SuppressWarnings ( "resource" ) final CipherOutputStream cos = new CipherOutputStream ( os , cipher ) ; os = cos ; } catch ( final CipherException e ) { LOGGER . error ( "Failed to encrypt preferences: " + e , e ) ; return ; } }
public void test() { if ( success && ( aFile . exists ( ) && ! aFile . delete ( ) || ! temp . renameTo ( aFile ) ) ) { LOGGER . warn ( "Failed to rename preferences temp file [" + temp + "] to [" + aFile + "]: preferences were not saved correctly." ) ; } else-if ( ! temp . delete ( ) && LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Failed to delete temp file: " + temp ) ; } }
public void test() { if ( success && ( aFile . exists ( ) && ! aFile . delete ( ) || ! temp . renameTo ( aFile ) ) ) { LOGGER . warn ( "Failed to rename preferences temp file [" + temp + "] to [" + aFile + "]: preferences were not saved correctly." ) ; } else-if ( ! temp . delete ( ) && LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Failed to delete temp file: " + temp ) ; } }
public void test() { if ( ! singleSuggestionMode && bestEvaluatedDescriptions . getBestAccuracy ( ) > currentHighestAccuracy ) { currentHighestAccuracy = bestEvaluatedDescriptions . getBestAccuracy ( ) ; expressionTestCountLastImprovement = expressionTests ; timeLastImprovement = System . nanoTime ( ) ; long durationInMillis = getCurrentRuntimeInMilliSeconds ( ) ; String durationStr = getDurationAsString ( durationInMillis ) ; logger . info ( "more accurate (" + dfPercent . format ( currentHighestAccuracy ) + ") class expression found after " + durationStr + ": " + descriptionToString ( bestEvaluatedDescriptions . getBest ( ) . getDescription ( ) ) ) ; } }
@ Override public void onException ( ChunkedOutput < OutboundEvent > chunkedOutput , Exception exception ) { super . onException ( chunkedOutput , exception ) ; LOG . debug ( "[{}] SSE exception" , this ) ; }
public void test() { try { getResponse ( ) . sendError ( 500 ) ; } catch ( Throwable t2 ) { LOG . error ( "While sending error for {}" , artifactDownloadRequest . getTargetDirectory ( ) , t2 ) ; } }
public void test() { try { keyInfo = SAML1ComponentBuilder . createKeyInfo ( keyInfoBean ) ; } catch ( SecurityException | WSSecurityException e ) { LOG . error ( e . getLocalizedMessage ( ) , e ) ; throw new SAMLComponentBuilderException ( e . getLocalizedMessage ( ) , e ) ; } }
public void test() { try { fsm . fire ( event ) ; return event ; } catch ( Exception e ) { log . error ( "onStatus threw" , e ) ; } }
public void test() { { LOG . error ( "schedule is not supported on Server.Please run your operation on Prism " ) ; throw FalconWebException . newAPIException ( "schedule is not supported on Server. Please run your operation " + "on Prism." ) ; } }
public void test() { if ( offset < 0 ) { log . error ( "offset < 0 in transfer get() operation, return from get method" ) ; return ; } else-if ( offset > 0 ) { code_block = IfStatement ; code_block = TryStatement ;  } }
public void test() { try { code_block = IfStatement ; local . seek ( offset ) ; } catch ( Exception e ) { log . error ( IO_EXEPTION_IN_PARALLEL_TRANSFER , parallelGetFileTransferStrategy . toString ( ) ) ; throw new JargonException ( IO_EXCEPTION_OCCURRED_DURING_PARALLEL_FILE_TRANSFER , e ) ; } }
private void processDisambiguatedOrg ( OrgDisambiguatedEntity entity ) { LOGGER . info ( "About to index disambiguated org, id={}" , entity . getId ( ) ) ; OrgDisambiguatedSolrDocument document = convertEntityToDocument ( entity ) ; code_block = IfStatement ; orgDisambiguatedDao . updateIndexingStatus ( entity . getId ( ) , IndexingStatus . DONE ) ; }
public void test() { if ( ! messaging . send ( document , updateSolrQueueName ) ) { LOGGER . error ( "Unable to send orgs disambiguated message for org: " + document . getOrgDisambiguatedName ( ) + "(" + document . getOrgDisambiguatedId ( ) + ")" ) ; orgDisambiguatedDao . updateIndexingStatus ( entity . getId ( ) , IndexingStatus . FAILED ) ; return ; } }
public void test() { -> { LOGGER . info ( CALLED ) ; latch . countDown ( ) ; return rs ; } }
public void test() { try { final List < ScheduleMeasurements_args > list = new ArrayList < ScheduleMeasurements_args > ( ) ; argQueue . drainTo ( list ) ; code_block = IfStatement ; Thread . sleep ( 30000 ) ; } catch ( InterruptedException e ) { _log . debug ( e , e ) ; } }
public void test() { try { code_block = WhileStatement ; } catch ( Throwable t ) { _log . error ( t , t ) ; } }
public Boolean apply ( WebDriver driver ) { driver . manage ( ) . window ( ) . maximize ( ) ; LOGGER . debug ( "Browser window size was maximized!" ) ; return true ; }
public void test() { try { AddPDXTypeOp . execute ( ( ExecutablePool ) pool , id , type ) ; } catch ( ServerConnectivityException serverConnectivityException ) { logger . debug ( "Received an exception sending pdx type to pool {}, {}" , pool , serverConnectivityException . getMessage ( ) , serverConnectivityException ) ; throw serverConnectivityException ; } }
public void test() { if ( tryPort == 0 ) { LOG . warn ( "Timeline server could not bind on a random free port." ) ; } else { LOG . warn ( String . format ( "Timeline server could not bind on port %d. " + "Attempting port %d + 1." , tryPort , tryPort ) ) ; } }
public void test() { if ( tryPort == 0 ) { LOG . warn ( "Timeline server could not bind on a random free port." ) ; } else { LOG . warn ( String . format ( "Timeline server could not bind on port %d. " + "Attempting port %d + 1." , tryPort , tryPort ) ) ; } }
public void test() { if ( e . getMessage ( ) != null && e . getMessage ( ) . contains ( "Failed to bind to" ) ) { code_block = IfStatement ; } else { LOG . warn ( String . format ( "Timeline server start failed on port %d. Attempting port %d + 1." , tryPort , tryPort ) , e ) ; } }
@ Test public void testObjectToString ( ) { Foo arg = new Foo ( ) ; expect ( mockLog . isLevelEnabled ( Level . TRACE ) ) . andReturn ( true ) ; mockLog . log ( Level . TRACE , Foo . TO_STRING ) ; replay ( mockLog ) ; logger . trace ( "{}" , arg ) ; verify ( mockLog ) ; }
public void test() { try { Hits hits = _assetHelper . search ( searchContext , assetEntryQuery , assetEntryQuery . getStart ( ) , assetEntryQuery . getEnd ( ) ) ; return _assetHelper . getAssetEntries ( hits ) ; } catch ( Exception exception ) { _log . error ( "Unable to get asset entries" , exception ) ; } }
public void testOpenL ( ) throws SyntaxNodeException { boolean b ; long t = System . currentTimeMillis ( ) ; b = executeBooleanOpenLExprression ( data , OPENL_EXPR ) ; assertTrue ( b ) ; b = executeBooleanOpenLExprression ( data , NEG_OPENL_EXPR ) ; assertTrue ( b ) ; log . info ( "TestOpenL: Elapsed time = {}." , System . currentTimeMillis ( ) - t ) ; }
public void test() { try { pin . open ( ) ; } catch ( KuraGPIODeviceException | KuraUnavailableDeviceException | IOException e ) { logger . error ( "Unable to open GPIO resource {}" , pin . getName ( ) , e ) ; } }
public void test() { if ( debug ) { log . debug ( "Logged in as '" + user + "'" ) ; } }
public void test() { try { String filename = CommandHistory . getHistorySaver ( workspace . getId ( ) ) . getHistoryFilepath ( worksheetId ) ; JSONArray historyJson = CommandHistory . getHistorySaver ( workspace . getId ( ) ) . loadHistory ( filename ) ; filteredHistoryJson = HistoryJsonUtil . filterCommandsByTag ( tag , historyJson ) ; } catch ( JSONException e ) { logger . error ( "Error occured while working with JSON!" , e ) ; } catch ( Exception e ) { logger . error ( "Error reading from history file!" , e ) ; } }
public void test() { try { String filename = CommandHistory . getHistorySaver ( workspace . getId ( ) ) . getHistoryFilepath ( worksheetId ) ; JSONArray historyJson = CommandHistory . getHistorySaver ( workspace . getId ( ) ) . loadHistory ( filename ) ; filteredHistoryJson = HistoryJsonUtil . filterCommandsByTag ( tag , historyJson ) ; } catch ( JSONException e ) { logger . error ( "Error occured while working with JSON!" , e ) ; } catch ( Exception e ) { logger . error ( "Error reading from history file!" , e ) ; } }
public void test() { if ( userMessageLogger != null ) { userMessageLogger . debug ( pattern , arguments ) ; } }
public void test() { try { directService . updateDomain ( updateDomain ) ; selectedDomain = null ; refreshDomains ( ) ; } catch ( DomainException domainException ) { FacesContext . getCurrentInstance ( ) . validationFailed ( ) ; FacesContext . getCurrentInstance ( ) . addMessage ( "domainEditErrors" , new FacesMessage ( FacesMessage . SEVERITY_ERROR , "Cannot update domain: " + domainException . getLocalizedMessage ( ) , "" ) ) ; LOG . error ( "Error updating domain: {}" , domainException . getLocalizedMessage ( ) , domainException ) ; } }
public void test() { try { serviceRegistry . register ( ) ; } catch ( RuntimeException e ) { logger . warn ( "Failed to register ServiceRegistry {}. Exception: {}" , serviceRegistry , e ) ; } }
public StgG20Anwb findById ( sernet . gs . reveng . StgG20AnwbId id ) { log . debug ( "getting StgG20Anwb instance with id: " + id ) ; code_block = TryStatement ;  }
public void test() { if ( instance == null ) { log . debug ( "get successful, no instance found" ) ; } else { log . debug ( "get successful, instance found" ) ; } }
public void test() { if ( instance == null ) { log . debug ( "get successful, no instance found" ) ; } else { log . debug ( "get successful, instance found" ) ; } }
public void test() { try { StgG20Anwb instance = ( StgG20Anwb ) sessionFactory . getCurrentSession ( ) . get ( "sernet.gs.reveng.StgG20Anwb" , id ) ; code_block = IfStatement ; return instance ; } catch ( RuntimeException re ) { log . error ( "get failed" , re ) ; throw re ; } }
public void test() { try { listener . onStartAsync ( event ) ; } catch ( Throwable e ) { LOG . warn ( "Async dispatch error" , e ) ; } }
@ Override public void addUserGroupAsGroupAdmin ( final UserGroup userGroup ) throws DuplicateDataException , JargonException { log . info ( "addUserGroupAsGroupAdmin()" ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "user group:{}" , userGroup ) ; code_block = IfStatement ; code_block = TryStatement ;  }
@ Override public void addUserGroupAsGroupAdmin ( final UserGroup userGroup ) throws DuplicateDataException , JargonException { log . info ( "addUserGroupAsGroupAdmin()" ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "user group:{}" , userGroup ) ; code_block = IfStatement ; code_block = TryStatement ;  }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( BaseServiceTest . getTestBanner ( "headSupported" , CLASS_NAME ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "headSupported url=" + url + " status=" + statusCode ) ; code_block = ForStatement ; } }
public void test() { for ( Header h : method . getResponseHeaders ( ) ) { logger . debug ( "headSupported header name=" + h . getName ( ) + " value=" + h . getValue ( ) ) ; } }
public void test() { try { int statusCode = httpClient . executeMethod ( method ) ; Assert . assertEquals ( method . getResponseBody ( ) , null , "expected null" ) ; code_block = IfStatement ; Assert . assertEquals ( statusCode , HttpStatus . SC_OK , "expected " + HttpStatus . SC_OK ) ; } catch ( HttpException e ) { logger . error ( "Fatal protocol violation: " , e ) ; } catch ( IOException e ) { logger . error ( "Fatal transport error" , e ) ; } catch ( Exception e ) { logger . error ( "unknown exception " , e ) ; } finally { method . releaseConnection ( ) ; } }
public void test() { try { int statusCode = httpClient . executeMethod ( method ) ; Assert . assertEquals ( method . getResponseBody ( ) , null , "expected null" ) ; code_block = IfStatement ; Assert . assertEquals ( statusCode , HttpStatus . SC_OK , "expected " + HttpStatus . SC_OK ) ; } catch ( HttpException e ) { logger . error ( "Fatal protocol violation: " , e ) ; } catch ( IOException e ) { logger . error ( "Fatal transport error" , e ) ; } catch ( Exception e ) { logger . error ( "unknown exception " , e ) ; } finally { method . releaseConnection ( ) ; } }
public void test() { try { int statusCode = httpClient . executeMethod ( method ) ; Assert . assertEquals ( method . getResponseBody ( ) , null , "expected null" ) ; code_block = IfStatement ; Assert . assertEquals ( statusCode , HttpStatus . SC_OK , "expected " + HttpStatus . SC_OK ) ; } catch ( HttpException e ) { logger . error ( "Fatal protocol violation: " , e ) ; } catch ( IOException e ) { logger . error ( "Fatal transport error" , e ) ; } catch ( Exception e ) { logger . error ( "unknown exception " , e ) ; } finally { method . releaseConnection ( ) ; } }
public void test() { try { listeners . put ( cls , ( ReaderListener ) cls . newInstance ( ) ) ; } catch ( Exception e ) { LOGGER . error ( "Failed to create ReaderListener" , e ) ; } }
public void test() { try { listener . beforeScan ( this , openAPI ) ; } catch ( Exception e ) { LOGGER . error ( "Unexpected error invoking beforeScan listener [" + listener . getClass ( ) . getName ( ) + "]" , e ) ; } }
public void test() { try { listener . afterScan ( this , openAPI ) ; } catch ( Exception e ) { LOGGER . error ( "Unexpected error invoking afterScan listener [" + listener . getClass ( ) . getName ( ) + "]" , e ) ; } }
public void test() { try { metadataIndexNode = MetadataIndexNode . deserializeFrom ( buffer ) ; } catch ( BufferOverflowException e ) { logger . error ( METADATA_INDEX_NODE_DESERIALIZE_ERROR , file ) ; throw e ; } }
public void test() { try { metadataIndexNode = MetadataIndexNode . deserializeFrom ( buffer ) ; } catch ( BufferOverflowException e ) { logger . error ( METADATA_INDEX_NODE_DESERIALIZE_ERROR , file ) ; throw e ; } }
public void test() { try { Gson gson = new GsonBuilder ( ) . setPrettyPrinting ( ) . create ( ) ; String json = gson . toJson ( data ) ; Date timestamp = new Date ( System . currentTimeMillis ( ) ) ; Path logPath = Paths . get ( path , "timeMeasuring" ) ; Files . createDirectories ( logPath ) ; Files . write ( Paths . get ( logPath . toString ( ) , String . format ( "%s-%s.json" , name , dateFormat . format ( timestamp ) ) ) , json . getBytes ( Charset . forName ( "UTF-8" ) ) ) ; } catch ( Exception ex ) { LOGGER . warn ( "Cannot save output of time measuring: " + ex . getMessage ( ) ) ; } }
@ Override public DocumentInfo findDocumentByDocumentId ( int documentId ) { log . info ( "Finding document by id." ) ; DocumentInfo document = dao . findById ( DocumentInfo . class , documentId ) ; return document ; }
@ Test public void testTargetMappingUpdatesAfterRebind ( ) throws Exception { log . info ( "starting testTargetMappingUpdatesAfterRebind" ) ; Iterable < StubAppServer > members = Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ; assertExpectedTargetsEventually ( members ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) ) ; rebind ( ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) , "location not managed after rebind" ) ; Iterable < StubAppServer > members2 = Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ; StubAppServer target1 = Iterables . get ( members2 , 0 ) ; StubAppServer target2 = Iterables . get ( members2 , 1 ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target1 , target2 ) ) ; log . info ( "resizing " + cluster + " - " + cluster . getChildren ( ) ) ; Integer result = cluster . resize ( 3 ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) ) ; log . info ( "resized " + cluster + " (" + result + ") - " + cluster . getChildren ( ) ) ; HashSet < StubAppServer > newEntities = Sets . newHashSet ( Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ) ; newEntities . remove ( target1 ) ; newEntities . remove ( target2 ) ; StubAppServer target3 = Iterables . getOnlyElement ( newEntities ) ; log . info ( "expecting " + ImmutableSet . of ( target1 , target2 , target3 ) ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target1 , target2 , target3 ) ) ; log . info ( "pretending one node down" ) ; target1 . sensors ( ) . set ( StubAppServer . SERVICE_UP , false ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target2 , target3 ) ) ; log . info ( "unmanaging another node" ) ; Entities . unmanage ( target2 ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target3 ) ) ; log . info ( "success - testTargetMappingUpdatesAfterRebind" ) ; }
@ Test public void testTargetMappingUpdatesAfterRebind ( ) throws Exception { log . info ( "starting testTargetMappingUpdatesAfterRebind" ) ; Iterable < StubAppServer > members = Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ; assertExpectedTargetsEventually ( members ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) ) ; rebind ( ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) , "location not managed after rebind" ) ; Iterable < StubAppServer > members2 = Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ; StubAppServer target1 = Iterables . get ( members2 , 0 ) ; StubAppServer target2 = Iterables . get ( members2 , 1 ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target1 , target2 ) ) ; log . info ( "resizing " + cluster + " - " + cluster . getChildren ( ) ) ; Integer result = cluster . resize ( 3 ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) ) ; log . info ( "resized " + cluster + " (" + result + ") - " + cluster . getChildren ( ) ) ; HashSet < StubAppServer > newEntities = Sets . newHashSet ( Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ) ; newEntities . remove ( target1 ) ; newEntities . remove ( target2 ) ; StubAppServer target3 = Iterables . getOnlyElement ( newEntities ) ; log . info ( "expecting " + ImmutableSet . of ( target1 , target2 , target3 ) ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target1 , target2 , target3 ) ) ; log . info ( "pretending one node down" ) ; target1 . sensors ( ) . set ( StubAppServer . SERVICE_UP , false ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target2 , target3 ) ) ; log . info ( "unmanaging another node" ) ; Entities . unmanage ( target2 ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target3 ) ) ; log . info ( "success - testTargetMappingUpdatesAfterRebind" ) ; }
@ Test public void testTargetMappingUpdatesAfterRebind ( ) throws Exception { log . info ( "starting testTargetMappingUpdatesAfterRebind" ) ; Iterable < StubAppServer > members = Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ; assertExpectedTargetsEventually ( members ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) ) ; rebind ( ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) , "location not managed after rebind" ) ; Iterable < StubAppServer > members2 = Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ; StubAppServer target1 = Iterables . get ( members2 , 0 ) ; StubAppServer target2 = Iterables . get ( members2 , 1 ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target1 , target2 ) ) ; log . info ( "resizing " + cluster + " - " + cluster . getChildren ( ) ) ; Integer result = cluster . resize ( 3 ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) ) ; log . info ( "resized " + cluster + " (" + result + ") - " + cluster . getChildren ( ) ) ; HashSet < StubAppServer > newEntities = Sets . newHashSet ( Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ) ; newEntities . remove ( target1 ) ; newEntities . remove ( target2 ) ; StubAppServer target3 = Iterables . getOnlyElement ( newEntities ) ; log . info ( "expecting " + ImmutableSet . of ( target1 , target2 , target3 ) ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target1 , target2 , target3 ) ) ; log . info ( "pretending one node down" ) ; target1 . sensors ( ) . set ( StubAppServer . SERVICE_UP , false ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target2 , target3 ) ) ; log . info ( "unmanaging another node" ) ; Entities . unmanage ( target2 ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target3 ) ) ; log . info ( "success - testTargetMappingUpdatesAfterRebind" ) ; }
@ Test public void testTargetMappingUpdatesAfterRebind ( ) throws Exception { log . info ( "starting testTargetMappingUpdatesAfterRebind" ) ; Iterable < StubAppServer > members = Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ; assertExpectedTargetsEventually ( members ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) ) ; rebind ( ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) , "location not managed after rebind" ) ; Iterable < StubAppServer > members2 = Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ; StubAppServer target1 = Iterables . get ( members2 , 0 ) ; StubAppServer target2 = Iterables . get ( members2 , 1 ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target1 , target2 ) ) ; log . info ( "resizing " + cluster + " - " + cluster . getChildren ( ) ) ; Integer result = cluster . resize ( 3 ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) ) ; log . info ( "resized " + cluster + " (" + result + ") - " + cluster . getChildren ( ) ) ; HashSet < StubAppServer > newEntities = Sets . newHashSet ( Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ) ; newEntities . remove ( target1 ) ; newEntities . remove ( target2 ) ; StubAppServer target3 = Iterables . getOnlyElement ( newEntities ) ; log . info ( "expecting " + ImmutableSet . of ( target1 , target2 , target3 ) ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target1 , target2 , target3 ) ) ; log . info ( "pretending one node down" ) ; target1 . sensors ( ) . set ( StubAppServer . SERVICE_UP , false ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target2 , target3 ) ) ; log . info ( "unmanaging another node" ) ; Entities . unmanage ( target2 ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target3 ) ) ; log . info ( "success - testTargetMappingUpdatesAfterRebind" ) ; }
@ Test public void testTargetMappingUpdatesAfterRebind ( ) throws Exception { log . info ( "starting testTargetMappingUpdatesAfterRebind" ) ; Iterable < StubAppServer > members = Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ; assertExpectedTargetsEventually ( members ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) ) ; rebind ( ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) , "location not managed after rebind" ) ; Iterable < StubAppServer > members2 = Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ; StubAppServer target1 = Iterables . get ( members2 , 0 ) ; StubAppServer target2 = Iterables . get ( members2 , 1 ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target1 , target2 ) ) ; log . info ( "resizing " + cluster + " - " + cluster . getChildren ( ) ) ; Integer result = cluster . resize ( 3 ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) ) ; log . info ( "resized " + cluster + " (" + result + ") - " + cluster . getChildren ( ) ) ; HashSet < StubAppServer > newEntities = Sets . newHashSet ( Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ) ; newEntities . remove ( target1 ) ; newEntities . remove ( target2 ) ; StubAppServer target3 = Iterables . getOnlyElement ( newEntities ) ; log . info ( "expecting " + ImmutableSet . of ( target1 , target2 , target3 ) ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target1 , target2 , target3 ) ) ; log . info ( "pretending one node down" ) ; target1 . sensors ( ) . set ( StubAppServer . SERVICE_UP , false ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target2 , target3 ) ) ; log . info ( "unmanaging another node" ) ; Entities . unmanage ( target2 ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target3 ) ) ; log . info ( "success - testTargetMappingUpdatesAfterRebind" ) ; }
@ Test public void testTargetMappingUpdatesAfterRebind ( ) throws Exception { log . info ( "starting testTargetMappingUpdatesAfterRebind" ) ; Iterable < StubAppServer > members = Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ; assertExpectedTargetsEventually ( members ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) ) ; rebind ( ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) , "location not managed after rebind" ) ; Iterable < StubAppServer > members2 = Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ; StubAppServer target1 = Iterables . get ( members2 , 0 ) ; StubAppServer target2 = Iterables . get ( members2 , 1 ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target1 , target2 ) ) ; log . info ( "resizing " + cluster + " - " + cluster . getChildren ( ) ) ; Integer result = cluster . resize ( 3 ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) ) ; log . info ( "resized " + cluster + " (" + result + ") - " + cluster . getChildren ( ) ) ; HashSet < StubAppServer > newEntities = Sets . newHashSet ( Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ) ; newEntities . remove ( target1 ) ; newEntities . remove ( target2 ) ; StubAppServer target3 = Iterables . getOnlyElement ( newEntities ) ; log . info ( "expecting " + ImmutableSet . of ( target1 , target2 , target3 ) ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target1 , target2 , target3 ) ) ; log . info ( "pretending one node down" ) ; target1 . sensors ( ) . set ( StubAppServer . SERVICE_UP , false ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target2 , target3 ) ) ; log . info ( "unmanaging another node" ) ; Entities . unmanage ( target2 ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target3 ) ) ; log . info ( "success - testTargetMappingUpdatesAfterRebind" ) ; }
@ Test public void testTargetMappingUpdatesAfterRebind ( ) throws Exception { log . info ( "starting testTargetMappingUpdatesAfterRebind" ) ; Iterable < StubAppServer > members = Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ; assertExpectedTargetsEventually ( members ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) ) ; rebind ( ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) , "location not managed after rebind" ) ; Iterable < StubAppServer > members2 = Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ; StubAppServer target1 = Iterables . get ( members2 , 0 ) ; StubAppServer target2 = Iterables . get ( members2 , 1 ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target1 , target2 ) ) ; log . info ( "resizing " + cluster + " - " + cluster . getChildren ( ) ) ; Integer result = cluster . resize ( 3 ) ; Assert . assertTrue ( mgmt ( ) . getLocationManager ( ) . isManaged ( Iterables . getOnlyElement ( cluster . getLocations ( ) ) ) ) ; log . info ( "resized " + cluster + " (" + result + ") - " + cluster . getChildren ( ) ) ; HashSet < StubAppServer > newEntities = Sets . newHashSet ( Iterables . filter ( cluster . getChildren ( ) , StubAppServer . class ) ) ; newEntities . remove ( target1 ) ; newEntities . remove ( target2 ) ; StubAppServer target3 = Iterables . getOnlyElement ( newEntities ) ; log . info ( "expecting " + ImmutableSet . of ( target1 , target2 , target3 ) ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target1 , target2 , target3 ) ) ; log . info ( "pretending one node down" ) ; target1 . sensors ( ) . set ( StubAppServer . SERVICE_UP , false ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target2 , target3 ) ) ; log . info ( "unmanaging another node" ) ; Entities . unmanage ( target2 ) ; assertExpectedTargetsEventually ( ImmutableSet . of ( target3 ) ) ; log . info ( "success - testTargetMappingUpdatesAfterRebind" ) ; }
public void test() { if ( session . getAllowedStateTransitions ( docRef ) . contains ( LifeCycleConstants . UNDELETE_TRANSITION ) ) { undeleteDocument ( session , doc ) ; undeleted . add ( docRef ) ; } else { log . debug ( "Impossible to undelete document " + docRef + " as it does not support transition " + LifeCycleConstants . UNDELETE_TRANSITION ) ; } }
public void test() { try { MapSqlParameterSource params = new MapSqlParameterSource ( ) ; params . addValue ( "processId" , processId ) ; List < JobProcessStatus > jobProcessStatusList = baseDao . geoApiNamedJbdcTemaplate . query ( JobProcessQuery . GET_JOB_PROCESS_STATUS . getSql ( baseDao . getJobSchema ( ) ) , params , statusHandler ) ; code_block = IfStatement ; } catch ( Exception ex ) { logger . error ( "Failed to retrieve job process status for process " + processId , ex ) ; } }
public void test() { if ( closingConnection . get ( ) || closed . get ( ) || failed . get ( ) ) { requests . remove ( id ) ; super . onFailure ( result ) ; } else { LOG . debug ( "Request received error: {}" , result . getMessage ( ) ) ; processAlternates ( provider . getAlternateURIs ( ) ) ; handleProviderFailure ( activeProvider , ProviderExceptionSupport . createOrPassthroughFatal ( result ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "to be created, pottag common" ) ; logger . debug ( objectAsXmlString ( pottagCommon , PottagsCommon . class ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "to be created, pottag common" ) ; logger . debug ( objectAsXmlString ( pottagCommon , PottagsCommon . class ) ) ; } }
public void test() { try { return new CSVParser ( reader , csvFormat ) ; } catch ( final IOException e ) { final String message = "couldn't process lookup table to JSON, because couldn't read it with the given CSV format configuration" ; LookupResource . LOG . error ( message ) ; throw new DMPControllerException ( message , e ) ; } }
public void test() { try ( GenericXmlApplicationContext applicationContext = new GenericXmlApplicationContext ( new UrlResource ( resource ) ) ) { Map < String , AppEngine > beansOfType = applicationContext . getBeansOfType ( AppEngine . class ) ; code_block = IfStatement ; AppEngine appEngine = beansOfType . values ( ) . iterator ( ) . next ( ) ; LOGGER . debug ( "==== SPRING APP ENGINE CREATED ==================================================================" ) ; return appEngine ; } }
public void test() { try { AccessControlPolicy [ ] applicable = editor . editAccessControlPolicies ( absPath ) ; return new AccessControlPolicyIteratorAdapter ( Arrays . asList ( applicable ) ) ; } catch ( AccessControlException e ) { log . debug ( "No applicable policy at " + absPath ) ; } }
public void test() { try { com . liferay . portal . kernel . model . Region returnValue = RegionServiceUtil . addRegion ( countryId , active , name , position , regionCode , serviceContext ) ; return com . liferay . portal . kernel . model . RegionSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( mgmt == null ) { throw new IllegalStateException ( "Cannot persist bundles without a management context" ) ; } }
@ Override public void onClose ( ) { logger . info ( "Connection to Minecraft server stopped" ) ; subscriber . onCompleted ( ) ; }
public void test() { if ( JBoss6VFS . valid != null && JBoss6VFS . valid ) { log . debug ( "JBoss 6 VFS API is not available in this environment." ) ; JBoss6VFS . valid = false ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Routes: \n{}" , ModelHelper . dumpModelAsXml ( camelContext , routes ) ) ; } }
public void test() { try ( AccumuloClient client = Accumulo . newClient ( ) . to ( config . getInstanceName ( ) , config . getZookeepers ( ) ) . as ( config . getUsername ( ) , config . getPassword ( ) ) . build ( ) ) { Collection < Authorizations > authCollection = Collections . singleton ( new Authorizations ( config . getAuths ( ) . split ( "," ) ) ) ; code_block = IfStatement ; code_block = TryStatement ;  } catch ( Exception e ) { log . error ( e ) ; } }
public void test() { if ( ! isSet ( key + IDP_ID ) ) { log . warn ( "No entityId to " + key + " ignoring IdP" ) ; return false ; } }
public void test() { if ( getCertificateNames ( key ) . size ( ) == 0 ) { log . warn ( "No certificate to " + entityId + " ignoring IdP" ) ; return false ; } }
public void test() { if ( ( translatioProfile == null || translatioProfile . isEmpty ( ) ) && ( embeddedTranslatioProfile == null || embeddedTranslatioProfile . isEmpty ( ) ) ) { log . warn ( "No translation profile to " + entityId + " ignoring IdP" ) ; return false ; } }
public void test() { if ( ! allCollectionList . contains ( solr_collection_name ) ) { int shardsCalculation = solrCloudClient != null ? solrCloudClient . getClusterStateProvider ( ) . getLiveNodes ( ) . size ( ) : DEFAULT_VALUE ; int no_of_shards = EmbeddedServerUtil . getIntConfig ( SOLR_NO_SHARDS , shardsCalculation ) ; logger . info ( "No. of shards provided is : " + no_of_shards ) ; CollectionAdminRequest . Create createCollection = CollectionAdminRequest . createCollection ( solr_collection_name , solr_config_name , no_of_shards , no_of_replicas ) ; createCollection . setMaxShardsPerNode ( max_node_per_shards ) ; CollectionAdminResponse createResponse = createCollection . process ( solrClient ) ; code_block = IfStatement ; } else { logger . info ( "Collection already exists with name " + solr_collection_name ) ; return true ; } }
public void test() { if ( createResponse . getStatus ( ) != 0 ) { logger . severe ( "Error creating collection. collectionName=" + solr_collection_name + " , solr config name = " + solr_config_name + " , replicas = " + no_of_replicas + ", shards=" + no_of_shards + " , max node per shards = " + max_node_per_shards + ", response=" + createResponse ) ; return false ; } else { logger . info ( "Created collection " + solr_collection_name + " with config name " + solr_config_name + " replicas =  " + no_of_replicas + " Shards = " + no_of_shards + " max node per shards  = " + max_node_per_shards ) ; return true ; } }
public void test() { if ( ! allCollectionList . contains ( solr_collection_name ) ) { int shardsCalculation = solrCloudClient != null ? solrCloudClient . getClusterStateProvider ( ) . getLiveNodes ( ) . size ( ) : DEFAULT_VALUE ; int no_of_shards = EmbeddedServerUtil . getIntConfig ( SOLR_NO_SHARDS , shardsCalculation ) ; logger . info ( "No. of shards provided is : " + no_of_shards ) ; CollectionAdminRequest . Create createCollection = CollectionAdminRequest . createCollection ( solr_collection_name , solr_config_name , no_of_shards , no_of_replicas ) ; createCollection . setMaxShardsPerNode ( max_node_per_shards ) ; CollectionAdminResponse createResponse = createCollection . process ( solrClient ) ; code_block = IfStatement ; } else { logger . info ( "Collection already exists with name " + solr_collection_name ) ; return true ; } }
public void initTimer ( ) { log . debug ( "Initializing Metadata Validation Timer" ) ; final int delay = 30 ; final int interval = DEFAULT_INTERVAL ; timerEvent . fire ( new TimerEvent ( new TimerSchedule ( delay , interval ) , new MetadataValidationEvent ( ) , Scheduled . Literal . INSTANCE ) ) ; }
public void test() { if ( ! raftInitialized ( ) ) { LOGGER . error ( "Raft incomplete initialization!" ) ; return false ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "putClusterMeta {} {}" , metaType , metaKey ) ; } }
public void test() { try { OvmStoragePool . Details d = OvmStoragePool . getDetailsByUuid ( _conn , cmd . getStorageId ( ) ) ; return new GetStorageStatsAnswer ( cmd , d . totalSpace , d . usedSpace ) ; } catch ( Exception e ) { s_logger . debug ( "GetStorageStatsCommand on pool " + cmd . getStorageId ( ) + " failed" , e ) ; return new GetStorageStatsAnswer ( cmd , e . getMessage ( ) ) ; } }
public void test() { if ( clientResponse . getStatus ( ) != 200 ) { logger . warn ( "Couldn't contact AIDRTaggerAPI for sending error message" ) ; } }
public void test() { try { WebTarget webResource = client . target ( AnalyticsConfigurator . getInstance ( ) . getProperty ( AnalyticsConfigurationProperty . TAGGER_REST_URI ) + "/misc/sendErrorEmail" ) ; Form form = new Form ( ) ; form . param ( "module" , "AIDRAnalytics" ) ; form . param ( "code" , code ) ; form . param ( "description" , errorMsg ) ; clientResponse = webResource . request ( ) . post ( Entity . entity ( form , MediaType . APPLICATION_FORM_URLENCODED ) , Response . class ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Error in contacting AIDRTaggerAPI: " + clientResponse ) ; } }
public void test() { if ( s_logger . isDebugEnabled ( ) ) { s_logger . debug ( "Usage timezone = " + _usageTimezone + " AggregationDuration=" + _aggregationDuration ) ; } }
public void test() { try { sender . sendHeartbeat ( ) ; } catch ( Throwable e ) { RecordLog . warn ( "[HeartbeatSender] Send heartbeat error" , e ) ; } }
public void test() { if ( f . exists ( ) ) { FileUtils . forceDelete ( f ) ; LOG . info ( "Deleted file [{}]" , f ) ; } }
public void test() { if ( logger . isTraceEnabled ( LogMarker . SERIALIZER_VERBOSE ) ) { logger . trace ( LogMarker . SERIALIZER_VERBOSE , "Writing Short {}" , value ) ; } }
public void test() { try { activeMQComponent . start ( ) ; } catch ( Exception e ) { logger . warn ( "could not start activemq" , e ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Encrypting byte[] of size [{}] on thread [{}]" , input == null ? null : input . length , Thread . currentThread ( ) . getName ( ) ) ; } }
public void test() { if ( channelFuture . isSuccess ( ) ) { LOG . info ( "Connection to TSO [{}] established. Channel {}" , tsoAddress , channelFuture . channel ( ) ) ; } else { LOG . error ( "Failed connection attempt to TSO [{}] failed. Channel {}" , tsoAddress , channelFuture . channel ( ) ) ; fsm . sendEvent ( new ErrorEvent ( new ConnectionException ( ) ) ) ; } }
public void test() { if ( channelFuture . isSuccess ( ) ) { LOG . info ( "Connection to TSO [{}] established. Channel {}" , tsoAddress , channelFuture . channel ( ) ) ; } else { LOG . error ( "Failed connection attempt to TSO [{}] failed. Channel {}" , tsoAddress , channelFuture . channel ( ) ) ; fsm . sendEvent ( new ErrorEvent ( new ConnectionException ( ) ) ) ; } }
@ ShutdownHandler ( phase = Phase . INBOUND_EVENT_CONNECTORS ) @ Override public CompletableFuture < Void > shutdownAsync ( ) { logger . info ( "Stopping processor [{}]" , name ) ; return coordinator . stop ( ) ; }
private void prepareKeyShareLength ( ) { entry . setPublicKeyLength ( entry . getPublicKey ( ) . getValue ( ) . length ) ; LOGGER . debug ( "KeyShareLength: " + entry . getPublicKeyLength ( ) . getValue ( ) ) ; }
@ Override public void evaluate ( ) throws Throwable { URL composeYml = getClass ( ) . getResource ( REDIS_COMPOSE_YML ) ; assertThat ( composeYml ) . as ( "Cannot load resource " + REDIS_COMPOSE_YML ) . isNotNull ( ) ; redisCluster = new DockerComposeContainer < > ( "acceptance" , new File ( composeYml . getFile ( ) ) ) ; code_block = ForStatement ; redisCluster . withLocalCompose ( true ) ; redisCluster . waitingFor ( "redis-cluster-init_1" , Wait . forLogMessage ( ".*Ready to accept connections.*" , 1 ) ) ; redisCluster . start ( ) ; int port = redisCluster . getServicePort ( "redis-node-0" , REDIS_PORT ) ; Jedis jedis = new Jedis ( "localhost" , port ) ; List < ClusterNode > nodes = ClusterNodes . parseClusterNodes ( jedis . clusterNodes ( ) ) . getNodes ( ) ; nodes . forEach ( logger :: info ) ; assertThat ( nodes . stream ( ) . mapToInt ( x -> x . primary ? 1 : 0 ) . sum ( ) ) . as ( "Incorrect primary node count" ) . isEqualTo ( 3 ) ; Map < HostPort , HostPort > translationMappings = new HashMap < > ( ) ; List < RedisProxy > proxies = new ArrayList < > ( ) ; code_block = ForStatement ; proxies . forEach ( p -> p . configure ( translationMappings ) ) ; logger . info ( "Started redis cluster with mapped ports: {}" , translationMappings ) ; code_block = TryStatement ;  }
@ DisplayName ( "Gather all derived tables, export data up to height = 8000," + " delete rows up to height = 8000, import data back into db table" ) @ Test void testExportAndImportData ( ) { DirProvider dirProvider = mock ( DirProvider . class ) ; doReturn ( temporaryFolderExtension . newFolder ( "csvExport" ) . toPath ( ) ) . when ( dirProvider ) . getDataExportDir ( ) ; Set < String > excludeColumnNames = Set . of ( "DB_ID" , "LATEST" ) ; Collection < DerivedTableInterface > result = registry . getDerivedTables ( ) ; assertNotNull ( result ) ; log . debug ( "Processing [{}] tables" , result . size ( ) ) ; int targetHeight = Integer . MAX_VALUE ; result . forEach ( item code_block = LoopStatement ; ) ; log . debug ( "Processed Tables = {}" , result ) ; }
public void test() { if ( minMaxValue . getCount ( ) > 0 ) { do code_block = "" ; while ( processedCount > 0 ) ; log . debug ( "Table = {}, exported rows = {}" , item . toString ( ) , totalCount ) ; assertEquals ( minMaxValue . getCount ( ) , totalCount ) ; int deletedCount = dropDataByName ( minDbValue , maxDbValue , item . toString ( ) ) ; assertEquals ( minMaxValue . getCount ( ) , deletedCount ) ; int imported = importCsv ( item . toString ( ) , batchLimit , dirProvider . getDataExportDir ( ) ) ; log . debug ( "Table = {}, imported rows = {}" , item . toString ( ) , imported ) ; assertEquals ( minMaxValue . getCount ( ) , imported , "incorrect value to '" + item . toString ( ) + "'" ) ; } }
public void test() { if ( minMaxValue . getCount ( ) > 0 ) { do code_block = "" ; while ( processedCount > 0 ) ; log . debug ( "Table = {}, exported rows = {}" , item . toString ( ) , totalCount ) ; assertEquals ( minMaxValue . getCount ( ) , totalCount ) ; int deletedCount = dropDataByName ( minDbValue , maxDbValue , item . toString ( ) ) ; assertEquals ( minMaxValue . getCount ( ) , deletedCount ) ; int imported = importCsv ( item . toString ( ) , batchLimit , dirProvider . getDataExportDir ( ) ) ; log . debug ( "Table = {}, imported rows = {}" , item . toString ( ) , imported ) ; assertEquals ( minMaxValue . getCount ( ) , imported , "incorrect value to '" + item . toString ( ) + "'" ) ; } }
public void test() { try ( Connection con = extension . getDatabaseManager ( ) . getDataSource ( ) . getConnection ( ) ; PreparedStatement pstmt = con . prepareStatement ( "select * from " + item . toString ( ) + " where db_id BETWEEN ? and  ? limit ?" ) ; CsvWriter csvWriter = new CsvWriterImpl ( dirProvider . getDataExportDir ( ) , excludeColumnNames , translator ) ; ) { csvWriter . setOptions ( "fieldDelimiter=" ) ; MinMaxValue minMaxValue = item . getMinMaxValue ( targetHeight ) ; minDbValue = minMaxValue . getMin ( ) ; maxDbValue = minMaxValue . getMax ( ) ; assertTrue ( minMaxValue . getMax ( ) >= 0 ) ; log . debug ( "Table = {}, Min/Max = {} at height = {}" , item . toString ( ) , minMaxValue , targetHeight ) ; code_block = IfStatement ; } catch ( Exception e ) { log . error ( "Exception" , e ) ; Throwables . throwIfUnchecked ( e ) ; throw new RuntimeException ( e ) ; } }
@ DisplayName ( "Gather all derived tables, export data up to height = 8000," + " delete rows up to height = 8000, import data back into db table" ) @ Test void testExportAndImportData ( ) { DirProvider dirProvider = mock ( DirProvider . class ) ; doReturn ( temporaryFolderExtension . newFolder ( "csvExport" ) . toPath ( ) ) . when ( dirProvider ) . getDataExportDir ( ) ; Set < String > excludeColumnNames = Set . of ( "DB_ID" , "LATEST" ) ; Collection < DerivedTableInterface > result = registry . getDerivedTables ( ) ; assertNotNull ( result ) ; log . debug ( "Processing [{}] tables" , result . size ( ) ) ; int targetHeight = Integer . MAX_VALUE ; result . forEach ( item code_block = LoopStatement ; ) ; log . debug ( "Processed Tables = {}" , result ) ; }
public void test() { try { java . util . List < com . liferay . commerce . inventory . model . CommerceInventoryWarehouseItem > returnValue = CommerceInventoryWarehouseItemServiceUtil . getCommerceInventoryWarehouseItems ( companyId , sku , start , end ) ; return com . liferay . commerce . inventory . model . CommerceInventoryWarehouseItemSoap . toSoapModels ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( jmsTaskManagerState == STATE_PAUSED ) { log . info ( "Attempt to re-start paused TaskManager is ignored. Please use resume instead" ) ; return ; } }
public void test() { switch ( cacheLevel ) { case JMSConstants . CACHE_NONE : log . debug ( "No JMS resources will be cached/shared between poller " + "worker tasks of " + jmsConsumerName ) ; break ; case JMSConstants . CACHE_CONNECTION : log . debug ( "Only the JMS Connection will be cached and shared between *all* " + "poller task invocations" ) ; break ; case JMSConstants . CACHE_SESSION : log . debug ( "The JMS Connection and Session will be cached and shared between " + "successive poller task invocations" ) ; break ; case JMSConstants . CACHE_CONSUMER : log . debug ( "The JMS Connection, Session and MessageConsumer will be cached and " + "shared between successive poller task invocations" ) ; break ; code_block = "" ; } }
public void test() { switch ( cacheLevel ) { case JMSConstants . CACHE_NONE : log . debug ( "No JMS resources will be cached/shared between poller " + "worker tasks of " + jmsConsumerName ) ; break ; case JMSConstants . CACHE_CONNECTION : log . debug ( "Only the JMS Connection will be cached and shared between *all* " + "poller task invocations" ) ; break ; case JMSConstants . CACHE_SESSION : log . debug ( "The JMS Connection and Session will be cached and shared between " + "successive poller task invocations" ) ; break ; case JMSConstants . CACHE_CONSUMER : log . debug ( "The JMS Connection, Session and MessageConsumer will be cached and " + "shared between successive poller task invocations" ) ; break ; code_block = "" ; } }
public void test() { switch ( cacheLevel ) { case JMSConstants . CACHE_NONE : log . debug ( "No JMS resources will be cached/shared between poller " + "worker tasks of " + jmsConsumerName ) ; break ; case JMSConstants . CACHE_CONNECTION : log . debug ( "Only the JMS Connection will be cached and shared between *all* " + "poller task invocations" ) ; break ; case JMSConstants . CACHE_SESSION : log . debug ( "The JMS Connection and Session will be cached and shared between " + "successive poller task invocations" ) ; break ; case JMSConstants . CACHE_CONSUMER : log . debug ( "The JMS Connection, Session and MessageConsumer will be cached and " + "shared between successive poller task invocations" ) ; break ; code_block = "" ; } }
public void test() { switch ( cacheLevel ) { case JMSConstants . CACHE_NONE : log . debug ( "No JMS resources will be cached/shared between poller " + "worker tasks of " + jmsConsumerName ) ; break ; case JMSConstants . CACHE_CONNECTION : log . debug ( "Only the JMS Connection will be cached and shared between *all* " + "poller task invocations" ) ; break ; case JMSConstants . CACHE_SESSION : log . debug ( "The JMS Connection and Session will be cached and shared between " + "successive poller task invocations" ) ; break ; case JMSConstants . CACHE_CONSUMER : log . debug ( "The JMS Connection, Session and MessageConsumer will be cached and " + "shared between successive poller task invocations" ) ; break ; code_block = "" ; } }
public synchronized void start ( ) { code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = SwitchStatement ; code_block = ForStatement ; jmsTaskManagerState = STATE_STARTED ; log . info ( "Task manager to " + jmsConsumerName + " [re-]initialized" ) ; }
public void test() { try { addPortletTitleAddJournalArticleMenuItems ( menuItems , themeDisplay , portletRequest ) ; } catch ( Exception exception ) { _log . error ( "Unable to add folder menu item" , exception ) ; } }
public void test() { try { return new SimpleAccount ( token . getPrincipal ( ) , token . getCredentials ( ) , "CloudSession" ) ; } catch ( Throwable t ) { LOG . error ( "Unexpected exception creating account object" , t ) ; } }
public void test() { try { code_block = IfStatement ; throw new AuthenticationException ( "Unable to authenticate token" ) ; } catch ( UnknownUserException ex ) { LOG . warn ( "Authentication failed. Message: {}" , ex . getMessage ( ) ) ; throw new AuthenticationException ( ex . getMessage ( ) ) ; } catch ( UserBlockedException ex ) { LOG . warn ( "Blocked user {}" , ex ) ; throw new AuthenticationException ( ex . getMessage ( ) ) ; } catch ( EmailNotConfirmedException ex ) { LOG . warn ( "Authentication failed. Message: {}" , ex . getMessage ( ) ) ; throw new AuthenticationException ( "EmailNotConfirmed" ) ; } catch ( InsufficientBucketTokensException ex ) { LOG . info ( "Insufficient bucket tokens: {}" , ex . getMessage ( ) ) ; throw new AuthenticationException ( ex . getMessage ( ) ) ; } catch ( NullPointerException npe ) { LOG . warn ( "NullPointer" , npe ) ; throw new AuthenticationException ( npe . getMessage ( ) ) ; } catch ( Throwable t ) { LOG . warn ( "Throwable" , t ) ; } }
public void test() { try { code_block = IfStatement ; throw new AuthenticationException ( "Unable to authenticate token" ) ; } catch ( UnknownUserException ex ) { LOG . warn ( "Authentication failed. Message: {}" , ex . getMessage ( ) ) ; throw new AuthenticationException ( ex . getMessage ( ) ) ; } catch ( UserBlockedException ex ) { LOG . warn ( "Blocked user {}" , ex ) ; throw new AuthenticationException ( ex . getMessage ( ) ) ; } catch ( EmailNotConfirmedException ex ) { LOG . warn ( "Authentication failed. Message: {}" , ex . getMessage ( ) ) ; throw new AuthenticationException ( "EmailNotConfirmed" ) ; } catch ( InsufficientBucketTokensException ex ) { LOG . info ( "Insufficient bucket tokens: {}" , ex . getMessage ( ) ) ; throw new AuthenticationException ( ex . getMessage ( ) ) ; } catch ( NullPointerException npe ) { LOG . warn ( "NullPointer" , npe ) ; throw new AuthenticationException ( npe . getMessage ( ) ) ; } catch ( Throwable t ) { LOG . warn ( "Throwable" , t ) ; } }
public void test() { try { code_block = IfStatement ; throw new AuthenticationException ( "Unable to authenticate token" ) ; } catch ( UnknownUserException ex ) { LOG . warn ( "Authentication failed. Message: {}" , ex . getMessage ( ) ) ; throw new AuthenticationException ( ex . getMessage ( ) ) ; } catch ( UserBlockedException ex ) { LOG . warn ( "Blocked user {}" , ex ) ; throw new AuthenticationException ( ex . getMessage ( ) ) ; } catch ( EmailNotConfirmedException ex ) { LOG . warn ( "Authentication failed. Message: {}" , ex . getMessage ( ) ) ; throw new AuthenticationException ( "EmailNotConfirmed" ) ; } catch ( InsufficientBucketTokensException ex ) { LOG . info ( "Insufficient bucket tokens: {}" , ex . getMessage ( ) ) ; throw new AuthenticationException ( ex . getMessage ( ) ) ; } catch ( NullPointerException npe ) { LOG . warn ( "NullPointer" , npe ) ; throw new AuthenticationException ( npe . getMessage ( ) ) ; } catch ( Throwable t ) { LOG . warn ( "Throwable" , t ) ; } }
public void test() { try { code_block = IfStatement ; throw new AuthenticationException ( "Unable to authenticate token" ) ; } catch ( UnknownUserException ex ) { LOG . warn ( "Authentication failed. Message: {}" , ex . getMessage ( ) ) ; throw new AuthenticationException ( ex . getMessage ( ) ) ; } catch ( UserBlockedException ex ) { LOG . warn ( "Blocked user {}" , ex ) ; throw new AuthenticationException ( ex . getMessage ( ) ) ; } catch ( EmailNotConfirmedException ex ) { LOG . warn ( "Authentication failed. Message: {}" , ex . getMessage ( ) ) ; throw new AuthenticationException ( "EmailNotConfirmed" ) ; } catch ( InsufficientBucketTokensException ex ) { LOG . info ( "Insufficient bucket tokens: {}" , ex . getMessage ( ) ) ; throw new AuthenticationException ( ex . getMessage ( ) ) ; } catch ( NullPointerException npe ) { LOG . warn ( "NullPointer" , npe ) ; throw new AuthenticationException ( npe . getMessage ( ) ) ; } catch ( Throwable t ) { LOG . warn ( "Throwable" , t ) ; } }
@ ExceptionHandler ( OAuthProblemException . class ) public ModelAndView handleOAuthProblemException ( OAuthProblemException ex ) { logger . error ( "OAuth exception: " + ex . getMessage ( ) ) ; ModelAndView modelAndView = new ModelAndView ( "oauth_error" ) ; modelAndView . addObject ( "exception" , ex ) ; return modelAndView ; }
public void test() { try { Constructor < ? extends Component > con = componentClass . getConstructor ( new Class [ ] code_block = "" ; ) ; Component comp = con . newInstance ( MockPageWithLinkAndComponent . COMPONENT_ID ) ; page . replace ( comp ) ; comp . setOutputMarkupId ( true ) ; target . add ( comp ) ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( LOGGER . isTraceEnabled ( ) ) { LOGGER . trace ( format ( "Generate iterator for select : %s" , statementWrapper . getBoundStatement ( ) . preparedStatement ( ) . getQueryString ( ) ) ) ; } }
public void test() { { log . debug ( "Tick" ) ; latch . countDown ( ) ; } }
public void test() { if ( log ) { LOG . debug ( "Using cache directory: {}" , cacheDirectory ) ; } }
public void test() { if ( log ) { LOG . info ( "Downloading {}:{}:{}" , groupId , artifactId , version ) ; } }
public void test() { if ( log ) { LOG . warn ( "Error during add components from artifact {}:{}:{} due {}" , groupId , artifactId , version , e . getMessage ( ) , e ) ; } }
public void test() { try { ConnectionFactory factory = new ActiveMQConnectionFactory ( USER_NAME , PASSWORD , brokenUrl ) ; connection = factory . createConnection ( ) ; connection . start ( ) ; connection . getMetaData ( ) . getJMSVersion ( ) ; connection . close ( ) ; } catch ( Exception ex ) { LOGGER . error ( ex ) ; code_block = TryStatement ;  return FAIL ; } }
public void test() { try { session . close ( ) ; connection . close ( ) ; } catch ( JMSException e ) { LOGGER . error ( e ) ; } }
@ Override public < T > Page < T > queryForPage ( ViewQuery query , PageRequest pr , Class < T > type ) { Assert . notNull ( query , "query may not be null" ) ; Assert . notNull ( pr , "PageRequest may not be null" ) ; Assert . notNull ( type , "type may not be null" ) ; query . dbPath ( dbURI . toString ( ) ) ; LOG . debug ( "startKey: {}" , pr . getStartKey ( ) ) ; LOG . debug ( "startDocId: {}" , pr . getStartKeyDocId ( ) ) ; PageResponseHandler < T > ph = new PageResponseHandler < T > ( pr , type , objectMapper , query . isIgnoreNotFound ( ) ) ; query = PageRequest . applyPagingParameters ( query , pr ) ; return executeQuery ( query , ph ) ; }
@ Override public < T > Page < T > queryForPage ( ViewQuery query , PageRequest pr , Class < T > type ) { Assert . notNull ( query , "query may not be null" ) ; Assert . notNull ( pr , "PageRequest may not be null" ) ; Assert . notNull ( type , "type may not be null" ) ; query . dbPath ( dbURI . toString ( ) ) ; LOG . debug ( "startKey: {}" , pr . getStartKey ( ) ) ; LOG . debug ( "startDocId: {}" , pr . getStartKeyDocId ( ) ) ; PageResponseHandler < T > ph = new PageResponseHandler < T > ( pr , type , objectMapper , query . isIgnoreNotFound ( ) ) ; query = PageRequest . applyPagingParameters ( query , pr ) ; return executeQuery ( query , ph ) ; }
public void test() { if ( ! directory . exists ( ) ) { log . error ( "Cannot load stream definitions from " + directory . getAbsolutePath ( ) + " directory not exist" ) ; return streamDefinitions ; } }
public void test() { if ( ! directory . isDirectory ( ) ) { log . error ( "Cannot load stream definitions from " + directory . getAbsolutePath ( ) + " not a directory" ) ; return streamDefinitions ; } }
public void test() { try { bufferedReader = new BufferedReader ( new FileReader ( fileEntry ) ) ; String line ; code_block = WhileStatement ; StreamDefinition streamDefinition = EventDefinitionConverterUtils . convertFromJson ( stringBuilder . toString ( ) . trim ( ) ) ; streamDefinitions . put ( streamDefinition . getStreamId ( ) , streamDefinition ) ; } catch ( FileNotFoundException e ) { log . error ( "Error in reading file " + fileEntry . getName ( ) , e ) ; } catch ( IOException e ) { log . error ( "Error in reading file " + fileEntry . getName ( ) , e ) ; } catch ( MalformedStreamDefinitionException e ) { log . error ( "Error in converting Stream definition " + e . getMessage ( ) , e ) ; } finally { code_block = TryStatement ;  } }
public void test() { try { code_block = IfStatement ; } catch ( IOException e ) { log . error ( "Error occurred when reading the file : " + e . getMessage ( ) , e ) ; } }
private void assertEqualXMLConfigurations ( String expectedXMLConfiguration , String generatedXMLConfiguration ) throws IOException , SAXException , TransformerException , ParserConfigurationException { log . trace ( "Produced XML configuration:\n" + generatedXMLConfiguration ) ; log . trace ( "Expected XML configuration:\n" + expectedXMLConfiguration ) ; Assert . assertTrue ( "Generated configuration XML and expected configuration XML must be equal" , XmlHelper . compareXMLStrings ( generatedXMLConfiguration , expectedXMLConfiguration ) ) ; }
private void assertEqualXMLConfigurations ( String expectedXMLConfiguration , String generatedXMLConfiguration ) throws IOException , SAXException , TransformerException , ParserConfigurationException { log . trace ( "Produced XML configuration:\n" + generatedXMLConfiguration ) ; log . trace ( "Expected XML configuration:\n" + expectedXMLConfiguration ) ; Assert . assertTrue ( "Generated configuration XML and expected configuration XML must be equal" , XmlHelper . compareXMLStrings ( generatedXMLConfiguration , expectedXMLConfiguration ) ) ; }
public void test() { if ( waitTillDone ) { String requestBody = getDeployTargetRequestBody ( true ) ; HttpEntity requestEntity = new StringEntity ( requestBody , ContentType . APPLICATION_JSON ) ; postRequest . setEntity ( requestEntity ) ; } }
public void test() { try { CloseableHttpResponse response = httpClient . execute ( postRequest ) ; HttpStatus httpStatus = HttpStatus . valueOf ( response . getStatusLine ( ) . getStatusCode ( ) ) ; code_block = IfStatement ; } catch ( IOException e ) { logger . error ( "Error while sending preview sync request for site " + site , e ) ; } finally { postRequest . releaseConnection ( ) ; } }
@ Override public void onTimeOut ( ) { _logger . info ( "Scheduler msg timeout " + _originalMessage . getMsgId ( ) + " timout with " + _timeout + " Ms" ) ; _statusUpdateUtil . logError ( _originalMessage , SchedulerAsyncCallback . class , "Task timeout" , _manager ) ; addSummary ( _resultSummaryMap , _originalMessage , _manager , true ) ; }
public void createIdpEventInitCustomer ( final IdentityProvider sourceEvent ) { LOGGER . info ( "Create Provider {}" , sourceEvent . toString ( ) ) ; create ( getCurrentProofTenantIdentifier ( ) , sourceEvent . getIdentifier ( ) , MongoDbCollections . PROVIDERS , EventType . EXT_VITAMUI_CREATE_IDP , converters . getIdpConverter ( ) . convertToLogbook ( converters . getIdpConverter ( ) . convertEntityToDto ( sourceEvent ) ) ) ; }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { long percentage = Math . round ( bytes / ( ( double ) size ) * 100.0 ) ; LOGGER . debug ( "overall bytes transfered: {} progress {}%" , bytes , percentage ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { long percentage = Math . round ( bytes / ( ( double ) size ) * 100.0 ) ; LOGGER . debug ( "overall bytes transfered: {} progress {}%" , bytes , percentage ) ; } }
public void test() { if ( NumberUtils . compareInf ( cfg . getMaxBytes ( ) , forceMaxBytesPerDomain ) < 0 || ( job . isConfigurationSetsByteLimit ( ) && NumberUtils . compareInf ( cfg . getMaxBytes ( ) , forceMaxBytesPerDomain ) != 0 ) ) { log . debug ( "Job for HD {} BYTE_LIMIT of config (domain,config={},{}) incompatible with current job" , job . getOrigHarvestDefinitionID ( ) , cfg . getDomainName ( ) , cfg . getName ( ) ) ; return false ; } }
public void test() { if ( NumberUtils . compareInf ( cfg . getMaxBytes ( ) , forceMaxBytesPerDomain ) < 0 || ( job . isConfigurationSetsByteLimit ( ) && NumberUtils . compareInf ( cfg . getMaxBytes ( ) , forceMaxBytesPerDomain ) != 0 ) ) { log . debug ( "Job for HD {} BYTE_LIMIT of config (domain,config={},{}) incompatible with current job" , job . getOrigHarvestDefinitionID ( ) , cfg . getDomainName ( ) , cfg . getName ( ) ) ; return false ; } }
public void test() { if ( NumberUtils . compareInf ( cfg . getMaxBytes ( ) , forceMaxBytesPerDomain ) < 0 || ( job . isConfigurationSetsByteLimit ( ) && NumberUtils . compareInf ( cfg . getMaxBytes ( ) , forceMaxBytesPerDomain ) != 0 ) ) { log . debug ( "Job for HD {} BYTE_LIMIT of config (domain,config={},{}) incompatible with current job" , job . getOrigHarvestDefinitionID ( ) , cfg . getDomainName ( ) , cfg . getName ( ) ) ; return false ; } }
public void test() { if ( NumberUtils . compareInf ( cfg . getMaxBytes ( ) , forceMaxBytesPerDomain ) < 0 || ( job . isConfigurationSetsByteLimit ( ) && NumberUtils . compareInf ( cfg . getMaxBytes ( ) , forceMaxBytesPerDomain ) != 0 ) ) { log . debug ( "Job for HD {} BYTE_LIMIT of config (domain,config={},{}) incompatible with current job" , job . getOrigHarvestDefinitionID ( ) , cfg . getDomainName ( ) , cfg . getName ( ) ) ; return false ; } }
public void test() { if ( yminCountObjects == 0 ) { yminCountObjects = 1 ; } }
public void test() { try { ResourceMonitor resourceMonitor = getOrCreateResourceMonitor ( resourceName ) ; code_block = IfStatement ; } catch ( Exception e ) { LOG . error ( "Fail to set resource status, resource: " + idealState . getResourceName ( ) , e ) ; } }
public void test() { if ( preCheck == ResponseCodeEnum . RECEIPT_NOT_FOUND ) { return null ; } else-if ( code == ResponseCodeEnum . UNKNOWN ) { Thread . sleep ( 200 ) ; } else-if ( code == ResponseCodeEnum . SUCCESS ) { return response . getTransactionGetReceipt ( ) ; } else { log . warn ( "Unexpected receipt response {} " , response ) ; return response . getTransactionGetReceipt ( ) ; } }
public void test() { if ( maybeLoadBalancerPort . isPresent ( ) ) { String upstream = String . format ( "%s:%d" , task . getHostname ( ) , maybeLoadBalancerPort . get ( ) ) ; Optional < String > group = loadBalancerUpstreamGroup ; code_block = IfStatement ; upstreams . add ( new LoadBalancerUpstream ( upstream , group . orElse ( "default" ) , task . getRackId ( ) ) ) ; } else { LOG . warn ( "Task {} is missing port but is being passed to LB  ({})" , task . getTaskId ( ) , task ) ; } }
public void test() { for ( Pair < INDArray , String > p : NDArrayCreationUtil . getAllTestMatricesWithShape ( origShape [ 0 ] , origShape [ 1 ] , 12345 , DataType . DOUBLE ) ) { INDArray inArr = p . getFirst ( ) . muli ( 100 ) ; SameDiff sd = SameDiff . create ( ) ; SDVariable in = sd . var ( "in" , inArr ) ; SDVariable expand = sd . expandDims ( in , i ) ; SDVariable stdev = sd . standardDeviation ( "out" , expand , true ) ; Map < String , INDArray > m = sd . outputAll ( null ) ; INDArray expOut = in . getArr ( ) . std ( true ) ; assertArrayEquals ( expExpandShape , m . get ( expand . name ( ) ) . shape ( ) ) ; INDArray expExpand = inArr . dup ( 'c' ) . reshape ( expExpandShape ) ; String msg = "expandDim=" + i + ", source=" + p . getSecond ( ) ; log . info ( "Starting: " + msg ) ; TestCase tc = new TestCase ( sd ) ; tc . testName ( msg ) . expectedOutput ( "out" , expOut ) . expectedOutput ( expand . name ( ) , expExpand ) ; String error = OpValidation . validate ( tc ) ; code_block = IfStatement ; } }
public void test() { if ( consumerLog == null ) { LOG . warn ( "æ²¡æå¯¹åºçconsumerLog, subject:{}" , subject ) ; return ; } }
public void test() { switch ( consumeFromWhere ) { case UNKNOWN : LOG . info ( "UNKNOWN consumeFromWhere code, {}" , consumeFromWhereCode ) ; break ; case EARLIEST : consumeQueueManager . update ( subject , group , bound . getMinOffset ( ) ) ; break ; case LATEST : consumeQueueManager . update ( subject , group , bound . getMaxOffset ( ) ) ; break ; } }
public void test() { if ( newFile . lastModified ( ) < entry . getTime ( ) ) { logger . info ( "Overwriting existing file {} because import file is newer." , entry . getName ( ) ) ; overwrite = true ; } }
public void test() { if ( _cache == null ) { _log . info ( "creating initial " + type + " cache with timeout " + timeout + "..." ) ; _cache = CacheBuilder . newBuilder ( ) . expireAfterWrite ( timeout , TimeUnit . SECONDS ) . build ( ) ; _log . info ( "done" ) ; } }
public void test() { if ( appendEntryLimiter . tryAcquire ( positions . getRight ( ) ) ) { final var listener = new Listener ( this , positions . getRight ( ) , ActorClock . currentTimeMillis ( ) ) ; logStorage . append ( positions . getLeft ( ) , positions . getRight ( ) , copiedBuffer , listener ) ; blockPeek . markCompleted ( ) ; } else { appendBackpressureMetrics . deferred ( ) ; LOG . trace ( "Backpressure happens: in flight {} limit {}" , appendEntryLimiter . getInflight ( ) , appendEntryLimiter . getLimit ( ) ) ; } }
@ Override public Object invoke ( Object object , Method method , Object [ ] args ) throws Throwable { LOG . debug ( "invoke!!!" ) ; LOG . debug ( object . getClass ( ) . getName ( ) + "" + method . getName ( ) + "()" ) ; return "RESULT" ; }
@ Override public Object invoke ( Object object , Method method , Object [ ] args ) throws Throwable { LOG . debug ( "invoke!!!" ) ; LOG . debug ( object . getClass ( ) . getName ( ) + "" + method . getName ( ) + "()" ) ; return "RESULT" ; }
public void test() { try { if ( logger . isTraceEnabled ( ) ) logger . trace ( "Responding with error: {}, v={} ON {}" , error . getMessage ( ) , request . connection ( ) . getVersion ( ) , Thread . currentThread ( ) . getName ( ) ) ; UnexpectedChannelExceptionHandler handler = new UnexpectedChannelExceptionHandler ( ctx . channel ( ) , true ) ; if ( error instanceof ExecutionException ) error = error . getCause ( ) ; if ( error instanceof CompletionException ) error = error . getCause ( ) ; flush ( new Message . Dispatcher . FlushItem ( ctx , ErrorMessage . fromException ( error , handler ) . setStreamId ( request . getStreamId ( ) ) , request . getSourceFrameBodySizeInBytes ( ) , this ) ) ; } catch ( Throwable t ) { logger . error ( "Failed to reply with error {}, got error whilst writing error reply: {}" , error . getMessage ( ) , t . getMessage ( ) , t ) ; } finally { ClientWarn . instance . resetWarnings ( ) ; } }
public void test() { try { configEntry = directoryService . getAdminSession ( ) . lookup ( defaultSearchDn ) ; } catch ( LdapException e ) { LOG . debug ( "No configuration data found for class loader default search contexts." ) ; } }
public void test() { for ( Value val : attr ) { Dn dn = directoryService . getDnFactory ( ) . create ( val . getString ( ) ) ; searchContexts . add ( dn ) ; } }
public void test() { for ( Value val : attr ) { Dn dn = directoryService . getDnFactory ( ) . create ( val . getString ( ) ) ; searchContexts . add ( dn ) ; } }
public void test() { try { Entry configEntry = null ; code_block = TryStatement ;  code_block = IfStatement ; code_block = IfStatement ; } catch ( ClassNotFoundException e ) { String msg = I18n . err ( I18n . ERR_293 , name ) ; LOG . debug ( msg ) ; throw new ClassNotFoundException ( msg ) ; } catch ( Exception e ) { String msg = I18n . err ( I18n . ERR_70 , name ) ; LOG . error ( msg , e ) ; throw new ClassNotFoundException ( msg ) ; } }
public void test() { try { Entry configEntry = null ; code_block = TryStatement ;  code_block = IfStatement ; code_block = IfStatement ; } catch ( ClassNotFoundException e ) { String msg = I18n . err ( I18n . ERR_293 , name ) ; LOG . debug ( msg ) ; throw new ClassNotFoundException ( msg ) ; } catch ( Exception e ) { String msg = I18n . err ( I18n . ERR_70 , name ) ; LOG . error ( msg , e ) ; throw new ClassNotFoundException ( msg ) ; } }
public void test() { try { code_block = ForStatement ; } catch ( ConfigurationException e ) { logger . error ( "Error in restore sahi config properties" , e ) ; } }
@ Override public void stop ( ) { mLogger . debug ( "==> LocalFileLogBuffer.stop()" ) ; DestinationDispatcherThread < T > dispatcherThread = mDispatcherThread ; mDispatcherThread = null ; code_block = IfStatement ; closeFile ( ) ; mLogger . debug ( "<== LocalFileLogBuffer.stop()" ) ; }
public void test() { try { dispatcherThread . join ( ) ; } catch ( InterruptedException e ) { mLogger . warn ( "LocalFileLogBuffer.stop(): failed in waiting for DispatcherThread" , e ) ; } }
@ Override public void stop ( ) { mLogger . debug ( "==> LocalFileLogBuffer.stop()" ) ; DestinationDispatcherThread < T > dispatcherThread = mDispatcherThread ; mDispatcherThread = null ; code_block = IfStatement ; closeFile ( ) ; mLogger . debug ( "<== LocalFileLogBuffer.stop()" ) ; }
@ Test public void testLogInitialize ( ) throws Exception { log = new JavaLogger ( ) ; ( ( JavaLogger ) log ) . setWorkDirectory ( U . defaultWorkDirectory ( ) ) ; ( ( LoggerNodeIdAware ) log ) . setNodeId ( UUID . fromString ( "00000000-1111-2222-3333-444444444444" ) ) ; System . out . println ( log . toString ( ) ) ; assertTrue ( log . toString ( ) . contains ( "JavaLogger" ) ) ; assertTrue ( log . toString ( ) . contains ( JavaLogger . DFLT_CONFIG_PATH ) ) ; if ( log . isDebugEnabled ( ) ) log . debug ( "This is 'debug' message." ) ; assert log . isInfoEnabled ( ) ; log . info ( "This is 'info' message." ) ; log . warning ( "This is 'warning' message." ) ; log . warning ( "This is 'warning' message." , new Exception ( "It's a test warning exception" ) ) ; log . error ( "This is 'error' message." ) ; log . error ( "This is 'error' message." , new Exception ( "It's a test error exception" ) ) ; assert log . getLogger ( JavaLoggerTest . class . getName ( ) ) instanceof JavaLogger ; assert log . fileName ( ) != null ; assert ! log . fileName ( ) . contains ( "%" ) ; }
@ Test public void testLogInitialize ( ) throws Exception { log = new JavaLogger ( ) ; ( ( JavaLogger ) log ) . setWorkDirectory ( U . defaultWorkDirectory ( ) ) ; ( ( LoggerNodeIdAware ) log ) . setNodeId ( UUID . fromString ( "00000000-1111-2222-3333-444444444444" ) ) ; System . out . println ( log . toString ( ) ) ; assertTrue ( log . toString ( ) . contains ( "JavaLogger" ) ) ; assertTrue ( log . toString ( ) . contains ( JavaLogger . DFLT_CONFIG_PATH ) ) ; if ( log . isDebugEnabled ( ) ) log . debug ( "This is 'debug' message." ) ; assert log . isInfoEnabled ( ) ; log . info ( "This is 'info' message." ) ; log . warning ( "This is 'warning' message." ) ; log . warning ( "This is 'warning' message." , new Exception ( "It's a test warning exception" ) ) ; log . error ( "This is 'error' message." ) ; log . error ( "This is 'error' message." , new Exception ( "It's a test error exception" ) ) ; assert log . getLogger ( JavaLoggerTest . class . getName ( ) ) instanceof JavaLogger ; assert log . fileName ( ) != null ; assert ! log . fileName ( ) . contains ( "%" ) ; }
@ Test public void testLogInitialize ( ) throws Exception { log = new JavaLogger ( ) ; ( ( JavaLogger ) log ) . setWorkDirectory ( U . defaultWorkDirectory ( ) ) ; ( ( LoggerNodeIdAware ) log ) . setNodeId ( UUID . fromString ( "00000000-1111-2222-3333-444444444444" ) ) ; System . out . println ( log . toString ( ) ) ; assertTrue ( log . toString ( ) . contains ( "JavaLogger" ) ) ; assertTrue ( log . toString ( ) . contains ( JavaLogger . DFLT_CONFIG_PATH ) ) ; if ( log . isDebugEnabled ( ) ) log . debug ( "This is 'debug' message." ) ; assert log . isInfoEnabled ( ) ; log . info ( "This is 'info' message." ) ; log . warning ( "This is 'warning' message." ) ; log . warning ( "This is 'warning' message." , new Exception ( "It's a test warning exception" ) ) ; log . error ( "This is 'error' message." ) ; log . error ( "This is 'error' message." , new Exception ( "It's a test error exception" ) ) ; assert log . getLogger ( JavaLoggerTest . class . getName ( ) ) instanceof JavaLogger ; assert log . fileName ( ) != null ; assert ! log . fileName ( ) . contains ( "%" ) ; }
public void test() { try { IOUtils . copy ( stream . getInputStream ( ) , out ) ; } catch ( IOException ex ) { LoggerFactory . getLogger ( RestFileResource . class ) . error ( ex . getMessage ( ) , ex ) ; throw ex ; } }
public void test() { try { long requestTimestamp = Instant . now ( ) . getMillis ( ) ; client . ingestHL7v2Message ( hl7v2Store . get ( ) , model ) ; messageIngestLatencyMs . update ( Instant . now ( ) . getMillis ( ) - requestTimestamp ) ; } catch ( Exception e ) { failedMessageWrites . inc ( ) ; LOG . warn ( String . format ( "Failed to ingest message Error: %s Stacktrace: %s" , e . getMessage ( ) , Throwables . getStackTraceAsString ( e ) ) ) ; HealthcareIOError < HL7v2Message > err = HealthcareIOError . of ( msg , e ) ; LOG . warn ( String . format ( "%s %s" , err . getErrorMessage ( ) , err . getStackTrace ( ) ) ) ; context . output ( err ) ; } }
public void test() { try { long requestTimestamp = Instant . now ( ) . getMillis ( ) ; client . ingestHL7v2Message ( hl7v2Store . get ( ) , model ) ; messageIngestLatencyMs . update ( Instant . now ( ) . getMillis ( ) - requestTimestamp ) ; } catch ( Exception e ) { failedMessageWrites . inc ( ) ; LOG . warn ( String . format ( "Failed to ingest message Error: %s Stacktrace: %s" , e . getMessage ( ) , Throwables . getStackTraceAsString ( e ) ) ) ; HealthcareIOError < HL7v2Message > err = HealthcareIOError . of ( msg , e ) ; LOG . warn ( String . format ( "%s %s" , err . getErrorMessage ( ) , err . getStackTrace ( ) ) ) ; context . output ( err ) ; } }
public void test() { if ( getInitParameters ( ) . isDebug ( ) ) { LOGGER . debug ( "New mail - sender: {}, recipients: {}, name: {}, remoteHost: {}, remoteAddr: {}, state: {}, lastUpdated: {}, errorMessage: {}" , newMail . getMaybeSender ( ) , newMail . getRecipients ( ) , newMail . getName ( ) , newMail . getRemoteHost ( ) , newMail . getRemoteAddr ( ) , newMail . getState ( ) , newMail . getLastUpdated ( ) , newMail . getErrorMessage ( ) ) ; } }
@ Test public void testDenseSearch ( ) throws Exception { EntityManager em = app . getEntityManager ( ) ; assertNotNull ( em ) ; int numEntities = 25 ; float minLatitude = 48.32455f ; float maxLatitude = 48.46481f ; float minLongitude = 9.89561f ; float maxLongitude = 10.0471f ; float latitudeDelta = ( maxLatitude - minLatitude ) / numEntities ; float longitudeDelta = ( maxLongitude - minLongitude ) / numEntities ; code_block = ForStatement ; app . waitForQueueDrainAndRefreshIndex ( ) ; int limit = 8 ; long startTime = System . currentTimeMillis ( ) ; Query query = Query . fromQL ( "location within 1000 of 48.38626, 9.94175" ) ; query . setLimit ( limit ) ; Results results = em . searchCollection ( em . getApplicationRef ( ) , "stores" , query ) ; assertEquals ( 0 , results . size ( ) ) ; long endTime = System . currentTimeMillis ( ) ; logger . info ( "Runtime took {} milliseconds to search" , endTime - startTime ) ; }
public void test() { if ( iteration % printIterations == 0 ) { double score = model . score ( ) ; log . info ( "Score at iteration {} is {}" , iteration , score ) ; } }
public void test() { if ( isNotNullWithoutDefault ) { String sql = String . format ( "ALTER TABLE %s MODIFY %s VARCHAR(40) CHARACTER SET ascii COLLATE ascii_bin" , tableName , col . LAST_IP ) ; st . execute ( sql ) ; logger . info ( "Changed last login column to allow NULL values. Please verify the registration feature " + "if you are hooking into a forum." ) ; } }
public void test() { if ( isErrorEnabled ( ) ) { FormattingTuple formattingTuple = MessageFormatter . format ( format , argument ) ; _log . error ( formattingTuple . getMessage ( ) , formattingTuple . getThrowable ( ) ) ; } }
public void test() { if ( logger . isTraceEnabled ( LogMarker . DM_VERBOSE ) ) { logger . trace ( LogMarker . DM_VERBOSE , "DistTXCommitPhaseTwoReplyMessage process invoking reply processor with processorId:{}" , this . processorId ) ; } }
public void test() { if ( logger . isTraceEnabled ( LogMarker . DM_VERBOSE ) ) { logger . trace ( LogMarker . DM_VERBOSE , "DistTXCommitPhaseTwoReplyMessage processor not found" ) ; } }
public void test() { if ( task == null ) { log . warn ( "Task '" + taskId + "' not found" ) ; throw new ObjectRetrievalFailureException ( TaskDefinition . class , taskId ) ; } }
public void test() { try { setterMethod . invoke ( step , value ) ; return true ; } catch ( Exception e ) { LOG . warn ( e . getMessage ( ) , e ) ; } }
public void test() { if ( LOG . isWarnEnabled ( ) ) { LOG . warn ( "Cannot find setter in class '{}' for [{}] column" , step . getClass ( ) . getName ( ) , column . getColumnName ( ) ) ; } }
public void test() { if ( StringUtils . isBlank ( tokenValue ) ) { log . error ( "token is null." ) ; unionFailResponse ( servletResponse ) ; return false ; } }
public void test() { try { subject . login ( token ) ; } catch ( Exception e ) { log . error ( "token is warning. token : {}." , tokenValue , e ) ; unionFailResponse ( servletResponse ) ; return false ; } }
public void test() { try { InputStream fin = VocabDefinitions . class . getClassLoader ( ) . getResourceAsStream ( CF_PARAMETERS ) ; InputStreamReader freader = new InputStreamReader ( fin ) ; cfSet = new HashSet < String > ( ) ; StringBuilder builder = new StringBuilder ( ) ; char [ ] buffer = new char [ 1 ] ; code_block = WhileStatement ; } catch ( Exception ex ) { _log . error ( ex . toString ( ) ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( exception , exception ) ; } }
@ Override public synchronized void start ( ) { LOG . info ( "Starting Recon Task Controller." ) ; executorService = Executors . newFixedThreadPool ( threadCount ) ; }
public void test() { try { countryCentrePoints = CountryCentrePoints . getInstance ( alaConfig . getLocationInfoConfig ( ) ) ; stateProvinceCentrePoints = StateProvinceCentrePoints . getInstance ( alaConfig . getLocationInfoConfig ( ) ) ; stateProvinceParser = StateProvinceParser . getInstance ( alaConfig . getLocationInfoConfig ( ) . getStateProvinceNamesFile ( ) ) ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; throw new RuntimeException ( e . getMessage ( ) ) ; } }
public void test() { if ( ! ledgerRootExists ) { log . info ( "There is no existing cluster with ledgersRootPath: {} in ZKServers: {}, " + "so exiting nuke operation" , ledgersRootPath , zkServers ) ; return true ; } }
public void test() { if ( rwBookies != null && ! rwBookies . isEmpty ( ) ) { log . error ( "Bookies are still up and connected to this cluster, " + "stop all bookies before nuking the cluster" ) ; return false ; } }
public void test() { if ( roBookies != null && ! roBookies . isEmpty ( ) ) { log . error ( "Readonly Bookies are still up and connected to this cluster, " + "stop all bookies before nuking the cluster" ) ; return false ; } }
public void test() { try { Map < Locale , String > nameMap = LocalizationUtil . getLocalizationMap ( nameMapLanguageIds , nameMapValues ) ; Map < Locale , String > descriptionMap = LocalizationUtil . getLocalizationMap ( descriptionMapLanguageIds , descriptionMapValues ) ; com . liferay . dynamic . data . mapping . model . DDMTemplate returnValue = DDMTemplateServiceUtil . updateTemplate ( templateId , classPK , nameMap , descriptionMap , type , mode , language , script , cacheable , serviceContext ) ; return com . liferay . dynamic . data . mapping . model . DDMTemplateSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
@ RequestMapping ( value = "/api/samples/{sampleId}/metadata" , method = RequestMethod . GET ) public ModelMap getSampleMetadata ( @ PathVariable Long sampleId ) { logger . trace ( "Getting sample metadata to " + sampleId ) ; ModelMap modelMap = new ModelMap ( ) ; Sample s = sampleService . read ( sampleId ) ; Set < MetadataEntry > metadataForSample = sampleService . getMetadataForSample ( s ) ; SampleMetadataResponse response = buildSampleMetadataResponse ( s , metadataForSample ) ; modelMap . addAttribute ( RESTGenericController . RESOURCE_NAME , response ) ; return modelMap ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "processing QueueSynchronizationMessage region {} does not exist." , regionName ) ; } }
public void test() { try { graph . commit ( ) ; } catch ( Exception ex ) { LOG . warn ( "Graph transaction commit failed: {}; attempting to rollback graph transaction." , ex ) ; graphRollback ( ) ; } }
public void test() { try { txId = ( TXId ) context . getArguments ( ) ; } catch ( ClassCastException e ) { logger . info ( "CommitFunction should be invoked with a TransactionId as an argument i.e. setArguments(txId).execute(function)" ) ; throw e ; } }
public void test() { if ( isDebugEnabled ) { logger . debug ( "CommitFunction: for transaction: {} returning result: {}" , txId , result ) ; } }
public void test() { if ( isDebugEnabled ) { logger . debug ( "CommitFunction: resumed transaction: {}" , txId ) ; } }
public void test() { if ( isDebugEnabled ) { logger . debug ( "CommitFunction: resumed transaction: {}" , txId ) ; } }
public void test() { if ( isDebugEnabled ) { logger . debug ( "CommitFunction: resumed transaction: {}" , txId ) ; } }
public void test() { if ( exc instanceof Exception ) { context . runOnContext ( ( v ) code_block = LoopStatement ; ) ; } else { log . error ( "Error occurred" , exc ) ; } }
public void test() { try { return Files . exists ( localPath ) && calculateLocalAttachmentHash ( localPath ) . equals ( attachment . getSha1 ( ) ) ? Optional . of ( localPath ) : Optional . empty ( ) ; } catch ( SW360ClientException e ) { LOG . warn ( "Failed to verify local attachment file" , e ) ; return Optional . empty ( ) ; } }
public void updateConfigGroup ( String clusterName , String groupId , ApiConfigGroup configGroup ) throws AmbariApiException { String confGroup = ApiUtils . objectToJson ( configGroup ) ; logger . debug ( "Updating config group: " + confGroup ) ; Response response = null ; code_block = TryStatement ;  handleAmbariResponse ( response ) ; }
public void test() { try { attemptSchedule ( taskNode ) ; schedulingAgent . schedule ( taskNode , new LifecycleState ( ) ) ; logger . info ( "Successfully scheduled {} to run every {}" , taskNode , taskNode . getSchedulingPeriod ( ) ) ; } catch ( final Exception e ) { logger . error ( "Could not schedule {} to run. Will try again in 30 seconds." , taskNode , e ) ; componentLifeCycleThreadPool . schedule ( this , 30 , TimeUnit . SECONDS ) ; } }
public void test() { if ( sessionId . isEmpty ( ) ) { setUserState ( UserState . Disconnected ) ; lostConnection ( ) ; logger . trace ( "USER - lost connection: " + userName + " id: " + userId ) ; } else-if ( userState == UserState . Created ) { setUserState ( UserState . Connected ) ; logger . trace ( "USER - created: " + userName + " id: " + userId ) ; } else { setUserState ( UserState . Connected ) ; reconnect ( ) ; logger . trace ( "USER - reconnected: " + userName + " id: " + userId ) ; } }
public void test() { if ( sessionId . isEmpty ( ) ) { setUserState ( UserState . Disconnected ) ; lostConnection ( ) ; logger . trace ( "USER - lost connection: " + userName + " id: " + userId ) ; } else-if ( userState == UserState . Created ) { setUserState ( UserState . Connected ) ; logger . trace ( "USER - created: " + userName + " id: " + userId ) ; } else { setUserState ( UserState . Connected ) ; reconnect ( ) ; logger . trace ( "USER - reconnected: " + userName + " id: " + userId ) ; } }
public void test() { if ( sessionId . isEmpty ( ) ) { setUserState ( UserState . Disconnected ) ; lostConnection ( ) ; logger . trace ( "USER - lost connection: " + userName + " id: " + userId ) ; } else-if ( userState == UserState . Created ) { setUserState ( UserState . Connected ) ; logger . trace ( "USER - created: " + userName + " id: " + userId ) ; } else { setUserState ( UserState . Connected ) ; reconnect ( ) ; logger . trace ( "USER - reconnected: " + userName + " id: " + userId ) ; } }
public void test() { if ( ind . getMapProtocolVersion ( ) >= 3 ) { this . logger . debug ( "onSendAuthenticationInfoResp_V3" ) ; te = TestEvent . createReceivedEvent ( EventType . SendAuthenticationInfoResp_V3 , ind , sequence ++ ) ; } else { this . logger . debug ( "onSendAuthenticationInfoResp_V2" ) ; te = TestEvent . createReceivedEvent ( EventType . SendAuthenticationInfoResp_V2 , ind , sequence ++ ) ; } }
public void test() { if ( ind . getMapProtocolVersion ( ) >= 3 ) { this . logger . debug ( "onSendAuthenticationInfoResp_V3" ) ; te = TestEvent . createReceivedEvent ( EventType . SendAuthenticationInfoResp_V3 , ind , sequence ++ ) ; } else { this . logger . debug ( "onSendAuthenticationInfoResp_V2" ) ; te = TestEvent . createReceivedEvent ( EventType . SendAuthenticationInfoResp_V2 , ind , sequence ++ ) ; } }
public void onServiceStartedEvent ( ServiceStartedEvent event , ActivityContextInterface aci , EventContext eventContext ) { ServiceID serviceID = event . getService ( ) ; this . logger . info ( "Rx: onServiceStartedEvent: event=" + event + ", serviceID=" + serviceID ) ; SbbStates . setSmscRxSmppServerServiceState ( true ) ; }
public void test() { try { MethodKey methodKey = new MethodKey ( DDMStructureVersionServiceUtil . class , "getStructureVersion" , _getStructureVersionParameterTypes1 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , structureVersionId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . dynamic . data . mapping . model . DDMStructureVersion ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { trackModels ( true ) ; } catch ( Throwable t ) { LOGGER . error ( "Model tracking failed for core: {}" , coreName , t ) ; } }
public void test() { try { prepare ( 4 ) ; FakeAllocatableAction instance0 = new FakeAllocatableAction ( fao , 0 ) ; instance0 . assignResource ( rs ) ; FakeAllocatableAction instance1 = new FakeAllocatableAction ( fao , 1 ) ; instance1 . assignResource ( rs ) ; FakeAllocatableAction instance2 = new FakeAllocatableAction ( fao , 2 ) ; instance2 . assignResource ( rs ) ; FakeAllocatableAction instance3 = new FakeAllocatableAction ( fao , 3 ) ; instance3 . assignResource ( rs ) ; instance0 . tryToLaunch ( ) ; completed ( instance0 ) ; checkExecutions ( new int [ ] code_block = "" ; ) ; } catch ( Exception e ) { LOGGER . error ( e ) ; fail ( e . getMessage ( ) ) ; } }
public void test() { try { IEntityManager entityManager = this . getEntityManager ( ) ; Map < String , AttributeInterface > attributeTypes = entityManager . getEntityAttributePrototypes ( ) ; Iterator < AttributeInterface > attributeIter = attributeTypes . values ( ) . iterator ( ) ; code_block = WhileStatement ; Collections . sort ( attributes , new BeanComparator ( "type" ) ) ; } catch ( Throwable t ) { _logger . error ( "Error extracting the allowed types of attribute elements" , t ) ; throw new RuntimeException ( "Error extracting the allowed types of attribute elements" , t ) ; } }
public static TokenizerEngine create ( ) { final TokenizerEngine engine = doCreate ( ) ; StaticLog . debug ( "Use [{}] Tokenizer Engine As Default." , StrUtil . removeSuffix ( engine . getClass ( ) . getSimpleName ( ) , "Engine" ) ) ; return engine ; }
public void test() { if ( ! config . isSet ( UnityServerConfiguration . SMS_CONF ) ) { log . info ( "SMS configuration file is not set, SMS notification channel won't be loaded." ) ; return ; } }
public void test() { try { code_block = IfStatement ; File smsCfgFile = config . getFileValue ( UnityServerConfiguration . SMS_CONF , false ) ; String smsCfg = FileUtils . readFileToString ( smsCfgFile , Charset . defaultCharset ( ) ) ; NotificationChannel smsCh = new NotificationChannel ( UnityServerConfiguration . DEFAULT_SMS_CHANNEL , "Default SMS channel" , smsCfg , SMSFacility . NAME ) ; notManagement . addNotificationChannel ( smsCh ) ; log . info ( "Created a notification channel: " + smsCh . getName ( ) + " [" + smsCh . getFacilityId ( ) + "]" ) ; } catch ( Exception e ) { throw new ConfigurationException ( "Can't load SMS notification channel configuration" , e ) ; } }
@ Test public void simplestUsage ( ) { Log log = LogFactory . getLog ( CommonsLoggingApiTest . class ) ; log . info ( "INFO" , new Throwable ( ) ) ; log . trace ( "TRACE" ) ; }
@ Test public void simplestUsage ( ) { Log log = LogFactory . getLog ( CommonsLoggingApiTest . class ) ; log . info ( "INFO" , new Throwable ( ) ) ; log . trace ( "TRACE" ) ; }
public void test() { { logger . debug ( "Deleting a named, parametrized Query with ID ({})." , queryId ) ; deleteNamedQuery ( PARAMETERIZED_QUERIES_REGION , queryId ) ; compiledQueries . remove ( queryId ) ; return new ResponseEntity < > ( HttpStatus . OK ) ; } }
public void test() { if ( client . isErrorRecoverable ( mktoResult . getErrors ( ) ) ) { LOG . debug ( "Recoverable error during operation : `{}`. Retrying..." , mktoResult . getErrorsString ( ) ) ; waitForRetryAttempInterval ( ) ; continue ; } else { LOG . error ( "Unrecoverable error : `{}`." , mktoResult . getErrorsString ( ) ) ; break ; } }
public void test() { if ( client . isErrorRecoverable ( mktoResult . getErrors ( ) ) ) { LOG . debug ( "Recoverable error during operation : `{}`. Retrying..." , mktoResult . getErrorsString ( ) ) ; waitForRetryAttempInterval ( ) ; continue ; } else { LOG . error ( "Unrecoverable error : `{}`." , mktoResult . getErrorsString ( ) ) ; break ; } }
@ Override public List < String > findUserGroupNames ( String userGroupName , boolean caseInsensitive ) throws JargonException { log . info ( "findUserGroups()" ) ; code_block = IfStatement ; log . info ( "caseInsensitive:{}" , caseInsensitive ) ; log . info ( "for user group name:{}" , userGroupName ) ; List < String > userGroups = new ArrayList < String > ( ) ; IRODSGenQueryBuilder builder = new IRODSGenQueryBuilder ( true , caseInsensitive , null ) ; code_block = TryStatement ;  IRODSGenQueryExecutor irodsGenQueryExecutor = getGenQueryExecutor ( ) ; StringBuilder sb = new StringBuilder ( ) ; sb . append ( userGroupName . trim ( ) ) ; sb . append ( '%' ) ; builder . addConditionAsGenQueryField ( RodsGenQueryEnum . COL_USER_GROUP_NAME , QueryConditionOperators . LIKE , sb . toString ( ) ) . addConditionAsGenQueryField ( RodsGenQueryEnum . COL_USER_TYPE , QueryConditionOperators . EQUAL , RODS_GROUP ) ; IRODSQueryResultSet resultSet = null ; code_block = TryStatement ;  code_block = ForStatement ; return userGroups ; }
@ Override public List < String > findUserGroupNames ( String userGroupName , boolean caseInsensitive ) throws JargonException { log . info ( "findUserGroups()" ) ; code_block = IfStatement ; log . info ( "caseInsensitive:{}" , caseInsensitive ) ; log . info ( "for user group name:{}" , userGroupName ) ; List < String > userGroups = new ArrayList < String > ( ) ; IRODSGenQueryBuilder builder = new IRODSGenQueryBuilder ( true , caseInsensitive , null ) ; code_block = TryStatement ;  IRODSGenQueryExecutor irodsGenQueryExecutor = getGenQueryExecutor ( ) ; StringBuilder sb = new StringBuilder ( ) ; sb . append ( userGroupName . trim ( ) ) ; sb . append ( '%' ) ; builder . addConditionAsGenQueryField ( RodsGenQueryEnum . COL_USER_GROUP_NAME , QueryConditionOperators . LIKE , sb . toString ( ) ) . addConditionAsGenQueryField ( RodsGenQueryEnum . COL_USER_TYPE , QueryConditionOperators . EQUAL , RODS_GROUP ) ; IRODSQueryResultSet resultSet = null ; code_block = TryStatement ;  code_block = ForStatement ; return userGroups ; }
@ Override public List < String > findUserGroupNames ( String userGroupName , boolean caseInsensitive ) throws JargonException { log . info ( "findUserGroups()" ) ; code_block = IfStatement ; log . info ( "caseInsensitive:{}" , caseInsensitive ) ; log . info ( "for user group name:{}" , userGroupName ) ; List < String > userGroups = new ArrayList < String > ( ) ; IRODSGenQueryBuilder builder = new IRODSGenQueryBuilder ( true , caseInsensitive , null ) ; code_block = TryStatement ;  IRODSGenQueryExecutor irodsGenQueryExecutor = getGenQueryExecutor ( ) ; StringBuilder sb = new StringBuilder ( ) ; sb . append ( userGroupName . trim ( ) ) ; sb . append ( '%' ) ; builder . addConditionAsGenQueryField ( RodsGenQueryEnum . COL_USER_GROUP_NAME , QueryConditionOperators . LIKE , sb . toString ( ) ) . addConditionAsGenQueryField ( RodsGenQueryEnum . COL_USER_TYPE , QueryConditionOperators . EQUAL , RODS_GROUP ) ; IRODSQueryResultSet resultSet = null ; code_block = TryStatement ;  code_block = ForStatement ; return userGroups ; }
public void test() { try { builder . addSelectAsGenQueryValue ( RodsGenQueryEnum . COL_USER_GROUP_NAME ) ; } catch ( GenQueryBuilderException e ) { log . error ( "error building query" , e ) ; throw new JargonException ( "query builder error" , e ) ; } }
@ Override protected Object executeJob ( int gridSize , String type ) throws GridException { log . info ( ">>> Starting new grid node [currGridSize=" + gridSize + ", arg=" + type + "]" ) ; if ( type == null ) throw new IllegalArgumentException ( "Node type to start should be specified." ) ; GridConfiguration cfg = getConfig ( type ) ; String gridName = cfg . getGridName ( ) + " (" + UUID . randomUUID ( ) + ")" ; cfg . setGridName ( gridName ) ; Grid g = G . start ( cfg ) ; log . info ( ">>> Grid started [nodeId=" + g . localNode ( ) . id ( ) + ", name='" + g . name ( ) + "']" ) ; return true ; }
@ Override protected Object executeJob ( int gridSize , String type ) throws GridException { log . info ( ">>> Starting new grid node [currGridSize=" + gridSize + ", arg=" + type + "]" ) ; if ( type == null ) throw new IllegalArgumentException ( "Node type to start should be specified." ) ; GridConfiguration cfg = getConfig ( type ) ; String gridName = cfg . getGridName ( ) + " (" + UUID . randomUUID ( ) + ")" ; cfg . setGridName ( gridName ) ; Grid g = G . start ( cfg ) ; log . info ( ">>> Grid started [nodeId=" + g . localNode ( ) . id ( ) + ", name='" + g . name ( ) + "']" ) ; return true ; }
public void test() { if ( trustCentre == null ) { logger . debug ( "SEP Extension: Trust Centre not found in network nodes list" ) ; updateClientState ( SmartEnergyClientState . DISCOVER_TRUST_CENTRE ) ; return ; } }
public void test() { switch ( seState ) { case DISCOVER_KEY_ESTABLISHMENT_CLUSTER : code_block = IfStatement ; trustCentre = networkManager . getNode ( 0 ) ; code_block = IfStatement ; ZigBeeNode updatedTrustCentre = new ZigBeeNode ( networkManager , trustCentre . getIeeeAddress ( ) ) ; trustCenterKeyEstablishmentEndpoint = response . getMatchList ( ) . get ( 0 ) ; logger . debug ( "SEP Extension: SEP discovery is using endpoint {} for KeyEstablishment" , trustCenterKeyEstablishmentEndpoint ) ; ZigBeeEndpoint keEndpoint = new ZigBeeEndpoint ( trustCentre , trustCenterKeyEstablishmentEndpoint ) ; keEndpoint . setProfileId ( ZigBeeProfileType . ZIGBEE_SMART_ENERGY . getKey ( ) ) ; updatedTrustCentre . addEndpoint ( keEndpoint ) ; ZclKeyEstablishmentCluster keCluster = new ZclKeyEstablishmentCluster ( keEndpoint ) ; keEndpoint . addInputCluster ( keCluster ) ; networkManager . updateNode ( updatedTrustCentre ) ; updateClientState ( SmartEnergyClientState . PERFORM_KEY_ESTABLISHMENT ) ; break ; case DISCOVER_METERING_SERVERS : ZigBeeNode node = networkManager . getNode ( response . getSourceAddress ( ) . getAddress ( ) ) ; code_block = IfStatement ; code_block = IfStatement ; ZigBeeNode updatedNode = new ZigBeeNode ( networkManager , node . getIeeeAddress ( ) , node . getNetworkAddress ( ) ) ; code_block = ForStatement ; setProfileSecurity ( updatedNode ) ; networkManager . updateNode ( updatedNode ) ; discoveryComplete ( ) ; break ; case DISCOVER_KEEP_ALIVE : code_block = IfStatement ; trustCenterKeepAliveEndpoint = response . getMatchList ( ) . get ( 0 ) ; logger . debug ( "SEP Extension: SEP discovery is using endpoint {} for KeepAlive" , trustCenterKeepAliveEndpoint ) ; updateClientState ( SmartEnergyClientState . KEEP_ALIVE ) ; break ; default : break ; } }
public void test() { if ( ! cbkeProvider . isAuthorised ( node . getIeeeAddress ( ) ) ) { logger . debug ( "{}: SEP Extension: SEP discovery node is not authorised" , node . getIeeeAddress ( ) ) ; return ; } }
public void test() { try { requestSimpleDescriptor ( endpoint ) ; } catch ( InterruptedException | ExecutionException e ) { logger . debug ( "{}: SEP Extension: Endpoint {} Exception getting simple descriptor" , node . getIeeeAddress ( ) , endpoint . getEndpointAddress ( ) ) ; } }
public void test() { switch ( seState ) { case DISCOVER_KEY_ESTABLISHMENT_CLUSTER : code_block = IfStatement ; trustCentre = networkManager . getNode ( 0 ) ; code_block = IfStatement ; ZigBeeNode updatedTrustCentre = new ZigBeeNode ( networkManager , trustCentre . getIeeeAddress ( ) ) ; trustCenterKeyEstablishmentEndpoint = response . getMatchList ( ) . get ( 0 ) ; logger . debug ( "SEP Extension: SEP discovery is using endpoint {} for KeyEstablishment" , trustCenterKeyEstablishmentEndpoint ) ; ZigBeeEndpoint keEndpoint = new ZigBeeEndpoint ( trustCentre , trustCenterKeyEstablishmentEndpoint ) ; keEndpoint . setProfileId ( ZigBeeProfileType . ZIGBEE_SMART_ENERGY . getKey ( ) ) ; updatedTrustCentre . addEndpoint ( keEndpoint ) ; ZclKeyEstablishmentCluster keCluster = new ZclKeyEstablishmentCluster ( keEndpoint ) ; keEndpoint . addInputCluster ( keCluster ) ; networkManager . updateNode ( updatedTrustCentre ) ; updateClientState ( SmartEnergyClientState . PERFORM_KEY_ESTABLISHMENT ) ; break ; case DISCOVER_METERING_SERVERS : ZigBeeNode node = networkManager . getNode ( response . getSourceAddress ( ) . getAddress ( ) ) ; code_block = IfStatement ; code_block = IfStatement ; ZigBeeNode updatedNode = new ZigBeeNode ( networkManager , node . getIeeeAddress ( ) , node . getNetworkAddress ( ) ) ; code_block = ForStatement ; setProfileSecurity ( updatedNode ) ; networkManager . updateNode ( updatedNode ) ; discoveryComplete ( ) ; break ; case DISCOVER_KEEP_ALIVE : code_block = IfStatement ; trustCenterKeepAliveEndpoint = response . getMatchList ( ) . get ( 0 ) ; logger . debug ( "SEP Extension: SEP discovery is using endpoint {} for KeepAlive" , trustCenterKeepAliveEndpoint ) ; updateClientState ( SmartEnergyClientState . KEEP_ALIVE ) ; break ; default : break ; } }
public ApplicationMap selectApplicationMapWithScatterData ( FilteredMapServiceOption option ) { StopWatch watch = new StopWatch ( ) ; watch . start ( ) ; final List < List < SpanBo > > filterList = selectFilteredSpan ( option . getTransactionIdList ( ) , option . getFilter ( ) , option . getColumnGetCount ( ) ) ; FilteredMapBuilder filteredMapBuilder = new FilteredMapBuilder ( applicationFactory , registry , option . getOriginalRange ( ) , option . getVersion ( ) ) ; filteredMapBuilder . serverMapDataFilter ( serverMapDataFilter ) ; filteredMapBuilder . addTransactions ( filterList ) ; FilteredMap filteredMap = filteredMapBuilder . build ( ) ; ApplicationMap map = createMap ( option , filteredMap ) ; Map < Application , ScatterData > applicationScatterData = filteredMap . getApplicationScatterData ( option . getOriginalRange ( ) . getFrom ( ) , option . getOriginalRange ( ) . getTo ( ) , option . getxGroupUnit ( ) , option . getyGroupUnit ( ) ) ; ApplicationMapWithScatterData applicationMapWithScatterData = new ApplicationMapWithScatterData ( map , applicationScatterData ) ; watch . stop ( ) ; logger . debug ( "Select filtered application map elapsed. {}ms" , watch . getTotalTimeMillis ( ) ) ; return applicationMapWithScatterData ; }
public void test() { if ( interpreterGroup == null ) { LOGGER . warn ( "Unable to register interpreter process, because no such interpreterGroup: {}" , registerInfo . getInterpreterGroupId ( ) ) ; return ; } }
public void test() { if ( interpreterProcess == null ) { LOGGER . warn ( "Unable to register interpreter process, because no interpreter process associated with " + "interpreterGroup: {}" , registerInfo . getInterpreterGroupId ( ) ) ; return ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( exception , exception ) ; } }
public void test() { if ( ! type . equals ( DataQuerySnapshot . TYPE ) ) { LOG . error ( "Found type {} in the query json, but expected type {}." , type , DataQuerySnapshot . TYPE ) ; return null ; } }
public void test() { if ( ! fieldsSet . add ( field ) ) { LOG . error ( "The field {} was listed more than once, this is an invalid query." , field ) ; } }
@ Test ( expected = InvalidQueryException . class ) public void updateInvalidScript ( ) { Version fromVersion = Version . of ( 1 ) ; Version toVersion = Version . of ( 2 ) ; String schema = "schema" ; expect ( schemaProducer . schema ( fromVersion , toVersion ) ) . andReturn ( schema ) ; expect ( session . execute ( schema ) ) . andThrow ( new InvalidQueryException ( "expected message" ) ) ; logger . info ( "Executing CQL migration from version {} to {}" , fromVersion . get ( ) , toVersion . get ( ) ) ; expectLastCall ( ) ; logger . debug ( "CQL: {}" , schema ) ; expectLastCall ( ) ; mocks . replay ( ) ; code_block = TryStatement ;  }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( exception , exception ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { XLog . error ( "" , e ) ; return false ; } }
public void test() { if ( location . getIndexDirectory ( ) . equals ( oldIndexDir ) ) { final IndexLocation updatedLocation = new IndexLocation ( newIndexDir , location . getIndexStartTimestamp ( ) , location . getPartitionName ( ) ) ; itr . set ( updatedLocation ) ; replaced = true ; logger . debug ( "Replaced {} with {}" , location , updatedLocation ) ; } }
public void test() { try { FileUtils . deleteFile ( oldIndexDir , true ) ; } catch ( IOException e ) { logger . warn ( "Failed to delete index directory {}; this directory should be cleaned up manually" , oldIndexDir , e ) ; } }
public void test() { if ( ArrayUtil . contains ( listener , getListeners ( ) ) ) { LOG . error ( "Already registered: " + listener ) ; } }
public void test() { try { final KVMStoragePool localStoragePool = _storagePoolMgr . createStoragePool ( _localStorageUUID , "localhost" , - 1 , _localStoragePath , "" , StoragePoolType . Filesystem ) ; final com . cloud . agent . api . StoragePoolInfo pi = new com . cloud . agent . api . StoragePoolInfo ( localStoragePool . getUuid ( ) , cmd . getPrivateIpAddress ( ) , _localStoragePath , _localStoragePath , StoragePoolType . Filesystem , localStoragePool . getCapacity ( ) , localStoragePool . getAvailable ( ) ) ; sscmd = new StartupStorageCommand ( ) ; sscmd . setPoolInfo ( pi ) ; sscmd . setGuid ( pi . getUuid ( ) ) ; sscmd . setDataCenter ( _dcId ) ; sscmd . setResourceType ( Storage . StorageResourceType . STORAGE_POOL ) ; } catch ( final CloudRuntimeException e ) { s_logger . debug ( "Unable to initialize local storage pool: " + e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( SyncDeviceServiceUtil . class , "unregisterSyncDevice" , _unregisterSyncDeviceParameterTypes1 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , uuid ) ; code_block = TryStatement ;  } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { tx . rollback ( ) ; return Status . BACKOFF ; } catch ( Exception e2 ) { log . error ( "Rollback Exception:{}" , e2 ) ; } }
public void test() { try { tx . begin ( ) ; Event event = channel . take ( ) ; code_block = IfStatement ; String data = null ; code_block = IfStatement ; producer . send ( new KeyedMessage < String , String > ( topic , data ) ) ; tx . commit ( ) ; return Status . READY ; } catch ( Exception e ) { code_block = TryStatement ;  log . error ( "KafkaSink Exception:{}" , e ) ; return Status . BACKOFF ; } finally { tx . close ( ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "State get for {} {} {} {}" , pTransformId , userStateId , Arrays . toString ( keyedStateBackend . getCurrentKey ( ) . array ( ) ) , window ) ; } }
public void test() { if ( model . hasPropertyAndNotNull ( ReservedIniKeys . OUTPUT_DIR . getKey ( ) ) ) { code_block = TryStatement ;  } else { Log . error ( "You need to specify the output dir for your workflow using either an override parameter at schedule-time or in your workflow INI file as output_dir!" ) ; } }
public void test() { if ( model . hasPropertyAndNotNull ( ReservedIniKeys . OUTPUT_DIR . getKey ( ) ) ) { code_block = TryStatement ;  } else { Log . error ( "You need to specify the output dir for your workflow using either an override parameter at schedule-time or in your workflow INI file as output_dir!" ) ; } }
public void test() { if ( logger != null && logger . isDebugEnabled ( ) ) { logger . debug ( LogUtil . getMsg ( msg ) , obj ) ; } }
public void test() { try { Thread . sleep ( monitorInterval ) ; } catch ( InterruptedException e ) { LOG . info ( "PingChecker interrupted. - masterName:" + masterName ) ; break ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "currentActiveMaster:" + currentActiveMaster + ", thisMasterName:" + masterName ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { LOG . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "zkclient {} Prefetch data for path: {}" , _uid , path ) ; } }
public void test() { if ( ! pathExists ) { stat = getStat ( path , false ) ; } else { stat = installWatchOnlyPathExist ( path ) ; } }
public void test() { if ( query instanceof GraphCentricQueryBuilder ) { LOG . debug ( "NativeJanusGraphQuery.vertices({}, {}): resultSize={}, {}" , offset , limit , getCountForDebugLog ( it ) , ( ( GraphCentricQueryBuilder ) query ) . constructQuery ( ElementCategory . VERTEX ) ) ; } else { LOG . debug ( "NativeJanusGraphQuery.vertices({}, {}): resultSize={}, {}" , offset , limit , getCountForDebugLog ( it ) , query ) ; } }
public void test() { if ( query instanceof GraphCentricQueryBuilder ) { LOG . debug ( "NativeJanusGraphQuery.vertices({}, {}): resultSize={}, {}" , offset , limit , getCountForDebugLog ( it ) , ( ( GraphCentricQueryBuilder ) query ) . constructQuery ( ElementCategory . VERTEX ) ) ; } else { LOG . debug ( "NativeJanusGraphQuery.vertices({}, {}): resultSize={}, {}" , offset , limit , getCountForDebugLog ( it ) , query ) ; } }
@ Override public void rollback ( ) throws TransactionException { logger . debug ( "Rollback Conn Transaction" ) ; }
public void test() { if ( log . isDebugEnabled ( ) || debug == 1 ) { log . warn ( "alias fail, missing " + LessStrings . join ( p , " / " ) ) ; } }
private int stageBundleForServing ( BundleDeployStatus status , String s3Path ) { _log . info ( "stageBundleForServing(" + s3Path + ")" ) ; int bundlesDownloaded = 0 ; List < String > bundles = listFiles ( s3Path , MAX_RESULTS ) ; code_block = IfStatement ; code_block = ForStatement ; status . setStatus ( BundleDeployStatus . STATUS_COMPLETE ) ; return bundlesDownloaded ; }
public void test() { if ( bundles != null && ! bundles . isEmpty ( ) ) { clearBundleStagingDirectory ( ) ; } else { _log . error ( "no bundles found at path=" + s3Path ) ; return bundlesDownloaded ; } }
public void test() { try { String bundleFileLocation = _localBundleStagingPath + File . separator + bundleFilename ; _log . info ( "unGzip(" + bundleFileLocation + ", " + _localBundleStagingPath + ")" ) ; _fileUtil . unGzip ( new File ( bundleFileLocation ) , new File ( _localBundleStagingPath ) ) ; String tarFilename = parseTarName ( bundleFileLocation ) ; _log . info ( "unTar(" + tarFilename + ", " + _localBundleStagingPath + ")" ) ; _fileUtil . unTar ( new File ( tarFilename ) , new File ( _localBundleStagingPath ) ) ; _log . info ( "deleting bundle tar.gz=" + bundleFileLocation ) ; status . addBundleName ( bundleFilename ) ; new File ( tarFilename ) . delete ( ) ; new File ( bundleFileLocation ) . delete ( ) ; bundlesDownloaded ++ ; } catch ( Exception e ) { _log . error ( "exception exploding bundle=" + bundle , e ) ; } }
public void test() { try { String bundleFileLocation = _localBundleStagingPath + File . separator + bundleFilename ; _log . info ( "unGzip(" + bundleFileLocation + ", " + _localBundleStagingPath + ")" ) ; _fileUtil . unGzip ( new File ( bundleFileLocation ) , new File ( _localBundleStagingPath ) ) ; String tarFilename = parseTarName ( bundleFileLocation ) ; _log . info ( "unTar(" + tarFilename + ", " + _localBundleStagingPath + ")" ) ; _fileUtil . unTar ( new File ( tarFilename ) , new File ( _localBundleStagingPath ) ) ; _log . info ( "deleting bundle tar.gz=" + bundleFileLocation ) ; status . addBundleName ( bundleFilename ) ; new File ( tarFilename ) . delete ( ) ; new File ( bundleFileLocation ) . delete ( ) ; bundlesDownloaded ++ ; } catch ( Exception e ) { _log . error ( "exception exploding bundle=" + bundle , e ) ; } }
public void test() { try { String bundleFileLocation = _localBundleStagingPath + File . separator + bundleFilename ; _log . info ( "unGzip(" + bundleFileLocation + ", " + _localBundleStagingPath + ")" ) ; _fileUtil . unGzip ( new File ( bundleFileLocation ) , new File ( _localBundleStagingPath ) ) ; String tarFilename = parseTarName ( bundleFileLocation ) ; _log . info ( "unTar(" + tarFilename + ", " + _localBundleStagingPath + ")" ) ; _fileUtil . unTar ( new File ( tarFilename ) , new File ( _localBundleStagingPath ) ) ; _log . info ( "deleting bundle tar.gz=" + bundleFileLocation ) ; status . addBundleName ( bundleFilename ) ; new File ( tarFilename ) . delete ( ) ; new File ( bundleFileLocation ) . delete ( ) ; bundlesDownloaded ++ ; } catch ( Exception e ) { _log . error ( "exception exploding bundle=" + bundle , e ) ; } }
public void test() { try { String bundleFileLocation = _localBundleStagingPath + File . separator + bundleFilename ; _log . info ( "unGzip(" + bundleFileLocation + ", " + _localBundleStagingPath + ")" ) ; _fileUtil . unGzip ( new File ( bundleFileLocation ) , new File ( _localBundleStagingPath ) ) ; String tarFilename = parseTarName ( bundleFileLocation ) ; _log . info ( "unTar(" + tarFilename + ", " + _localBundleStagingPath + ")" ) ; _fileUtil . unTar ( new File ( tarFilename ) , new File ( _localBundleStagingPath ) ) ; _log . info ( "deleting bundle tar.gz=" + bundleFileLocation ) ; status . addBundleName ( bundleFilename ) ; new File ( tarFilename ) . delete ( ) ; new File ( bundleFileLocation ) . delete ( ) ; bundlesDownloaded ++ ; } catch ( Exception e ) { _log . error ( "exception exploding bundle=" + bundle , e ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Set start loop point of Buffer {} to {}" , this . getSystemName ( ) , startLoopPoint ) ; } }
public void test() { switch ( ( ( TellstickDevice ) device ) . getStatus ( ) ) { case JNA . CLibrary . TELLSTICK_TURNON : dimValue = new BigDecimal ( 100 ) ; break ; case JNA . CLibrary . TELLSTICK_TURNOFF : break ; case JNA . CLibrary . TELLSTICK_DIM : dimValue = new BigDecimal ( ( ( TellstickDevice ) device ) . getData ( ) ) ; dimValue = dimValue . multiply ( new BigDecimal ( 100 ) ) ; dimValue = dimValue . divide ( new BigDecimal ( 255 ) , 0 , RoundingMode . HALF_UP ) ; break ; default : logger . warn ( "Could not handle {} for {}" , ( ( TellstickDevice ) device ) . getStatus ( ) , device ) ; } }
public void test() { try { DatabaseDriverLoader . loadDriver ( new File ( d ) ) ; } catch ( IOException ioe ) { LOGGER . warn ( "Can't load driver file \"" + d + "\"" + ( ioe . getMessage ( ) != null ? ", reason: " + ioe . getMessage ( ) : "." ) ) ; } }
public void test() { if ( ! authority . isEmpty ( ) ) { List < Role > groupRoles ; if ( groupRoleProvider != null && addAsGroup ) groupRoles = groupRoleProvider . getRolesForGroup ( authority ) ; else groupRoles = Collections . emptyList ( ) ; String prefix = this . prefix ; code_block = IfStatement ; authority = ( prefix + authority ) . replaceAll ( ROLE_CLEAN_REGEXP , ROLE_CLEAN_REPLACEMENT ) ; logger . debug ( "Parsed LDAP role \"{}\" to role \"{}\"" , value , authority ) ; code_block = IfStatement ; authorities . add ( new SimpleGrantedAuthority ( authority ) ) ; } else { logger . debug ( "Found empty authority. Ignoring..." ) ; } }
protected Job doLoad ( Configuration conf , TableDescriptor tableDescriptor ) throws Exception { Path outputDir = getTestDir ( TEST_NAME , "load-output" ) ; LOG . info ( "Load output dir: " + outputDir ) ; NMapInputFormat . setNumMapTasks ( conf , conf . getInt ( NUM_MAP_TASKS_KEY , NUM_MAP_TASKS_DEFAULT ) ) ; conf . set ( TABLE_NAME_KEY , tableDescriptor . getTableName ( ) . getNameAsString ( ) ) ; Job job = Job . getInstance ( conf ) ; job . setJobName ( TEST_NAME + " Load to " + tableDescriptor . getTableName ( ) ) ; job . setJarByClass ( this . getClass ( ) ) ; setMapperClass ( job ) ; job . setInputFormatClass ( NMapInputFormat . class ) ; job . setNumReduceTasks ( 0 ) ; setJobScannerConf ( job ) ; FileOutputFormat . setOutputPath ( job , outputDir ) ; TableMapReduceUtil . addDependencyJars ( job ) ; TableMapReduceUtil . addDependencyJarsForClasses ( job . getConfiguration ( ) , AbstractHBaseTool . class ) ; TableMapReduceUtil . initCredentials ( job ) ; assertTrue ( job . waitForCompletion ( true ) ) ; return job ; }
public void test() { if ( blocksConnectedThisRound > 0 ) { log . info ( "Connected {} orphan blocks." , blocksConnectedThisRound ) ; } }
public void test() { if ( target == null ) { logger . warn ( "The given event " + event . getClass ( ) . getName ( ) + " is not supported " ) ; } else { code_block = TryStatement ;  } }
public void test() { try { iEvent = target . getDeclaredConstructor ( event . getClass ( ) ) . newInstance ( event ) ; } catch ( NoSuchMethodException | SecurityException | InstantiationException | IllegalAccessException | IllegalArgumentException | InvocationTargetException e ) { CoherentEventFactory . logger . error ( e , e ) ; } }
public void test() { if ( autoDetectedFactory != null ) { logger . info ( String . format ( "Auto-detection selected discovery strategy: %s" , autoDetectedFactory . getClass ( ) ) ) ; discoveryStrategies . add ( autoDetectedFactory . newDiscoveryStrategy ( discoveryNode , logger , Collections . emptyMap ( ) ) ) ; } else { logger . info ( "No discovery strategy is applicable for auto-detection" ) ; } }
public void test() { try { String sql = "SELECT id FROM dmhist_services WHERE system_name=? AND service_name=? LIMIT 1;" ; stmt = conn . prepareStatement ( sql ) ; stmt . setString ( 1 , systemName ) ; stmt . setString ( 2 , serviceName ) ; ResultSet rs = stmt . executeQuery ( ) ; rs . next ( ) ; id = rs . getInt ( "id" ) ; rs . close ( ) ; stmt . close ( ) ; } catch ( Exception e ) { logger . debug ( "serviceToID: " + e . toString ( ) ) ; id = - 1 ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( BookmarksFolderServiceUtil . class , "getSubfolderIds" , _getSubfolderIdsParameterTypes16 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , folderIds , groupId , folderId , recurse ) ; code_block = TryStatement ;  } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { CollectRecord record = recordManager . load ( survey , s . getId ( ) , step , false ) ; modelWriter . printData ( record ) ; } catch ( Exception e ) { LOG . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( sb . toString ( ) ) ; } }
@ Override @ Transactional public void disableScheduling ( ) { requireNotDisposed ( ) ; _logger . info ( "Disabling scheduling via the management service." ) ; _schedulingService . disableScheduling ( ) ; }
public GroupDto getGroupById ( final ExternalHttpContext context , final String id ) { LOGGER . debug ( "Get {}" , id ) ; final HttpEntity < Void > request = new HttpEntity < > ( buildHeaders ( context ) ) ; final URIBuilder builder = getUriBuilderFromPath ( "/groups/" + id + "/" ) ; final ResponseEntity < GroupDto > response = restTemplate . exchange ( buildUriBuilder ( builder ) , HttpMethod . GET , request , GroupDto . class ) ; checkResponse ( response ) ; return response . getBody ( ) ; }
public void test() { if ( LOG . isDebugEnabled ( ) && ! lockManager . isBtreeLockedForWrite ( getLockName ( ) ) ) { LOG . debug ( "The file doesn't own a write lock" ) ; } }
public void test() { try { pageHeader . setNextDataPage ( NO_PAGE ) ; pageHeader . setPrevDataPage ( NO_PAGE ) ; pageHeader . setDataLength ( 0 ) ; pageHeader . setNextTupleID ( ItemId . UNKNOWN_ID ) ; pageHeader . setRecordCount ( ( short ) 0 ) ; unlinkPages ( page . page ) ; page . setDirty ( true ) ; dataCache . remove ( page ) ; } catch ( final IOException ioe ) { LOG . error ( ioe ) ; } }
public void test() { try { processList = client . listAllProcesses ( ) . getProcessInfo ( ) ; code_block = IfStatement ; code_block = ForStatement ; } catch ( final RemoteException e ) { OdeConnector . LOG . error ( "Unable to resolve a list of all processes available at ODE" , e ) ; } }
public void test() { try { consentPurpose . setDisplayOrder ( Integer . parseInt ( member . getText ( ) ) ) ; } catch ( NumberFormatException e ) { log . warn ( "DisplayOrder should be an Integer. Found: " + member . getText ( ) + " instead. Setting " + "default display order: " + DEFAULT_DISPLAY_ORDER ) ; consentPurpose . setDisplayOrder ( DEFAULT_DISPLAY_ORDER ) ; } }
@ Override public void registered ( ExecutorDriver executorDriver , Protos . ExecutorInfo executorInfo , Protos . FrameworkInfo frameworkInfo , Protos . SlaveInfo agentInfo ) { LOG . debug ( "Registered {} with Mesos agent {} for framework {}" , executorInfo . getExecutorId ( ) . getValue ( ) , agentInfo . getId ( ) . getValue ( ) , frameworkInfo . getId ( ) . getValue ( ) ) ; LOG . trace ( "Registered {} with Mesos agent {} for framework {}" , MesosUtils . formatForLogging ( executorInfo ) , MesosUtils . formatForLogging ( agentInfo ) , MesosUtils . formatForLogging ( frameworkInfo ) ) ; }
@ Override public void registered ( ExecutorDriver executorDriver , Protos . ExecutorInfo executorInfo , Protos . FrameworkInfo frameworkInfo , Protos . SlaveInfo agentInfo ) { LOG . debug ( "Registered {} with Mesos agent {} for framework {}" , executorInfo . getExecutorId ( ) . getValue ( ) , agentInfo . getId ( ) . getValue ( ) , frameworkInfo . getId ( ) . getValue ( ) ) ; LOG . trace ( "Registered {} with Mesos agent {} for framework {}" , MesosUtils . formatForLogging ( executorInfo ) , MesosUtils . formatForLogging ( agentInfo ) , MesosUtils . formatForLogging ( frameworkInfo ) ) ; }
private void reindexSpaces ( String index ) throws ImejiException { LOGGER . info ( "Indexing Spaces..." ) ; ElasticIndexer indexer = new ElasticIndexer ( index , ElasticTypes . spaces , ElasticService . ANALYSER ) ; SpaceController controller = new SpaceController ( ) ; List < Space > items = controller . retrieveAll ( ) ; LOGGER . info ( "+++ " + items . size ( ) + " items to index +++" ) ; indexer . indexBatch ( items ) ; indexer . commit ( ) ; LOGGER . info ( "Items reindexed!" ) ; }
private void reindexSpaces ( String index ) throws ImejiException { LOGGER . info ( "Indexing Spaces..." ) ; ElasticIndexer indexer = new ElasticIndexer ( index , ElasticTypes . spaces , ElasticService . ANALYSER ) ; SpaceController controller = new SpaceController ( ) ; List < Space > items = controller . retrieveAll ( ) ; LOGGER . info ( "+++ " + items . size ( ) + " items to index +++" ) ; indexer . indexBatch ( items ) ; indexer . commit ( ) ; LOGGER . info ( "Items reindexed!" ) ; }
private void reindexSpaces ( String index ) throws ImejiException { LOGGER . info ( "Indexing Spaces..." ) ; ElasticIndexer indexer = new ElasticIndexer ( index , ElasticTypes . spaces , ElasticService . ANALYSER ) ; SpaceController controller = new SpaceController ( ) ; List < Space > items = controller . retrieveAll ( ) ; LOGGER . info ( "+++ " + items . size ( ) + " items to index +++" ) ; indexer . indexBatch ( items ) ; indexer . commit ( ) ; LOGGER . info ( "Items reindexed!" ) ; }
@ Override public JSONObject getFirstPage ( ) throws IOException { URL apiURL = new URL ( baseURL + "&consumer_key=" + CONSUMER_KEY ) ; logger . debug ( "apiURL: " + apiURL ) ; JSONObject json = Http . url ( apiURL ) . getJSON ( ) ; code_block = IfStatement ; return json ; }
public void test() { if ( applicationContext . getResource ( location ) . exists ( ) ) { loadedConfigurations . add ( location ) ; LOGGER . info ( "Log4j : {} added" , location ) ; hasSource = true ; ResourcePropertySource source = new ResourcePropertySource ( applicationContext . getResource ( location ) ) ; sources . addFirst ( source ) ; propertyNames . addAll ( Arrays . asList ( source . getPropertyNames ( ) ) ) ; } else { ignoredConfigurations . add ( location ) ; LOGGER . info ( "Log4j : {} not found" , location ) ; } }
public void test() { if ( applicationContext . getResource ( location ) . exists ( ) ) { loadedConfigurations . add ( location ) ; LOGGER . info ( "Log4j : {} added" , location ) ; hasSource = true ; ResourcePropertySource source = new ResourcePropertySource ( applicationContext . getResource ( location ) ) ; sources . addFirst ( source ) ; propertyNames . addAll ( Arrays . asList ( source . getPropertyNames ( ) ) ) ; } else { ignoredConfigurations . add ( location ) ; LOGGER . info ( "Log4j : {} not found" , location ) ; } }
public void test() { if ( resolver . containsProperty ( propertyName ) ) { LOGGER . debug ( "Log4j : property resolved {} -> {}" , propertyName , resolver . getProperty ( propertyName ) ) ; properties . put ( propertyName , resolver . getProperty ( propertyName ) ) ; } else { LOGGER . warn ( "Log4j : property {} cannot be resolved" , propertyName ) ; } }
public void test() { if ( LOGGER . isWarnEnabled ( ) ) { LOGGER . warn ( "Log4j : no additional files configured in {}" , Joiner . on ( "," ) . join ( log4jLocations ) ) ; } }
public void test() { if ( ! dir . isAbsolute ( ) ) { LOGGER . error ( "Override directory must be absolute [" + ovrTDir + "]" ) ; return false ; } }
public void test() { if ( ! dir . exists ( ) || ! dir . isDirectory ( ) || ! dir . canWrite ( ) ) { LOGGER . error ( "Bad override dir '" + dir + "'" ) ; return false ; } }
public void test() { if ( expectedContextEntries == null ) { LOGGER . debug ( "Code {} was logged but returned null to getExpectedContextEntries(). Should be empty list if no expected entries." , code ) ; } else { List < String > missingEntries = new ArrayList < > ( ) ; code_block = ForStatement ; code_block = IfStatement ; } }
public void test() { if ( ! context . contains ( expectedEntry ) ) { missingEntries . add ( expectedEntry ) ; } }
public void test() { if ( ostream != null ) { LOG . debug ( "Acquired lock on file {}. LockFile= {}, Spout = {}" , fileToLock , lockFile , spoutId ) ; return new FileLock ( fs , lockFile , ostream , spoutId ) ; } else { LOG . debug ( "Cannot lock file {} as its already locked. Spout = {}" , fileToLock , spoutId ) ; return null ; } }
public void test() { try { FSDataOutputStream ostream = HdfsUtils . tryCreateFile ( fs , lockFile ) ; code_block = IfStatement ; } catch ( IOException e ) { LOG . error ( "Error when acquiring lock on file " + fileToLock + " Spout = " + spoutId , e ) ; throw e ; } }
private String createExistingServersDescription ( final String managementMachinePrefix , final MachineDetails [ ] existingManagementServers ) { logger . info ( "Found existing servers matching the name: " + managementMachinePrefix ) ; final StringBuilder sb = new StringBuilder ( ) ; boolean first = true ; code_block = ForStatement ; final String serverDescriptions = sb . toString ( ) ; return serverDescriptions ; }
private void setXATerminator ( final XATerminator terminator ) { this . xaTerminator = terminator ; logger . debug ( "XATerminator set." ) ; }
public void test() { try { typeOfChangeInProgress = ChangeType . REDOING ; List < OWLOntologyChange > redoChanges = redoStack . pop ( ) ; manager . applyChanges ( redoChanges ) ; } catch ( Exception e ) { logger . error ( "An error occurred whilst redoing the last set of undone changes." , e ) ; } finally { typeOfChangeInProgress = ChangeType . NORMAL ; } }
public void test() { try { return Integer . parseInt ( val ) ; } catch ( NumberFormatException e ) { log . debug ( "val=" + val + " is not a valid number.  Fudge factor will be 0 seconds" ) ; return 0 ; } }
public void test() { try { Class < ? > clazz = Class . forName ( "org.apache.jasper.servlet.JspServletWrapper" ) ; Method method = ReflectionUtil . getDeclaredMethod ( clazz , "getDependants" ) ; _jspServletDependantsMap = Map . class . isAssignableFrom ( method . getReturnType ( ) ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; } }
public void test() { try ( CallableStatement query = connection . prepareCall ( "{call update_task_state(?, ?)}" ) ) { query . setObject ( 1 , taskId ) ; query . setString ( 2 , state . toString ( ) ) ; query . executeQuery ( ) ; } catch ( SQLException e ) { LOG . error ( "Unable to set task as running in recovery db: SQLState: {} Error: {}" , e . getSQLState ( ) , e . getMessage ( ) ) ; } }
public void test() { try { BroadcastSmResult broadcastSmResult = fireAcceptBroadcastSm ( broadcastSm ) ; code_block = IfStatement ; return broadcastSmResult ; } catch ( ProcessRequestException e ) { throw e ; } catch ( Exception e ) { String msg = "Invalid runtime exception thrown when processing broadcast_sm" ; logger . error ( msg , e ) ; throw new ProcessRequestException ( msg , SMPPConstant . STAT_ESME_RSYSERR ) ; } }
@ Test public void wildcardResourcesAreOrderedAlphabetically ( ) { final WroModel model = new WroModel ( ) ; final String uri = String . format ( ClasspathUriLocator . PREFIX + "%s/expander/order/**.js" , WroUtil . toPackageAsFolder ( getClass ( ) ) ) ; model . addGroup ( new Group ( "group" ) . addResource ( Resource . create ( uri , ResourceType . JS ) ) ) ; Mockito . when ( decoratedFactory . create ( ) ) . thenReturn ( model ) ; final WroModel changedModel = transformer . transform ( model ) ; LOG . debug ( "model: {}" , changedModel ) ; final Group group = new WroModelInspector ( changedModel ) . getGroupByName ( "group" ) ; assertEquals ( 7 , group . getResources ( ) . size ( ) ) ; final List < Resource > resources = group . getResources ( ) ; assertEquals ( "01-xyc.js" , FilenameUtils . getName ( resources . get ( 0 ) . getUri ( ) ) ) ; assertEquals ( "02-xyc.js" , FilenameUtils . getName ( resources . get ( 1 ) . getUri ( ) ) ) ; assertEquals ( "03-jquery-ui.js" , FilenameUtils . getName ( resources . get ( 2 ) . getUri ( ) ) ) ; assertEquals ( "04-xyc.js" , FilenameUtils . getName ( resources . get ( 3 ) . getUri ( ) ) ) ; assertEquals ( "05-xyc.js" , FilenameUtils . getName ( resources . get ( 4 ) . getUri ( ) ) ) ; assertEquals ( "06-xyc.js" , FilenameUtils . getName ( resources . get ( 5 ) . getUri ( ) ) ) ; assertEquals ( "07-jquery-impromptu.js" , FilenameUtils . getName ( resources . get ( 6 ) . getUri ( ) ) ) ; }
public void test() { if ( session != null ) { LOG . debug ( "Setting Robot Timeout back to default values : Selenium " + session . getCerberus_selenium_wait_element_default ( ) + " Appium " + session . getCerberus_appium_wait_element_default ( ) + " Sikuli " + session . getCerberus_sikuli_wait_element_default ( ) ) ; session . setCerberus_selenium_wait_element ( session . getCerberus_selenium_wait_element_default ( ) ) ; session . setCerberus_appium_wait_element ( session . getCerberus_appium_wait_element_default ( ) ) ; session . setCerberus_sikuli_wait_element ( session . getCerberus_sikuli_wait_element_default ( ) ) ; LOG . debug ( "Setting Robot highlightElement back to default values : Selenium " + session . getCerberus_selenium_highlightElement_default ( ) + " Sikuli " + session . getCerberus_sikuli_highlightElement_default ( ) ) ; session . setCerberus_selenium_highlightElement ( session . getCerberus_selenium_highlightElement_default ( ) ) ; session . setCerberus_sikuli_highlightElement ( session . getCerberus_sikuli_highlightElement_default ( ) ) ; LOG . debug ( "Setting Robot minSimilarity back to default values : " + session . getCerberus_sikuli_minSimilarity_default ( ) ) ; session . setCerberus_sikuli_minSimilarity ( session . getCerberus_sikuli_minSimilarity_default ( ) ) ; } }
public void test() { if ( session != null ) { LOG . debug ( "Setting Robot Timeout back to default values : Selenium " + session . getCerberus_selenium_wait_element_default ( ) + " Appium " + session . getCerberus_appium_wait_element_default ( ) + " Sikuli " + session . getCerberus_sikuli_wait_element_default ( ) ) ; session . setCerberus_selenium_wait_element ( session . getCerberus_selenium_wait_element_default ( ) ) ; session . setCerberus_appium_wait_element ( session . getCerberus_appium_wait_element_default ( ) ) ; session . setCerberus_sikuli_wait_element ( session . getCerberus_sikuli_wait_element_default ( ) ) ; LOG . debug ( "Setting Robot highlightElement back to default values : Selenium " + session . getCerberus_selenium_highlightElement_default ( ) + " Sikuli " + session . getCerberus_sikuli_highlightElement_default ( ) ) ; session . setCerberus_selenium_highlightElement ( session . getCerberus_selenium_highlightElement_default ( ) ) ; session . setCerberus_sikuli_highlightElement ( session . getCerberus_sikuli_highlightElement_default ( ) ) ; LOG . debug ( "Setting Robot minSimilarity back to default values : " + session . getCerberus_sikuli_minSimilarity_default ( ) ) ; session . setCerberus_sikuli_minSimilarity ( session . getCerberus_sikuli_minSimilarity_default ( ) ) ; } }
public void test() { if ( session != null ) { LOG . debug ( "Setting Robot Timeout back to default values : Selenium " + session . getCerberus_selenium_wait_element_default ( ) + " Appium " + session . getCerberus_appium_wait_element_default ( ) + " Sikuli " + session . getCerberus_sikuli_wait_element_default ( ) ) ; session . setCerberus_selenium_wait_element ( session . getCerberus_selenium_wait_element_default ( ) ) ; session . setCerberus_appium_wait_element ( session . getCerberus_appium_wait_element_default ( ) ) ; session . setCerberus_sikuli_wait_element ( session . getCerberus_sikuli_wait_element_default ( ) ) ; LOG . debug ( "Setting Robot highlightElement back to default values : Selenium " + session . getCerberus_selenium_highlightElement_default ( ) + " Sikuli " + session . getCerberus_sikuli_highlightElement_default ( ) ) ; session . setCerberus_selenium_highlightElement ( session . getCerberus_selenium_highlightElement_default ( ) ) ; session . setCerberus_sikuli_highlightElement ( session . getCerberus_sikuli_highlightElement_default ( ) ) ; LOG . debug ( "Setting Robot minSimilarity back to default values : " + session . getCerberus_sikuli_minSimilarity_default ( ) ) ; session . setCerberus_sikuli_minSimilarity ( session . getCerberus_sikuli_minSimilarity_default ( ) ) ; } }
@ Override public void onStopContainerError ( ContainerId containerId , Throwable t ) { LOG . error ( "Failed to stop Container " + containerId ) ; containers . remove ( containerId ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Look in cache for answer to: " + promptText + ", got " + response ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Asked the user, answer: " + response ) ; } }
public void test() { try { IGefaehrdungsBaumElement elmt = ( IGefaehrdungsBaumElement ) parentElement ; return elmt . getGefaehrdungsBaumChildren ( ) . toArray ( ) ; } catch ( Exception e ) { LOG . error ( "Error in getChildren()" , e ) ; return null ; } }
public void test() { if ( values . contains ( StorageConstants . LATEST_PARTITION_VALUE ) ) { log . info ( "dropping latest partition from fact storage table: {}. Spec: {}" , storageTableName , partition . getSpec ( ) ) ; getClient ( ) . dropPartition ( storageTableName , values , false ) ; continue ; } }
public void test() { try { URL url = FileLocator . find ( Activator . getDefault ( ) . getBundle ( ) , new Path ( "/WebContent/WEB-INF/reportDeposit/" ) , null ) ; URL fileUrl = FileLocator . toFileURL ( url ) ; return FileUtils . toFile ( fileUrl ) . getAbsolutePath ( ) ; } catch ( IOException ex ) { LOG . error ( "jar file with reports not found" , ex ) ; } }
public void test() { if ( ! name . contains ( "." ) ) { String spec = ( String ) namedLocationProps . asMapWithStringKeys ( ) . get ( k ) ; String id = Identifiers . makeRandomId ( 8 ) ; Map < String , Object > config = ConfigUtils . filterForPrefixAndStrip ( namedLocationProps . asMapWithStringKeys ( ) , k + "." ) ; definedLocations . put ( id , new BasicLocationDefinition ( id , name , spec , config ) ) ; count ++ ; } }
public void test() { try { connection . close ( ) ; } catch ( JMSException e ) { logger . error ( "Failed to close JMS connection: " , e ) ; } }
public void test() { try { geo = gmlReader . read ( xml , null ) ; code_block = IfStatement ; } catch ( SAXException | IOException | ParserConfigurationException | GeoFormatException e ) { geo = null ; LOGGER . debug ( ERROR_PARSING_MESSAGE , e ) ; } }
public void test() { try { ser = reader . getValue ( ) . getBytes ( UTF8_ENCODING ) ; } catch ( UnsupportedEncodingException e ) { LOGGER . debug ( "Error encoding the binary value into the metacard." , e ) ; } }
public void test() { if ( isDebug ) { logger . debug ( "Set header {}={}" , name , value ) ; } }
@ Test public void testJacksonJSONParsing ( ) throws Exception { RepairScheduleStatus data = new RepairScheduleStatus ( ) ; data . setClusterName ( "testCluster" ) ; data . setColumnFamilies ( Lists . < String > newArrayList ( ) ) ; data . setCreationTime ( DateTime . now ( ) . withMillis ( 0 ) ) ; data . setDaysBetween ( 2 ) ; data . setId ( UUIDs . timeBased ( ) ) ; data . setIntensity ( 0.75 ) ; data . setIncrementalRepair ( false ) ; data . setKeyspaceName ( "testKeyspace" ) ; data . setOwner ( "testuser" ) ; data . setRepairParallelism ( RepairParallelism . PARALLEL ) ; data . setState ( RepairSchedule . State . ACTIVE ) ; ObjectMapper mapper = new ObjectMapper ( ) ; String dataAsJson = mapper . writeValueAsString ( data ) ; LOG . info ( "DATA: " + dataAsJson ) ; RepairScheduleStatus dataAfter = SimpleReaperClient . parseRepairScheduleStatusJSON ( dataAsJson ) ; assertEquals ( data . getClusterName ( ) , dataAfter . getClusterName ( ) ) ; assertEquals ( data . getColumnFamilies ( ) , dataAfter . getColumnFamilies ( ) ) ; assertEquals ( data . getCreationTime ( ) , dataAfter . getCreationTime ( ) ) ; assertEquals ( data . getDaysBetween ( ) , dataAfter . getDaysBetween ( ) ) ; assertEquals ( data . getId ( ) , dataAfter . getId ( ) ) ; assertEquals ( data . getIntensity ( ) , dataAfter . getIntensity ( ) , 0.0 ) ; assertEquals ( data . getIncrementalRepair ( ) , dataAfter . getIncrementalRepair ( ) ) ; assertEquals ( data . getKeyspaceName ( ) , dataAfter . getKeyspaceName ( ) ) ; assertEquals ( data . getRepairParallelism ( ) , dataAfter . getRepairParallelism ( ) ) ; assertEquals ( data . getState ( ) , dataAfter . getState ( ) ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( String . format ( "Accept: %s" , accept ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( user + " " + req . getURL ( ) ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Initializing autoflush handler on channel {}" , ctx . channel ( ) ) ; } }
public void test() { try { long start = System . currentTimeMillis ( ) ; LOG . info ( "Testcase: " + this . getClass ( ) . getName ( ) + "testGetRecord, testDir=" + dataStoreDir ) ; doGetRecordTest ( ) ; LOG . info ( "Testcase: " + this . getClass ( ) . getName ( ) + "testGetRecord finished, time taken = [" + ( System . currentTimeMillis ( ) - start ) + "]ms" ) ; } catch ( Exception e ) { LOG . error ( "error:" , e ) ; } }
public void test() { try { long start = System . currentTimeMillis ( ) ; LOG . info ( "Testcase: " + this . getClass ( ) . getName ( ) + "testGetRecord, testDir=" + dataStoreDir ) ; doGetRecordTest ( ) ; LOG . info ( "Testcase: " + this . getClass ( ) . getName ( ) + "testGetRecord finished, time taken = [" + ( System . currentTimeMillis ( ) - start ) + "]ms" ) ; } catch ( Exception e ) { LOG . error ( "error:" , e ) ; } }
public void test() { try { long start = System . currentTimeMillis ( ) ; LOG . info ( "Testcase: " + this . getClass ( ) . getName ( ) + "testGetRecord, testDir=" + dataStoreDir ) ; doGetRecordTest ( ) ; LOG . info ( "Testcase: " + this . getClass ( ) . getName ( ) + "testGetRecord finished, time taken = [" + ( System . currentTimeMillis ( ) - start ) + "]ms" ) ; } catch ( Exception e ) { LOG . error ( "error:" , e ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "==> RangerBasePlugin.createAdminClient(" + pluginConfig . getServiceName ( ) + ", " + pluginConfig . getAppId ( ) + ", " + pluginConfig . getPropertyPrefix ( ) + ")" ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "==> RangerBasePlugin.createAdminClient(" + pluginConfig . getServiceName ( ) + ", " + pluginConfig . getAppId ( ) + ", " + pluginConfig . getPropertyPrefix ( ) + ")" ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "==> RangerBasePlugin.createAdminClient(" + pluginConfig . getServiceName ( ) + ", " + pluginConfig . getAppId ( ) + ", " + pluginConfig . getPropertyPrefix ( ) + ")" ) ; } }
public void test() { try { @ SuppressWarnings ( "unchecked" ) Class < RangerAdminClient > adminClass = ( Class < RangerAdminClient > ) Class . forName ( policySourceImpl ) ; ret = adminClass . newInstance ( ) ; } catch ( Exception excp ) { LOG . error ( "failed to instantiate policy source of type '" + policySourceImpl + "'. Will use policy source of type '" + RangerAdminRESTClient . class . getName ( ) + "'" , excp ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "<== RangerBasePlugin.createAdminClient(" + pluginConfig . getServiceName ( ) + ", " + pluginConfig . getAppId ( ) + ", " + pluginConfig . getPropertyPrefix ( ) + "): policySourceImpl=" + policySourceImpl + ", client=" + ret ) ; } }
public void test() { try { RetryMessageModel retryMessageModel = DaoUtil . queryObject ( readDataSource , GET_SQL , new DaoUtil . QueryCallback < RetryMessageModel > ( ) code_block = "" ; ) ; return retryMessageModel ; } catch ( Exception e ) { logger . error ( "getMessageById error" , e ) ; throw new JoyQueueException ( String . format ( "%s topic:%s,app:%s,id:%d" , JoyQueueCode . CN_DB_ERROR . getMessage ( ) , topic , app , id ) , e , JoyQueueCode . CN_DB_ERROR . getCode ( ) ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( String . format ( "Run with feature %s" , feature ) ) ; } }
@ AcceptsPreStepAuth @ GET @ Path ( "/consents/{consentId}/status" ) public Response getConsentStatus ( @ NotEmpty @ NotBlank @ PathParam ( "consentId" ) String consentId ) throws BankRequestFailedException { XS2AFactoryInput xs2AFactoryInput = new XS2AFactoryInput ( ) ; xs2AFactoryInput . setConsentId ( consentId ) ; IOProcessor ioProcessor = new IOProcessor ( getXS2AStandard ( ) ) ; ioProcessor . modifyInput ( xs2AFactoryInput ) ; AISRequest request = new AISRequestFactory ( ) . create ( getXS2AStandard ( ) . getRequestClassProvider ( ) . consentStatus ( ) , xs2AFactoryInput ) ; request . getHeaders ( ) . putAll ( getAdditionalHeaders ( ) ) ; ioProcessor . modifyRequest ( request , xs2AFactoryInput ) ; ConsentStatus state = getXS2AStandard ( ) . getCs ( ) . getStatus ( request ) ; GetConsentStatusResponse response = new GetConsentStatusResponse ( state ) ; LOG . info ( "Successfully fetched consent status entity for bic={}, consentId={}" , getXS2AStandard ( ) . getAspsp ( ) . getBic ( ) , consentId ) ; return Response . status ( ResponseConstant . OK ) . entity ( response ) . build ( ) ; }
@ Override public void init ( FilterConfig filterConfig ) throws ServletException { super . init ( filterConfig ) ; forwardPath = filterConfig . getInitParameter ( "forwardPath" ) ; displayPath = filterConfig . getInitParameter ( "displayPath" ) ; logger . debug ( "CatchAllFilter [" + displayPath + "] received provided allowed context paths from NiFi properties: " + getAllowedContextPaths ( ) ) ; }
public void test() { try { logger . info ( "Initializing Storage Resource Adaptor for storage resource : " + storageResourceId + ", gateway : " + gatewayId + ", user " + loginUser + ", token : " + token ) ; StorageResourceDescription storageResource = AgentUtils . getRegistryServiceClient ( ) . getStorageResource ( storageResourceId ) ; logger . info ( "Fetching credentials for cred store token " + token ) ; SSHCredential sshCredential = AgentUtils . getCredentialClient ( ) . getSSHCredential ( token , gatewayId ) ; code_block = IfStatement ; logger . info ( "Description for token : " + token + " : " + sshCredential . getDescription ( ) ) ; SshAdaptorParams adaptorParams = new SshAdaptorParams ( ) ; adaptorParams . setHostName ( storageResource . getHostName ( ) ) ; adaptorParams . setUserName ( loginUser ) ; adaptorParams . setPassphrase ( sshCredential . getPassphrase ( ) ) ; adaptorParams . setPrivateKey ( sshCredential . getPrivateKey ( ) . getBytes ( ) ) ; adaptorParams . setPublicKey ( sshCredential . getPublicKey ( ) . getBytes ( ) ) ; adaptorParams . setStrictHostKeyChecking ( false ) ; init ( adaptorParams ) ; } catch ( Exception e ) { logger . error ( "Error while initializing ssh agent for storage resource " + storageResourceId + " to token " + token , e ) ; throw new AgentException ( "Error while initializing ssh agent for storage resource " + storageResourceId + " to token " + token , e ) ; } }
public void test() { try { logger . info ( "Initializing Storage Resource Adaptor for storage resource : " + storageResourceId + ", gateway : " + gatewayId + ", user " + loginUser + ", token : " + token ) ; StorageResourceDescription storageResource = AgentUtils . getRegistryServiceClient ( ) . getStorageResource ( storageResourceId ) ; logger . info ( "Fetching credentials for cred store token " + token ) ; SSHCredential sshCredential = AgentUtils . getCredentialClient ( ) . getSSHCredential ( token , gatewayId ) ; code_block = IfStatement ; logger . info ( "Description for token : " + token + " : " + sshCredential . getDescription ( ) ) ; SshAdaptorParams adaptorParams = new SshAdaptorParams ( ) ; adaptorParams . setHostName ( storageResource . getHostName ( ) ) ; adaptorParams . setUserName ( loginUser ) ; adaptorParams . setPassphrase ( sshCredential . getPassphrase ( ) ) ; adaptorParams . setPrivateKey ( sshCredential . getPrivateKey ( ) . getBytes ( ) ) ; adaptorParams . setPublicKey ( sshCredential . getPublicKey ( ) . getBytes ( ) ) ; adaptorParams . setStrictHostKeyChecking ( false ) ; init ( adaptorParams ) ; } catch ( Exception e ) { logger . error ( "Error while initializing ssh agent for storage resource " + storageResourceId + " to token " + token , e ) ; throw new AgentException ( "Error while initializing ssh agent for storage resource " + storageResourceId + " to token " + token , e ) ; } }
public void test() { try { logger . info ( "Initializing Storage Resource Adaptor for storage resource : " + storageResourceId + ", gateway : " + gatewayId + ", user " + loginUser + ", token : " + token ) ; StorageResourceDescription storageResource = AgentUtils . getRegistryServiceClient ( ) . getStorageResource ( storageResourceId ) ; logger . info ( "Fetching credentials for cred store token " + token ) ; SSHCredential sshCredential = AgentUtils . getCredentialClient ( ) . getSSHCredential ( token , gatewayId ) ; code_block = IfStatement ; logger . info ( "Description for token : " + token + " : " + sshCredential . getDescription ( ) ) ; SshAdaptorParams adaptorParams = new SshAdaptorParams ( ) ; adaptorParams . setHostName ( storageResource . getHostName ( ) ) ; adaptorParams . setUserName ( loginUser ) ; adaptorParams . setPassphrase ( sshCredential . getPassphrase ( ) ) ; adaptorParams . setPrivateKey ( sshCredential . getPrivateKey ( ) . getBytes ( ) ) ; adaptorParams . setPublicKey ( sshCredential . getPublicKey ( ) . getBytes ( ) ) ; adaptorParams . setStrictHostKeyChecking ( false ) ; init ( adaptorParams ) ; } catch ( Exception e ) { logger . error ( "Error while initializing ssh agent for storage resource " + storageResourceId + " to token " + token , e ) ; throw new AgentException ( "Error while initializing ssh agent for storage resource " + storageResourceId + " to token " + token , e ) ; } }
public void test() { try { logger . info ( "Initializing Storage Resource Adaptor for storage resource : " + storageResourceId + ", gateway : " + gatewayId + ", user " + loginUser + ", token : " + token ) ; StorageResourceDescription storageResource = AgentUtils . getRegistryServiceClient ( ) . getStorageResource ( storageResourceId ) ; logger . info ( "Fetching credentials for cred store token " + token ) ; SSHCredential sshCredential = AgentUtils . getCredentialClient ( ) . getSSHCredential ( token , gatewayId ) ; code_block = IfStatement ; logger . info ( "Description for token : " + token + " : " + sshCredential . getDescription ( ) ) ; SshAdaptorParams adaptorParams = new SshAdaptorParams ( ) ; adaptorParams . setHostName ( storageResource . getHostName ( ) ) ; adaptorParams . setUserName ( loginUser ) ; adaptorParams . setPassphrase ( sshCredential . getPassphrase ( ) ) ; adaptorParams . setPrivateKey ( sshCredential . getPrivateKey ( ) . getBytes ( ) ) ; adaptorParams . setPublicKey ( sshCredential . getPublicKey ( ) . getBytes ( ) ) ; adaptorParams . setStrictHostKeyChecking ( false ) ; init ( adaptorParams ) ; } catch ( Exception e ) { logger . error ( "Error while initializing ssh agent for storage resource " + storageResourceId + " to token " + token , e ) ; throw new AgentException ( "Error while initializing ssh agent for storage resource " + storageResourceId + " to token " + token , e ) ; } }
@ Override public boolean addTicketHostRestriction ( final String ticketId , final String host ) throws JargonException { Tag ticketOperationResponse = null ; boolean response = true ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "modifying ticket id/string:{}" , ticketId ) ; TicketAdminInp ticketPI = TicketAdminInp . instanceForModifyAddAccess ( ticketId , TicketModifyAddOrRemoveTypeEnum . TICKET_MODIFY_HOST , host ) ; log . info ( EXECUTING_TICKET_PI ) ; ProtocolExtensionPoint pep = irodsAccessObjectFactory . getProtocolExtensionPoint ( irodsAccount ) ; code_block = TryStatement ;  log . info ( "received response from ticket operation:{}" , ticketOperationResponse ) ; return response ; }
@ Override public boolean addTicketHostRestriction ( final String ticketId , final String host ) throws JargonException { Tag ticketOperationResponse = null ; boolean response = true ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "modifying ticket id/string:{}" , ticketId ) ; TicketAdminInp ticketPI = TicketAdminInp . instanceForModifyAddAccess ( ticketId , TicketModifyAddOrRemoveTypeEnum . TICKET_MODIFY_HOST , host ) ; log . info ( EXECUTING_TICKET_PI ) ; ProtocolExtensionPoint pep = irodsAccessObjectFactory . getProtocolExtensionPoint ( irodsAccount ) ; code_block = TryStatement ;  log . info ( "received response from ticket operation:{}" , ticketOperationResponse ) ; return response ; }
@ Override public boolean addTicketHostRestriction ( final String ticketId , final String host ) throws JargonException { Tag ticketOperationResponse = null ; boolean response = true ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "modifying ticket id/string:{}" , ticketId ) ; TicketAdminInp ticketPI = TicketAdminInp . instanceForModifyAddAccess ( ticketId , TicketModifyAddOrRemoveTypeEnum . TICKET_MODIFY_HOST , host ) ; log . info ( EXECUTING_TICKET_PI ) ; ProtocolExtensionPoint pep = irodsAccessObjectFactory . getProtocolExtensionPoint ( irodsAccount ) ; code_block = TryStatement ;  log . info ( "received response from ticket operation:{}" , ticketOperationResponse ) ; return response ; }
public void test() { try { return new ScriptEngineManager ( ) ; } finally { long end = System . currentTimeMillis ( ) ; LOG . info ( ScriptEngineManager . class . getName ( ) + " initialized in " + ( end - start ) + " ms" ) ; } }
public void test() { if ( debug ) { logger . info ( "SGBProxy done e1 " + e1 . getClass ( ) . getName ( ) ) ; } }
public void test() { try { List < GenSolvablePolynomial < C > > G = e1 . twosidedGB ( modv , F ) ; code_block = IfStatement ; return G ; } catch ( PreemptingException e ) { throw new RuntimeException ( "SGBProxy e1 preempted " + e ) ; } catch ( Exception e ) { logger . info ( "SGBProxy e1 " + e ) ; logger . info ( "Exception SGBProxy F = " + F ) ; throw new RuntimeException ( "SGBProxy e1 " + e ) ; } }
public void test() { try { List < GenSolvablePolynomial < C > > G = e1 . twosidedGB ( modv , F ) ; code_block = IfStatement ; return G ; } catch ( PreemptingException e ) { throw new RuntimeException ( "SGBProxy e1 preempted " + e ) ; } catch ( Exception e ) { logger . info ( "SGBProxy e1 " + e ) ; logger . info ( "Exception SGBProxy F = " + F ) ; throw new RuntimeException ( "SGBProxy e1 " + e ) ; } }
@ Override public void audit ( String message , Supplier ... paramSuppliers ) { StringBuilder messageBuilder = new StringBuilder ( ) ; requestIpAndPortAndUserMessage ( PhaseInterceptorChain . getCurrentMessage ( ) , messageBuilder ) ; LOGGER . info ( messageBuilder . append ( cleanAndEncode ( message ) ) . toString ( ) , paramSuppliers ) ; }
public void test() { try { Integer value = ( Integer ) myFormatter . stringToValue ( myCustomHeightField . getText ( ) ) ; return value . intValue ( ) ; } catch ( ParseException e ) { LOGGER . error ( "Unable to parse custom height value: " + e . getMessage ( ) ) ; } }
public void test() { try { client . close ( ) ; } catch ( IOException ex ) { LOGGER . warn ( "Failed to close Kie Server Controller Client due to: " + ex . getMessage ( ) , ex ) ; } }
private void createSchema ( Connection conn ) throws IOException { String createSchemaScriptPath = getCreateSchemaScriptPath ( ) ; logger . info ( "Database schema does not exists." ) ; logger . info ( "Creating database schema from script " + createSchemaScriptPath ) ; ScriptRunner sr = new ScriptRunner ( conn ) ; sr . setDelimiter ( delimiter ) ; sr . setStopOnError ( true ) ; sr . setLogWriter ( null ) ; InputStream is = getClass ( ) . getClassLoader ( ) . getResourceAsStream ( createSchemaScriptPath ) ; String scriptContent = IOUtils . toString ( is ) ; Reader reader = new StringReader ( scriptContent . replaceAll ( CRAFTER_SCHEMA_NAME , studioConfiguration . getProperty ( DB_SCHEMA ) ) ) ; code_block = TryStatement ;  }
public void test() { try { sr . runScript ( reader ) ; } catch ( RuntimeSqlException e ) { logger . error ( "Error while running create DB script" , e ) ; } }
public TransactionReceipt transfer ( Transaction transaction ) throws Throwable { log . info ( "\n-----------------------------------\ntransfer: request = " + com . hedera . services . legacy . proto . utils . CommonUtils . toReadableString ( transaction ) ) ; TransactionBody txBody = com . hedera . services . legacy . proto . utils . CommonUtils . extractTransactionBody ( transaction ) ; Instant consensusTime = new Date ( ) . toInstant ( ) ; TransactionRecord record = cryptoHandler . cryptoTransfer ( txBody , consensusTime ) ; log . info ( "Transfer record :: " + record ) ; Assert . assertNotNull ( record ) ; TransactionReceipt receipt = record . getReceipt ( ) ; return receipt ; }
public TransactionReceipt transfer ( Transaction transaction ) throws Throwable { log . info ( "\n-----------------------------------\ntransfer: request = " + com . hedera . services . legacy . proto . utils . CommonUtils . toReadableString ( transaction ) ) ; TransactionBody txBody = com . hedera . services . legacy . proto . utils . CommonUtils . extractTransactionBody ( transaction ) ; Instant consensusTime = new Date ( ) . toInstant ( ) ; TransactionRecord record = cryptoHandler . cryptoTransfer ( txBody , consensusTime ) ; log . info ( "Transfer record :: " + record ) ; Assert . assertNotNull ( record ) ; TransactionReceipt receipt = record . getReceipt ( ) ; return receipt ; }
public void test() { try { MethodKey methodKey = new MethodKey ( MDRRuleServiceUtil . class , "addRule" , _addRuleParameterTypes0 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , ruleGroupId , nameMap , descriptionMap , type , typeSettings , serviceContext ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . mobile . device . rules . model . MDRRule ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public static IRealization selectRealization ( OLAPContext olapContext ) throws NoRealizationFoundException { ProjectManager prjMgr = ProjectManager . getInstance ( olapContext . olapSchema . getConfig ( ) ) ; String factTableName = olapContext . firstTableScan . getTableName ( ) ; String projectName = olapContext . olapSchema . getProjectName ( ) ; List < IRealization > realizations = Lists . newArrayList ( prjMgr . getRealizationsByTable ( projectName , factTableName ) ) ; logger . info ( "Find candidates by table " + factTableName + " and project=" + projectName + " : " + StringUtils . join ( realizations , "," ) ) ; RoutingRule . applyRules ( realizations , olapContext ) ; code_block = IfStatement ; logger . info ( "The realizations remaining: " ) ; logger . info ( RoutingRule . getPrintableText ( realizations ) ) ; logger . info ( "The realization being chosen: " + realizations . get ( 0 ) . getName ( ) ) ; return realizations . get ( 0 ) ; }
public static IRealization selectRealization ( OLAPContext olapContext ) throws NoRealizationFoundException { ProjectManager prjMgr = ProjectManager . getInstance ( olapContext . olapSchema . getConfig ( ) ) ; String factTableName = olapContext . firstTableScan . getTableName ( ) ; String projectName = olapContext . olapSchema . getProjectName ( ) ; List < IRealization > realizations = Lists . newArrayList ( prjMgr . getRealizationsByTable ( projectName , factTableName ) ) ; logger . info ( "Find candidates by table " + factTableName + " and project=" + projectName + " : " + StringUtils . join ( realizations , "," ) ) ; RoutingRule . applyRules ( realizations , olapContext ) ; code_block = IfStatement ; logger . info ( "The realizations remaining: " ) ; logger . info ( RoutingRule . getPrintableText ( realizations ) ) ; logger . info ( "The realization being chosen: " + realizations . get ( 0 ) . getName ( ) ) ; return realizations . get ( 0 ) ; }
public static IRealization selectRealization ( OLAPContext olapContext ) throws NoRealizationFoundException { ProjectManager prjMgr = ProjectManager . getInstance ( olapContext . olapSchema . getConfig ( ) ) ; String factTableName = olapContext . firstTableScan . getTableName ( ) ; String projectName = olapContext . olapSchema . getProjectName ( ) ; List < IRealization > realizations = Lists . newArrayList ( prjMgr . getRealizationsByTable ( projectName , factTableName ) ) ; logger . info ( "Find candidates by table " + factTableName + " and project=" + projectName + " : " + StringUtils . join ( realizations , "," ) ) ; RoutingRule . applyRules ( realizations , olapContext ) ; code_block = IfStatement ; logger . info ( "The realizations remaining: " ) ; logger . info ( RoutingRule . getPrintableText ( realizations ) ) ; logger . info ( "The realization being chosen: " + realizations . get ( 0 ) . getName ( ) ) ; return realizations . get ( 0 ) ; }
public static IRealization selectRealization ( OLAPContext olapContext ) throws NoRealizationFoundException { ProjectManager prjMgr = ProjectManager . getInstance ( olapContext . olapSchema . getConfig ( ) ) ; String factTableName = olapContext . firstTableScan . getTableName ( ) ; String projectName = olapContext . olapSchema . getProjectName ( ) ; List < IRealization > realizations = Lists . newArrayList ( prjMgr . getRealizationsByTable ( projectName , factTableName ) ) ; logger . info ( "Find candidates by table " + factTableName + " and project=" + projectName + " : " + StringUtils . join ( realizations , "," ) ) ; RoutingRule . applyRules ( realizations , olapContext ) ; code_block = IfStatement ; logger . info ( "The realizations remaining: " ) ; logger . info ( RoutingRule . getPrintableText ( realizations ) ) ; logger . info ( "The realization being chosen: " + realizations . get ( 0 ) . getName ( ) ) ; return realizations . get ( 0 ) ; }
@ Override protected void learnAxioms ( ) { logger . info ( "Pattern: " + pattern ) ; int modalDepth = MaximumModalDepthDetector . getMaxModalDepth ( pattern ) ; modalDepth ++ ; logger . info ( "Modal depth: " + modalDepth ) ; Model fragment = fragmentExtractor . extractFragment ( cls , modalDepth ) ; Set < OWLAxiom > instantiations = applyPattern ( pattern , dataFactory . getOWLClass ( IRI . create ( cls . toStringID ( ) ) ) , fragment ) ; code_block = ForStatement ; }
@ Override protected void learnAxioms ( ) { logger . info ( "Pattern: " + pattern ) ; int modalDepth = MaximumModalDepthDetector . getMaxModalDepth ( pattern ) ; modalDepth ++ ; logger . info ( "Modal depth: " + modalDepth ) ; Model fragment = fragmentExtractor . extractFragment ( cls , modalDepth ) ; Set < OWLAxiom > instantiations = applyPattern ( pattern , dataFactory . getOWLClass ( IRI . create ( cls . toStringID ( ) ) ) , fragment ) ; code_block = ForStatement ; }
public void test() { if ( rowIds . length == 0 ) { LOG . warn ( "parameter rowIds is empty, you should check it, just return a empty vector array now!!!" ) ; FutureResult < Vector [ ] > result = new FutureResult < > ( ) ; result . set ( new Vector [ 0 ] ) ; return result ; } }
public void test() { if ( ! notConverged . isEmpty ( ) ) { LOGGER . info ( "Some key(s) on index {} do not currently have status {}: {}" , graphIndexName , status , waitingOn ) ; } else { LOGGER . info ( "All {} key(s) on index {} have status {}" , converged . size ( ) , graphIndexName , status ) ; return new GraphIndexStatusReport ( true , graphIndexName , status , notConverged , converged , t . elapsed ( ) ) ; } }
public void test() { while ( iterator . hasNext ( ) ) { final MessageExt message = iterator . next ( ) ; String msgId = message . getMsgId ( ) ; log . info ( "BrokerName {}, topicName {}, queueId {} is pause, Discard the message {}" , messageQueue . getBrokerName ( ) , messageQueue . getTopic ( ) , message . getQueueId ( ) , msgId ) ; iterator . remove ( ) ; } }
public void start ( ) throws Exception { Objects . nonNull ( config ) ; LOGGER . info ( "RiakHttpClient.start {}" , config ) ; this . client = com . basho . riak . client . api . RiakClient . newClient ( config . getPort ( ) . intValue ( ) , config . getHosts ( ) ) ; Objects . nonNull ( client ) ; Objects . nonNull ( client . getRiakCluster ( ) ) ; assert ( client . getRiakCluster ( ) . getNodes ( ) . size ( ) > 0 ) ; }
@ BeforeClass public static void start ( ) throws Exception { LOG . info ( "SubmarineServerClusterTest:start()" ) ; SubmarineConfiguration conf = SubmarineConfiguration . getInstance ( ) ; String serverHost = NetworkUtils . findAvailableHostAddress ( ) ; int serverPort = NetworkUtils . findRandomAvailablePortOnAllLocalInterfaces ( ) ; String clusterAdd = serverHost + ":" + serverPort ; conf . setClusterAddress ( clusterAdd ) ; startUp ( SubmarineServerClusterTest . class . getSimpleName ( ) ) ; Class clazz = ClusterClient . class ; Constructor constructor = null ; constructor = clazz . getDeclaredConstructor ( ) ; constructor . setAccessible ( true ) ; clusterClient = ( ClusterClient ) constructor . newInstance ( ) ; clusterClient . start ( SubmarineServerClusterTest . class . getSimpleName ( ) ) ; int wait = 0 ; code_block = WhileStatement ; assertTrue ( "Can not start Submarine server!" , clusterClient . raftInitialized ( ) ) ; sleep ( 10000 ) ; }
public void test() { if ( poolState == POOL_NORMAL ) { logger . error ( "{} - Error thrown while acquiring connection from data source" , poolName , e . getCause ( ) ) ; lastConnectionFailure . set ( e ) ; } }
public void test() { if ( maxLifetime > 0 ) { final long variance = maxLifetime > 10_000 ? ThreadLocalRandom . current ( ) . nextLong ( maxLifetime / 40 ) : 0 ; final long lifetime = maxLifetime - variance ; poolEntry . setFutureEol ( houseKeepingExecutorService . schedule ( new MaxLifetimeTask ( poolEntry ) , lifetime , MILLISECONDS ) ) ; } }
public void test() { if ( poolState == POOL_NORMAL ) { logger . debug ( "{} - Cannot acquire connection from data source" , poolName , e ) ; } }
@ Override public Double visit ( LessThanFilter lessThanFilter ) { int minBound = 9 - IntStream . rangeClosed ( 0 , 9 ) . filter ( i -> percentiles [ 9 - i ] < lessThanFilter . getValue ( ) . doubleValue ( ) ) . findFirst ( ) . orElse ( 0 ) ; final double result = ( ( double ) minBound + 1.0 ) / 10.0 ; log . debug ( "cacheKey:{} LessThanFilter:{} percentiles[{}] = {} multiplier: {}" , cacheKey , lessThanFilter , minBound , percentiles [ minBound ] , result ) ; return result ; }
public void test() { try { driver . stop ( ) ; } catch ( LensException e ) { log . error ( "Failed to stop driver " + driver , e ) ; } }
public synchronized void stop ( ) { code_block = ForStatement ; drivers . clear ( ) ; udfStatusExpirySvc . shutdownNow ( ) ; log . info ( "Stopped ML service" ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "path=" + path ) ; logger . debug ( "extension=" + extension ) ; logger . debug ( "n=" + n ) ; logger . debug ( "to=" + to ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "path=" + path ) ; logger . debug ( "extension=" + extension ) ; logger . debug ( "n=" + n ) ; logger . debug ( "to=" + to ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "path=" + path ) ; logger . debug ( "extension=" + extension ) ; logger . debug ( "n=" + n ) ; logger . debug ( "to=" + to ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "path=" + path ) ; logger . debug ( "extension=" + extension ) ; logger . debug ( "n=" + n ) ; logger . debug ( "to=" + to ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "convert:" + DateUtils . getWasteTime ( startTime ) ) ; } }
public void test() { try { sessionMatchSpecs . set ( dao . getSessionMatchSpecs ( ) ) ; } catch ( RuntimeException e ) { log . error ( e , "Error reloading configuration" ) ; } }
public void test() { if ( conf == null ) { LOG . debug ( MessageFormat . format ( "Missed explicit Hadoop conf directory: {0}" , ENV_HADOOP_CONF ) ) ; return null ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( MessageFormat . format ( "Found explicit Hadoop confdir: {0}={1}" , ENV_HADOOP_CONF , conf ) ) ; } }
public void test() { if ( name != null ) { log . warn ( "Cannot close: " + name + ". Reason: " + e . getMessage ( ) , e ) ; } else { log . warn ( "Cannot close. Reason: " + e . getMessage ( ) , e ) ; } }
public void test() { for ( ATTRIBUTE a : attribute2Transition2Double . keySet ( ) ) { logger . info ( "======={}=======" , a ) ; logger . info ( "1<=>1 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_11 ) ) ; logger . info ( "2<=>2 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_22 ) ) ; logger . info ( "3<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_33 ) ) ; logger . info ( "1<=>2 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_12 ) ) ; logger . info ( "1<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_13 ) ) ; logger . info ( "2<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_23 ) ) ; } }
public void test() { for ( ATTRIBUTE a : attribute2Transition2Double . keySet ( ) ) { logger . info ( "======={}=======" , a ) ; logger . info ( "1<=>1 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_11 ) ) ; logger . info ( "2<=>2 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_22 ) ) ; logger . info ( "3<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_33 ) ) ; logger . info ( "1<=>2 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_12 ) ) ; logger . info ( "1<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_13 ) ) ; logger . info ( "2<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_23 ) ) ; } }
public void test() { for ( ATTRIBUTE a : attribute2Transition2Double . keySet ( ) ) { logger . info ( "======={}=======" , a ) ; logger . info ( "1<=>1 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_11 ) ) ; logger . info ( "2<=>2 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_22 ) ) ; logger . info ( "3<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_33 ) ) ; logger . info ( "1<=>2 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_12 ) ) ; logger . info ( "1<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_13 ) ) ; logger . info ( "2<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_23 ) ) ; } }
public void test() { for ( ATTRIBUTE a : attribute2Transition2Double . keySet ( ) ) { logger . info ( "======={}=======" , a ) ; logger . info ( "1<=>1 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_11 ) ) ; logger . info ( "2<=>2 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_22 ) ) ; logger . info ( "3<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_33 ) ) ; logger . info ( "1<=>2 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_12 ) ) ; logger . info ( "1<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_13 ) ) ; logger . info ( "2<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_23 ) ) ; } }
public void test() { for ( ATTRIBUTE a : attribute2Transition2Double . keySet ( ) ) { logger . info ( "======={}=======" , a ) ; logger . info ( "1<=>1 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_11 ) ) ; logger . info ( "2<=>2 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_22 ) ) ; logger . info ( "3<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_33 ) ) ; logger . info ( "1<=>2 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_12 ) ) ; logger . info ( "1<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_13 ) ) ; logger . info ( "2<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_23 ) ) ; } }
public void test() { for ( ATTRIBUTE a : attribute2Transition2Double . keySet ( ) ) { logger . info ( "======={}=======" , a ) ; logger . info ( "1<=>1 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_11 ) ) ; logger . info ( "2<=>2 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_22 ) ) ; logger . info ( "3<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_33 ) ) ; logger . info ( "1<=>2 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_12 ) ) ; logger . info ( "1<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_13 ) ) ; logger . info ( "2<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_23 ) ) ; } }
public void test() { for ( ATTRIBUTE a : attribute2Transition2Double . keySet ( ) ) { logger . info ( "======={}=======" , a ) ; logger . info ( "1<=>1 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_11 ) ) ; logger . info ( "2<=>2 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_22 ) ) ; logger . info ( "3<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_33 ) ) ; logger . info ( "1<=>2 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_12 ) ) ; logger . info ( "1<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_13 ) ) ; logger . info ( "2<=>3 = {}" , attribute2Transition2Double . get ( a ) . get ( TRANSITION . BETWEEN_23 ) ) ; } }
public void test() { try { ByteArrayOutputStream output = new ByteArrayOutputStream ( ) ; writeXMLObject ( jaxbObject , output ) ; return readXMLObject ( new ByteArrayInputStream ( output . toByteArray ( ) ) , target ) ; } catch ( JAXBException e ) { LOGGER . error ( e , e ) ; } }
public void test() { try { com . liferay . commerce . inventory . model . CommerceInventoryWarehouse returnValue = CommerceInventoryWarehouseServiceUtil . addCommerceInventoryWarehouse ( externalReferenceCode , name , description , active , street1 , street2 , street3 , city , zip , commerceRegionCode , commerceCountryCode , latitude , longitude , serviceContext ) ; return com . liferay . commerce . inventory . model . CommerceInventoryWarehouseSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Closing file handle " + this ) ; } }
public void test() { if ( span . getDuration ( ) < coreConfiguration . getSpanMinDuration ( ) . getMillis ( ) * 1000 ) { logger . debug ( "Span faster than span_min_duration. Request discarding {}" , span ) ; span . requestDiscarding ( ) ; } }
public void test() { try { return fs . listStatus ( new Path ( filePath ) ) ; } catch ( IOException e ) { logger . error ( "Get file list exception" , e ) ; throw new Exception ( "Get file list exception" , e ) ; } }
public void test() { if ( _log . isWarnEnabled ( ) ) { _log . warn ( "A JSON web service action is already registered at " + path ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception exception ) { StringBundler sb = new StringBundler ( 17 ) ; sb . append ( "Something went wrong attempting to register service " ) ; sb . append ( "method {contextName=" ) ; sb . append ( contextName ) ; sb . append ( ",contextPath=" ) ; sb . append ( contextPath ) ; sb . append ( ",actionObject=" ) ; sb . append ( actionObject ) ; sb . append ( ",actionClass=" ) ; sb . append ( actionClass ) ; sb . append ( ",actionMethod=" ) ; sb . append ( actionMethod ) ; sb . append ( ",path=" ) ; sb . append ( path ) ; sb . append ( ",method=" ) ; sb . append ( method ) ; sb . append ( "} due to " ) ; sb . append ( exception . getMessage ( ) ) ; _log . warn ( sb . toString ( ) ) ; } }
@ Override public void execute ( ) { log . debug ( "starting" ) ; code_block = IfStatement ; log . debug ( "finished" ) ; }
@ Override public void execute ( ) { log . debug ( "starting" ) ; code_block = IfStatement ; log . debug ( "finished" ) ; }
public void test() { try { DDMDataProviderOutputParametersSettings [ ] ddmDataProviderOutputParametersSettings = getDDMDataProviderOutputParametersSettings ( dataProviderInstanceId ) ; code_block = ForStatement ; } catch ( Exception exception ) { _log . error ( String . format ( "Unable to get the output parameters for data provider " + "instance with id '%d'" , dataProviderInstanceId ) , exception ) ; } }
public void test() { try { EncryptRequest req = new EncryptRequest ( ) . withKeyId ( kmsKeyArn ) . withEncryptionAlgorithm ( ASYMMETRIC_ALGORITHM ) . withPlaintext ( ByteBuffer . wrap ( tmp . getBytes ( ) ) ) ; ByteBuffer plainText = kmsClient . encrypt ( req ) . getCiphertextBlob ( ) ; return TO_DECRYPT_PREFIX + Base64 . getEncoder ( ) . encodeToString ( plainText . array ( ) ) + TO_DECRYPT_POSTFIX ; } catch ( RuntimeException e ) { logger . error ( "Error when trying to encrypt using asymmetric key. Please check the following:\n" + "\t- Does the application use an IAM role?\n" + "\t- Does the application's role have the permission to use the CMK the value was encrypted with?\n" + "More information on that topic: https://confluence.in.here.com/display/CMECMCPDOWS/Encryption+of+secrets" , e ) ; } }
public void test() { if ( logger . isTraceEnabled ( LogMarker . DM_VERBOSE ) ) { logger . trace ( "{}: updateEntryVersionLocally in bucket: {}, key: {}" , getClass ( ) . getName ( ) , bucket , key ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "{}: operateOnRegion caught EntryNotFoundException" , getClass ( ) . getName ( ) ) ; } }
@ Override protected void setUp ( ) throws Exception { code_block = IfStatement ; factory = createConnectionFactory ( bindAddress ) ; managementConnection = factory . createConnection ( ) ; managementSession = managementConnection . createSession ( false , Session . AUTO_ACKNOWLEDGE ) ; Destination startDestination = createDestination ( managementSession , getClass ( ) + ".start" ) ; Destination endDestination = createDestination ( managementSession , getClass ( ) + ".end" ) ; LOG . info ( "Running with " + numberOfClients + " clients - sending " + numberOfBatches + " batches of " + batchSize + " messages" ) ; controller = new LoadController ( "Controller" , factory ) ; controller . setBatchSize ( batchSize ) ; controller . setNumberOfBatches ( numberOfBatches ) ; controller . setDeliveryMode ( deliveryMode ) ; controller . setConnectionPerMessage ( connectionPerMessage ) ; controller . setStartDestination ( startDestination ) ; controller . setNextDestination ( endDestination ) ; controller . setTimeout ( timeout ) ; clients = new LoadClient [ numberOfClients ] ; code_block = ForStatement ; super . setUp ( ) ; }
public void test() { try { jdbcTemplate . execute ( sql ) ; } catch ( BadSqlGrammarException ex ) { log . debug ( ex . getMessage ( ) ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "==> AtlasStructDefStoreV1.preCreate({})" , structDef ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "<== AtlasStructDefStoreV1.preCreate({}): {}" , structDef , ret ) ; } }
public void test() { try { String userName = getUserName ( request . getAllHeaders ( ) ) ; reportIdDir = getDatasetBaseLocation ( ReportGenerationApp . REPORT_FILESET ) . append ( userName ) . append ( reportId ) ; code_block = IfStatement ; String shareId = encodeShareId ( new ReportIdentifier ( userName , reportId ) ) ; responder . sendJson ( 200 , new ShareId ( shareId ) , ShareId . class , GSON ) ; } catch ( IOException | GeneralSecurityException e ) { LOG . error ( "Failed to read report with id {}" , reportId , e ) ; responder . sendError ( 500 , String . format ( "Failed to read report with id %s because of error: %s" , reportId , e . getMessage ( ) ) ) ; return ; } }
public void test() { if ( object instanceof RegionEntry ) { RegionEntry regionEntry = ( RegionEntry ) object ; newContext = createExecutionContext ( regionEntry ) ; value = getTargetObjectForUpdate ( regionEntry ) ; } }
public void test() { try { log . debug ( "Starting Citrus before suite lifecycle" ) ; citrusInstance . get ( ) . beforeSuite ( configurationInstance . get ( ) . getSuiteName ( ) ) ; } catch ( Exception e ) { log . error ( CitrusExtensionConstants . CITRUS_EXTENSION_ERROR , e ) ; throw e ; } }
public void test() { try { tm ( 0 ) . commit ( ) ; assert false : "Exception expected" ; } catch ( Exception e ) { log . debug ( "Ignoring expected exception during 1-phase prepare" , e ) ; } }
public void test() { try { final String filename = args . getJsonDump ( ) ; final JsonDumper jsonDumper = new JsonDumper ( filename , args . getLanguages ( ) , args . getExtraTags ( ) ) ; NominatimConnector nominatimConnector = new NominatimConnector ( args . getHost ( ) , args . getPort ( ) , args . getDatabase ( ) , args . getUser ( ) , args . getPassword ( ) ) ; nominatimConnector . setImporter ( jsonDumper ) ; nominatimConnector . readEntireDatabase ( args . getCountryCodes ( ) . split ( "," ) ) ; log . info ( "json dump was created: " + filename ) ; } catch ( FileNotFoundException e ) { log . error ( "cannot create dump" , e ) ; } }
public void test() { try { final String filename = args . getJsonDump ( ) ; final JsonDumper jsonDumper = new JsonDumper ( filename , args . getLanguages ( ) , args . getExtraTags ( ) ) ; NominatimConnector nominatimConnector = new NominatimConnector ( args . getHost ( ) , args . getPort ( ) , args . getDatabase ( ) , args . getUser ( ) , args . getPassword ( ) ) ; nominatimConnector . setImporter ( jsonDumper ) ; nominatimConnector . readEntireDatabase ( args . getCountryCodes ( ) . split ( "," ) ) ; log . info ( "json dump was created: " + filename ) ; } catch ( FileNotFoundException e ) { log . error ( "cannot create dump" , e ) ; } }
public void test() { if ( lockImpl == null ) { log . info ( "No lock implementation defined, going to pretend like we got the lock" ) ; future . complete ( Boolean . TRUE ) ; return future ; } }
public void test() { try { String body = issueComment . getBody ( ) ; code_block = IfStatement ; } catch ( IOException ex ) { LOG . error ( "Couldn't check comment {}, skipping it." , issueComment . getId ( ) , ex ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "menuKeyEquivalentTarget_forEvent:" + menu ) ; } }
public void test() { if ( trcResult . getChannel ( ) . waitForChannelToReachAgi ( 30 , TimeUnit . SECONDS ) ) { logger . info ( "Call reached AGI" ) ; } else { logger . error ( "Call never reached agi" ) ; } }
public void test() { if ( trcResult . isSuccess ( ) ) { code_block = IfStatement ; } else { logger . warn ( "Originate failed: " + trcResult . getAbortReason ( ) ) ; } }
@ RequestMapping ( "/execute" ) public @ ResponseBody String execute ( @ RequestParam ( value = "experimentData" ) String experimentData ) { LOGGER . debug ( "Got request on /execute with experimentData={}" , experimentData ) ; Object obj = JSONValue . parse ( experimentData ) ; JSONObject configuration = ( JSONObject ) obj ; String typeString = ( String ) configuration . get ( "type" ) ; ExperimentType type = null ; code_block = TryStatement ;  String matching = ( String ) configuration . get ( "matching" ) ; JSONArray jsonAnnotators = ( JSONArray ) configuration . get ( "annotator" ) ; String [ ] annotators = new String [ jsonAnnotators . size ( ) ] ; code_block = ForStatement ; JSONArray jsonDataset = ( JSONArray ) configuration . get ( "dataset" ) ; String [ ] datasets = new String [ jsonDataset . size ( ) ] ; code_block = ForStatement ; ExperimentTaskConfiguration [ ] configs = new ExperimentTaskConfiguration [ annotators . length * datasets . length ] ; int count = 0 ; code_block = ForStatement ; String experimentId = IDCreator . getInstance ( ) . createID ( ) ; Experimenter exp = new Experimenter ( overseer , dao , globalRetriever , evFactory , configs , experimentId ) ; exp . setAnnotatorOutputWriter ( annotatorOutputWriter ) ; exp . run ( ) ; return experimentId ; }
public void test() { try { type = ExperimentType . valueOf ( typeString ) ; } catch ( IllegalArgumentException e ) { LOGGER . warn ( "Got a request containing a wrong ExperimentType (\"{}\"). Ignoring it." , typeString ) ; return null ; } }
public void test() { try { logger . info ( "parsing db-schema from " + options . model_database ) ; model = MolgenisModelParser . parseDbSchema ( options . model_database ) ; code_block = ForStatement ; logger . debug ( "read: " + model ) ; MolgenisModelValidator . validate ( model , options ) ; logger . info ( "parsing ui-schema" ) ; model = MolgenisModelParser . parseUiSchema ( options . path + options . model_userinterface , model ) ; MolgenisModelValidator . validateUI ( model , options ) ; logger . debug ( "validated: " + model ) ; } catch ( MolgenisModelException e ) { logger . error ( "Parsing failed: " + e . getMessage ( ) ) ; e . printStackTrace ( ) ; throw e ; } }
public void test() { try { logger . info ( "parsing db-schema from " + options . model_database ) ; model = MolgenisModelParser . parseDbSchema ( options . model_database ) ; code_block = ForStatement ; logger . debug ( "read: " + model ) ; MolgenisModelValidator . validate ( model , options ) ; logger . info ( "parsing ui-schema" ) ; model = MolgenisModelParser . parseUiSchema ( options . path + options . model_userinterface , model ) ; MolgenisModelValidator . validateUI ( model , options ) ; logger . debug ( "validated: " + model ) ; } catch ( MolgenisModelException e ) { logger . error ( "Parsing failed: " + e . getMessage ( ) ) ; e . printStackTrace ( ) ; throw e ; } }
public void test() { try { logger . info ( "parsing db-schema from " + options . model_database ) ; model = MolgenisModelParser . parseDbSchema ( options . model_database ) ; code_block = ForStatement ; logger . debug ( "read: " + model ) ; MolgenisModelValidator . validate ( model , options ) ; logger . info ( "parsing ui-schema" ) ; model = MolgenisModelParser . parseUiSchema ( options . path + options . model_userinterface , model ) ; MolgenisModelValidator . validateUI ( model , options ) ; logger . debug ( "validated: " + model ) ; } catch ( MolgenisModelException e ) { logger . error ( "Parsing failed: " + e . getMessage ( ) ) ; e . printStackTrace ( ) ; throw e ; } }
public void test() { try { logger . info ( "parsing db-schema from " + options . model_database ) ; model = MolgenisModelParser . parseDbSchema ( options . model_database ) ; code_block = ForStatement ; logger . debug ( "read: " + model ) ; MolgenisModelValidator . validate ( model , options ) ; logger . info ( "parsing ui-schema" ) ; model = MolgenisModelParser . parseUiSchema ( options . path + options . model_userinterface , model ) ; MolgenisModelValidator . validateUI ( model , options ) ; logger . debug ( "validated: " + model ) ; } catch ( MolgenisModelException e ) { logger . error ( "Parsing failed: " + e . getMessage ( ) ) ; e . printStackTrace ( ) ; throw e ; } }
public void test() { try { logger . info ( "parsing db-schema from " + options . model_database ) ; model = MolgenisModelParser . parseDbSchema ( options . model_database ) ; code_block = ForStatement ; logger . debug ( "read: " + model ) ; MolgenisModelValidator . validate ( model , options ) ; logger . info ( "parsing ui-schema" ) ; model = MolgenisModelParser . parseUiSchema ( options . path + options . model_userinterface , model ) ; MolgenisModelValidator . validateUI ( model , options ) ; logger . debug ( "validated: " + model ) ; } catch ( MolgenisModelException e ) { logger . error ( "Parsing failed: " + e . getMessage ( ) ) ; e . printStackTrace ( ) ; throw e ; } }
public void test() { if ( customLog . isPresent ( ) ) { customLog . get ( ) . accept ( log , records ) ; } else { log . info ( records ) ; } }
@ Test public void testReaderRead ( ) throws Exception { BeanConfig config = new BeanConfig ( ) ; config . setHost ( "localhost:8080" ) ; config . setSchemes ( new String [ ] code_block = "" ; ) ; config . setBasePath ( "/api" ) ; config . setTitle ( "Day" ) ; config . setLicense ( "Apache 2.0" ) ; config . setLicenseUrl ( "http://www.apache.org/licenses/LICENSE-2.0.html" ) ; config . setVersion ( "2.0" ) ; RestOpenApiReader reader = new RestOpenApiReader ( ) ; OasDocument openApi = reader . read ( context , context . getRestDefinitions ( ) , null , config , context . getName ( ) , new DefaultClassResolver ( ) ) ; assertNotNull ( openApi ) ; ObjectMapper mapper = new ObjectMapper ( ) ; mapper . enable ( SerializationFeature . INDENT_OUTPUT ) ; mapper . setSerializationInclusion ( JsonInclude . Include . NON_NULL ) ; Object dump = Library . writeNode ( openApi ) ; String json = mapper . writeValueAsString ( dump ) ; log . info ( json ) ; assertTrue ( json . contains ( "\"host\" : \"localhost:8080\"" ) ) ; assertTrue ( json . contains ( "\"default\" : \"friday\"" ) ) ; assertTrue ( json . contains ( "\"enum\" : [ \"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\" ]" ) ) ; assertTrue ( json . contains ( "\"$ref\" : \"/definitions/DayResponse\"" ) ) ; assertTrue ( json . contains ( "\"format\" : \"org.apache.camel.openapi.DayResponse\"" ) ) ; assertTrue ( json . contains ( "\"X-Rate-Limit-Limit\" : {" ) ) ; assertTrue ( json . contains ( "\"description\" : \"The number of allowed requests in the current period\"" ) ) ; context . stop ( ) ; }
public void test() { if ( resp == null ) { logger . error ( "getKVPessimisticRollbackMethod failed without a cause" ) ; regionManager . onRequestFail ( region ) ; bo . doBackOff ( BoRegionMiss , new TiClientInternalException ( "getKVPessimisticRollbackMethod failed without a cause" ) ) ; continue ; } }
public void test() { if ( reportId < 0 ) { log . error ( "Invalid arguments, reportId must not be less thann 0!" ) ; return null ; } }
public void test() { try { ResponseEntity < Resource > responseEntity = restTemplate . exchange ( baseUrl + "report/" + reportId , HttpMethod . GET , new HttpEntity < > ( headers ) , Resource . class ) ; return responseEntity . getBody ( ) . getInputStream ( ) ; } catch ( RestClientException | IOException e ) { log . error ( "Error while trying to download report with id {}." , e , reportId ) ; return null ; } }
public void test() { try { jdbcTemplate . update ( "DELETE FROM auth WHERE auth LIKE 'SSO %' AND object_type=9" ) ; jobsAdded . incrementAndGet ( ) ; } catch ( RuntimeException e ) { LOGGER . error ( "Failed to delete SSO auth lines from auth index table" , e ) ; } }
public void test() { try { listener . gotUserLists ( lists ) ; } catch ( Exception e ) { logger . warn ( "Exception at getUserLists" , e ) ; } }
public void doGet ( HttpServletRequest request , HttpServletResponse response ) throws ServletException , IOException { logger . debug ( "Request URL: " + request . getRequestURI ( ) ) ; logger . debug ( "Request Path Info: " + request . getPathInfo ( ) ) ; logger . debug ( "Request Param: " + request . getQueryString ( ) ) ; String jsonOutput ; String inGeomWKT = request . getParameter ( "geometry" ) ; String fromSRID = request . getParameter ( "srid" ) ; JSONObject obj = new JSONObject ( ) ; JSONArray arr = new JSONArray ( ) ; code_block = TryStatement ;  jsonOutput = arr . toString ( ) ; PrintWriter pw = response . getWriter ( ) ; response . setContentType ( MimeType . APPLICATION_JSON ) ; pw . write ( jsonOutput ) ; return ; }
public void doGet ( HttpServletRequest request , HttpServletResponse response ) throws ServletException , IOException { logger . debug ( "Request URL: " + request . getRequestURI ( ) ) ; logger . debug ( "Request Path Info: " + request . getPathInfo ( ) ) ; logger . debug ( "Request Param: " + request . getQueryString ( ) ) ; String jsonOutput ; String inGeomWKT = request . getParameter ( "geometry" ) ; String fromSRID = request . getParameter ( "srid" ) ; JSONObject obj = new JSONObject ( ) ; JSONArray arr = new JSONArray ( ) ; code_block = TryStatement ;  jsonOutput = arr . toString ( ) ; PrintWriter pw = response . getWriter ( ) ; response . setContentType ( MimeType . APPLICATION_JSON ) ; pw . write ( jsonOutput ) ; return ; }
public void doGet ( HttpServletRequest request , HttpServletResponse response ) throws ServletException , IOException { logger . debug ( "Request URL: " + request . getRequestURI ( ) ) ; logger . debug ( "Request Path Info: " + request . getPathInfo ( ) ) ; logger . debug ( "Request Param: " + request . getQueryString ( ) ) ; String jsonOutput ; String inGeomWKT = request . getParameter ( "geometry" ) ; String fromSRID = request . getParameter ( "srid" ) ; JSONObject obj = new JSONObject ( ) ; JSONArray arr = new JSONArray ( ) ; code_block = TryStatement ;  jsonOutput = arr . toString ( ) ; PrintWriter pw = response . getWriter ( ) ; response . setContentType ( MimeType . APPLICATION_JSON ) ; pw . write ( jsonOutput ) ; return ; }
public void test() { try { code_block = IfStatement ; } catch ( JSONException e ) { logger . error ( "Cannot write JSON!" , e ) ; } }
@ Override public void save ( Note note , AuthenticationInfo subject ) throws IOException { code_block = IfStatement ; String jsonNote = GSON . toJson ( note ) ; String token = getUserToken ( subject . getUser ( ) ) ; LOG . info ( "ZeppelinHub REST API saving note {} " , note . getId ( ) ) ; restApiClient . put ( token , jsonNote ) ; }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "{} connecting {} to {} ({})" , remoteEndpoint , localAddress , remoteAddress , connectTimeout ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "{} connected {} {}->{}" , remoteEndpoint , session . getId ( ) , session . getLocalAddress ( ) , session . getRemoteAddress ( ) ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "{} connection to {} failed ({}); terminating operation" , remoteEndpoint , remoteAddress , cause . getClass ( ) ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "{} connection to {} failed ({}); retrying connection to the next address" , remoteEndpoint , remoteAddress , cause . getClass ( ) ) ; } }
public void test() { try { URL log4jurl = getURL ( Log4jURL ) ; DOMConfigurator . configure ( log4jurl ) ; } catch ( Exception e ) { logger . info ( "Failed to initialize LOG4J with xml file." ) ; return false ; } }
public void test() { try { com . liferay . portal . kernel . model . Layout returnValue = LayoutServiceUtil . fetchLayout ( groupId , privateLayout , layoutId ) ; return com . liferay . portal . kernel . model . LayoutSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { for ( SQLException ex = e ; ex != null ; ex = ex . getNextException ( ) ) { WGLOG . error ( ex , "E04003" , profile . getResourceName ( ) , script . getName ( ) , script . getTableName ( ) , script . getColumnNames ( ) ) ; } }
public void test() { if ( ! ( cdkAtom instanceof IPseudoAtom ) ) { code_block = IfStatement ; } }
public void test() { try { customizer . customize ( cdkAtom , cmlAtom ) ; } catch ( Exception exception ) { logger . error ( "Error while customizing CML output with customizer: " , customizer . getClass ( ) . getName ( ) ) ; logger . debug ( exception ) ; } }
public void test() { try { MantaHttpHeaders headers = new MantaHttpHeaders ( currentResponse . getAllHeaders ( ) ) ; e . setResponseHeaders ( headers ) ; } catch ( RuntimeException re ) { LOG . warn ( "Unable to convert response headers to MantaHttpHeaders" , e ) ; } }
@ Override public void removeProcedureForOffering ( String offering , String procedure ) { CacheValidation . notNullOrEmpty ( OFFERING , offering ) ; CacheValidation . notNullOrEmpty ( PROCEDURE , procedure ) ; LOG . trace ( "Removing procedure {} from offering {}" , procedure , offering ) ; this . proceduresForOfferings . getOrDefault ( offering , Collections . emptySet ( ) ) . remove ( procedure ) ; }
public void test() { { log . info ( Color . GREEN + "Transfer_4_1 : missing column min_transfer_time" + Color . NORMAL ) ; Context context = new Context ( ) ; CheckPointReport result = verifyValidation ( log , context , "transfer_4_1" , GTFS_1_GTFS_Common_15 , SEVERITY . ERROR , RESULT . NOK , true ) ; Assert . assertEquals ( result . getCheckPointErrorCount ( ) , 5 , "detail count" ) ; int i = 2 ; code_block = ForStatement ; } }
public void test() { { logger . debug ( "New getIntraPingMedianMeasurement get request recieved with attribute: {}" , attribute ) ; final QoSIntraPingMeasurementResponseDTO response = pingService . getMedianIntraPingMeasurement ( Utilities . convertStringToQoSMeasurementAttribute ( attribute ) ) ; logger . debug ( "PingMeasurement entry successfully retrieved" ) ; return response ; } }
public void test() { { logger . debug ( "New getIntraPingMedianMeasurement get request recieved with attribute: {}" , attribute ) ; final QoSIntraPingMeasurementResponseDTO response = pingService . getMedianIntraPingMeasurement ( Utilities . convertStringToQoSMeasurementAttribute ( attribute ) ) ; logger . debug ( "PingMeasurement entry successfully retrieved" ) ; return response ; } }
public void test() { if ( INFO . getInt ( 0 ) < 0 ) { throw new Error ( "Parameter " + INFO . getInt ( 0 ) + " to gesvd() was not valid" ) ; } else-if ( INFO . getInt ( 0 ) > 0 ) { log . warn ( "The matrix contains singular elements. Check S matrix at row " + INFO . getInt ( 0 ) ) ; } }
@ Override public void initialize ( ) { logger . debug ( "Initializing RME handler." ) ; code_block = IfStatement ; code_block = IfStatement ; port = ( String ) getConfig ( ) . get ( PORT ) ; sleep = 250 ; interval = 5000 ; super . initialize ( ) ; }
public void test() { if ( _log . isWarnEnabled ( ) ) { _log . warn ( exception , exception ) ; } }
public void test() { if ( _log . isWarnEnabled ( ) ) { _log . warn ( throwable . getMessage ( ) ) ; } }
public void test() { if ( throwable instanceof FileNotFoundException ) { code_block = IfStatement ; } else { _log . error ( exception , exception ) ; } }
@ Test public void testDeserialize_out ( ) throws Exception { Optional < ? extends RpcDefinition > loadRpc = ConverterUtils . loadRpc ( this . effectiveModelContext , SIMPLE_IO_RPC_QNAME ) ; String loadIoRpcOut = loadResourceAsString ( "input-output-rpc-out.json" ) ; NormalizedNode < ? , ? > deserializeRpc = bindingSerializer . deserialize ( loadRpc . get ( ) , new StringReader ( loadIoRpcOut ) ) ; Assert . assertNotNull ( deserializeRpc ) ; LOG . info ( deserializeRpc . toString ( ) ) ; }
public void test() { try { MetricSlice currentSlice = MetricSlice . from ( me . getId ( ) , startTime , endTime , me . getFilters ( ) ) ; DataFrame df = this . aggregationLoader . loadAggregate ( currentSlice , Collections . < String > emptyList ( ) , - 1 ) ; anomaly . setAvgCurrentVal ( df . getDouble ( DataFrame . COL_VALUE , 0 ) ) ; } catch ( Exception e ) { LOG . warn ( "Can't get the current value for {}, from {}-{}" , me . getId ( ) , startTime , endTime , e ) ; anomaly . setAvgCurrentVal ( Double . NaN ) ; } }
public void test() { if ( k instanceof ChukwaArchiveKey && v instanceof ChunkImpl ) { ChunkImpl value = ( ChunkImpl ) v ; Report xtrReport = Report . createFromString ( new String ( value . getData ( ) ) ) ; code_block = TryStatement ;  t = new Text ( value . getData ( ) ) ; } else-if ( k instanceof ChukwaRecordKey && v instanceof ChukwaRecord ) { ChukwaRecord value = ( ChukwaRecord ) v ; Report xtrReport = Report . createFromString ( value . getValue ( Record . bodyField ) ) ; bw = new BytesWritable ( xtrReport . getMetadata ( ) . getTaskId ( ) . get ( ) ) ; t = new Text ( value . getValue ( Record . bodyField ) ) ; } else { log . error ( "unexpected key/value types: " + k . getClass ( ) . getCanonicalName ( ) + " and " + v . getClass ( ) . getCanonicalName ( ) ) ; return ; } }
public void test() { try { User user = userDAO . findByLoginName ( loginName ) ; code_block = ForStatement ; } catch ( InstanceNotFoundException e ) { LOG . warn ( "there isn't a logged user for:" + loginName , e ) ; } }
public void test() { try { packetInfoList = PcapParser . parse ( input . getBinary ( 0 ) ) ; code_block = IfStatement ; } catch ( Exception e ) { collector . fail ( input ) ; e . printStackTrace ( ) ; LOG . error ( "Exception while processing tuple" , e ) ; JSONObject error = ErrorGenerator . generateErrorMessage ( "Alerts problem: " + input . getBinary ( 0 ) , e ) ; collector . emit ( "error" , new Values ( error ) ) ; return ; } }
public void test() { try { hikari . setMetricsTrackerFactory ( new MicrometerMetricsTrackerFactory ( this . registry ) ) ; } catch ( Exception ex ) { logger . warn ( LogMessage . format ( "Failed to bind Hikari metrics: %s" , ex . getMessage ( ) ) ) ; } }
public void test() { if ( this . logger . isDebugEnabled ( ) ) { this . logger . debug ( "Found function for POST: " + path ) ; } }
public void test() { if ( path . startsWith ( this . prefix ) ) { path = path . substring ( this . prefix . length ( ) ) ; } }
public void test() { try { BayesParameters params = new BayesParameters ( job . get ( "bayes.parameters" , "" ) ) ; log . info ( "Bayes Parameter {}" , params . print ( ) ) ; log . info ( "{}" , params . print ( ) ) ; Algorithm algorithm ; Datastore datastore ; code_block = IfStatement ; classifier = new ClassifierContext ( algorithm , datastore ) ; classifier . initialize ( ) ; defaultCategory = params . get ( "defaultCat" ) ; gramSize = params . getGramSize ( ) ; } catch ( IOException ex ) { log . warn ( ex . toString ( ) , ex ) ; } catch ( InvalidDatastoreException e ) { log . error ( e . toString ( ) , e ) ; } }
public void test() { try { BayesParameters params = new BayesParameters ( job . get ( "bayes.parameters" , "" ) ) ; log . info ( "Bayes Parameter {}" , params . print ( ) ) ; log . info ( "{}" , params . print ( ) ) ; Algorithm algorithm ; Datastore datastore ; code_block = IfStatement ; classifier = new ClassifierContext ( algorithm , datastore ) ; classifier . initialize ( ) ; defaultCategory = params . get ( "defaultCat" ) ; gramSize = params . getGramSize ( ) ; } catch ( IOException ex ) { log . warn ( ex . toString ( ) , ex ) ; } catch ( InvalidDatastoreException e ) { log . error ( e . toString ( ) , e ) ; } }
public void test() { if ( "bayes" . equalsIgnoreCase ( params . get ( "classifierType" ) ) ) { log . info ( "Testing Bayes Classifier" ) ; algorithm = new BayesAlgorithm ( ) ; datastore = new InMemoryBayesDatastore ( params ) ; } else-if ( "cbayes" . equalsIgnoreCase ( params . get ( "classifierType" ) ) ) { log . info ( "Testing Complementary Bayes Classifier" ) ; algorithm = new CBayesAlgorithm ( ) ; datastore = new InMemoryBayesDatastore ( params ) ; } else { throw new IllegalArgumentException ( "Unrecognized classifier type: " + params . get ( "classifierType" ) ) ; } }
public void test() { if ( "bayes" . equalsIgnoreCase ( params . get ( "classifierType" ) ) ) { log . info ( "Testing Bayes Classifier" ) ; algorithm = new BayesAlgorithm ( ) ; datastore = new InMemoryBayesDatastore ( params ) ; } else-if ( "cbayes" . equalsIgnoreCase ( params . get ( "classifierType" ) ) ) { log . info ( "Testing Complementary Bayes Classifier" ) ; algorithm = new CBayesAlgorithm ( ) ; datastore = new InMemoryBayesDatastore ( params ) ; } else { throw new IllegalArgumentException ( "Unrecognized classifier type: " + params . get ( "classifierType" ) ) ; } }
public void test() { try { BayesParameters params = new BayesParameters ( job . get ( "bayes.parameters" , "" ) ) ; log . info ( "Bayes Parameter {}" , params . print ( ) ) ; log . info ( "{}" , params . print ( ) ) ; Algorithm algorithm ; Datastore datastore ; code_block = IfStatement ; classifier = new ClassifierContext ( algorithm , datastore ) ; classifier . initialize ( ) ; defaultCategory = params . get ( "defaultCat" ) ; gramSize = params . getGramSize ( ) ; } catch ( IOException ex ) { log . warn ( ex . toString ( ) , ex ) ; } catch ( InvalidDatastoreException e ) { log . error ( e . toString ( ) , e ) ; } }
public void test() { try { BayesParameters params = new BayesParameters ( job . get ( "bayes.parameters" , "" ) ) ; log . info ( "Bayes Parameter {}" , params . print ( ) ) ; log . info ( "{}" , params . print ( ) ) ; Algorithm algorithm ; Datastore datastore ; code_block = IfStatement ; classifier = new ClassifierContext ( algorithm , datastore ) ; classifier . initialize ( ) ; defaultCategory = params . get ( "defaultCat" ) ; gramSize = params . getGramSize ( ) ; } catch ( IOException ex ) { log . warn ( ex . toString ( ) , ex ) ; } catch ( InvalidDatastoreException e ) { log . error ( e . toString ( ) , e ) ; } }
public void test() { try { changeZkState ( States . CLOSED ) ; } catch ( IOException e ) { LOG . warn ( "Connection close fails when migrates state from {} to CLOSED" , getZkState ( ) ) ; } }
protected void initialize ( ) { log . debug ( "Creating persisted atom mannager." ) ; atomManager = createAtomManager ( db ) ; log . debug ( "Atom manager initialization complete." ) ; reasoner = createReasoner ( ) ; termStore = createTermStore ( ) ; groundRuleStore = createGroundRuleStore ( ) ; termGenerator = createTermGenerator ( ) ; termStore . ensureVariableCapacity ( atomManager . getCachedRVACount ( ) ) ; completeInitialize ( ) ; }
protected void initialize ( ) { log . debug ( "Creating persisted atom mannager." ) ; atomManager = createAtomManager ( db ) ; log . debug ( "Atom manager initialization complete." ) ; reasoner = createReasoner ( ) ; termStore = createTermStore ( ) ; groundRuleStore = createGroundRuleStore ( ) ; termGenerator = createTermGenerator ( ) ; termStore . ensureVariableCapacity ( atomManager . getCachedRVACount ( ) ) ; completeInitialize ( ) ; }
public void test() { try { exporter . writeCsar ( entryServiceTemplate . get ( ) , out , exportConfiguration ) ; } catch ( RepositoryCorruptException | InterruptedException | AccountabilityException | ExecutionException e ) { LOGGER . warn ( "Exporting the csar failed with an exception" , e ) ; throw new IOException ( "Failed to export CSAR" , e ) ; } }
public void stopForcefully ( ) { log . info ( "Stopping forcefully (status: [%s])" , status ) ; stopRequested . set ( true ) ; runThread . interrupt ( ) ; }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "==> RangerHdfsAuditHandler.logHadoopEvent(" + path + ", " + action + ", " + accessGranted + ")" ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "<== RangerHdfsAuditHandler.logHadoopEvent(" + path + ", " + action + ", " + accessGranted + "): " + auditEvent ) ; } }
@ Test public void testBlackListMultiValueIncluded ( ) throws Exception { log . info ( "------  testBlackListMultiValueIncluded  ------" ) ; String cont = "'europe'" ; String state = "'mississippi'" ; code_block = ForStatement ; }
public void test() { if ( LOG . isWarnEnabled ( ) ) { LOG . warn ( INVALID_REASON , what . get ( ) , reason . get ( ) ) ; } }
public void test() { if ( s_logger . isDebugEnabled ( ) ) { s_logger . debug ( "Copying " + script . getPath ( ) + " to " + s_ovsAgentPath + " on " + _ip + " with permission 0644" ) ; } }
public void test() { try { String result = connectController . getConfigManagementService ( ) . putConnectorConfig ( connectorName , configs ) ; code_block = IfStatement ; } catch ( Exception e ) { log . error ( "Handle createConnector error ." , e ) ; context . result ( "failed" ) ; } }
public void test() { if ( ! exec . isTerminated ( ) ) { log . warn ( "Failed to terminate monitor executor service." ) ; } }
public void test() { for ( PropertySource < ? > propertySource : propertySources ) { logger . info ( "Environment order " + propertySource . getName ( ) ) ; } }
public void attachDirty ( ModZobjBstMassMitarb instance ) { log . debug ( "attaching dirty ModZobjBstMassMitarb instance" ) ; code_block = TryStatement ;  }
public void test() { try { getSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { getSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { result = metaServerConsoleServiceManagerWrapper . get ( dc ) . changePrimaryDcCheck ( cluster , shard , newPrimaryDc ) ; future ( ) . setSuccess ( result ) ; } catch ( Exception e ) { getLogger ( ) . error ( "[MigrateDcCheck][Failed]{}-{}-{}-{}" , cluster , shard , dc , newPrimaryDc ) ; future ( ) . setFailure ( e ) ; } }
public void test() { if ( refreshSchema ) { logger . error ( "Error during schema refresh ({}). The schema from Cluster.getMetadata() might appear stale. Asynchronously submitting job to fix." , e . getMessage ( ) ) ; submitSchemaRefresh ( targetType , targetKeyspace , targetName ) ; } else { logger . warn ( "Error while waiting for schema agreement" , e ) ; } }
public void test() { try { client . startElection ( request , handler ) ; } catch ( Exception e ) { logger . error ( "{}: Cannot request a vote from {}" , memberName , node , e ) ; } }
public void test() { if ( line . contains ( ".." ) ) { log . warn ( "{} skip delete file {} located in parent directory " , LOG_PREFIX , line ) ; } else { String fullPath = TARGET_DIR + File . separator + line ; File file = new File ( fullPath ) ; log . info ( "{} deleting file  {}" , LOG_PREFIX , fullPath ) ; code_block = IfStatement ; } }
public void test() { if ( file . delete ( ) ) { log . info ( "{} successfully deleted file {}" , LOG_PREFIX , fullPath ) ; } else { log . error ( "{} could not delete file {}" , LOG_PREFIX , fullPath ) ; } }
public void test() { if ( file . delete ( ) ) { log . info ( "{} successfully deleted file {}" , LOG_PREFIX , fullPath ) ; } else { log . error ( "{} could not delete file {}" , LOG_PREFIX , fullPath ) ; } }
public void test() { try ( BufferedReader br = new BufferedReader ( new InputStreamReader ( new FileInputStream ( deleteFileListName ) , StandardCharsets . UTF_8 ) ) ; ) { String line ; code_block = WhileStatement ; } catch ( SecurityException | IOException e ) { log . error ( "Delete file exception " , e ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "[" + id_ + "] moveTo(" + x + ", " + y + ")" ) ; } }
public void test() { if ( ! validate ( key , authData ) ) { keyStr = new String ( key , UTF_8 ) ; LOG . debug ( "KeyAuthenticationProvider handleAuthentication ({}, {}) -> FAIL.\n" , keyStr , authStr ) ; return KeeperException . Code . AUTHFAILED ; } }
@ Override public KeeperException . Code handleAuthentication ( ServerObjs serverObjs , byte [ ] authData ) { byte [ ] key = getKey ( serverObjs . getZks ( ) ) ; String authStr = new String ( authData , UTF_8 ) ; String keyStr = "" ; code_block = IfStatement ; LOG . debug ( "KeyAuthenticationProvider handleAuthentication -> OK.\n" ) ; serverObjs . getCnxn ( ) . addAuthInfo ( new Id ( getScheme ( ) , keyStr ) ) ; return KeeperException . Code . OK ; }
public boolean checkPermission ( String permission ) { LOGGER . debug ( "Execute MCRAccessBaseImpl checkPermission for permission {}" , permission ) ; return true ; }
@ Override public Point doGetLocation ( ) { Point point = element . getLocation ( ) ; LOGGER . debug ( Messager . ELEMENT_ATTRIBUTE_FOUND . getMessage ( "Location" , point . toString ( ) , getName ( ) ) ) ; return point ; }
@ GET @ Produces ( MediaType . APPLICATION_JSON + ";charset=UTF-8" ) @ Path ( "{personID}/{resourceID}" ) public Response < Resource > getResourceFromPersonById ( @ PathParam ( "said" ) String said , @ PathParam ( "personID" ) String personID , @ PathParam ( "resourceID" ) String resourceID ) { Data < Resource > data ; logger . info ( "called API method: GET /dime/rest/" + said + "/resource/" + personID + "/" + resourceID ) ; code_block = TryStatement ;  return Response . ok ( data ) ; }
public void test() { try { SchemaRegistry < Schema > registry = ( SchemaRegistry < Schema > ) Class . forName ( props . getProperty ( KafkaAvroMessageEncoder . KAFKA_MESSAGE_CODER_SCHEMA_REGISTRY_CLASS ) ) . newInstance ( ) ; log . info ( "Prop " + KafkaAvroMessageEncoder . KAFKA_MESSAGE_CODER_SCHEMA_REGISTRY_CLASS + " is: " + props . getProperty ( KafkaAvroMessageEncoder . KAFKA_MESSAGE_CODER_SCHEMA_REGISTRY_CLASS ) ) ; log . info ( "Underlying schema registry for topic: " + topicName + " is: " + registry ) ; registry . init ( props ) ; this . registry = new CachedSchemaRegistry < Schema > ( registry , props ) ; this . latestSchema = registry . getLatestSchemaByTopic ( topicName ) . getSchema ( ) ; } catch ( Exception e ) { throw new MessageDecoderException ( e ) ; } }
public void test() { try { SchemaRegistry < Schema > registry = ( SchemaRegistry < Schema > ) Class . forName ( props . getProperty ( KafkaAvroMessageEncoder . KAFKA_MESSAGE_CODER_SCHEMA_REGISTRY_CLASS ) ) . newInstance ( ) ; log . info ( "Prop " + KafkaAvroMessageEncoder . KAFKA_MESSAGE_CODER_SCHEMA_REGISTRY_CLASS + " is: " + props . getProperty ( KafkaAvroMessageEncoder . KAFKA_MESSAGE_CODER_SCHEMA_REGISTRY_CLASS ) ) ; log . info ( "Underlying schema registry for topic: " + topicName + " is: " + registry ) ; registry . init ( props ) ; this . registry = new CachedSchemaRegistry < Schema > ( registry , props ) ; this . latestSchema = registry . getLatestSchemaByTopic ( topicName ) . getSchema ( ) ; } catch ( Exception e ) { throw new MessageDecoderException ( e ) ; } }
private void onThingWithSerialNumber ( final ThingTypeUID deviceType , final String deviceTypeName , final DeviceDTO device , final String name ) { final String serialNumber = device . getSerialNumber ( ) ; logger . debug ( "{} discovered, serialnumber: {}" , deviceTypeName , serialNumber ) ; ThingUID localBridgeUID = this . bridgeUID ; code_block = IfStatement ; }
@ BeforeClass public static void createFsCrawlerJobDir ( ) throws IOException { metadataDir = rootTmpDir . resolve ( ".fscrawler" ) ; code_block = IfStatement ; copyDefaultResources ( metadataDir ) ; staticLogger . debug ( "  --> Test metadata dir ready in [{}]" , metadataDir ) ; }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { if ( event . getMinResponseTime ( ) != 0 ) { logger . info ( "{} dbMaintainer start" , traceItem ) ; dbMaintainer . run ( traceItem ) ; logger . info ( "{} dbMaintainer finished" , traceItem ) ; return new HealthStatus ( ) . withStatus ( "OK" ) ; } }
public void test() { try { LOGGER . info ( String . format ( "Loading '%s' from the classpath." , defaultConfigFile ) ) ; configurationUrl = Config . class . getClassLoader ( ) . getResource ( defaultConfigFile ) ; code_block = IfStatement ; in = Config . class . getClassLoader ( ) . getResourceAsStream ( defaultConfigFile ) ; code_block = IfStatement ; } catch ( RuntimeException e ) { throw new HazelcastException ( e ) ; } }
public void test() { try { IEditorInput editorInput = editorReference . getEditorInput ( ) ; code_block = IfStatement ; } catch ( Exception e ) { LOG . error ( "Error accessing document" , e ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( exception , exception ) ; } }
public void test() { try { executor . submit ( ( ) -> executeThread ( task , description ) ) . get ( timeout . toMillis ( ) , TimeUnit . MILLISECONDS ) ; } catch ( InterruptedException e ) { log . warn ( "{} was interrupted running task: {}" , name , description ) ; } catch ( TimeoutException e ) { log . error ( "{} timed out running task: {}" , name , description ) ; } catch ( Throwable e ) { log . error ( "{} caught exception in task: {}" , name , description , e ) ; } }
public void test() { try { executor . submit ( ( ) -> executeThread ( task , description ) ) . get ( timeout . toMillis ( ) , TimeUnit . MILLISECONDS ) ; } catch ( InterruptedException e ) { log . warn ( "{} was interrupted running task: {}" , name , description ) ; } catch ( TimeoutException e ) { log . error ( "{} timed out running task: {}" , name , description ) ; } catch ( Throwable e ) { log . error ( "{} caught exception in task: {}" , name , description , e ) ; } }
public void test() { try { executor . submit ( ( ) -> executeThread ( task , description ) ) . get ( timeout . toMillis ( ) , TimeUnit . MILLISECONDS ) ; } catch ( InterruptedException e ) { log . warn ( "{} was interrupted running task: {}" , name , description ) ; } catch ( TimeoutException e ) { log . error ( "{} timed out running task: {}" , name , description ) ; } catch ( Throwable e ) { log . error ( "{} caught exception in task: {}" , name , description , e ) ; } }
public void test() { if ( thing == null ) { logger . warn ( "Cannot create thing of type '{}'. Binding '{}' says it supports it, but it could not be created." , thingTypeUID , thingHandlerFactory . getClass ( ) . getName ( ) ) ; } else { thing . setLabel ( label ) ; return thing ; } }
public void test() { if ( KeystoreType . PKCS12 . toString ( ) . equalsIgnoreCase ( keyStoreType ) ) { logger . info ( "Command line argument --" + KEY_STORE_TYPE_ARG + "=" + keyStoreType + " only applies to keystore, recommended truststore type of " + KeystoreType . JKS . toString ( ) + " unaffected." ) ; } }
@ Override public void bindServiceInstance ( String applicationName , String serviceInstanceName ) { logger . debug ( Messages . BINDING_SERVICE_INSTANCE_0_TO_APPLICATION_1 , serviceInstanceName , applicationName ) ; delegate . bindServiceInstance ( applicationName , serviceInstanceName ) ; }
public void test() { try { registryMutex . acquire ( ) ; } catch ( InterruptedException e ) { LOGGER . error ( "Exception while waiting for registry mutex. Returning null." , e ) ; return null ; } }
public void test() { if ( ! events . isEmpty ( ) ) { StringBuilder sb = new StringBuilder ( ) ; sb . append ( "[" ) ; events . stream ( ) . sorted ( Comparator . comparing ( ConfigEvent :: getName ) ) . forEach ( event code_block = LoopStatement ; ) ; sb . append ( "\n" ) . append ( "]" ) ; LOGGER . info ( sb . toString ( ) ) ; } }
@ PreDestroy public void stop ( ) throws IOException { _log . info ( "stopping orbcad ftp download client" ) ; super . stop ( ) ; if ( _ftpClient != null ) _ftpClient . disconnect ( ) ; }
public void test() { if ( IS_TIMER_COMPSS_ENABLED ) { timeBindOriginalFilesStart = System . nanoTime ( ) ; } }
private void assertSendReceiveLargeMessage ( JmsProvider jmsProvider , MessageProducer sender , MessageConsumer receiver , double sizeInMB , int mode , int count , List < javax . jms . Message > messages ) { List < javax . jms . Message > recvd ; jmsProvider . sendMessages ( sender , messages , mode , javax . jms . Message . DEFAULT_PRIORITY , javax . jms . Message . DEFAULT_TIME_TO_LIVE ) ; LOGGER . info ( "{}MB {} message sent" , sizeInMB , mode == DeliveryMode . PERSISTENT ? "durable" : "non-durable" ) ; recvd = jmsProvider . receiveMessages ( receiver , count , 2000 ) ; assertThat ( "Wrong count of received messages" , recvd . size ( ) , Matchers . is ( count ) ) ; LOGGER . info ( "{}MB {} message received" , sizeInMB , mode == DeliveryMode . PERSISTENT ? "durable" : "non-durable" ) ; }
private void assertSendReceiveLargeMessage ( JmsProvider jmsProvider , MessageProducer sender , MessageConsumer receiver , double sizeInMB , int mode , int count , List < javax . jms . Message > messages ) { List < javax . jms . Message > recvd ; jmsProvider . sendMessages ( sender , messages , mode , javax . jms . Message . DEFAULT_PRIORITY , javax . jms . Message . DEFAULT_TIME_TO_LIVE ) ; LOGGER . info ( "{}MB {} message sent" , sizeInMB , mode == DeliveryMode . PERSISTENT ? "durable" : "non-durable" ) ; recvd = jmsProvider . receiveMessages ( receiver , count , 2000 ) ; assertThat ( "Wrong count of received messages" , recvd . size ( ) , Matchers . is ( count ) ) ; LOGGER . info ( "{}MB {} message received" , sizeInMB , mode == DeliveryMode . PERSISTENT ? "durable" : "non-durable" ) ; }
public void test() { try ( Session session = modelDBHibernateUtil . getSessionFactory ( ) . openSession ( ) ) { ExperimentRunEntity experimentRunObj = session . get ( ExperimentRunEntity . class , experimentRunId ) ; LOGGER . debug ( "Got ExperimentRun Tags" ) ; return experimentRunObj . getProtoObject ( ) . getTagsList ( ) ; } catch ( Exception ex ) { code_block = IfStatement ; } }
public void test() { try { Thread . sleep ( 1000 ) ; } catch ( InterruptedException e ) { logger . error ( e . toString ( ) , e ) ; } }
public void test() { try { Map < String , Object > config = note . getConfig ( ) ; code_block = IfStatement ; } catch ( ClassCastException e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "findDocs() NXQL: " + query ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Caught exception " , e ) ; } }
@ Override public void close ( ) throws Exception { LOG . info ( "Closing the SlotManager." ) ; suspend ( ) ; slotManagerMetricGroup . close ( ) ; }
public void test() { try { metadata = OIDCProviderMetadata . parse ( resourceRetriever . retrieveResource ( new URL ( discoveryUrl ) ) . getContent ( ) ) ; } catch ( IOException | ParseException e ) { LOGGER . error ( "Unable to retrieve OAuth provider's metadata." , e ) ; return null ; } }
protected FilterHolder getCorsFilter ( ) { CrossOriginFilter filter = new CrossOriginFilter ( ) ; FilterHolder filterHolder = new FilterHolder ( filter ) ; List < String > allowedOrigins = genericEndpointProperties . getListOfValues ( RESTEndpointProperties . ENABLED_CORS_ORIGINS ) ; StringJoiner originsJoiner = new StringJoiner ( "," ) ; allowedOrigins . forEach ( origin -> originsJoiner . add ( origin ) ) ; List < String > allowedHeaders = genericEndpointProperties . getListOfValues ( RESTEndpointProperties . ENABLED_CORS_HEADERS ) ; StringJoiner headersJoiner = new StringJoiner ( "," ) ; allowedHeaders . forEach ( origin -> headersJoiner . add ( origin ) ) ; String allowedHeadersSpec = allowedHeaders . isEmpty ( ) ? "*" : headersJoiner . toString ( ) ; filterHolder . setInitParameter ( CrossOriginFilter . ALLOWED_HEADERS_PARAM , allowedHeadersSpec ) ; filterHolder . setInitParameter ( CrossOriginFilter . ALLOWED_ORIGINS_PARAM , originsJoiner . toString ( ) ) ; filterHolder . setInitParameter ( CrossOriginFilter . ALLOWED_METHODS_PARAM , "GET,POST,HEAD,DELETE,PUT,OPTIONS" ) ; log . debug ( "Will allow CORS for the following origins: " + originsJoiner . toString ( ) ) ; return filterHolder ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "doClose({})[id={}] SSH_FXP_CLOSE (handle={}[{}])" , session , id , handle , h ) ; } }
@ Test public void ff_addProjectNegativeTags ( ) { LOGGER . info ( "Add Project Tags Negative test start................................" ) ; List < String > tagsList = new ArrayList < > ( ) ; tagsList . add ( "Add Test Tag " + Calendar . getInstance ( ) . getTimeInMillis ( ) ) ; tagsList . add ( "Add Test Tag 2 " + Calendar . getInstance ( ) . getTimeInMillis ( ) ) ; AddProjectTags addProjectTagsRequest = AddProjectTags . newBuilder ( ) . addAllTags ( tagsList ) . build ( ) ; code_block = TryStatement ;  addProjectTagsRequest = AddProjectTags . newBuilder ( ) . setId ( "sdasd" ) . addAllTags ( project . getTagsList ( ) ) . build ( ) ; code_block = TryStatement ;  LOGGER . info ( "Add Project tags Negative test stop................................" ) ; }
public void test() { try { projectServiceStub . addProjectTags ( addProjectTagsRequest ) ; fail ( ) ; } catch ( StatusRuntimeException ex ) { Status status = Status . fromThrowable ( ex ) ; LOGGER . warn ( "Error Code : " + status . getCode ( ) + " Description : " + status . getDescription ( ) ) ; assertEquals ( Status . INVALID_ARGUMENT . getCode ( ) , status . getCode ( ) ) ; } }
@ Test public void ff_addProjectNegativeTags ( ) { LOGGER . info ( "Add Project Tags Negative test start................................" ) ; List < String > tagsList = new ArrayList < > ( ) ; tagsList . add ( "Add Test Tag " + Calendar . getInstance ( ) . getTimeInMillis ( ) ) ; tagsList . add ( "Add Test Tag 2 " + Calendar . getInstance ( ) . getTimeInMillis ( ) ) ; AddProjectTags addProjectTagsRequest = AddProjectTags . newBuilder ( ) . addAllTags ( tagsList ) . build ( ) ; code_block = TryStatement ;  addProjectTagsRequest = AddProjectTags . newBuilder ( ) . setId ( "sdasd" ) . addAllTags ( project . getTagsList ( ) ) . build ( ) ; code_block = TryStatement ;  LOGGER . info ( "Add Project tags Negative test stop................................" ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Error code: 0x" + Hexdump . toHexString ( resp . getErrorCode ( ) , 8 ) ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Got referral " + dr ) ; } }
public void test() { try { LOG . debug ( "Checking whether {} is listening for RPCs" , mNodeAddress ) ; NetworkAddressUtils . pingService ( mNodeAddress , mServiceType , mConf , mUserState ) ; LOG . debug ( "Successfully connected to {}" , mNodeAddress ) ; return true ; } catch ( UnavailableException e ) { LOG . debug ( "Failed to connect to {}" , mNodeAddress ) ; } catch ( AlluxioStatusException e ) { throw new RuntimeException ( e ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { if ( e instanceof ThriftSecurityException ) { result . sec = ( ThriftSecurityException ) e ; result . setSecIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof ThriftSecurityException ) { result . sec = ( ThriftSecurityException ) e ; result . setSecIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof ThriftSecurityException ) { result . sec = ( ThriftSecurityException ) e ; result . setSecIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { try { fcall . sendResponse ( fb , msg , msgType , seqid ) ; } catch ( java . lang . Exception ex ) { _LOGGER . error ( "Exception writing to internal frame buffer" , ex ) ; fb . close ( ) ; } }
public void test() { try { kuraPayload = new CloudPayloadProtoBufDecoderImpl ( payload ) . buildFromByteArray ( ) ; } catch ( Exception e ) { logger . debug ( "Received message on topic {} that could not be decoded. Wrapping it into an KuraPayload." , topic ) ; kuraPayload = new KuraPayload ( ) ; kuraPayload . setBody ( payload ) ; } }
@ Test public void scenario18Test2 ( ) throws IOException { final String testObj = ingestObj ( "/rest/read_append_resource" ) ; final String id = "/rest/read_append_resource/" + getRandomUniqueId ( ) ; ingestObj ( id ) ; logger . debug ( "user18 can read (has ACL:READ): {}" , id ) ; final HttpGet requestGet = getObjMethod ( id ) ; setAuth ( requestGet , "user18" ) ; assertEquals ( HttpStatus . SC_OK , getStatus ( requestGet ) ) ; logger . debug ( "user18 can't append (no ACL): {}" , id ) ; final HttpPatch requestPatch = patchObjMethod ( id ) ; setAuth ( requestPatch , "user18" ) ; requestPatch . setHeader ( "Content-type" , "application/sparql-update" ) ; requestPatch . setEntity ( new StringEntity ( "INSERT code_block = "" ; WHERE {}" ) ) ; assertEquals ( HttpStatus . SC_FORBIDDEN , getStatus ( requestPatch ) ) ; ingestAcl ( "fedoraAdmin" , "/acls/18/read-append-acl.ttl" , testObj + "/fcr:acl" ) ; logger . debug ( "user18 can't delete (no ACL): {}" , id ) ; final HttpDelete requestDelete = deleteObjMethod ( id ) ; setAuth ( requestDelete , "user18" ) ; assertEquals ( HttpStatus . SC_FORBIDDEN , getStatus ( requestDelete ) ) ; logger . debug ( "user18 can read (ACL read, append): {}" , id ) ; assertEquals ( HttpStatus . SC_OK , getStatus ( requestGet ) ) ; logger . debug ( "user18 can append (ACL read, append): {}" , id ) ; assertEquals ( HttpStatus . SC_NO_CONTENT , getStatus ( requestPatch ) ) ; logger . debug ( "user18 still can't delete (ACL read, append): {}" , id ) ; assertEquals ( HttpStatus . SC_FORBIDDEN , getStatus ( requestDelete ) ) ; }
@ Test public void scenario18Test2 ( ) throws IOException { final String testObj = ingestObj ( "/rest/read_append_resource" ) ; final String id = "/rest/read_append_resource/" + getRandomUniqueId ( ) ; ingestObj ( id ) ; logger . debug ( "user18 can read (has ACL:READ): {}" , id ) ; final HttpGet requestGet = getObjMethod ( id ) ; setAuth ( requestGet , "user18" ) ; assertEquals ( HttpStatus . SC_OK , getStatus ( requestGet ) ) ; logger . debug ( "user18 can't append (no ACL): {}" , id ) ; final HttpPatch requestPatch = patchObjMethod ( id ) ; setAuth ( requestPatch , "user18" ) ; requestPatch . setHeader ( "Content-type" , "application/sparql-update" ) ; requestPatch . setEntity ( new StringEntity ( "INSERT code_block = "" ; WHERE {}" ) ) ; assertEquals ( HttpStatus . SC_FORBIDDEN , getStatus ( requestPatch ) ) ; ingestAcl ( "fedoraAdmin" , "/acls/18/read-append-acl.ttl" , testObj + "/fcr:acl" ) ; logger . debug ( "user18 can't delete (no ACL): {}" , id ) ; final HttpDelete requestDelete = deleteObjMethod ( id ) ; setAuth ( requestDelete , "user18" ) ; assertEquals ( HttpStatus . SC_FORBIDDEN , getStatus ( requestDelete ) ) ; logger . debug ( "user18 can read (ACL read, append): {}" , id ) ; assertEquals ( HttpStatus . SC_OK , getStatus ( requestGet ) ) ; logger . debug ( "user18 can append (ACL read, append): {}" , id ) ; assertEquals ( HttpStatus . SC_NO_CONTENT , getStatus ( requestPatch ) ) ; logger . debug ( "user18 still can't delete (ACL read, append): {}" , id ) ; assertEquals ( HttpStatus . SC_FORBIDDEN , getStatus ( requestDelete ) ) ; }
@ Test public void scenario18Test2 ( ) throws IOException { final String testObj = ingestObj ( "/rest/read_append_resource" ) ; final String id = "/rest/read_append_resource/" + getRandomUniqueId ( ) ; ingestObj ( id ) ; logger . debug ( "user18 can read (has ACL:READ): {}" , id ) ; final HttpGet requestGet = getObjMethod ( id ) ; setAuth ( requestGet , "user18" ) ; assertEquals ( HttpStatus . SC_OK , getStatus ( requestGet ) ) ; logger . debug ( "user18 can't append (no ACL): {}" , id ) ; final HttpPatch requestPatch = patchObjMethod ( id ) ; setAuth ( requestPatch , "user18" ) ; requestPatch . setHeader ( "Content-type" , "application/sparql-update" ) ; requestPatch . setEntity ( new StringEntity ( "INSERT code_block = "" ; WHERE {}" ) ) ; assertEquals ( HttpStatus . SC_FORBIDDEN , getStatus ( requestPatch ) ) ; ingestAcl ( "fedoraAdmin" , "/acls/18/read-append-acl.ttl" , testObj + "/fcr:acl" ) ; logger . debug ( "user18 can't delete (no ACL): {}" , id ) ; final HttpDelete requestDelete = deleteObjMethod ( id ) ; setAuth ( requestDelete , "user18" ) ; assertEquals ( HttpStatus . SC_FORBIDDEN , getStatus ( requestDelete ) ) ; logger . debug ( "user18 can read (ACL read, append): {}" , id ) ; assertEquals ( HttpStatus . SC_OK , getStatus ( requestGet ) ) ; logger . debug ( "user18 can append (ACL read, append): {}" , id ) ; assertEquals ( HttpStatus . SC_NO_CONTENT , getStatus ( requestPatch ) ) ; logger . debug ( "user18 still can't delete (ACL read, append): {}" , id ) ; assertEquals ( HttpStatus . SC_FORBIDDEN , getStatus ( requestDelete ) ) ; }
@ Test public void scenario18Test2 ( ) throws IOException { final String testObj = ingestObj ( "/rest/read_append_resource" ) ; final String id = "/rest/read_append_resource/" + getRandomUniqueId ( ) ; ingestObj ( id ) ; logger . debug ( "user18 can read (has ACL:READ): {}" , id ) ; final HttpGet requestGet = getObjMethod ( id ) ; setAuth ( requestGet , "user18" ) ; assertEquals ( HttpStatus . SC_OK , getStatus ( requestGet ) ) ; logger . debug ( "user18 can't append (no ACL): {}" , id ) ; final HttpPatch requestPatch = patchObjMethod ( id ) ; setAuth ( requestPatch , "user18" ) ; requestPatch . setHeader ( "Content-type" , "application/sparql-update" ) ; requestPatch . setEntity ( new StringEntity ( "INSERT code_block = "" ; WHERE {}" ) ) ; assertEquals ( HttpStatus . SC_FORBIDDEN , getStatus ( requestPatch ) ) ; ingestAcl ( "fedoraAdmin" , "/acls/18/read-append-acl.ttl" , testObj + "/fcr:acl" ) ; logger . debug ( "user18 can't delete (no ACL): {}" , id ) ; final HttpDelete requestDelete = deleteObjMethod ( id ) ; setAuth ( requestDelete , "user18" ) ; assertEquals ( HttpStatus . SC_FORBIDDEN , getStatus ( requestDelete ) ) ; logger . debug ( "user18 can read (ACL read, append): {}" , id ) ; assertEquals ( HttpStatus . SC_OK , getStatus ( requestGet ) ) ; logger . debug ( "user18 can append (ACL read, append): {}" , id ) ; assertEquals ( HttpStatus . SC_NO_CONTENT , getStatus ( requestPatch ) ) ; logger . debug ( "user18 still can't delete (ACL read, append): {}" , id ) ; assertEquals ( HttpStatus . SC_FORBIDDEN , getStatus ( requestDelete ) ) ; }
@ Test public void scenario18Test2 ( ) throws IOException { final String testObj = ingestObj ( "/rest/read_append_resource" ) ; final String id = "/rest/read_append_resource/" + getRandomUniqueId ( ) ; ingestObj ( id ) ; logger . debug ( "user18 can read (has ACL:READ): {}" , id ) ; final HttpGet requestGet = getObjMethod ( id ) ; setAuth ( requestGet , "user18" ) ; assertEquals ( HttpStatus . SC_OK , getStatus ( requestGet ) ) ; logger . debug ( "user18 can't append (no ACL): {}" , id ) ; final HttpPatch requestPatch = patchObjMethod ( id ) ; setAuth ( requestPatch , "user18" ) ; requestPatch . setHeader ( "Content-type" , "application/sparql-update" ) ; requestPatch . setEntity ( new StringEntity ( "INSERT code_block = "" ; WHERE {}" ) ) ; assertEquals ( HttpStatus . SC_FORBIDDEN , getStatus ( requestPatch ) ) ; ingestAcl ( "fedoraAdmin" , "/acls/18/read-append-acl.ttl" , testObj + "/fcr:acl" ) ; logger . debug ( "user18 can't delete (no ACL): {}" , id ) ; final HttpDelete requestDelete = deleteObjMethod ( id ) ; setAuth ( requestDelete , "user18" ) ; assertEquals ( HttpStatus . SC_FORBIDDEN , getStatus ( requestDelete ) ) ; logger . debug ( "user18 can read (ACL read, append): {}" , id ) ; assertEquals ( HttpStatus . SC_OK , getStatus ( requestGet ) ) ; logger . debug ( "user18 can append (ACL read, append): {}" , id ) ; assertEquals ( HttpStatus . SC_NO_CONTENT , getStatus ( requestPatch ) ) ; logger . debug ( "user18 still can't delete (ACL read, append): {}" , id ) ; assertEquals ( HttpStatus . SC_FORBIDDEN , getStatus ( requestDelete ) ) ; }
@ Test public void scenario18Test2 ( ) throws IOException { final String testObj = ingestObj ( "/rest/read_append_resource" ) ; final String id = "/rest/read_append_resource/" + getRandomUniqueId ( ) ; ingestObj ( id ) ; logger . debug ( "user18 can read (has ACL:READ): {}" , id ) ; final HttpGet requestGet = getObjMethod ( id ) ; setAuth ( requestGet , "user18" ) ; assertEquals ( HttpStatus . SC_OK , getStatus ( requestGet ) ) ; logger . debug ( "user18 can't append (no ACL): {}" , id ) ; final HttpPatch requestPatch = patchObjMethod ( id ) ; setAuth ( requestPatch , "user18" ) ; requestPatch . setHeader ( "Content-type" , "application/sparql-update" ) ; requestPatch . setEntity ( new StringEntity ( "INSERT code_block = "" ; WHERE {}" ) ) ; assertEquals ( HttpStatus . SC_FORBIDDEN , getStatus ( requestPatch ) ) ; ingestAcl ( "fedoraAdmin" , "/acls/18/read-append-acl.ttl" , testObj + "/fcr:acl" ) ; logger . debug ( "user18 can't delete (no ACL): {}" , id ) ; final HttpDelete requestDelete = deleteObjMethod ( id ) ; setAuth ( requestDelete , "user18" ) ; assertEquals ( HttpStatus . SC_FORBIDDEN , getStatus ( requestDelete ) ) ; logger . debug ( "user18 can read (ACL read, append): {}" , id ) ; assertEquals ( HttpStatus . SC_OK , getStatus ( requestGet ) ) ; logger . debug ( "user18 can append (ACL read, append): {}" , id ) ; assertEquals ( HttpStatus . SC_NO_CONTENT , getStatus ( requestPatch ) ) ; logger . debug ( "user18 still can't delete (ACL read, append): {}" , id ) ; assertEquals ( HttpStatus . SC_FORBIDDEN , getStatus ( requestDelete ) ) ; }
public void test() { try { fileName = key . substring ( key . lastIndexOf ( '/' ) + 1 ) ; s3client . copyObject ( s3Bucket , key , s3Bucket , to + "/" + fileName ) ; log . debug ( "    Copy " + fileName + " to backup folder" ) ; } catch ( Exception e ) { log . info ( "    Copy " + fileName + "failed" , e ) ; ErrorManageUtil . uploadError ( "all" , "all" , "all" , e . getMessage ( ) ) ; } }
public void test() { try { fileName = key . substring ( key . lastIndexOf ( '/' ) + 1 ) ; s3client . copyObject ( s3Bucket , key , s3Bucket , to + "/" + fileName ) ; log . debug ( "    Copy " + fileName + " to backup folder" ) ; } catch ( Exception e ) { log . info ( "    Copy " + fileName + "failed" , e ) ; ErrorManageUtil . uploadError ( "all" , "all" , "all" , e . getMessage ( ) ) ; } }
private StringBuilder buildExportTableArg ( StringBuilder builder , CatalogTable catalog ) throws FalconException { LOG . info ( "Catalog URI {}" , catalog . getUri ( ) ) ; builder . append ( "--skip-dist-cache" ) . append ( ImportExportCommon . ARG_SEPARATOR ) ; Iterator < String > itr = Splitter . on ( "" ) . split ( catalog . getUri ( ) ) . iterator ( ) ; String dbTable = itr . next ( ) ; String partitions = itr . next ( ) ; Iterator < String > itrDbTable = Splitter . on ( ":" ) . split ( dbTable ) . iterator ( ) ; itrDbTable . next ( ) ; String db = itrDbTable . next ( ) ; String table = itrDbTable . next ( ) ; LOG . debug ( "Target database {}, table {}" , db , table ) ; builder . append ( "--hcatalog-database" ) . append ( ImportExportCommon . ARG_SEPARATOR ) . append ( String . format ( "${coord:databaseIn('%s')}" , FeedExportCoordinatorBuilder . EXPORT_DATAIN_NAME ) ) . append ( ImportExportCommon . ARG_SEPARATOR ) ; builder . append ( "--hcatalog-table" ) . append ( ImportExportCommon . ARG_SEPARATOR ) . append ( String . format ( "${coord:tableIn('%s')}" , FeedExportCoordinatorBuilder . EXPORT_DATAIN_NAME ) ) . append ( ImportExportCommon . ARG_SEPARATOR ) ; Map < String , String > partitionsMap = ImportExportCommon . getPartitionKeyValues ( partitions ) ; code_block = IfStatement ; return builder ; }
private StringBuilder buildExportTableArg ( StringBuilder builder , CatalogTable catalog ) throws FalconException { LOG . info ( "Catalog URI {}" , catalog . getUri ( ) ) ; builder . append ( "--skip-dist-cache" ) . append ( ImportExportCommon . ARG_SEPARATOR ) ; Iterator < String > itr = Splitter . on ( "" ) . split ( catalog . getUri ( ) ) . iterator ( ) ; String dbTable = itr . next ( ) ; String partitions = itr . next ( ) ; Iterator < String > itrDbTable = Splitter . on ( ":" ) . split ( dbTable ) . iterator ( ) ; itrDbTable . next ( ) ; String db = itrDbTable . next ( ) ; String table = itrDbTable . next ( ) ; LOG . debug ( "Target database {}, table {}" , db , table ) ; builder . append ( "--hcatalog-database" ) . append ( ImportExportCommon . ARG_SEPARATOR ) . append ( String . format ( "${coord:databaseIn('%s')}" , FeedExportCoordinatorBuilder . EXPORT_DATAIN_NAME ) ) . append ( ImportExportCommon . ARG_SEPARATOR ) ; builder . append ( "--hcatalog-table" ) . append ( ImportExportCommon . ARG_SEPARATOR ) . append ( String . format ( "${coord:tableIn('%s')}" , FeedExportCoordinatorBuilder . EXPORT_DATAIN_NAME ) ) . append ( ImportExportCommon . ARG_SEPARATOR ) ; Map < String , String > partitionsMap = ImportExportCommon . getPartitionKeyValues ( partitions ) ; code_block = IfStatement ; return builder ; }
public void test() { if ( counterMap . containsKey ( key ) ) { int tally = counterMap . get ( key ) . decrementAndGet ( ) ; logger . debug ( "DecrementAndGet key: {}, count: {}" , key , tally ) ; return tally ; } else { counterMap . put ( key , new AtomicInteger ( - 1 ) ) ; logger . debug ( "DecrementAndGet key: {}, count: {}" , key , - 1 ) ; return - 1 ; } }
public void test() { if ( counterMap . containsKey ( key ) ) { int tally = counterMap . get ( key ) . decrementAndGet ( ) ; logger . debug ( "DecrementAndGet key: {}, count: {}" , key , tally ) ; return tally ; } else { counterMap . put ( key , new AtomicInteger ( - 1 ) ) ; logger . debug ( "DecrementAndGet key: {}, count: {}" , key , - 1 ) ; return - 1 ; } }
public void test() { if ( m_trackedObjectHash . remove ( obj ) == null ) { LOGGER . debug ( "Attempted to remove " + name + ", which was not tracked" ) ; } else { LOGGER . debug ( "Removing " + name + " (" + m_trackedObjectHash . size ( ) + " remaining)" ) ; } }
public void test() { if ( m_trackedObjectHash . remove ( obj ) == null ) { LOGGER . debug ( "Attempted to remove " + name + ", which was not tracked" ) ; } else { LOGGER . debug ( "Removing " + name + " (" + m_trackedObjectHash . size ( ) + " remaining)" ) ; } }
@ Test public void callSetAndGetGetAttributeArrayTypeDef ( ) { logger . info ( name . getMethodName ( ) ) ; String [ ] stringArray = code_block = "" ; ; ArrayTypeDefStruct arrayTypeDefArg = new ArrayTypeDefStruct ( stringArray ) ; genericGetterSetterTestMethod ( arrayTypeDefArg , "AttributeArrayTypeDef" ) ; logger . info ( name . getMethodName ( ) + " - OK" ) ; }
public void test() { for ( String entityName : names ) { LOGGER . info ( "Working with " + entityType + " : " + entityName ) ; r = helper . getProcessInstanceStatus ( entityName , null ) ; InstancesResult . Instance [ ] instancesFromStatus = r . getInstances ( ) ; LOGGER . info ( "Instances from -getStatus API: " + Arrays . toString ( instancesFromStatus ) ) ; EntitySummaryResult . EntitySummary [ ] summaries = helper . getEntitySummary ( clusterName , null ) . getEntitySummaryResult ( ) . getEntitySummaries ( ) ; EntitySummaryResult . EntitySummary summaryItem = null ; code_block = ForStatement ; Assert . assertNotNull ( summaryItem , "Appropriate summary not found for : " + entityName ) ; EntitySummaryResult . Instance [ ] instancesFromSummary = summaryItem . getInstances ( ) ; LOGGER . info ( "Instances from SummaryResult: " + Arrays . toString ( instancesFromSummary ) ) ; softAssert . assertEquals ( instancesFromSummary . length , 7 , "7 instances should be present in " + "summary." ) ; code_block = ForStatement ; } }
public void test() { for ( String entityName : names ) { LOGGER . info ( "Working with " + entityType + " : " + entityName ) ; r = helper . getProcessInstanceStatus ( entityName , null ) ; InstancesResult . Instance [ ] instancesFromStatus = r . getInstances ( ) ; LOGGER . info ( "Instances from -getStatus API: " + Arrays . toString ( instancesFromStatus ) ) ; EntitySummaryResult . EntitySummary [ ] summaries = helper . getEntitySummary ( clusterName , null ) . getEntitySummaryResult ( ) . getEntitySummaries ( ) ; EntitySummaryResult . EntitySummary summaryItem = null ; code_block = ForStatement ; Assert . assertNotNull ( summaryItem , "Appropriate summary not found for : " + entityName ) ; EntitySummaryResult . Instance [ ] instancesFromSummary = summaryItem . getInstances ( ) ; LOGGER . info ( "Instances from SummaryResult: " + Arrays . toString ( instancesFromSummary ) ) ; softAssert . assertEquals ( instancesFromSummary . length , 7 , "7 instances should be present in " + "summary." ) ; code_block = ForStatement ; } }
public void test() { for ( String entityName : names ) { LOGGER . info ( "Working with " + entityType + " : " + entityName ) ; r = helper . getProcessInstanceStatus ( entityName , null ) ; InstancesResult . Instance [ ] instancesFromStatus = r . getInstances ( ) ; LOGGER . info ( "Instances from -getStatus API: " + Arrays . toString ( instancesFromStatus ) ) ; EntitySummaryResult . EntitySummary [ ] summaries = helper . getEntitySummary ( clusterName , null ) . getEntitySummaryResult ( ) . getEntitySummaries ( ) ; EntitySummaryResult . EntitySummary summaryItem = null ; code_block = ForStatement ; Assert . assertNotNull ( summaryItem , "Appropriate summary not found for : " + entityName ) ; EntitySummaryResult . Instance [ ] instancesFromSummary = summaryItem . getInstances ( ) ; LOGGER . info ( "Instances from SummaryResult: " + Arrays . toString ( instancesFromSummary ) ) ; softAssert . assertEquals ( instancesFromSummary . length , 7 , "7 instances should be present in " + "summary." ) ; code_block = ForStatement ; } }
public void test() { try { code_block = IfStatement ; } catch ( Throwable t ) { LOG . error ( "Encountered a throwable while notifying observers: of type : {}({})" , observer . getClass ( ) . getCanonicalName ( ) , observer , t ) ; } }
public void test() { try { Runtime . main ( new String [ ] code_block = "" ; ) ; WebGui webgui = ( WebGui ) Runtime . create ( "webgui" , "WebGui" ) ; webgui . autoStartBrowser ( false ) ; webgui . setPort ( 8888 ) ; webgui . startService ( ) ; Runtime . start ( "python" , "Python" ) ; boolean done = true ; code_block = IfStatement ; MqttBroker broker = ( MqttBroker ) Runtime . start ( "broker" , "MqttBroker" ) ; broker . listen ( ) ; Mqtt mqtt01 = ( Mqtt ) Runtime . start ( "mqtt01" , "Mqtt" ) ; mqtt01 . connect ( "mqtt://localhost:1883" ) ; Runtime . start ( "neo" , "NeoPixel" ) ; Arduino arduino = ( Arduino ) Runtime . start ( "arduino" , "Arduino" ) ; arduino . connect ( "/dev/ttyACM0" ) ; code_block = ForStatement ; Platform . setVirtual ( true ) ; Servo pan = ( Servo ) Runtime . start ( "pan" , "Servo" ) ; Servo tilt = ( Servo ) Runtime . start ( "tilt" , "Servo" ) ; pan . setPin ( 3 ) ; pan . setMinMax ( 30.0 , 70.0 ) ; tilt . setPin ( "D4" ) ; arduino . attach ( pan ) ; arduino . attach ( tilt ) ; log . info ( "leaving main" ) ; } catch ( Exception e ) { log . error ( "main threw" , e ) ; } }
public void test() { try { Runtime . main ( new String [ ] code_block = "" ; ) ; WebGui webgui = ( WebGui ) Runtime . create ( "webgui" , "WebGui" ) ; webgui . autoStartBrowser ( false ) ; webgui . setPort ( 8888 ) ; webgui . startService ( ) ; Runtime . start ( "python" , "Python" ) ; boolean done = true ; code_block = IfStatement ; MqttBroker broker = ( MqttBroker ) Runtime . start ( "broker" , "MqttBroker" ) ; broker . listen ( ) ; Mqtt mqtt01 = ( Mqtt ) Runtime . start ( "mqtt01" , "Mqtt" ) ; mqtt01 . connect ( "mqtt://localhost:1883" ) ; Runtime . start ( "neo" , "NeoPixel" ) ; Arduino arduino = ( Arduino ) Runtime . start ( "arduino" , "Arduino" ) ; arduino . connect ( "/dev/ttyACM0" ) ; code_block = ForStatement ; Platform . setVirtual ( true ) ; Servo pan = ( Servo ) Runtime . start ( "pan" , "Servo" ) ; Servo tilt = ( Servo ) Runtime . start ( "tilt" , "Servo" ) ; pan . setPin ( 3 ) ; pan . setMinMax ( 30.0 , 70.0 ) ; tilt . setPin ( "D4" ) ; arduino . attach ( pan ) ; arduino . attach ( tilt ) ; log . info ( "leaving main" ) ; } catch ( Exception e ) { log . error ( "main threw" , e ) ; } }
public void test() { try { Message [ ] messages = monitoringStrategy . monitor ( folder ) ; code_block = ForStatement ; } catch ( FolderClosedException ex ) { logger . debug ( "Folder closed, reopening" ) ; code_block = IfStatement ; } catch ( MessagingException ex ) { logger . warn ( ex ) ; } }
public void test() { try { openFolder ( ) ; code_block = WhileStatement ; } catch ( InterruptedException ex ) { Thread . currentThread ( ) . interrupt ( ) ; } catch ( MessagingException ex ) { logger . error ( ex ) ; } }
public void onBecomeOfflineFromSlave ( Message message , NotificationContext context ) { DummyProcess . sleep ( _transDelay ) ; logger . info ( "DummyStateModel.onBecomeOfflineFromSlave()" ) ; }
@ Override public synchronized void mark ( final int readlimit ) { LOG . debug ( "mark at {}" , position ( ) ) ; byteBuffer . mark ( ) ; }
public void test() { try { SecretKey rootKey = KeyProviderFactory . requiresRootKey ( repositoryEncryptionConfiguration . getKeyProviderImplementation ( ) ) ? CryptoUtils . getRootKey ( ) : null ; return buildKeyProviderFromConfig ( rootKey , repositoryEncryptionConfiguration ) ; } catch ( KeyManagementException e ) { String msg = "Encountered an error building the key provider" ; logger . error ( msg , e ) ; throw new IOException ( msg , e ) ; } }
public static Long addMedicalData ( String providerNo , MyOscarLoggedInInfo myOscarLoggedInInfo , MedicalDataTransfer4 medicalDataTransfer , String oscarDataType , Object localOscarObjectId , boolean completed , boolean active ) throws NotAuthorisedException_Exception , UnsupportedEncodingException_Exception , InvalidRequestException_Exception { Long resultId = MedicalDataManager . addMedicalData ( myOscarLoggedInInfo , medicalDataTransfer , completed , active ) ; logger . debug ( "addMedicalData success : resultId=" + resultId ) ; addSendRemoteDataLog ( providerNo , oscarDataType , localOscarObjectId , medicalDataTransfer . getData ( ) ) ; return ( resultId ) ; }
@ Override public boolean isSatisified ( ) throws Exception { LOG . debug ( "Current Backup Count = " + failoverTransport . getCurrentBackups ( ) ) ; return failoverTransport . getCurrentBackups ( ) == 1 ; }
public void test() { try { super . start ( ) ; super . destroy ( ) ; this . with ( LAST_UPDATED_ORDERBY . clone ( ) ) . direction ( DIRECTION . DESC ) . with ( POSTED_TIME_ORDERBY . clone ( ) ) . direction ( DIRECTION . DESC ) ; } catch ( Exception ex ) { LOG . warn ( ex ) ; } }
@ Override public void run ( ) { WeldRequestScopeAdapter . getInstance ( ) . activateContext ( ) ; WeldSessionScopeAdapter . getInstance ( ) . activateContext ( httpSession ) ; WeldConversationScopeAdapter . getInstance ( ) . activateContext ( conversationState ) ; Assert . assertFalse ( conversation . isTransient ( ) ) ; Assert . assertTrue ( conversationState . isLongRunning ( ) ) ; final ConversationScopeBean conversationScopeBean = getInstance ( ConversationScopeBean . class ) ; Assert . assertEquals ( "wrong state" , conversationState . getConversationId ( ) , conversationScopeBean . getId ( ) ) ; WeldConversationScopeAdapter . getInstance ( ) . deactivateContext ( ) ; WeldSessionScopeAdapter . getInstance ( ) . deactivateContext ( ) ; WeldRequestScopeAdapter . getInstance ( ) . deactivateContext ( ) ; final int runner = doneRunnerCount . incrementAndGet ( ) ; LOGGER . info ( "conversation {} runner: {} done" , conversationState . getConversationId ( ) , runner ) ; }
public void test() { try { mongoPort = NetworkUtil . getAvailableLocalPort ( ) ; RuntimeConfig runtimeConfig = Defaults . runtimeConfigFor ( Command . MongoD , LOGGER ) . processOutput ( ProcessOutput . getDefaultInstanceSilent ( ) ) . build ( ) ; MongodConfig mongodConfig = MongodConfig . builder ( ) . version ( Version . V4_0_12 ) . net ( new Net ( DEFAULT_MONGO_HOST , mongoPort , Network . localhostIsIPv6 ( ) ) ) . build ( ) ; MongodStarter runtime = MongodStarter . getInstance ( runtimeConfig ) ; mongodExecutable = runtime . prepare ( mongodConfig ) ; mongodExecutable . start ( ) ; } catch ( IOException e ) { LOGGER . error ( "Exception when starting embedded mongo" , e ) ; } }
public void test() { try { exec . setProgress ( "Starting command" ) ; Runtime rt = Runtime . getRuntime ( ) ; LOGGER . debug ( "Launching command: '" + getCmdString ( ) + "'" ) ; exec . setProgress ( "External command is running..." ) ; final Process proc ; code_block = IfStatement ; final MutableBoolean procDone = new MutableBoolean ( false ) ; ThreadUtils . threadWithContext ( new CheckCanceledRunnable ( proc , procDone , exec ) ) . start ( ) ; Thread stdErrThread = ThreadUtils . threadWithContext ( new StdErrCatchRunnable ( proc , exec , m_extErrout ) ) ; stdErrThread . setName ( "ExtTool StdErr collector" ) ; stdErrThread . start ( ) ; Thread stdOutThread = ThreadUtils . threadWithContext ( new StdOutCatchRunnable ( proc , exec , m_extOutput ) ) ; stdOutThread . setName ( "ExtTool StdOut collector" ) ; stdOutThread . start ( ) ; exitVal = proc . waitFor ( ) ; synchronized ( procDone ) code_block = "" ; exec . checkCanceled ( ) ; exec . setProgress ( "External command done." ) ; String message = "External commands terminated with exit code: " + exitVal ; code_block = IfStatement ; } catch ( InterruptedException ie ) { throw ie ; } catch ( Exception e ) { LOGGER . error ( "Execution failed (with exception): " + e . getMessage ( ) , e ) ; throw e ; } catch ( Throwable t ) { LOGGER . fatal ( "Execution failed (with error): " + t . getMessage ( ) , t ) ; throw new Exception ( t ) ; } }
public void test() { if ( exitVal == 0 ) { LOGGER . debug ( message ) ; } else { LOGGER . info ( message ) ; } }
public void test() { if ( exitVal == 0 ) { LOGGER . debug ( message ) ; } else { LOGGER . info ( message ) ; } }
public void test() { try { exec . setProgress ( "Starting command" ) ; Runtime rt = Runtime . getRuntime ( ) ; LOGGER . debug ( "Launching command: '" + getCmdString ( ) + "'" ) ; exec . setProgress ( "External command is running..." ) ; final Process proc ; code_block = IfStatement ; final MutableBoolean procDone = new MutableBoolean ( false ) ; ThreadUtils . threadWithContext ( new CheckCanceledRunnable ( proc , procDone , exec ) ) . start ( ) ; Thread stdErrThread = ThreadUtils . threadWithContext ( new StdErrCatchRunnable ( proc , exec , m_extErrout ) ) ; stdErrThread . setName ( "ExtTool StdErr collector" ) ; stdErrThread . start ( ) ; Thread stdOutThread = ThreadUtils . threadWithContext ( new StdOutCatchRunnable ( proc , exec , m_extOutput ) ) ; stdOutThread . setName ( "ExtTool StdOut collector" ) ; stdOutThread . start ( ) ; exitVal = proc . waitFor ( ) ; synchronized ( procDone ) code_block = "" ; exec . checkCanceled ( ) ; exec . setProgress ( "External command done." ) ; String message = "External commands terminated with exit code: " + exitVal ; code_block = IfStatement ; } catch ( InterruptedException ie ) { throw ie ; } catch ( Exception e ) { LOGGER . error ( "Execution failed (with exception): " + e . getMessage ( ) , e ) ; throw e ; } catch ( Throwable t ) { LOGGER . fatal ( "Execution failed (with error): " + t . getMessage ( ) , t ) ; throw new Exception ( t ) ; } }
public void test() { try { exec . setProgress ( "Starting command" ) ; Runtime rt = Runtime . getRuntime ( ) ; LOGGER . debug ( "Launching command: '" + getCmdString ( ) + "'" ) ; exec . setProgress ( "External command is running..." ) ; final Process proc ; code_block = IfStatement ; final MutableBoolean procDone = new MutableBoolean ( false ) ; ThreadUtils . threadWithContext ( new CheckCanceledRunnable ( proc , procDone , exec ) ) . start ( ) ; Thread stdErrThread = ThreadUtils . threadWithContext ( new StdErrCatchRunnable ( proc , exec , m_extErrout ) ) ; stdErrThread . setName ( "ExtTool StdErr collector" ) ; stdErrThread . start ( ) ; Thread stdOutThread = ThreadUtils . threadWithContext ( new StdOutCatchRunnable ( proc , exec , m_extOutput ) ) ; stdOutThread . setName ( "ExtTool StdOut collector" ) ; stdOutThread . start ( ) ; exitVal = proc . waitFor ( ) ; synchronized ( procDone ) code_block = "" ; exec . checkCanceled ( ) ; exec . setProgress ( "External command done." ) ; String message = "External commands terminated with exit code: " + exitVal ; code_block = IfStatement ; } catch ( InterruptedException ie ) { throw ie ; } catch ( Exception e ) { LOGGER . error ( "Execution failed (with exception): " + e . getMessage ( ) , e ) ; throw e ; } catch ( Throwable t ) { LOGGER . fatal ( "Execution failed (with error): " + t . getMessage ( ) , t ) ; throw new Exception ( t ) ; } }
public void test() { -> { logger . debug ( "updating device [tenant: {}, device-id: {}]" , tenantId . result ( ) , deviceId . result ( ) ) ; final Optional < String > resourceVersion = Optional . ofNullable ( ctx . get ( KEY_RESOURCE_VERSION ) ) ; return getService ( ) . updateDevice ( tenantId . result ( ) , deviceId . result ( ) , device . result ( ) , resourceVersion , span ) ; } }
public void test() { if ( JBoss6VFS . valid == Boolean . TRUE ) { log . debug ( "JBoss 6 VFS API is not available in this environment." ) ; JBoss6VFS . valid = Boolean . FALSE ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . trace ( "AIO on error issued. Error(code: " + errno + " msg: " + message + ")" ) ; } }
public void test() { try { onException . accept ( e ) ; } catch ( Exception ex ) { logger . warn ( "Exception consumer {} in handle {} threw an exception" , onException , this , ex ) ; } }
private void throwValidationError ( ErrorMessage error ) { log . warn ( String . format ( "Validation result query failed, code: '%s', message: '%s'" , error . getErrorCode ( ) , error . getMessage ( ) ) ) ; throw new IllegalQueryException ( error ) ; }
public void test() { if ( allowed ) { httpExchange . putAttachment ( PRINCIPAL_NAME_KEY , ( ( JwtAuthenticationToken ) a ) . getName ( ) ) ; httpExchange . setStatusCode ( StatusCodes . OK ) ; return ; } else { LOG . debug ( "Authenticated principal {} doesn't have authority to access resource." , ( ( JwtAuthenticationToken ) a ) . getName ( ) ) ; } }
public void test() { if ( a instanceof JwtAuthenticationToken ) { LOG . debug ( "Authentication token is present." ) ; boolean allowed = false ; Collection < GrantedAuthority > grantedAuthorities = ( ( JwtAuthenticationToken ) a ) . getAuthorities ( ) ; code_block = ForStatement ; code_block = IfStatement ; } else { LOG . warn ( "Authentication token is not present. Access is FORBIDDEN." ) ; } }
public void test() { try { checksumFile = new File ( new URI ( checksumStr ) ) ; code_block = IfStatement ; } catch ( URISyntaxException e ) { LOG . error ( "Failed to delete token crc file." , e ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "do periodic commit; offsets: [{}]" , HonoKafkaConsumerHelper . getOffsetsDebugString ( offsets ) ) ; } }
public void test() { if ( error != null ) { log . info ( "periodic commit failed: {}" , error . toString ( ) ) ; } else { log . trace ( "periodic commit succeeded" ) ; setCommittedOffsets ( committedOffsets ) ; } }
public void test() { if ( error != null ) { log . info ( "periodic commit failed: {}" , error . toString ( ) ) ; } else { log . trace ( "periodic commit succeeded" ) ; setCommittedOffsets ( committedOffsets ) ; } }
public void test() { if ( ! offsets . isEmpty ( ) ) { code_block = IfStatement ; final Consumer < String , Buffer > wrappedConsumer = getKafkaConsumer ( ) . asStream ( ) . unwrap ( ) ; wrappedConsumer . commitAsync ( offsets , ( committedOffsets , error ) code_block = LoopStatement ; ) ; } else { log . trace ( "skip periodic commit - no offsets to commit" ) ; } }
public void test() { -> { log . info ( "Refreshing system access control from %s" , config . getConfigFile ( ) ) ; return new FileBasedAccessControl ( config ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "No node found in the affinity map. It is null. We select new node: " + node ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "The assigned node in the affinity map is still alive: " + node ) ; } }
public void test() { if ( AppConfig . get ( ) . isMaintenanceMode ( ) ) { LOG . debug ( "This service is maintenance mode now." ) ; return ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "finish NotifyMailBat [result]" + result . getResultCode ( ) ) ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( result . getStdout ( ) ) ; } }
public void test() { try { JobResult result = job . execute ( ) ; code_block = IfStatement ; code_block = IfStatement ; } catch ( Exception e ) { LOG . error ( "Failed to Notify" , e ) ; } }
public void test() { try { code_block = IfStatement ; new Thread ( ( ) code_block = LoopStatement ; ) . start ( ) ; } catch ( InterruptedException e ) { log . warn ( "Unexpected exception while map clean-up" , e ) ; Thread . currentThread ( ) . interrupt ( ) ; } }
public void test() { try { FedizResponse federationResponse = processSigninRequest ( responseToken , request , response ) ; code_block = IfStatement ; LOG . debug ( "RSTR validated successfully" ) ; return createPrincipal ( request , response , federationResponse ) ; } catch ( ProcessingException e ) { LOG . error ( "Federation processing failed: " + e . getMessage ( ) ) ; } }
@ Bean ( name = "wsDistributionAutomationInboundDomainResponsesMessageListenerContainer" ) public DefaultMessageListenerContainer messageListenerContainer ( @ Qualifier ( "wsDistributionAutomationInboundDomainResponsesMessageListener" ) final MessageListener messageListener ) { LOGGER . info ( "Initializing wsDistributionAutomationInboundDomainResponsesMessageListenerContainer bean." ) ; return this . jmsConfigurationFactory . initMessageListenerContainer ( messageListener ) ; }
@ Override public Object visit ( PropertyIsEqualTo filter , Object data ) { LOGGER . debug ( "ENTERING: PropertyIsEqualTo filter" ) ; ExpressionValueVisitor expressionVisitor = new ExpressionValueVisitor ( ) ; String propertyName = ( String ) filter . getExpression1 ( ) . accept ( expressionVisitor , data ) ; Object literalValue = filter . getExpression2 ( ) . accept ( expressionVisitor , data ) ; String mappedPropertyName = getMappedPropertyName ( propertyName ) ; return new SolrQuery ( mappedPropertyName + ":" + QUOTE + escapeSpecialCharacters ( literalValue . toString ( ) ) + QUOTE ) ; }
public void test() { try { stat = conn . prepareStatement ( DELETE_WORK_GUI ) ; stat . setString ( 1 , typeCode ) ; stat . setString ( 2 , stepCode ) ; stat . executeUpdate ( ) ; } catch ( Throwable t ) { _logger . error ( "Error deleting work gui - typeCode {} - stepCode {}" , typeCode , stepCode , t ) ; throw new RuntimeException ( "Error deleting work gui - typeCode " + typeCode + " - stepCode " + stepCode , t ) ; } finally { closeDaoResources ( null , stat ) ; } }
@ GetMapping ( CommonConstants . PATH_ID ) public LogbookOperationDto getAllPaginated ( @ PathVariable ( "id" ) String id ) { LOGGER . debug ( "get Ingest Entities for id={} " , id ) ; ParameterChecker . checkParameter ( "The Identifier is a mandatory parameter: " , id ) ; final VitamContext vitamContext = securityService . buildVitamContext ( securityService . getTenantIdentifier ( ) ) ; return ingestInternalService . getOne ( vitamContext , id ) ; }
public void test() { try { RandomAccessFile raf = myInsertToRAFMap . get ( Integer . valueOf ( dcr . getInsertNum ( ) ) ) ; code_block = IfStatement ; code_block = IfStatement ; } catch ( IOException | BadPaddingException | IllegalBlockSizeException | ShortBufferException e ) { LOG . error ( e ) ; } finally { myRetrieveLock . unlock ( ) ; } }
public static int checkIfFeedCoordExist ( AbstractEntityHelper helper , String feedName , String coordType ) throws OozieClientException { LOGGER . info ( "feedName: " + feedName ) ; int numberOfCoord = 0 ; final OozieClient oozieClient = helper . getOozieClient ( ) ; code_block = IfStatement ; List < String > bundleIds = OozieUtil . getBundles ( oozieClient , feedName , EntityType . FEED ) ; LOGGER . info ( "bundleIds: " + bundleIds ) ; code_block = ForStatement ; return numberOfCoord ; }
public static int checkIfFeedCoordExist ( AbstractEntityHelper helper , String feedName , String coordType ) throws OozieClientException { LOGGER . info ( "feedName: " + feedName ) ; int numberOfCoord = 0 ; final OozieClient oozieClient = helper . getOozieClient ( ) ; code_block = IfStatement ; List < String > bundleIds = OozieUtil . getBundles ( oozieClient , feedName , EntityType . FEED ) ; LOGGER . info ( "bundleIds: " + bundleIds ) ; code_block = ForStatement ; return numberOfCoord ; }
public void test() { for ( String bundleId : bundleIds ) { LOGGER . info ( "bundleId: " + bundleId ) ; OozieUtil . waitForCoordinatorJobCreation ( oozieClient , bundleId ) ; List < CoordinatorJob > coords = OozieUtil . getBundleCoordinators ( oozieClient , bundleId ) ; LOGGER . info ( "coords: " + coords ) ; code_block = ForStatement ; } }
public void test() { for ( String bundleId : bundleIds ) { LOGGER . info ( "bundleId: " + bundleId ) ; OozieUtil . waitForCoordinatorJobCreation ( oozieClient , bundleId ) ; List < CoordinatorJob > coords = OozieUtil . getBundleCoordinators ( oozieClient , bundleId ) ; LOGGER . info ( "coords: " + coords ) ; code_block = ForStatement ; } }
public void test() { try { zk = ZooKeeperStorage . zkOpen ( appConf ) ; nodes = getChildList ( zk ) ; code_block = ForStatement ; zk . close ( ) ; } catch ( Exception e ) { LOG . error ( "Cleanup cycle failed: " + e . getMessage ( ) ) ; } finally { if ( zk != null ) zk . close ( ) ; } }
public void test() { try { code_block = TryStatement ;  long sleepMillis = ( long ) ( Math . random ( ) * interval ) ; LOG . info ( "Next execution: " + new Date ( new Date ( ) . getTime ( ) + sleepMillis ) ) ; Thread . sleep ( sleepMillis ) ; } catch ( Exception e ) { isRunning = false ; LOG . error ( "Cleanup failed: " + e . getMessage ( ) , e ) ; } }
public void test() { try { code_block = TryStatement ;  long sleepMillis = ( long ) ( Math . random ( ) * interval ) ; LOG . info ( "Next execution: " + new Date ( new Date ( ) . getTime ( ) + sleepMillis ) ) ; Thread . sleep ( sleepMillis ) ; } catch ( Exception e ) { isRunning = false ; LOG . error ( "Cleanup failed: " + e . getMessage ( ) , e ) ; } }
@ Override public void onSubscribe ( Subscription s ) { subscription = s ; log . trace ( "Calling onSubscribe of subscriber" ) ; subscriber . onSubscribe ( this ) ; }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( msg ) ; } }
@ Override public void nodeFinished ( CnATreeElement node , int depth ) { LOG . info ( "finished node: " + node + " depth: " + depth ) ; }
@ Override public void exists ( ) { logger . debug ( String . format ( "Skipped '%s' because idempotent and exists in the machine." , id ) ) ; status = Status . EXIST ; signalChildren ( ) ; }
@ Override public COREEnvelopeRealTimeResponse realTimeTransaction ( COREEnvelopeRealTimeRequest msg , AssertionType assertion ) { LOG . trace ( "Using NoOp Implementation for Adapter CORE X12 Doc Submission Service" ) ; return new COREEnvelopeRealTimeResponse ( ) ; }
public void test() { try { readOperation = getReadOperation ( ) ; } catch ( Exception exn ) { readOperation = null ; LOG . info ( "Unable to get read operation." , exn ) ; return new NullProgressTracker ( ) ; } }
public void test() { try { grpcWriteOperation = Iterables . getOnlyElement ( Iterables . filter ( operations , RemoteGrpcPortWriteOperation . class ) ) ; bundleProcessOperation = Iterables . getOnlyElement ( Iterables . filter ( operations , RegisterAndProcessBundleOperation . class ) ) ; } catch ( IllegalArgumentException | NoSuchElementException exn ) { grpcWriteOperation = null ; bundleProcessOperation = null ; LOG . debug ( "Does not have exactly one grpcWRite and bundleProcess operation." , exn ) ; } }
public void test() { try { cQuery . registerCq ( clientProxyId , ccn , cqState ) ; code_block = IfStatement ; } catch ( CqException cqe ) { logger . info ( "Exception while registering CQ on server. CqName : {}" , cQuery . getName ( ) ) ; throw cqe ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Successfully created CQ on the server. CqName : {}" , cQuery . getName ( ) ) ; } }
public void test() { try ( FileInputStream fis = new FileInputStream ( file ) ) { return parseSuppressionRules ( fis ) ; } catch ( SAXException | IOException ex ) { LOGGER . debug ( "" , ex ) ; throw new SuppressionParseException ( ex ) ; } }
public void test() { if ( _log . isInfoEnabled ( ) ) { _log . info ( StringBundler . concat ( "Ignoring group " , newValue , " because it " , "cannot be converted to scope" ) ) ; } }
public void test() { if ( _log . isInfoEnabled ( ) ) { _log . info ( StringBundler . concat ( "Ignoring scope " , newValue , " because the " , "referenced group was not found" ) , noSuchGroupException ) ; } }
public void test() { if ( _log . isInfoEnabled ( ) ) { _log . info ( StringBundler . concat ( "Ignoring scope " , newValue , " because the " , "referenced layout was not found" ) , noSuchLayoutException ) ; } }
public void test() { if ( _log . isInfoEnabled ( ) ) { _log . info ( StringBundler . concat ( "Ignoring scope " , newValue , " because the " , "referenced parent group no longer allows sharing " , "content with child sites" ) , principalException ) ; } }
public void test() { try { ri . activate ( ) ; return ri . getComponent ( ) ; } catch ( Exception e ) { log . error ( "Failed to get service: " + serviceClass + ", " + e . getMessage ( ) ) ; } }
public void test() { if ( ! serviceClass . getSimpleName ( ) . equals ( "TypeProvider" ) ) { log . debug ( "The component exposing the service " + serviceClass + " is not resolved" ) ; } }
public void test() { if ( log . isInfoEnabled ( ) ) { log . info ( col ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( IOException | RemoteControllerException e ) { logger . warn ( "{}: cannot send command" , this . getClass ( ) . getSimpleName ( ) , e ) ; } }
public void testExampleFunctionsYamlMatch ( ) throws IOException { Reader input = Streams . reader ( new ResourceUtils ( this ) . getResourceFromUrl ( "example-with-function.yaml" ) ) ; DeploymentPlan plan = platform . pdp ( ) . parseDeploymentPlan ( input ) ; log . info ( "DP is:\n" + plan . toString ( ) ) ; Map < ? , ? > cfg1 = ( Map < ? , ? > ) plan . getServices ( ) . get ( 0 ) . getCustomAttributes ( ) . get ( BrooklynCampReservedKeys . BROOKLYN_CONFIG ) ; Map < ? , ? > cfg = MutableMap . copyOf ( cfg1 ) ; Assert . assertEquals ( cfg . remove ( "literalValue1" ) , "$brooklyn: is a fun place" ) ; Assert . assertEquals ( cfg . remove ( "literalValue2" ) , "$brooklyn: is a fun place" ) ; Assert . assertEquals ( cfg . remove ( "literalValue3" ) , "$brooklyn: is a fun place" ) ; Assert . assertEquals ( cfg . remove ( "literalValue4" ) , "$brooklyn: is a fun place" ) ; Assert . assertEquals ( cfg . remove ( "$brooklyn:1" ) , "key to the city" ) ; Assert . assertTrue ( cfg . isEmpty ( ) , "" + cfg ) ; Assert . assertEquals ( plan . getName ( ) , "example-with-function" ) ; Assert . assertEquals ( plan . getCustomAttributes ( ) . get ( "location" ) , "localhost" ) ; AssemblyTemplate at = platform . pdp ( ) . registerDeploymentPlan ( plan ) ; Assert . assertEquals ( at . getName ( ) , "example-with-function" ) ; Assert . assertEquals ( at . getCustomAttributes ( ) . get ( "location" ) , "localhost" ) ; PlatformComponentTemplate pct = at . getPlatformComponentTemplates ( ) . links ( ) . iterator ( ) . next ( ) . resolve ( ) ; Object cfg2 = pct . getCustomAttributes ( ) . get ( BrooklynCampReservedKeys . BROOKLYN_CONFIG ) ; Assert . assertEquals ( cfg2 , cfg1 ) ; }
public void test() { try { JSONObject form = json . getJSONObject ( "form" ) ; putPropertyArrayToObject ( form ) ; Object fields = form . get ( "field" ) ; code_block = IfStatement ; } catch ( JSONException e ) { logger . debug ( "exception while formatting :: {}" , e . getMessage ( ) , e ) ; } }
public void test() { try { ProcessorFactory . getProcessor ( thing ) . processInfoUpdate ( this , station , name , location ) ; } catch ( ProcessorNotFoundException e ) { logger . debug ( "Unable to process info event: {}" , e . getMessage ( ) ) ; } }
public void test() { if ( _log . isInfoEnabled ( ) ) { _log . info ( "Deleting background task " + backgroundTask . toString ( ) ) ; } }
public void test() { try { return new AdapterRestClient ( this . props , this . getUri ( "/" + aaiVnfId + VF_MODULES + aaiVfModuleId + "/rollback" ) . build ( ) ) . delete ( req , RollbackVfModuleResponse . class ) ; } catch ( InternalServerErrorException e ) { logger . error ( "InternalServerErrorException in rollbackVfModule" , e ) ; throw new VnfAdapterClientException ( e . getMessage ( ) ) ; } }
public void test() { if ( snapshotObj == null ) { s_logger . debug ( "Can't find snapshot; deleting it in DB" ) ; snapshotDao . remove ( snapshotId ) ; return true ; } }
public void test() { try { snapshotObj . processEvent ( Snapshot . Event . OperationFailed ) ; } catch ( NoTransitionException e1 ) { s_logger . debug ( "Failed to change snapshot state: " + e1 . toString ( ) ) ; } }
public void test() { try { snapshotObj . processEvent ( Snapshot . Event . DestroyRequested ) ; List < VolumeDetailVO > volumesFromSnapshot = volumeDetailsDaoImpl . findDetails ( "SNAPSHOT_ID" , String . valueOf ( snapshotId ) , null ) ; code_block = IfStatement ; } catch ( NoTransitionException e ) { s_logger . debug ( "Failed to set the state to destroying: " , e ) ; return false ; } }
public void test() { try { snapshotSvr . deleteSnapshot ( snapshotObj ) ; snapshotObj . processEvent ( Snapshot . Event . OperationSucceeded ) ; UsageEventUtils . publishUsageEvent ( EventTypes . EVENT_SNAPSHOT_OFF_PRIMARY , snapshotObj . getAccountId ( ) , snapshotObj . getDataCenterId ( ) , snapshotId , snapshotObj . getName ( ) , null , null , 0L , snapshotObj . getClass ( ) . getName ( ) , snapshotObj . getUuid ( ) ) ; } catch ( Exception e ) { s_logger . debug ( "Failed to delete snapshot: " , e ) ; code_block = TryStatement ;  return false ; } }
public void test() { try { snapshotObj . processEvent ( Snapshot . Event . OperationFailed ) ; } catch ( NoTransitionException e1 ) { s_logger . debug ( "Failed to change snapshot state: " + e . toString ( ) ) ; } }
public void test() { try { initConcreteDataGrid ( ( JvstmDataGridConfig ) jvstmConfig ) ; } catch ( Exception e ) { logger . error ( "Failed to initialize data grid: {}" , e ) ; throw new RuntimeException ( e ) ; } }
private void initFile ( File writeOnlyFile ) throws IOException { code_block = IfStatement ; code_block = IfStatement ; writeOnlyFile . createNewFile ( ) ; LOG . info ( "Spilling to file location " + writeOnlyFile . getAbsolutePath ( ) + " in host (" + InetAddress . getLocalHost ( ) . getHostAddress ( ) + ") with hostname (" + InetAddress . getLocalHost ( ) . getHostName ( ) + ")" ) ; writeOnlyFile . deleteOnExit ( ) ; addShutDownHook ( ) ; }
public synchronized void removePendingHpcJob ( final HpcJobInfo hpcJobInfo ) { HpcAccount hpcAccount = hpcJobInfo . getHpcAccount ( ) ; LOGGER . debug ( "removedPendingHpcJob: connection: " + hpcAccount . getConnectionName ( ) ) ; LOGGER . debug ( "removedPendingHpcJob: algorithm: " + hpcJobInfo . getAlgoId ( ) ) ; LOGGER . debug ( "removedPendingHpcJob: status: " + hpcJobInfo . getStatus ( ) ) ; LOGGER . debug ( "removedPendingHpcJob: pid: " + hpcJobInfo . getPid ( ) ) ; Set < HpcJobInfo > hpcJobInfos = pendingHpcJobInfoMap . get ( hpcAccount ) ; code_block = IfStatement ; }
public synchronized void removePendingHpcJob ( final HpcJobInfo hpcJobInfo ) { HpcAccount hpcAccount = hpcJobInfo . getHpcAccount ( ) ; LOGGER . debug ( "removedPendingHpcJob: connection: " + hpcAccount . getConnectionName ( ) ) ; LOGGER . debug ( "removedPendingHpcJob: algorithm: " + hpcJobInfo . getAlgoId ( ) ) ; LOGGER . debug ( "removedPendingHpcJob: status: " + hpcJobInfo . getStatus ( ) ) ; LOGGER . debug ( "removedPendingHpcJob: pid: " + hpcJobInfo . getPid ( ) ) ; Set < HpcJobInfo > hpcJobInfos = pendingHpcJobInfoMap . get ( hpcAccount ) ; code_block = IfStatement ; }
public synchronized void removePendingHpcJob ( final HpcJobInfo hpcJobInfo ) { HpcAccount hpcAccount = hpcJobInfo . getHpcAccount ( ) ; LOGGER . debug ( "removedPendingHpcJob: connection: " + hpcAccount . getConnectionName ( ) ) ; LOGGER . debug ( "removedPendingHpcJob: algorithm: " + hpcJobInfo . getAlgoId ( ) ) ; LOGGER . debug ( "removedPendingHpcJob: status: " + hpcJobInfo . getStatus ( ) ) ; LOGGER . debug ( "removedPendingHpcJob: pid: " + hpcJobInfo . getPid ( ) ) ; Set < HpcJobInfo > hpcJobInfos = pendingHpcJobInfoMap . get ( hpcAccount ) ; code_block = IfStatement ; }
public synchronized void removePendingHpcJob ( final HpcJobInfo hpcJobInfo ) { HpcAccount hpcAccount = hpcJobInfo . getHpcAccount ( ) ; LOGGER . debug ( "removedPendingHpcJob: connection: " + hpcAccount . getConnectionName ( ) ) ; LOGGER . debug ( "removedPendingHpcJob: algorithm: " + hpcJobInfo . getAlgoId ( ) ) ; LOGGER . debug ( "removedPendingHpcJob: status: " + hpcJobInfo . getStatus ( ) ) ; LOGGER . debug ( "removedPendingHpcJob: pid: " + hpcJobInfo . getPid ( ) ) ; Set < HpcJobInfo > hpcJobInfos = pendingHpcJobInfoMap . get ( hpcAccount ) ; code_block = IfStatement ; }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "monitoring from " + req . getRemoteAddr ( ) + ", request=" + req . getRequestURI ( ) + ( req . getQueryString ( ) != null ? '?' + req . getQueryString ( ) : "" ) + ", application=" + application + " in " + ( System . currentTimeMillis ( ) - start ) + "ms" ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( StringUtils . replacePrms ( "Waiting for unlocked ({0}) key={1}, contextInfo={2}..." , Thread . currentThread ( ) . getName ( ) , getterId . get ( bean ) , contextInfo ) ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( StringUtils . replacePrms ( "Waiting ended ({0}) key={1}, contextInfo={2}..." , Thread . currentThread ( ) . getName ( ) , getterId . get ( bean ) , contextInfo ) ) ; } }
public void test() { try { executor . submit ( new RunCommand ( successCommand , exchange ) ) . get ( ) ; } catch ( Exception e ) { LOG . error ( "Could not run completion of exchange {}" , exchange , e ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "HeliosVentilation: Read from serial port: {}" , String . format ( "%02x %02x %02x %02x" , frame [ 1 ] , frame [ 2 ] , frame [ 3 ] , frame [ 4 ] ) ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "HeliosVentilation: Read frame with not matching checksum from serial port: {}" , String . format ( "%02x %02x %02x %02x %02x %02x (expected %02x)" , frame [ 0 ] , frame [ 1 ] , frame [ 2 ] , frame [ 3 ] , frame [ 4 ] , frame [ 5 ] , sum ) ) ; } }
public void test() { try { do code_block = "" ; while ( in . available ( ) > 0 ) ; } catch ( IOException e1 ) { logger . debug ( "Error reading from serial port: {}" , e1 . getMessage ( ) , e1 ) ; } }
public void test() { try { ParameterizedType pt = ( ParameterizedType ) f . getGenericType ( ) ; return ( Type [ ] ) pt . getActualTypeArguments ( ) ; } catch ( Exception e ) { log . debug ( "Unable to retrieve generic type of field: " + f , e ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( field . getProperty ( ) + ": " + msg ) ; } }
public void test() { try { String orcid = emailManagerReadOnly . findOrcidIdByEmail ( username ) ; code_block = IfStatement ; } catch ( javax . persistence . NoResultException nre ) { LOGGER . error ( "User " + username + " was not found" ) ; } catch ( Exception e ) { LOGGER . error ( "Error finding user " + username , e ) ; } }
public void test() { try { String orcid = emailManagerReadOnly . findOrcidIdByEmail ( username ) ; code_block = IfStatement ; } catch ( javax . persistence . NoResultException nre ) { LOGGER . error ( "User " + username + " was not found" ) ; } catch ( Exception e ) { LOGGER . error ( "Error finding user " + username , e ) ; } }
public void test() { try { ApplicationContext appContext = WebApplicationContextUtils . getWebApplicationContext ( this . getServletContext ( ) ) ; testcaseService = appContext . getBean ( ITestCaseService . class ) ; applicationService = appContext . getBean ( IApplicationService . class ) ; PolicyFactory policy = Sanitizers . FORMATTING . and ( Sanitizers . LINKS ) ; String test = policy . sanitize ( httpServletRequest . getParameter ( "test" ) ) ; String testcase = policy . sanitize ( httpServletRequest . getParameter ( "testcase" ) ) ; JSONObject export = new JSONObject ( ) ; export . put ( "version" , Infos . getInstance ( ) . getProjectVersion ( ) ) ; export . put ( "user" , httpServletRequest . getUserPrincipal ( ) ) ; SimpleDateFormat formatter = new SimpleDateFormat ( "yyyy-MM-dd'T'HH:mm:ss.SSSXXX" ) ; export . put ( "date" , formatter . format ( new Date ( ) ) ) ; TestCase tcInfo = testcaseService . findTestCaseByKeyWithDependency ( test , testcase ) ; ObjectMapper mapper = new ObjectMapper ( ) ; JSONObject tcInfoJSON = new JSONObject ( mapper . writeValueAsString ( tcInfo ) ) ; tcInfoJSON . remove ( "bugs" ) ; tcInfoJSON . put ( "bugs" , tcInfo . getBugs ( ) ) ; tcInfoJSON . remove ( "conditionOptions" ) ; tcInfoJSON . put ( "conditionOptions" , tcInfo . getConditionOptions ( ) ) ; JSONArray tcJA = new JSONArray ( ) ; tcJA . add ( tcInfoJSON ) ; export . put ( "testcases" , tcJA ) ; Application appInfo = applicationService . convert ( applicationService . readByKey ( tcInfo . getApplication ( ) ) ) ; JSONObject app = new JSONObject ( mapper . writeValueAsString ( appInfo ) ) ; export . put ( "application" , app ) ; export . put ( "invariants" , new JSONArray ( ) ) ; export . put ( "applicationsObjects" , new JSONArray ( ) ) ; export . put ( "datalibs" , new JSONArray ( ) ) ; export . put ( "services" , new JSONArray ( ) ) ; export . put ( "libraryTestcases" , new JSONArray ( ) ) ; httpServletResponse . setContentType ( "application/json" ) ; httpServletResponse . setHeader ( "Content-Disposition" , "attachment; filename=\"" + getFilename ( test , testcase ) + ".json\"" ) ; httpServletResponse . getOutputStream ( ) . print ( export . toString ( 1 ) ) ; } catch ( CerberusException | JSONException ex ) { LOG . warn ( ex ) ; } }
public void electLeader ( ) { log . debug ( "Elect a new leader now." ) ; jobNodeStorage . executeInLeader ( LeaderNode . LATCH , new LeaderElectionExecutionCallback ( ) ) ; log . debug ( "Leader election completed." ) ; }
@ Override public List < IBaseResource > process ( List < ResourcePersistentId > theResourcePersistentId ) { String collect = theResourcePersistentId . stream ( ) . map ( pid -> pid . getId ( ) . toString ( ) ) . collect ( Collectors . joining ( "," ) ) ; ourLog . trace ( "Processing PIDs: {}" + collect ) ; IFhirResourceDao < ? > dao = myDaoRegistry . getResourceDao ( myResourceType ) ; Class < ? extends IBaseResource > resourceTypeClass = myContext . getResourceDefinition ( myResourceType ) . getImplementingClass ( ) ; ISearchBuilder sb = mySearchBuilderFactory . newSearchBuilder ( dao , myResourceType , resourceTypeClass ) ; List < IBaseResource > outgoing = new ArrayList < > ( ) ; sb . loadResourcesByPid ( theResourcePersistentId , Collections . emptyList ( ) , outgoing , false , null ) ; ourLog . trace ( "Loaded resources: {}" , outgoing . stream ( ) . map ( t -> t . getIdElement ( ) . getValue ( ) ) . collect ( Collectors . joining ( ", " ) ) ) ; return outgoing ; }
public void test() { if ( expiredObject != null ) { LOG . debug ( "Found ExpiredObject: " + expiredObject . getKey ( ) ) ; return expiredObject ; } else { LOG . error ( "ExpiredObject not found: " + key ) ; return expiredObject ; } }
public void test() { if ( expiredObject != null ) { LOG . debug ( "Found ExpiredObject: " + expiredObject . getKey ( ) ) ; return expiredObject ; } else { LOG . error ( "ExpiredObject not found: " + key ) ; return expiredObject ; } }
public void test() { try { conn = provider . getConnection ( ) ; conn . setAutoCommit ( false ) ; PreparedStatement query = conn . prepareStatement ( "select " + this . expiredObjectColumnName + ", value, type, iat, exp from expired_objects where " + this . expiredObjectColumnName + " = ?" ) ; query . setString ( 1 , key . trim ( ) ) ; ResultSet rs = query . executeQuery ( ) ; ExpiredObject expiredObject = null ; rs . next ( ) ; code_block = IfStatement ; query . close ( ) ; conn . commit ( ) ; code_block = IfStatement ; } catch ( Exception e ) { LOG . error ( "Failed to find ExpiredObject: " + key + ". Error: " + e . getMessage ( ) , e ) ; rollbackSilently ( conn ) ; return null ; } finally { IOUtils . closeSilently ( conn ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "[DSU] skip " + xpp . getName ( ) ) ; } }
public void test() { if ( null == entry . elementText ( "updated" ) ) { LOGGER . debug ( "Entry has no 'updated' element, won't process" ) ; return ; } }
public void test() { try { channelManager . deleteNodeItemById ( node , entry . elementText ( "id" ) ) ; } catch ( NodeStoreException e ) { LOGGER . error ( "Attempt to delete an item which didn't exist... its ok" ) ; } }
public void test() { try { code_block = IfStatement ; String inReplyTo = null ; Element reply ; code_block = IfStatement ; Date updatedDate = Conf . parseDate ( entry . elementText ( "updated" ) ) ; NodeItemImpl nodeItem = new NodeItemImpl ( node , GlobalItemIDImpl . toLocalId ( entry . elementText ( "id" ) ) , updatedDate , entry . asXML ( ) , inReplyTo ) ; code_block = TryStatement ;  channelManager . addNodeItem ( nodeItem ) ; } catch ( IllegalArgumentException e ) { LOGGER . error ( e ) ; e . printStackTrace ( ) ; return ; } }
public void test() { try { code_block = IfStatement ; } catch ( JsonSyntaxException e ) { logger . trace ( "Unable to parse sensor definition: {}" , json , e ) ; } }
public void test() { try { registryRoot = registryHelper . ensureDirectory ( PIPELINE_EXECUTOR_REGISTRY_PATH ) ; } catch ( Exception e ) { logger . error ( "An error was produced during " + VFSPipelineExecutorRegistry . class . getName ( ) + " directories initialization." , e ) ; } }
public void test() { if ( ! Arrays . equals ( mLatestLayout , layoutUpdate ) ) { LOG . debug ( "Received layout update for table {}: {}." , mTableURI , Bytes . toStringBinary ( layoutUpdate ) ) ; mLatestLayout = layoutUpdate ; mHandler . update ( layoutUpdate ) ; } }
public void test() { try { final byte [ ] layoutUpdate = mZKClient . getData ( mTableLayoutFile , mWatcher , mLayoutStat ) ; code_block = IfStatement ; } catch ( KeeperException . NoNodeException nne ) { LOG . info ( "Tracked table layout node for table {} has been removed. Tracking will cease." , mTableURI ) ; } catch ( KeeperException ke ) { LOG . error ( "Unrecoverable ZooKeeper error: {}" , ke . getMessage ( ) ) ; } }
public void test() { try { final byte [ ] layoutUpdate = mZKClient . getData ( mTableLayoutFile , mWatcher , mLayoutStat ) ; code_block = IfStatement ; } catch ( KeeperException . NoNodeException nne ) { LOG . info ( "Tracked table layout node for table {} has been removed. Tracking will cease." , mTableURI ) ; } catch ( KeeperException ke ) { LOG . error ( "Unrecoverable ZooKeeper error: {}" , ke . getMessage ( ) ) ; } }
public void test() { try { LOG . debug ( "Starting SNMP Trap producer on {}" , this . endpoint . getAddress ( ) ) ; code_block = IfStatement ; snmp = new Snmp ( transport ) ; LOG . debug ( "SnmpTrap: getting pdu from body" ) ; PDU trap = exchange . getIn ( ) . getBody ( PDU . class ) ; trap . setErrorIndex ( 0 ) ; trap . setErrorStatus ( 0 ) ; trap . setMaxRepetitions ( 0 ) ; code_block = IfStatement ; LOG . debug ( "SnmpTrap: sending" ) ; snmp . send ( trap , this . target ) ; LOG . debug ( "SnmpTrap: sent" ) ; } finally { code_block = TryStatement ;  code_block = TryStatement ;  } }
public void test() { try { LOG . debug ( "Starting SNMP Trap producer on {}" , this . endpoint . getAddress ( ) ) ; code_block = IfStatement ; snmp = new Snmp ( transport ) ; LOG . debug ( "SnmpTrap: getting pdu from body" ) ; PDU trap = exchange . getIn ( ) . getBody ( PDU . class ) ; trap . setErrorIndex ( 0 ) ; trap . setErrorStatus ( 0 ) ; trap . setMaxRepetitions ( 0 ) ; code_block = IfStatement ; LOG . debug ( "SnmpTrap: sending" ) ; snmp . send ( trap , this . target ) ; LOG . debug ( "SnmpTrap: sent" ) ; } finally { code_block = TryStatement ;  code_block = TryStatement ;  } }
public void test() { try { JcrWorkspaceFilter . saveFilter ( filter , defNode , autoSave ) ; } catch ( RepositoryException e ) { log . error ( "Error while saving filter." , e ) ; } }
public void test() { try { FileVersion fileVersion = fileEntry . getFileVersion ( ) ; code_block = TryStatement ;  } catch ( Exception exception ) { _log . error ( exception , exception ) ; return null ; } }
@ Test public void callMethodWithExistingErrorEnum ( ) { logger . info ( name . getMethodName ( ) + "" ) ; code_block = TryStatement ;  code_block = TryStatement ;  code_block = TryStatement ;  logger . info ( name . getMethodName ( ) + " - OK" ) ; }
@ Test public void callMethodWithExistingErrorEnum ( ) { logger . info ( name . getMethodName ( ) + "" ) ; code_block = TryStatement ;  code_block = TryStatement ;  code_block = TryStatement ;  logger . info ( name . getMethodName ( ) + " - OK" ) ; }
public void test() { if ( totalQueryTime > _defaultLargeQueryLatencyMs ) { LOGGER . info ( "Trace Info: request handler processing time : {}, send response latency: {}, total time to handle request: {}" , _lastProcessingLatency . getLatencyMs ( ) , _lastSendResponseLatency . getLatencyMs ( ) , totalQueryTime ) ; } }
@ Test public void testResponseUp ( ) { log . info ( "Testing log resource" ) ; WebTarget target = target ( ) . path ( "logs" ) ; String s = target . request ( ) . get ( String . class ) ; assertEquals ( "Logs resource is up!" , s ) ; }
public void test() { if ( cancelParams instanceof CancelParams ) { String id = ( ( CancelParams ) cancelParams ) . getId ( ) ; LOG . debug ( "Client cancels: " + id ) ; CompletableFuture < ? > future ; synchronized ( receivedRequestMap ) code_block = "" ; if ( future != null ) future . cancel ( true ) ; else LOG . debug ( "Unmatched cancel notification for request id " + id ) ; return true ; } else { LOG . warn ( "Cancellation support is disabled, since the '" + MessageJsonHandler . CANCEL_METHOD . getMethodName ( ) + "' method has been registered explicitly." ) ; } }
public void test() { if ( cancelParams != null ) { code_block = IfStatement ; } else { LOG . warn ( "Missing 'params' attribute of cancel notification." ) ; } }
public void test() { try { code_block = IfStatement ; UserDetails user = this . extractUser ( authentication . getPrincipal ( ) . toString ( ) , authentication . getCredentials ( ) . toString ( ) , false ) ; code_block = IfStatement ; } catch ( AuthenticationException e ) { throw e ; } catch ( Exception e ) { logger . error ( "Error detected during user authentication" , e ) ; throw new AuthenticationServiceException ( "Error detected during user authentication" , e ) ; } }
public void test() { if ( ! isWSDLAvailable ( serviceUrl ) ) { isServiceUnDeployed = true ; log . info ( "Proxy UnDeployed in " + time + " millis in worker" ) ; break ; } }
public void test() { try { siegfriedPlugin . init ( ) ; } catch ( PluginException e ) { LOGGER . error ( "Error doing {} init" , SiegfriedPlugin . class . getName ( ) , e ) ; } }
public void test() { try { IdentifierNode [ ] variableAndType = Tokenizer . tokenize ( typeIdentifierNode . getModule ( ) , SpreadsheetSymbols . TYPE_DELIMITER . toString ( ) ) ; code_block = IfStatement ; } catch ( OpenLCompilationException e ) { SyntaxNodeException error = SyntaxNodeExceptionUtils . createError ( "Cannot parse header." , typeIdentifierNode ) ; getBindingContext ( ) . addError ( error ) ; LOG . debug ( "Error occurred: " , e ) ; } }
public void test() { if ( log . isFatalEnabled ( ) ) { log . fatal ( MessageFormat . format ( message . toString ( ) , args ) , t ) ; } }
public void test() { switch ( mAppearance ) { case FLASHRED : case RED : lowTurnout . getBean ( ) . setCommandedState ( Turnout . THROWN ) ; break ; case FLASHYELLOW : case YELLOW : highTurnout . getBean ( ) . setCommandedState ( Turnout . THROWN ) ; break ; case FLASHGREEN : case GREEN : lowTurnout . getBean ( ) . setCommandedState ( Turnout . CLOSED ) ; break ; case DARK : highTurnout . getBean ( ) . setCommandedState ( Turnout . CLOSED ) ; break ; default : log . error ( "Invalid state request: {}" , mAppearance ) ; } }
@ Override public void ping ( ) { invocationCount ++ ; LOG . info ( "call for oneway ping" ) ; }
private void assignToOwner ( Entity entity ) { EntityIdentity entityIdentity = new EntityIdentity ( entity ) ; String ownerName = entity . getString ( ownerAttributeName ) ; LOG . debug ( "Assigning entity {} to owner {}..." , entityIdentity , ownerName ) ; Sid ownerSid = createUserSid ( ownerName ) ; MutableAcl acl = ( MutableAcl ) mutableAclService . readAclById ( entityIdentity ) ; acl . setOwner ( ownerSid ) ; removeAllEntries ( acl ) ; acl . insertAce ( 0 , WRITE , ownerSid , true ) ; mutableAclService . updateAcl ( acl ) ; LOG . info ( "Assigned entity {} to owner {}." , entityIdentity , ownerName ) ; }
private void assignToOwner ( Entity entity ) { EntityIdentity entityIdentity = new EntityIdentity ( entity ) ; String ownerName = entity . getString ( ownerAttributeName ) ; LOG . debug ( "Assigning entity {} to owner {}..." , entityIdentity , ownerName ) ; Sid ownerSid = createUserSid ( ownerName ) ; MutableAcl acl = ( MutableAcl ) mutableAclService . readAclById ( entityIdentity ) ; acl . setOwner ( ownerSid ) ; removeAllEntries ( acl ) ; acl . insertAce ( 0 , WRITE , ownerSid , true ) ; mutableAclService . updateAcl ( acl ) ; LOG . info ( "Assigned entity {} to owner {}." , entityIdentity , ownerName ) ; }
public void test() { try { code_block = IfStatement ; } catch ( HiveException e ) { String msg = "Error in role operation " + operation . getOperationName ( ) + " on role name " + name + ", error message " + e . getMessage ( ) ; LOG . warn ( msg , e ) ; console . printError ( msg ) ; return RETURN_CODE_FAILURE ; } catch ( IOException e ) { String msg = "IO Error in role operation " + e . getMessage ( ) ; LOG . info ( msg , e ) ; console . printError ( msg ) ; return RETURN_CODE_FAILURE ; } finally { closeQuiet ( outStream ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( HiveException e ) { String msg = "Error in role operation " + operation . getOperationName ( ) + " on role name " + name + ", error message " + e . getMessage ( ) ; LOG . warn ( msg , e ) ; console . printError ( msg ) ; return RETURN_CODE_FAILURE ; } catch ( IOException e ) { String msg = "IO Error in role operation " + e . getMessage ( ) ; LOG . info ( msg , e ) ; console . printError ( msg ) ; return RETURN_CODE_FAILURE ; } finally { closeQuiet ( outStream ) ; } }
public void test() { try { return Context . getCurrentContext ( ) . evaluateString ( scope , IOUtils . toString ( is ) , filename , 0 , null ) ; } catch ( IOException e ) { LOGGER . error ( "File not readable %s" , filename ) ; } }
@ Override public void initialize ( ) { logger . debug ( "initializing handler for thing {}" , getThing ( ) . getUID ( ) ) ; commManager = null ; String threadName = "OH-binding-" + getThing ( ) . getUID ( ) . getAsString ( ) ; String errorMsg = null ; code_block = IfStatement ; code_block = IfStatement ; }
public void test() { try { logger . trace ( "Powermax job..." ) ; updateMotionSensorState ( ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . warn ( "Exception in scheduled job: {}" , e . getMessage ( ) , e ) ; } }
public void test() { if ( graphResponse . get ( "response" ) . getAsJsonObject ( ) . get ( "results" ) . getAsJsonArray ( ) . get ( 0 ) . getAsJsonObject ( ) . get ( "data" ) . getAsJsonArray ( ) . size ( ) == 0 ) { log . debug ( "Another Health query {} " , healthQuery ) ; healthQuery = "" ; healthQuery = healthQuery + CREATE + nodeLabels + " {props})" ; JsonObject graphResponse1 = dbHandler . executeQueryWithData ( healthQuery , dataList ) ; parseGraphResponseForError ( graphResponse1 , routingKey ) ; } }
public void test() { if ( customArgument instanceof AdditionalInfo ) { final AdditionalInfo info = ( AdditionalInfo ) customArgument ; final String authId = info . get ( EXT_INFO_KEY_HONO_AUTH_ID , String . class ) ; final Device device = info . get ( EXT_INFO_KEY_HONO_DEVICE , Device . class ) ; LOG . debug ( "get additional info auth-id: {}, device: {}@{}" , authId , device . getDeviceId ( ) , device . getTenantId ( ) ) ; return info ; } }
public void recordImportFailure ( Owner owner , Throwable error , String filename ) { ImportRecord record = new ImportRecord ( owner ) ; log . error ( "Recording import failure" , error ) ; code_block = IfStatement ; record . setUpstreamConsumer ( createImportUpstreamConsumer ( owner , null ) ) ; record . setFileName ( filename ) ; record . recordStatus ( ImportRecord . Status . FAILURE , error . getMessage ( ) ) ; this . importRecordCurator . create ( record ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Executing applications synchronization task" ) ; } }
public void test() { if ( context == null ) { logger . warn ( "Cannot correctly name transaction for method {} because JobExecutionContext is null" , signature ) ; transaction = createAndActivateTransaction ( clazz , signature ) ; } else-if ( active == null ) { transaction = createAndActivateTransaction ( clazz , context . getJobDetail ( ) . getKey ( ) . toString ( ) ) ; } else { logger . debug ( "Not creating transaction for method {} because there is already a transaction running ({})" , signature , active ) ; } }
public void test() { if ( context == null ) { logger . warn ( "Cannot correctly name transaction for method {} because JobExecutionContext is null" , signature ) ; transaction = createAndActivateTransaction ( clazz , signature ) ; } else-if ( active == null ) { transaction = createAndActivateTransaction ( clazz , context . getJobDetail ( ) . getKey ( ) . toString ( ) ) ; } else { logger . debug ( "Not creating transaction for method {} because there is already a transaction running ({})" , signature , active ) ; } }
public void test() { if ( file . exists ( ) ) { LOGGER . warn ( "File {} yet exists, overwrite." , filename ) ; } }
@ MCRCommand ( syntax = "export all permissions to file {0}" , help = "Export all permissions from the Access Control System to the file {0}." , order = 50 ) public static void exportAllPermissionsToFile ( String filename ) throws Exception { MCRAccessInterface accessImpl = MCRAccessManager . getAccessImpl ( ) ; Element mcrpermissions = new Element ( "mcrpermissions" ) ; mcrpermissions . addNamespaceDeclaration ( XSI_NAMESPACE ) ; mcrpermissions . addNamespaceDeclaration ( XLINK_NAMESPACE ) ; mcrpermissions . setAttribute ( "noNamespaceSchemaLocation" , "MCRPermissions.xsd" , XSI_NAMESPACE ) ; Document doc = new Document ( mcrpermissions ) ; Collection < String > permissions = accessImpl . getPermissions ( ) ; code_block = ForStatement ; File file = new File ( filename ) ; code_block = IfStatement ; FileOutputStream fos = new FileOutputStream ( file ) ; LOGGER . info ( "Writing to file {} ..." , filename ) ; String mcrEncoding = MCRConfiguration2 . getString ( "MCR.Metadata.DefaultEncoding" ) . orElse ( DEFAULT_ENCODING ) ; XMLOutputter out = new XMLOutputter ( Format . getPrettyFormat ( ) . setEncoding ( mcrEncoding ) ) ; out . output ( doc , fos ) ; }
@ Test public void test_apply_variant_01 ( ) { Log . debug ( "Test" ) ; Variant variant = new Variant ( transcript . getParent ( ) , 290 , "TTT" , "AAA" ) ; checkApplyMnp ( variant , transcript . cds ( ) , transcript . protein ( ) , 1 , 300 , 399 ) ; }
public boolean clientExistsThenEvict ( Integer demographicNo ) { boolean exists = false ; Demographic existingDemo = this . getClientByDemographicNo ( demographicNo ) ; exists = ( existingDemo != null ) ; if ( exists ) this . getHibernateTemplate ( ) . evict ( existingDemo ) ; log . debug ( "exists (then evict): " + exists ) ; return exists ; }
public void test() { try { java . util . List < com . liferay . dynamic . data . lists . model . DDLRecordVersion > returnValue = DDLRecordVersionServiceUtil . getRecordVersions ( recordId ) ; return com . liferay . dynamic . data . lists . model . DDLRecordVersionSoap . toSoapModels ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try { byte [ ] arr = U . marshal ( ctx , th ) ; errorsBytes . add ( arr ) ; } catch ( IgniteCheckedException e ) { log . error ( "Failed to marshal deployment error, err=" + th , e ) ; } }
public void test() { try { MessageDigest . getInstance ( "MD4" ) ; } catch ( NoSuchAlgorithmException ex ) { logger . warn ( "No MD4 digest provider found !" ) ; return ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( knowledgeId ) ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( key + " = " + value + "  (" + value . getClass ( ) . getName ( ) + ")" ) ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( PropertyUtil . reflectionToString ( stock ) ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Member attribute changed: [" + memberAttributeEvent . getKey ( ) + "] " + memberAttributeEvent . getValue ( ) ) ; } }
public void testBSPTaskSelfDestroy ( ) { LOG . info ( "Testing self kill on lost contact." ) ; CompletionService < Integer > completionService = new ExecutorCompletionService < Integer > ( this . testBSPTaskService ) ; TestBSPProcessRunner runner = new TestBSPProcessRunner ( 0 , workerServer . getListenerAddress ( ) . getPort ( ) ) ; Future < Integer > future = completionService . submit ( runner ) ; code_block = TryStatement ;  workerServer . stop ( ) ; umbilical = null ; workerServer = null ; Integer exitValue = - 1 ; code_block = TryStatement ;  assertEquals ( 69 , exitValue . intValue ( ) ) ; runner . destroyProcess ( ) ; }
public void test() { try { code_block = WhileStatement ; } catch ( Exception e ) { LOG . error ( "Interrupted the timer for 1 sec." , e ) ; } }
public void test() { try { exitValue = future . get ( 20000 , TimeUnit . MILLISECONDS ) ; } catch ( InterruptedException e1 ) { LOG . error ( "Interrupted Exception." , e1 ) ; } catch ( ExecutionException e1 ) { LOG . error ( "ExecutionException Exception." , e1 ) ; } catch ( TimeoutException e ) { LOG . error ( "TimeoutException Exception." , e ) ; } }
public void test() { try { exitValue = future . get ( 20000 , TimeUnit . MILLISECONDS ) ; } catch ( InterruptedException e1 ) { LOG . error ( "Interrupted Exception." , e1 ) ; } catch ( ExecutionException e1 ) { LOG . error ( "ExecutionException Exception." , e1 ) ; } catch ( TimeoutException e ) { LOG . error ( "TimeoutException Exception." , e ) ; } }
public void test() { try { exitValue = future . get ( 20000 , TimeUnit . MILLISECONDS ) ; } catch ( InterruptedException e1 ) { LOG . error ( "Interrupted Exception." , e1 ) ; } catch ( ExecutionException e1 ) { LOG . error ( "ExecutionException Exception." , e1 ) ; } catch ( TimeoutException e ) { LOG . error ( "TimeoutException Exception." , e ) ; } }
@ Test public void testLedgerDeleteWithExistingEntryLogs ( ) throws Exception { LedgerHandle [ ] lhs = writeLedgerEntries ( 3 , 1024 , 1024 ) ; restartBookies ( ) ; code_block = ForStatement ; LOG . info ( "Finished deleting all ledgers so waiting for the GC thread to clean up the entryLogs" ) ; Thread . sleep ( 2 * baseConf . getGcWaitTime ( ) ) ; code_block = ForStatement ; }
public void test() { { Map < String , String > env = setupEnvironment ( peer . getConfiguration ( ) ) ; List < String > cmd = setupCommand ( peer . getConfiguration ( ) ) ; TaskAttemptID taskid = peer . getTaskId ( ) ; File stdout = TaskLog . getTaskLogFile ( taskid , TaskLog . LogName . STDOUT ) ; File stderr = TaskLog . getTaskLogFile ( taskid , TaskLog . LogName . STDERR ) ; long logLength = TaskLog . getTaskLogLength ( peer . getConfiguration ( ) ) ; code_block = IfStatement ; checkParentFile ( stdout ) ; LOG . debug ( "STDOUT: " + stdout . getAbsolutePath ( ) ) ; checkParentFile ( stderr ) ; LOG . debug ( "STDERR: " + stderr . getAbsolutePath ( ) ) ; LOG . debug ( "DEBUG: cmd: " + cmd ) ; process = runClient ( cmd , env ) ; code_block = TryStatement ;  } }
public void test() { { Map < String , String > env = setupEnvironment ( peer . getConfiguration ( ) ) ; List < String > cmd = setupCommand ( peer . getConfiguration ( ) ) ; TaskAttemptID taskid = peer . getTaskId ( ) ; File stdout = TaskLog . getTaskLogFile ( taskid , TaskLog . LogName . STDOUT ) ; File stderr = TaskLog . getTaskLogFile ( taskid , TaskLog . LogName . STDERR ) ; long logLength = TaskLog . getTaskLogLength ( peer . getConfiguration ( ) ) ; code_block = IfStatement ; checkParentFile ( stdout ) ; LOG . debug ( "STDOUT: " + stdout . getAbsolutePath ( ) ) ; checkParentFile ( stderr ) ; LOG . debug ( "STDERR: " + stderr . getAbsolutePath ( ) ) ; LOG . debug ( "DEBUG: cmd: " + cmd ) ; process = runClient ( cmd , env ) ; code_block = TryStatement ;  } }
public void test() { { Map < String , String > env = setupEnvironment ( peer . getConfiguration ( ) ) ; List < String > cmd = setupCommand ( peer . getConfiguration ( ) ) ; TaskAttemptID taskid = peer . getTaskId ( ) ; File stdout = TaskLog . getTaskLogFile ( taskid , TaskLog . LogName . STDOUT ) ; File stderr = TaskLog . getTaskLogFile ( taskid , TaskLog . LogName . STDERR ) ; long logLength = TaskLog . getTaskLogLength ( peer . getConfiguration ( ) ) ; code_block = IfStatement ; checkParentFile ( stdout ) ; LOG . debug ( "STDOUT: " + stdout . getAbsolutePath ( ) ) ; checkParentFile ( stderr ) ; LOG . debug ( "STDERR: " + stderr . getAbsolutePath ( ) ) ; LOG . debug ( "DEBUG: cmd: " + cmd ) ; process = runClient ( cmd , env ) ; code_block = TryStatement ;  } }
public void test() { if ( ! streamingEnabled ) { cmd = TaskLog . captureOutAndError ( null , cmd , stdout , stderr , logLength ) ; } else { cmd = TaskLog . captureOutAndErrorTee ( null , cmd , stdout , stderr , logLength ) ; } }
public void test() { if ( streamingEnabled ) { downlink = new StreamingProtocol ( peer , process . getOutputStream ( ) , process . getInputStream ( ) ) ; } else { LOG . debug ( "DEBUG: waiting for Client at " + serverSocket . getLocalSocketAddress ( ) ) ; serverSocket . setSoTimeout ( SERVER_SOCKET_TIMEOUT ) ; clientSocket = serverSocket . accept ( ) ; LOG . debug ( "DEBUG: Client connected! - start BinaryProtocol!" ) ; downlink = new BinaryProtocol < K1 , V1 , K2 , V2 , M > ( peer , clientSocket . getOutputStream ( ) , clientSocket . getInputStream ( ) ) ; } }
public void test() { while ( ( inputLine = br . readLine ( ) ) != null ) { LOG . error ( "PipesApp Error: " + inputLine ) ; } }
public void test() { if ( table . getSelectedRow ( ) < 0 ) { MessageDialog . error ( null , "angal.common.pleaseselectarow.msg" ) ; } else { ExamRowBrowsingManager manager = Context . getApplicationContext ( ) . getBean ( ExamRowBrowsingManager . class ) ; ExamRow row = ( ExamRow ) ( ( ( ExamRowBrowsingModel ) model ) . getValueAt ( table . getSelectedRow ( ) , - 1 ) ) ; int n = JOptionPane . showConfirmDialog ( null , MessageBundle . getMessage ( "angal.exa.deleteexamresult" ) + " \"" + row . getDescription ( ) + "\" ?" , MessageBundle . getMessage ( "angal.hospital" ) , JOptionPane . YES_NO_OPTION ) ; code_block = IfStatement ; } }
@ Override public void instantiateServiceInstance ( ) { LOG . info ( "Instantiating {}" , this . getClass ( ) . getSimpleName ( ) ) ; endpointRpcRegistry = new EndpointRpcRegistry ( dataBroker ) ; serviceRpcRegistration = rpcProviderRegistry . addRpcImplementation ( EndpointService . class , this ) ; }
public void test() { try { dispatcher . execute ( ( ) -> deliverNextPending ( ) ) ; } catch ( RejectedExecutionException rje ) { LOG . debug ( "Rejected on attempt to queue message dispatch" , rje ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "TaskService.updateTask() for TaskId={} INSERTED an Attachment={}." , newTaskImpl . getId ( ) , attachmentImpl ) ; } }
public void test() { -> { logger . warn ( "Generic error warning." ) ; nc . addNotification ( "Sorry, there was an error." ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Using SPN: {}" , spn ) ; } }
public void test() { if ( constraint != null ) { log . info ( "Scenario '" + tc . getScenario ( ) + "' skipped due to constraints " + Arrays . toString ( constraint . value ( ) ) ) ; } else { exec . forkScenario ( c . getSimpleName ( ) ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( WikiPageServiceUtil . class , "getRecentChanges" , _getRecentChangesParameterTypes35 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , nodeId , start , end ) ; Object returnObj = null ; code_block = TryStatement ;  return ( java . util . List < com . liferay . wiki . model . WikiPage > ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { ResultSet resultSetComment = statement . executeQuery ( sqlQueryComment ) ; code_block = WhileStatement ; JdbcUtils . close ( resultSetComment ) ; } catch ( SQLException e1 ) { logger . error ( "[buildDasColumn executeQuery Exception] --> " + "the exception message is:" + e1 . getMessage ( ) ) ; } }
public void test() { try { int columnCount = metaData . getColumnCount ( ) ; code_block = ForStatement ; Statement statement = connection . createStatement ( ) ; code_block = IfStatement ; JdbcUtils . close ( statement ) ; } catch ( SQLException e ) { logger . error ( "[buildDasColumn Exception] --> " + "the exception message is:" + e . getMessage ( ) ) ; } }
public void updateVfModule ( String cloudSiteId , String cloudOwner , String tenantId , String vnfType , String vnfVersion , String vnfName , String requestType , String volumeGroupHeatStackId , String baseVfHeatStackId , String vfModuleStackId , String modelCustomizationUuid , Map < String , Object > inputs , MsoRequest msoRequest , Holder < Map < String , String > > outputs , Holder < VnfRollback > rollback ) throws VnfException { logger . debug ( "Update VF Module command attempted but not supported" ) ; throw new VnfException ( "UpdateVfModule:  Unsupported command" , MsoExceptionCategory . USERDATA ) ; }
private void regexReplace ( String predicate , String regexMatch , String newValue ) throws IOException { String query = "" + "SELECT ?s ?o \n" + "WHERE {\n" + "  ?s <" + predicate + "> ?o .\n" + "  FILTER (regex(str(?o), \"" + regexMatch + "\", \"s\")) .\n" + "}" ; log . debug ( query ) ; StringBuilder insertQ = new StringBuilder ( "INSERT DATA {\n" ) ; StringBuilder deleteQ = new StringBuilder ( "DELETE DATA {\n" ) ; int modifyCounter = 0 ; code_block = ForStatement ; log . debug ( "Modifying " + Integer . toString ( modifyCounter ) + " Records." ) ; insertQ . append ( "} \n" ) ; deleteQ . append ( "} \n" ) ; log . debug ( "Removing old data:\n" + deleteQ ) ; this . model . executeUpdateQuery ( deleteQ . toString ( ) ) ; log . debug ( "Inserting updated data:\n" + insertQ ) ; this . model . executeUpdateQuery ( insertQ . toString ( ) ) ; }
public void test() { for ( QuerySolution s : IterableAdaptor . adapt ( this . model . executeSelectQuery ( query ) ) ) { modifyCounter ++ ; Literal obj = s . getLiteral ( "o" ) ; RDFDatatype datatype = obj . getDatatype ( ) ; String lang = obj . getLanguage ( ) ; String objStr = obj . getValue ( ) . toString ( ) ; String oldStr = encodeString ( objStr , datatype , lang ) ; log . trace ( "Replacing record" ) ; log . debug ( "oldValue: " + oldStr ) ; String newStr = encodeString ( objStr . replaceAll ( regexMatch , newValue ) , datatype , lang ) ; String sUri = s . getResource ( "s" ) . getURI ( ) ; log . debug ( "newValue: " + newStr ) ; deleteQ . append ( "  <" + sUri + "> <" + predicate + "> " + oldStr + " . \n" ) ; insertQ . append ( "  <" + sUri + "> <" + predicate + "> " + newStr + " . \n" ) ; } }
public void test() { for ( QuerySolution s : IterableAdaptor . adapt ( this . model . executeSelectQuery ( query ) ) ) { modifyCounter ++ ; Literal obj = s . getLiteral ( "o" ) ; RDFDatatype datatype = obj . getDatatype ( ) ; String lang = obj . getLanguage ( ) ; String objStr = obj . getValue ( ) . toString ( ) ; String oldStr = encodeString ( objStr , datatype , lang ) ; log . trace ( "Replacing record" ) ; log . debug ( "oldValue: " + oldStr ) ; String newStr = encodeString ( objStr . replaceAll ( regexMatch , newValue ) , datatype , lang ) ; String sUri = s . getResource ( "s" ) . getURI ( ) ; log . debug ( "newValue: " + newStr ) ; deleteQ . append ( "  <" + sUri + "> <" + predicate + "> " + oldStr + " . \n" ) ; insertQ . append ( "  <" + sUri + "> <" + predicate + "> " + newStr + " . \n" ) ; } }
public void test() { for ( QuerySolution s : IterableAdaptor . adapt ( this . model . executeSelectQuery ( query ) ) ) { modifyCounter ++ ; Literal obj = s . getLiteral ( "o" ) ; RDFDatatype datatype = obj . getDatatype ( ) ; String lang = obj . getLanguage ( ) ; String objStr = obj . getValue ( ) . toString ( ) ; String oldStr = encodeString ( objStr , datatype , lang ) ; log . trace ( "Replacing record" ) ; log . debug ( "oldValue: " + oldStr ) ; String newStr = encodeString ( objStr . replaceAll ( regexMatch , newValue ) , datatype , lang ) ; String sUri = s . getResource ( "s" ) . getURI ( ) ; log . debug ( "newValue: " + newStr ) ; deleteQ . append ( "  <" + sUri + "> <" + predicate + "> " + oldStr + " . \n" ) ; insertQ . append ( "  <" + sUri + "> <" + predicate + "> " + newStr + " . \n" ) ; } }
private void regexReplace ( String predicate , String regexMatch , String newValue ) throws IOException { String query = "" + "SELECT ?s ?o \n" + "WHERE {\n" + "  ?s <" + predicate + "> ?o .\n" + "  FILTER (regex(str(?o), \"" + regexMatch + "\", \"s\")) .\n" + "}" ; log . debug ( query ) ; StringBuilder insertQ = new StringBuilder ( "INSERT DATA {\n" ) ; StringBuilder deleteQ = new StringBuilder ( "DELETE DATA {\n" ) ; int modifyCounter = 0 ; code_block = ForStatement ; log . debug ( "Modifying " + Integer . toString ( modifyCounter ) + " Records." ) ; insertQ . append ( "} \n" ) ; deleteQ . append ( "} \n" ) ; log . debug ( "Removing old data:\n" + deleteQ ) ; this . model . executeUpdateQuery ( deleteQ . toString ( ) ) ; log . debug ( "Inserting updated data:\n" + insertQ ) ; this . model . executeUpdateQuery ( insertQ . toString ( ) ) ; }
private void regexReplace ( String predicate , String regexMatch , String newValue ) throws IOException { String query = "" + "SELECT ?s ?o \n" + "WHERE {\n" + "  ?s <" + predicate + "> ?o .\n" + "  FILTER (regex(str(?o), \"" + regexMatch + "\", \"s\")) .\n" + "}" ; log . debug ( query ) ; StringBuilder insertQ = new StringBuilder ( "INSERT DATA {\n" ) ; StringBuilder deleteQ = new StringBuilder ( "DELETE DATA {\n" ) ; int modifyCounter = 0 ; code_block = ForStatement ; log . debug ( "Modifying " + Integer . toString ( modifyCounter ) + " Records." ) ; insertQ . append ( "} \n" ) ; deleteQ . append ( "} \n" ) ; log . debug ( "Removing old data:\n" + deleteQ ) ; this . model . executeUpdateQuery ( deleteQ . toString ( ) ) ; log . debug ( "Inserting updated data:\n" + insertQ ) ; this . model . executeUpdateQuery ( insertQ . toString ( ) ) ; }
private void regexReplace ( String predicate , String regexMatch , String newValue ) throws IOException { String query = "" + "SELECT ?s ?o \n" + "WHERE {\n" + "  ?s <" + predicate + "> ?o .\n" + "  FILTER (regex(str(?o), \"" + regexMatch + "\", \"s\")) .\n" + "}" ; log . debug ( query ) ; StringBuilder insertQ = new StringBuilder ( "INSERT DATA {\n" ) ; StringBuilder deleteQ = new StringBuilder ( "DELETE DATA {\n" ) ; int modifyCounter = 0 ; code_block = ForStatement ; log . debug ( "Modifying " + Integer . toString ( modifyCounter ) + " Records." ) ; insertQ . append ( "} \n" ) ; deleteQ . append ( "} \n" ) ; log . debug ( "Removing old data:\n" + deleteQ ) ; this . model . executeUpdateQuery ( deleteQ . toString ( ) ) ; log . debug ( "Inserting updated data:\n" + insertQ ) ; this . model . executeUpdateQuery ( insertQ . toString ( ) ) ; }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "ignoring axis, can not be built: {} {}{}" , nameStep . getAxis ( ) , prefix . isEmpty ( ) ? "" : prefix + ":" , name ) ; } }
private void shrinkIfBelowCapacity ( ) { int currentLoad = maxTotalInFlight . getAndSet ( totalInFlight . get ( ) ) ; int maxRequestsPerConnection = options ( ) . getMaxRequestsPerConnection ( hostDistance ) ; int needed = currentLoad / maxRequestsPerConnection + 1 ; if ( currentLoad % maxRequestsPerConnection > options ( ) . getNewConnectionThreshold ( hostDistance ) ) needed += 1 ; needed = Math . max ( needed , options ( ) . getCoreConnectionsPerHost ( hostDistance ) ) ; int actual = open . get ( ) ; int toTrash = Math . max ( 0 , actual - needed ) ; logger . trace ( "Current inFlight = {}, {} connections needed, {} connections available, trashing {}" , currentLoad , needed , actual , toTrash ) ; if ( toTrash <= 0 ) return ; for ( Connection connection : connections ) code_block = IfStatement ; }
public void test() { try { Files . delete ( file . toPath ( ) ) ; } catch ( IOException e ) { logger . error ( "{} can not be deleted." , file , e ) ; } }
public void test() { try { session = hibernateTemplate . getSessionFactory ( ) . openSession ( ) ; query = session . createQuery ( "From ComprehensionTestQuestionBo CTQBO where CTQBO.studyId=:studyId" + " and CTQBO.active=1 order by CTQBO.sequenceNo asc" ) ; query . setInteger ( "studyId" , studyId ) ; comprehensionTestQuestionList = query . list ( ) ; } catch ( Exception e ) { logger . error ( "StudyDAOImpl - getComprehensionTestQuestionList() - Error" , e ) ; } finally { code_block = IfStatement ; } }
@ Override public void start ( Map < String , String > configProps ) { LOG . info ( "Connector config keys: {}" , String . join ( ", " , configProps . keySet ( ) ) ) ; this . configProps = configProps ; }
public static void main ( String [ ] args ) throws Exception { logger . warn ( "org.apache.kylin.storage.hbase.util.StorageCleanupJob is deprecated, use org.apache.kylin.tool.StorageCleanupJob instead" ) ; StorageCleanupJob cli = new StorageCleanupJob ( ) ; cli . execute ( args ) ; }
public void test() { try { ObjectMetadata objectMetadata = ( ObjectMetadata ) callCOSClientWithRetry ( getObjectMetadataRequest ) ; return objectMetadata . getContentLength ( ) ; } catch ( Exception e ) { String errMsg = String . format ( "Getting file length occurs an exception." + "COS key: %s, exception: %s" , key , e . toString ( ) ) ; LOG . error ( errMsg ) ; handleException ( new Exception ( errMsg ) , key ) ; return 0 ; } }
private void reAttemptDelivery ( Mail mail , int retries ) throws MailQueue . MailQueueException { LOGGER . debug ( "Storing message {} into outgoing after {} retries" , mail . getName ( ) , retries ) ; DeliveryRetriesHelper . incrementRetries ( mail ) ; mail . setLastUpdated ( dateSupplier . get ( ) ) ; Duration delay = getNextDelay ( DeliveryRetriesHelper . retrieveRetries ( mail ) ) ; code_block = IfStatement ; queue . enQueue ( mail , delay ) ; }
protected void onInPacketAdded ( final String networkId , final InPacketAdded msg ) { log . debug ( "" ) ; code_block = IfStatement ; }
public void test() { try { DataSources . destroy ( sourceConfig . getDataSource ( ) ) ; } catch ( SQLException e ) { LOGGER . error ( "Destroying datasource" , e ) ; } }
public void test() { try { txn . start ( ) ; String sql = INSERT_ACCOUNT ; PreparedStatement pstmt = null ; pstmt = txn . prepareAutoCloseStatement ( sql ) ; code_block = ForStatement ; pstmt . executeBatch ( ) ; txn . commit ( ) ; } catch ( Exception ex ) { txn . rollback ( ) ; s_logger . error ( "error saving account to cloud_usage db" , ex ) ; throw new CloudRuntimeException ( ex . getMessage ( ) ) ; } }
public void test() { if ( topic . isPresent ( ) ) { Future future = kafkaProducer . send ( new ProducerRecord < String , String > ( topic . get ( ) , jsonMessage ) ) ; results . add ( new AbstractMap . SimpleEntry < > ( messageId , future ) ) ; } else { LOG . debug ( "Dropping {} because no topic is specified." , jsonMessage ) ; } }
public void test() { try { SearchControls ctls = new SearchControls ( ) ; ctls . setReturningObjFlag ( true ) ; ctls . setSearchScope ( SearchControls . OBJECT_SCOPE ) ; ctls . setReturningAttributes ( new String [ ] code_block = "" ; ) ; NamingEnumeration < SearchResult > objResults = ctx . search ( "" , "objectclass=*" , ctls ) ; code_block = WhileStatement ; if ( dNs . isEmpty ( ) ) LOGGER . warn ( "No base DNs could be located for LDAP context" ) ; } catch ( Exception e ) { LOGGER . warn ( "ERROR looking up base DNs for LDAP context" , e ) ; } }
@ PayloadRoot ( localPart = "SendNotificationRequest" , namespace = MICROGRIDS_NOTIFICATION_NAMESPACE ) @ ResponsePayload public SendNotificationResponse sendNotification ( @ OrganisationIdentification final String organisationIdentification , @ RequestPayload final SendNotificationRequest request ) throws WebServiceException { LOGGER . info ( "Incoming SendNotificationRequest for organisation: {} device: {}." , organisationIdentification , request . getNotification ( ) . getDeviceIdentification ( ) ) ; this . notificationService . handleNotification ( request . getNotification ( ) , organisationIdentification ) ; return new SendNotificationResponse ( ) ; }
public void test() { try { date = sdf . parse ( strDate ) ; cal . setTime ( date ) ; month = cal . get ( Calendar . MONTH ) ; year = cal . get ( Calendar . YEAR ) ; } catch ( ParseException ex ) { LOGGER . error ( "Could not parse the given date: {}" , ex ) ; } }
public void test() { try { patternKey = Pattern . compile ( regExpKey ) ; patterns . put ( regExpKey , patternKey ) ; } catch ( PatternSyntaxException ex ) { log . error ( "Error compiling pattern: " + regExpKey ) ; } }
public void test() { try { patternValue = Pattern . compile ( regExpValue ) ; patterns . put ( regExpValue , patternValue ) ; } catch ( PatternSyntaxException ex ) { log . error ( "Error compiling pattern: " + regExpValue ) ; } }
public void test() { try { double value = Double . parseDouble ( sValue ) ; code_block = IfStatement ; } catch ( NumberFormatException ex ) { log . error ( "Value extracted is not a number: " + sValue ) ; } }
public void test() { if ( oldValues . containsKey ( key ) ) { double delta = value - oldValues . get ( key ) ; addRecord ( key , time , delta ) ; } }
public void test() { if ( confirmationServletFactory == null ) { log . info ( "Confirmation servlet factory is not available, skipping its deploymnet" ) ; return ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Handling command [{}]" , command . getCommandName ( ) ) ; } }
public void test() { { logger . debug ( "-----deleteGroup--- , Group Name: {}" , groupName ) ; assertObjectExists ( groupManagerService . get ( groupName ) , "group" , groupName ) ; UberfireRestResponse response = resourceHelper . removeGroup ( groupName ) ; return createResponse ( response ) ; } }
public void test() { try { realConsumer . accept ( objectMapper . readValue ( value , clazz ) ) ; } catch ( JsonProcessingException e ) { logger . info ( "Invalid payload {}" , value , e ) ; } }
public void test() { try { String [ ] modes = StringUtils . split ( mode , ',' ) ; world = AccessType . valueOf ( modes [ 0 ] . trim ( ) ) ; trusted = AccessType . valueOf ( modes [ 1 ] . trim ( ) ) ; } catch ( RuntimeException e ) { LOGGER . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( item != null && item instanceof KeyValue ) { final KeyValue keyValue = ( KeyValue ) item ; result . put ( keyValue . getKey ( ) , keyValue . getValue ( ) ) ; } else { logger . warn ( "KeyValue source {} of {} returned invalid value {}" , _keySource . getUuid ( ) , getUuid ( ) , item ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( String . format ( "Kubernetes cluster : %s is already stopped" , kubernetesCluster . getName ( ) ) ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( String . format ( "Kubernetes cluster : %s is getting stopped" , kubernetesCluster . getName ( ) ) ) ; } }
public void test() { if ( errorOk ) { return null ; } else { String errorMessage = String . format ( "Error performing %s on %s: %d, %s" , method , path , response . code ( ) , responseString ) ; log . warn ( errorMessage ) ; throw new RuntimeException ( errorMessage ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "handled image error with type " + onErrorType , e ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { if ( e instanceof ThriftSecurityException ) { result . sec = ( ThriftSecurityException ) e ; result . setSecIsSet ( true ) ; msg = result ; } else-if ( e instanceof ThriftTableOperationException ) { result . tope = ( ThriftTableOperationException ) e ; result . setTopeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof ThriftSecurityException ) { result . sec = ( ThriftSecurityException ) e ; result . setSecIsSet ( true ) ; msg = result ; } else-if ( e instanceof ThriftTableOperationException ) { result . tope = ( ThriftTableOperationException ) e ; result . setTopeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof ThriftSecurityException ) { result . sec = ( ThriftSecurityException ) e ; result . setSecIsSet ( true ) ; msg = result ; } else-if ( e instanceof ThriftTableOperationException ) { result . tope = ( ThriftTableOperationException ) e ; result . setTopeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { try { fcall . sendResponse ( fb , msg , msgType , seqid ) ; } catch ( java . lang . Exception ex ) { _LOGGER . error ( "Exception writing to internal frame buffer" , ex ) ; fb . close ( ) ; } }
private void takeSnapshot ( OutputStream out ) throws IOException { ObjectOutputStream stream = new ObjectOutputStream ( out ) ; stream . writeUTF ( getClass ( ) . getName ( ) ) ; stream . writeInt ( LOG_VERSION ) ; stream . writeObject ( myServiceID ) ; stream . writeLong ( eventID ) ; stream . writeInt ( unicastPort ) ; stream . writeObject ( memberGroups ) ; stream . writeObject ( lookupGroups ) ; stream . writeLong ( announcementSeqNo ) ; marshalAttributes ( lookupAttrs , stream ) ; marshalLocators ( lookupLocators , stream ) ; code_block = ForStatement ; stream . writeObject ( null ) ; code_block = ForStatement ; stream . writeObject ( null ) ; stream . flush ( ) ; logger . debug ( "wrote state snapshot" ) ; }
public void test() { if ( ploc . frameIndexList . size ( ) >= MAX_FRAME_DEPTH ) { log . warn ( "Nested (i)frame depth exceeded limit: " + MAX_FRAME_DEPTH ) ; return null ; } }
@ Override public void connectionLost ( Throwable throwable ) { String message = getLogPrefix ( ) + "connection lost" ; receiver . getAdapter ( ) . getMessageKeeper ( ) . add ( message ) ; log . debug ( message ) ; ibisExceptionListener . exceptionThrown ( this , throwable ) ; }
public void test() { try ( Writer writer = new FileWriterWithEncoding ( cacheFile , StandardCharsets . UTF_8 ) ) { writer . write ( token ) ; writer . flush ( ) ; cacheFile . setReadable ( false , false ) ; cacheFile . setReadable ( true , true ) ; code_block = IfStatement ; } catch ( IOException ioe ) { code_block = IfStatement ; } catch ( KrbException e ) { LOG . error ( "Failed to write token to cache File. " + e . toString ( ) ) ; } }
public void test() { if ( iteration > 1 && timeSpent > 86400 && ! actType . equals ( "home" ) ) { log . trace ( "{} spent {} outside home" , person , timeSpent ) ; } }
public void test() { if ( usingTeradataJdbcDriver ( connection ( ) ) ) { String tableNameInDatabase = mutableTablesState ( ) . get ( TABLE_NAME_MUTABLE ) . getNameInDatabase ( ) ; String insertSqlWithTable = String . format ( INSERT_SQL , tableNameInDatabase ) ; String selectSqlWithTable = String . format ( SELECT_STAR_SQL , tableNameInDatabase ) ; defaultQueryExecutor ( ) . executeQuery ( insertSqlWithTable , param ( TINYINT , null ) , param ( SMALLINT , null ) , param ( INTEGER , null ) , param ( BIGINT , null ) , param ( FLOAT , null ) , param ( DOUBLE , null ) , param ( DECIMAL , null ) , param ( DECIMAL , null ) , param ( TIMESTAMP , null ) , param ( DATE , null ) , param ( VARCHAR , null ) , param ( VARCHAR , null ) , param ( CHAR , null ) , param ( BOOLEAN , null ) , param ( VARBINARY , new byte [ ] code_block = "" ; ) ) ; QueryResult result = defaultQueryExecutor ( ) . executeQuery ( selectSqlWithTable ) ; assertColumnTypes ( result ) ; assertThat ( result ) . containsOnly ( row ( null , null , null , null , null , null , null , null , null , null , null , null , null , null , new byte [ ] code_block = "" ; ) ) ; } else { LOGGER . warn ( "preparedInsertVarbinaryApi() only applies to TeradataJdbcDriver" ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( JournalFolderServiceUtil . class , "getDDMStructures" , _getDDMStructuresParameterTypes5 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupIds , folderId , restrictionType , orderByComparator ) ; Object returnObj = null ; code_block = TryStatement ;  return ( java . util . List < com . liferay . dynamic . data . mapping . model . DDMStructure > ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( noSuchNodeException , noSuchNodeException ) ; } }
public void test() { if ( missEqualsFail ) { log . error ( "[{}] For colour {} (LL {},{}; pixel {},{}) the webservice gave zero locations." , kvStoreType , hex , lat , lng , x , y ) ; } else { log . warn ( "[{}] For colour {} (LL {},{}; pixel {},{}) the webservice gave zero locations." , kvStoreType , hex , lat , lng , x , y ) ; } }
public void test() { if ( missEqualsFail ) { log . error ( "[{}] For colour {} (LL {},{}; pixel {},{}) the webservice gave zero locations." , kvStoreType , hex , lat , lng , x , y ) ; } else { log . warn ( "[{}] For colour {} (LL {},{}; pixel {},{}) the webservice gave zero locations." , kvStoreType , hex , lat , lng , x , y ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Aborted {}" , this ) ; } }
public void test() { if ( null != kickUser ) { banIp = kickUser . getHostname ( ) ; final Map < ReturnableData , Object > kickData = new HashMap < ReturnableData , Object > ( ) ; kickData . put ( LongPollResponse . EVENT , LongPollEvent . BANNED . toString ( ) ) ; final QueuedMessage qm = new QueuedMessage ( MessageType . KICKED , kickData ) ; kickUser . enqueueMessage ( qm ) ; connectedUsers . removeUser ( kickUser , DisconnectReason . BANNED ) ; logger . info ( String . format ( "Banning %s (%s) by request of %s" , kickUser . getNickname ( ) , banIp , user . getNickname ( ) ) ) ; } else { banIp = request . getParameter ( AjaxRequest . NICKNAME ) ; logger . info ( String . format ( "Banning %s by request of %s" , banIp , user . getNickname ( ) ) ) ; } }
public void test() { if ( receivingEndpoint == null ) { log . warn ( "PARTICIPANT {}: Trying to connect to a user without receiving endpoint " + "(it seems is not yet fully connected)" , this . name ) ; return null ; } }
public void test() { if ( sendingEndpoints . get ( sender . getName ( ) ) != null ) { log . warn ( "PARTICIPANT {}: There is a sending endpoint to user {} " + "when trying to create another one" , this . name , sender . getName ( ) ) ; return null ; } }
public void test() { if ( sendingEndpoints . get ( sender . getName ( ) ) != null ) { log . warn ( "PARTICIPANT {}: There is a sending endpoint to user {} " + "when trying to create another one" , this . name , sender . getName ( ) ) ; return null ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Waiting thread is now locking version generation for {} region {} RVV {}" , locker , regionPath , System . identityHashCode ( this ) ) ; } }
public void test() { if ( handler == null ) { logger . debug ( HANDLER_IS_NULL ) ; return ; } }
public void test() { try { found = isFullTrigger ? definitionsService . getFullTrigger ( tenantId , triggerId ) : definitionsService . getTrigger ( tenantId , triggerId ) ; } catch ( NotFoundException e ) { } catch ( IllegalArgumentException e ) { throw new BadRequestException ( "Bad arguments: " + e . getMessage ( ) ) ; } catch ( Exception e ) { log . debug ( e . getMessage ( ) , e ) ; throw new InternalServerException ( e . toString ( ) ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( GroupServiceUtil . class , "updateGroup" , _updateGroupParameterTypes38 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , typeSettings ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . portal . kernel . model . Group ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( l != null && l . length > 0 ) { LOGGER . warn ( "Failed to delete the Archive Analyzer's temporary files from `{}`, " + "see the log for more details" , tempFileLocation . toString ( ) ) ; } }
public void test() { { log . info ( Color . GREEN + "Calendar_3_8 : empty column sunday" + Color . NORMAL ) ; Context context = new Context ( ) ; CheckPointReport result = verifyValidation ( log , context , "calendar_3_8" , GTFS_1_GTFS_Common_12 , SEVERITY . ERROR , RESULT . NOK , true ) ; Assert . assertEquals ( result . getCheckPointErrorCount ( ) , 1 , "detail count" ) ; code_block = ForStatement ; } }
public void test() { if ( table . getSelectedRow ( ) < 0 ) { MessageDialog . error ( MedicalBrowser . this , "angal.common.pleaseselectarow.msg" ) ; } else { selectedrow = table . convertRowIndexToModel ( table . getSelectedRow ( ) ) ; Medical med = ( Medical ) ( ( ( MedicalBrowsingModel ) model ) . getValueAt ( selectedrow , - 1 ) ) ; StringBuilder deleteMessage = new StringBuilder ( ) . append ( MessageBundle . getMessage ( "angal.medicals.deletemedical" ) ) . append ( " \"" ) . append ( med . getDescription ( ) ) . append ( "\" ?" ) ; int n = JOptionPane . showConfirmDialog ( MedicalBrowser . this , deleteMessage . toString ( ) , MessageBundle . getMessage ( "angal.hospital" ) , JOptionPane . YES_NO_OPTION ) ; boolean deleted ; code_block = TryStatement ;  code_block = IfStatement ; } }
@ PayloadRoot ( localPart = "ClearAlarmRegisterAsyncRequest" , namespace = SMARTMETER_MONITORING_NAMESPACE ) @ ResponsePayload public ClearAlarmRegisterResponse getClearAlarmRegisterResponse ( @ OrganisationIdentification final String organisationIdentification , @ RequestPayload final ClearAlarmRegisterAsyncRequest request ) throws OsgpException { log . info ( "Incoming clear alarm register request for meter: {}" , request . getDeviceIdentification ( ) ) ; ClearAlarmRegisterResponse response = null ; code_block = TryStatement ;  return response ; }
public void MethodWithClassDbAnnotated ( ) { s_logger . info ( "called" ) ; _dummy . sayHello ( ) ; }
public void test() { try { LOGGER . debug ( "POST" ) ; MimeType mimeType = getMimeType ( contentTypeList ) ; CreateResponse createResponse ; code_block = IfStatement ; String id = createResponse . getCreatedMetacards ( ) . get ( 0 ) . getId ( ) ; LOGGER . debug ( "Create Response id [{}]" , id ) ; LOGGER . debug ( "Entry successfully saved, id: {}" , id ) ; code_block = IfStatement ; return id ; } catch ( SourceUnavailableException e ) { String exceptionMessage = "Cannot create catalog entry because source is unavailable: " ; LOGGER . info ( exceptionMessage , e ) ; throw new InternalServerErrorException ( exceptionMessage ) ; } catch ( InternalIngestException e ) { String exceptionMessage = "Error while storing entry in catalog: " ; LOGGER . info ( exceptionMessage , e ) ; throw new InternalServerErrorException ( exceptionMessage ) ; } catch ( MetacardCreationException | IngestException e ) { String errorMessage = "Error while storing entry in catalog: " ; LOGGER . info ( errorMessage , e ) ; throw new CatalogServiceException ( errorMessage ) ; } finally { IOUtils . closeQuietly ( message ) ; } }
public void test() { try { LOGGER . debug ( "POST" ) ; MimeType mimeType = getMimeType ( contentTypeList ) ; CreateResponse createResponse ; code_block = IfStatement ; String id = createResponse . getCreatedMetacards ( ) . get ( 0 ) . getId ( ) ; LOGGER . debug ( "Create Response id [{}]" , id ) ; LOGGER . debug ( "Entry successfully saved, id: {}" , id ) ; code_block = IfStatement ; return id ; } catch ( SourceUnavailableException e ) { String exceptionMessage = "Cannot create catalog entry because source is unavailable: " ; LOGGER . info ( exceptionMessage , e ) ; throw new InternalServerErrorException ( exceptionMessage ) ; } catch ( InternalIngestException e ) { String exceptionMessage = "Error while storing entry in catalog: " ; LOGGER . info ( exceptionMessage , e ) ; throw new InternalServerErrorException ( exceptionMessage ) ; } catch ( MetacardCreationException | IngestException e ) { String errorMessage = "Error while storing entry in catalog: " ; LOGGER . info ( errorMessage , e ) ; throw new CatalogServiceException ( errorMessage ) ; } finally { IOUtils . closeQuietly ( message ) ; } }
public void test() { try { LOGGER . debug ( "POST" ) ; MimeType mimeType = getMimeType ( contentTypeList ) ; CreateResponse createResponse ; code_block = IfStatement ; String id = createResponse . getCreatedMetacards ( ) . get ( 0 ) . getId ( ) ; LOGGER . debug ( "Create Response id [{}]" , id ) ; LOGGER . debug ( "Entry successfully saved, id: {}" , id ) ; code_block = IfStatement ; return id ; } catch ( SourceUnavailableException e ) { String exceptionMessage = "Cannot create catalog entry because source is unavailable: " ; LOGGER . info ( exceptionMessage , e ) ; throw new InternalServerErrorException ( exceptionMessage ) ; } catch ( InternalIngestException e ) { String exceptionMessage = "Error while storing entry in catalog: " ; LOGGER . info ( exceptionMessage , e ) ; throw new InternalServerErrorException ( exceptionMessage ) ; } catch ( MetacardCreationException | IngestException e ) { String errorMessage = "Error while storing entry in catalog: " ; LOGGER . info ( errorMessage , e ) ; throw new CatalogServiceException ( errorMessage ) ; } finally { IOUtils . closeQuietly ( message ) ; } }
public void test() { if ( INGEST_LOGGER . isInfoEnabled ( ) ) { INGEST_LOGGER . info ( "Entry successfully saved, id: {}" , id ) ; } }
public void test() { try { LOGGER . debug ( "POST" ) ; MimeType mimeType = getMimeType ( contentTypeList ) ; CreateResponse createResponse ; code_block = IfStatement ; String id = createResponse . getCreatedMetacards ( ) . get ( 0 ) . getId ( ) ; LOGGER . debug ( "Create Response id [{}]" , id ) ; LOGGER . debug ( "Entry successfully saved, id: {}" , id ) ; code_block = IfStatement ; return id ; } catch ( SourceUnavailableException e ) { String exceptionMessage = "Cannot create catalog entry because source is unavailable: " ; LOGGER . info ( exceptionMessage , e ) ; throw new InternalServerErrorException ( exceptionMessage ) ; } catch ( InternalIngestException e ) { String exceptionMessage = "Error while storing entry in catalog: " ; LOGGER . info ( exceptionMessage , e ) ; throw new InternalServerErrorException ( exceptionMessage ) ; } catch ( MetacardCreationException | IngestException e ) { String errorMessage = "Error while storing entry in catalog: " ; LOGGER . info ( errorMessage , e ) ; throw new CatalogServiceException ( errorMessage ) ; } finally { IOUtils . closeQuietly ( message ) ; } }
public void test() { try { disconnect ( sessionId , "Processing has stopped." ) ; } catch ( IOException e ) { logger . warn ( "Failed to disconnect session {} due to {}" , sessionId , e , e ) ; } }
@ Override public void saveTestSuiteResult ( ) { testSuite . refreshState ( ) ; LOGGER . info ( "save the results of the test suite to the table 'sakuli_suites'" ) ; MapSqlParameterSource tcParameters = getCompleteParameters ( ) ; LOGGER . debug ( "write the following values to 'sakuli_suites': " + tcParameters . getValues ( ) + " ==>  now execute ...." ) ; String sqlStatement = "UPDATE sakuli_suites " + createSqlSetStringForNamedParameter ( tcParameters . getValues ( ) ) + " where id=:id" ; LOGGER . debug ( "SQL-Statement for update 'sakuli_suites': " + sqlStatement ) ; int affectedRows = getNamedParameterJdbcTemplate ( ) . update ( sqlStatement , tcParameters ) ; LOGGER . info ( "update 'sakuli_suites' affected " + affectedRows + " rows" ) ; }
@ Override public void saveTestSuiteResult ( ) { testSuite . refreshState ( ) ; LOGGER . info ( "save the results of the test suite to the table 'sakuli_suites'" ) ; MapSqlParameterSource tcParameters = getCompleteParameters ( ) ; LOGGER . debug ( "write the following values to 'sakuli_suites': " + tcParameters . getValues ( ) + " ==>  now execute ...." ) ; String sqlStatement = "UPDATE sakuli_suites " + createSqlSetStringForNamedParameter ( tcParameters . getValues ( ) ) + " where id=:id" ; LOGGER . debug ( "SQL-Statement for update 'sakuli_suites': " + sqlStatement ) ; int affectedRows = getNamedParameterJdbcTemplate ( ) . update ( sqlStatement , tcParameters ) ; LOGGER . info ( "update 'sakuli_suites' affected " + affectedRows + " rows" ) ; }
@ Override public void saveTestSuiteResult ( ) { testSuite . refreshState ( ) ; LOGGER . info ( "save the results of the test suite to the table 'sakuli_suites'" ) ; MapSqlParameterSource tcParameters = getCompleteParameters ( ) ; LOGGER . debug ( "write the following values to 'sakuli_suites': " + tcParameters . getValues ( ) + " ==>  now execute ...." ) ; String sqlStatement = "UPDATE sakuli_suites " + createSqlSetStringForNamedParameter ( tcParameters . getValues ( ) ) + " where id=:id" ; LOGGER . debug ( "SQL-Statement for update 'sakuli_suites': " + sqlStatement ) ; int affectedRows = getNamedParameterJdbcTemplate ( ) . update ( sqlStatement , tcParameters ) ; LOGGER . info ( "update 'sakuli_suites' affected " + affectedRows + " rows" ) ; }
@ Override public void saveTestSuiteResult ( ) { testSuite . refreshState ( ) ; LOGGER . info ( "save the results of the test suite to the table 'sakuli_suites'" ) ; MapSqlParameterSource tcParameters = getCompleteParameters ( ) ; LOGGER . debug ( "write the following values to 'sakuli_suites': " + tcParameters . getValues ( ) + " ==>  now execute ...." ) ; String sqlStatement = "UPDATE sakuli_suites " + createSqlSetStringForNamedParameter ( tcParameters . getValues ( ) ) + " where id=:id" ; LOGGER . debug ( "SQL-Statement for update 'sakuli_suites': " + sqlStatement ) ; int affectedRows = getNamedParameterJdbcTemplate ( ) . update ( sqlStatement , tcParameters ) ; LOGGER . info ( "update 'sakuli_suites' affected " + affectedRows + " rows" ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( sql . toString ( ) ) ; } }
public void test() { if ( ! ( candidateEventType instanceof BaseXMLEventType ) ) { log . warn ( "Event type by name '" + eventTypeName + "' is not an XML event type for property '" + propertyName + "'" ) ; return null ; } }
public void test() { if ( ! ( candidateEventType instanceof BaseXMLEventType ) ) { log . warn ( "Event type by name '" + eventTypeName + "' is not an XML event type for property '" + propertyName + "'" ) ; return null ; } }
public void testSoftRefCache ( ) throws Exception { ICache < byte [ ] > c = CacheManager . inst . getCache ( "bytesCache" , byte [ ] . class , CacheType . SOFT_REFERENCE ) ; code_block = ForStatement ; log . debug ( "size: " + c . size ( ) ) ; code_block = ForStatement ; log . debug ( "size: " + c . size ( ) ) ; log . debug ( c ) ; log . debug ( CacheManager . inst ) ; }
public void testSoftRefCache ( ) throws Exception { ICache < byte [ ] > c = CacheManager . inst . getCache ( "bytesCache" , byte [ ] . class , CacheType . SOFT_REFERENCE ) ; code_block = ForStatement ; log . debug ( "size: " + c . size ( ) ) ; code_block = ForStatement ; log . debug ( "size: " + c . size ( ) ) ; log . debug ( c ) ; log . debug ( CacheManager . inst ) ; }
public void test() { if ( powerResult == null ) { logger . warn ( "ZDO_POWER_DESC_REQ FAILED on {}" , node ) ; } else { node . setPowerDescriptor ( new ZigBeeNodePowerDescriptor ( powerResult ) ) ; } }
public void test() { if ( isNew ) { logger . debug ( "Inspecting new node {}" , node ) ; ZDO_NODE_DESC_REQ nodeDescriptorReq = new ZDO_NODE_DESC_REQ ( nwkAddress . get16BitValue ( ) ) ; final ZDO_NODE_DESC_RSP nodeResult = driver . sendZDONodeDescriptionRequest ( nodeDescriptorReq ) ; code_block = IfStatement ; ZDO_POWER_DESC_REQ powerDescriptorReq = new ZDO_POWER_DESC_REQ ( nwkAddress . get16BitValue ( ) ) ; final ZDO_POWER_DESC_RSP powerResult = driver . sendZDOPowerDescriptionRequest ( powerDescriptorReq ) ; code_block = IfStatement ; correctlyInspected = inspectEndpointOfNode ( nwk , node ) ; code_block = IfStatement ; } else { logger . trace ( "Inspecting existing node {}" , node ) ; code_block = IfStatement ; } }
public void test() { if ( nodeResult == null ) { logger . warn ( "ZDO_NODE_DESC_REQ FAILED on {}" , node ) ; } else { node . setNodeDescriptor ( new ZigBeeNodeDescriptor ( nodeResult ) ) ; code_block = IfStatement ; code_block = IfStatement ; } }
public void test() { if ( powerResult == null ) { logger . warn ( "ZDO_POWER_DESC_REQ FAILED on {}" , node ) ; } else { node . setPowerDescriptor ( new ZigBeeNodePowerDescriptor ( powerResult ) ) ; } }
public void test() { if ( isNew ) { logger . debug ( "Inspecting new node {}" , node ) ; ZDO_NODE_DESC_REQ nodeDescriptorReq = new ZDO_NODE_DESC_REQ ( nwkAddress . get16BitValue ( ) ) ; final ZDO_NODE_DESC_RSP nodeResult = driver . sendZDONodeDescriptionRequest ( nodeDescriptorReq ) ; code_block = IfStatement ; ZDO_POWER_DESC_REQ powerDescriptorReq = new ZDO_POWER_DESC_REQ ( nwkAddress . get16BitValue ( ) ) ; final ZDO_POWER_DESC_RSP powerResult = driver . sendZDOPowerDescriptionRequest ( powerDescriptorReq ) ; code_block = IfStatement ; correctlyInspected = inspectEndpointOfNode ( nwk , node ) ; code_block = IfStatement ; } else { logger . trace ( "Inspecting existing node {}" , node ) ; code_block = IfStatement ; } }
public void test() { try { KeyValue keyValue = parser . parse ( ) ; Assert . fail ( ) ; } catch ( OracleConnectionStringException e ) { logger . info ( "Expected error" , e ) ; } }
public static void main ( String [ ] args ) throws Exception { Preconditions . checkArgument ( args . length == 5 , "Illegal number of arguments. Expected: 5, Actual: " + args . length ) ; String stormId = args [ 0 ] ; String assignmentId = args [ 1 ] ; String supervisorPort = args [ 2 ] ; String portStr = args [ 3 ] ; String workerId = args [ 4 ] ; Map < String , Object > conf = ConfigUtils . readStormConfig ( ) ; Utils . setupWorkerUncaughtExceptionHandler ( ) ; StormCommon . validateDistributedMode ( conf ) ; int supervisorPortInt = Integer . parseInt ( supervisorPort ) ; Worker worker = new Worker ( conf , null , stormId , assignmentId , supervisorPortInt , Integer . parseInt ( portStr ) , workerId ) ; int workerShutdownSleepSecs = ObjectReader . getInt ( conf . get ( Config . SUPERVISOR_WORKER_SHUTDOWN_SLEEP_SECS ) ) ; LOG . info ( "Adding shutdown hook with kill in {} secs" , workerShutdownSleepSecs ) ; Utils . addShutdownHookWithDelayedForceKill ( worker :: shutdown , workerShutdownSleepSecs ) ; worker . start ( ) ; }
@ Override public boolean waitForChannelToReachAgi ( long timeout , TimeUnit timeunit ) throws InterruptedException { logger . info ( "Waiting for channel to reach agi " + this ) ; boolean tmp = hasReachedAgi . await ( timeout , timeunit ) ; logger . info ( "Result of waiting for channel to reach agi " + this + " " + tmp ) ; return tmp ; }
@ Override public boolean waitForChannelToReachAgi ( long timeout , TimeUnit timeunit ) throws InterruptedException { logger . info ( "Waiting for channel to reach agi " + this ) ; boolean tmp = hasReachedAgi . await ( timeout , timeunit ) ; logger . info ( "Result of waiting for channel to reach agi " + this + " " + tmp ) ; return tmp ; }
public void test() { try { service = applicationContext . getBean ( defName ) ; } catch ( Throwable t ) { logger . error ( "error in hasReferencingObjects" , t ) ; service = null ; } }
public void test() { try { String [ ] defNames = applicationContext . getBeanNamesForType ( PageUtilizer . class ) ; code_block = ForStatement ; } catch ( ApsSystemException ex ) { logger . error ( "error loading references for page {}" , page . getCode ( ) , ex ) ; throw new RestServerError ( "error in getReferencingObjects " , ex ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( String . format ( "Cache %s for file %s" , id , file ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( getString ( "Tracing.4" , introspectedColumn . getActualColumnName ( ) , entry . getKey ( ) . toString ( ) ) ) ; } }
public static synchronized Cipher getCipher ( String transformation ) throws NoSuchAlgorithmException , NoSuchPaddingException , NoSuchProviderException { Cipher result = Cipher . getInstance ( transformation ) ; LOG . trace ( "getCipher({}) -> {}" , transformation , safeClassname ( result ) ) ; return result ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( searchException , searchException ) ; } }
public void test() { -> { logger . warn ( "Interrupting test, it is taking too long{}Server:{}{}{}Client:{}{}" , System . lineSeparator ( ) , System . lineSeparator ( ) , server . dump ( ) , System . lineSeparator ( ) , System . lineSeparator ( ) , client . dump ( ) ) ; testThread . interrupt ( ) ; } }
public void test() { try { CountDownLatch latch = new CountDownLatch ( iterations ) ; long factor = ( logger . isDebugEnabled ( ) ? 25 : 1 ) * 100 ; Thread testThread = Thread . currentThread ( ) ; Scheduler . Task task = client . getScheduler ( ) . schedule ( ( ) code_block = LoopStatement ; , iterations * factor , TimeUnit . MILLISECONDS ) ; long successes = 0 ; long begin = System . nanoTime ( ) ; code_block = ForStatement ; assertTrue ( latch . await ( iterations , TimeUnit . SECONDS ) ) ; long end = System . nanoTime ( ) ; assertThat ( successes , Matchers . greaterThan ( 0L ) ) ; task . cancel ( ) ; long elapsed = TimeUnit . NANOSECONDS . toMillis ( end - begin ) ; logger . info ( "{} requests in {} ms, {}/{} success/failure, {} req/s" , iterations , elapsed , successes , iterations - successes , elapsed > 0 ? iterations * 1000L / elapsed : - 1 ) ; return true ; } catch ( Exception x ) { x . printStackTrace ( ) ; return false ; } }
public void test() { try { this . indexedFields = this . helper . getIndexedFields ( config . getDatatypeFilter ( ) ) ; } catch ( Exception ex ) { log . error ( "Could not determine whether a field is indexed" , ex ) ; throw new RuntimeException ( "got exception when using MetadataHelper to get indexed fields " , ex ) ; } }
public static void main ( String [ ] args ) throws Exception { Logging . initialize ( ) ; DistributedQueryRunner queryRunner = createRedisQueryRunner ( new RedisServer ( ) , ImmutableMap . of ( "http-server.http.port" , "8080" ) , "string" , TpchTable . getTables ( ) ) ; Logger log = Logger . get ( RedisQueryRunner . class ) ; log . info ( "======== SERVER STARTED ========" ) ; log . info ( "\n====\n%s\n====" , queryRunner . getCoordinator ( ) . getBaseUrl ( ) ) ; }
public static void main ( String [ ] args ) throws Exception { Logging . initialize ( ) ; DistributedQueryRunner queryRunner = createRedisQueryRunner ( new RedisServer ( ) , ImmutableMap . of ( "http-server.http.port" , "8080" ) , "string" , TpchTable . getTables ( ) ) ; Logger log = Logger . get ( RedisQueryRunner . class ) ; log . info ( "======== SERVER STARTED ========" ) ; log . info ( "\n====\n%s\n====" , queryRunner . getCoordinator ( ) . getBaseUrl ( ) ) ; }
@ Override public void clearCompositePhenomenonsForObservableProperty ( String observableProperty ) { LOG . trace ( "Clearing composite phenomenon for observable property {}" , observableProperty ) ; this . compositePhenomenonsForObservableProperty . remove ( observableProperty ) ; }
public void test() { try { com . liferay . portal . kernel . model . Group returnValue = GroupServiceUtil . getUserGroup ( companyId , userId ) ; return com . liferay . portal . kernel . model . GroupSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Removed Principal " + principal . getName ( ) ) ; } }
public void test() { -> { LOGGER . trace ( "Remove from scene graph view of module: " + model . getActiveModule ( ) ) ; view . removeView ( openModuleViews . get ( module ) ) ; openModuleViews . remove ( module ) ; } }
public void attachDirty ( StgMbGefaehrskatTxt instance ) { log . debug ( "attaching dirty StgMbGefaehrskatTxt instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { ConfigInfo oldConfigInfo = findConfigInfo ( configInfo . getDataId ( ) , configInfo . getGroup ( ) , configInfo . getTenant ( ) ) ; String appNameTmp = oldConfigInfo . getAppName ( ) ; code_block = IfStatement ; int rows = updateConfigInfoAtomicCas ( configInfo , srcIp , srcUser , time , configAdvanceInfo ) ; code_block = IfStatement ; String configTags = configAdvanceInfo == null ? null : ( String ) configAdvanceInfo . get ( "config_tags" ) ; code_block = IfStatement ; insertConfigHistoryAtomic ( oldConfigInfo . getId ( ) , oldConfigInfo , srcIp , srcUser , time , "U" ) ; } catch ( CannotGetJdbcConnectionException e ) { LogUtil . FATAL_LOG . error ( "[db-error] " + e . toString ( ) , e ) ; throw e ; } }
public void test() { try ( FileOutputStream stream = new FileOutputStream ( taskFile ) ) { stream . write ( errorMessage . getBytes ( ) ) ; code_block = IfStatement ; stream . close ( ) ; } catch ( IOException ioe ) { WORKER_LOGGER . error ( "IOException writing worker " + streamName + " file: " + taskFile , ioe ) ; } }
public void test() { if ( ! overwrite && getBackendConnector ( ) . isBugExisting ( vulnId ) ) { log . info ( "Bug [{}] already exists in backend, analysis will be skipped" , vulnId ) ; return ; } }
public void test() { for ( ConstructChange chg : changes ) { log . info ( chg . toString ( ) ) ; } }
private void shutdownDroplet ( Exchange exchange ) throws Exception { Action action = getEndpoint ( ) . getDigitalOceanClient ( ) . shutdownDroplet ( dropletId ) ; LOG . trace ( "Shutdown Droplet {} : [{}] " , dropletId , action ) ; exchange . getMessage ( ) . setBody ( action ) ; }
private void buildCluster ( ) { LOG . info ( "Creating the embedded Kafka connect instance" ) ; EmbeddedConnectCluster . Builder builder = new EmbeddedConnectCluster . Builder ( ) ; Properties brokerProps = new Properties ( ) ; brokerProps . put ( "auto.create.topics.enable" , String . valueOf ( true ) ) ; Map < String , String > workerProps = new HashMap < > ( ) ; workerProps . put ( WorkerConfig . OFFSET_COMMIT_INTERVAL_MS_CONFIG , String . valueOf ( OFFSET_COMMIT_INTERVAL_MS ) ) ; String address = "http://localhost:" + NetworkUtils . getFreePort ( ) ; LOG . info ( "Using the following address for  the listener configuration: {}" , address ) ; workerProps . put ( WorkerConfig . LISTENERS_CONFIG , address ) ; String pluginPaths = PluginPathHelper . getInstance ( ) . pluginPaths ( ) ; LOG . info ( "Adding the returned directories to the plugin path. This may take A VERY long time to complete" ) ; workerProps . put ( WorkerConfig . PLUGIN_PATH_CONFIG , pluginPaths ) ; LOG . info ( "Building the embedded Kafka connect instance" ) ; this . cluster = builder . name ( "connect-cluster" ) . numWorkers ( 1 ) . numBrokers ( 1 ) . brokerProps ( brokerProps ) . workerProps ( workerProps ) . maskExitProcedures ( true ) . build ( ) ; LOG . info ( "Built the embedded Kafka connect instance" ) ; }
private void buildCluster ( ) { LOG . info ( "Creating the embedded Kafka connect instance" ) ; EmbeddedConnectCluster . Builder builder = new EmbeddedConnectCluster . Builder ( ) ; Properties brokerProps = new Properties ( ) ; brokerProps . put ( "auto.create.topics.enable" , String . valueOf ( true ) ) ; Map < String , String > workerProps = new HashMap < > ( ) ; workerProps . put ( WorkerConfig . OFFSET_COMMIT_INTERVAL_MS_CONFIG , String . valueOf ( OFFSET_COMMIT_INTERVAL_MS ) ) ; String address = "http://localhost:" + NetworkUtils . getFreePort ( ) ; LOG . info ( "Using the following address for  the listener configuration: {}" , address ) ; workerProps . put ( WorkerConfig . LISTENERS_CONFIG , address ) ; String pluginPaths = PluginPathHelper . getInstance ( ) . pluginPaths ( ) ; LOG . info ( "Adding the returned directories to the plugin path. This may take A VERY long time to complete" ) ; workerProps . put ( WorkerConfig . PLUGIN_PATH_CONFIG , pluginPaths ) ; LOG . info ( "Building the embedded Kafka connect instance" ) ; this . cluster = builder . name ( "connect-cluster" ) . numWorkers ( 1 ) . numBrokers ( 1 ) . brokerProps ( brokerProps ) . workerProps ( workerProps ) . maskExitProcedures ( true ) . build ( ) ; LOG . info ( "Built the embedded Kafka connect instance" ) ; }
private void buildCluster ( ) { LOG . info ( "Creating the embedded Kafka connect instance" ) ; EmbeddedConnectCluster . Builder builder = new EmbeddedConnectCluster . Builder ( ) ; Properties brokerProps = new Properties ( ) ; brokerProps . put ( "auto.create.topics.enable" , String . valueOf ( true ) ) ; Map < String , String > workerProps = new HashMap < > ( ) ; workerProps . put ( WorkerConfig . OFFSET_COMMIT_INTERVAL_MS_CONFIG , String . valueOf ( OFFSET_COMMIT_INTERVAL_MS ) ) ; String address = "http://localhost:" + NetworkUtils . getFreePort ( ) ; LOG . info ( "Using the following address for  the listener configuration: {}" , address ) ; workerProps . put ( WorkerConfig . LISTENERS_CONFIG , address ) ; String pluginPaths = PluginPathHelper . getInstance ( ) . pluginPaths ( ) ; LOG . info ( "Adding the returned directories to the plugin path. This may take A VERY long time to complete" ) ; workerProps . put ( WorkerConfig . PLUGIN_PATH_CONFIG , pluginPaths ) ; LOG . info ( "Building the embedded Kafka connect instance" ) ; this . cluster = builder . name ( "connect-cluster" ) . numWorkers ( 1 ) . numBrokers ( 1 ) . brokerProps ( brokerProps ) . workerProps ( workerProps ) . maskExitProcedures ( true ) . build ( ) ; LOG . info ( "Built the embedded Kafka connect instance" ) ; }
private void buildCluster ( ) { LOG . info ( "Creating the embedded Kafka connect instance" ) ; EmbeddedConnectCluster . Builder builder = new EmbeddedConnectCluster . Builder ( ) ; Properties brokerProps = new Properties ( ) ; brokerProps . put ( "auto.create.topics.enable" , String . valueOf ( true ) ) ; Map < String , String > workerProps = new HashMap < > ( ) ; workerProps . put ( WorkerConfig . OFFSET_COMMIT_INTERVAL_MS_CONFIG , String . valueOf ( OFFSET_COMMIT_INTERVAL_MS ) ) ; String address = "http://localhost:" + NetworkUtils . getFreePort ( ) ; LOG . info ( "Using the following address for  the listener configuration: {}" , address ) ; workerProps . put ( WorkerConfig . LISTENERS_CONFIG , address ) ; String pluginPaths = PluginPathHelper . getInstance ( ) . pluginPaths ( ) ; LOG . info ( "Adding the returned directories to the plugin path. This may take A VERY long time to complete" ) ; workerProps . put ( WorkerConfig . PLUGIN_PATH_CONFIG , pluginPaths ) ; LOG . info ( "Building the embedded Kafka connect instance" ) ; this . cluster = builder . name ( "connect-cluster" ) . numWorkers ( 1 ) . numBrokers ( 1 ) . brokerProps ( brokerProps ) . workerProps ( workerProps ) . maskExitProcedures ( true ) . build ( ) ; LOG . info ( "Built the embedded Kafka connect instance" ) ; }
private void buildCluster ( ) { LOG . info ( "Creating the embedded Kafka connect instance" ) ; EmbeddedConnectCluster . Builder builder = new EmbeddedConnectCluster . Builder ( ) ; Properties brokerProps = new Properties ( ) ; brokerProps . put ( "auto.create.topics.enable" , String . valueOf ( true ) ) ; Map < String , String > workerProps = new HashMap < > ( ) ; workerProps . put ( WorkerConfig . OFFSET_COMMIT_INTERVAL_MS_CONFIG , String . valueOf ( OFFSET_COMMIT_INTERVAL_MS ) ) ; String address = "http://localhost:" + NetworkUtils . getFreePort ( ) ; LOG . info ( "Using the following address for  the listener configuration: {}" , address ) ; workerProps . put ( WorkerConfig . LISTENERS_CONFIG , address ) ; String pluginPaths = PluginPathHelper . getInstance ( ) . pluginPaths ( ) ; LOG . info ( "Adding the returned directories to the plugin path. This may take A VERY long time to complete" ) ; workerProps . put ( WorkerConfig . PLUGIN_PATH_CONFIG , pluginPaths ) ; LOG . info ( "Building the embedded Kafka connect instance" ) ; this . cluster = builder . name ( "connect-cluster" ) . numWorkers ( 1 ) . numBrokers ( 1 ) . brokerProps ( brokerProps ) . workerProps ( workerProps ) . maskExitProcedures ( true ) . build ( ) ; LOG . info ( "Built the embedded Kafka connect instance" ) ; }
private void prepareGetResourceConfigurations ( ) throws Exception { final String complexResourceJSONString = DMPPersistenceUtil . getResourceAsString ( "complex_resource.json" ) ; final Resource expectedComplexResource = objectMapper . readValue ( complexResourceJSONString , Resource . class ) ; Assert . assertNotNull ( "the complex resource shouldn't be null" , expectedComplexResource ) ; Assert . assertNotNull ( "the name of the complex resource shouldn't be null" , expectedComplexResource . getName ( ) ) ; Assert . assertNotNull ( "the description of the complex resource shouldn't be null" , expectedComplexResource . getDescription ( ) ) ; Assert . assertNotNull ( "the type of the complex resource shouldn't be null" , expectedComplexResource . getType ( ) ) ; Assert . assertNotNull ( "the attributes of the complex resource shouldn't be null" , expectedComplexResource . getAttributes ( ) ) ; Assert . assertNotNull ( "the configurations of the complex resource shouldn't be null" , expectedComplexResource . getConfigurations ( ) ) ; Assert . assertFalse ( "the configurations of the complex resource shouldn't be empty" , expectedComplexResource . getConfigurations ( ) . isEmpty ( ) ) ; final ResourceServiceTestUtils resourceServiceTestUtils = resourcesResourceTestUtils . getPersistenceServiceTestUtils ( ) ; Resource complexResource = resourceServiceTestUtils . getJpaService ( ) . createObjectTransactional ( ) . getObject ( ) ; final Set < Configuration > createdConfigurations = Sets . newLinkedHashSet ( ) ; final ConfigurationServiceTestUtils configurationServiceTestUtils = resourceServiceTestUtils . getConfigurationsServiceTestUtils ( ) ; code_block = ForStatement ; complexResource . setName ( expectedComplexResource . getName ( ) ) ; complexResource . setDescription ( expectedComplexResource . getDescription ( ) ) ; complexResource . setType ( expectedComplexResource . getType ( ) ) ; complexResource . setAttributes ( expectedComplexResource . getAttributes ( ) ) ; final Resource updatedComplexResource = resourceServiceTestUtils . updateAndCompareObject ( complexResource , complexResource ) ; Assert . assertNotNull ( "updated resource shouldn't be null" , updatedComplexResource ) ; Assert . assertNotNull ( "updated resource id shouldn't be null" , updatedComplexResource . getUuid ( ) ) ; ResourcesResourceTest . LOG . debug ( "try to retrieve configurations of resource '{}'" , updatedComplexResource . getUuid ( ) ) ; actualResource = updatedComplexResource ; expectedResource = actualResource ; exceptedConfigurations = createdConfigurations ; }
@ Test public void happyPath ( ) throws IOException { logStart ( ) ; String updateAAIGenericVnfRequest = FileUtil . readResourceFile ( "__files/VfModularity/UpdateAAIGenericVnfRequest.xml" ) ; MockGetGenericVnfByIdWithDepth ( wireMockServer , "skask" , 1 , "VfModularity/GenericVnf.xml" ) ; MockPutGenericVnf ( wireMockServer , "/skask" , 200 ) ; MockPatchGenericVnf ( wireMockServer , "skask" ) ; String businessKey = UUID . randomUUID ( ) . toString ( ) ; Map < String , Object > variables = new HashMap < > ( ) ; variables . put ( "mso-request-id" , UUID . randomUUID ( ) . toString ( ) ) ; variables . put ( "isDebugLogEnabled" , "true" ) ; variables . put ( "UpdateAAIGenericVnfRequest" , updateAAIGenericVnfRequest ) ; invokeSubProcess ( "UpdateAAIGenericVnf" , businessKey , variables ) ; Assert . assertTrue ( isProcessEnded ( businessKey ) ) ; String response = ( String ) getVariableFromHistory ( businessKey , "UAAIGenVnf_updateGenericVnfResponse" ) ; Integer responseCode = ( Integer ) getVariableFromHistory ( businessKey , "UAAIGenVnf_updateGenericVnfResponseCode" ) ; logger . debug ( "Subflow response code: {}" , responseCode ) ; logger . debug ( "Subflow response: {}" , response ) ; Assert . assertEquals ( 200 , responseCode . intValue ( ) ) ; logEnd ( ) ; }
@ Test public void happyPath ( ) throws IOException { logStart ( ) ; String updateAAIGenericVnfRequest = FileUtil . readResourceFile ( "__files/VfModularity/UpdateAAIGenericVnfRequest.xml" ) ; MockGetGenericVnfByIdWithDepth ( wireMockServer , "skask" , 1 , "VfModularity/GenericVnf.xml" ) ; MockPutGenericVnf ( wireMockServer , "/skask" , 200 ) ; MockPatchGenericVnf ( wireMockServer , "skask" ) ; String businessKey = UUID . randomUUID ( ) . toString ( ) ; Map < String , Object > variables = new HashMap < > ( ) ; variables . put ( "mso-request-id" , UUID . randomUUID ( ) . toString ( ) ) ; variables . put ( "isDebugLogEnabled" , "true" ) ; variables . put ( "UpdateAAIGenericVnfRequest" , updateAAIGenericVnfRequest ) ; invokeSubProcess ( "UpdateAAIGenericVnf" , businessKey , variables ) ; Assert . assertTrue ( isProcessEnded ( businessKey ) ) ; String response = ( String ) getVariableFromHistory ( businessKey , "UAAIGenVnf_updateGenericVnfResponse" ) ; Integer responseCode = ( Integer ) getVariableFromHistory ( businessKey , "UAAIGenVnf_updateGenericVnfResponseCode" ) ; logger . debug ( "Subflow response code: {}" , responseCode ) ; logger . debug ( "Subflow response: {}" , response ) ; Assert . assertEquals ( 200 , responseCode . intValue ( ) ) ; logEnd ( ) ; }
public void test() { try { return new BufferedInputStream ( fs . open ( new Path ( filePath ) ) ) ; } catch ( IOException e ) { logger . error ( "Failed to get buffered input stream for {}. " , filePath , e ) ; return null ; } }
synchronized void reset ( ) throws Exception { log . debug ( "reset" ) ; instanceIndex . incrementAndGet ( ) ; isConnected . set ( false ) ; connectionStartMs = System . currentTimeMillis ( ) ; handleHolder . closeAndReset ( ) ; handleHolder . getZooKeeper ( ) ; }
public void test() { try { MDC . clear ( ) ; loop ( ) ; } catch ( InterruptedException e ) { log . info ( "Interrupted. Stopping loop." ) ; Thread . currentThread ( ) . interrupt ( ) ; return ; } catch ( Throwable e ) { log . error ( "Error" , e ) ; throw e ; } }
public void test() { try { MDC . clear ( ) ; loop ( ) ; } catch ( InterruptedException e ) { log . info ( "Interrupted. Stopping loop." ) ; Thread . currentThread ( ) . interrupt ( ) ; return ; } catch ( Throwable e ) { log . error ( "Error" , e ) ; throw e ; } }
@ Bean public IamExternalRestClientFactory iamRestClientFactory ( ) { LOGGER . debug ( "Iam client factory: {}" , iamClientProperties ) ; return new IamExternalRestClientFactory ( iamClientProperties , restTemplateBuilder ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "got object to update with ID: " + knownResourceId ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "to be updated object" ) ; logger . debug ( objectAsXmlString ( conservationCommon , ConservationCommon . class ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( testName + ": status = " + statusCode ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "UTF-8 data sent=" + conservationCommon . getFabricationNote ( ) + "\n" + "UTF-8 data received=" + updatedConservationCommon . getFabricationNote ( ) ) ; } }
public void test() { case tests the Group Management APIs" ) public void GroupManagementTest ( ) throws AutomationFrameworkException { URL url = Thread . currentThread ( ) . getContextClassLoader ( ) . getResource ( "jmeter-scripts" + File . separator + "GroupManagementAPI.jmx" ) ; JMeterTest script = new JMeterTest ( new File ( url . getPath ( ) ) ) ; JMeterTestManager manager = new JMeterTestManager ( ) ; log . info ( "Running group management api test cases using jmeter scripts" ) ; manager . runTest ( script ) ; } }
public void test() { if ( isValidLeader ( expectedLeaderId ) ) { code_block = IfStatement ; } else { LOG . trace ( "Ignore forwarding '{}' because the leadership runner is no longer the valid leader for {}." , forwardDescription , expectedLeaderId ) ; } }
public void test() { try { disruptorExec . awaitTermination ( 3 , SECONDS ) ; LOG . info ( "\tReply Processor Disruptor executor shutdown" ) ; } catch ( InterruptedException e ) { LOG . error ( "Interrupted whilst finishing Reply Processor Disruptor executor" ) ; Thread . currentThread ( ) . interrupt ( ) ; } }
public void attachDirty ( DtsPackageTxt instance ) { log . debug ( "attaching dirty DtsPackageTxt instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "addBinding(" + binding + ") being called" ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "Adding binding " + binding + " into " + this + " bindingTable: " + debugBindings ( ) ) ; } }
public void test() { try { MigrationReportInstance reportInstance = processAdminServiceBase . migrateProcessInstance ( containerId , processInstanceId , targetContainerId , targetProcessId , payload , type ) ; return createCorrectVariant ( reportInstance , headers , Response . Status . CREATED , conversationIdHeader ) ; } catch ( ProcessInstanceNotFoundException e ) { return notFound ( MessageFormat . format ( PROCESS_INSTANCE_NOT_FOUND , processInstanceId ) , v , conversationIdHeader ) ; } catch ( DeploymentNotFoundException e ) { return notFound ( MessageFormat . format ( CONTAINER_NOT_FOUND , containerId ) , v , conversationIdHeader ) ; } catch ( Exception e ) { logger . error ( "Unexpected error during processing {}" , e . getMessage ( ) , e ) ; return internalServerError ( errorMessage ( e ) , v , conversationIdHeader ) ; } }
public void test() { try { InetSocketAddress address = NetUtils . createSocketAddr ( queryMasterHostAndPort ) ; ExecutionBlockContextRequest . Builder request = ExecutionBlockContextRequest . newBuilder ( ) ; request . setExecutionBlockId ( executionBlockId . getProto ( ) ) . setWorker ( getWorkerContext ( ) . getConnectionInfo ( ) . getProto ( ) ) ; client = RpcClientManager . getInstance ( ) . newClient ( address , QueryMasterProtocol . class , true , rpcParams ) ; QueryMasterProtocol . QueryMasterProtocolService . Interface stub = client . getStub ( ) ; CallFuture < ExecutionBlockContextResponse > callback = new CallFuture < > ( ) ; stub . getExecutionBlockContext ( callback . getController ( ) , request . build ( ) , callback ) ; ExecutionBlockContextResponse contextProto = callback . get ( ) ; ExecutionBlockContext context = new ExecutionBlockContext ( getWorkerContext ( ) , contextProto , client , pullServerService ) ; context . init ( ) ; return context ; } catch ( Throwable e ) { RpcClientManager . cleanup ( client ) ; LOG . fatal ( e . getMessage ( ) , e ) ; throw new RuntimeException ( e ) ; } }
public void test() { try { inProgressKBCommentsCount = _kbSuggestionListDisplayContext . getInProgressKBCommentsCount ( ) ; } catch ( PortalException portalException ) { _log . error ( "Unable to obtain in progress knowledge base comments count " + "for  group " + _kbSuggestionListDisplayContext . getGroupId ( ) , portalException ) ; } }
@ Test public void testQuery ( ) throws Exception { CreationTools . createJobDef ( null , true , "pyl.KillMe" , null , "jqm-tests/jqm-test-pyl/target/test.jar" , TestHelpers . qNormal , 42 , "jqm-test-kill" , null , "Franquin" , "ModuleMachin" , "other" , "other" , false , cnx ) ; cnx . commit ( ) ; JqmClientFactory . getClient ( ) . enqueue ( "jqm-test-kill" , "test" ) ; JqmClientFactory . getClient ( ) . enqueue ( "jqm-test-kill" , "test" ) ; JqmClientFactory . getClient ( ) . enqueue ( "jqm-test-kill" , "test" ) ; JqmClientFactory . getClient ( ) . enqueue ( "jqm-test-kill" , "test" ) ; JqmClientFactory . getClient ( ) . enqueue ( "jqm-test-kill" , "test" ) ; jqmlogger . debug ( "COUNT RUNNING " + cnx . runSelectSingle ( "ji_select_count_running" , Integer . class ) ) ; jqmlogger . debug ( "COUNT ALL     " + cnx . runSelectSingle ( "ji_select_count_all" , Integer . class ) ) ; Assert . assertEquals ( 0 , Query . create ( ) . setQueryLiveInstances ( true ) . setQueryHistoryInstances ( false ) . addStatusFilter ( com . enioka . jqm . api . State . RUNNING ) . addStatusFilter ( com . enioka . jqm . api . State . ENDED ) . run ( ) . size ( ) ) ; Assert . assertEquals ( 5 , Query . create ( ) . setQueryLiveInstances ( true ) . setQueryHistoryInstances ( false ) . addStatusFilter ( com . enioka . jqm . api . State . SUBMITTED ) . run ( ) . size ( ) ) ; }
@ Test public void testQuery ( ) throws Exception { CreationTools . createJobDef ( null , true , "pyl.KillMe" , null , "jqm-tests/jqm-test-pyl/target/test.jar" , TestHelpers . qNormal , 42 , "jqm-test-kill" , null , "Franquin" , "ModuleMachin" , "other" , "other" , false , cnx ) ; cnx . commit ( ) ; JqmClientFactory . getClient ( ) . enqueue ( "jqm-test-kill" , "test" ) ; JqmClientFactory . getClient ( ) . enqueue ( "jqm-test-kill" , "test" ) ; JqmClientFactory . getClient ( ) . enqueue ( "jqm-test-kill" , "test" ) ; JqmClientFactory . getClient ( ) . enqueue ( "jqm-test-kill" , "test" ) ; JqmClientFactory . getClient ( ) . enqueue ( "jqm-test-kill" , "test" ) ; jqmlogger . debug ( "COUNT RUNNING " + cnx . runSelectSingle ( "ji_select_count_running" , Integer . class ) ) ; jqmlogger . debug ( "COUNT ALL     " + cnx . runSelectSingle ( "ji_select_count_all" , Integer . class ) ) ; Assert . assertEquals ( 0 , Query . create ( ) . setQueryLiveInstances ( true ) . setQueryHistoryInstances ( false ) . addStatusFilter ( com . enioka . jqm . api . State . RUNNING ) . addStatusFilter ( com . enioka . jqm . api . State . ENDED ) . run ( ) . size ( ) ) ; Assert . assertEquals ( 5 , Query . create ( ) . setQueryLiveInstances ( true ) . setQueryHistoryInstances ( false ) . addStatusFilter ( com . enioka . jqm . api . State . SUBMITTED ) . run ( ) . size ( ) ) ; }
private static void pre ( ) { log . info ( TIPS ) ; PropertyUtils . init ( ) ; }
public void test() { try { writeFileHandle . close ( ) ; } catch ( IOException e ) { LOG . warn ( "Unable to close " + file , e ) ; } }
public void test() { try { User user = SecurityPasswordCreationContentPanel . this . getModelObject ( ) ; securityManagementService . updatePassword ( user , passwordModel . getObject ( ) ) ; Session . get ( ) . success ( getString ( "security.password.creation.validate.success" ) ) ; throw LoginSuccessPage . linkDescriptor ( ) . newRestartResponseException ( ) ; } catch ( RestartResponseException e ) { throw e ; } catch ( Exception e ) { LOGGER . error ( "Error occurred while creating password" , e ) ; Session . get ( ) . error ( getString ( "common.error.unexpected" ) ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( _NO_SUCH_ENTITY_WITH_PRIMARY_KEY + primaryKey ) ; } }
@ OnMessage public void echo ( Session session , String message ) throws Exception { LOGGER . info ( "OnMessage called '" + message + "'" ) ; session . getBasicRemote ( ) . sendObject ( message ) ; verifyRunningThread ( session , LOGGER ) ; }
private static void logIndentOptions ( @ Nonnull PsiFile file , @ Nonnull FileIndentOptionsProvider provider , @ Nonnull IndentOptions options ) { LOG . debug ( "Indent options returned by " + provider . getClass ( ) . getName ( ) + " to " + file . getName ( ) + ": indent size=" + options . INDENT_SIZE + ", use tabs=" + options . USE_TAB_CHARACTER + ", tab size=" + options . TAB_SIZE ) ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( sb . toString ( ) ) ; } }
public void test() { if ( codeSource == null ) { logger . debug ( "Skipping registration of service providers to " + ExchangeAttributeBuilder . class ) ; return ; } }
public void test() { if ( ! Files . exists ( serviceDescriptorFilePath ) ) { logger . debug ( "Skipping registration of service providers to " + ExchangeAttributeBuilder . class + " since no service descriptor file found" ) ; return ; } }
public void test() { try { com . liferay . segments . model . SegmentsEntry returnValue = SegmentsEntryServiceUtil . getSegmentsEntry ( segmentsEntryId ) ; return com . liferay . segments . model . SegmentsEntrySoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
@ Override public Model getConciseBoundedDescription ( String resource , int depth , boolean withTypesForLeafs ) { logger . debug ( "computing CBD of depth {} for {} ..." , resource , depth ) ; Model cbd = ModelFactory . createDefaultModel ( ) ; cbd . add ( getIncomingModel ( resource , depth ) ) ; cbd . add ( getOutgoingModel ( resource , depth ) ) ; logger . debug ( "CBD size: {}" , cbd . size ( ) ) ; return cbd ; }
@ Override public Model getConciseBoundedDescription ( String resource , int depth , boolean withTypesForLeafs ) { logger . debug ( "computing CBD of depth {} for {} ..." , resource , depth ) ; Model cbd = ModelFactory . createDefaultModel ( ) ; cbd . add ( getIncomingModel ( resource , depth ) ) ; cbd . add ( getOutgoingModel ( resource , depth ) ) ; logger . debug ( "CBD size: {}" , cbd . size ( ) ) ; return cbd ; }
public void test() { if ( s_logger . isTraceEnabled ( ) ) { s_logger . trace ( "Rollback called to " + _name + " when there's no transaction: " + buildName ( ) ) ; } }
public void test() { if ( s_logger . isDebugEnabled ( ) ) { s_logger . debug ( "Rolling back the transaction: Time = " + ( System . currentTimeMillis ( ) - _txnTime ) + " Name =  " + _name + "; called by " + buildName ( ) ) ; } }
public void test() { try { code_block = IfStatement ; clearLockTimes ( ) ; closeConnection ( ) ; } catch ( final SQLException e ) { s_logger . warn ( "Unable to rollback" , e ) ; } }
public void test() { { log . info ( "Exporting address list." ) ; final List < AddressDO > list = getList ( ) ; final byte [ ] xls = addressCampaignValueExport . export ( list , personalAddressMap , addressCampaignValueMap , form . getSearchFilter ( ) . getAddressCampaign ( ) != null ? form . getSearchFilter ( ) . getAddressCampaign ( ) . getTitle ( ) : "" ) ; code_block = IfStatement ; final String filename = "ProjectForge-AddressCampaignValueExport_" + DateHelper . getDateAsFilenameSuffix ( new Date ( ) ) + ".xlsx" ; DownloadUtils . setDownloadTarget ( xls , filename ) ; } }
public void test() { try { @ SuppressWarnings ( "unchecked" ) List < SyndEntry > entries = ( List < SyndEntry > ) feed . getEntries ( ) ; code_block = ForStatement ; } catch ( IllegalArgumentException e ) { log . error ( e . getMessage ( ) , e ) ; } }
@ Override public Integer getSerial ( ) { logger . debug ( "Getting Serial: " , super . getSerial ( ) ) ; return super . getSerial ( ) ; }
private List < BigQuerySplit > readFromBigQuery ( TableId tableId , Optional < List < ColumnHandle > > projectedColumns , int actualParallelism , Optional < String > filter ) { log . debug ( "readFromBigQuery(tableId=%s, projectedColumns=%s, actualParallelism=%s, filter=[%s])" , tableId , projectedColumns , actualParallelism , filter ) ; List < ColumnHandle > columns = projectedColumns . orElse ( ImmutableList . of ( ) ) ; List < String > projectedColumnsNames = columns . stream ( ) . map ( column -> ( ( BigQueryColumnHandle ) column ) . getName ( ) ) . collect ( toImmutableList ( ) ) ; ReadSession readSession = new ReadSessionCreator ( readSessionCreatorConfig , bigQueryClient , bigQueryStorageClientFactory ) . create ( tableId , projectedColumnsNames , filter , actualParallelism ) ; return readSession . getStreamsList ( ) . stream ( ) . map ( stream -> BigQuerySplit . forStream ( stream . getName ( ) , readSession . getAvroSchema ( ) . getSchema ( ) , columns ) ) . collect ( toImmutableList ( ) ) ; }
public void test() { try { removeListener ( listener ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { from ( "direct:start" ) . setHeader ( "QUERY" , constant ( "Carl*" ) ) . process ( new LuceneQueryProcessor ( "target/simpleindexDir" , analyzer , null , 20 , 20 ) ) . to ( "direct:next" ) ; } catch ( Exception e ) { LOG . warn ( "Unhandled exception: {}" , e . getMessage ( ) , e ) ; } }
public void test() { for ( int i = 0 ; i < hits . getNumberOfHits ( ) ; i ++ ) { LOG . debug ( "Hit " + i + " Index Location:" + hits . getHit ( ) . get ( i ) . getHitLocation ( ) ) ; LOG . debug ( "Hit " + i + " Score:" + hits . getHit ( ) . get ( i ) . getScore ( ) ) ; LOG . debug ( "Hit " + i + " Data:" + hits . getHit ( ) . get ( i ) . getData ( ) ) ; } }
public void test() { for ( int i = 0 ; i < hits . getNumberOfHits ( ) ; i ++ ) { LOG . debug ( "Hit " + i + " Index Location:" + hits . getHit ( ) . get ( i ) . getHitLocation ( ) ) ; LOG . debug ( "Hit " + i + " Score:" + hits . getHit ( ) . get ( i ) . getScore ( ) ) ; LOG . debug ( "Hit " + i + " Data:" + hits . getHit ( ) . get ( i ) . getData ( ) ) ; } }
public void test() { for ( int i = 0 ; i < hits . getNumberOfHits ( ) ; i ++ ) { LOG . debug ( "Hit " + i + " Index Location:" + hits . getHit ( ) . get ( i ) . getHitLocation ( ) ) ; LOG . debug ( "Hit " + i + " Score:" + hits . getHit ( ) . get ( i ) . getScore ( ) ) ; LOG . debug ( "Hit " + i + " Data:" + hits . getHit ( ) . get ( i ) . getData ( ) ) ; } }
public void test() { try { logFile . close ( ) ; } catch ( IOException e ) { LOGGER . warn ( "Unable to close " + file , e ) ; } }
public void test() { try { bundle . uninstall ( ) ; } catch ( BundleException bundleException ) { _log . error ( bundleException , bundleException ) ; } }
public void test() { if ( bugCheck != 1 ) { logger . warn ( "ambiguous span found(bug). span:{}" , span ) ; } }
public void test() { if ( userAgentMatch ( userAgent ) ) { LOGGER . debug ( "the user-agent matched and no Authorization header was sent, returning a 401." ) ; getAuthenticationEntryPoint ( ) . commence ( request , response , _exception ) ; } else { LOGGER . debug ( "the user-agent does not match, skipping filter." ) ; chain . doFilter ( request , response ) ; } }
public void test() { if ( userAgentMatch ( userAgent ) ) { LOGGER . debug ( "the user-agent matched and no Authorization header was sent, returning a 401." ) ; getAuthenticationEntryPoint ( ) . commence ( request , response , _exception ) ; } else { LOGGER . debug ( "the user-agent does not match, skipping filter." ) ; chain . doFilter ( request , response ) ; } }
@ Override public byte [ ] serializeHandshakeMessageContent ( ) { LOGGER . debug ( "Serializing EncryptedExtensionsMessage" ) ; code_block = IfStatement ; return getAlreadySerialized ( ) ; }
@ Test public void testGetJavaScriptOption ( ) { AnimateDuration animateDuration = new AnimateDuration ( 500 ) ; String expectedJavascript = "500" ; String generatedJavascript = animateDuration . getJavascriptOption ( ) . toString ( ) ; log . info ( expectedJavascript ) ; log . info ( generatedJavascript ) ; assertEquals ( generatedJavascript , expectedJavascript ) ; }
@ Test public void testGetJavaScriptOption ( ) { AnimateDuration animateDuration = new AnimateDuration ( 500 ) ; String expectedJavascript = "500" ; String generatedJavascript = animateDuration . getJavascriptOption ( ) . toString ( ) ; log . info ( expectedJavascript ) ; log . info ( generatedJavascript ) ; assertEquals ( generatedJavascript , expectedJavascript ) ; }
public void test() { try { log . info ( "Moving current data directory away to {" + dataSourceFolder ( ) . getAbsolutePath ( ) + "}" ) ; FileUtils . moveDirectory ( dataTargetFolder ( ) , dataSourceFolder ( ) ) ; } catch ( Exception e ) { throw new RuntimeException ( "Could not move data folder to backup location {" + dataTargetFolder ( ) . getAbsolutePath ( ) + "}. Maybe the permissions not allowing this?" ) ; } }
public void test() { if ( cacheEventLogger != null ) { final String cacheEventLoggerAttributePrefix = auxPrefix + CACHE_EVENT_LOGGER_PREFIX + ATTRIBUTE_PREFIX ; PropertySetter . setProperties ( cacheEventLogger , props , cacheEventLoggerAttributePrefix + "." ) ; log . info ( "Using custom cache event logger [{0}] for auxiliary [{1}]" , cacheEventLogger , auxPrefix ) ; } else { log . info ( "No cache event logger defined for auxiliary [{0}]" , auxPrefix ) ; } }
public void test() { if ( cacheEventLogger != null ) { final String cacheEventLoggerAttributePrefix = auxPrefix + CACHE_EVENT_LOGGER_PREFIX + ATTRIBUTE_PREFIX ; PropertySetter . setProperties ( cacheEventLogger , props , cacheEventLoggerAttributePrefix + "." ) ; log . info ( "Using custom cache event logger [{0}] for auxiliary [{1}]" , cacheEventLogger , auxPrefix ) ; } else { log . info ( "No cache event logger defined for auxiliary [{0}]" , auxPrefix ) ; } }
public void test() { try { super . start ( ) ; super . destroy ( ) ; this . with ( LAST_UPDATED_ORDERBY . clone ( ) ) . direction ( DIRECTION . DESC ) . with ( POSTED_TIME_ORDERBY . clone ( ) ) . direction ( DIRECTION . DESC ) ; } catch ( Exception ex ) { LOG . warn ( ex ) ; } }
@ Override public void timeout ( long timeoutMs ) { log . debug ( "{} Setting timeout to {} ms" , this , timeoutMs ) ; this . timeoutMs = timeoutMs ; }
public void test() { if ( StringUtils . isBlank ( value ) ) { LOG . debug ( "Null value for {}, ignoring" , schema . getKey ( ) ) ; } else { code_block = TryStatement ;  } }
public void test() { try { attr . add ( value , anyUtils ) ; } catch ( InvalidPlainAttrValueException e ) { String valueToPrint = value . length ( ) > 40 ? value . substring ( 0 , 20 ) + "..." : value ; LOG . warn ( "Invalid value for attribute " + schema . getKey ( ) + ": " + valueToPrint , e ) ; invalidValues . getElements ( ) . add ( schema . getKey ( ) + ": " + valueToPrint + " - " + e . getMessage ( ) ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Sample up to 10 outstanding migrations: {}" , migrations . stream ( ) . limit ( 10 ) . map ( String :: valueOf ) . collect ( Collectors . joining ( ", " ) ) ) ; } }
public void test() { if ( indexed > 0 && rows != indexed ) { Log . warn ( this , "Rows = " + rows + " but only " + indexed + " indexes" ) ; return rows ; } }
public void test() { try { BaseAppliance appliance = ondusService . getAppliance ( getRoom ( ) , config . applianceId ) . orElse ( null ) ; code_block = IfStatement ; return ( T ) appliance ; } catch ( IOException e ) { updateStatus ( ThingStatus . OFFLINE , ThingStatusDetail . COMMUNICATION_ERROR , e . getMessage ( ) ) ; logger . debug ( "Could not load appliance" , e ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { if ( e instanceof InterpreterRPCException ) { result . ex = ( InterpreterRPCException ) e ; result . setExIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof InterpreterRPCException ) { result . ex = ( InterpreterRPCException ) e ; result . setExIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof InterpreterRPCException ) { result . ex = ( InterpreterRPCException ) e ; result . setExIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { try { fcall . sendResponse ( fb , msg , msgType , seqid ) ; } catch ( java . lang . Exception ex ) { _LOGGER . error ( "Exception writing to internal frame buffer" , ex ) ; fb . close ( ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Query targeted local bucket not found. {}" , bme . getMessage ( ) , bme ) ; } }
public void test() { if ( logger . isInfoEnabled ( ) ) { final Instant now = Instant . now ( ) ; final String entityName = entity . getClass ( ) . getSimpleName ( ) ; logger . info ( marker , "{} {} {} of [{}] at [{}], unix [{}]" , operation . toString ( ) . toLowerCase ( Locale . ENGLISH ) , entityName , marker . getName ( ) . toLowerCase ( ) , entity . getUuid ( ) , now , now . getEpochSecond ( ) ) ; } }
public void test() { if ( currentRetries >= MAX_RETRIES ) { LOGGER . error ( "Giving up on retrying watcher" , cause ) ; giveUp . run ( ) ; } else { code_block = TryStatement ;  } }
public void test() { try { daoManager . startTransaction ( em ) ; UserDelegateEntity item = em . find ( UserDelegateEntity . class , id ) ; em . remove ( item ) ; daoManager . commitTransaction ( em ) ; } catch ( Exception e ) { daoManager . rollBackTransaction ( em ) ; LOG . error ( e . getMessage ( ) ) ; } finally { daoManager . closeEntityManager ( em ) ; } }
@ Override @ BeforeEach public void setUp ( ) throws Exception { super . setUp ( ) ; testDirectory ( "in" , true ) ; LOG . debug ( "Writing file..." ) ; Files . write ( testFile ( "in/file.dat" ) , "Line" . getBytes ( ) ) ; LOG . debug ( "Writing file DONE..." ) ; }
@ Override @ BeforeEach public void setUp ( ) throws Exception { super . setUp ( ) ; testDirectory ( "in" , true ) ; LOG . debug ( "Writing file..." ) ; Files . write ( testFile ( "in/file.dat" ) , "Line" . getBytes ( ) ) ; LOG . debug ( "Writing file DONE..." ) ; }
public void test() { try { String currentCounter = this . getCurrentIndex ( ) ; this . pageContext . getOut ( ) . print ( currentCounter ) ; } catch ( Throwable t ) { _logger . error ( "error creating (or modifying) counter" , t ) ; throw new JspException ( "error creating (or modifying) counter" , t ) ; } }
public void test() { try { ld = getAdminConnection ( ) ; ld . setTimeOut ( 0 ) ; RbacCreateSessionRequest rbacCreateSessionRequest = new RbacCreateSessionRequestImpl ( ) ; rbacCreateSessionRequest . setTenantId ( user . getContextId ( ) ) ; rbacCreateSessionRequest . setUserIdentity ( user . getUserId ( ) ) ; rbacCreateSessionRequest . setPassword ( new String ( user . getPassword ( ) ) ) ; code_block = IfStatement ; RbacCreateSessionResponse rbacCreateSessionResponse = ( RbacCreateSessionResponse ) ld . extended ( rbacCreateSessionRequest ) ; LOG . debug ( "createSession userId: {}, sessionId: {}, resultCode: {}" , user . getUserId ( ) , rbacCreateSessionResponse . getSessionId ( ) , rbacCreateSessionResponse . getLdapResult ( ) . getResultCode ( ) ) ; session = new Session ( user , rbacCreateSessionResponse . getSessionId ( ) ) ; code_block = IfStatement ; } catch ( LdapException e ) { String error = "createSession userId [" + user . getUserId ( ) + "] caught LDAPException=" + " msg=" + e . getMessage ( ) ; throw new SecurityException ( GlobalErrIds . ACEL_CREATE_SESSION_ERR , error , e ) ; } finally { closeAdminConnection ( ld ) ; } }
public void test() { try { serviceCenterTaskMonitor . beginCycle ( interval ) ; microserviceServiceCenterTask . run ( ) ; serviceCenterTaskMonitor . endCycle ( ) ; } catch ( Throwable e ) { LOGGER . error ( "unexpected exception caught from service center task. " , e ) ; } }
@ Override public void appendToComment ( String st ) { log . warn ( "Can't append '{}' to comment. SessionInfoService is not supported in {}" , st , jaggerPlace ) ; }
public void test() { if ( s_logger . isDebugEnabled ( ) ) { s_logger . debug ( "Checking if storage pool is suitable, name: " + pool . getName ( ) + " ,poolId: " + pool . getId ( ) ) ; } }
public void test() { if ( s_logger . isDebugEnabled ( ) ) { s_logger . debug ( "StoragePool is in avoid set, skipping this pool" ) ; } }
public void test() { if ( s_logger . isDebugEnabled ( ) ) { s_logger . debug ( "StoragePool's Cluster does not have required hypervisorType, skipping this pool" ) ; } }
public void test() { if ( s_logger . isDebugEnabled ( ) ) { s_logger . debug ( "StoragePool does not have required hypervisorType, skipping this pool" ) ; } }
public void test() { try { boolean isStoragePoolStoragepolicyComplaince = storageMgr . isStoragePoolComplaintWithStoragePolicy ( requestVolumes , pool ) ; code_block = IfStatement ; } catch ( StorageUnavailableException e ) { s_logger . warn ( String . format ( "Could not verify storage policy complaince against storage pool %s due to exception %s" , pool . getUuid ( ) , e . getMessage ( ) ) ) ; return false ; } }
public void test() { try ( DataOutputStream outputStream = new DataOutputStream ( new BufferedOutputStream ( new FileOutputStream ( tempFile ) ) ) ) { synchronized ( partitionTable ) code_block = "" ; } catch ( IOException e ) { logger . error ( "Cannot save the partition table" , e ) ; } }
public void test() { try { Files . delete ( Paths . get ( oldFile . getAbsolutePath ( ) ) ) ; } catch ( IOException e ) { logger . warn ( "Old partition table file is not successfully deleted" , e ) ; } }
public void test() { if ( ! tempFile . renameTo ( oldFile ) ) { logger . warn ( "New partition table file is not successfully renamed" ) ; } }
public void test() { if ( canRefresh ( ) ) { removeAll ( ) ; populateCache ( ) ; } else { log . info ( "Cluster message received to Refresh the cache, but uanble to as it is still being populated." ) ; } }
public void test() { try { this . getRoleDAO ( ) . addRole ( role ) ; this . getRoleCacheWrapper ( ) . addRole ( role ) ; } catch ( Throwable t ) { logger . error ( "Error while adding a role" , t ) ; throw new ApsSystemException ( "Error while adding a role" , t ) ; } }
public void test() { try { ret = parseEnum ( enumeration , name ) ; } catch ( IllegalArgumentException e ) { LOGGER . warn ( "Invalid name for enumeration: {}, name: {}" , enumeration . getName ( ) , name ) ; } catch ( NullPointerException e ) { LOGGER . warn ( "Error parsing enumeration: {}, name: {}" , enumeration . getName ( ) , name ) ; } }
public void test() { try { ret = parseEnum ( enumeration , name ) ; } catch ( IllegalArgumentException e ) { LOGGER . warn ( "Invalid name for enumeration: {}, name: {}" , enumeration . getName ( ) , name ) ; } catch ( NullPointerException e ) { LOGGER . warn ( "Error parsing enumeration: {}, name: {}" , enumeration . getName ( ) , name ) ; } }
public void test() { if ( object instanceof String ) { String name = ( String ) object ; code_block = TryStatement ;  } else { LOGGER . warn ( "Could not convert Solr object to enumeration: {}, unsupported class: {}" , enumeration . getName ( ) , object . getClass ( ) . getName ( ) ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "MULTI_SIG sign credential: {}" , credential ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "MULTI_SIG sign multi-signature: {}" , multiSigData . getHexString ( ) ) ; } }
public void test() { try { this . socket . close ( ) ; } catch ( IOException ioe ) { log . debug ( "Failed to close socket" , ioe ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "session established ok with " + this . address ) ; } }
@ Test public void onlyLastCoalescingWorkShouldBeExecuted ( ) throws InterruptedException { log . debug ( "StreamWorkManagerTest.onlyLastCoalescingWorkShouldBeExecuted() beginning" ) ; int longDurationMs = 20_000 ; int shortDurationMs = 1_000 ; SleepWork longWork = createCoalescing ( longDurationMs ) ; SleepWork shortWork = createCoalescing ( shortDurationMs ) ; service . schedule ( shortWork ) ; service . schedule ( longWork ) ; service . schedule ( longWork ) ; service . schedule ( shortWork ) ; long start = System . currentTimeMillis ( ) ; assertTrue ( service . awaitCompletion ( 60 , TimeUnit . SECONDS ) ) ; long elapsed = System . currentTimeMillis ( ) - start ; tracker . assertDiff ( 0 , 0 , 4 , 0 ) ; assertTrue ( String . valueOf ( elapsed ) , elapsed < longDurationMs ) ; log . debug ( "StreamWorkManagerTest.onlyLastCoalescingWorkShouldBeExecuted() ending" ) ; }
public void test() { try { insertPatternIdPs . setString ( 1 , axiomString ) ; insertPatternIdPs . setString ( 2 , axiomRenderer . render ( axiom ) ) ; insertPatternIdPs . setString ( 3 , getAxiomType ( axiom ) ) ; insertPatternIdPs . execute ( ) ; } catch ( SQLException e ) { LOGGER . error ( "Failed to insert pattern. Maybe too long with a length of " + axiomString . length ( ) + "?" , e ) ; } }
public void await ( long timeoutMillis ) throws InterruptedException { code_block = IfStatement ; long c = latch . getCount ( ) ; Preconditions . checkState ( 0 < c ) ; String msg = "Did not read expected number of messages before timeout was reached (latch count is " + c + ")" ; log . error ( msg ) ; throw new AssertionError ( msg ) ; }
public void test() { try { cursor = search ( "" , LdapConstants . OBJECT_CLASS_STAR , SearchScope . OBJECT , attributes ) ; code_block = IfStatement ; } catch ( Exception e ) { String msg = I18n . err ( I18n . ERR_04156_FAILED_FETCHING_ROOT_DSE ) ; LOG . error ( msg ) ; throw new LdapException ( msg , e ) ; } finally { code_block = IfStatement ; } }
public void test() { try { cursor . close ( ) ; } catch ( Exception e ) { LOG . error ( I18n . err ( I18n . ERR_04114_CURSOR_CLOSE_FAIL ) , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Invalidate: Entry already invalid: '{}'" , event . getKey ( ) ) ; } }
public void test() { if ( resourceIdentifier == null ) { ourLog . info ( "There is no field named {} in Resource {}" , theFieldName , resourceDefinition . getName ( ) ) ; return null ; } }
private List < String > installLinux ( List < String > urls , String saveAs ) { log . info ( "Installing " + getEntity ( ) + " using couchbase-server-{} {}" , getCommunityOrEnterprise ( ) , getVersion ( ) ) ; String apt = chainGroup ( installPackage ( MutableMap . of ( "apt" , "python-httplib2 libssl0.9.8" ) , null ) , sudo ( format ( "dpkg -i %s" , saveAs ) ) ) ; String yum = chainGroup ( "which yum" , ok ( sudo ( "sed -i.bk s/^enabled=1$/enabled=0/ /etc/yum/pluginconf.d/subscription-manager.conf" ) ) , ok ( sudo ( "yum check-update" ) ) , sudo ( "yum install -y pkgconfig" ) , sudo ( "[ -f /etc/redhat-release ] && (grep -i \"red hat\" /etc/redhat-release && sudo yum install -y openssl098e) || :" ) , sudo ( format ( "rpm --install %s" , saveAs ) ) ) ; String link = new DownloadProducerFromUrlAttribute ( ) . apply ( new BasicDownloadRequirement ( this ) ) . getPrimaryLocations ( ) . iterator ( ) . next ( ) ; return ImmutableList . < String > builder ( ) . add ( INSTALL_CURL ) . addAll ( Arrays . asList ( INSTALL_CURL , BashCommands . require ( BashCommands . alternatives ( BashCommands . simpleDownloadUrlAs ( urls , saveAs ) , "curl -f -L -k " + BashStringEscapes . wrapBash ( link ) + " -H 'Referer: http://www.couchbase.com/downloads'" + " -o " + saveAs ) , "Could not retrieve " + saveAs + " (from " + urls . size ( ) + " sites)" , 9 ) ) ) . add ( alternatives ( apt , yum ) ) . build ( ) ; }
SwaggerResource swaggerResource ( String name , String location ) { log . info ( "name:{},location:{}" , name , location ) ; SwaggerResource swaggerResource = new SwaggerResource ( ) ; swaggerResource . setName ( name ) ; swaggerResource . setLocation ( location ) ; swaggerResource . setSwaggerVersion ( "3.0" ) ; return swaggerResource ; }
@ Override @ RequestMapping public @ ResponseBody void query ( @ RequestParam ( value = "query" , required = false ) String query , @ RequestParam ( value = "format" , required = false ) String format , @ RequestParam ( value = "offset" , required = false ) Integer offset , @ RequestParam ( value = "limit" , required = false ) Integer limit , @ RequestParam ( value = "inference" , required = false ) boolean inference , HttpServletRequest request , HttpServletResponse response ) throws QueryParseException , LodeException , IOException { final long startTime = System . currentTimeMillis ( ) ; String v = request . getHeader ( "Referer" ) ; LoggingContext . put ( "webui" , v != null && v . contains ( "/mesh/query" ) ) ; if ( query != null ) LoggingContext . put ( "query" , query ) ; if ( format != null ) LoggingContext . put ( "format" , format ) ; if ( limit != null ) LoggingContext . put ( "limit" , limit ) ; if ( offset != null ) LoggingContext . put ( "offset" , offset ) ; LoggingContext . put ( "inference" , inference ) ; super . query ( query , format , offset , limit , inference , request , response ) ; final long responseTime = System . currentTimeMillis ( ) - startTime ; LoggingContext . put ( "responsetime" , responseTime ) ; apilog . info ( "sparql query" ) ; LoggingContext . clear ( ) ; }
public void test() { if ( e instanceof RuntimeException || e instanceof Error ) { LOG . warn ( logMsg , e ) ; } else-if ( exceptionsHandler . isTerse ( e . getClass ( ) ) ) { LOG . info ( logMsg ) ; } else { LOG . info ( logMsg , e ) ; } }
public void test() { if ( e instanceof RuntimeException || e instanceof Error ) { LOG . warn ( logMsg , e ) ; } else-if ( exceptionsHandler . isTerse ( e . getClass ( ) ) ) { LOG . info ( logMsg ) ; } else { LOG . info ( logMsg , e ) ; } }
public void test() { if ( e instanceof RuntimeException || e instanceof Error ) { LOG . warn ( logMsg , e ) ; } else-if ( exceptionsHandler . isTerse ( e . getClass ( ) ) ) { LOG . info ( logMsg ) ; } else { LOG . info ( logMsg , e ) ; } }
public void test() { if ( buf . size ( ) > maxRespSize ) { LOG . warn ( "Large response size " + buf . size ( ) + " for call " + call . toString ( ) ) ; buf = new ByteArrayOutputStream ( INITIAL_RESP_BUF_SIZE ) ; } }
public void test() { try { setupResponse ( buf , call , ( error == null ) ? Status . SUCCESS : Status . ERROR , value , errorClass , error ) ; code_block = IfStatement ; channelWrite ( ctx , call . response ) ; } catch ( Exception e ) { LOG . info ( this . getClass ( ) . getName ( ) + " caught: " + StringUtils . stringifyException ( e ) ) ; error = null ; } finally { IOUtils . closeStream ( buf ) ; } }
public void test() { if ( Boolean . TRUE . equals ( p . isArray ) ) { p . vendorExtensions . put ( "x-protobuf-type" , "repeated" ) ; } else-if ( Boolean . TRUE . equals ( p . isMap ) ) { LOGGER . warn ( "Map parameter (name: {}, operation ID: {}) not yet supported" , p . paramName , op . operationId ) ; } }
public void test() { try { long t = System . currentTimeMillis ( ) ; int numOfRows = transactionTemplate . execute ( new TransactionCallback < Integer > ( ) code_block = "" ; ) ; t = System . currentTimeMillis ( ) - t ; if ( logger . isDebugEnabled ( ) ) logger . debug ( getClass ( ) . getSimpleName ( ) + ": " + numOfRows + " rows where processed in " + t + " ms" ) ; else-if ( t > TimeUnit . MINUTES . toMillis ( 1 ) ) logger . warn ( "Rolling between table " + previousTable + " to table " + activeTable + ", took :" + t + " ms" ) ; } catch ( DataAccessException ex ) { logger . error ( getClass ( ) . getSimpleName ( ) + " failed to execute: " + sql , ex ) ; } }
public void test() { { log . info ( Color . GREEN + "Calendar_2_6 : missing column friday" + Color . NORMAL ) ; Context context = new Context ( ) ; CheckPointReport result = verifyValidation ( log , context , "calendar_2_6" , GTFS_1_GTFS_Common_9 , SEVERITY . ERROR , RESULT . NOK , true ) ; Assert . assertEquals ( result . getCheckPointErrorCount ( ) , 1 , "detail count" ) ; code_block = ForStatement ; } }
public void test() { try { long entryScopeGroupId = GetterUtil . getLong ( element . elementText ( OpenSearchUtil . getQName ( "scopeGroupId" , OpenSearchUtil . LIFERAY_NAMESPACE ) ) ) ; code_block = IfStatement ; resultRows . add ( element ) ; } catch ( Exception exception ) { _log . error ( "Unable to retrieve individual search result to " + className , exception ) ; totalRows -- ; } }
public void test() { try { xml = XMLUtil . stripInvalidChars ( xml ) ; Document document = SAXReaderUtil . read ( xml ) ; Element rootElement = document . getRootElement ( ) ; List < Element > elements = rootElement . elements ( "entry" ) ; totalRows = GetterUtil . getInteger ( rootElement . elementText ( OpenSearchUtil . getQName ( "totalResults" , OpenSearchUtil . OS_NAMESPACE ) ) ) ; code_block = ForStatement ; } catch ( Exception exception ) { _log . error ( "Unable to display content to " + className , exception ) ; } }
public void test() { try { String [ ] peerInfo = channel . remoteAddress ( ) . toString ( ) . replace ( "/" , "" ) . split ( ":" ) ; return new Pair < > ( peerInfo [ 0 ] , Integer . parseInt ( peerInfo [ 1 ] ) ) ; } catch ( Exception e ) { logger . debug ( "Failed to parse peer info for SSL engine initialization" , e ) ; } }
@ Test public void testNumericAndRange ( ) throws Exception { log . info ( "------  testNumericAndRange  ------" ) ; String query = "(" + CityField . NUM . name ( ) + GTE_OP + "30)" + AND_OP + "(" + CityField . NUM . name ( ) + LTE_OP + "105)" ; runTest ( "((_Bounded_ = true) && (" + query + "))" , query ) ; }
public void test() { if ( idsSet . size ( ) == upperBoundPerPartition ) { LOGGER . warn ( "At upper bound on partition.  Increase the bounds or condense the data." ) ; } }
public void test() { try { return Seq . seq ( podExecutor . exec ( pod , StackgresClusterContainers . PATRONI , "sh" , "-c" , Seq . seq ( PatroniStatsScripts . getScripts ( ) ) . map ( tt -> "echo \"" + tt . v1 . getName ( ) + ":$( (" + tt . v2 + ") 2>&1 | tr -d '\\n')\"\n" ) . toString ( ) ) ) . peek ( line code_block = LoopStatement ; ) . filter ( line -> ! line . endsWith ( "failed" ) ) . map ( line -> Tuple . tuple ( line , line . indexOf ( ":" ) ) ) . map ( tt -> Tuple . tuple ( PatroniStatsScripts . fromName ( tt . v1 . substring ( 0 , tt . v2 ) ) , tt . v1 . substring ( tt . v2 + 1 ) ) ) . collect ( ImmutableMap . toImmutableMap ( Tuple2 :: v1 , Tuple2 :: v2 ) ) ; } catch ( Exception ex ) { LOGGER . debug ( "An error accurred while retrieving stats for pod {}.{}: {}" , pod . getMetadata ( ) . getNamespace ( ) , pod . getMetadata ( ) . getName ( ) , ex . getMessage ( ) ) ; return ImmutableMap . < PatroniStatsScripts , String > of ( ) ; } }
@ Override public void run ( ) { RadioStation oldStation = currentStation ; currentStationIndex ++ ; currentStationIndex = currentStationIndex % stationsList . size ( ) ; currentStation = stationsList . get ( currentStationIndex ) ; currentStationChanged ( currentStation ) ; logger . info ( PRINT_BORDER + "shuffleStations: " + oldStation + " -> " + currentStation + PRINT_BORDER ) ; deferred . resolve ( ) ; }
@ Test public void dropImport ( ) throws Exception { Repository repository = createTestRepoUsingRepoService ( ) ; getL10nJCommander ( ) . run ( "push" , "-r" , repository . getName ( ) , "-s" , getInputResourcesTestDir ( "source" ) . getAbsolutePath ( ) ) ; Asset asset = assetClient . getAssetByPathAndRepositoryId ( "source-xliff.xliff" , repository . getId ( ) ) ; importTranslations ( asset . getId ( ) , "source-xliff_" , "fr-FR" ) ; importTranslations ( asset . getId ( ) , "source-xliff_" , "ja-JP" ) ; Asset asset2 = assetClient . getAssetByPathAndRepositoryId ( "source2-xliff.xliff" , repository . getId ( ) ) ; importTranslations ( asset2 . getId ( ) , "source2-xliff_" , "fr-FR" ) ; importTranslations ( asset2 . getId ( ) , "source2-xliff_" , "ja-JP" ) ; waitForRepositoryToHaveStringsForTranslations ( repository . getId ( ) ) ; getL10nJCommander ( ) . run ( "drop-export" , "-r" , repository . getName ( ) ) ; final Long dropId = getLastDropIdFromOutput ( outputCapture ) ; logger . debug ( "Mocking the console input code_block = ForStatement ; ) ; L10nJCommander l10nJCommander = getL10nJCommander ( ) ; DropImportCommand dropImportCommand = l10nJCommander . getCommand ( DropImportCommand . class ) ; dropImportCommand . console = mockConsole ; int numberOfFrenchTranslationsBefore = getNumberOfFrenchTranslations ( repository ) ; localizeDropFiles ( dropRepository . findById ( dropId ) . orElse ( null ) ) ; l10nJCommander . run ( new String [ ] code_block = "" ; ) ; int numberOfFrenchTranslationsAfter = getNumberOfFrenchTranslations ( repository ) ; assertEquals ( "2 new french translations must be added" , numberOfFrenchTranslationsBefore + 2 , numberOfFrenchTranslationsAfter ) ; getL10nJCommander ( ) . run ( "tm-export" , "-r" , repository . getName ( ) , "-t" , targetTestDir . getAbsolutePath ( ) , "--target-basename" , "fortest" ) ; modifyFilesInTargetTestDirectory ( XliffUtils . replaceCreatedDateFunction ( ) ) ; checkExpectedGeneratedResources ( ) ; }
public void test() { try { PrintWriter printwriter = new PrintWriter ( stringwriter ) ; printwriter . printf ( id , pattern ) ; formattedID = stringwriter . toString ( ) ; } catch ( IllegalFormatException e ) { logger . error ( "Error when attempting to format ID '" + id + "' using formatting pattern '" + pattern ) ; } }
@ RestAccessControl ( permission = Permission . SUPERUSER ) @ RequestMapping ( value = "/report/{reportCode}" , method = RequestMethod . GET , produces = MediaType . APPLICATION_JSON_VALUE ) public ResponseEntity < SimpleRestResponse < DumpReportDto > > getDumpReport ( @ PathVariable String reportCode ) { logger . debug ( "Required dump report -> code {}" , reportCode ) ; DumpReportDto result = this . getDatabaseService ( ) . getDumpReportDto ( reportCode ) ; logger . debug ( "Extracted dump report -> {}" , result ) ; return new ResponseEntity < > ( new SimpleRestResponse < > ( result ) , HttpStatus . OK ) ; }
@ RestAccessControl ( permission = Permission . SUPERUSER ) @ RequestMapping ( value = "/report/{reportCode}" , method = RequestMethod . GET , produces = MediaType . APPLICATION_JSON_VALUE ) public ResponseEntity < SimpleRestResponse < DumpReportDto > > getDumpReport ( @ PathVariable String reportCode ) { logger . debug ( "Required dump report -> code {}" , reportCode ) ; DumpReportDto result = this . getDatabaseService ( ) . getDumpReportDto ( reportCode ) ; logger . debug ( "Extracted dump report -> {}" , result ) ; return new ResponseEntity < > ( new SimpleRestResponse < > ( result ) , HttpStatus . OK ) ; }
public void test() { if ( debugEnabled ) { logger . debug ( "getConnectionAsync(" + intent + ")" ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( "Initializing verification " + verifyProcessClassName ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( "Running verification " + verifyProcessClassName ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( "Finished verification " + verifyProcessClassName ) ; } }
public void test() { try { Class < ? > clazz = Class . forName ( verifyProcessClassName ) ; VerifyProcess verifyProcess = ( VerifyProcess ) clazz . newInstance ( ) ; code_block = IfStatement ; verifyProcess . verify ( ) ; code_block = IfStatement ; return true ; } catch ( ClassNotFoundException classNotFoundException ) { _log . error ( verifyProcessClassName + " cannot be found" , classNotFoundException ) ; } catch ( IllegalAccessException illegalAccessException ) { _log . error ( verifyProcessClassName + " cannot be accessed" , illegalAccessException ) ; } catch ( InstantiationException instantiationException ) { _log . error ( verifyProcessClassName + " cannot be initiated" , instantiationException ) ; } }
public void test() { try { Class < ? > clazz = Class . forName ( verifyProcessClassName ) ; VerifyProcess verifyProcess = ( VerifyProcess ) clazz . newInstance ( ) ; code_block = IfStatement ; verifyProcess . verify ( ) ; code_block = IfStatement ; return true ; } catch ( ClassNotFoundException classNotFoundException ) { _log . error ( verifyProcessClassName + " cannot be found" , classNotFoundException ) ; } catch ( IllegalAccessException illegalAccessException ) { _log . error ( verifyProcessClassName + " cannot be accessed" , illegalAccessException ) ; } catch ( InstantiationException instantiationException ) { _log . error ( verifyProcessClassName + " cannot be initiated" , instantiationException ) ; } }
public void test() { try { Class < ? > clazz = Class . forName ( verifyProcessClassName ) ; VerifyProcess verifyProcess = ( VerifyProcess ) clazz . newInstance ( ) ; code_block = IfStatement ; verifyProcess . verify ( ) ; code_block = IfStatement ; return true ; } catch ( ClassNotFoundException classNotFoundException ) { _log . error ( verifyProcessClassName + " cannot be found" , classNotFoundException ) ; } catch ( IllegalAccessException illegalAccessException ) { _log . error ( verifyProcessClassName + " cannot be accessed" , illegalAccessException ) ; } catch ( InstantiationException instantiationException ) { _log . error ( verifyProcessClassName + " cannot be initiated" , instantiationException ) ; } }
public void debug ( Object msg , Throwable thr ) { logger . debug ( msg , thr ) ; }
public void test() { try { DigirMetadata metadata = getDigirMetadata ( endpoint ) ; String code = MachineTagUtils . firstTag ( dataset , TagName . DIGIR_CODE ) . getValue ( ) ; int numberOfRecords = metadata . getResources ( ) . stream ( ) . filter ( ( r ) -> code . equals ( r . getCode ( ) ) ) . findFirst ( ) . map ( DigirResource :: getNumberOfRecords ) . orElse ( 0 ) ; LOG . info ( "Retrieved count of {}" , numberOfRecords ) ; code_block = IfStatement ; return null ; } catch ( Exception e ) { throw new MetadataException ( "Unable to retrieve count of DiGIR dataset [" + dataset . getKey ( ) + "]" , e , ErrorCode . OTHER_ERROR ) ; } }
public void test() { if ( customScriptConfiguration == null ) { logger . error ( "Failed to get CustomScriptConfiguration. acr: '{}'" , this . authAcr ) ; } else { this . authAcr = customScriptConfiguration . getCustomScript ( ) . getName ( ) ; boolean result = externalAuthenticationService . executeExternalAuthenticate ( customScriptConfiguration , null , 1 ) ; logger . info ( "Authentication result for user '{}', result: '{}'" , credentials . getUsername ( ) , result ) ; code_block = IfStatement ; } }
public void test() { try { ecdsaPkGroupsStatic = new LinkedList < > ( ) ; ecdsaPkGroupsEphemeral = new LinkedList < > ( ) ; ecdsaPkGroupsTls13 = new LinkedList < > ( ) ; ecdsaCertSigGroupsStatic = new LinkedList < > ( ) ; ecdsaCertSigGroupsEphemeral = new LinkedList < > ( ) ; ecdsaCertSigGroupsTls13 = new LinkedList < > ( ) ; Set < CertificateChain > certificates = new HashSet < > ( ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; } catch ( Exception E ) { LOGGER . error ( "Could not scan to " + getProbeName ( ) , E ) ; return getCouldNotExecuteResult ( ) ; } }
public void test() { try { r . run ( ) ; } catch ( final Exception e ) { log . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( testName + ": Role with name \"" + knownRoleName + "\" should already exist, so this request should fail." ) ; logger . debug ( testName + ": status = " + statusCode ) ; logger . debug ( testName + ": " + res ) ; } }
private void killOozieJob ( String jobId ) { String out = sshOozieClient . killJob ( jobId ) ; log . info ( "Killing job {}. {}" , jobId , out ) ; }
public void test() { for ( ObjectName on : s ) { LOG . warn ( " |  " + on ) ; } }
public void test() { if ( ! connection . isValid ( 10 ) ) { LOG . info ( "db connection reconnect.." ) ; establishConnection ( ) ; jdbcWriter . prepareStatement ( connection ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( SQLException e ) { LOG . error ( "check connection open failed.." , e ) ; } catch ( ClassNotFoundException e ) { LOG . error ( "load jdbc class error when reconnect db.." , e ) ; } catch ( IOException e ) { LOG . error ( "jdbc io exception.." , e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( SQLException e ) { LOG . error ( "check connection open failed.." , e ) ; } catch ( ClassNotFoundException e ) { LOG . error ( "load jdbc class error when reconnect db.." , e ) ; } catch ( IOException e ) { LOG . error ( "jdbc io exception.." , e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( SQLException e ) { LOG . error ( "check connection open failed.." , e ) ; } catch ( ClassNotFoundException e ) { LOG . error ( "load jdbc class error when reconnect db.." , e ) ; } catch ( IOException e ) { LOG . error ( "jdbc io exception.." , e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CommercePriceEntryServiceUtil . class , "upsertCommercePriceEntry" , _upsertCommercePriceEntryParameterTypes27 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , commercePriceEntryId , cProductId , cpInstanceUuid , commercePriceListId , externalReferenceCode , price , discountDiscovery , discountLevel1 , discountLevel2 , discountLevel3 , discountLevel4 , displayDateMonth , displayDateDay , displayDateYear , displayDateHour , displayDateMinute , expirationDateMonth , expirationDateDay , expirationDateYear , expirationDateHour , expirationDateMinute , neverExpire , skuExternalReferenceCode , serviceContext ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . commerce . price . list . model . CommercePriceEntry ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
private void getAllLogicalSwitches ( ) { ResourceCollection < LogicalSwitch > logicalSwitches = client . getAll ( ) ; LOGGER . info ( "Logical switches returned to client: {}" , logicalSwitches . toJsonString ( ) ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Rebalancing job: {} maxTasksToMove={} maxPerHost={}" , job . getId ( ) , maxTasksToMove , maxPerHost ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "hostsSorted.size={} hostWithMost:{} hostWithLeast:{} mostTasksOnHost: {} leastTasksOnHost: {}" , hostsSorted . size ( ) , hostWithMost , hostWithLeast , mostTasksOnHost , leastTasksOnHost ) ; } }
public void test() { try ( CloseableHttpClient closeableHttpClient = httpClientBuilder . useSystemProperties ( ) . build ( ) ) { HttpPost httpPost = new HttpPost ( callbackURL ) ; httpPost . setEntity ( new StringEntity ( _objectMapper . writeValueAsString ( Collections . singletonMap ( id , executeStatus ) ) , ContentType . APPLICATION_JSON ) ) ; closeableHttpClient . execute ( httpPost ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "HostObjectComponent activated" ) ; } }
public void test() { if ( tokenResult . succeeded ( ) ) { log . debug ( "OAuth2 Keycloak exchange succeeded." ) ; AccessToken token = tokenResult . result ( ) ; headerMap . set ( "Authorization" , "Bearer " + token . principal ( ) . getString ( "access_token" ) ) ; resultHandler . handle ( Future . succeededFuture ( ) ) ; } else { log . error ( "Access Token Error: {0}." , tokenResult . cause ( ) . getMessage ( ) ) ; resultHandler . handle ( Future . failedFuture ( tokenResult . cause ( ) ) ) ; } }
public void test() { if ( tokenResult . succeeded ( ) ) { log . debug ( "OAuth2 Keycloak exchange succeeded." ) ; AccessToken token = tokenResult . result ( ) ; headerMap . set ( "Authorization" , "Bearer " + token . principal ( ) . getString ( "access_token" ) ) ; resultHandler . handle ( Future . succeededFuture ( ) ) ; } else { log . error ( "Access Token Error: {0}." , tokenResult . cause ( ) . getMessage ( ) ) ; resultHandler . handle ( Future . failedFuture ( tokenResult . cause ( ) ) ) ; } }
public void test() { try { org . structr . web . entity . dom . DocumentFragment fragment = app . create ( org . structr . web . entity . dom . DocumentFragment . class ) ; fragment . setOwnerDocument ( thisPage ) ; return fragment ; } catch ( FrameworkException fex ) { final Logger logger = LoggerFactory . getLogger ( Page . class ) ; logger . warn ( "" , fex ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "retrieving document(" + documentId + "," + user . getPrincipalName ( ) + ")" ) ; } }
public void test() { try { final InetAddress address = SocketUtils . addressByName ( host ) ; socket = new ServerSocket ( 0 , 3 , address ) ; return ( InetSocketAddress ) socket . getLocalSocketAddress ( ) ; } catch ( final Exception e ) { log . error ( "Failed to find address." ) ; return null ; } finally { code_block = IfStatement ; } }
public void test() { try { socket . close ( ) ; } catch ( final Exception e ) { log . error ( "Failed to close socket." ) ; } }
public void saveModel ( CustomModel custModel , String filename ) throws IOException { File dir = new File ( modelDir ) ; code_block = IfStatement ; File f = new File ( filename ) ; log . info ( "Saving DL4J computation graph model to {}" , f . getAbsolutePath ( ) ) ; ModelSerializer . writeModel ( custModel . getModel ( ) , filename , true ) ; String labelFilename = filename + ".labels" ; FileWriter fw = new FileWriter ( new File ( labelFilename ) ) ; fw . write ( StringUtils . join ( custModel . getLabels ( ) , "|" ) ) ; fw . flush ( ) ; fw . close ( ) ; log . info ( "Model saved: {}" , f . getAbsolutePath ( ) ) ; }
public void saveModel ( CustomModel custModel , String filename ) throws IOException { File dir = new File ( modelDir ) ; code_block = IfStatement ; File f = new File ( filename ) ; log . info ( "Saving DL4J computation graph model to {}" , f . getAbsolutePath ( ) ) ; ModelSerializer . writeModel ( custModel . getModel ( ) , filename , true ) ; String labelFilename = filename + ".labels" ; FileWriter fw = new FileWriter ( new File ( labelFilename ) ) ; fw . write ( StringUtils . join ( custModel . getLabels ( ) , "|" ) ) ; fw . flush ( ) ; fw . close ( ) ; log . info ( "Model saved: {}" , f . getAbsolutePath ( ) ) ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( sb . toString ( ) ) ; } }
private GrpcTransportConfig loadGrpcTransportConfig ( ) { GrpcTransportConfig grpcTransportConfig = new GrpcTransportConfig ( ) ; grpcTransportConfig . read ( profilerConfig . getProperties ( ) ) ; logger . info ( "{}" , grpcTransportConfig ) ; return grpcTransportConfig ; }
public void test() { if ( buttonList != null && component <= buttonList . size ( ) ) { return buttonList . get ( component - 1 ) ; } else { logger . debug ( "Could not find button component {} for id {}" , component , integrationID ) ; return 0 ; } }
public void test() { if ( deviceButtonMap != null ) { List < Integer > buttonList = deviceButtonMap . get ( integrationID ) ; code_block = IfStatement ; } else { logger . debug ( "Device to button map not populated" ) ; return 0 ; } }
@ Override public void versionCommand ( final org . locationtech . geowave . service . grpc . protobuf . VersionCommandParametersProtos request , final io . grpc . stub . StreamObserver < org . locationtech . geowave . service . grpc . protobuf . GeoWaveReturnTypesProtos . StringResponseProtos > responseObserver ) { final VersionCommand cmd = new VersionCommand ( ) ; final Map < FieldDescriptor , Object > m = request . getAllFields ( ) ; GeoWaveGrpcServiceCommandUtil . setGrpcToCommandFields ( m , cmd ) ; final File configFile = GeoWaveGrpcServiceOptions . geowaveConfigFile ; final OperationParams params = new ManualOperationParams ( ) ; params . getContext ( ) . put ( ConfigOptions . PROPERTIES_FILE_CONTEXT , configFile ) ; cmd . prepare ( params ) ; LOGGER . info ( "Executing VersionCommand..." ) ; code_block = TryStatement ;  }
public void test() { try { cmd . computeResults ( params ) ; final StringResponseProtos resp = StringResponseProtos . newBuilder ( ) . build ( ) ; responseObserver . onNext ( resp ) ; responseObserver . onCompleted ( ) ; } catch ( final Exception e ) { LOGGER . error ( "Exception encountered executing command" , e ) ; responseObserver . onError ( e ) ; } }
public void startPeriodicFetching ( ) { code_block = IfStatement ; _log . debug ( "Starting PeriodicFetching Splits ..." ) ; _scheduledFuture = _scheduledExecutorService . scheduleWithFixedDelay ( _splitFetcher . get ( ) , 0L , _refreshEveryNSeconds . get ( ) , TimeUnit . SECONDS ) ; }
public void test() { if ( query . getFields ( ) == null ) { client . getSession ( ) . execute ( CassandraQueryFactory . getTruncateTableQuery ( mapping ) ) ; } else { LOG . error ( "Delete by Query is not supported for the Queries which didn't specify Query keys with fields." ) ; } }
public void test() { try { List < Object > objectArrayList = new ArrayList < > ( ) ; code_block = IfStatement ; LOG . info ( "Delete By Query method doesn't return the deleted element count." ) ; return 0 ; } catch ( Exception e ) { throw new GoraException ( e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( UnsupportedEncodingException e ) { logger . warn ( "error deserializing event message" , e ) ; return null ; } }
@ Override public OutputStream createOutputStream ( long arg0 ) throws IOException { logger . trace ( "createOutputStream: {}" , arg0 ) ; file = new MyOutputStream ( ) ; return file ; }
public void test() { if ( log . isInfoEnabled ( ) ) { log . info ( String . format ( "Found inconsistent statistics version %d instead of %d. " + "Collected local statistics will be cleaned." , storeVer , VERSION ) ) ; } }
public void test() { if ( bridge == null ) { logger . debug ( "Unable to handle command. Bridge is null." ) ; return ; } }
public void test() { if ( x10Function > 0 ) { X10Interface x10Interface = cm11aHandler . getX10Interface ( ) ; x10Interface . scheduleHWUpdate ( this ) ; } else { logger . debug ( "Received invalid command for switch {} command: {}" , houseUnitCode , command ) ; } }
public void test() { if ( cm11aHandler != null && cm11aHandler . getThing ( ) . getStatus ( ) . equals ( ThingStatus . ONLINE ) ) { code_block = IfStatement ; code_block = IfStatement ; } else { logger . debug ( "Attempted to change switch to {} for {} because the cm11a is not online" , command , houseUnitCode ) ; } }
public void test() { try { file . delete ( ) ; } catch ( Exception e ) { logger . warn ( "Error deleting settings temp file: " + file , e ) ; } }
public void test() { switch ( strategy ) { case IGNORE : LOG . debug ( message ) ; break ; case LOGGING : LOG . warn ( message ) ; break ; case FAIL : throw new IllegalArgumentException ( message ) ; default : throw new AssertionError ( strategy ) ; } }
public void test() { switch ( strategy ) { case IGNORE : LOG . debug ( message ) ; break ; case LOGGING : LOG . warn ( message ) ; break ; case FAIL : throw new IllegalArgumentException ( message ) ; default : throw new AssertionError ( strategy ) ; } }
public void test() { while ( jobs . hasNext ( ) ) { long oldID = jobs . next ( ) ; long newID = jobDaoProvider . get ( ) . rescheduleJob ( oldID ) ; log . info ( "Resubmitting old job {} as {}" , oldID , newID ) ; ++ resubmitcount ; } }
public void test() { try { AIP aip = model . retrieveAIP ( representation . getAipId ( ) ) ; List < String > ancestors = SolrUtils . getAncestors ( aip . getParentId ( ) , model ) ; indexRepresentation ( aip , representation , ancestors ) . addTo ( ret ) ; code_block = IfStatement ; } catch ( RequestNotValidException | NotFoundException | GenericException | AuthorizationDeniedException e ) { LOGGER . error ( "Cannot index representation: {}" , representation , e ) ; ret . add ( e ) ; } }
protected void runTestQueryWithUniqueness ( Set < Set < String > > expected , String querystr , Date startDate , Date endDate , Map < String , String > extraParms , AccumuloClient client ) throws Exception { log . debug ( "runTestQueryWithUniqueness" ) ; QueryImpl settings = new QueryImpl ( ) ; settings . setBeginDate ( startDate ) ; settings . setEndDate ( endDate ) ; settings . setPagesize ( Integer . MAX_VALUE ) ; settings . setQueryAuthorizations ( auths . serialize ( ) ) ; settings . setQuery ( querystr ) ; settings . setParameters ( extraParms ) ; settings . setId ( UUID . randomUUID ( ) ) ; log . debug ( "query: " + settings . getQuery ( ) ) ; log . debug ( "logic: " + settings . getQueryLogicName ( ) ) ; GenericQueryConfiguration config = logic . initialize ( client , settings , authSet ) ; logic . setupQuery ( config ) ; DocumentTransformer transformer = ( DocumentTransformer ) ( logic . getTransformer ( settings ) ) ; TransformIterator iter = new DatawaveTransformIterator ( logic . iterator ( ) , transformer ) ; List < Object > eventList = new ArrayList < > ( ) ; code_block = WhileStatement ; BaseQueryResponse response = transformer . createResponse ( eventList ) ; Assert . assertTrue ( response instanceof DefaultEventQueryResponse ) ; DefaultEventQueryResponse eventQueryResponse = ( DefaultEventQueryResponse ) response ; code_block = ForStatement ; Assert . assertTrue ( expected . isEmpty ( ) ) ; }
protected void runTestQueryWithUniqueness ( Set < Set < String > > expected , String querystr , Date startDate , Date endDate , Map < String , String > extraParms , AccumuloClient client ) throws Exception { log . debug ( "runTestQueryWithUniqueness" ) ; QueryImpl settings = new QueryImpl ( ) ; settings . setBeginDate ( startDate ) ; settings . setEndDate ( endDate ) ; settings . setPagesize ( Integer . MAX_VALUE ) ; settings . setQueryAuthorizations ( auths . serialize ( ) ) ; settings . setQuery ( querystr ) ; settings . setParameters ( extraParms ) ; settings . setId ( UUID . randomUUID ( ) ) ; log . debug ( "query: " + settings . getQuery ( ) ) ; log . debug ( "logic: " + settings . getQueryLogicName ( ) ) ; GenericQueryConfiguration config = logic . initialize ( client , settings , authSet ) ; logic . setupQuery ( config ) ; DocumentTransformer transformer = ( DocumentTransformer ) ( logic . getTransformer ( settings ) ) ; TransformIterator iter = new DatawaveTransformIterator ( logic . iterator ( ) , transformer ) ; List < Object > eventList = new ArrayList < > ( ) ; code_block = WhileStatement ; BaseQueryResponse response = transformer . createResponse ( eventList ) ; Assert . assertTrue ( response instanceof DefaultEventQueryResponse ) ; DefaultEventQueryResponse eventQueryResponse = ( DefaultEventQueryResponse ) response ; code_block = ForStatement ; Assert . assertTrue ( expected . isEmpty ( ) ) ; }
protected void runTestQueryWithUniqueness ( Set < Set < String > > expected , String querystr , Date startDate , Date endDate , Map < String , String > extraParms , AccumuloClient client ) throws Exception { log . debug ( "runTestQueryWithUniqueness" ) ; QueryImpl settings = new QueryImpl ( ) ; settings . setBeginDate ( startDate ) ; settings . setEndDate ( endDate ) ; settings . setPagesize ( Integer . MAX_VALUE ) ; settings . setQueryAuthorizations ( auths . serialize ( ) ) ; settings . setQuery ( querystr ) ; settings . setParameters ( extraParms ) ; settings . setId ( UUID . randomUUID ( ) ) ; log . debug ( "query: " + settings . getQuery ( ) ) ; log . debug ( "logic: " + settings . getQueryLogicName ( ) ) ; GenericQueryConfiguration config = logic . initialize ( client , settings , authSet ) ; logic . setupQuery ( config ) ; DocumentTransformer transformer = ( DocumentTransformer ) ( logic . getTransformer ( settings ) ) ; TransformIterator iter = new DatawaveTransformIterator ( logic . iterator ( ) , transformer ) ; List < Object > eventList = new ArrayList < > ( ) ; code_block = WhileStatement ; BaseQueryResponse response = transformer . createResponse ( eventList ) ; Assert . assertTrue ( response instanceof DefaultEventQueryResponse ) ; DefaultEventQueryResponse eventQueryResponse = ( DefaultEventQueryResponse ) response ; code_block = ForStatement ; Assert . assertTrue ( expected . isEmpty ( ) ) ; }
public void test() { try { count ++ ; TextMessage tm = ( TextMessage ) m ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; } catch ( Exception e ) { log . error ( "Caught exception" , e ) ; failed = true ; latch . countDown ( ) ; } }
public void test() { try { longVal = ( Long ) value ; } catch ( ClassCastException cce ) { logger . warn ( "Could not convert a Nuxeo integer value to its string equivalent: " + cce . getMessage ( ) ) ; return "" ; } }
public void test() { try { spi ( grid0 ) . waitForBlocked ( ) ; } catch ( InterruptedException e ) { log . error ( "Waiting is interrupted." , e ) ; } }
public void test() { if ( QUERY_LOGGER . isDebugEnabled ( ) ) { QUERY_LOGGER . debug ( "Executing doCount: {}" , request ) ; } }
public void test() { if ( eventTableOrNull == null ) { QUERY_PLAN_LOG . info ( prefix + indexText ) ; } else { QUERY_PLAN_LOG . info ( prefix + indexText + eventTableOrNull . toQueryPlan ( ) ) ; } }
public void test() { if ( eventTableOrNull == null ) { QUERY_PLAN_LOG . info ( prefix + indexText ) ; } else { QUERY_PLAN_LOG . info ( prefix + indexText + eventTableOrNull . toQueryPlan ( ) ) ; } }
public void test() { try { code_block = IfStatement ; if ( null != result ) return result ; StringApiResponse response = new StringApiResponse ( ) ; response . setResult ( SUCCESS , null ) ; result = response ; } catch ( NoSuchMethodException e ) { _logger . error ( "No such method '{}' of class '{}'" , apiMethod . getSpringBeanMethod ( ) , bean . getClass ( ) , e ) ; throw new ApiException ( IApiErrorCodes . API_METHOD_ERROR , "Method not supported - " + this . buildApiSignature ( apiMethod ) , Response . Status . INTERNAL_SERVER_ERROR ) ; } catch ( InvocationTargetException e ) { code_block = IfStatement ; } catch ( Throwable t ) { _logger . error ( "Error invoking method '{}' of class '{}'" , apiMethod . getSpringBeanMethod ( ) , bean . getClass ( ) , t ) ; throw t ; } }
private void successMessage ( String msg ) { messages . add ( msg ) ; log . trace ( msg ) ; }
public void test() { try { int elementIndex = this . getElementIndex ( ) ; ListAttributeInterface currentAttribute = ( ListAttributeInterface ) entity . getAttribute ( this . getAttributeName ( ) ) ; code_block = IfStatement ; _logger . debug ( "Element oy type {} removed fomr the list {}" , currentAttribute . getNestedAttributeTypeCode ( ) , currentAttribute . getName ( ) ) ; } catch ( Throwable t ) { _logger . error ( "error in removeListElement" , t ) ; return FAILURE ; } }
public void test() { try { RpObject rpObj = new RpObject ( getDnForRp ( rp . getOxdId ( ) ) , rp . getOxdId ( ) , Jackson2 . serializeWithoutNulls ( rp ) ) ; this . persistenceEntryManager . merge ( rpObj ) ; LOG . debug ( "RP updated successfully. RP : {} " , rpObj ) ; return true ; } catch ( Exception e ) { LOG . error ( "Failed to update RP: {} " , rp , e ) ; } }
public void test() { try { RpObject rpObj = new RpObject ( getDnForRp ( rp . getOxdId ( ) ) , rp . getOxdId ( ) , Jackson2 . serializeWithoutNulls ( rp ) ) ; this . persistenceEntryManager . merge ( rpObj ) ; LOG . debug ( "RP updated successfully. RP : {} " , rpObj ) ; return true ; } catch ( Exception e ) { LOG . error ( "Failed to update RP: {} " , rp , e ) ; } }
public void test() { try { Properties props = new Properties ( ) ; props . load ( new FileReader ( desc ) ) ; return props ; } catch ( IOException e ) { log . warn ( "Unable to load presets" , e ) ; } }
public void test() { if ( ! this . worldDir . delete ( ) ) { this . logger . warn ( "Could not delete temp directory" ) ; } }
private void verifyContextualFilter ( Filter filter , String expectedPropertyName , String expectedSearchTerm ) { LikeFilterImpl likeFilter = ( LikeFilterImpl ) filter ; AttributeExpressionImpl expression = ( AttributeExpressionImpl ) likeFilter . getExpression ( ) ; LOGGER . debug ( "propertyName = {}" , expression . getPropertyName ( ) ) ; assertEquals ( expectedPropertyName , expression . getPropertyName ( ) ) ; String extractedSearchTerm = likeFilter . getLiteral ( ) ; LOGGER . debug ( "extractedSearchTerm = [{}]" , extractedSearchTerm ) ; assertEquals ( expectedSearchTerm , extractedSearchTerm ) ; }
private void verifyContextualFilter ( Filter filter , String expectedPropertyName , String expectedSearchTerm ) { LikeFilterImpl likeFilter = ( LikeFilterImpl ) filter ; AttributeExpressionImpl expression = ( AttributeExpressionImpl ) likeFilter . getExpression ( ) ; LOGGER . debug ( "propertyName = {}" , expression . getPropertyName ( ) ) ; assertEquals ( expectedPropertyName , expression . getPropertyName ( ) ) ; String extractedSearchTerm = likeFilter . getLiteral ( ) ; LOGGER . debug ( "extractedSearchTerm = [{}]" , extractedSearchTerm ) ; assertEquals ( expectedSearchTerm , extractedSearchTerm ) ; }
public void test() { try { theClient . update ( ) . resource ( next ) . execute ( ) ; } catch ( BaseServerResponseException e ) { ourLog . warn ( "Server responded HTTP " + e . getStatusCode ( ) + ": " + e . toString ( ) ) ; } }
public void test() { if ( LOGGER . isInfoEnabled ( ) ) { LOGGER . info ( "Global transaction is disabled." ) ; } }
public void test() { try { Map < Integer , String > oldWorkerGroupMap = workerGroupDao . queryAllOldWorkerGroup ( dataSource . getConnection ( ) ) ; Map < Integer , String > processDefinitionJsonMap = processDefinitionDao . queryAllProcessDefinition ( dataSource . getConnection ( ) ) ; code_block = ForStatement ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "update process definition json workergroup error" , e ) ; } }
public void test() { try { Ignite ignite = grid == null ? startGrid ( idx ) : grid ; IgniteCache < Object , Object > cache = getCache ( ignite ) ; cache . put ( ignite . cluster ( ) . localNode ( ) . id ( ) , UUID . randomUUID ( ) ) ; code_block = WhileStatement ; } catch ( Exception e ) { log . error ( "Unexpected error: " + e , e ) ; failed . set ( true ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "After encoding" + ( ( Command ) msg ) . toString ( ) ) ; } }
public void test() { if ( signalDocument == null ) { logger . warn ( "Target document doesn't exist or is not a signal" ) ; return ; } }
public void test() { try { Pair < String , CT > pair = ServerClient . getConnection ( context , factory , true ) ; server = pair . getFirst ( ) ; client = pair . getSecond ( ) ; return exec . execute ( client ) ; } catch ( TApplicationException tae ) { throw new AccumuloServerException ( server , tae ) ; } catch ( TTransportException tte ) { log . debug ( "ClientService request failed " + server + ", retrying ... " , tte ) ; sleepUninterruptibly ( 100 , TimeUnit . MILLISECONDS ) ; } finally { if ( client != null ) ServerClient . close ( client ) ; } }
public void test() { try { admin . removeCustomizedStateConfig ( clusterId ) ; } catch ( Exception ex ) { LOG . error ( "Cannot remove CustomizedStateConfig from cluster: {}, Exception: {}" , clusterId , ex ) ; return serverError ( ex ) ; } }
public void test() { try { log . debug ( "Validating usecase: {}" , usecase ) ; JsonObject response = new JsonObject ( ) ; response . addProperty ( "UniqueUsecase" , autoMLConfigDAL . isUsecaseExisting ( usecase ) ) ; response . addProperty ( "Usecase" , usecase ) ; return response ; } catch ( Exception e ) { log . error ( e . getMessage ( ) ) ; throw new InsightsCustomException ( e . getMessage ( ) ) ; } }
public void test() { if ( command . intValue ( ) > 6 ) { addContentItemToPresetContainer ( command . intValue ( ) , currentContentItem ) ; } else { logger . warn ( "{}: Only PresetID >6 is allowed" , handler . getDeviceName ( ) ) ; } }
private CdiContainer doCreateContainer ( Bundle bundle ) { Set < Bundle > extensions = new HashSet < > ( ) ; findExtensions ( bundle , extensions ) ; log . info ( "creating CDI container for bean bundle {} with extension bundles {}" , bundle , extensions ) ; return factory . createContainer ( bundle , extensions ) ; }
public void test() { switch ( e . getResponseCode ( ) ) { case HttpStatus . SC_NOT_FOUND : fileid . cache ( file , null ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "table:" + metadataTableName + " stateHasChanged(" + reader + ", " + value + ") to " + triStateName ) ; } }
public void test() { try { watcher . registerTriState ( triStateName , new SharedTriStateListener ( ) code_block = "" ; ) ; code_block = IfStatement ; } catch ( Exception e ) { log . error ( e ) ; } }
public void test() { if ( commandQueue . size ( ) >= WEB_REQUEST_QUEUE_MAX_SIZE ) { logger . debug ( "Could not add command to command queue because queue is already full. Maybe SolarEdge is down?" ) ; } else { logger . warn ( "Could not add command to queue - IllegalStateException" ) ; } }
public void test() { if ( commandQueue . size ( ) >= WEB_REQUEST_QUEUE_MAX_SIZE ) { logger . debug ( "Could not add command to command queue because queue is already full. Maybe SolarEdge is down?" ) ; } else { logger . warn ( "Could not add command to queue - IllegalStateException" ) ; } }
public void test() { try { log . debug ( "Starting..." ) ; cleanupAndMakeTmpTaskDir ( ) ; registerLocationListener ( ) ; restoreRestorableTasks ( ) ; initAssignedTasks ( ) ; initCompletedTasks ( ) ; scheduleCompletedTasksCleanup ( ) ; lifecycleLock . started ( ) ; log . debug ( "Started." ) ; } catch ( Exception e ) { log . makeAlert ( e , "Exception starting WorkerTaskManager." ) . emit ( ) ; throw e ; } finally { lifecycleLock . exitStart ( ) ; } }
public void test() { try { log . debug ( "Starting..." ) ; cleanupAndMakeTmpTaskDir ( ) ; registerLocationListener ( ) ; restoreRestorableTasks ( ) ; initAssignedTasks ( ) ; initCompletedTasks ( ) ; scheduleCompletedTasksCleanup ( ) ; lifecycleLock . started ( ) ; log . debug ( "Started." ) ; } catch ( Exception e ) { log . makeAlert ( e , "Exception starting WorkerTaskManager." ) . emit ( ) ; throw e ; } finally { lifecycleLock . exitStart ( ) ; } }
private void activateResource ( String resourceId ) throws ResourceException , CorruptStateException { logger . debug ( "Activating resource " + resourceId + " ..." ) ; Resource resource = ( Resource ) getResource ( resourceId ) ; checkResourceCanBeStarted ( resource ) ; code_block = IfStatement ; logger . debug ( "  Obtaining capabilities..." ) ; List < ? extends ICapability > oldCapabilities = resource . getCapabilities ( ) ; List < ICapability > capabilities = createCapabilities ( resource ) ; resource . setCapabilities ( capabilities ) ; logger . debug ( "  Capabilities obtained. Loading bootstrapper..." ) ; IResourceBootstrapper oldBootstrapper = resource . getBootstrapper ( ) ; code_block = IfStatement ; logger . debug ( "  Bootstrapper loaded" ) ; IProfile oldProfile = resource . getProfile ( ) ; code_block = IfStatement ; code_block = TryStatement ;  }
public void test() { try { destroyProtocolSessions ( resourceId ) ; } catch ( Exception e1 ) { logger . warn ( "Error destroying protocol sessions" , e1 ) ; } }
private void activateResource ( String resourceId ) throws ResourceException , CorruptStateException { logger . debug ( "Activating resource " + resourceId + " ..." ) ; Resource resource = ( Resource ) getResource ( resourceId ) ; checkResourceCanBeStarted ( resource ) ; code_block = IfStatement ; logger . debug ( "  Obtaining capabilities..." ) ; List < ? extends ICapability > oldCapabilities = resource . getCapabilities ( ) ; List < ICapability > capabilities = createCapabilities ( resource ) ; resource . setCapabilities ( capabilities ) ; logger . debug ( "  Capabilities obtained. Loading bootstrapper..." ) ; IResourceBootstrapper oldBootstrapper = resource . getBootstrapper ( ) ; code_block = IfStatement ; logger . debug ( "  Bootstrapper loaded" ) ; IProfile oldProfile = resource . getProfile ( ) ; code_block = IfStatement ; code_block = TryStatement ;  }
private void activateResource ( String resourceId ) throws ResourceException , CorruptStateException { logger . debug ( "Activating resource " + resourceId + " ..." ) ; Resource resource = ( Resource ) getResource ( resourceId ) ; checkResourceCanBeStarted ( resource ) ; code_block = IfStatement ; logger . debug ( "  Obtaining capabilities..." ) ; List < ? extends ICapability > oldCapabilities = resource . getCapabilities ( ) ; List < ICapability > capabilities = createCapabilities ( resource ) ; resource . setCapabilities ( capabilities ) ; logger . debug ( "  Capabilities obtained. Loading bootstrapper..." ) ; IResourceBootstrapper oldBootstrapper = resource . getBootstrapper ( ) ; code_block = IfStatement ; logger . debug ( "  Bootstrapper loaded" ) ; IProfile oldProfile = resource . getProfile ( ) ; code_block = IfStatement ; code_block = TryStatement ;  }
private void activateResource ( String resourceId ) throws ResourceException , CorruptStateException { logger . debug ( "Activating resource " + resourceId + " ..." ) ; Resource resource = ( Resource ) getResource ( resourceId ) ; checkResourceCanBeStarted ( resource ) ; code_block = IfStatement ; logger . debug ( "  Obtaining capabilities..." ) ; List < ? extends ICapability > oldCapabilities = resource . getCapabilities ( ) ; List < ICapability > capabilities = createCapabilities ( resource ) ; resource . setCapabilities ( capabilities ) ; logger . debug ( "  Capabilities obtained. Loading bootstrapper..." ) ; IResourceBootstrapper oldBootstrapper = resource . getBootstrapper ( ) ; code_block = IfStatement ; logger . debug ( "  Bootstrapper loaded" ) ; IProfile oldProfile = resource . getProfile ( ) ; code_block = IfStatement ; code_block = TryStatement ;  }
public void test() { try { code_block = TryStatement ;  } catch ( ResourceException re ) { logger . debug ( "Rolling back activation..." ) ; resource . setCapabilities ( oldCapabilities ) ; resource . setBootstrapper ( oldBootstrapper ) ; resource . setProfile ( oldProfile ) ; code_block = IfStatement ; logger . debug ( "Rolling back done" ) ; throw re ; } }
public void test() { try { destroyProtocolSessions ( resourceId ) ; } catch ( Exception e ) { logger . warn ( "Error destroying protocol sessions" , e ) ; } }
public void test() { try { code_block = TryStatement ;  } catch ( ResourceException re ) { logger . debug ( "Rolling back activation..." ) ; resource . setCapabilities ( oldCapabilities ) ; resource . setBootstrapper ( oldBootstrapper ) ; resource . setProfile ( oldProfile ) ; code_block = IfStatement ; logger . debug ( "Rolling back done" ) ; throw re ; } }
@ Test public void fileImageInputStreamExtImpl ( ) { LOGGER . info ( "Testing capabilities of FileImageInputStreamExt" ) ; code_block = TryStatement ;  LOGGER . info ( "Testing capabilities of URLImageInputStreamSpi: SUCCESS!!!" ) ; }
public void test() { if ( refreshPathMappings != null ) { refreshPaths = refreshPathMappings . get ( UifConstants . ViewPhases . APPLY_MODEL ) ; } }
public void test() { try { List < ExecuteBuildingBlock > flowsToExecute = ( List < ExecuteBuildingBlock > ) execution . getVariable ( "flowsToExecute" ) ; String handlingCode = ( String ) execution . getVariable ( HANDLINGCODE ) ; final boolean aLaCarte = ( boolean ) execution . getVariable ( BBConstants . G_ALACARTE ) ; int currentSequence = ( int ) execution . getVariable ( BBConstants . G_CURRENT_SEQUENCE ) ; logger . debug ( "Current Sequence: {}" , currentSequence ) ; ExecuteBuildingBlock ebb = flowsToExecute . get ( currentSequence - 1 ) ; String bbFlowName = ebb . getBuildingBlock ( ) . getBpmnFlowName ( ) ; code_block = IfStatement ; flowManipulatorListenerRunner . postModifyFlows ( flowsToExecute , new DelegateExecutionImpl ( execution ) ) ; } catch ( Exception ex ) { logger . error ( "Exception in postProcessingExecuteBB" , ex ) ; workflowAction . buildAndThrowException ( execution , "Failed to post process Execute BB" ) ; } }
@ AfterAll static void cdiContainerDown ( ) { KafkaConnector factory = getInstance ( KafkaConnector . class , KAFKA_CONNECTOR_LITERAL ) . get ( ) ; Collection < KafkaPublisher < ? , ? > > resources = factory . resources ( ) ; assertFalse ( resources . isEmpty ( ) ) ; cdiContainer . close ( ) ; assertTrue ( resources . isEmpty ( ) ) ; LOGGER . info ( "Container destroyed" ) ; }
public void test() { if ( value instanceof String ) { log . debug ( "Found plugin {}" , value ) ; extensionInfo . plugins . add ( ( String ) value ) ; } else-if ( value instanceof String [ ] ) { log . debug ( "Found plugins {}" , Arrays . toString ( ( String [ ] ) value ) ) ; extensionInfo . plugins . addAll ( Arrays . asList ( ( String [ ] ) value ) ) ; } else { log . debug ( "Found plugin {}" , value . toString ( ) ) ; extensionInfo . plugins . add ( value . toString ( ) ) ; } }
public void test() { if ( value instanceof String ) { log . debug ( "Found plugin {}" , value ) ; extensionInfo . plugins . add ( ( String ) value ) ; } else-if ( value instanceof String [ ] ) { log . debug ( "Found plugins {}" , Arrays . toString ( ( String [ ] ) value ) ) ; extensionInfo . plugins . addAll ( Arrays . asList ( ( String [ ] ) value ) ) ; } else { log . debug ( "Found plugin {}" , value . toString ( ) ) ; extensionInfo . plugins . add ( value . toString ( ) ) ; } }
public void test() { if ( value instanceof String ) { log . debug ( "Found plugin {}" , value ) ; extensionInfo . plugins . add ( ( String ) value ) ; } else-if ( value instanceof String [ ] ) { log . debug ( "Found plugins {}" , Arrays . toString ( ( String [ ] ) value ) ) ; extensionInfo . plugins . addAll ( Arrays . asList ( ( String [ ] ) value ) ) ; } else { log . debug ( "Found plugin {}" , value . toString ( ) ) ; extensionInfo . plugins . add ( value . toString ( ) ) ; } }
public void test() { try { TreeMapState ps = new TreeMapState ( this , tree , path , bundle ) ; ps . process ( ) ; processNodes . addAndGet ( ps . touched ( ) ) ; } catch ( RuntimeException ex ) { throw ex ; } catch ( Exception ex ) { log . warn ( "" , ex ) ; } }
public void test() { if ( future . isSuccess ( ) ) { LOG . debug ( "Request {} was successfully answered after {} ms." , request , durationMillis ) ; stats . reportSuccessfulRequest ( durationMillis ) ; } else { LOG . debug ( "Request {} failed after {} ms" , request , durationMillis , future . cause ( ) ) ; stats . reportFailedRequest ( ) ; } }
public void test() { if ( future . isSuccess ( ) ) { LOG . debug ( "Request {} was successfully answered after {} ms." , request , durationMillis ) ; stats . reportSuccessfulRequest ( durationMillis ) ; } else { LOG . debug ( "Request {} failed after {} ms" , request , durationMillis , future . cause ( ) ) ; stats . reportFailedRequest ( ) ; } }
public void test() { if ( ! "rwx" . equals ( permissions ) ) { LOG . warn ( "The provided home directory '{}' for namespace '{}' has group permissions of '{}'. It is " + "recommended to set the group permissions to 'rwx'" , customNamespacedLocation . toString ( ) , namespaceMeta . getNamespaceId ( ) , permissions ) ; } }
public void test() { if ( ! "rwx" . equals ( permissions ) ) { LOG . warn ( "The provided home directory '{}' for namespace '{}' has group permissions of '{}'. It is " + "recommended to set the group permissions to 'rwx'" , customNamespacedLocation . toString ( ) , namespaceMeta . getNamespaceId ( ) , permissions ) ; } }
public void test() { if ( _log . isWarnEnabled ( ) ) { _log . warn ( "Unable to get guest or current user ID" , principalException ) ; } }
public void test() { try { return this . componentManager . getInstance ( DashboardRenderer . class , layout ) ; } catch ( ComponentLookupException e ) { this . logger . warn ( "Could not find the Dashboard renderer for layout \"" + layout + "\"" ) ; return null ; } }
public void test() { if ( _log . isWarnEnabled ( ) ) { _log . warn ( "Token " + token + " is invalid and was deleted" ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( exception , exception ) ; } }
public void test() { if ( providerEventStates != null ) { recordEventState ( provider , providerEventStates ) ; } else { logger . info ( "Could not initiate event tracker from GII provider {}" , provider ) ; } }
public List findByExample ( StgMbMasGef instance ) { log . debug ( "finding StgMbMasGef instance by example" ) ; code_block = TryStatement ;  }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.StgMbMasGef" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.StgMbMasGef" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
@ Test public void testHandleGetRequest ( ) { String jsonResponse = JsonLoader . loadJson ( DomainHelper . getRestUrlV2 ( ) + "/content/emojis" ) ; logger . info ( "jsonResponse: " + jsonResponse ) ; JSONArray emojisJSONArray = new JSONArray ( jsonResponse ) ; logger . info ( "emojisJSONArray.length(): " + emojisJSONArray . length ( ) ) ; assertThat ( emojisJSONArray . length ( ) > 0 , is ( true ) ) ; JSONObject emojiJsonObject = emojisJSONArray . getJSONObject ( 0 ) ; assertThat ( emojiJsonObject . getString ( "glyph" ) , not ( nullValue ( ) ) ) ; }
@ Test public void testHandleGetRequest ( ) { String jsonResponse = JsonLoader . loadJson ( DomainHelper . getRestUrlV2 ( ) + "/content/emojis" ) ; logger . info ( "jsonResponse: " + jsonResponse ) ; JSONArray emojisJSONArray = new JSONArray ( jsonResponse ) ; logger . info ( "emojisJSONArray.length(): " + emojisJSONArray . length ( ) ) ; assertThat ( emojisJSONArray . length ( ) > 0 , is ( true ) ) ; JSONObject emojiJsonObject = emojisJSONArray . getJSONObject ( 0 ) ; assertThat ( emojiJsonObject . getString ( "glyph" ) , not ( nullValue ( ) ) ) ; }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "[" + id_ + "] fillText('" + text + "', " + x + ", " + y + ")" ) ; } }
public void test() { if ( ! Objects . equals ( rawPluginPath , transformedPluginPath ) ) { log . warn ( "Variables cannot be used in the 'plugin.path' property, since the property is " + "used by plugin scanning before the config providers that replace the " + "variables are initialized. The raw value '{}' was used for plugin scanning, as " + "opposed to the transformed value '{}', and this may cause unexpected results." , rawPluginPath , transformedPluginPath ) ; } }
public void test() { try { code_block = IfStatement ; if ( fieldNames . isEmpty ( ) ) return ; code_block = IfStatement ; } catch ( IllegalArgumentException e ) { Log . error ( "Failed to read help descriptionModel" , e ) ; } }
public void test() { try { closeProducer ( producer ) ; closeSession ( session ) ; connection . close ( ) ; } catch ( JMSException e ) { LOGGER . error ( "Error while closing session" , e ) ; } }
public void failedToCapture ( SnapshotStateId componentId ) { LOG . debug ( "Failed to capture snapshot %d for component %s" , componentId . getSnapshotId ( ) , componentId ) ; updateCapture ( componentId , SnapshotComponentCounter . ComponentState . FAILED ) ; }
public void test() { try { mStream . close ( ) ; } catch ( IOException e ) { log . info ( "Error closing mjpeg frame grabber." , e ) ; return ; } }
@ Override public void start ( ) throws Exception { mRunning = true ; mJournalSystem . start ( ) ; startMasters ( false ) ; LOG . info ( "Secondary started" ) ; code_block = IfStatement ; code_block = TryStatement ;  code_block = WhileStatement ; }
public void test() { try { mLeaderSelector . start ( getRpcAddress ( ) ) ; } catch ( IOException e ) { LOG . error ( e . getMessage ( ) , e ) ; throw new RuntimeException ( e ) ; } }
public void test() { try { File file = P2Cache . getCacheFile ( indexUrl ) ; code_block = TryStatement ;  } catch ( Exception e ) { LOG . trace ( "writing index file to cache failed" , e ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( " Continuing to " + latData . getLatitudeBand ( ) + index ) ; } }
public void test() { try { return new NaturalDateParser ( ) . parse ( string ) . asMap ( ) ; } catch ( NaturalDateParser . DateNotParsableException e ) { LOG . debug ( "Could not parse from natural date: " + string , e ) ; throw new WebApplicationException ( e , 422 ) ; } }
public void test() { if ( user != null ) { sortedUsers . add ( user ) ; } else { log . warn ( "Group with id '" + id + "' not found in UserGroupCache. groupIds string was: " + userIds ) ; } }
@ Override public void onLinkRemoteOpen ( Event event ) { log . debug ( "{} receiver link with link correlation id {} was successfully opened" , getLinkInstanceType ( ) , this . linkCorrelationId ) ; this . linkStateCallback . onReceiverLinkRemoteOpen ( ) ; }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Processing object type: {} (Month: {})..." , objectType , month ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Executing processor {} on month {}..." , processor . getClass ( ) . getSimpleName ( ) , month ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Executing processor {} on month {}...DONE" , processor . getClass ( ) . getSimpleName ( ) , month ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Processing object type: {} (Month: {})...DONE" , objectType , month ) ; } }
public void test() { if ( sn != UNSET_SUBNETWORK && sn != UNCLEAR_SUBNETWORK ) { LOGGER . error ( "subnetworkId for node " + nodeId + " (" + createPoint ( graph , nodeId ) + ") already set (" + sn + "). " + "Cannot change to " + subnetworkId ) ; failed . set ( true ) ; return false ; } }
public void test() { try { int retVal = remoteTaskAssignmentEJB . insertOneTaskAssignment ( id , userId ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Error in assignNewTaskToUser for id : " + id + " and userID : " + userId ) ; } }
private void handleFlowFileTooBig ( final ProcessSession session , final FlowFile flowFileCandidate , final String message ) { final FlowFile tooBig = session . putAttribute ( flowFileCandidate , message , "record too big " + flowFileCandidate . getSize ( ) + " max allowed " + MAX_MESSAGE_SIZE ) ; session . transfer ( tooBig , REL_FAILURE ) ; getLogger ( ) . error ( "Failed to publish to kinesis records {} because the size was greater than {} bytes" , tooBig , MAX_MESSAGE_SIZE ) ; }
@ Override byte [ ] execute ( ) throws KeeperException , InterruptedException { LOG . debug ( "ZK Call - getData [{0}] [{1}] [{2}]" , path , watch , stat ) ; return ZooKeeperClient . super . getData ( path , watch , stat ) ; }
public void test() { if ( leak ) { long leaks = ( created + acquired ) - ( released + discarded ) ; log . warn ( "{}{} ({}) usage (leaks detected: {}) [pooled: {}, created: {}, acquired: {} released: {}, discarded: {}]" , name , id , uri , leaks , pooled , created , acquired , released , discarded ) ; } else { log . info ( "{}{} ({}) usage [pooled: {}, created: {}, acquired: {} released: {}, discarded: {}]" , name , id , uri , pooled , created , acquired , released , discarded ) ; } }
public void test() { if ( leak ) { long leaks = ( created + acquired ) - ( released + discarded ) ; log . warn ( "{}{} ({}) usage (leaks detected: {}) [pooled: {}, created: {}, acquired: {} released: {}, discarded: {}]" , name , id , uri , leaks , pooled , created , acquired , released , discarded ) ; } else { log . info ( "{}{} ({}) usage [pooled: {}, created: {}, acquired: {} released: {}, discarded: {}]" , name , id , uri , pooled , created , acquired , released , discarded ) ; } }
public void test() { try { String cusId = FdahpStudyDesignerUtil . isEmpty ( request . getParameter ( "cusId" ) ) ? "" : request . getParameter ( "cusId" ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "StudyController - resetStudy - ERROR" , e ) ; } }
public void test() { if ( s_logger . isDebugEnabled ( ) ) { s_logger . debug ( "Notifying management server join event took " + profiler . getDurationInMillis ( ) + " ms" ) ; } }
public void test() { if ( profiler . getDurationInMillis ( ) > 1000 ) { code_block = IfStatement ; } else { s_logger . warn ( "Notifying management server join event took " + profiler . getDurationInMillis ( ) + " ms" ) ; } }
public void test() { if ( s_logger . isDebugEnabled ( ) ) { s_logger . debug ( "Notifying management server leave event took " + profiler . getDurationInMillis ( ) + " ms" ) ; } }
public void test() { if ( profiler . getDurationInMillis ( ) > 1000 ) { code_block = IfStatement ; } else { s_logger . warn ( "Notifying management server leave event took " + profiler . getDurationInMillis ( ) + " ms" ) ; } }
public void test() { try { code_block = SwitchStatement ; } catch ( final Throwable e ) { s_logger . warn ( "Unexpected exception during cluster notification. " , e ) ; } }
private Future < Void > invokePermissionsForModule ( Tenant tenant , ModuleDescriptor mdFrom , ModuleDescriptor mdTo , ModuleDescriptor permsModule , ProxyContext pc ) { PermissionList pl ; InterfaceDescriptor permInt = permsModule . getSystemInterface ( "_tenantPermissions" ) ; String permIntVer = permInt . getVersion ( ) ; code_block = SwitchStatement ; String pljson = Json . encodePrettily ( pl ) ; logger . info ( "tenantPerms Req: {}" , pljson ) ; String permPath = "" ; List < RoutingEntry > routingEntries = permInt . getAllRoutingEntries ( ) ; ModuleInstance permInst = null ; code_block = IfStatement ; code_block = IfStatement ; logger . debug ( "tenantPerms: {} and {}" , permsModule . getId ( ) , permPath ) ; code_block = IfStatement ; return proxyService . callSystemInterface ( tenant . getId ( ) , permInst , pljson , pc ) . compose ( cres code_block = LoopStatement ; ) ; }
private Properties readPropertyFile ( String file ) throws IOException { String fileName = file . startsWith ( "/" ) ? file : "/" + file ; LOGGER . info ( "Reading properties from: " + fileName + ". Will try classpath, then file system." ) ; return Util . readProperties ( fileName ) ; }
public void test() { try { riskIncidence . setId ( IdUtils . createUUID ( ) ) ; riskIncidence . setDetectedOn ( new Date ( ) ) ; String riskIncidenceAsJson = JsonUtils . getJsonFromObject ( riskIncidence ) ; StoragePath riskIncidencePath = ModelUtils . getRiskIncidenceStoragePath ( riskIncidence . getId ( ) ) ; storage . createBinary ( riskIncidencePath , new StringContentPayload ( riskIncidenceAsJson ) , false ) ; } catch ( GenericException | RequestNotValidException | AuthorizationDeniedException | NotFoundException | AlreadyExistsException e ) { LOGGER . error ( "Error creating risk incidence in storage" , e ) ; } }
public void test() { if ( records == null || records . isEmpty ( ) ) { log . warn ( "Could not parse any records from given xml - returning null." ) ; } else-if ( records . size ( ) == 1 ) { result = records . get ( 0 ) ; } else-if ( unitQualifier == null ) { log . warn ( "Got multiple records from given xml, but no unitQualifier set - returning first record as a guess." ) ; result = records . get ( 0 ) ; } else { code_block = ForStatement ; code_block = IfStatement ; } }
public void test() { if ( result == null ) { log . warn ( "Got multiple records from xml but none matched unitQualifier - returning null" ) ; } }
public void test() { try { String inputPath = "../../data/census/census_148d_test.dummy" ; String loadPath = LOCAL_FS + TMP_PATH + "/model/deepFM" ; String predictPath = LOCAL_FS + TMP_PATH + "/predict" ; conf . set ( AngelConf . ANGEL_PREDICT_DATA_PATH , inputPath ) ; conf . set ( AngelConf . ANGEL_LOAD_MODEL_PATH , loadPath ) ; conf . set ( AngelConf . ANGEL_PREDICT_PATH , predictPath ) ; conf . set ( AngelConf . ANGEL_ACTION_TYPE , MLConf . ANGEL_ML_PREDICT ( ) ) ; GraphRunner runner = new GraphRunner ( ) ; runner . predict ( conf ) ; } catch ( Exception x ) { LOG . error ( "run predictTest failed " , x ) ; throw x ; } }
public void test() { try { redirect ( getUserPageUrl ( ) ) ; } catch ( IOException e ) { LOGGER . error ( "Error reloading user page" , e ) ; } }
public void test() { if ( e < EPS_MIN ) { String msg = String . format ( "epsilon:%.2g less than esp_min=%.2g" , eps , EPS_MIN ) ; logger . warn ( msg ) ; e = max ( eps , EPS_MIN ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( exception , exception ) ; } }
public String pom ( ) { String requestUrl = this . base + "/pom.xml" ; log . info ( "Will send a request to [" + requestUrl + "]" ) ; return this . restTemplate . exchange ( RequestEntity . get ( URI . create ( requestUrl ) ) . accept ( mediaTypes ( ) ) . build ( ) , String . class ) . getBody ( ) ; }
public DeviceConnection connect ( final DeviceConnectionParameters deviceConnectionParameters , final String organisationIdentification , final boolean cacheConnection ) throws ConnectionFailureException { final String deviceIdentification = deviceConnectionParameters . getDeviceIdentification ( ) ; final String serverName = deviceConnectionParameters . getServerName ( ) ; final IED ied = deviceConnectionParameters . getIed ( ) ; code_block = TryStatement ;  final InetAddress inetAddress = this . convertIpAddress ( deviceConnectionParameters . getIpAddress ( ) ) ; LOGGER . info ( "Trying to connect to deviceIdentification: {} at IP address {} using response time-out: {}" , deviceIdentification , deviceConnectionParameters . getIpAddress ( ) , this . responseTimeout ) ; final DateTime startTime = DateTime . now ( ) ; Iec61850ClientBaseEventListener eventListener = null ; code_block = TryStatement ;  final Iec61850Device iec61850Device = this . iec61850DeviceRepository . findByDeviceIdentification ( deviceIdentification ) ; final int port = this . determinePortForIec61850Device ( ied , iec61850Device ) ; final Iec61850ClientAssociation iec61850ClientAssociation = this . iec61850Client . connect ( deviceIdentification , inetAddress , eventListener , port ) ; final ClientAssociation clientAssociation = iec61850ClientAssociation . getClientAssociation ( ) ; clientAssociation . setResponseTimeout ( this . responseTimeout ) ; ServerModel serverModel ; code_block = TryStatement ;  final Iec61850Connection iec61850Connection = new Iec61850Connection ( iec61850ClientAssociation , serverModel , startTime , ied ) ; code_block = IfStatement ; final DeviceConnection connection = new DeviceConnection ( iec61850Connection , deviceIdentification , organisationIdentification , serverName ) ; final DateTime endTime = DateTime . now ( ) ; LOGGER . info ( "Connected to device: {}, fetched server model. Start time: {}, end time: {}, total time in milliseconds: {}" , deviceIdentification , startTime , endTime , endTime . minus ( startTime . getMillis ( ) ) . getMillis ( ) ) ; this . iec61850RtuDeviceReportingService . enableReportingForDevice ( connection , deviceIdentification , serverName ) ; return connection ; }
public DeviceConnection connect ( final DeviceConnectionParameters deviceConnectionParameters , final String organisationIdentification , final boolean cacheConnection ) throws ConnectionFailureException { final String deviceIdentification = deviceConnectionParameters . getDeviceIdentification ( ) ; final String serverName = deviceConnectionParameters . getServerName ( ) ; final IED ied = deviceConnectionParameters . getIed ( ) ; code_block = TryStatement ;  final InetAddress inetAddress = this . convertIpAddress ( deviceConnectionParameters . getIpAddress ( ) ) ; LOGGER . info ( "Trying to connect to deviceIdentification: {} at IP address {} using response time-out: {}" , deviceIdentification , deviceConnectionParameters . getIpAddress ( ) , this . responseTimeout ) ; final DateTime startTime = DateTime . now ( ) ; Iec61850ClientBaseEventListener eventListener = null ; code_block = TryStatement ;  final Iec61850Device iec61850Device = this . iec61850DeviceRepository . findByDeviceIdentification ( deviceIdentification ) ; final int port = this . determinePortForIec61850Device ( ied , iec61850Device ) ; final Iec61850ClientAssociation iec61850ClientAssociation = this . iec61850Client . connect ( deviceIdentification , inetAddress , eventListener , port ) ; final ClientAssociation clientAssociation = iec61850ClientAssociation . getClientAssociation ( ) ; clientAssociation . setResponseTimeout ( this . responseTimeout ) ; ServerModel serverModel ; code_block = TryStatement ;  final Iec61850Connection iec61850Connection = new Iec61850Connection ( iec61850ClientAssociation , serverModel , startTime , ied ) ; code_block = IfStatement ; final DeviceConnection connection = new DeviceConnection ( iec61850Connection , deviceIdentification , organisationIdentification , serverName ) ; final DateTime endTime = DateTime . now ( ) ; LOGGER . info ( "Connected to device: {}, fetched server model. Start time: {}, end time: {}, total time in milliseconds: {}" , deviceIdentification , startTime , endTime , endTime . minus ( startTime . getMillis ( ) ) . getMillis ( ) ) ; this . iec61850RtuDeviceReportingService . enableReportingForDevice ( connection , deviceIdentification , serverName ) ; return connection ; }
public void test() { try { scanner . stop ( ) ; code_block = IfStatement ; } catch ( Exception e ) { LOG . warn ( "Error restarting webapp" , e ) ; } }
public void test() { try { final Patient patient = this . repository . get ( patientId ) ; code_block = IfStatement ; final JSONObject genePanel = this . genePanelFactory . withMatchCount ( withMatchCount ) . build ( patient , excludeRejectedGenes ) . toJSON ( ) ; return Response . ok ( genePanel , MediaType . APPLICATION_JSON_TYPE ) . build ( ) ; } catch ( final SecurityException ex ) { this . slf4Jlogger . error ( "View access denied on patient record [{}]: {}" , patientId , ex . getMessage ( ) ) ; return Response . status ( Response . Status . UNAUTHORIZED ) . build ( ) ; } }
public void test() { if ( ! eachRowArray . get ( 0 ) . isJsonNull ( ) ) { String contentText = eachRowArray . get ( 0 ) . getAsString ( ) ; String contentId = eachRowArray . get ( 1 ) . getAsString ( ) ; code_block = IfStatement ; observationList . add ( contentText ) ; } else { log . debug ( "Worlflow Detail ==== generateChartsConfig for Content {} value is null in kpiContentArray  {} " , eachRowArray . get ( 1 ) . getAsString ( ) , eachData ) ; } }
@ Override public ListenableFuture < ? extends DOMRpcResult > create ( final LogicalDatastoreType store , final YangInstanceIdentifier path , final NormalizedNode < ? , ? > data , final Optional < ModifyAction > defaultOperation ) { LOG . debug ( "{}: Create {} {} via actor {}" , id , store , path , masterActor ) ; masterActor . tell ( new CreateEditConfigRequest ( store , new NormalizedNodeMessage ( path , data ) , defaultOperation . orElse ( null ) ) , ActorRef . noSender ( ) ) ; return createResult ( ) ; }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "[run][listening]" + port ) ; } }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "[run][new socket]" + socket ) ; } }
public void test() { try { code_block = TryStatement ;  code_block = WhileStatement ; } catch ( IOException e ) { logger . warn ( "[run]" + port + "," + e . getMessage ( ) ) ; } finally { } }
public void test() { try { obj . put ( JsonKeys . updateType . name ( ) , "PropertyList" ) ; code_block = ForStatement ; obj . put ( JsonKeys . properties . name ( ) , resultArray ) ; pw . println ( obj . toString ( ) ) ; } catch ( Exception e ) { logger . error ( "Exception:" , e ) ; e . printStackTrace ( ) ; } }
public void test() { try { properties . load ( inStream ) ; return properties ; } catch ( IOException e ) { LOG . error ( "I/O error reading the stream: {}" , e . getMessage ( ) , e ) ; throw new IllegalAccessError ( "test-options.properties could not be found" ) ; } }
@ Test public void test_04_Intron ( ) { Log . debug ( "Test" ) ; if ( verbose ) Log . debug ( transcript ) ; Variant variant = new Variant ( chromosome , 920 , "" , "C" , "" ) ; if ( verbose ) Log . debug ( "Variant: " + variant ) ; if ( verbose ) Log . debug ( "Variant (before): " + variant ) ; Variant variantShifted = variant . realignLeft ( ) ; if ( verbose ) Log . debug ( "Variant (after): " + variantShifted ) ; Assert . assertFalse ( variant == variantShifted ) ; Assert . assertEquals ( 925 , variantShifted . getStart ( ) ) ; }
public void test() { try { mailService . sendApiFeatureStateEmail ( apiFeature , stateValue , email , createStateMailMessage ( ( TenantApiUsageState ) state , apiFeature , stateValue ) ) ; } catch ( ThingsboardException e ) { log . warn ( "[{}] Can't send update of the API state to tenant with provided email [{}]" , state . getTenantId ( ) , email , e ) ; } }
public void test() { if ( StringUtils . isNotEmpty ( email ) ) { result . forEach ( ( apiFeature , stateValue ) code_block = LoopStatement ; ) ; } else { log . warn ( "[{}] Can't send update of the API state to tenant with empty email!" , state . getTenantId ( ) ) ; } }
@ Override protected void doSync ( ) throws IOException { LOG . debug ( "Attempting to sync all data to filesystem" ) ; this . writer . hsync ( ) ; }
public void test() { if ( count % 100 == 0 ) { logger . info ( "read {} entities" , count ) ; } }
@ Override @ SuppressWarnings ( "unchecked" ) public void put ( Object tuple ) { LOG . debug ( "processing tuple" ) ; inputPort . process ( ( T ) tuple ) ; }
public void test() { try { commitBlock ( sessionId , blockId , false ) ; } catch ( BlockDoesNotExistException e ) { LOG . warn ( "Block {} does not exist while being committed." , blockId ) ; } catch ( InvalidWorkerStateException e ) { LOG . debug ( "Invalid worker state while committing block." , e ) ; } }
public void test() { try { commitBlock ( sessionId , blockId , false ) ; } catch ( BlockDoesNotExistException e ) { LOG . warn ( "Block {} does not exist while being committed." , blockId ) ; } catch ( InvalidWorkerStateException e ) { LOG . debug ( "Invalid worker state while committing block." , e ) ; } }
public void test() { try { Compute computeService = createComputeService ( ) ; Compute . Instances . List request = computeService . instances ( ) . list ( project , zone ) ; List < String > instanceIds = new ArrayList < > ( ) ; InstanceList response ; do code_block = "" ; while ( response . getNextPageToken ( ) != null ) ; log . debug ( "Converted to [%s]" , String . join ( "," , instanceIds ) ) ; return instanceIds ; } catch ( Exception e ) { log . error ( e , "Unable to convert IPs to IDs." ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( _NO_SUCH_ENTITY_WITH_PRIMARY_KEY + primaryKey ) ; } }
public void test() { try { doMergeFragments ( fragmentsToMerge ) ; } catch ( Exception e ) { logger . error ( "error happens when merge fragments:" + fragmentsToMerge , e ) ; } }
public void test() { if ( waitForOperation ( context . getGceApi ( ) . operations ( ) , operation ) == 1 ) { logger . warn ( String . format ( "%s operation has timedout: %s\n" , operation . operationType ( ) , operation . httpErrorMessage ( ) ) ) ; } else { logger . info ( String . format ( "Operation %s  was successfully done on %s\n." , operation . operationType ( ) , operation . targetLink ( ) ) ) ; } }
public void test() { if ( waitForOperation ( context . getGceApi ( ) . operations ( ) , operation ) == 1 ) { logger . warn ( String . format ( "%s operation has timedout: %s\n" , operation . operationType ( ) , operation . httpErrorMessage ( ) ) ) ; } else { logger . info ( String . format ( "Operation %s  was successfully done on %s\n." , operation . operationType ( ) , operation . targetLink ( ) ) ) ; } }
public void test() { try { return property . get ( this ) ; } catch ( IllegalAccessException e ) { log . error ( "" , e ) ; } }
@ Test public void importFetched ( ) throws Exception { Repository repository = createTestRepoUsingRepoService ( ) ; getL10nJCommander ( ) . run ( "push" , "-r" , repository . getName ( ) , "-s" , getInputResourcesTestDir ( "source" ) . getAbsolutePath ( ) ) ; Asset asset = assetClient . getAssetByPathAndRepositoryId ( "source-xliff.xliff" , repository . getId ( ) ) ; importTranslations ( asset . getId ( ) , "source-xliff_" , "fr-FR" ) ; importTranslations ( asset . getId ( ) , "source-xliff_" , "ja-JP" ) ; Asset asset2 = assetClient . getAssetByPathAndRepositoryId ( "source2-xliff.xliff" , repository . getId ( ) ) ; importTranslations ( asset2 . getId ( ) , "source2-xliff_" , "fr-FR" ) ; importTranslations ( asset2 . getId ( ) , "source2-xliff_" , "ja-JP" ) ; waitForRepositoryToHaveStringsForTranslations ( repository . getId ( ) ) ; getL10nJCommander ( ) . run ( "drop-export" , "-r" , repository . getName ( ) ) ; final Long dropId = getLastDropIdFromOutput ( outputCapture ) ; logger . debug ( "Mocking the console input" ) ; Console mockConsole = mock ( Console . class ) ; verify ( mockConsole , never ( ) ) . readLine ( Long . class ) ; L10nJCommander l10nJCommander = getL10nJCommander ( ) ; DropImportCommand dropImportCommand = l10nJCommander . getCommand ( DropImportCommand . class ) ; dropImportCommand . console = mockConsole ; int numberOfFrenchTranslationsBefore = getNumberOfFrenchTranslations ( repository ) ; localizeDropFiles ( dropRepository . findById ( dropId ) . orElse ( null ) ) ; l10nJCommander . run ( new String [ ] code_block = "" ; ) ; int numberOfFrenchTranslationsAfter = getNumberOfFrenchTranslations ( repository ) ; assertEquals ( "2 new french translations must be added" , numberOfFrenchTranslationsBefore + 2 , numberOfFrenchTranslationsAfter ) ; getL10nJCommander ( ) . run ( "tm-export" , "-r" , repository . getName ( ) , "-t" , targetTestDir . getAbsolutePath ( ) , "--target-basename" , "fortest" ) ; modifyFilesInTargetTestDirectory ( XliffUtils . replaceCreatedDateFunction ( ) ) ; checkExpectedGeneratedResources ( ) ; }
public void test() { if ( offset > validBytes ) { packet . setIsCorrupt ( true ) ; logger . debug ( "Invalid packet at offset {}" , old ) ; } }
public void test() { try { complete ( buildExecutionSession , runningEnvironment , onComplete ) ; } catch ( InterruptedException e ) { log . error ( "Interrupted" , e ) ; } }
public void test() { if ( LOGGER . isInfoEnabled ( ) ) { LOGGER . info ( "Executing resource NetworkUsageCommand " + cmd ) ; } }
public void test() { try { ExternalResponse result = remoteLogics . exec ( AuthenticationToken . ANONYMOUS , MainController . getSessionInfo ( ) , "Authentication.syncUsers[ISTRING[100], JSONFILE]" , new ExternalRequest ( new Object [ ] code_block = "" ; ) ) ; JSONArray unlockedUsers = new JSONArray ( new String ( ( ( FileData ) result . results [ 0 ] ) . getRawFile ( ) . getBytes ( ) , StandardCharsets . UTF_8 ) ) ; List < Object > currentUsers = unlockedUsers . toList ( ) ; List < UserInfo > newUserInfos = new ArrayList < > ( ) ; code_block = ForStatement ; code_block = ForStatement ; userInfos . set ( newUserInfos ) ; } catch ( RemoteException e ) { ClientLoggers . clientLogger . error ( "Error synchronizing users" , e ) ; } }
public String enqueueGetConfigurationRequest ( @ Identification final String organisationIdentification , @ Identification final String deviceIdentification , final int messagePriority ) throws FunctionalException { final Organisation organisation = this . domainHelperService . findOrganisation ( organisationIdentification ) ; final Device device = this . domainHelperService . findActiveDevice ( deviceIdentification ) ; this . domainHelperService . isAllowed ( organisation , device , DeviceFunction . GET_CONFIGURATION ) ; this . domainHelperService . isInMaintenance ( device ) ; LOGGER . debug ( "enqueueGetConfigurationRequest called with organisation {} and device {}" , organisationIdentification , deviceIdentification ) ; final String correlationUid = this . correlationIdProviderService . getCorrelationId ( organisationIdentification , deviceIdentification ) ; final DeviceMessageMetadata deviceMessageMetadata = new DeviceMessageMetadata ( deviceIdentification , organisationIdentification , correlationUid , MessageType . GET_CONFIGURATION . name ( ) , messagePriority ) ; final CommonRequestMessage message = new CommonRequestMessage . Builder ( ) . deviceMessageMetadata ( deviceMessageMetadata ) . build ( ) ; this . commonRequestMessageSender . send ( message ) ; return correlationUid ; }
public void test() { try { MethodKey methodKey = new MethodKey ( JournalArticleServiceUtil . class , "addArticleDefaultValues" , _addArticleDefaultValuesParameterTypes4 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , classNameId , classPK , titleMap , descriptionMap , content , ddmStructureKey , ddmTemplateKey , layoutUuid , indexable , smallImage , smallImageURL , smallImageFile , serviceContext ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . journal . model . JournalArticle ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( ! channelFuture . isSuccess ( ) ) { LOG . trace ( "Validating unsuccessful channel request: {} -> {}" , channelFuture , false ) ; return false ; } }
@ Override public boolean validateObject ( ChannelFuture channelFuture ) { code_block = IfStatement ; code_block = IfStatement ; Channel channel = channelFuture . channel ( ) ; boolean answer = channel . isActive ( ) ; LOG . trace ( "Validating channel: {} -> {}" , channel , answer ) ; return answer ; }
public void test() { try { clientProxy . subscribe ( serviceInfo . getName ( ) , serviceInfo . getGroupName ( ) , serviceInfo . getClusters ( ) ) ; } catch ( NacosException e ) { LogUtils . NAMING_LOGGER . warn ( String . format ( "re subscribe service %s failed" , serviceInfo . getName ( ) ) , e ) ; } }
@ Override @ SuppressWarnings ( "unchecked" ) public List < ProjectSampleAnalysisOutputInfo > getAllAnalysisOutputInfoSharedWithProject ( Long projectId , Set < UUID > workflowIds ) { final String query = "SELECT\n" + "  s.id AS sampleId,\n" + "  s.sampleName AS sampleName,\n" + "  a.id AS analysisId,\n" + "  aofmap.analysis_output_file_key AS analysisOutputFileKey,\n" + "  aof.file_path AS filePath,\n" + "  aof.id AS analysisOutputFileId,\n" + "  a.analysis_type AS analysisType,\n" + "  asub.workflow_id AS workflowId,\n" + "  aof.created_date AS createdDate,\n" + "  asub.name AS analysisSubmissionName,\n" + "  asub.id AS analysisSubmissionId,\n" + "  u.id AS userId,\n" + "  u.firstName AS userFirstName,\n" + "  u.lastName AS userLastName\n" + "FROM analysis_output_file aof\n" + "  INNER JOIN analysis_output_file_map aofmap ON aof.id = aofmap.analysisOutputFilesMap_id\n" + "  INNER JOIN analysis a ON aofmap.analysis_id = a.id\n" + "  INNER JOIN analysis_submission asub ON a.id = asub.analysis_id\n" + "  INNER JOIN analysis_submission_sequencing_object o ON asub.id = o.analysis_submission_id\n" + "  INNER JOIN sample_sequencingobject sso ON sso.sequencingobject_id = o.sequencing_object_id\n" + "  INNER JOIN sample s ON sso.sample_id = s.id\n" + "  INNER JOIN project_sample psample ON s.id = psample.sample_id\n" + "  INNER JOIN user u ON asub.submitter = u.id\n" + "  INNER JOIN project_analysis_submission pasub ON asub.id = pasub.analysis_submission_id\n" + "WHERE\n" + "  psample.project_id = :projectId\n" + "  AND asub.workflow_id IN (:workflowIds)\n" ; MapSqlParameterSource parameters = new MapSqlParameterSource ( ) ; final List < String > workflowUUIDStrings = workflowIds . stream ( ) . map ( UUID :: toString ) . collect ( Collectors . toList ( ) ) ; parameters . addValue ( "projectId" , projectId ) ; parameters . addValue ( "workflowIds" , workflowUUIDStrings ) ; logger . trace ( "Getting all shared analysis output file info for project id=" + projectId ) ; NamedParameterJdbcTemplate tmpl = new NamedParameterJdbcTemplate ( dataSource ) ; return tmpl . query ( query , parameters , new BeanPropertyRowMapper ( ProjectSampleAnalysisOutputInfo . class ) ) ; }
public void test() { if ( bSession == null ) { logger . error ( "Base Session is null for sessionId: {}" , data . getSessionId ( ) ) ; return ; } else { code_block = TryStatement ;  } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Caught exception from session object!" , e ) ; } }
public void test() { try { DiameterTimerTaskData data = ( DiameterTimerTaskData ) getData ( ) ; BaseSession bSession = sessionDataSource . getSession ( data . getSessionId ( ) ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Failure executing timer task" , e ) ; } }
public void test() { if ( ! schema . exists ( ) ) { code_block = IfStatement ; LOG . debug ( "Creating schema: " + schema ) ; schema . create ( ) ; createdSchemas . add ( schema ) ; } else { LOG . debug ( "Skipping creation of existing schema: " + schema ) ; } }
public void test() { if ( ! schema . exists ( ) ) { code_block = IfStatement ; LOG . debug ( "Creating schema: " + schema ) ; schema . create ( ) ; createdSchemas . add ( schema ) ; } else { LOG . debug ( "Skipping creation of existing schema: " + schema ) ; } }
public void test() { try { LOG . debug ( "Schema creation failed. Retrying in 1 sec ..." ) ; Thread . sleep ( 1000 ) ; } catch ( InterruptedException e1 ) { } }
public void test() { if ( commitNeeded ) { stateMgr . flushCache ( ) ; recordCollector . flush ( ) ; hasPendingTxCommit = eosEnabled ; log . debug ( "Prepared {} task for committing" , state ( ) ) ; return committableOffsetsAndMetadata ( ) ; } else { log . debug ( "Skipped preparing {} task for commit since there is nothing to commit" , state ( ) ) ; return Collections . emptyMap ( ) ; } }
public void test() { if ( commitNeeded ) { stateMgr . flushCache ( ) ; recordCollector . flush ( ) ; hasPendingTxCommit = eosEnabled ; log . debug ( "Prepared {} task for committing" , state ( ) ) ; return committableOffsetsAndMetadata ( ) ; } else { log . debug ( "Skipped preparing {} task for commit since there is nothing to commit" , state ( ) ) ; return Collections . emptyMap ( ) ; } }
private void getExtraUnmanagedStorageVolumes ( ) { ResourceCollection < ExtraStorageVolume > extraStorageVolumes = this . storageVolumeAttachmentClient . getExtraUnmanagedStorageVolumes ( ) ; LOGGER . info ( "Extra unmanaged storage volume attachments returned to client: {}" , JsonPrettyPrinter . print ( extraStorageVolumes ) ) ; }
public void test() { try { ResourceInterface resource = loadResource ( resourceId ) ; BaseResourceDataBean resourceFile = new BaseResourceDataBean ( ) ; resourceFile . setResourceType ( resource . getType ( ) ) ; resourceFile . setResourceId ( resourceId ) ; resourceFile . setMetadata ( resource . getMetadata ( ) ) ; resourceFile . setOwner ( resource . getOwner ( ) ) ; resourceFile . setFolderPath ( resource . getFolderPath ( ) ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; resourceFile . setMainGroup ( resource . getMainGroup ( ) ) ; resourceFile . setCategories ( convertCategories ( categories ) ) ; resourceManager . updateResource ( resourceFile ) ; return convertResourceToDto ( resourceManager . loadResource ( resourceId ) ) ; } catch ( ApsSystemException e ) { throw new RestServerError ( "plugins.jacms.resources.resourceManager.error.persistence" , e ) ; } catch ( IOException e ) { log . error ( "Error reading file input stream" , e ) ; throw new RestServerError ( "plugins.jacms.resources.image.errorReadingStream" , e ) ; } }
public void test() { try { tunnelClient . connect ( true ) ; } catch ( Throwable e ) { logger . error ( "reconnect error" , e ) ; } }
public void test() { try ( AutoLock l = lock . lockForWrite ( ) ) { AclRecord record = ( ( MutableAclRecord ) mutableAcl ) . getAclRecord ( ) ; crud . save ( record ) ; logger . debug ( "ACL of " + mutableAcl . getObjectIdentity ( ) + " updated successfully." ) ; } catch ( IOException e ) { throw new InternalErrorException ( e ) ; } }
@ Override void refreshConfiguration ( ResourceManager configuration ) { super . refreshConfiguration ( configuration ) ; int newMaxUnits = getIntegerParameter ( PRM_QUANTITY ) ; int delta = newMaxUnits - previousMaxUnits ; previousMaxUnits = newMaxUnits ; this . availableUnits . addAndGet ( delta ) ; this . defaultConsumption = getIntegerParameter ( PRM_CONSUMPTION ) ; jqmlogger . info ( "\tConfigured quantity resource manager [{}] with max count {} - currently free {} - taking {} per JI by default" , this . key , newMaxUnits , this . availableUnits . intValue ( ) , this . defaultConsumption ) ; }
@ Override protected ModelAndView onSubmit ( HttpServletRequest request , HttpServletResponse response , Object command , BindException errors ) throws Exception { AddFileMetadataCommand addFileMetadataCommand = ( AddFileMetadataCommand ) command ; int fileId = Integer . parseInt ( request . getParameter ( "fileId" ) ) ; ModelAndView mav = new ModelAndView ( "redirect:/experiments/data/detail.html?fileId=" + fileId ) ; log . debug ( "Checking the permission." ) ; code_block = IfStatement ; log . debug ( "Creating new FileMetadata object" ) ; FileMetadataParamVal metadata = new FileMetadataParamVal ( ) ; metadata . setId ( new FileMetadataParamValId ( addFileMetadataCommand . getParamId ( ) , fileId ) ) ; log . debug ( "Setting the metadata value = " + addFileMetadataCommand . getParamValue ( ) ) ; metadata . setMetadataValue ( addFileMetadataCommand . getParamValue ( ) ) ; log . debug ( "Saving new file metadata entry" ) ; fileMetadataParamValDao . create ( metadata ) ; log . debug ( "Returning MAV" ) ; return mav ; }
public void test() { if ( ! auth . userIsOwnerOrCoexpOfCorrespExperiment ( fileId ) ) { log . debug ( "User does not have permission to add metadata value - no data saved, returning MAV." ) ; return mav ; } }
@ Override protected ModelAndView onSubmit ( HttpServletRequest request , HttpServletResponse response , Object command , BindException errors ) throws Exception { AddFileMetadataCommand addFileMetadataCommand = ( AddFileMetadataCommand ) command ; int fileId = Integer . parseInt ( request . getParameter ( "fileId" ) ) ; ModelAndView mav = new ModelAndView ( "redirect:/experiments/data/detail.html?fileId=" + fileId ) ; log . debug ( "Checking the permission." ) ; code_block = IfStatement ; log . debug ( "Creating new FileMetadata object" ) ; FileMetadataParamVal metadata = new FileMetadataParamVal ( ) ; metadata . setId ( new FileMetadataParamValId ( addFileMetadataCommand . getParamId ( ) , fileId ) ) ; log . debug ( "Setting the metadata value = " + addFileMetadataCommand . getParamValue ( ) ) ; metadata . setMetadataValue ( addFileMetadataCommand . getParamValue ( ) ) ; log . debug ( "Saving new file metadata entry" ) ; fileMetadataParamValDao . create ( metadata ) ; log . debug ( "Returning MAV" ) ; return mav ; }
@ Override protected ModelAndView onSubmit ( HttpServletRequest request , HttpServletResponse response , Object command , BindException errors ) throws Exception { AddFileMetadataCommand addFileMetadataCommand = ( AddFileMetadataCommand ) command ; int fileId = Integer . parseInt ( request . getParameter ( "fileId" ) ) ; ModelAndView mav = new ModelAndView ( "redirect:/experiments/data/detail.html?fileId=" + fileId ) ; log . debug ( "Checking the permission." ) ; code_block = IfStatement ; log . debug ( "Creating new FileMetadata object" ) ; FileMetadataParamVal metadata = new FileMetadataParamVal ( ) ; metadata . setId ( new FileMetadataParamValId ( addFileMetadataCommand . getParamId ( ) , fileId ) ) ; log . debug ( "Setting the metadata value = " + addFileMetadataCommand . getParamValue ( ) ) ; metadata . setMetadataValue ( addFileMetadataCommand . getParamValue ( ) ) ; log . debug ( "Saving new file metadata entry" ) ; fileMetadataParamValDao . create ( metadata ) ; log . debug ( "Returning MAV" ) ; return mav ; }
@ Override protected ModelAndView onSubmit ( HttpServletRequest request , HttpServletResponse response , Object command , BindException errors ) throws Exception { AddFileMetadataCommand addFileMetadataCommand = ( AddFileMetadataCommand ) command ; int fileId = Integer . parseInt ( request . getParameter ( "fileId" ) ) ; ModelAndView mav = new ModelAndView ( "redirect:/experiments/data/detail.html?fileId=" + fileId ) ; log . debug ( "Checking the permission." ) ; code_block = IfStatement ; log . debug ( "Creating new FileMetadata object" ) ; FileMetadataParamVal metadata = new FileMetadataParamVal ( ) ; metadata . setId ( new FileMetadataParamValId ( addFileMetadataCommand . getParamId ( ) , fileId ) ) ; log . debug ( "Setting the metadata value = " + addFileMetadataCommand . getParamValue ( ) ) ; metadata . setMetadataValue ( addFileMetadataCommand . getParamValue ( ) ) ; log . debug ( "Saving new file metadata entry" ) ; fileMetadataParamValDao . create ( metadata ) ; log . debug ( "Returning MAV" ) ; return mav ; }
@ Override protected ModelAndView onSubmit ( HttpServletRequest request , HttpServletResponse response , Object command , BindException errors ) throws Exception { AddFileMetadataCommand addFileMetadataCommand = ( AddFileMetadataCommand ) command ; int fileId = Integer . parseInt ( request . getParameter ( "fileId" ) ) ; ModelAndView mav = new ModelAndView ( "redirect:/experiments/data/detail.html?fileId=" + fileId ) ; log . debug ( "Checking the permission." ) ; code_block = IfStatement ; log . debug ( "Creating new FileMetadata object" ) ; FileMetadataParamVal metadata = new FileMetadataParamVal ( ) ; metadata . setId ( new FileMetadataParamValId ( addFileMetadataCommand . getParamId ( ) , fileId ) ) ; log . debug ( "Setting the metadata value = " + addFileMetadataCommand . getParamValue ( ) ) ; metadata . setMetadataValue ( addFileMetadataCommand . getParamValue ( ) ) ; log . debug ( "Saving new file metadata entry" ) ; fileMetadataParamValDao . create ( metadata ) ; log . debug ( "Returning MAV" ) ; return mav ; }
public void test() { if ( s_logger . isDebugEnabled ( ) ) { s_logger . debug ( "sync snapshot " + snapshotId + " from cache to object store..." ) ; } }
@ Secured ( ServicesData . ROLE_CREATE_CONTEXTS ) @ ResponseStatus ( HttpStatus . CREATED ) @ PostMapping public ContextDto create ( final @ Valid @ RequestBody ContextDto contextDto ) { LOGGER . debug ( "Create {}" , contextDto ) ; return contextExternalService . create ( contextDto ) ; }
private void commitCheckpointMark ( KafkaCheckpointMark checkpointMark ) { LOG . debug ( "{}: Committing finalized checkpoint {}" , this , checkpointMark ) ; consumer . commitSync ( checkpointMark . getPartitions ( ) . stream ( ) . filter ( p -> p . getNextOffset ( ) != UNINITIALIZED_OFFSET ) . collect ( Collectors . toMap ( p -> new TopicPartition ( p . getTopic ( ) , p . getPartition ( ) ) , p -> new OffsetAndMetadata ( p . getNextOffset ( ) ) ) ) ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "receive the jvm metrics from service instance, name: {}, instance: {}" , request . getService ( ) , request . getServiceInstance ( ) ) ; } }
public void test() { if ( ! warningsLogged [ 0 ] ) { logger . warn ( annotation + ANNOTATION_HC_WARN_MSG + ", not " + walker . getClass ( ) . getSimpleName ( ) ) ; warningsLogged [ 0 ] = true ; } }
public void test() { if ( ! warningsLogged [ 1 ] ) { logger . warn ( "Annotation will not be calculated, can only be used with likelihood based annotations in the HaplotypeCaller" ) ; warningsLogged [ 1 ] = true ; } }
public void test() { if ( ! warningsLogged [ 2 ] ) { logger . warn ( "Annotation will not be calculated, genotype is not called" ) ; warningsLogged [ 2 ] = true ; } }
@ Test public void testPOSTObjectsWithLegacyEntitiesAndInbuiltSchemaAsOutputSchema ( ) throws Exception { LegacyProjectResourceTest . LOG . debug ( "start POST {}s with legacy entities test" , pojoClassName ) ; objectJSONString = DMPPersistenceUtil . getResourceAsString ( "old.fotothek.project.json" ) ; final Response response = target ( "/robust" ) . request ( MediaType . APPLICATION_JSON_TYPE ) . accept ( MediaType . APPLICATION_JSON_TYPE ) . post ( Entity . json ( objectJSONString ) ) ; Assert . assertEquals ( "201 Created was expected" , 201 , response . getStatus ( ) ) ; final String responseString = response . readEntity ( String . class ) ; Assert . assertNotNull ( "the response JSON shouldn't be null" , responseString ) ; final Project actualObject = objectMapper . readValue ( responseString , pojoClass ) ; Assert . assertNotNull ( "the response project shouldn't be null" , actualObject ) ; LegacyProjectResourceTest . LOG . debug ( "end POST {}s with legacy entities test" , pojoClassName ) ; }
@ Test public void testPOSTObjectsWithLegacyEntitiesAndInbuiltSchemaAsOutputSchema ( ) throws Exception { LegacyProjectResourceTest . LOG . debug ( "start POST {}s with legacy entities test" , pojoClassName ) ; objectJSONString = DMPPersistenceUtil . getResourceAsString ( "old.fotothek.project.json" ) ; final Response response = target ( "/robust" ) . request ( MediaType . APPLICATION_JSON_TYPE ) . accept ( MediaType . APPLICATION_JSON_TYPE ) . post ( Entity . json ( objectJSONString ) ) ; Assert . assertEquals ( "201 Created was expected" , 201 , response . getStatus ( ) ) ; final String responseString = response . readEntity ( String . class ) ; Assert . assertNotNull ( "the response JSON shouldn't be null" , responseString ) ; final Project actualObject = objectMapper . readValue ( responseString , pojoClass ) ; Assert . assertNotNull ( "the response project shouldn't be null" , actualObject ) ; LegacyProjectResourceTest . LOG . debug ( "end POST {}s with legacy entities test" , pojoClassName ) ; }
public void test() { try { code_block = IfStatement ; response . setContentType ( "application/json" ) ; response . getWriter ( ) . print ( jsonResponse . toString ( ) ) ; } catch ( JSONException e ) { LOG . warn ( e ) ; response . setContentType ( "text/html" ) ; response . getWriter ( ) . print ( e . getMessage ( ) ) ; } catch ( CerberusException ex ) { LOG . warn ( "JSON exception when getting Country List." , ex ) ; } }
public void test() { try { code_block = IfStatement ; response . setContentType ( "application/json" ) ; response . getWriter ( ) . print ( jsonResponse . toString ( ) ) ; } catch ( JSONException e ) { LOG . warn ( e ) ; response . setContentType ( "text/html" ) ; response . getWriter ( ) . print ( e . getMessage ( ) ) ; } catch ( CerberusException ex ) { LOG . warn ( "JSON exception when getting Country List." , ex ) ; } }
public void test() { if ( ( task . getState ( ) == JobTaskState . ALLOCATED ) || task . getState ( ) . isQueuedState ( ) ) { log . warn ( "[task.revert] node in allocated state {}/{} host = {}" , jobUUID , task . getTaskID ( ) , host . getHost ( ) ) ; } }
public void test() { if ( task != null ) { task . setPreFailErrorCode ( 0 ) ; HostState host = hostManager . getHostState ( task . getHostUUID ( ) ) ; code_block = IfStatement ; log . warn ( "[task.revert] sending revert message to host: {}/{}" , host . getHost ( ) , host . getHostUuid ( ) ) ; spawnMQ . sendControlMessage ( new CommandTaskRevert ( host . getHostUuid ( ) , jobUUID , task . getTaskID ( ) , backupType , rev , time , getTaskReplicaTargets ( task . getAllReplicas ( ) ) , false ) ) ; } else { log . warn ( "[task.revert] task {}/{}] not found" , jobUUID , taskID ) ; } }
public void start ( ModuleDefineHolder moduleDefineHolder , int ttl ) { log . info ( "Cache updateServiceInventory timer start" ) ; final long timeInterval = 10 ; Executors . newSingleThreadScheduledExecutor ( ) . scheduleAtFixedRate ( new RunnableWithExceptionProtection ( ( ) -> update ( moduleDefineHolder ) , t -> log . error ( "Cache update failure." , t ) ) , 1 , timeInterval , TimeUnit . SECONDS ) ; this . ttl = ttl ; }
void logTextUnitSearcherError ( TextUnitSearcherError textUnitSearcherError ) throws TextUnitSearcherError { logger . error ( "TextUnitSearcher couldn't recover for \"call\": {}\n{}" , textUnitSearcherError . getMessage ( ) , textUnitSearcherError . nativeCriteria . getQueryInfo ( ) . toString ( ) ) ; }
public void test() { try { GroupRestClient . provisionMembers ( model . getObject ( ) . getKey ( ) , ProvisionAction . DEPROVISION ) ; SyncopeConsoleSession . get ( ) . success ( getString ( Constants . OPERATION_SUCCEEDED ) ) ; target . add ( container ) ; } catch ( SyncopeClientException e ) { LOG . error ( "While provisioning members of group {}" , model . getObject ( ) . getKey ( ) , e ) ; SyncopeConsoleSession . get ( ) . onException ( e ) ; } }
public void test() { try { LOGGER . debug ( "Started refresh with git pull" ) ; final PullResult result = getGit ( ) . pull ( ) . setProgressMonitor ( PROGRESS_MONITOR ) . setRebase ( true ) . setCredentialsProvider ( user ) . call ( ) ; code_block = IfStatement ; LOGGER . debug ( "Finished refresh" ) ; } catch ( final Exception e ) { LOGGER . error ( "Error when refreshing git directory " + workspaceProvider . getRootDirectory ( ) , e ) ; } }
public void test() { try { LOGGER . debug ( "Started refresh with git pull" ) ; final PullResult result = getGit ( ) . pull ( ) . setProgressMonitor ( PROGRESS_MONITOR ) . setRebase ( true ) . setCredentialsProvider ( user ) . call ( ) ; code_block = IfStatement ; LOGGER . debug ( "Finished refresh" ) ; } catch ( final Exception e ) { LOGGER . error ( "Error when refreshing git directory " + workspaceProvider . getRootDirectory ( ) , e ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( this + ": no values" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( this + ": constant value of size " + count ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( this + ": using offset " + linearOffset ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( this + ": using factor " + gcd ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( this + ": creating values of count " + count + ", value length " + valueLength ) ; } }
private void loadConfig ( ComponentContext cc ) throws ConfigurationException { String orgStr = OsgiUtil . getComponentContextProperty ( cc , ORGANIZATION_KEY , DEFAULT_ORGANIZATION_VALUE ) ; code_block = TryStatement ;  url = OsgiUtil . getComponentContextProperty ( cc , CANVAS_URL_KEY ) ; code_block = IfStatement ; logger . debug ( "Canvas URL: {}" , url ) ; token = OsgiUtil . getComponentContextProperty ( cc , CANVAS_USER_TOKEN_KEY ) ; String cacheSizeStr = OsgiUtil . getComponentContextProperty ( cc , CACHE_SIZE_KEY , DEFAULT_CACHE_SIZE_VALUE . toString ( ) ) ; cacheSize = NumberUtils . toInt ( cacheSizeStr ) ; String cacheExpireStr = OsgiUtil . getComponentContextProperty ( cc , CACHE_EXPIRATION_KEY , DEFAULT_CACHE_EXPIRATION_VALUE . toString ( ) ) ; cacheExpiration = NumberUtils . toInt ( cacheExpireStr ) ; String rolesStr = OsgiUtil . getComponentContextProperty ( cc , CANVAS_INSTRUCTOR_ROLES_KEY , DEFAULT_CANVAS_INSTRUCTOR_ROLES ) ; instructorRoles = parsePropertyLineAsSet ( rolesStr ) ; logger . debug ( "Canvas instructor roles: {}" , instructorRoles ) ; String ignoredUsersStr = OsgiUtil . getComponentContextProperty ( cc , IGNORED_USERNAMES_KEY , DEFAULT_INGROED_USERNAMES ) ; ignoredUsernames = parsePropertyLineAsSet ( ignoredUsersStr ) ; logger . debug ( "Ignored users: {}" , ignoredUsernames ) ; }
private void loadConfig ( ComponentContext cc ) throws ConfigurationException { String orgStr = OsgiUtil . getComponentContextProperty ( cc , ORGANIZATION_KEY , DEFAULT_ORGANIZATION_VALUE ) ; code_block = TryStatement ;  url = OsgiUtil . getComponentContextProperty ( cc , CANVAS_URL_KEY ) ; code_block = IfStatement ; logger . debug ( "Canvas URL: {}" , url ) ; token = OsgiUtil . getComponentContextProperty ( cc , CANVAS_USER_TOKEN_KEY ) ; String cacheSizeStr = OsgiUtil . getComponentContextProperty ( cc , CACHE_SIZE_KEY , DEFAULT_CACHE_SIZE_VALUE . toString ( ) ) ; cacheSize = NumberUtils . toInt ( cacheSizeStr ) ; String cacheExpireStr = OsgiUtil . getComponentContextProperty ( cc , CACHE_EXPIRATION_KEY , DEFAULT_CACHE_EXPIRATION_VALUE . toString ( ) ) ; cacheExpiration = NumberUtils . toInt ( cacheExpireStr ) ; String rolesStr = OsgiUtil . getComponentContextProperty ( cc , CANVAS_INSTRUCTOR_ROLES_KEY , DEFAULT_CANVAS_INSTRUCTOR_ROLES ) ; instructorRoles = parsePropertyLineAsSet ( rolesStr ) ; logger . debug ( "Canvas instructor roles: {}" , instructorRoles ) ; String ignoredUsersStr = OsgiUtil . getComponentContextProperty ( cc , IGNORED_USERNAMES_KEY , DEFAULT_INGROED_USERNAMES ) ; ignoredUsernames = parsePropertyLineAsSet ( ignoredUsersStr ) ; logger . debug ( "Ignored users: {}" , ignoredUsernames ) ; }
private void loadConfig ( ComponentContext cc ) throws ConfigurationException { String orgStr = OsgiUtil . getComponentContextProperty ( cc , ORGANIZATION_KEY , DEFAULT_ORGANIZATION_VALUE ) ; code_block = TryStatement ;  url = OsgiUtil . getComponentContextProperty ( cc , CANVAS_URL_KEY ) ; code_block = IfStatement ; logger . debug ( "Canvas URL: {}" , url ) ; token = OsgiUtil . getComponentContextProperty ( cc , CANVAS_USER_TOKEN_KEY ) ; String cacheSizeStr = OsgiUtil . getComponentContextProperty ( cc , CACHE_SIZE_KEY , DEFAULT_CACHE_SIZE_VALUE . toString ( ) ) ; cacheSize = NumberUtils . toInt ( cacheSizeStr ) ; String cacheExpireStr = OsgiUtil . getComponentContextProperty ( cc , CACHE_EXPIRATION_KEY , DEFAULT_CACHE_EXPIRATION_VALUE . toString ( ) ) ; cacheExpiration = NumberUtils . toInt ( cacheExpireStr ) ; String rolesStr = OsgiUtil . getComponentContextProperty ( cc , CANVAS_INSTRUCTOR_ROLES_KEY , DEFAULT_CANVAS_INSTRUCTOR_ROLES ) ; instructorRoles = parsePropertyLineAsSet ( rolesStr ) ; logger . debug ( "Canvas instructor roles: {}" , instructorRoles ) ; String ignoredUsersStr = OsgiUtil . getComponentContextProperty ( cc , IGNORED_USERNAMES_KEY , DEFAULT_INGROED_USERNAMES ) ; ignoredUsernames = parsePropertyLineAsSet ( ignoredUsersStr ) ; logger . debug ( "Ignored users: {}" , ignoredUsernames ) ; }
public void test() { if ( ! optTmpNewRegistry . isPresent ( ) ) { log . error ( "Could not save RESTXQ Registry to disk!" ) ; } else { final Path tmpNewRegistry = optTmpNewRegistry . get ( ) ; log . info ( "Preparing new RESTXQ registry on disk: {}" , tmpNewRegistry . toAbsolutePath ( ) . toString ( ) ) ; code_block = TryStatement ;  } }
public void test() { for ( final Entry < URI , List < FunctionSignature > > xqueryServiceFunctions : xqueryServices . entrySet ( ) ) { writer . print ( xqueryServiceFunctions . getKey ( ) + FIELD_SEP ) ; final List < FunctionSignature > fnSigs = xqueryServiceFunctions . getValue ( ) ; code_block = ForStatement ; writer . println ( ) ; } }
public void test() { try { code_block = TryStatement ;  final Optional < Path > optRegistry = getRegistryFile ( false ) ; code_block = IfStatement ; } catch ( final IOException ioe ) { log . error ( ioe . getMessage ( ) , ioe ) ; } finally { TemporaryFileManager . getInstance ( ) . returnTemporaryFile ( tmpNewRegistry ) ; } }
public void test() { if ( getLabHandler ( ) . getResponseCode ( ) == HL7LabHandler . OK ) { logger . info ( getServiceName ( ) + " lab files have been saved to local file system: " + savePath ) ; } }
public void test() { try { List < String > versionList = new ArrayList < > ( ) ; String executorsPath = "/" + namespace + ExecutorNodePath . getExecutorNodePath ( ) ; code_block = IfStatement ; List < String > executors = curatorFrameworkOp . getChildren ( executorsPath ) ; code_block = IfStatement ; code_block = ForStatement ; return getVersionStrFromList ( versionList ) ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; return "" ; } }
public void test() { { logger . debug ( "Received Subscription discovery response " + response ) ; XdsSchedulerManager . getInstance ( ) . stopSubscriptionDiscoveryScheduling ( ) ; latestReceived = response ; code_block = TryStatement ;  } }
public void test() { { logger . error ( "Error occurred during Subscription discovery" , throwable ) ; XdsSchedulerManager . getInstance ( ) . startSubscriptionDiscoveryScheduling ( ) ; nack ( throwable ) ; } }
public void test() { try { DiscoveryRequest req = DiscoveryRequest . newBuilder ( ) . setNode ( Node . newBuilder ( ) . setId ( nodeId ) . build ( ) ) . setVersionInfo ( latestACKed . getVersionInfo ( ) ) . setTypeUrl ( Constants . SUBSCRIPTION_LIST_TYPE_URL ) . build ( ) ; reqObserver . onNext ( req ) ; logger . debug ( "Sent Discovery request for type url: " + Constants . SUBSCRIPTION_LIST_TYPE_URL ) ; } catch ( Exception e ) { logger . error ( "Unexpected error occurred in API discovery service" , e ) ; reqObserver . onError ( e ) ; } }
public void test() { try { DiscoveryRequest req = DiscoveryRequest . newBuilder ( ) . setNode ( Node . newBuilder ( ) . setId ( nodeId ) . build ( ) ) . setVersionInfo ( latestACKed . getVersionInfo ( ) ) . setTypeUrl ( Constants . SUBSCRIPTION_LIST_TYPE_URL ) . build ( ) ; reqObserver . onNext ( req ) ; logger . debug ( "Sent Discovery request for type url: " + Constants . SUBSCRIPTION_LIST_TYPE_URL ) ; } catch ( Exception e ) { logger . error ( "Unexpected error occurred in API discovery service" , e ) ; reqObserver . onError ( e ) ; } }
public void onSessionInit ( @ Observes @ Initialized ( SessionScoped . class ) Object payload ) { log . info ( "SessionContext got started. Init Object: " + payload . toString ( ) ) ; }
@ Test public void callMethodWithMultipleByteBufferParametersAsync ( ) { logger . info ( name . getMethodName ( ) ) ; final Semaphore resultAvailable = new Semaphore ( 0 ) ; code_block = TryStatement ;  logger . info ( name . getMethodName ( ) + " - OK" ) ; }
public void test() { if ( error instanceof JoynrRuntimeException ) { logger . info ( name . getMethodName ( ) + " - callback - caught exception " + ( ( JoynrRuntimeException ) error ) . getMessage ( ) ) ; } else { logger . info ( name . getMethodName ( ) + " - callback - caught exception" ) ; } }
public void test() { if ( error instanceof JoynrRuntimeException ) { logger . info ( name . getMethodName ( ) + " - callback - caught exception " + ( ( JoynrRuntimeException ) error ) . getMessage ( ) ) ; } else { logger . info ( name . getMethodName ( ) + " - callback - caught exception" ) ; } }
public void test() { try { logger . info ( name . getMethodName ( ) + " - about to wait for callback" ) ; Assert . assertTrue ( name . getMethodName ( ) + " - FAILED - callback not received in time" , resultAvailable . tryAcquire ( 10 , TimeUnit . SECONDS ) ) ; logger . info ( name . getMethodName ( ) + " - wait for callback is over" ) ; Assert . assertTrue ( name . getMethodName ( ) + " - FAILED - callback reported error" , methodWithMultipleByteBufferParametersAsyncCallbackResult ) ; } catch ( InterruptedException | JoynrRuntimeException e ) { fail ( name . getMethodName ( ) + " - FAILED - caught unexpected exception: " + e . getMessage ( ) ) ; } }
public void test() { try { logger . info ( name . getMethodName ( ) + " - about to wait for callback" ) ; Assert . assertTrue ( name . getMethodName ( ) + " - FAILED - callback not received in time" , resultAvailable . tryAcquire ( 10 , TimeUnit . SECONDS ) ) ; logger . info ( name . getMethodName ( ) + " - wait for callback is over" ) ; Assert . assertTrue ( name . getMethodName ( ) + " - FAILED - callback reported error" , methodWithMultipleByteBufferParametersAsyncCallbackResult ) ; } catch ( InterruptedException | JoynrRuntimeException e ) { fail ( name . getMethodName ( ) + " - FAILED - caught unexpected exception: " + e . getMessage ( ) ) ; } }
public void test() { try { CommerceAccountGroupRelServiceUtil . deleteCommerceAccountGroupRels ( className , classPK ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try { Thread . sleep ( 10000 ) ; } catch ( InterruptedException e ) { log . info ( "Rollup Generator Thread interrupted" ) ; running = false ; } }
public void test() { try { code_block = IfStatement ; } catch ( Throwable e ) { log . error ( "Exception encountered while calculating rollups" , e ) ; throw new RuntimeException ( e ) ; } }
@ ParallelNamespaceTest @ Tag ( INTERNAL_CLIENTS_USED ) void testSendMessagesPlainAnonymous ( ExtensionContext extensionContext ) { final String namespaceName = extensionContext . getStore ( ExtensionContext . Namespace . GLOBAL ) . get ( Constants . NAMESPACE_KEY ) . toString ( ) ; final String clusterName = mapWithClusterNames . get ( extensionContext . getDisplayName ( ) ) ; final String topicName = mapWithTestTopics . get ( extensionContext . getDisplayName ( ) ) ; final String clientsName = mapWithKafkaClientNames . get ( extensionContext . getDisplayName ( ) ) ; resourceManager . createResource ( extensionContext , KafkaTemplates . kafkaEphemeral ( clusterName , 3 ) . build ( ) ) ; resourceManager . createResource ( extensionContext , KafkaTopicTemplates . topic ( clusterName , topicName ) . build ( ) ) ; resourceManager . createResource ( extensionContext , KafkaClientsTemplates . kafkaClients ( false , clusterName + "-" + Constants . KAFKA_CLIENTS ) . build ( ) ) ; final String defaultKafkaClientsPodName = kubeClient ( namespaceName ) . listPodsByPrefixInName ( namespaceName , clusterName + "-" + Constants . KAFKA_CLIENTS ) . get ( 0 ) . getMetadata ( ) . getName ( ) ; InternalKafkaClient internalKafkaClient = new InternalKafkaClient . Builder ( ) . withUsingPodName ( defaultKafkaClientsPodName ) . withTopicName ( topicName ) . withNamespaceName ( namespaceName ) . withClusterName ( clusterName ) . withMessageCount ( MESSAGE_COUNT ) . withListenerName ( Constants . PLAIN_LISTENER_DEFAULT_NAME ) . build ( ) ; LOGGER . info ( "Checking produced and consumed messages to pod:{}" , defaultKafkaClientsPodName ) ; internalKafkaClient . checkProducedAndConsumedMessages ( internalKafkaClient . sendMessagesPlain ( ) , internalKafkaClient . receiveMessagesPlain ( ) ) ; Service kafkaService = kubeClient ( namespaceName ) . getService ( namespaceName , KafkaResources . bootstrapServiceName ( clusterName ) ) ; String kafkaServiceDiscoveryAnnotation = kafkaService . getMetadata ( ) . getAnnotations ( ) . get ( "strimzi.io/discovery" ) ; JsonArray serviceDiscoveryArray = new JsonArray ( kafkaServiceDiscoveryAnnotation ) ; assertThat ( StUtils . expectedServiceDiscoveryInfo ( "none" , "none" , false , true ) , is ( serviceDiscoveryArray ) ) ; }
public void test() { try { return slaveSyncClient . syncCheckpoint ( ) ; } catch ( Exception e ) { LOG . warn ( "sync checkpoint failed, will retry after 2 seconds" , e ) ; code_block = TryStatement ;  } }
public void test() { if ( recognizer == null ) { log . warn ( "{}.attachSpeechRecognizer(null)" , getName ( ) ) ; return ; } }
public void refreshCurrentMasterHealthStatus ( ) { long currentTime = System . currentTimeMillis ( ) ; Set < String > toDeleteTargetDcIds = new HashSet < > ( ) ; Set < String > unhealthyTargetDcIds = new HashSet < > ( ) ; code_block = ForStatement ; logger . debug ( "[refreshCurrentMasterHealthStatus] cluster {}, shard {} remove not exist targetDcId {}" , clusterId , shardId , toDeleteTargetDcIds ) ; toDeleteTargetDcIds . forEach ( remoteMasterLastHealthDelayTimes :: remove ) ; toDeleteTargetDcIds . forEach ( remoteMasterInstances :: remove ) ; updateCurrentMasterHealthStatus ( clusterId , shardId , unhealthyTargetDcIds ) ; }
public void test() { if ( major > CURRENT_MAJOR_VERSION || ( major == CURRENT_MAJOR_VERSION && minor > CURRENT_MINOR_VERSION ) ) { log . warn ( "ORC file %s was written by a newer Hive version %s. This file may not be readable by this version of Hive (%s.%s)." , orcDataSource , Joiner . on ( '.' ) . join ( version ) , CURRENT_MAJOR_VERSION , CURRENT_MINOR_VERSION ) ; } }
public void test() { try ( Connection conn = buildConnection ( serverUrl ) ; Statement stmt = conn . createStatement ( ) ) { StringBuilder sb = new StringBuilder ( ) ; code_block = IfStatement ; ResultSet rs = stmt . executeQuery ( sb . toString ( ) ) ; code_block = WhileStatement ; } catch ( SQLException e ) { LOGGER . error ( "Error while checking for database" , e ) ; } }
public void test() { { int trans = ++ transRover ; boolean relevantTrans = true ; ClientType clientType = relevantTrans ? ClientType . randomClientType ( ) : null ; int count = 1000 ; LOG . info ( "Sending Trans[id=" + trans + ", count=" + count + ", clientType=" + clientType + ", firstID=" + ( messageRover + 1 ) + "]" ) ; Connection con = cf . createConnection ( ) ; Session sess = con . createSession ( true , Session . SESSION_TRANSACTED ) ; MessageProducer prod = sess . createProducer ( null ) ; code_block = ForStatement ; Message message = sess . createMessage ( ) ; message . setIntProperty ( "ID" , ++ messageRover ) ; message . setIntProperty ( "TRANS" , trans ) ; message . setBooleanProperty ( "COMMIT" , true ) ; message . setBooleanProperty ( "RELEVANT" , relevantTrans ) ; prod . send ( topic , message ) ; clientManager . onServerMessage ( message ) ; committingTransaction = trans ; sess . commit ( ) ; committingTransaction = - 1 ; LOG . info ( "Committed Trans[id=" + trans + ", count=" + count + ", clientType=" + clientType + "], ID=" + messageRover ) ; sess . close ( ) ; con . close ( ) ; } }
@ Test public void testEmptyStructuredConfig ( ) throws Exception { Entity app = createAndStartApplication ( loadYaml ( "test-entity-basic-template.yaml" , "  brooklyn.config:" , "    test.confName: \"\"" , "    test.confListThing: !!seq []" , "    test.confSetThing: !!set {}" , "    test.confMapThing: !!map {}" ) ) ; waitForApplicationTasks ( app ) ; Assert . assertEquals ( app . getDisplayName ( ) , "test-entity-basic-template" ) ; log . info ( "App started:" ) ; Dumper . dumpInfo ( app ) ; Entity entity = app . getChildren ( ) . iterator ( ) . next ( ) ; Assert . assertNotNull ( entity , "Expected app to have child entity" ) ; Assert . assertTrue ( entity instanceof TestEntity , "Expected TestEntity, found " + entity . getClass ( ) ) ; TestEntity testEntity = ( TestEntity ) entity ; List < String > thingList = ( List < String > ) testEntity . getConfig ( TestEntity . CONF_LIST_THING ) ; Set < String > thingSet = ( Set < String > ) testEntity . getConfig ( TestEntity . CONF_SET_THING ) ; Map < String , String > thingMap = testEntity . getConfig ( TestEntity . CONF_MAP_THING ) ; Assert . assertEquals ( thingList , Lists . newArrayList ( ) ) ; Assert . assertEquals ( thingSet , ImmutableSet . of ( ) ) ; Assert . assertEquals ( thingMap , ImmutableMap . of ( ) ) ; }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( e , e ) ; } }
public void test() { if ( ! success ) { LOGGER . error ( error ) ; } }
public void test() { -> { LOG . trace ( "OnShutdown" ) ; code_block = IfStatement ; LOG . trace ( "OnShutdown complete" ) ; } }
public void test() { if ( waits > 0 && waits % 5 == 0 ) { LOG . info ( msg ) ; } else { LOG . trace ( msg ) ; } }
public void test() { if ( waits > 0 && waits % 5 == 0 ) { LOG . info ( msg ) ; } else { LOG . trace ( msg ) ; } }
public void test() { try { InputStream source = ResourceUtils . create ( ) . getResourceFromUrl ( url ) ; CatalogDto result = ( CatalogDto ) new CatalogXmlSerializer ( ) . deserialize ( new InputStreamReader ( source ) ) ; if ( log . isDebugEnabled ( ) ) log . debug ( "Retrieved catalog from: {}" , url ) ; return result ; } catch ( Throwable t ) { log . debug ( "Unable to retrieve catalog from: " + url + " (" + t + ")" ) ; throw Exceptions . propagate ( t ) ; } }
public void test() { try { Ignite node = ignite ( idx . getAndIncrement ( ) % nodes ) ; log . info ( "Start thread [node=" + node . name ( ) + ']' ) ; IgniteCache cache = node . cache ( cacheName ) ; Map < Integer , Integer > map = new LinkedHashMap < > ( ) ; code_block = IfStatement ; while ( ! stop . get ( ) ) cache . putAll ( map ) ; } catch ( Exception e ) { err . set ( true ) ; log . error ( "Unexpected error: " + e , e ) ; stop . set ( true ) ; } }
protected void handleError ( final Exception e , final String correlationUid , final String organisationIdentification , final String deviceIdentification , final String messageType , final int messagePriority ) { LOGGER . error ( "handling error: {} for message type: {}" , e . getMessage ( ) , messageType , e ) ; OsgpException osgpException = null ; code_block = IfStatement ; final ResponseMessage responseMessage = ResponseMessage . newResponseMessageBuilder ( ) . withCorrelationUid ( correlationUid ) . withOrganisationIdentification ( organisationIdentification ) . withDeviceIdentification ( deviceIdentification ) . withResult ( ResponseMessageResultType . NOT_OK ) . withOsgpException ( osgpException ) . withMessagePriority ( messagePriority ) . build ( ) ; this . responseMessageSender . send ( responseMessage , messageType ) ; }
public void test() { if ( LOG . isWarnEnabled ( ) ) { LOG . warn ( "Ignoring XMLHttpRequest.setRequestHeader to " + name + ": it is a restricted header" ) ; } }
public void test() { try { JtsSpatialContextFactory contextFactory = new JtsSpatialContextFactory ( ) ; contextFactory . allowMultiOverlap = true ; SpatialContext spatialContext = contextFactory . newSpatialContext ( ) ; Shape shape = ( Shape ) spatialContext . readShapeFromWkt ( wkt ) ; Point center = shape . getCenter ( ) ; return center ; } catch ( java . text . ParseException parseException ) { LOGGER . debug ( parseException . getMessage ( ) , parseException ) ; } }
public void test() { if ( request . getAuditMessage ( ) == null ) { LOG . error ( "Audit Request Message is null." ) ; return result ; } }
public void test() { try { String url = oProxyHelper . getUrlLocalHomeCommunity ( NhincConstants . AUDIT_REPO_SERVICE_NAME ) ; code_block = IfStatement ; } catch ( Exception e ) { LOG . error ( "Failed to call the web service ({}).  An unexpected exception occurred. Exception: {}" , NhincConstants . AUDIT_REPO_SERVICE_NAME , e . getLocalizedMessage ( ) , e ) ; } }
public void test() { try { readRecordsFromStorage ( records ) ; } catch ( Exception e ) { log . warn ( "Error reading measurement list from storage = '" + e + "' ," + " trying to convert the list records size" ) ; store . convertListToCurrentRecordSize ( MeasurementSchedule . PROP_MSCHED ) ; readRecordsFromStorage ( records ) ; } }
public void test() { if ( _specificVerboseLogger . isTraceEnabled ( ) ) { String packetsBatchText = packets . isEmpty ( ) ? "IDLE-STATE" : "firstPacketKey=" + packets . get ( 0 ) . getKey ( ) + ", lastPacketKey=" + packets . get ( packets . size ( ) - 1 ) . getEndKey ( ) ; _specificVerboseLogger . trace ( "Returning " + processResult . toString ( ) + " for received packets batch [" + packetsBatchText + "]" ) ; } }
public void test() { try { listener . checkedUserListMembership ( user ) ; } catch ( Exception e ) { logger . warn ( "Exception at showUserListMembership" , e ) ; } }
public void test() { if ( name . equals ( OrganizationIdentityProvider . NAME ) ) { setType ( Type . USER ) ; } else-if ( name . equals ( SpaceIdentityProvider . NAME ) ) { setType ( Type . SPACE ) ; } else { LOG . warn ( "Failed to set activity stream type with type:" + name ) ; } }
public void test() { try { Map < String , Object > result = new HashMap < > ( ) ; result . put ( "status" , - 1 ) ; result . put ( "error" , message . getMessageId ( ) ) ; ServletResponse response = asyncContext . getResponse ( ) ; response . setContentType ( "application/json" ) ; MAPPER . writeValue ( response . getWriter ( ) , result ) ; } catch ( Exception e ) { logger . error ( "return message error {} - {}" , message . getSubject ( ) , message . getMessageId ( ) ) ; } finally { asyncContext . complete ( ) ; } }
public void test() { if ( key != null ) { code_block = IfStatement ; code_block = IfStatement ; } else { LOG . debug ( "Cannot store null key" ) ; } }
public void test() { if ( hasChanges ) { LOG . trace ( "Node {} has changes" , target ) ; } }
public void test() { if ( new File ( file ) . exists ( ) ) { entityManagerFactoryCallable . getUnitInfo ( ) . addMappingFileName ( file ) ; } else { LOGGER . error ( "file " + file + " doesn't exists" ) ; } }
@ Override protected PaymentTransactionInfoPlugin doCallSpecificOperationCallback ( ) throws PaymentPluginApiException { logger . debug ( "Starting CHARGEBACK for payment {} ({} {})" , paymentStateContext . getPaymentId ( ) , paymentStateContext . getAmount ( ) , paymentStateContext . getCurrency ( ) ) ; return new DefaultNoOpPaymentInfoPlugin ( paymentStateContext . getPaymentId ( ) , paymentStateContext . getTransactionId ( ) , TransactionType . CHARGEBACK , paymentStateContext . getAmount ( ) , paymentStateContext . getCurrency ( ) , null , null , PaymentPluginStatus . PROCESSED , null , null ) ; }
public void test() { try { JSONObject jsonObject = jsonFactory . createJSONObject ( propertyValue ) ; String languageId = themeDisplay . getLanguageId ( ) ; code_block = IfStatement ; return jsonObject . getString ( getDefaultLanguageId ( ) ) ; } catch ( JSONException jsonException ) { _log . error ( String . format ( "Unable to deserialize JSON localized property \"%s\" " + "from request" , propertyName ) , jsonException ) ; } }
@ Override public void afterBulk ( long executionId , BulkRequest request , BulkResponse response ) { LOG . debug ( "afterBulk [{}] with {} responses" , executionId , request . numberOfActions ( ) ) ; long msec = response . getTook ( ) . getMillis ( ) ; eventCounter . scope ( "bulks_received" ) . incrBy ( 1 ) ; eventCounter . scope ( "bulk_msec" ) . incrBy ( msec ) ; Iterator < BulkItemResponse > bulkitemiterator = response . iterator ( ) ; int itemcount = 0 ; int acked = 0 ; int failurecount = 0 ; synchronized ( waitAck ) code_block = "" ; }
public void test() { if ( f . getStatus ( ) . equals ( RestStatus . CONFLICT ) ) { eventCounter . scope ( "doc_conflicts" ) . incrBy ( 1 ) ; LOG . debug ( "Doc conflict ID {}" , id ) ; } else { LOG . error ( "Update ID {}, failure: {}" , id , f ) ; failed = true ; } }
public void test() { if ( xx != null ) { LOG . debug ( "Acked {} tuple(s) for ID {}" , xx . size ( ) , id ) ; code_block = ForStatement ; waitAck . invalidate ( id ) ; } else { LOG . warn ( "Could not find unacked tuple for {}" , id ) ; } }
public void test() { for ( String kinaw : waitAck . asMap ( ) . keySet ( ) ) { LOG . debug ( "Still in wait ack after bulk response [{}] => {}" , executionId , kinaw ) ; } }
@ Test public void testMultipleErrors ( ) { Logger log = Mockito . mock ( Logger . class ) ; ScheduledExecutorService executor = Mockito . mock ( ScheduledExecutorService . class ) ; Duration seconds = Duration . seconds ( 30 ) ; RateLimitedLog rateLimitedLog = new DefaultRateLimitedLogFactory ( executor , seconds ) . from ( log ) ; Mockito . verify ( executor ) . scheduleWithFixedDelay ( any ( ) , eq ( 1L ) , eq ( 1L ) , eq ( TimeUnit . MINUTES ) ) ; Throwable t1 = new Throwable ( ) ; Throwable t2 = new Throwable ( ) ; Throwable t3 = new Throwable ( ) ; rateLimitedLog . error ( t1 , "Test error: {}" , "first!" ) ; rateLimitedLog . error ( t2 , "Test error: {}" , "second!" ) ; rateLimitedLog . error ( t3 , "Test error: {}" , "third!" ) ; Mockito . verify ( log ) . error ( "Test error: first!" , t1 ) ; ArgumentCaptor < Runnable > captor1 = ArgumentCaptor . forClass ( Runnable . class ) ; Mockito . verify ( executor ) . schedule ( captor1 . capture ( ) , eq ( 30L ) , eq ( TimeUnit . SECONDS ) ) ; Mockito . verifyNoMoreInteractions ( log , executor ) ; Mockito . reset ( executor ) ; captor1 . getValue ( ) . run ( ) ; Mockito . verify ( log ) . error ( "Encountered {} {} within the last {}: {}" , 2L , "errors" , Duration . seconds ( 30 ) , "Test error: third!" , t3 ) ; ArgumentCaptor < Runnable > captor2 = ArgumentCaptor . forClass ( Runnable . class ) ; Mockito . verify ( executor ) . schedule ( captor2 . capture ( ) , eq ( 30L ) , eq ( TimeUnit . SECONDS ) ) ; Mockito . verifyNoMoreInteractions ( log , executor ) ; captor2 . getValue ( ) . run ( ) ; Mockito . verifyNoMoreInteractions ( log , executor ) ; }
@ Test public void testMultipleErrors ( ) { Logger log = Mockito . mock ( Logger . class ) ; ScheduledExecutorService executor = Mockito . mock ( ScheduledExecutorService . class ) ; Duration seconds = Duration . seconds ( 30 ) ; RateLimitedLog rateLimitedLog = new DefaultRateLimitedLogFactory ( executor , seconds ) . from ( log ) ; Mockito . verify ( executor ) . scheduleWithFixedDelay ( any ( ) , eq ( 1L ) , eq ( 1L ) , eq ( TimeUnit . MINUTES ) ) ; Throwable t1 = new Throwable ( ) ; Throwable t2 = new Throwable ( ) ; Throwable t3 = new Throwable ( ) ; rateLimitedLog . error ( t1 , "Test error: {}" , "first!" ) ; rateLimitedLog . error ( t2 , "Test error: {}" , "second!" ) ; rateLimitedLog . error ( t3 , "Test error: {}" , "third!" ) ; Mockito . verify ( log ) . error ( "Test error: first!" , t1 ) ; ArgumentCaptor < Runnable > captor1 = ArgumentCaptor . forClass ( Runnable . class ) ; Mockito . verify ( executor ) . schedule ( captor1 . capture ( ) , eq ( 30L ) , eq ( TimeUnit . SECONDS ) ) ; Mockito . verifyNoMoreInteractions ( log , executor ) ; Mockito . reset ( executor ) ; captor1 . getValue ( ) . run ( ) ; Mockito . verify ( log ) . error ( "Encountered {} {} within the last {}: {}" , 2L , "errors" , Duration . seconds ( 30 ) , "Test error: third!" , t3 ) ; ArgumentCaptor < Runnable > captor2 = ArgumentCaptor . forClass ( Runnable . class ) ; Mockito . verify ( executor ) . schedule ( captor2 . capture ( ) , eq ( 30L ) , eq ( TimeUnit . SECONDS ) ) ; Mockito . verifyNoMoreInteractions ( log , executor ) ; captor2 . getValue ( ) . run ( ) ; Mockito . verifyNoMoreInteractions ( log , executor ) ; }
@ Test public void testMultipleErrors ( ) { Logger log = Mockito . mock ( Logger . class ) ; ScheduledExecutorService executor = Mockito . mock ( ScheduledExecutorService . class ) ; Duration seconds = Duration . seconds ( 30 ) ; RateLimitedLog rateLimitedLog = new DefaultRateLimitedLogFactory ( executor , seconds ) . from ( log ) ; Mockito . verify ( executor ) . scheduleWithFixedDelay ( any ( ) , eq ( 1L ) , eq ( 1L ) , eq ( TimeUnit . MINUTES ) ) ; Throwable t1 = new Throwable ( ) ; Throwable t2 = new Throwable ( ) ; Throwable t3 = new Throwable ( ) ; rateLimitedLog . error ( t1 , "Test error: {}" , "first!" ) ; rateLimitedLog . error ( t2 , "Test error: {}" , "second!" ) ; rateLimitedLog . error ( t3 , "Test error: {}" , "third!" ) ; Mockito . verify ( log ) . error ( "Test error: first!" , t1 ) ; ArgumentCaptor < Runnable > captor1 = ArgumentCaptor . forClass ( Runnable . class ) ; Mockito . verify ( executor ) . schedule ( captor1 . capture ( ) , eq ( 30L ) , eq ( TimeUnit . SECONDS ) ) ; Mockito . verifyNoMoreInteractions ( log , executor ) ; Mockito . reset ( executor ) ; captor1 . getValue ( ) . run ( ) ; Mockito . verify ( log ) . error ( "Encountered {} {} within the last {}: {}" , 2L , "errors" , Duration . seconds ( 30 ) , "Test error: third!" , t3 ) ; ArgumentCaptor < Runnable > captor2 = ArgumentCaptor . forClass ( Runnable . class ) ; Mockito . verify ( executor ) . schedule ( captor2 . capture ( ) , eq ( 30L ) , eq ( TimeUnit . SECONDS ) ) ; Mockito . verifyNoMoreInteractions ( log , executor ) ; captor2 . getValue ( ) . run ( ) ; Mockito . verifyNoMoreInteractions ( log , executor ) ; }
public void requestReadAlarmRegister ( final DeviceMessageMetadata deviceMessageMetadata , final ReadAlarmRegisterRequest readAlarmRegisterRequestValueObject ) throws FunctionalException { LOGGER . info ( "requestReadAlarmRegister for organisationIdentification: {} for deviceIdentification: {}" , deviceMessageMetadata . getOrganisationIdentification ( ) , deviceMessageMetadata . getDeviceIdentification ( ) ) ; final SmartMeter smartMeteringDevice = this . domainHelperService . findSmartMeter ( deviceMessageMetadata . getDeviceIdentification ( ) ) ; final ReadAlarmRegisterRequestDto readAlarmRegisterRequestDto = this . monitoringMapper . map ( readAlarmRegisterRequestValueObject , ReadAlarmRegisterRequestDto . class ) ; this . osgpCoreRequestMessageSender . send ( new RequestMessage ( deviceMessageMetadata . getCorrelationUid ( ) , deviceMessageMetadata . getOrganisationIdentification ( ) , deviceMessageMetadata . getDeviceIdentification ( ) , smartMeteringDevice . getIpAddress ( ) , readAlarmRegisterRequestDto ) , deviceMessageMetadata . getMessageType ( ) , deviceMessageMetadata . getMessagePriority ( ) , deviceMessageMetadata . getScheduleTime ( ) , deviceMessageMetadata . bypassRetry ( ) ) ; }
@ Override public void initialize ( ) { logger . debug ( "Initializing VigiCrues handler." ) ; StationConfiguration config = getConfigAs ( StationConfiguration . class ) ; logger . debug ( "config refresh = {} min" , config . refresh ) ; updateStatus ( ThingStatus . UNKNOWN ) ; code_block = IfStatement ; getReferences ( ) ; refreshJob = scheduler . scheduleWithFixedDelay ( this :: updateAndPublish , 0 , config . refresh , TimeUnit . MINUTES ) ; }
@ Override public void initialize ( ) { logger . debug ( "Initializing VigiCrues handler." ) ; StationConfiguration config = getConfigAs ( StationConfiguration . class ) ; logger . debug ( "config refresh = {} min" , config . refresh ) ; updateStatus ( ThingStatus . UNKNOWN ) ; code_block = IfStatement ; getReferences ( ) ; refreshJob = scheduler . scheduleWithFixedDelay ( this :: updateAndPublish , 0 , config . refresh , TimeUnit . MINUTES ) ; }
@ Override public Map < String , IMonomer > getMonomers ( ) { logger . debug ( "Getting monomers as hashtable" ) ; return super . getMonomers ( ) ; }
public List findByExample ( StgMsZykTxt instance ) { log . debug ( "finding StgMsZykTxt instance by example" ) ; code_block = TryStatement ;  }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.StgMsZykTxt" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.StgMsZykTxt" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
@ Override public void validationError ( Throwable throwable ) { LOGGER . error ( throwable . getMessage ( ) , throwable ) ; wbNotification . fire ( new NotificationEvent ( i18n . validationError ( ) , NotificationEvent . NotificationType . ERROR ) ) ; }
public void test() { if ( ! maybeRegex . isPresent ( ) ) { LOG . trace ( "No task log error regex provided." ) ; return Optional . empty ( ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( PatternSyntaxException e ) { LOG . error ( "Invalid task log error regex supplied: \"{}\". Received exception: {}" , regex , e ) ; return Optional . empty ( ) ; } }
public void test() { if ( event . cSID != null ) { log . debug ( "Destroying container session : " + event . sID . getLongId ( ) ) ; int count = event . session . destroySession ( event . cSID ) ; event . status . getOperatorMeasurement ( ) . changeActiveValue ( - count ) ; } else { log . debug ( "Destroying plan session : " + event . sID . getLongId ( ) ) ; int count = event . session . destroySession ( ) ; event . status . getOperatorMeasurement ( ) . changeActiveValue ( - count ) ; } }
public void test() { if ( event . cSID != null ) { log . debug ( "Destroying container session : " + event . sID . getLongId ( ) ) ; int count = event . session . destroySession ( event . cSID ) ; event . status . getOperatorMeasurement ( ) . changeActiveValue ( - count ) ; } else { log . debug ( "Destroying plan session : " + event . sID . getLongId ( ) ) ; int count = event . session . destroySession ( ) ; event . status . getOperatorMeasurement ( ) . changeActiveValue ( - count ) ; } }
public void test() { if ( started ) { started = false ; Enumeration < String > keys = allocatedLimiters . keys ( ) ; Collections . asList ( keys ) . forEach ( name -> Tasks . shutdownExecutor ( name , allocatedLimiters . get ( name ) . executor , 5 ) ) ; } else { log . warn ( "The service is already shut down." ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( ClassCastException E ) { log . warn ( "It was found a group without members. Error ahead." ) ; return null ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "It was found a group without member definitios. Error ahead." ) ; } }
public void warn ( CharSequence charSequence ) { seenWarning ( charSequence ) ; originalLog . warn ( charSequence ) ; }
@ ExceptionHandler ( UaaException . class ) public ResponseEntity < UaaException > handleException ( UaaException e ) { logger . info ( "Handling error: " + e . getClass ( ) . getSimpleName ( ) + ", " + e . getMessage ( ) ) ; code_block = IfStatement ; return new ResponseEntity < > ( e , HttpStatus . valueOf ( e . getHttpStatus ( ) ) ) ; }
@ Override public void getCurrentConfigFailed ( VehicleState . GetCurrentConfigErrorEnum error , ReplyContext replyContext ) { logger . error ( "Unable to get config {} / {}" , replyContext , error ) ; }
public void test() { try { Entry entry = readEntry ( ldif ) ; Dn newDn = getDnFactory ( ) . create ( dn ) ; entry . setDn ( newDn ) ; return new DefaultEntry ( schemaManager , entry ) ; } catch ( Exception e ) { LOG . error ( I18n . err ( I18n . ERR_78 , ldif , dn ) ) ; return null ; } }
public void test() { if ( mBus . getShuntSusceptance ( ) != 0 ) { String busId = getId ( BUS_PREFIX , mBus . getNumber ( ) ) ; String shuntId = getId ( SHUNT_PREFIX , mBus . getNumber ( ) ) ; double zb = voltageLevel . getNominalV ( ) * voltageLevel . getNominalV ( ) / perUnitContext . getBaseMva ( ) ; ShuntCompensatorAdder adder = voltageLevel . newShuntCompensator ( ) . setId ( shuntId ) . setConnectableBus ( busId ) . setBus ( busId ) . setSectionCount ( 1 ) ; adder . newLinearModel ( ) . setBPerSection ( mBus . getShuntSusceptance ( ) / perUnitContext . getBaseMva ( ) / zb ) . setMaximumSectionCount ( 1 ) . add ( ) ; ShuntCompensator newShunt = adder . add ( ) ; LOGGER . debug ( "Created shunt {}" , newShunt . getId ( ) ) ; } }
@ Override public void sendCommfaultTag ( long tagID , String tagName , boolean value , String pDescription ) { log . debug ( "Sending CommfaultTag tag {} ({})" , tagName , tagID ) ; long timestamp = System . currentTimeMillis ( ) ; SourceDataTagValue commfaultTagValue = SourceDataTagValue . builder ( ) . id ( tagID ) . name ( tagName ) . controlTag ( true ) . value ( value ) . quality ( new SourceDataTagQuality ( ) ) . timestamp ( new Timestamp ( timestamp ) ) . daqTimestamp ( new Timestamp ( timestamp ) ) . priority ( JmsMessagePriority . PRIORITY_HIGHEST . getPriority ( ) ) . timeToLive ( DataTagConstants . TTL_FOREVER ) . valueDescription ( pDescription ) . build ( ) ; distributeValue ( commfaultTagValue ) ; }
public void test() { default : { logger . warn ( "Unknown error type in proto serialization, setting unknown " + errorType ) ; return SenseiProtos . ErrorType . UnknownError ; } }
public void test() { try { if ( msg . getSrc ( ) == null ) msg . setSrc ( local_addr ) ; ByteArrayDataOutputStream out = new ByteArrayDataOutputStream ( msg . size ( ) ) ; msg . writeTo ( out ) ; code_block = ForStatement ; } catch ( Exception ex ) { log . error ( String . format ( "%s: failed sending discovery request" , local_addr ) , ex ) ; } }
@ Override public String getName ( ) { logger . debug ( "Getting Name: " , super . getName ( ) ) ; return super . getName ( ) ; }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Got response {" + result . encodePrettily ( ) + "}" ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Got response {" + scrollHits . encodePrettily ( ) + "}" ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Got response {" + scrollHits . encodePrettily ( ) + "}" ) ; } }
public void test() { for ( int i = 0 ; i < size ; i ++ ) { logger . info ( result . getRows ( ) . get ( i ) . getValues ( ) . get ( 0 ) + " " + result . getRows ( ) . get ( i ) . getValues ( ) . get ( 1 ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "validate() action=" + action . name ( ) ) ; } }
public void test() { try { zone = Integer . parseInt ( m . group ( 1 ) ) ; } catch ( NumberFormatException e ) { logger . error ( "This should never happen, pattern should only match integers" ) ; return ; } }
public void test() { try { dci . executeTemplateCommands ( ) ; dci . scanGlobalDocumentCommands ( ) ; dci . scanInsertFormValueCommands ( ) ; } catch ( WMCommandsFailedException e ) { LOGGER . debug ( "" , e ) ; } }
public void test() { try { elasticsearchConnection . getClient ( ) . prepareIndex ( ) . setIndex ( FQL_STORE_INDEX ) . setType ( DOCUMENT_TYPE_NAME ) . setId ( fqlStore . getId ( ) ) . setSource ( objectMapper . writeValueAsBytes ( fqlStore ) , XContentType . JSON ) . execute ( ) . get ( ) ; logger . info ( "Saved FQL Query : {}" , fqlStore . getQuery ( ) ) ; } catch ( Exception e ) { throw new FqlPersistenceException ( "Couldn't save FQL query: " + fqlStore . getQuery ( ) + " Error Message: " + e . getMessage ( ) , e ) ; } }
public void test() { try { com . liferay . dynamic . data . mapping . model . DDMTemplate returnValue = DDMTemplateServiceUtil . getTemplate ( groupId , classNameId , templateKey , includeAncestorTemplates ) ; return com . liferay . dynamic . data . mapping . model . DDMTemplateSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try { TextMessage tm = ( TextMessage ) m ; receivedText = tm . getText ( ) ; latch . countDown ( ) ; LOG . info ( "consumer received message :" + receivedText ) ; consumerSession . commit ( ) ; LOG . info ( "committed transaction" ) ; } catch ( JMSException e ) { code_block = TryStatement ;  LOG . info ( e . toString ( ) ) ; e . printStackTrace ( ) ; } }
public void test() { try { tm = producerSession . createTextMessage ( ) ; tm . setText ( "Hello, " + new Date ( ) ) ; producer . send ( tm ) ; LOG . info ( "producer sent message :" + tm . getText ( ) ) ; } catch ( JMSException e ) { e . printStackTrace ( ) ; } }
public void testTransaction ( ) throws Exception { ActiveMQConnectionFactory factory = new ActiveMQConnectionFactory ( "vm://localhost?broker.persistent=false" ) ; connection = factory . createConnection ( ) ; queue = new ActiveMQQueue ( getClass ( ) . getName ( ) + "." + getName ( ) ) ; producerSession = connection . createSession ( false , Session . AUTO_ACKNOWLEDGE ) ; consumerSession = connection . createSession ( true , 0 ) ; producer = producerSession . createProducer ( queue ) ; consumer = consumerSession . createConsumer ( queue ) ; consumer . setMessageListener ( new MessageListener ( ) code_block = "" ; ) ; connection . start ( ) ; TextMessage tm = null ; code_block = TryStatement ;  LOG . info ( "Waiting for latch" ) ; latch . await ( 2 , TimeUnit . SECONDS ) ; assertNotNull ( receivedText ) ; LOG . info ( "test completed, destination=" + receivedText ) ; }
public void testTransaction ( ) throws Exception { ActiveMQConnectionFactory factory = new ActiveMQConnectionFactory ( "vm://localhost?broker.persistent=false" ) ; connection = factory . createConnection ( ) ; queue = new ActiveMQQueue ( getClass ( ) . getName ( ) + "." + getName ( ) ) ; producerSession = connection . createSession ( false , Session . AUTO_ACKNOWLEDGE ) ; consumerSession = connection . createSession ( true , 0 ) ; producer = producerSession . createProducer ( queue ) ; consumer = consumerSession . createConsumer ( queue ) ; consumer . setMessageListener ( new MessageListener ( ) code_block = "" ; ) ; connection . start ( ) ; TextMessage tm = null ; code_block = TryStatement ;  LOG . info ( "Waiting for latch" ) ; latch . await ( 2 , TimeUnit . SECONDS ) ; assertNotNull ( receivedText ) ; LOG . info ( "test completed, destination=" + receivedText ) ; }
public void test() { try { listener . onSuccess ( ) ; } catch ( final IllegalArgumentException e ) { listener . onIgnore ( ) ; LOG . warn ( "Could not register request RTT (likely caused by clock problems). Consider using the 'fixed' backpressure algorithm." , e ) ; } }
public void test() { if ( listener != null ) { code_block = TryStatement ;  metrics . decInflight ( ) ; } else { Loggers . LOGSTREAMS_LOGGER . warn ( "We encountered an problem on releasing the acquired in flight append." + " There was no listener registered for the given position {}, this should not happen." , position ) ; } }
@ GetMapping ( RestApi . ARCHIVE_UNIT_INFO + CommonConstants . PATH_ID ) @ Secured ( ServicesData . ROLE_GET_ARCHIVE ) public ResponseEntity < ResultsDto > findUnitById ( final @ PathVariable ( "id" ) String id ) { LOGGER . info ( "the UA by id {} " , id ) ; ParameterChecker . checkParameter ( "The Identifier is a mandatory parameter: " , id ) ; return archivesSearchExternalService . findUnitById ( id ) ; }
public void test() { try { tmSocket . close ( ) ; } catch ( IOException e ) { log . warn ( "Exception got when closing the tm socket:" , e ) ; } }
public void test() { try { conn = this . getConnection ( ) ; conn . setAutoCommit ( false ) ; stat = conn . prepareStatement ( DELETE_USED_TOKEN ) ; stat . setString ( 1 , token ) ; stat . executeUpdate ( ) ; conn . commit ( ) ; } catch ( Throwable t ) { this . executeRollback ( conn ) ; _logger . error ( "Error removing consumed Token" , t ) ; throw new RuntimeException ( "Error removing consumed Token" , t ) ; } finally { closeDaoResources ( null , stat , conn ) ; } }
public void test() { if ( resource != null ) { LOG . info ( "rest({}).packages({})" , paths , pkg ) ; } else { LOG . info ( "No Beans in '{}' found. Requests {} will fail." , pkg , paths ) ; } }
public void test() { if ( resource != null ) { LOG . info ( "rest({}).packages({})" , paths , pkg ) ; } else { LOG . info ( "No Beans in '{}' found. Requests {} will fail." , pkg , paths ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( portalException , portalException ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "receive browser error log" ) ; } }
public void test() { try { ErrorLogAnalyzer analyzer = new ErrorLogAnalyzer ( moduleManager , errorLogListenerManager , config ) ; analyzer . doAnalysis ( browserErrorLog ) ; } catch ( Throwable e ) { log . error ( e . getMessage ( ) , e ) ; logErrorCounter . inc ( ) ; } finally { timer . finish ( ) ; } }
public void test() { try { CPDefinitionSpecificationOptionValueServiceUtil . deleteCPDefinitionSpecificationOptionValues ( cpDefinitionId ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
protected InputStream getInputStream ( String profileId ) throws IOException { final String profileUrl = registryBaseURl + profileId + "/xml" ; LOG . debug ( "Opening input stream at {}" , profileUrl ) ; return new URL ( profileUrl ) . openStream ( ) ; }
public void test() { if ( result . isSuccessful ( ) ) { LOG . debug ( "{} finished successfully: {}" , prefix , nodeId . getValue ( ) ) ; } else { final Collection < RpcError > errors = requireNonNullElse ( result . getErrors ( ) , ImmutableList . of ( ) ) ; LOG . debug ( "{} failed: {} -> {}" , prefix , nodeId . getValue ( ) , errors ) ; } }
public void test() { if ( result . isSuccessful ( ) ) { LOG . debug ( "{} finished successfully: {}" , prefix , nodeId . getValue ( ) ) ; } else { final Collection < RpcError > errors = requireNonNullElse ( result . getErrors ( ) , ImmutableList . of ( ) ) ; LOG . debug ( "{} failed: {} -> {}" , prefix , nodeId . getValue ( ) , errors ) ; } }
public void test() { if ( result != null ) { code_block = IfStatement ; } else { LOG . debug ( "{} reconciliation failed: {} -> null result" , prefix , nodeId . getValue ( ) ) ; } }
private void updateRouterNetworkRef ( Connection conn ) { s_logger . debug ( "Updating router network ref" ) ; code_block = TryStatement ;  s_logger . debug ( "Done updating router/network references" ) ; }
public void test() { try { updatePriority ( id , taskParam . getPriority ( ) . ordinal ( ) ) ; } catch ( Exception e ) { LOG . error ( e . getMessage ( ) ) ; } }
@ OnWebSocketConnect public void onConnect ( Session session ) { LOG . info ( "Connect [{}]" , session ) ; this . session = session ; }
public void test() { try { final Connection testConnection = DriverManager . getConnection ( configuration . getConnectionUrl ( ) , configuration . getUsername ( ) , configuration . getPassword ( ) ) ; testConnection . close ( ) ; available = true ; } catch ( SQLException ex ) { LOGGER . trace ( "Unable to open a connection to the test database." , ex ) ; available = false ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Looking for matching API with basepath: {} and version: {}" , basePath , version ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Waiting for bulk queue to empty..." ) ; } }
public void test() { try { code_block = IfStatement ; Thread . sleep ( 2000 ) ; } catch ( InterruptedException e ) { logger . warn ( "checkIndexStatistics interrupted" , e ) ; } }
public void test() { try { com . liferay . portal . kernel . model . User returnValue = UserServiceUtil . updateUser ( userId , oldPassword , newPassword1 , newPassword2 , passwordReset , reminderQueryQuestion , reminderQueryAnswer , screenName , emailAddress , facebookId , openId , hasPortrait , portraitBytes , languageId , timeZoneId , greeting , comments , firstName , middleName , lastName , prefixId , suffixId , male , birthdayMonth , birthdayDay , birthdayYear , smsSn , facebookSn , jabberSn , skypeSn , twitterSn , jobTitle , groupIds , organizationIds , roleIds , com . liferay . portal . model . impl . UserGroupRoleModelImpl . toModels ( userGroupRoles ) , userGroupIds , com . liferay . portal . model . impl . AddressModelImpl . toModels ( addresses ) , com . liferay . portal . model . impl . EmailAddressModelImpl . toModels ( emailAddresses ) , com . liferay . portal . model . impl . PhoneModelImpl . toModels ( phones ) , com . liferay . portal . model . impl . WebsiteModelImpl . toModels ( websites ) , com . liferay . portlet . announcements . model . impl . AnnouncementsDeliveryModelImpl . toModels ( announcementsDelivers ) , serviceContext ) ; return com . liferay . portal . kernel . model . UserSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( matcher . find ( ) ) { handler . updateChannel ( PLAY_MODE , new StringType ( KaleidescapeStatusCodes . PLAY_MODE . get ( matcher . group ( 1 ) ) ) ) ; handler . updateChannel ( PLAY_SPEED , new StringType ( matcher . group ( 2 ) ) ) ; handler . updateChannel ( TITLE_NUM , new DecimalType ( Integer . parseInt ( matcher . group ( 3 ) ) ) ) ; handler . updateChannel ( TITLE_LENGTH , new QuantityType < Time > ( Integer . parseInt ( matcher . group ( 4 ) ) , handler . apiSecondUnit ) ) ; handler . updateChannel ( TITLE_LOC , new QuantityType < Time > ( Integer . parseInt ( matcher . group ( 5 ) ) , handler . apiSecondUnit ) ) ; handler . updateChannel ( CHAPTER_NUM , new DecimalType ( Integer . parseInt ( matcher . group ( 6 ) ) ) ) ; handler . updateChannel ( CHAPTER_LENGTH , new QuantityType < Time > ( Integer . parseInt ( matcher . group ( 7 ) ) , handler . apiSecondUnit ) ) ; handler . updateChannel ( CHAPTER_LOC , new QuantityType < Time > ( Integer . parseInt ( matcher . group ( 8 ) ) , handler . apiSecondUnit ) ) ; } else { logger . debug ( "PLAY_STATUS - no match on message: {}" , message ) ; } }
private void persistCoviCodes ( LocalDate localDate ) throws Exception { List < CoviCode > coviCodes = this . coviCodeGenerator . generateCoviCodes ( localDate ) ; logger . info ( "Persisting " + coviCodes . size ( ) + " covicodes for start " + coviCodes . get ( 0 ) . getStartInterval ( ) + " to end " + coviCodes . get ( coviCodes . size ( ) - 1 ) . getEndInterval ( ) ) ; coviCodes . forEach ( code -> coviCodeRepository . saveDoNothingOnConflict ( code . getCode ( ) , code . getStartInterval ( ) , code . getEndInterval ( ) ) ) ; logger . info ( "Done persisting" ) ; }
private void persistCoviCodes ( LocalDate localDate ) throws Exception { List < CoviCode > coviCodes = this . coviCodeGenerator . generateCoviCodes ( localDate ) ; logger . info ( "Persisting " + coviCodes . size ( ) + " covicodes for start " + coviCodes . get ( 0 ) . getStartInterval ( ) + " to end " + coviCodes . get ( coviCodes . size ( ) - 1 ) . getEndInterval ( ) ) ; coviCodes . forEach ( code -> coviCodeRepository . saveDoNothingOnConflict ( code . getCode ( ) , code . getStartInterval ( ) , code . getEndInterval ( ) ) ) ; logger . info ( "Done persisting" ) ; }
public void test() { try { InputStream schRules = FileUtils . openInputStream ( new File ( path ) ) ; rules = TemplatesFactory . newInstance ( ) . getTemplates ( schRules , transformerFactory ) ; } catch ( FileNotFoundException e ) { LOG . debug ( "Schematron rules not found in the file system {}" , path ) ; throw classPathException ; } }
public void test() { try { InetSocketAddress inetSocketAddress = ( InetSocketAddress ) socketAddress ; String hostname = inetSocketAddress . getHostName ( ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . warn ( "Unable to retrieve client hostname" , e ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( "...Ready Go! CountDownRace just begun! (runner=" + runnerCount + ")" ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( "All runners finished line! (runner=" + runnerCount + ")" ) ; } }
public void test() { try { WebHookConfig webHookConfig = populateWebHookConfiguration ( registerWebhookjson ) ; status = webhookConfigurationDAL . updateWebHookConfiguration ( webHookConfig ) ; } catch ( Exception e ) { log . error ( "Error in updating the webhook.. " , e ) ; throw new InsightsCustomException ( e . toString ( ) ) ; } }
public void test() { if ( ! lastRefreshTokenTried . compareAndSet ( null , currentRefreshToken ) ) { LOG . info ( "Already tried to refresh the access token with the current refresh token" ) ; return ; } }
public void test() { if ( statusLine . getStatusCode ( ) != HttpStatus . SC_OK ) { final HttpEntity entity = response . getEntity ( ) ; LOG . error ( "Unable to refresh the access token, received status: `{}`, response: `{}`" , statusLine , EntityUtils . toString ( entity ) ) ; return ; } }
public void test() { try ( CloseableHttpClient client = createHttpClient ( ) ) { final HttpUriRequest request = createHttpRequest ( ) ; code_block = TryStatement ;  } catch ( final IOException e ) { LOG . error ( "Unable to refresh the access token" , e ) ; } }
public void test() { -> { code_block = IfStatement ; return Single . just ( routingMap ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( _NO_SUCH_ENTITY_WITH_PRIMARY_KEY + primaryKey ) ; } }
public void test() { if ( debug ) { log . info ( "Sending Message: softReset to {}" , serial . getName ( ) ) ; } }
public void test() { try { appendMessage ( baos , MAGIC_NUMBER ) ; appendMessage ( baos , 1 ) ; appendMessage ( baos , SOFT_RESET ) ; byte [ ] message = sendMessage ( baos ) ; code_block = IfStatement ; code_block = IfStatement ; return message ; } catch ( Exception e ) { log . error ( "softReset threw" , e ) ; return null ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Got null bucket region for bucketId={}{}{} for PartitionedRegion = {}" , this . partitionedRegion . getPRId ( ) , PartitionedRegion . BUCKET_ID_SEPARATOR , bucketId , this . partitionedRegion ) ; } }
public void test() { try { XNamed textSection = UNO . XNamed ( textSections . get ( sectionNameComplete ) ) ; String nameBase = generateCompleteName ( ) ; String name = nameBase ; int count = 1 ; code_block = WhileStatement ; textSection . setName ( name ) ; sectionNameComplete = name ; return true ; } catch ( Exception x ) { LOGGER . error ( L . m ( "Fehler beim Versuch, Bereich zu updaten: \"%1\"" , sectionNameComplete ) , x ) ; return false ; } }
@ Override public void run ( ) { log . debug ( " EngineCorrelatorModule start  ====" ) ; code_block = TryStatement ;  log . debug ( " EngineCorrelatorModule Completed ====" ) ; }
public void test() { try { ApplicationConfigInterface . loadConfiguration ( ) ; code_block = IfStatement ; EngineStatusLogger . getInstance ( ) . createEngineStatusNode ( "Correlation Execution Completed" , PlatformServiceConstants . SUCCESS ) ; } catch ( Exception e ) { log . error ( "Error in correlation module " , e ) ; EngineStatusLogger . getInstance ( ) . createEngineStatusNode ( "Correlation Execution has some issue  " , PlatformServiceConstants . FAILURE ) ; } }
@ Override public void run ( ) { log . debug ( " EngineCorrelatorModule start  ====" ) ; code_block = TryStatement ;  log . debug ( " EngineCorrelatorModule Completed ====" ) ; }
public void test() { if ( logger . isErrorEnabled ( ) ) { logger . error ( "Application run failed" , failure ) ; registerLoggedException ( failure ) ; } }
public void test() { try { return this . eventData . getClass ( ) . getMethod ( method . getName ( ) ) ; } catch ( NoSuchMethodException exception ) { LOG . error ( exception . getMessage ( ) , exception ) ; throw new RuntimeException ( exception ) ; } }
public void test() { if ( hdr == null || hdr . corrId != this . corr_id ) { log . trace ( "ID of request correlator header (%s) is different from ours (%d). Msg not accepted, passed up" , hdr != null ? String . valueOf ( hdr . corrId ) : "null" , this . corr_id ) ; return false ; } }
public void test() { if ( isDebug ) { logger . debug ( "Set asyncContext {}" , asyncContext ) ; } }
public void test() { try { System . setProperty ( "org.apache.catalina.startup.EXIT_ON_INIT_FAILURE" , "true" ) ; _tomcat1 = getTestUtils ( ) . tomcatBuilder ( ) . port ( _portTomcat1 ) . memcachedNodes ( "http://localhost:" + couchbasePort + "/pools" ) . sticky ( true ) . memcachedProtocol ( "binary" ) . username ( "default" ) . buildAndStart ( ) ; } catch ( final Throwable e ) { LOG . error ( "could not start tomcat." , e ) ; throw e ; } }
public void test() { if ( isDebug ) { logger . debug ( "Request PAgentInfo={}" , MessageFormatUtils . debugLog ( agentInfo ) ) ; } }
public void test() { try { executor . execute ( new Runnable ( ) code_block = "" ; ) ; } catch ( RejectedExecutionException ree ) { logger . warn ( "Failed to request. Rejected execution, executor={}" , executor ) ; } }
public void test() { try { return context . getClient ( AmazonEC2Client . class ) . describeInstances ( new DescribeInstancesRequest ( ) . withInstanceIds ( instanceId ) ) . getReservations ( ) . stream ( ) . map ( Reservation :: getInstances ) . flatMap ( Collection :: stream ) . filter ( i -> i . getInstanceId ( ) . equals ( instanceId ) ) . map ( Instance :: getImageId ) . findFirst ( ) ; } catch ( final AmazonClientException e ) { log . warn ( "Could not describe instance " + instanceId , e ) ; return empty ( ) ; } }
public void test() { if ( size > warnLimit ) { String type = sessionId == null ? "stateless" : "stateful" ; log . warn ( "Warn limit reached to " + type + " beans. Currently there " + "are " + size + " " + type + " EJB stubs cached in '" + name + "' " + "beanstalk." ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( "onScript" ) ; } }
public void test() { if ( fileLocation == null ) { log . debug ( "Could not find Beeline configuration file: {}" , DEFAULT_BEELINE_SITE_FILE_NAME ) ; return props ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( _NO_SUCH_ENTITY_WITH_PRIMARY_KEY + primaryKey ) ; } }
public void test() { if ( ! serviceTemplate . hasBuildPlan ( ) ) { BPELSituationAwareBuildProcessBuilder . LOG . debug ( "ServiceTemplate {} has no BuildPlan, generating BuildPlan" , serviceTemplate . getQName ( ) . toString ( ) ) ; final BPELPlan newBuildPlan = buildPlan ( csarName , definitions , serviceTemplate ) ; code_block = IfStatement ; } else { BPELSituationAwareBuildProcessBuilder . LOG . debug ( "ServiceTemplate {} has BuildPlan, no generation needed" , serviceTemplate . getQName ( ) . toString ( ) ) ; } }
public void test() { if ( newBuildPlan != null ) { BPELSituationAwareBuildProcessBuilder . LOG . debug ( "Created BuildPlan " + newBuildPlan . getBpelProcessElement ( ) . getAttribute ( "name" ) ) ; plans . add ( newBuildPlan ) ; } }
@ Override public void register ( EhcacheXAResource resource , boolean forRecovery ) { log . info ( "register XA resource" ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "MRule matches at Delivery phase to a message: rule: " + rule + "message: " + sms ) ; } }
public void test() { try { code_block = ForStatement ; } catch ( Throwable e ) { logger . error ( "Exception when invoking rule.matches(message) or onPostDelivery(): " + e . getMessage ( ) , e ) ; return new MProcResult ( ) ; } }
public void test() { try { _log . warn ( throwable ) ; } catch ( Exception exception ) { printMsg ( throwable . getMessage ( ) ) ; } }
public void test() { try { idName = featureModel . getEntityMetadata ( ) . getIdentifierPropertyName ( ) ; } catch ( LayerException e ) { log . warn ( "Cannot read idName, defaulting to 'id'" , e ) ; idName = HIBERNATE_ID ; } }
public void test() { if ( semsResponse . isOk ( ) ) { StatusResponse statusResponse = gson . fromJson ( response , StatusResponse . class ) ; code_block = IfStatement ; currentStatus = statusResponse . getStatus ( ) ; updateStatus ( ThingStatus . ONLINE ) ; return currentStatus ; } else-if ( semsResponse . isSessionInvalid ( ) ) { logger . debug ( "Session is invalidated. Attempting new login." ) ; login ( ) ; return getStationStatus ( stationUUID ) ; } else-if ( semsResponse . isError ( ) ) { throw new ConfigurationException ( "ERROR status code received from SEMS portal. Please check your station ID" ) ; } else { throw new CommunicationException ( String . format ( "Unknown status code received from SEMS portal: %s - %s" , semsResponse . getCode ( ) , semsResponse . getMsg ( ) ) ) ; } }
public String decodeSAMLPostResponse ( String encodedReponse ) { String trimmed = encodedReponse . replaceAll ( "\r\n" , "" ) ; String base64DecodedResponse = new String ( Base64 . decodeBase64 ( trimmed ) ) ; LOG . debug ( "Decoded SAML: \n{}\n" , base64DecodedResponse ) ; return base64DecodedResponse ; }
void initValue ( ) { String effectiveKey = doUpdateFinalValue ( ) ; LOGGER . debug ( "config inited, \"{}\" set to {}, effective key is \"{}\"." , joinedPriorityKeys , finalValue , effectiveKey ) ; }
public void load ( ) throws RuntimeException { LOG . debug ( "getting list of organisations from registry" ) ; List < Organisation > tempOrganisations ; tempOrganisations = registryManager . getOrganisations ( ) ; Organisation o = new Organisation ( ) ; o . setName ( "" ) ; organisations . add ( o ) ; organisations . addAll ( tempOrganisations ) ; LOG . debug ( "organisations returned: " + organisations . size ( ) ) ; }
public void load ( ) throws RuntimeException { LOG . debug ( "getting list of organisations from registry" ) ; List < Organisation > tempOrganisations ; tempOrganisations = registryManager . getOrganisations ( ) ; Organisation o = new Organisation ( ) ; o . setName ( "" ) ; organisations . add ( o ) ; organisations . addAll ( tempOrganisations ) ; LOG . debug ( "organisations returned: " + organisations . size ( ) ) ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( sb . toString ( ) ) ; } }
@ Override public void error ( String msg , Object [ ] os , Throwable t ) { logger . error ( msg , os ) ; logger . error ( "" , t ) ; }
@ Override public void error ( String msg , Object [ ] os , Throwable t ) { logger . error ( msg , os ) ; logger . error ( "" , t ) ; }
@ Test public void loadCacheErrorDirectTemp ( ) throws Exception { LOG . info ( "Started loadCacheErrorDirectTemp" ) ; loadDirectBackendTemp ( 64 * 1024 * 1024 ) ; LOG . info ( "Finished loadCacheErrorDirectTemp" ) ; }
@ Test public void loadCacheErrorDirectTemp ( ) throws Exception { LOG . info ( "Started loadCacheErrorDirectTemp" ) ; loadDirectBackendTemp ( 64 * 1024 * 1024 ) ; LOG . info ( "Finished loadCacheErrorDirectTemp" ) ; }
@ Override public Operator run ( ) { long start = System . currentTimeMillis ( ) ; UResultOperator uResultOperator = new UResultOperator ( _planNode . run ( ) ) ; long end = System . currentTimeMillis ( ) ; LOGGER . info ( "InstanceResponsePlanNode.run took: " + ( end - start ) ) ; return uResultOperator ; }
public void persist ( StgSysNetzMapping transientInstance ) { log . debug ( "persisting StgSysNetzMapping instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . persist ( transientInstance ) ; log . debug ( "persist successful" ) ; } catch ( RuntimeException re ) { log . error ( "persist failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . persist ( transientInstance ) ; log . debug ( "persist successful" ) ; } catch ( RuntimeException re ) { log . error ( "persist failed" , re ) ; throw re ; } }
@ Override public IRingSet getConnectedRings ( IRing ring ) { logger . debug ( "Getting connected rings for ring: " , ring ) ; return super . getConnectedRings ( ring ) ; }
public void test() { try { listener . onInterceptorConfig ( interceptors ) ; } catch ( Throwable ex ) { log . error ( ex . toString ( ) , ex ) ; } }
@ RestAccessControl ( permission = Permission . SUPERUSER ) @ RequestMapping ( value = "/profileTypeAttributes" , method = RequestMethod . GET , produces = MediaType . APPLICATION_JSON_VALUE ) public ResponseEntity < PagedRestResponse < String > > getUserProfileAttributeTypes ( RestListRequest requestList ) throws JsonProcessingException { this . getProfileTypeValidator ( ) . validateRestListRequest ( requestList , AttributeTypeDto . class ) ; PagedMetadata < String > result = this . getUserProfileTypeService ( ) . getAttributeTypes ( requestList ) ; logger . debug ( "Main Response -> {}" , result ) ; this . getProfileTypeValidator ( ) . validateRestListResult ( requestList , result ) ; return new ResponseEntity < > ( new PagedRestResponse < > ( result ) , HttpStatus . OK ) ; }
public void test() { if ( result instanceof RegistrationResponse . Success ) { log . debug ( "Registration with {} at {} was successful." , targetName , targetAddress ) ; S success = ( S ) result ; completionFuture . complete ( RetryingRegistrationResult . success ( gateway , success ) ) ; } else-if ( result instanceof RegistrationResponse . Rejection ) { log . debug ( "Registration with {} at {} was rejected." , targetName , targetAddress ) ; R rejection = ( R ) result ; completionFuture . complete ( RetryingRegistrationResult . rejection ( rejection ) ) ; } else { code_block = IfStatement ; log . info ( "Pausing and re-attempting registration in {} ms" , retryingRegistrationConfiguration . getRefusedDelayMillis ( ) ) ; registerLater ( gateway , 1 , retryingRegistrationConfiguration . getInitialRegistrationTimeoutMillis ( ) , retryingRegistrationConfiguration . getRefusedDelayMillis ( ) ) ; } }
public void test() { if ( result instanceof RegistrationResponse . Success ) { log . debug ( "Registration with {} at {} was successful." , targetName , targetAddress ) ; S success = ( S ) result ; completionFuture . complete ( RetryingRegistrationResult . success ( gateway , success ) ) ; } else-if ( result instanceof RegistrationResponse . Rejection ) { log . debug ( "Registration with {} at {} was rejected." , targetName , targetAddress ) ; R rejection = ( R ) result ; completionFuture . complete ( RetryingRegistrationResult . rejection ( rejection ) ) ; } else { code_block = IfStatement ; log . info ( "Pausing and re-attempting registration in {} ms" , retryingRegistrationConfiguration . getRefusedDelayMillis ( ) ) ; registerLater ( gateway , 1 , retryingRegistrationConfiguration . getInitialRegistrationTimeoutMillis ( ) , retryingRegistrationConfiguration . getRefusedDelayMillis ( ) ) ; } }
public void test() { if ( result instanceof RegistrationResponse . Failure ) { RegistrationResponse . Failure failure = ( RegistrationResponse . Failure ) result ; log . info ( "Registration failure at {} occurred." , targetName , failure . getReason ( ) ) ; } else { log . error ( "Received unknown response to registration attempt: {}" , result ) ; } }
public void test() { if ( result instanceof RegistrationResponse . Failure ) { RegistrationResponse . Failure failure = ( RegistrationResponse . Failure ) result ; log . info ( "Registration failure at {} occurred." , targetName , failure . getReason ( ) ) ; } else { log . error ( "Received unknown response to registration attempt: {}" , result ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Registration at {} ({}) attempt {} timed out after {} ms" , targetName , targetAddress , attempt , timeoutMillis ) ; } }
public void test() { if ( ExceptionUtils . stripCompletionException ( failure ) instanceof TimeoutException ) { code_block = IfStatement ; long newTimeoutMillis = Math . min ( 2 * timeoutMillis , retryingRegistrationConfiguration . getMaxRegistrationTimeoutMillis ( ) ) ; register ( gateway , attempt + 1 , newTimeoutMillis ) ; } else { log . error ( "Registration at {} failed due to an error" , targetName , failure ) ; log . info ( "Pausing and re-attempting registration in {} ms" , retryingRegistrationConfiguration . getErrorDelayMillis ( ) ) ; registerLater ( gateway , 1 , retryingRegistrationConfiguration . getInitialRegistrationTimeoutMillis ( ) , retryingRegistrationConfiguration . getErrorDelayMillis ( ) ) ; } }
public void test() { if ( ExceptionUtils . stripCompletionException ( failure ) instanceof TimeoutException ) { code_block = IfStatement ; long newTimeoutMillis = Math . min ( 2 * timeoutMillis , retryingRegistrationConfiguration . getMaxRegistrationTimeoutMillis ( ) ) ; register ( gateway , attempt + 1 , newTimeoutMillis ) ; } else { log . error ( "Registration at {} failed due to an error" , targetName , failure ) ; log . info ( "Pausing and re-attempting registration in {} ms" , retryingRegistrationConfiguration . getErrorDelayMillis ( ) ) ; registerLater ( gateway , 1 , retryingRegistrationConfiguration . getInitialRegistrationTimeoutMillis ( ) , retryingRegistrationConfiguration . getErrorDelayMillis ( ) ) ; } }
public void test() { if ( entry == null ) { String msg = "Cannot add an empty entry" ; LOG . debug ( msg ) ; throw new IllegalArgumentException ( msg ) ; } }
public void test() { { LOGGER . info ( "Running test updateTimeAfterEndTimeProcess" ) ; String startTime = TimeUtil . getTimeWrtSystemTime ( - 15 ) ; String endTime = TimeUtil . getTimeWrtSystemTime ( 60 ) ; processBundle . setProcessValidity ( startTime , endTime ) ; processBundle . submitFeedsScheduleProcess ( prism ) ; TimeUtil . sleepSeconds ( 10 ) ; InstanceUtil . waitTillInstanceReachState ( serverOC . get ( 0 ) , Util . readEntityName ( processBundle . getProcessData ( ) ) , 0 , CoordinatorAction . Status . WAITING , EntityType . PROCESS ) ; String oldProcess = processBundle . getProcessData ( ) ; String oldBundleID = OozieUtil . getLatestBundleID ( cluster1OC , Util . readEntityName ( oldProcess ) , EntityType . PROCESS ) ; List < String > oldNominalTimes = OozieUtil . getActionsNominalTime ( cluster1OC , oldBundleID , EntityType . PROCESS ) ; processBundle . setProcessProperty ( "someProp" , "someVal" ) ; String updateTime = TimeUtil . addMinsToTime ( endTime , 60 ) ; LOGGER . info ( "Original Feed : " + Util . prettyPrintXml ( oldProcess ) ) ; LOGGER . info ( "Updated Feed :" + Util . prettyPrintXml ( processBundle . getProcessData ( ) ) ) ; LOGGER . info ( "Update Time : " + updateTime ) ; ServiceResponse r = prism . getProcessHelper ( ) . update ( oldProcess , processBundle . getProcessData ( ) ) ; AssertUtil . assertSucceeded ( r ) ; OozieUtil . verifyNewBundleCreation ( cluster1OC , oldBundleID , oldNominalTimes , oldProcess , true , false ) ; InstanceUtil . waitTillInstancesAreCreated ( cluster1OC , processBundle . getProcessData ( ) , 1 ) ; OozieUtil . verifyNewBundleCreation ( cluster1OC , oldBundleID , oldNominalTimes , oldProcess , true , true ) ; } }
public void test() { { LOGGER . info ( "Running test updateTimeAfterEndTimeProcess" ) ; String startTime = TimeUtil . getTimeWrtSystemTime ( - 15 ) ; String endTime = TimeUtil . getTimeWrtSystemTime ( 60 ) ; processBundle . setProcessValidity ( startTime , endTime ) ; processBundle . submitFeedsScheduleProcess ( prism ) ; TimeUtil . sleepSeconds ( 10 ) ; InstanceUtil . waitTillInstanceReachState ( serverOC . get ( 0 ) , Util . readEntityName ( processBundle . getProcessData ( ) ) , 0 , CoordinatorAction . Status . WAITING , EntityType . PROCESS ) ; String oldProcess = processBundle . getProcessData ( ) ; String oldBundleID = OozieUtil . getLatestBundleID ( cluster1OC , Util . readEntityName ( oldProcess ) , EntityType . PROCESS ) ; List < String > oldNominalTimes = OozieUtil . getActionsNominalTime ( cluster1OC , oldBundleID , EntityType . PROCESS ) ; processBundle . setProcessProperty ( "someProp" , "someVal" ) ; String updateTime = TimeUtil . addMinsToTime ( endTime , 60 ) ; LOGGER . info ( "Original Feed : " + Util . prettyPrintXml ( oldProcess ) ) ; LOGGER . info ( "Updated Feed :" + Util . prettyPrintXml ( processBundle . getProcessData ( ) ) ) ; LOGGER . info ( "Update Time : " + updateTime ) ; ServiceResponse r = prism . getProcessHelper ( ) . update ( oldProcess , processBundle . getProcessData ( ) ) ; AssertUtil . assertSucceeded ( r ) ; OozieUtil . verifyNewBundleCreation ( cluster1OC , oldBundleID , oldNominalTimes , oldProcess , true , false ) ; InstanceUtil . waitTillInstancesAreCreated ( cluster1OC , processBundle . getProcessData ( ) , 1 ) ; OozieUtil . verifyNewBundleCreation ( cluster1OC , oldBundleID , oldNominalTimes , oldProcess , true , true ) ; } }
public void test() { { LOGGER . info ( "Running test updateTimeAfterEndTimeProcess" ) ; String startTime = TimeUtil . getTimeWrtSystemTime ( - 15 ) ; String endTime = TimeUtil . getTimeWrtSystemTime ( 60 ) ; processBundle . setProcessValidity ( startTime , endTime ) ; processBundle . submitFeedsScheduleProcess ( prism ) ; TimeUtil . sleepSeconds ( 10 ) ; InstanceUtil . waitTillInstanceReachState ( serverOC . get ( 0 ) , Util . readEntityName ( processBundle . getProcessData ( ) ) , 0 , CoordinatorAction . Status . WAITING , EntityType . PROCESS ) ; String oldProcess = processBundle . getProcessData ( ) ; String oldBundleID = OozieUtil . getLatestBundleID ( cluster1OC , Util . readEntityName ( oldProcess ) , EntityType . PROCESS ) ; List < String > oldNominalTimes = OozieUtil . getActionsNominalTime ( cluster1OC , oldBundleID , EntityType . PROCESS ) ; processBundle . setProcessProperty ( "someProp" , "someVal" ) ; String updateTime = TimeUtil . addMinsToTime ( endTime , 60 ) ; LOGGER . info ( "Original Feed : " + Util . prettyPrintXml ( oldProcess ) ) ; LOGGER . info ( "Updated Feed :" + Util . prettyPrintXml ( processBundle . getProcessData ( ) ) ) ; LOGGER . info ( "Update Time : " + updateTime ) ; ServiceResponse r = prism . getProcessHelper ( ) . update ( oldProcess , processBundle . getProcessData ( ) ) ; AssertUtil . assertSucceeded ( r ) ; OozieUtil . verifyNewBundleCreation ( cluster1OC , oldBundleID , oldNominalTimes , oldProcess , true , false ) ; InstanceUtil . waitTillInstancesAreCreated ( cluster1OC , processBundle . getProcessData ( ) , 1 ) ; OozieUtil . verifyNewBundleCreation ( cluster1OC , oldBundleID , oldNominalTimes , oldProcess , true , true ) ; } }
public void test() { { LOGGER . info ( "Running test updateTimeAfterEndTimeProcess" ) ; String startTime = TimeUtil . getTimeWrtSystemTime ( - 15 ) ; String endTime = TimeUtil . getTimeWrtSystemTime ( 60 ) ; processBundle . setProcessValidity ( startTime , endTime ) ; processBundle . submitFeedsScheduleProcess ( prism ) ; TimeUtil . sleepSeconds ( 10 ) ; InstanceUtil . waitTillInstanceReachState ( serverOC . get ( 0 ) , Util . readEntityName ( processBundle . getProcessData ( ) ) , 0 , CoordinatorAction . Status . WAITING , EntityType . PROCESS ) ; String oldProcess = processBundle . getProcessData ( ) ; String oldBundleID = OozieUtil . getLatestBundleID ( cluster1OC , Util . readEntityName ( oldProcess ) , EntityType . PROCESS ) ; List < String > oldNominalTimes = OozieUtil . getActionsNominalTime ( cluster1OC , oldBundleID , EntityType . PROCESS ) ; processBundle . setProcessProperty ( "someProp" , "someVal" ) ; String updateTime = TimeUtil . addMinsToTime ( endTime , 60 ) ; LOGGER . info ( "Original Feed : " + Util . prettyPrintXml ( oldProcess ) ) ; LOGGER . info ( "Updated Feed :" + Util . prettyPrintXml ( processBundle . getProcessData ( ) ) ) ; LOGGER . info ( "Update Time : " + updateTime ) ; ServiceResponse r = prism . getProcessHelper ( ) . update ( oldProcess , processBundle . getProcessData ( ) ) ; AssertUtil . assertSucceeded ( r ) ; OozieUtil . verifyNewBundleCreation ( cluster1OC , oldBundleID , oldNominalTimes , oldProcess , true , false ) ; InstanceUtil . waitTillInstancesAreCreated ( cluster1OC , processBundle . getProcessData ( ) , 1 ) ; OozieUtil . verifyNewBundleCreation ( cluster1OC , oldBundleID , oldNominalTimes , oldProcess , true , true ) ; } }
public void test() { try { SpringApplication . run ( PowerJobServerApplication . class , args ) ; } catch ( Throwable t ) { log . error ( TIPS ) ; throw t ; } }
public void test() { if ( LOG . isInfoEnabled ( ) ) { LOG . info ( "BodyTrackController.setFacetMetadata(): Attempting to set metadata for facet [" + facetId + "] for connector [" + connectorName + "] and object type [" + objectTypeName + "]" ) ; } }
public void setLevel ( @ Nonnull Level level ) { this . level = level ; parent . setLevel ( level ) ; log . info ( "Level changed to " + level . getName ( ) ) ; }
public void test() { if ( e . getCode ( ) == HttpURLConnection . HTTP_CONFLICT ) { log . debug ( "received {} when updating lease lock" , e . getCode ( ) , e ) ; } else { log . error ( "received {} when updating lease lock" , e . getCode ( ) , e ) ; } }
public void test() { if ( e . getCode ( ) == HttpURLConnection . HTTP_CONFLICT ) { log . debug ( "received {} when updating lease lock" , e . getCode ( ) , e ) ; } else { log . error ( "received {} when updating lease lock" , e . getCode ( ) , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "validate() action=" + action . name ( ) ) ; } }
public void test() { if ( invalid ) { String msg = msgBldr . toString ( ) ; logger . error ( msg ) ; throw new InvalidDocumentException ( msg ) ; } }
public void test() { if ( peekNullable ( ) == null && waitTime > 0 ) { boolean await = dataAvailable . await ( Math . min ( waitTime , MAX_AWAIT_AVAILABLE_DATA ) , TimeUnit . MILLISECONDS ) ; logger . trace ( await ? "Signaled new events are available" : "No signal received for new events, exiting await" ) ; } }
public void test() { try { code_block = WhileStatement ; return peekNullable ( ) != null ; } catch ( InterruptedException e ) { logger . warn ( "Event consumer thread was interrupted. Returning thread to event processor." , e ) ; Thread . currentThread ( ) . interrupt ( ) ; return false ; } }
public void test() { if ( limit != Integer . MAX_VALUE ) { logger . warn ( "Setting limit to {} but in current olap context, the limit is already {}, won't apply" , l , limit ) ; } else { limit = l ; } }
public void test() { if ( ! readMarker . hasIdentifier ( ) ) { this . messageTimeStart = readMarker . getStartTime ( times ) ; log . info ( "Loaded unidentified ReadMarker start time {} into {}" , messageTimeStart , this ) ; } else { long savedTimestamp = readSetting ( readMarker . getIdentifier ( ) , getMarkerColumn ( partitionId , bucketId ) , times . getTime ( readMarker . getStartTime ( times ) ) ) ; this . messageTimeStart = times . getTime ( savedTimestamp ) ; log . info ( "Loaded identified ReadMarker start time {} into {}" , messageTimeStart , this ) ; } }
@ Override public Document getNextPage ( Document doc ) throws IOException { String nextUrl = "" ; Element elem = doc . select ( "li.next > a" ) . first ( ) ; logger . info ( elem ) ; nextUrl = elem . attr ( "href" ) ; code_block = IfStatement ; return Http . url ( "https://eroshare.com" + nextUrl ) . get ( ) ; }
public void test() { try { logger . info ( "getPublicLandingPageTemplate: " + crisisID ) ; logger . info ( "url: " + taggerMainUrl + "/customuitemplate/crisisID/" + crisisID ) ; WebTarget webResource = client . target ( taggerMainUrl + "/customuitemplate/crisisID/" + crisisID ) ; ObjectMapper objectMapper = JacksonWrapper . getObjectMapper ( ) ; Response clientResponse = webResource . request ( MediaType . APPLICATION_JSON ) . get ( ) ; String jsonResponse = clientResponse . readEntity ( String . class ) ; logger . info ( "Received jsonResponse: " + jsonResponse ) ; code_block = IfStatement ; return null ; } catch ( Exception e ) { logger . error ( "Error while creating new template in Tagger" , e ) ; throw new AidrException ( "Error while creating new template in Tagger" , e ) ; } }
public void test() { try { logger . info ( "getPublicLandingPageTemplate: " + crisisID ) ; logger . info ( "url: " + taggerMainUrl + "/customuitemplate/crisisID/" + crisisID ) ; WebTarget webResource = client . target ( taggerMainUrl + "/customuitemplate/crisisID/" + crisisID ) ; ObjectMapper objectMapper = JacksonWrapper . getObjectMapper ( ) ; Response clientResponse = webResource . request ( MediaType . APPLICATION_JSON ) . get ( ) ; String jsonResponse = clientResponse . readEntity ( String . class ) ; logger . info ( "Received jsonResponse: " + jsonResponse ) ; code_block = IfStatement ; return null ; } catch ( Exception e ) { logger . error ( "Error while creating new template in Tagger" , e ) ; throw new AidrException ( "Error while creating new template in Tagger" , e ) ; } }
public void test() { try { logger . info ( "getPublicLandingPageTemplate: " + crisisID ) ; logger . info ( "url: " + taggerMainUrl + "/customuitemplate/crisisID/" + crisisID ) ; WebTarget webResource = client . target ( taggerMainUrl + "/customuitemplate/crisisID/" + crisisID ) ; ObjectMapper objectMapper = JacksonWrapper . getObjectMapper ( ) ; Response clientResponse = webResource . request ( MediaType . APPLICATION_JSON ) . get ( ) ; String jsonResponse = clientResponse . readEntity ( String . class ) ; logger . info ( "Received jsonResponse: " + jsonResponse ) ; code_block = IfStatement ; return null ; } catch ( Exception e ) { logger . error ( "Error while creating new template in Tagger" , e ) ; throw new AidrException ( "Error while creating new template in Tagger" , e ) ; } }
public void test() { try { logger . info ( "getPublicLandingPageTemplate: " + crisisID ) ; logger . info ( "url: " + taggerMainUrl + "/customuitemplate/crisisID/" + crisisID ) ; WebTarget webResource = client . target ( taggerMainUrl + "/customuitemplate/crisisID/" + crisisID ) ; ObjectMapper objectMapper = JacksonWrapper . getObjectMapper ( ) ; Response clientResponse = webResource . request ( MediaType . APPLICATION_JSON ) . get ( ) ; String jsonResponse = clientResponse . readEntity ( String . class ) ; logger . info ( "Received jsonResponse: " + jsonResponse ) ; code_block = IfStatement ; return null ; } catch ( Exception e ) { logger . error ( "Error while creating new template in Tagger" , e ) ; throw new AidrException ( "Error while creating new template in Tagger" , e ) ; } }
public void test() { if ( blob == null ) { log . debug ( "Document {} is a BlobHolder without a blob." , doc :: getId ) ; } }
public void test() { if ( ! ( any instanceof Element ) ) { LOG . debug ( "Properties is not of class Element." ) ; return null ; } }
public void attachClean ( SysHilfe instance ) { log . debug ( "attaching clean SysHilfe instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . lock ( instance , LockMode . NONE ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { db = DB . newEmbeddedDB ( DBConfigurationBuilder . newBuilder ( ) . build ( ) ) ; db . start ( ) ; dbUrl = db . getConfiguration ( ) . getURL ( DEFAULT_DB_NAME ) ; connection = DriverManager . getConnection ( dbUrl ) ; code_block = TryStatement ;  } catch ( Exception e ) { logger . warn ( "Initialize DB failed." , e ) ; stopDb ( ) ; } }
public void test() { try { restoreFromDb ( sessionHandle ) ; } catch ( LensException le ) { log . warn ( "Session  " + sessionHandle . getPublicId ( ) + " is invalid." ) ; } }
@ Test public void test_systemSchemaAccess_stateOnlyNoLdapGroupUser ( ) throws Exception { HttpUtil . makeRequest ( stateOnlyUserClient , HttpMethod . GET , config . getBrokerUrl ( ) + "/status" , null ) ; LOG . info ( "Checking sys.segments query as stateOnlyNoLdapGroupUser..." ) ; verifySystemSchemaQuery ( stateOnlyNoLdapGroupUserClient , SYS_SCHEMA_SEGMENTS_QUERY , Collections . emptyList ( ) ) ; LOG . info ( "Checking sys.servers query as stateOnlyNoLdapGroupUser..." ) ; verifySystemSchemaServerQuery ( stateOnlyNoLdapGroupUserClient , SYS_SCHEMA_SERVERS_QUERY , adminServers ) ; LOG . info ( "Checking sys.server_segments query as stateOnlyNoLdapGroupUser..." ) ; verifySystemSchemaQuery ( stateOnlyNoLdapGroupUserClient , SYS_SCHEMA_SERVER_SEGMENTS_QUERY , Collections . emptyList ( ) ) ; LOG . info ( "Checking sys.tasks query as stateOnlyNoLdapGroupUser..." ) ; verifySystemSchemaQuery ( stateOnlyNoLdapGroupUserClient , SYS_SCHEMA_TASKS_QUERY , Collections . emptyList ( ) ) ; }
@ Test public void test_systemSchemaAccess_stateOnlyNoLdapGroupUser ( ) throws Exception { HttpUtil . makeRequest ( stateOnlyUserClient , HttpMethod . GET , config . getBrokerUrl ( ) + "/status" , null ) ; LOG . info ( "Checking sys.segments query as stateOnlyNoLdapGroupUser..." ) ; verifySystemSchemaQuery ( stateOnlyNoLdapGroupUserClient , SYS_SCHEMA_SEGMENTS_QUERY , Collections . emptyList ( ) ) ; LOG . info ( "Checking sys.servers query as stateOnlyNoLdapGroupUser..." ) ; verifySystemSchemaServerQuery ( stateOnlyNoLdapGroupUserClient , SYS_SCHEMA_SERVERS_QUERY , adminServers ) ; LOG . info ( "Checking sys.server_segments query as stateOnlyNoLdapGroupUser..." ) ; verifySystemSchemaQuery ( stateOnlyNoLdapGroupUserClient , SYS_SCHEMA_SERVER_SEGMENTS_QUERY , Collections . emptyList ( ) ) ; LOG . info ( "Checking sys.tasks query as stateOnlyNoLdapGroupUser..." ) ; verifySystemSchemaQuery ( stateOnlyNoLdapGroupUserClient , SYS_SCHEMA_TASKS_QUERY , Collections . emptyList ( ) ) ; }
@ Test public void test_systemSchemaAccess_stateOnlyNoLdapGroupUser ( ) throws Exception { HttpUtil . makeRequest ( stateOnlyUserClient , HttpMethod . GET , config . getBrokerUrl ( ) + "/status" , null ) ; LOG . info ( "Checking sys.segments query as stateOnlyNoLdapGroupUser..." ) ; verifySystemSchemaQuery ( stateOnlyNoLdapGroupUserClient , SYS_SCHEMA_SEGMENTS_QUERY , Collections . emptyList ( ) ) ; LOG . info ( "Checking sys.servers query as stateOnlyNoLdapGroupUser..." ) ; verifySystemSchemaServerQuery ( stateOnlyNoLdapGroupUserClient , SYS_SCHEMA_SERVERS_QUERY , adminServers ) ; LOG . info ( "Checking sys.server_segments query as stateOnlyNoLdapGroupUser..." ) ; verifySystemSchemaQuery ( stateOnlyNoLdapGroupUserClient , SYS_SCHEMA_SERVER_SEGMENTS_QUERY , Collections . emptyList ( ) ) ; LOG . info ( "Checking sys.tasks query as stateOnlyNoLdapGroupUser..." ) ; verifySystemSchemaQuery ( stateOnlyNoLdapGroupUserClient , SYS_SCHEMA_TASKS_QUERY , Collections . emptyList ( ) ) ; }
@ Test public void test_systemSchemaAccess_stateOnlyNoLdapGroupUser ( ) throws Exception { HttpUtil . makeRequest ( stateOnlyUserClient , HttpMethod . GET , config . getBrokerUrl ( ) + "/status" , null ) ; LOG . info ( "Checking sys.segments query as stateOnlyNoLdapGroupUser..." ) ; verifySystemSchemaQuery ( stateOnlyNoLdapGroupUserClient , SYS_SCHEMA_SEGMENTS_QUERY , Collections . emptyList ( ) ) ; LOG . info ( "Checking sys.servers query as stateOnlyNoLdapGroupUser..." ) ; verifySystemSchemaServerQuery ( stateOnlyNoLdapGroupUserClient , SYS_SCHEMA_SERVERS_QUERY , adminServers ) ; LOG . info ( "Checking sys.server_segments query as stateOnlyNoLdapGroupUser..." ) ; verifySystemSchemaQuery ( stateOnlyNoLdapGroupUserClient , SYS_SCHEMA_SERVER_SEGMENTS_QUERY , Collections . emptyList ( ) ) ; LOG . info ( "Checking sys.tasks query as stateOnlyNoLdapGroupUser..." ) ; verifySystemSchemaQuery ( stateOnlyNoLdapGroupUserClient , SYS_SCHEMA_TASKS_QUERY , Collections . emptyList ( ) ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( String . format ( "Run with feature %s" , feature ) ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( String . format ( "Artifact repository password encrypted: [application-id] %s " + "[tenant-id] %d [repo-url] %s" , applicationSignUp . getApplicationId ( ) , applicationSignUp . getTenantId ( ) , artifactRepository . getRepoUrl ( ) ) ) ; } }
public void test() { if ( state . error ( ) . isEmpty ( ) ) { log . info ( "{}: Worker {} finished with status '{}'" , nodeName , task . id , JsonUtil . toJsonString ( state . status ( ) ) ) ; } else { log . warn ( "{}: Worker {} finished with error '{}' and status '{}'" , nodeName , task . id , state . error ( ) , JsonUtil . toJsonString ( state . status ( ) ) ) ; task . maybeSetError ( state . error ( ) ) ; } }
public void test() { if ( state . error ( ) . isEmpty ( ) ) { log . info ( "{}: Worker {} finished with status '{}'" , nodeName , task . id , JsonUtil . toJsonString ( state . status ( ) ) ) ; } else { log . warn ( "{}: Worker {} finished with error '{}' and status '{}'" , nodeName , task . id , state . error ( ) , JsonUtil . toJsonString ( state . status ( ) ) ) ; task . maybeSetError ( state . error ( ) ) ; } }
public void test() { if ( activeWorkerIds . isEmpty ( ) ) { task . doneMs = time . milliseconds ( ) ; task . state = TaskStateType . DONE ; log . info ( "{}: Task {} is now complete on {} with error: {}" , nodeName , task . id , Utils . join ( task . workerIds . keySet ( ) , ", " ) , task . error . isEmpty ( ) ? "(none)" : task . error ) ; } else-if ( ( task . state == TaskStateType . RUNNING ) && ( ! task . error . isEmpty ( ) ) ) { log . info ( "{}: task {} stopped with error {}.  Stopping worker(s): {}" , nodeName , task . id , task . error , Utils . mkString ( activeWorkerIds , "{" , "}" , ": " , ", " ) ) ; task . state = TaskStateType . STOPPING ; code_block = ForStatement ; } }
public void test() { if ( activeWorkerIds . isEmpty ( ) ) { task . doneMs = time . milliseconds ( ) ; task . state = TaskStateType . DONE ; log . info ( "{}: Task {} is now complete on {} with error: {}" , nodeName , task . id , Utils . join ( task . workerIds . keySet ( ) , ", " ) , task . error . isEmpty ( ) ? "(none)" : task . error ) ; } else-if ( ( task . state == TaskStateType . RUNNING ) && ( ! task . error . isEmpty ( ) ) ) { log . info ( "{}: task {} stopped with error {}.  Stopping worker(s): {}" , nodeName , task . id , task . error , Utils . mkString ( activeWorkerIds , "{" , "}" , ": " , ", " ) ) ; task . state = TaskStateType . STOPPING ; code_block = ForStatement ; } }
void receiveRecords ( TlsContext receiveFromCtx ) { LOGGER . debug ( "Receiving records..." ) ; receivedRecords = receiveMessageHelper . receiveRecords ( receiveFromCtx ) ; LOGGER . info ( "Records received (" + receiveFromAlias + "): " + receivedRecords . size ( ) ) ; executedAsPlanned = true ; }
void receiveRecords ( TlsContext receiveFromCtx ) { LOGGER . debug ( "Receiving records..." ) ; receivedRecords = receiveMessageHelper . receiveRecords ( receiveFromCtx ) ; LOGGER . info ( "Records received (" + receiveFromAlias + "): " + receivedRecords . size ( ) ) ; executedAsPlanned = true ; }
public void test() { try { ownedBundle . handleUnloadRequest ( pulsar , 5 , TimeUnit . MINUTES ) ; } catch ( IllegalStateException ex ) { } catch ( Exception ex ) { LOG . error ( "Unexpected exception occur when register owned bundle {}. Shutdown broker now !!!" , ownedBundle . getNamespaceBundle ( ) , ex ) ; pulsar . getShutdownService ( ) . shutdown ( - 1 ) ; } }
public void test() { if ( record . getId ( ) == null ) { logger . error ( "Invalid record: Id missing in " + record ) ; continue ; } }
public void test() { try { Constructor < T > constructor = clazz . getConstructor ( new Class [ ] code_block = "" ; ) ; T instance = constructor . newInstance ( record ) ; map . put ( record . getId ( ) , instance ) ; } catch ( Exception e ) { logger . error ( "Error creating an Object of type:" + clazz . getCanonicalName ( ) , e ) ; } }
public final void start ( BundleContext context ) throws Exception { serviceRegistration = context . registerService ( ProxyTargetLocatorFactory . class , new SpringDMProxyTargetLocatorFactory ( ) , null ) ; LOGGER . info ( "registered Spring DM injection SPI for PAX Wicket." ) ; }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { LOG . warn ( "Unable to extract assertion from context." , e ) ; } }
public void test() { try { Log . debug ( "Annotating Lane " + laneSWID ) ; Lane lane = ll . findLane ( "/" + laneSWID ) ; lane . getLaneAttributes ( ) . clear ( ) ; code_block = ForStatement ; ll . updateLane ( "/" + laneSWID , lane ) ; } catch ( IOException ex ) { Log . error ( "IOException while updating lane " + laneSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } catch ( JAXBException ex ) { Log . error ( "JAXBException while updating lane " + laneSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } catch ( ResourceException ex ) { Log . error ( "ResourceException while updating lane " + laneSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } }
public void test() { try { Log . debug ( "Annotating Lane " + laneSWID ) ; Lane lane = ll . findLane ( "/" + laneSWID ) ; lane . getLaneAttributes ( ) . clear ( ) ; code_block = ForStatement ; ll . updateLane ( "/" + laneSWID , lane ) ; } catch ( IOException ex ) { Log . error ( "IOException while updating lane " + laneSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } catch ( JAXBException ex ) { Log . error ( "JAXBException while updating lane " + laneSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } catch ( ResourceException ex ) { Log . error ( "ResourceException while updating lane " + laneSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } }
public void test() { try { Log . debug ( "Annotating Lane " + laneSWID ) ; Lane lane = ll . findLane ( "/" + laneSWID ) ; lane . getLaneAttributes ( ) . clear ( ) ; code_block = ForStatement ; ll . updateLane ( "/" + laneSWID , lane ) ; } catch ( IOException ex ) { Log . error ( "IOException while updating lane " + laneSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } catch ( JAXBException ex ) { Log . error ( "JAXBException while updating lane " + laneSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } catch ( ResourceException ex ) { Log . error ( "ResourceException while updating lane " + laneSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } }
public void test() { try { Log . debug ( "Annotating Lane " + laneSWID ) ; Lane lane = ll . findLane ( "/" + laneSWID ) ; lane . getLaneAttributes ( ) . clear ( ) ; code_block = ForStatement ; ll . updateLane ( "/" + laneSWID , lane ) ; } catch ( IOException ex ) { Log . error ( "IOException while updating lane " + laneSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } catch ( JAXBException ex ) { Log . error ( "JAXBException while updating lane " + laneSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } catch ( ResourceException ex ) { Log . error ( "ResourceException while updating lane " + laneSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } }
private List < ModuleType > getAllModuleTypesFallback ( Throwable e ) { log . error ( "getAllModuleTypesFallback()" ) ; e . printStackTrace ( ) ; return null ; }
public void test() { try { MBEAN_SERVER . registerMBean ( object , objectName ) ; } catch ( LinkageError | Exception e ) { log . warn ( "Failed to complete JMX registration to " + objectName , e ) ; } }
public void test() { if ( getCause ( e ) instanceof ConnectException ) { logger . error ( String . format ( "%s, cannot connect to %s" , m , jmx ) ) ; } else-if ( e instanceof InterruptedException ) { logger . error ( String . format ( "%s, interrupted" , m ) ) ; } else { logger . error ( String . format ( "%s, cannot setup beans" , m ) , e ) ; } }
public void test() { if ( getCause ( e ) instanceof ConnectException ) { logger . error ( String . format ( "%s, cannot connect to %s" , m , jmx ) ) ; } else-if ( e instanceof InterruptedException ) { logger . error ( String . format ( "%s, interrupted" , m ) ) ; } else { logger . error ( String . format ( "%s, cannot setup beans" , m ) , e ) ; } }
public void test() { if ( getCause ( e ) instanceof ConnectException ) { logger . error ( String . format ( "%s, cannot connect to %s" , m , jmx ) ) ; } else-if ( e instanceof InterruptedException ) { logger . error ( String . format ( "%s, interrupted" , m ) ) ; } else { logger . error ( String . format ( "%s, cannot setup beans" , m ) , e ) ; } }
public void test() { if ( LOG . isWarnEnabled ( ) ) { LOG . warn ( "Failed to find artist bio to " + artistName , e ) ; } }
protected void loadConf ( Configuration conf ) { this . slop = normalizeSlop ( conf . getFloat ( "hbase.regions.slop" , getDefaultSlop ( ) ) ) ; this . rackManager = new RackManager ( conf ) ; useRegionFinder = conf . getBoolean ( "hbase.master.balancer.uselocality" , true ) ; code_block = IfStatement ; this . isByTable = conf . getBoolean ( HConstants . HBASE_MASTER_LOADBALANCE_BYTABLE , isByTable ) ; LOG . info ( "slop={}" , this . slop ) ; }
public void test() { if ( _logger . isDebugEnabled ( ) ) { _logger . debug ( description + ", reducing priority by " + ServiceReplicationStatus . INCONSISTENT_WITH_MIRROR_FACTOR ) ; } }
public void test() { if ( _logger . isDebugEnabled ( ) ) { _logger . debug ( description + ", reducing priority by " + ServiceReplicationStatus . INCONSISTENT_WITH_SPACE_FACTOR ) ; } }
public void test() { if ( authenticator != null ) { authenticator . decorateProxyRequest ( clientRequest , proxyResponse , proxyRequest ) ; } else { log . error ( "Can not find Authenticator with Name [%s]" , authenticationResult . getAuthenticatedBy ( ) ) ; } }
public void test() { if ( applicationInstanceId == null ) { applicationInstanceId = this . applicationDescriptor . toString ( ) ; set ( AstrixSettings . APPLICATION_INSTANCE_ID , this . applicationDescriptor . toString ( ) ) ; log . info ( "No applicationInstanceId set, using name of ApplicationDescriptor as applicationInstanceId: {}" , applicationInstanceId ) ; Objects . requireNonNull ( AstrixSettings . APPLICATION_INSTANCE_ID . getFrom ( config ) . get ( ) ) ; } else { log . info ( "Current applicationInstanceId={}" , applicationInstanceId ) ; } }
public void test() { if ( applicationInstanceId == null ) { applicationInstanceId = this . applicationDescriptor . toString ( ) ; set ( AstrixSettings . APPLICATION_INSTANCE_ID , this . applicationDescriptor . toString ( ) ) ; log . info ( "No applicationInstanceId set, using name of ApplicationDescriptor as applicationInstanceId: {}" , applicationInstanceId ) ; Objects . requireNonNull ( AstrixSettings . APPLICATION_INSTANCE_ID . getFrom ( config ) . get ( ) ) ; } else { log . info ( "Current applicationInstanceId={}" , applicationInstanceId ) ; } }
@ ExceptionHandler ( HttpMessageNotReadableException . class ) @ ResponseStatus ( HttpStatus . BAD_REQUEST ) @ ResponseBody public ValidationErrorResponse handleHttpMessageNotReadableException ( HttpMessageNotReadableException e ) { logger . error ( "request failed with HttpMessageNotReadableException" , e ) ; ValidationErrorResponse error = new ValidationErrorResponse ( ) ; error . getViolations ( ) . add ( new Violation ( "" , "request body is required" ) ) ; return error ; }
public void test() { try { HttpSession session = request . getSession ( ) ; SessionObject userSession = ( SessionObject ) session . getAttribute ( FdahpStudyDesignerConstants . SESSION_OBJECT ) ; userBO . setModifiedBy ( userSession . getUserId ( ) ) ; userBO . setModifiedOn ( FdahpStudyDesignerUtil . getCurrentDateTime ( ) ) ; userId = userSession . getUserId ( ) ; message = dashBoardAndProfileService . updateProfileDetails ( userBO , userId , userSession ) ; code_block = IfStatement ; code_block = IfStatement ; mav = new ModelAndView ( "redirect:/adminDashboard/viewUserDetails.do" ) ; } catch ( Exception e ) { logger . error ( "DashBoardAndProfileController:  updateProfileDetails()' = " , e ) ; } }
public void test() { if ( ex . getMessage ( ) . contains ( "Does Not Exist" ) ) { } else { logger . debug ( "Exception occurred when trying to remove inactive device '{}': {}" , address , ex . getMessage ( ) ) ; } }
public void test() { try { dev . getAdapter ( ) . removeDevice ( dev . getRawDevice ( ) ) ; } catch ( DBusException ex ) { code_block = IfStatement ; } catch ( RuntimeException ex ) { logger . debug ( "Exception occurred when trying to remove inactive device '{}': {}" , address , ex . getMessage ( ) ) ; } }
public void test() { if ( ! oldOrg . isEndorsementApproved ( ) && newOrg . isEndorsementApproved ( ) ) { LOG . info ( "Starting crawl of all datasets for newly endorsed org [{}]" , newOrg . getKey ( ) ) ; DatasetVisitor visitor = dataset code_block = LoopStatement ; ; visitOwnedDatasets ( newOrg . getKey ( ) , visitor ) ; } else-if ( occurrenceMutator . requiresUpdate ( oldOrg , newOrg ) && newOrg . getNumPublishedDatasets ( ) > 0 ) { LOG . info ( "Starting ingestion of all datasets of org [{}] because it has changed country from [{}] to [{}]" , newOrg . getKey ( ) , oldOrg . getCountry ( ) , newOrg . getCountry ( ) ) ; DatasetVisitor visitor = dataset code_block = LoopStatement ; ; visitOwnedDatasets ( newOrg . getKey ( ) , visitor ) ; } }
public void test() { try { messagePublisher . send ( new StartCrawlMessage ( dataset . getKey ( ) ) ) ; } catch ( IOException e ) { LOG . warn ( "Could not send start crawl message for newly endorsed dataset key [{}]" , dataset . getKey ( ) , e ) ; } }
public void test() { if ( ! oldOrg . isEndorsementApproved ( ) && newOrg . isEndorsementApproved ( ) ) { LOG . info ( "Starting crawl of all datasets for newly endorsed org [{}]" , newOrg . getKey ( ) ) ; DatasetVisitor visitor = dataset code_block = LoopStatement ; ; visitOwnedDatasets ( newOrg . getKey ( ) , visitor ) ; } else-if ( occurrenceMutator . requiresUpdate ( oldOrg , newOrg ) && newOrg . getNumPublishedDatasets ( ) > 0 ) { LOG . info ( "Starting ingestion of all datasets of org [{}] because it has changed country from [{}] to [{}]" , newOrg . getKey ( ) , oldOrg . getCountry ( ) , newOrg . getCountry ( ) ) ; DatasetVisitor visitor = dataset code_block = LoopStatement ; ; visitOwnedDatasets ( newOrg . getKey ( ) , visitor ) ; } }
public void test() { try { Class < CustomDatatype < ? > > datatypeClazz = ( Class < CustomDatatype < ? > > ) Class . forName ( datatypeClassname ) . asSubclass ( CustomDatatype . class ) ; globalProperty = new GlobalProperty ( property , defaultValue , description , datatypeClazz , datatypeConfig ) ; } catch ( ClassCastException ex ) { log . error ( "The class specified by 'datatypeClassname' (" + datatypeClassname + ") must be a subtype of 'org.openmrs.customdatatype.CustomDatatype<?>'." , ex ) ; } catch ( ClassNotFoundException ex ) { log . error ( "The class specified by 'datatypeClassname' (" + datatypeClassname + ") could not be found." , ex ) ; } }
public void test() { try { Class < CustomDatatype < ? > > datatypeClazz = ( Class < CustomDatatype < ? > > ) Class . forName ( datatypeClassname ) . asSubclass ( CustomDatatype . class ) ; globalProperty = new GlobalProperty ( property , defaultValue , description , datatypeClazz , datatypeConfig ) ; } catch ( ClassCastException ex ) { log . error ( "The class specified by 'datatypeClassname' (" + datatypeClassname + ") must be a subtype of 'org.openmrs.customdatatype.CustomDatatype<?>'." , ex ) ; } catch ( ClassNotFoundException ex ) { log . error ( "The class specified by 'datatypeClassname' (" + datatypeClassname + ") could not be found." , ex ) ; } }
@ Test public void pausedDoesNotAppend ( ) { alertAppender . pause ( ) ; logger . warn ( logMessage ) ; assertThat ( alertAppender . getLogEvents ( ) ) . isEmpty ( ) ; }
public void test() { try { Timeout t = new Timeout ( Duration . create ( 10 , TimeUnit . SECONDS ) ) ; obj = Await . result ( Patterns . ask ( actorRef , request , t ) , t . duration ( ) ) ; } catch ( ProjectCommonException pce ) { throw pce ; } catch ( Exception e ) { logger . error ( context , "searchManagedUser: Exception occurred with error message = " + e . getMessage ( ) , e ) ; ProjectCommonException . throwServerErrorException ( ResponseCode . unableToCommunicateWithActor , ResponseCode . unableToCommunicateWithActor . getErrorMessage ( ) ) ; } }
public void test() { if ( notifyAllNodes ) { logger . debug ( "Notifying all nodes that status changed from {} to {}" , currentStatus , status ) ; } else { logger . debug ( "Notifying cluster coordinator that node status changed from {} to {}" , currentStatus , status ) ; } }
public void test() { if ( notifyAllNodes ) { logger . debug ( "Notifying all nodes that status changed from {} to {}" , currentStatus , status ) ; } else { logger . debug ( "Notifying cluster coordinator that node status changed from {} to {}" , currentStatus , status ) ; } }
public void test() { if ( currentState == null || currentState != status . getState ( ) ) { final boolean notifyAllNodes = isActiveClusterCoordinator ( ) ; code_block = IfStatement ; notifyOthersOfNodeStatusChange ( status , notifyAllNodes , waitForCoordinator ) ; } else { logger . debug ( "Not notifying other nodes that status changed because previous state of {} is same as new state of {}" , currentState , status . getState ( ) ) ; } }
public void test() { if ( resp . isFailed ( ) && resp . getFailure ( ) != null ) { final DocWriteRequest < ? > req = requests . get ( i ) ; final Failure failure = resp . getFailure ( ) ; logger . debug ( "Failed Request: {}\n=>{}" , req , failure . getMessage ( ) ) ; } }
public void test() { try { ss = new ServerSocket ( port ) ; } catch ( IOException e ) { Log . error ( e , "Couldn't open command server port: " + port ) ; System . exit ( - 1 ) ; } }
public void test() { try { String check = "<p><a href=\"smb://hoge/data\" title=\"UNCPathLink\" rel=\"nofollow\">UNCPathLink</a></p>" ; org . junit . Assert . assertEquals ( check , result ) ; } catch ( AssertionError e ) { LOG . info ( "Sanitize" ) ; LOG . info ( "[Base]   : " + base ) ; LOG . info ( "[Result] : " + result ) ; throw e ; } }
public void test() { try { String check = "<p><a href=\"smb://hoge/data\" title=\"UNCPathLink\" rel=\"nofollow\">UNCPathLink</a></p>" ; org . junit . Assert . assertEquals ( check , result ) ; } catch ( AssertionError e ) { LOG . info ( "Sanitize" ) ; LOG . info ( "[Base]   : " + base ) ; LOG . info ( "[Result] : " + result ) ; throw e ; } }
public void test() { try { String check = "<p><a href=\"smb://hoge/data\" title=\"UNCPathLink\" rel=\"nofollow\">UNCPathLink</a></p>" ; org . junit . Assert . assertEquals ( check , result ) ; } catch ( AssertionError e ) { LOG . info ( "Sanitize" ) ; LOG . info ( "[Base]   : " + base ) ; LOG . info ( "[Result] : " + result ) ; throw e ; } }
public void createAccountsAndTransfer ( CryptoServiceGrpc . CryptoServiceBlockingStub stub ) throws Exception { Path startUpAccountPathJson = getStartupPath ( ) ; String path = startUpAccountPathJson . toString ( ) ; hederaStartupAccount = getStartupAccountMap ( path ) ; AccountKeyListObj payerAccountDetails = getPayerAccount ( hederaStartupAccount ) ; AccountID nodeAccount = AccountID . newBuilder ( ) . setAccountNum ( 3 ) . setRealmNum ( 0 ) . setShardNum ( 0 ) . build ( ) ; KeyPair accountKeyPair = new KeyPairGenerator ( ) . generateKeyPair ( ) ; KeyPairObj genKeyPairObj = payerAccountDetails . getKeyPairList ( ) . get ( 0 ) ; PrivateKey genesisPrivateKey = genKeyPairObj . getPrivateKey ( ) ; KeyPair genKeyPair = new KeyPair ( genKeyPairObj . getPublicKey ( ) , genesisPrivateKey ) ; TestHelper . initializeFeeClient ( channel , payerAccountDetails . getAccountId ( ) , genKeyPair , nodeAccount ) ; Transaction transaction = TestHelper . createAccountWithFee ( payerAccountDetails . getAccountId ( ) , nodeAccount , accountKeyPair , 10000l , Collections . singletonList ( genesisPrivateKey ) ) ; TransactionResponse response = stub . createAccount ( transaction ) ; Assert . assertNotNull ( response ) ; Assert . assertEquals ( ResponseCodeEnum . OK , response . getNodeTransactionPrecheckCode ( ) ) ; log . info ( "Pre Check Response of Create first account :: " + response . getNodeTransactionPrecheckCode ( ) . name ( ) ) ; TransactionBody body = TransactionBody . parseFrom ( transaction . getBodyBytes ( ) ) ; AccountID newlyCreateAccountId1 = TestHelper . getTxReceipt ( body . getTransactionID ( ) , stub ) . getAccountID ( ) ; Assert . assertNotNull ( newlyCreateAccountId1 ) ; log . info ( "Account ID " + newlyCreateAccountId1 . getAccountNum ( ) + " created successfully." ) ; log . info ( "--------------------------------------" ) ; Transaction transfer = TestHelper . createTransferSigMap ( payerAccountDetails . getAccountId ( ) , genKeyPair , newlyCreateAccountId1 , payerAccountDetails . getAccountId ( ) , genKeyPair , nodeAccount , 1000l ) ; log . info ( "Transferring 1000 coin from Genesis account to Newly created account...." ) ; TransactionResponse transferRes = stub . cryptoTransfer ( transfer ) ; Assert . assertNotNull ( transferRes ) ; Assert . assertEquals ( ResponseCodeEnum . OK , transferRes . getNodeTransactionPrecheckCode ( ) ) ; log . info ( "Pre Check Response transfer :: " + transferRes . getNodeTransactionPrecheckCode ( ) . name ( ) ) ; TransactionBody transferBody = TransactionBody . parseFrom ( transaction . getBodyBytes ( ) ) ; TransactionReceipt txReceipt = TestHelper . getTxReceipt ( transferBody . getTransactionID ( ) , stub ) ; Assert . assertNotNull ( txReceipt ) ; log . info ( "Transfer from Genesis to newly created account completed...." ) ; log . info ( "-----------------------------------------" ) ; }
public void createAccountsAndTransfer ( CryptoServiceGrpc . CryptoServiceBlockingStub stub ) throws Exception { Path startUpAccountPathJson = getStartupPath ( ) ; String path = startUpAccountPathJson . toString ( ) ; hederaStartupAccount = getStartupAccountMap ( path ) ; AccountKeyListObj payerAccountDetails = getPayerAccount ( hederaStartupAccount ) ; AccountID nodeAccount = AccountID . newBuilder ( ) . setAccountNum ( 3 ) . setRealmNum ( 0 ) . setShardNum ( 0 ) . build ( ) ; KeyPair accountKeyPair = new KeyPairGenerator ( ) . generateKeyPair ( ) ; KeyPairObj genKeyPairObj = payerAccountDetails . getKeyPairList ( ) . get ( 0 ) ; PrivateKey genesisPrivateKey = genKeyPairObj . getPrivateKey ( ) ; KeyPair genKeyPair = new KeyPair ( genKeyPairObj . getPublicKey ( ) , genesisPrivateKey ) ; TestHelper . initializeFeeClient ( channel , payerAccountDetails . getAccountId ( ) , genKeyPair , nodeAccount ) ; Transaction transaction = TestHelper . createAccountWithFee ( payerAccountDetails . getAccountId ( ) , nodeAccount , accountKeyPair , 10000l , Collections . singletonList ( genesisPrivateKey ) ) ; TransactionResponse response = stub . createAccount ( transaction ) ; Assert . assertNotNull ( response ) ; Assert . assertEquals ( ResponseCodeEnum . OK , response . getNodeTransactionPrecheckCode ( ) ) ; log . info ( "Pre Check Response of Create first account :: " + response . getNodeTransactionPrecheckCode ( ) . name ( ) ) ; TransactionBody body = TransactionBody . parseFrom ( transaction . getBodyBytes ( ) ) ; AccountID newlyCreateAccountId1 = TestHelper . getTxReceipt ( body . getTransactionID ( ) , stub ) . getAccountID ( ) ; Assert . assertNotNull ( newlyCreateAccountId1 ) ; log . info ( "Account ID " + newlyCreateAccountId1 . getAccountNum ( ) + " created successfully." ) ; log . info ( "--------------------------------------" ) ; Transaction transfer = TestHelper . createTransferSigMap ( payerAccountDetails . getAccountId ( ) , genKeyPair , newlyCreateAccountId1 , payerAccountDetails . getAccountId ( ) , genKeyPair , nodeAccount , 1000l ) ; log . info ( "Transferring 1000 coin from Genesis account to Newly created account...." ) ; TransactionResponse transferRes = stub . cryptoTransfer ( transfer ) ; Assert . assertNotNull ( transferRes ) ; Assert . assertEquals ( ResponseCodeEnum . OK , transferRes . getNodeTransactionPrecheckCode ( ) ) ; log . info ( "Pre Check Response transfer :: " + transferRes . getNodeTransactionPrecheckCode ( ) . name ( ) ) ; TransactionBody transferBody = TransactionBody . parseFrom ( transaction . getBodyBytes ( ) ) ; TransactionReceipt txReceipt = TestHelper . getTxReceipt ( transferBody . getTransactionID ( ) , stub ) ; Assert . assertNotNull ( txReceipt ) ; log . info ( "Transfer from Genesis to newly created account completed...." ) ; log . info ( "-----------------------------------------" ) ; }
public void createAccountsAndTransfer ( CryptoServiceGrpc . CryptoServiceBlockingStub stub ) throws Exception { Path startUpAccountPathJson = getStartupPath ( ) ; String path = startUpAccountPathJson . toString ( ) ; hederaStartupAccount = getStartupAccountMap ( path ) ; AccountKeyListObj payerAccountDetails = getPayerAccount ( hederaStartupAccount ) ; AccountID nodeAccount = AccountID . newBuilder ( ) . setAccountNum ( 3 ) . setRealmNum ( 0 ) . setShardNum ( 0 ) . build ( ) ; KeyPair accountKeyPair = new KeyPairGenerator ( ) . generateKeyPair ( ) ; KeyPairObj genKeyPairObj = payerAccountDetails . getKeyPairList ( ) . get ( 0 ) ; PrivateKey genesisPrivateKey = genKeyPairObj . getPrivateKey ( ) ; KeyPair genKeyPair = new KeyPair ( genKeyPairObj . getPublicKey ( ) , genesisPrivateKey ) ; TestHelper . initializeFeeClient ( channel , payerAccountDetails . getAccountId ( ) , genKeyPair , nodeAccount ) ; Transaction transaction = TestHelper . createAccountWithFee ( payerAccountDetails . getAccountId ( ) , nodeAccount , accountKeyPair , 10000l , Collections . singletonList ( genesisPrivateKey ) ) ; TransactionResponse response = stub . createAccount ( transaction ) ; Assert . assertNotNull ( response ) ; Assert . assertEquals ( ResponseCodeEnum . OK , response . getNodeTransactionPrecheckCode ( ) ) ; log . info ( "Pre Check Response of Create first account :: " + response . getNodeTransactionPrecheckCode ( ) . name ( ) ) ; TransactionBody body = TransactionBody . parseFrom ( transaction . getBodyBytes ( ) ) ; AccountID newlyCreateAccountId1 = TestHelper . getTxReceipt ( body . getTransactionID ( ) , stub ) . getAccountID ( ) ; Assert . assertNotNull ( newlyCreateAccountId1 ) ; log . info ( "Account ID " + newlyCreateAccountId1 . getAccountNum ( ) + " created successfully." ) ; log . info ( "--------------------------------------" ) ; Transaction transfer = TestHelper . createTransferSigMap ( payerAccountDetails . getAccountId ( ) , genKeyPair , newlyCreateAccountId1 , payerAccountDetails . getAccountId ( ) , genKeyPair , nodeAccount , 1000l ) ; log . info ( "Transferring 1000 coin from Genesis account to Newly created account...." ) ; TransactionResponse transferRes = stub . cryptoTransfer ( transfer ) ; Assert . assertNotNull ( transferRes ) ; Assert . assertEquals ( ResponseCodeEnum . OK , transferRes . getNodeTransactionPrecheckCode ( ) ) ; log . info ( "Pre Check Response transfer :: " + transferRes . getNodeTransactionPrecheckCode ( ) . name ( ) ) ; TransactionBody transferBody = TransactionBody . parseFrom ( transaction . getBodyBytes ( ) ) ; TransactionReceipt txReceipt = TestHelper . getTxReceipt ( transferBody . getTransactionID ( ) , stub ) ; Assert . assertNotNull ( txReceipt ) ; log . info ( "Transfer from Genesis to newly created account completed...." ) ; log . info ( "-----------------------------------------" ) ; }
public void createAccountsAndTransfer ( CryptoServiceGrpc . CryptoServiceBlockingStub stub ) throws Exception { Path startUpAccountPathJson = getStartupPath ( ) ; String path = startUpAccountPathJson . toString ( ) ; hederaStartupAccount = getStartupAccountMap ( path ) ; AccountKeyListObj payerAccountDetails = getPayerAccount ( hederaStartupAccount ) ; AccountID nodeAccount = AccountID . newBuilder ( ) . setAccountNum ( 3 ) . setRealmNum ( 0 ) . setShardNum ( 0 ) . build ( ) ; KeyPair accountKeyPair = new KeyPairGenerator ( ) . generateKeyPair ( ) ; KeyPairObj genKeyPairObj = payerAccountDetails . getKeyPairList ( ) . get ( 0 ) ; PrivateKey genesisPrivateKey = genKeyPairObj . getPrivateKey ( ) ; KeyPair genKeyPair = new KeyPair ( genKeyPairObj . getPublicKey ( ) , genesisPrivateKey ) ; TestHelper . initializeFeeClient ( channel , payerAccountDetails . getAccountId ( ) , genKeyPair , nodeAccount ) ; Transaction transaction = TestHelper . createAccountWithFee ( payerAccountDetails . getAccountId ( ) , nodeAccount , accountKeyPair , 10000l , Collections . singletonList ( genesisPrivateKey ) ) ; TransactionResponse response = stub . createAccount ( transaction ) ; Assert . assertNotNull ( response ) ; Assert . assertEquals ( ResponseCodeEnum . OK , response . getNodeTransactionPrecheckCode ( ) ) ; log . info ( "Pre Check Response of Create first account :: " + response . getNodeTransactionPrecheckCode ( ) . name ( ) ) ; TransactionBody body = TransactionBody . parseFrom ( transaction . getBodyBytes ( ) ) ; AccountID newlyCreateAccountId1 = TestHelper . getTxReceipt ( body . getTransactionID ( ) , stub ) . getAccountID ( ) ; Assert . assertNotNull ( newlyCreateAccountId1 ) ; log . info ( "Account ID " + newlyCreateAccountId1 . getAccountNum ( ) + " created successfully." ) ; log . info ( "--------------------------------------" ) ; Transaction transfer = TestHelper . createTransferSigMap ( payerAccountDetails . getAccountId ( ) , genKeyPair , newlyCreateAccountId1 , payerAccountDetails . getAccountId ( ) , genKeyPair , nodeAccount , 1000l ) ; log . info ( "Transferring 1000 coin from Genesis account to Newly created account...." ) ; TransactionResponse transferRes = stub . cryptoTransfer ( transfer ) ; Assert . assertNotNull ( transferRes ) ; Assert . assertEquals ( ResponseCodeEnum . OK , transferRes . getNodeTransactionPrecheckCode ( ) ) ; log . info ( "Pre Check Response transfer :: " + transferRes . getNodeTransactionPrecheckCode ( ) . name ( ) ) ; TransactionBody transferBody = TransactionBody . parseFrom ( transaction . getBodyBytes ( ) ) ; TransactionReceipt txReceipt = TestHelper . getTxReceipt ( transferBody . getTransactionID ( ) , stub ) ; Assert . assertNotNull ( txReceipt ) ; log . info ( "Transfer from Genesis to newly created account completed...." ) ; log . info ( "-----------------------------------------" ) ; }
public void createAccountsAndTransfer ( CryptoServiceGrpc . CryptoServiceBlockingStub stub ) throws Exception { Path startUpAccountPathJson = getStartupPath ( ) ; String path = startUpAccountPathJson . toString ( ) ; hederaStartupAccount = getStartupAccountMap ( path ) ; AccountKeyListObj payerAccountDetails = getPayerAccount ( hederaStartupAccount ) ; AccountID nodeAccount = AccountID . newBuilder ( ) . setAccountNum ( 3 ) . setRealmNum ( 0 ) . setShardNum ( 0 ) . build ( ) ; KeyPair accountKeyPair = new KeyPairGenerator ( ) . generateKeyPair ( ) ; KeyPairObj genKeyPairObj = payerAccountDetails . getKeyPairList ( ) . get ( 0 ) ; PrivateKey genesisPrivateKey = genKeyPairObj . getPrivateKey ( ) ; KeyPair genKeyPair = new KeyPair ( genKeyPairObj . getPublicKey ( ) , genesisPrivateKey ) ; TestHelper . initializeFeeClient ( channel , payerAccountDetails . getAccountId ( ) , genKeyPair , nodeAccount ) ; Transaction transaction = TestHelper . createAccountWithFee ( payerAccountDetails . getAccountId ( ) , nodeAccount , accountKeyPair , 10000l , Collections . singletonList ( genesisPrivateKey ) ) ; TransactionResponse response = stub . createAccount ( transaction ) ; Assert . assertNotNull ( response ) ; Assert . assertEquals ( ResponseCodeEnum . OK , response . getNodeTransactionPrecheckCode ( ) ) ; log . info ( "Pre Check Response of Create first account :: " + response . getNodeTransactionPrecheckCode ( ) . name ( ) ) ; TransactionBody body = TransactionBody . parseFrom ( transaction . getBodyBytes ( ) ) ; AccountID newlyCreateAccountId1 = TestHelper . getTxReceipt ( body . getTransactionID ( ) , stub ) . getAccountID ( ) ; Assert . assertNotNull ( newlyCreateAccountId1 ) ; log . info ( "Account ID " + newlyCreateAccountId1 . getAccountNum ( ) + " created successfully." ) ; log . info ( "--------------------------------------" ) ; Transaction transfer = TestHelper . createTransferSigMap ( payerAccountDetails . getAccountId ( ) , genKeyPair , newlyCreateAccountId1 , payerAccountDetails . getAccountId ( ) , genKeyPair , nodeAccount , 1000l ) ; log . info ( "Transferring 1000 coin from Genesis account to Newly created account...." ) ; TransactionResponse transferRes = stub . cryptoTransfer ( transfer ) ; Assert . assertNotNull ( transferRes ) ; Assert . assertEquals ( ResponseCodeEnum . OK , transferRes . getNodeTransactionPrecheckCode ( ) ) ; log . info ( "Pre Check Response transfer :: " + transferRes . getNodeTransactionPrecheckCode ( ) . name ( ) ) ; TransactionBody transferBody = TransactionBody . parseFrom ( transaction . getBodyBytes ( ) ) ; TransactionReceipt txReceipt = TestHelper . getTxReceipt ( transferBody . getTransactionID ( ) , stub ) ; Assert . assertNotNull ( txReceipt ) ; log . info ( "Transfer from Genesis to newly created account completed...." ) ; log . info ( "-----------------------------------------" ) ; }
public void createAccountsAndTransfer ( CryptoServiceGrpc . CryptoServiceBlockingStub stub ) throws Exception { Path startUpAccountPathJson = getStartupPath ( ) ; String path = startUpAccountPathJson . toString ( ) ; hederaStartupAccount = getStartupAccountMap ( path ) ; AccountKeyListObj payerAccountDetails = getPayerAccount ( hederaStartupAccount ) ; AccountID nodeAccount = AccountID . newBuilder ( ) . setAccountNum ( 3 ) . setRealmNum ( 0 ) . setShardNum ( 0 ) . build ( ) ; KeyPair accountKeyPair = new KeyPairGenerator ( ) . generateKeyPair ( ) ; KeyPairObj genKeyPairObj = payerAccountDetails . getKeyPairList ( ) . get ( 0 ) ; PrivateKey genesisPrivateKey = genKeyPairObj . getPrivateKey ( ) ; KeyPair genKeyPair = new KeyPair ( genKeyPairObj . getPublicKey ( ) , genesisPrivateKey ) ; TestHelper . initializeFeeClient ( channel , payerAccountDetails . getAccountId ( ) , genKeyPair , nodeAccount ) ; Transaction transaction = TestHelper . createAccountWithFee ( payerAccountDetails . getAccountId ( ) , nodeAccount , accountKeyPair , 10000l , Collections . singletonList ( genesisPrivateKey ) ) ; TransactionResponse response = stub . createAccount ( transaction ) ; Assert . assertNotNull ( response ) ; Assert . assertEquals ( ResponseCodeEnum . OK , response . getNodeTransactionPrecheckCode ( ) ) ; log . info ( "Pre Check Response of Create first account :: " + response . getNodeTransactionPrecheckCode ( ) . name ( ) ) ; TransactionBody body = TransactionBody . parseFrom ( transaction . getBodyBytes ( ) ) ; AccountID newlyCreateAccountId1 = TestHelper . getTxReceipt ( body . getTransactionID ( ) , stub ) . getAccountID ( ) ; Assert . assertNotNull ( newlyCreateAccountId1 ) ; log . info ( "Account ID " + newlyCreateAccountId1 . getAccountNum ( ) + " created successfully." ) ; log . info ( "--------------------------------------" ) ; Transaction transfer = TestHelper . createTransferSigMap ( payerAccountDetails . getAccountId ( ) , genKeyPair , newlyCreateAccountId1 , payerAccountDetails . getAccountId ( ) , genKeyPair , nodeAccount , 1000l ) ; log . info ( "Transferring 1000 coin from Genesis account to Newly created account...." ) ; TransactionResponse transferRes = stub . cryptoTransfer ( transfer ) ; Assert . assertNotNull ( transferRes ) ; Assert . assertEquals ( ResponseCodeEnum . OK , transferRes . getNodeTransactionPrecheckCode ( ) ) ; log . info ( "Pre Check Response transfer :: " + transferRes . getNodeTransactionPrecheckCode ( ) . name ( ) ) ; TransactionBody transferBody = TransactionBody . parseFrom ( transaction . getBodyBytes ( ) ) ; TransactionReceipt txReceipt = TestHelper . getTxReceipt ( transferBody . getTransactionID ( ) , stub ) ; Assert . assertNotNull ( txReceipt ) ; log . info ( "Transfer from Genesis to newly created account completed...." ) ; log . info ( "-----------------------------------------" ) ; }
public void createAccountsAndTransfer ( CryptoServiceGrpc . CryptoServiceBlockingStub stub ) throws Exception { Path startUpAccountPathJson = getStartupPath ( ) ; String path = startUpAccountPathJson . toString ( ) ; hederaStartupAccount = getStartupAccountMap ( path ) ; AccountKeyListObj payerAccountDetails = getPayerAccount ( hederaStartupAccount ) ; AccountID nodeAccount = AccountID . newBuilder ( ) . setAccountNum ( 3 ) . setRealmNum ( 0 ) . setShardNum ( 0 ) . build ( ) ; KeyPair accountKeyPair = new KeyPairGenerator ( ) . generateKeyPair ( ) ; KeyPairObj genKeyPairObj = payerAccountDetails . getKeyPairList ( ) . get ( 0 ) ; PrivateKey genesisPrivateKey = genKeyPairObj . getPrivateKey ( ) ; KeyPair genKeyPair = new KeyPair ( genKeyPairObj . getPublicKey ( ) , genesisPrivateKey ) ; TestHelper . initializeFeeClient ( channel , payerAccountDetails . getAccountId ( ) , genKeyPair , nodeAccount ) ; Transaction transaction = TestHelper . createAccountWithFee ( payerAccountDetails . getAccountId ( ) , nodeAccount , accountKeyPair , 10000l , Collections . singletonList ( genesisPrivateKey ) ) ; TransactionResponse response = stub . createAccount ( transaction ) ; Assert . assertNotNull ( response ) ; Assert . assertEquals ( ResponseCodeEnum . OK , response . getNodeTransactionPrecheckCode ( ) ) ; log . info ( "Pre Check Response of Create first account :: " + response . getNodeTransactionPrecheckCode ( ) . name ( ) ) ; TransactionBody body = TransactionBody . parseFrom ( transaction . getBodyBytes ( ) ) ; AccountID newlyCreateAccountId1 = TestHelper . getTxReceipt ( body . getTransactionID ( ) , stub ) . getAccountID ( ) ; Assert . assertNotNull ( newlyCreateAccountId1 ) ; log . info ( "Account ID " + newlyCreateAccountId1 . getAccountNum ( ) + " created successfully." ) ; log . info ( "--------------------------------------" ) ; Transaction transfer = TestHelper . createTransferSigMap ( payerAccountDetails . getAccountId ( ) , genKeyPair , newlyCreateAccountId1 , payerAccountDetails . getAccountId ( ) , genKeyPair , nodeAccount , 1000l ) ; log . info ( "Transferring 1000 coin from Genesis account to Newly created account...." ) ; TransactionResponse transferRes = stub . cryptoTransfer ( transfer ) ; Assert . assertNotNull ( transferRes ) ; Assert . assertEquals ( ResponseCodeEnum . OK , transferRes . getNodeTransactionPrecheckCode ( ) ) ; log . info ( "Pre Check Response transfer :: " + transferRes . getNodeTransactionPrecheckCode ( ) . name ( ) ) ; TransactionBody transferBody = TransactionBody . parseFrom ( transaction . getBodyBytes ( ) ) ; TransactionReceipt txReceipt = TestHelper . getTxReceipt ( transferBody . getTransactionID ( ) , stub ) ; Assert . assertNotNull ( txReceipt ) ; log . info ( "Transfer from Genesis to newly created account completed...." ) ; log . info ( "-----------------------------------------" ) ; }
public void test() { if ( testCase . getLogicalOperator ( ) != SHACL . LogicalConstraint . atomic && this . getExecutionType ( ) != TestCaseExecutionType . shaclLiteTestCaseResult && this . getExecutionType ( ) != TestCaseExecutionType . shaclTestCaseResult ) { log . warn ( "Logical constraints evaluation is not supported by the current execution type: " + this . getExecutionType ( ) ) ; return Collections . emptyList ( ) ; } else { return testCase . evaluateInternalResults ( internalResults ) ; } }
public void test() { try { conn = this . getConnection ( ) ; stat = conn . prepareStatement ( LOAD_BPMWIDGETINFOS_ID ) ; res = stat . executeQuery ( ) ; code_block = WhileStatement ; } catch ( Throwable t ) { _logger . error ( "Error loading BpmWidgetInfo list" , t ) ; throw new RuntimeException ( "Error loading BpmWidgetInfo list" , t ) ; } finally { closeDaoResources ( res , stat , conn ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( AclServiceException e ) { logger . warn ( "Error deleting manged acl with id '{}': {}" , aclId , e ) ; throw new WebApplicationException ( Response . Status . INTERNAL_SERVER_ERROR ) ; } }
@ Override public synchronized void start ( ) { code_block = ForStatement ; _isStarted = true ; LOGGER . info ( "InstanceDataManager is started! " + getServerInfo ( ) ) ; }
public void test() { if ( task == null ) { logger . error ( "Attempting to insert empty task" ) ; return - 1L ; } }
public void test() { try { DocumentDTO doc = ( DocumentDTO ) task ; CollectionDTO crisisDTO = remoteCrisisEJB . findCrisisByID ( crisisID ) ; doc . setCrisisDTO ( crisisDTO ) ; doc . setHasHumanLabels ( false ) ; DocumentDTO savedDoc = remoteDocumentEJB . addDocument ( doc ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Error in saving new document for crisisID : " + crisisID , e ) ; } }
public int send ( @ NotNull RawMercuryRequest request , @ NotNull Callback callback ) throws IOException { ByteArrayOutputStream bytesOut = new ByteArrayOutputStream ( ) ; DataOutputStream out = new DataOutputStream ( bytesOut ) ; int seq ; synchronized ( seqHolder ) code_block = "" ; LOGGER . trace ( "Send Mercury request, seq: {}, uri: {}, method: {}" , seq , request . header . getUri ( ) , request . header . getMethod ( ) ) ; out . writeShort ( ( short ) 4 ) ; out . writeInt ( seq ) ; out . writeByte ( 1 ) ; out . writeShort ( 1 + request . payload . length ) ; byte [ ] headerBytes = request . header . toByteArray ( ) ; out . writeShort ( headerBytes . length ) ; out . write ( headerBytes ) ; code_block = ForStatement ; Packet . Type cmd = Packet . Type . forMethod ( request . header . getMethod ( ) ) ; session . send ( cmd , bytesOut . toByteArray ( ) ) ; callbacks . put ( ( long ) seq , callback ) ; return seq ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "removing item for key: " + key ) ; } }
@ Override public void createOrReplaceFunction ( String schemaName , String functionName , Supplier < String > supplier ) { final String objectName = DataDefinitionUtil . getQualifiedName ( schemaName , functionName ) ; logger . info ( "Create or replace function " + objectName ) ; final StringBuilder ddl = new StringBuilder ( ) . append ( "CREATE OR REPLACE FUNCTION " ) . append ( objectName ) . append ( System . lineSeparator ( ) ) . append ( supplier . get ( ) ) ; final String ddlString = ddl . toString ( ) ; code_block = IfStatement ; runStatement ( ddlString ) ; }
@ Test public void applicationLoggerAboveLevelUnaffectedByLoweringLogLevelChanges ( ) { geodeConsoleAppender . clearLogEvents ( ) ; when ( config . getLogLevel ( ) ) . thenReturn ( FINE . intLevel ( ) ) ; configuration . configChanged ( ) ; applicationLogger . info ( logMessage ) ; assertThat ( geodeConsoleAppender . getLogEvents ( ) ) . hasSize ( 1 ) ; }
public void test() { try { MethodKey methodKey = new MethodKey ( BookmarksEntryServiceUtil . class , "addEntry" , _addEntryParameterTypes0 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , folderId , name , url , description , serviceContext ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . bookmarks . model . BookmarksEntry ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { syncUser = project . getRemoteStatus ( ) . getReadBy ( ) ; } catch ( NullPointerException e ) { logger . error ( "Tried to email a sync user for a non-remote project" , e ) ; } }
public void test() { try { final MimeMessage mimeMessage = this . javaMailSender . createMimeMessage ( ) ; final MimeMessageHelper message = new MimeMessageHelper ( mimeMessage , "UTF-8" ) ; message . setSubject ( messageSource . getMessage ( "email.syncexpired.subject" , new Object [ ] code_block = "" ; , locale ) ) ; message . setFrom ( serverEmail ) ; message . setTo ( syncUser . getEmail ( ) ) ; final String htmlContent = templateEngine . process ( SYNC_EXPIRED_TEMPLATE , ctx ) ; message . setText ( htmlContent , true ) ; javaMailSender . send ( mimeMessage ) ; message . setText ( htmlContent , true ) ; javaMailSender . send ( mimeMessage ) ; } catch ( Exception e ) { logger . error ( "Error trying to send sync failed email." , e ) ; throw new MailSendException ( "Failed to send e-mail for sync failure." , e ) ; } }
public String createMessage ( Command c ) { String message = null ; String page = null ; String behavior = null ; String relay = null ; code_block = IfStatement ; code_block = IfStatement ; message = "GET /" + page + " HTTP 1.1\r\n\r\n" ; Freedomotic . logger . info ( "Sending 'GET /" + page + " HTTP 1.1' to relay board" ) ; return ( message ) ; }
public CloudWithRelaysResponseDTO getCloudsWithExclusiveGatewayAndPublicRelays ( final String operator , final String name ) { logger . debug ( "getCloudsWithGatewayAndPublicRelays started..." ) ; Assert . isTrue ( ! Utilities . isEmpty ( operator ) , "operator is null or empty" ) ; Assert . isTrue ( ! Utilities . isEmpty ( name ) , "name is null or empty" ) ; final UriComponents uri = getGatekeeperGetCloudUri ( operator , name ) ; final ResponseEntity < CloudWithRelaysResponseDTO > response = httpService . sendRequest ( uri , HttpMethod . GET , CloudWithRelaysResponseDTO . class ) ; return response . getBody ( ) ; }
public void test() { try { temp = fileIOOperations . fileRead ( fd , b , off , len ) ; } catch ( JargonException e ) { log . error ( "JargonException in read is converted to IOException for method contract" , e ) ; throw new IOException ( e ) ; } }
protected void addContribution ( RepositoryDescriptor cdesc ) { log . warn ( "Using old-style extension point" + " org.nuxeo.ecm.core.repository.RepositoryService" + " for repository \"" + cdesc . name + "\", use org.nuxeo.ecm.core.storage.sql.RepositoryService instead" ) ; RepositoryDescriptor descriptor = getRepositoryDescriptor ( cdesc ) ; SQLRepositoryService sqlRepositoryService = Framework . getLocalService ( SQLRepositoryService . class ) ; sqlRepositoryService . addContribution ( descriptor ) ; }
public void test() { if ( dagNode == null ) { LOG . warn ( "jobStartedNotification - unrecorgnized operator name found to " + "jobId " + jobIDStr ) ; return ; } }
public void test() { try { boolean isUpdated = updateJobState ( ) ; code_block = IfStatement ; } catch ( IOException e ) { LOG . error ( "Error getting job info!" , e ) ; } }
public void test() { if ( ! knownAliases . containsKey ( alias ) ) { log . warn ( "Attempted to unregister unknown alias: {}" , alias ) ; return ; } }
public void test() { if ( registeredCommands . containsKey ( alias ) ) { log . warn ( "Attempted to unregister alias {}, but it is the base command name." , alias ) ; return ; } }
public void test() { try { code_block = IfStatement ; activityFieldStore . flush ( oldBatch . getUpdates ( ) ) ; } catch ( Exception ex ) { logger . error ( "Failure to store the field values to file" + oldBatch . getUpdates ( ) , ex ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Set parameter " + name + " = " + value ) ; } }
public void test() { try { pd = getProjectDescriptor ( project ) ; } catch ( Exception e ) { log . warn ( "Cannot get project descriptor for project '{}'. Physical project name will be used. Cause: {}" , project . getName ( ) , e . getMessage ( ) , e ) ; } }
private void loadGraph ( String name , String path ) { final Graph graph = GraphFactory . open ( path ) ; this . graphs . put ( name , graph ) ; LOG . info ( "Graph '{}' was successfully configured via '{}'" , name , path ) ; code_block = IfStatement ; }
public void test() { if ( this . requireAuthentication ( ) && ! ( graph instanceof HugeGraphAuthProxy ) ) { LOG . warn ( "You may need to support access control to '{}' with {}" , path , HugeFactoryAuthProxy . GRAPH_FACTORY ) ; } }
public void test() { if ( checkResultIsEmpty ( result ) ) { initApp ( id , appName ) ; } else { logger . info ( this , "AppHubInit CHECK APP done :uavapp_" + appName + " EXIST" ) ; } }
public void test() { try { LOG . trace ( "New Histogram:\n" + tmp_userIdHistogram ) ; } catch ( NullPointerException ex ) { code_block = ForStatement ; throw ex ; } }
public void test() { if ( ! _suppressTruncateTable ) { truncateTableIfPossible ( conn , tableMetaList ) ; } else { _log . info ( "*Suppress truncating tables" ) ; } }
public void test() { if ( ! _suppressDropForeignKey ) { dropForeignKey ( conn , tableMetaList ) ; } else { _log . info ( "*Suppress dropping foreign keys" ) ; } }
public void test() { if ( ! _suppressDropTable ) { dropTable ( conn , tableMetaList ) ; } else { _log . info ( "*Suppress dropping tables" ) ; } }
public void test() { if ( ! _suppressDropSequence ) { dropSequence ( conn , tableMetaList ) ; } else { _log . info ( "*Suppress dropping sequences" ) ; } }
@ Override @ NonNullByDefault ( { } ) public void handleStandaloneElement ( final @ Nullable String elementName , final @ Nullable Map < String , String > attributes , final boolean minimized , final int line , final int col ) throws ParseException code_block = "" ;
protected final Connection allocateConnection ( ConnectionRequestInfo connRequestInfo ) throws ResourceException { logger . debug ( "allocateConnection(connRequestInfo)..." ) ; Connection connection = ( Connection ) this . connectionManager . allocateConnection ( this . managedConnectionFactory , connRequestInfo ) ; return connection ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( "A JSON web service action is already registered at " + path ) ; } }
public void test() { if ( _log . isWarnEnabled ( ) ) { StringBundler sb = new StringBundler ( 14 ) ; sb . append ( "Unable to register service method {actionClass=" ) ; sb . append ( actionClass ) ; sb . append ( ", actionMethod=" ) ; sb . append ( actionMethod ) ; sb . append ( ", contextName=" ) ; sb . append ( contextName ) ; sb . append ( ", contextPath=" ) ; sb . append ( contextPath ) ; sb . append ( ", method=" ) ; sb . append ( method ) ; sb . append ( ", path=" ) ; sb . append ( path ) ; sb . append ( "} due to " ) ; sb . append ( exception . getMessage ( ) ) ; _log . warn ( sb . toString ( ) ) ; } }
@ Override public XDRAcknowledgementType provideAndRegisterDocumentSetBAsyncResponse ( RegistryResponseType request , AssertionType assertion , NhinTargetCommunitiesType targets ) { LOG . trace ( "Using NoOp Implementation for Entity Doc Submission Deferred Response Service" ) ; XDRAcknowledgementType ack = new XDRAcknowledgementType ( ) ; RegistryResponseType regResp = new RegistryResponseType ( ) ; regResp . setStatus ( NhincConstants . XDR_RESP_ACK_STATUS_MSG ) ; ack . setMessage ( regResp ) ; return ack ; }
@ Override public void onSuccess ( TbQueueMsgMetadata metadata ) { log . debug ( "Successfully send ENTITY_CREATED EVENT to rule engine [{}]" , device ) ; }
public void test() { if ( resource == null ) { _log . warn ( "resource not found: " + resourceId ) ; return null ; } }
public void test() { if ( isReservedWord ( name ) ) { LOGGER . warn ( name + " (reserved word) cannot be used as model name. Renamed to " + camelize ( "model_" + name ) ) ; name = "model_" + name ; } }
public void test() { if ( name . matches ( "^\\d.*" ) ) { LOGGER . warn ( name + " (model name starts with number) cannot be used as model name. Renamed to " + camelize ( "model_" + name ) ) ; name = "model_" + name ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( sb . toString ( ) ) ; } }
public void test() { if ( senseHatInterfaceRefCnt . getAndIncrement ( ) == 0 ) { logger . info ( "Opening Sense Hat..." ) ; senseHatInterface = new SenseHatInterface ( senseHat ) ; logger . info ( "Opening Sense Hat...done" ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { ScmConfig scmConfig = conf . getObject ( ScmConfig . class ) ; LOG . debug ( "Ozone security is enabled. Attempting login for SCM user. " + "Principal: {}, keytab: {}" , scmConfig . getKerberosPrincipal ( ) , scmConfig . getKerberosKeytab ( ) ) ; } }
public void test() { try { Collection < SupervisionEvent > allCurrentEvents = clientRequestHandler . getCurrentSupervisionStatus ( ) ; code_block = ForStatement ; c2monConnectionEstablished = true ; log . info ( "refreshSupervisionStatus() - supervision event cache was successfully updated with " + allCurrentEvents . size ( ) + " events." ) ; } catch ( Exception e ) { log . error ( "refreshSupervisionStatus() - Could not initialize/update the supervision event cache. Reason: " + e . getMessage ( ) , e ) ; c2monConnectionEstablished = false ; } }
public void test() { try { writeJSON ( request , response , jsonObject ) ; } catch ( IOException e ) { log . error ( "Problem rendering RequestStatus" , e ) ; } }
private void layoutAcyclicParts ( ) throws CDKException { logger . debug ( "Start of handleAliphatics" ) ; int safetyCounter = 0 ; IAtomContainer unplacedAtoms = null ; IAtomContainer placedAtoms = null ; IAtomContainer longestUnplacedChain = null ; IAtom atom = null ; Vector2d direction = null ; Vector2d startVector = null ; boolean done ; do code_block = "" ; while ( ! done && safetyCounter <= molecule . getAtomCount ( ) ) ; logger . debug ( "End of handleAliphatics" ) ; }
public void test() { if ( atom != null ) { unplacedAtoms = getUnplacedAtoms ( atom ) ; placedAtoms = getPlacedAtoms ( atom ) ; longestUnplacedChain = atomPlacer . getLongestUnplacedChain ( molecule , atom ) ; logger . debug ( "---start of longest unplaced chain---" ) ; code_block = TryStatement ;  logger . debug ( "---end of longest unplaced chain---" ) ; code_block = IfStatement ; } else { done = true ; } }
public void test() { try { logger . debug ( "Start at atom no. " + ( molecule . indexOf ( atom ) + 1 ) ) ; logger . debug ( AtomPlacer . listNumbers ( molecule , longestUnplacedChain ) ) ; } catch ( Exception exc ) { logger . debug ( exc ) ; } }
public void test() { try { logger . debug ( "Start at atom no. " + ( molecule . indexOf ( atom ) + 1 ) ) ; logger . debug ( AtomPlacer . listNumbers ( molecule , longestUnplacedChain ) ) ; } catch ( Exception exc ) { logger . debug ( exc ) ; } }
public void test() { try { logger . debug ( "Start at atom no. " + ( molecule . indexOf ( atom ) + 1 ) ) ; logger . debug ( AtomPlacer . listNumbers ( molecule , longestUnplacedChain ) ) ; } catch ( Exception exc ) { logger . debug ( exc ) ; } }
public void test() { if ( atom != null ) { unplacedAtoms = getUnplacedAtoms ( atom ) ; placedAtoms = getPlacedAtoms ( atom ) ; longestUnplacedChain = atomPlacer . getLongestUnplacedChain ( molecule , atom ) ; logger . debug ( "---start of longest unplaced chain---" ) ; code_block = TryStatement ;  logger . debug ( "---end of longest unplaced chain---" ) ; code_block = IfStatement ; } else { done = true ; } }
public void test() { if ( placedAtoms . getAtomCount ( ) > 1 ) { logger . debug ( "More than one atoms placed already" ) ; logger . debug ( "trying to place neighbors of atom " + ( molecule . indexOf ( atom ) + 1 ) ) ; atomPlacer . distributePartners ( atom , placedAtoms , GeometryUtil . get2DCenter ( placedAtoms ) , unplacedAtoms , bondLength ) ; direction = new Vector2d ( longestUnplacedChain . getAtom ( 1 ) . getPoint2d ( ) ) ; startVector = new Vector2d ( atom . getPoint2d ( ) ) ; direction . sub ( startVector ) ; logger . debug ( "Done placing neighbors of atom " + ( molecule . indexOf ( atom ) + 1 ) ) ; } else { logger . debug ( "Less than or equal one atoms placed already" ) ; logger . debug ( "Trying to get next bond vector." ) ; direction = atomPlacer . getNextBondVector ( atom , placedAtoms . getAtom ( 0 ) , GeometryUtil . get2DCenter ( molecule ) , true ) ; } }
public void test() { if ( placedAtoms . getAtomCount ( ) > 1 ) { logger . debug ( "More than one atoms placed already" ) ; logger . debug ( "trying to place neighbors of atom " + ( molecule . indexOf ( atom ) + 1 ) ) ; atomPlacer . distributePartners ( atom , placedAtoms , GeometryUtil . get2DCenter ( placedAtoms ) , unplacedAtoms , bondLength ) ; direction = new Vector2d ( longestUnplacedChain . getAtom ( 1 ) . getPoint2d ( ) ) ; startVector = new Vector2d ( atom . getPoint2d ( ) ) ; direction . sub ( startVector ) ; logger . debug ( "Done placing neighbors of atom " + ( molecule . indexOf ( atom ) + 1 ) ) ; } else { logger . debug ( "Less than or equal one atoms placed already" ) ; logger . debug ( "Trying to get next bond vector." ) ; direction = atomPlacer . getNextBondVector ( atom , placedAtoms . getAtom ( 0 ) , GeometryUtil . get2DCenter ( molecule ) , true ) ; } }
public void test() { if ( placedAtoms . getAtomCount ( ) > 1 ) { logger . debug ( "More than one atoms placed already" ) ; logger . debug ( "trying to place neighbors of atom " + ( molecule . indexOf ( atom ) + 1 ) ) ; atomPlacer . distributePartners ( atom , placedAtoms , GeometryUtil . get2DCenter ( placedAtoms ) , unplacedAtoms , bondLength ) ; direction = new Vector2d ( longestUnplacedChain . getAtom ( 1 ) . getPoint2d ( ) ) ; startVector = new Vector2d ( atom . getPoint2d ( ) ) ; direction . sub ( startVector ) ; logger . debug ( "Done placing neighbors of atom " + ( molecule . indexOf ( atom ) + 1 ) ) ; } else { logger . debug ( "Less than or equal one atoms placed already" ) ; logger . debug ( "Trying to get next bond vector." ) ; direction = atomPlacer . getNextBondVector ( atom , placedAtoms . getAtom ( 0 ) , GeometryUtil . get2DCenter ( molecule ) , true ) ; } }
public void test() { if ( placedAtoms . getAtomCount ( ) > 1 ) { logger . debug ( "More than one atoms placed already" ) ; logger . debug ( "trying to place neighbors of atom " + ( molecule . indexOf ( atom ) + 1 ) ) ; atomPlacer . distributePartners ( atom , placedAtoms , GeometryUtil . get2DCenter ( placedAtoms ) , unplacedAtoms , bondLength ) ; direction = new Vector2d ( longestUnplacedChain . getAtom ( 1 ) . getPoint2d ( ) ) ; startVector = new Vector2d ( atom . getPoint2d ( ) ) ; direction . sub ( startVector ) ; logger . debug ( "Done placing neighbors of atom " + ( molecule . indexOf ( atom ) + 1 ) ) ; } else { logger . debug ( "Less than or equal one atoms placed already" ) ; logger . debug ( "Trying to get next bond vector." ) ; direction = atomPlacer . getNextBondVector ( atom , placedAtoms . getAtom ( 0 ) , GeometryUtil . get2DCenter ( molecule ) , true ) ; } }
private void layoutAcyclicParts ( ) throws CDKException { logger . debug ( "Start of handleAliphatics" ) ; int safetyCounter = 0 ; IAtomContainer unplacedAtoms = null ; IAtomContainer placedAtoms = null ; IAtomContainer longestUnplacedChain = null ; IAtom atom = null ; Vector2d direction = null ; Vector2d startVector = null ; boolean done ; do code_block = "" ; while ( ! done && safetyCounter <= molecule . getAtomCount ( ) ) ; logger . debug ( "End of handleAliphatics" ) ; }
public void test() { try { AccountManager user = new AccountManager ( connection ) ; user . createAccount ( userName , password ) ; LOGGER . debug ( "XMPP user created" ) ; connection . login ( userName , password ) ; } catch ( XMPPException e ) { LOGGER . debug ( "XMPP user existing" ) ; connection . login ( userName , password ) ; } }
public void test() { try { AccountManager user = new AccountManager ( connection ) ; user . createAccount ( userName , password ) ; LOGGER . debug ( "XMPP user created" ) ; connection . login ( userName , password ) ; } catch ( XMPPException e ) { LOGGER . debug ( "XMPP user existing" ) ; connection . login ( userName , password ) ; } }
public void test() { if ( sessionIdObject == null ) { final String reason = "sid parameter in request is not valid. Logout is rejected. sid parameter in request can be skipped or otherwise valid value must be provided." ; log . error ( reason ) ; throw new WebApplicationException ( createErrorResponse ( postLogoutRedirectUri , EndSessionErrorResponseType . INVALID_GRANT_AND_SESSION , reason ) ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Executing member started extension" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Member started script returned:" + output ) ; } }
public void test() { if ( log . isErrorEnabled ( ) ) { log . error ( "Could not execute member started extension" , e ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { final Set < String > names = properties . keySet ( ) ; LOG . debug ( "Found endpoint properties {}" , names . retainAll ( ENDPOINT_CONFIG_FIELDS ) ) ; } }
public TMTextUnitVariant addTextUnitVariant ( long assetId , long localeId , String name , String content , String comment ) throws VirtualAssetRequiredException , VirutalAssetMissingTextUnitException { logger . debug ( "Add text unit variant to virtual assetId: {}, with name: {}" , assetId , name ) ; TextUnitSearcherParameters textUnitSearcherParameters = new TextUnitSearcherParameters ( ) ; textUnitSearcherParameters . setAssetId ( assetId ) ; textUnitSearcherParameters . setName ( name ) ; textUnitSearcherParameters . setSearchType ( SearchType . EXACT ) ; textUnitSearcherParameters . setUsedFilter ( UsedFilter . USED ) ; textUnitSearcherParameters . setLimit ( 1 ) ; List < TextUnitDTO > textUnitDTOs = textUnitSearcher . search ( textUnitSearcherParameters ) ; code_block = IfStatement ; code_block = IfStatement ; return tmService . addCurrentTMTextUnitVariant ( textUnitDTOs . get ( 0 ) . getTmTextUnitId ( ) , localeId , content ) ; }
public void test() { if ( textUnitDTOs . isEmpty ( ) ) { String msg = MessageFormat . format ( "Missing TmTextUnit for assetId: {0} and name: {1}" , assetId , name ) ; logger . debug ( msg ) ; throw new VirutalAssetMissingTextUnitException ( msg ) ; } }
@ GET @ Timed @ Path ( "{id}" ) @ Produces ( APPLICATION_JSON_WITH_CHARSET ) public String get ( @ Context GraphManager manager , @ PathParam ( "graph" ) String graph , @ PathParam ( "id" ) String id ) { LOG . debug ( "Graph [{}] get user: {}" , graph , id ) ; HugeGraph g = graph ( manager , graph ) ; HugeUser user = manager . authManager ( ) . getUser ( IdGenerator . of ( id ) ) ; return manager . serializer ( g ) . writeAuthElement ( user ) ; }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( cf + " " + endCf ) ; } }
public void testRelevanceHashMapInt2StringArrayWay ( ) throws Exception { logger . info ( "executing test case testRelevanceHashMapInt2StringArrayWay" ) ; String req = "{\"sort\":[\"_score\"],\"query\":{\"query_string\":{\"query\":\"\",\"relevance\":{\"model\":{\"function_params\":[\"_INNER_SCORE\",\"thisYear\",\"year\",\"goodYear\",\"mileageWeight\",\"mileage\",\"color\",\"yearcolor\",\"colorweight\",\"category\",\"categorycolor\"],\"facets\":{\"int\":[\"year\",\"mileage\"],\"string\":[\"color\",\"category\"],\"long\":[\"groupid\"]},\"function\":\"if(yearcolor.containsKey(year) && yearcolor.get(year).equals(color)) return 100000f; if(goodYear.contains(year)) return (float)Math.exp(2d);   if(year==thisYear) return 87f   ; return  _INNER_SCORE;\",\"variables\":{\"map_int_float\":[\"mileageWeight\"],\"map_int_string\":[\"yearcolor\"],\"set_int\":[\"goodYear\"],\"int\":[\"thisYear\"],\"map_string_float\":[\"colorweight\"],\"map_string_string\":[\"categorycolor\"]}},\"values\":{\"thisYear\":2001,\"yearcolor\":{\"value\":[\"red\"],\"key\":[1998]},\"mileageWeight\":{\"value\":[777.9,10.2],\"key\":[11400,11000]},\"colorweight\":{\"value\":[335.5],\"key\":[\"red\"]},\"goodYear\":[1996,1997],\"categorycolor\":{\"value\":[\"red\"],\"key\":[\"compact\"]}}}}},\"fetchStored\":false,\"from\":0,\"explain\":false,\"size\":6}" ; JSONObject res = search ( new JSONObject ( req ) ) ; assertEquals ( "numhits is wrong" , 15000 , res . getInt ( "numhits" ) ) ; JSONArray hits = res . getJSONArray ( "hits" ) ; JSONObject firstHit = hits . getJSONObject ( 0 ) ; JSONObject secondHit = hits . getJSONObject ( 1 ) ; double firstScore = firstHit . getDouble ( "_score" ) ; double secondScore = secondHit . getDouble ( "_score" ) ; String firstYear = firstHit . getJSONArray ( "year" ) . getString ( 0 ) ; String secondYear = secondHit . getJSONArray ( "year" ) . getString ( 0 ) ; String firstColor = firstHit . getJSONArray ( "color" ) . getString ( 0 ) ; String secondColor = secondHit . getJSONArray ( "color" ) . getString ( 0 ) ; assertEquals ( "inner score for first is not correct." , true , Math . abs ( firstScore - 100000 ) < 1 ) ; assertEquals ( "inner score for second is not correct." , true , Math . abs ( secondScore - 100000 ) < 1 ) ; assertEquals ( "year for first is not correct." , true , Integer . parseInt ( firstYear ) == 1998 ) ; assertEquals ( "year for second is not correct." , true , Integer . parseInt ( secondYear ) == 1998 ) ; assertEquals ( "color for first is not correct." , true , firstColor . equals ( "red" ) ) ; assertEquals ( "color for second is not correct." , true , secondColor . equals ( "red" ) ) ; }
public void test() { if ( configs == null ) { LOG . error ( "Connection ConfigMap is empty" ) ; } else { String knoxUrl = configs . get ( "knox.url" ) ; String knoxAdminUser = configs . get ( "username" ) ; String knoxAdminPassword = configs . get ( "password" ) ; knoxClient = new KnoxClient ( knoxUrl , knoxAdminUser , knoxAdminPassword ) ; } }
public void stop ( ) { logger . info ( "SmscStatProviderJmx Stopping ..." ) ; statsReporter . stop ( ) ; logger . info ( "SmscStatProviderJmx Stopped ..." ) ; }
public void stop ( ) { logger . info ( "SmscStatProviderJmx Stopping ..." ) ; statsReporter . stop ( ) ; logger . info ( "SmscStatProviderJmx Stopped ..." ) ; }
public void test() { try { lh = bkc . createLedger ( digestType , ledgerPassword ) ; ledgerId = lh . getId ( ) ; LOG . info ( "Ledger ID: " + lh . getId ( ) ) ; LedgerHandle lhOpen = bkc . openLedgerNoRecovery ( ledgerId , digestType , ledgerPassword ) ; LOG . debug ( "Checking that it is empty" ) ; long readLastConfirmed = lhOpen . readLastConfirmed ( ) ; assertTrue ( "Last confirmed has the wrong value" , readLastConfirmed == LedgerHandle . INVALID_ENTRY_ID ) ; LOG . debug ( "Going to write one entry" ) ; ByteBuffer entry = ByteBuffer . allocate ( 4 ) ; entry . putInt ( rng . nextInt ( maxInt ) ) ; entry . position ( 0 ) ; entries . add ( entry . array ( ) ) ; entriesSize . add ( entry . array ( ) . length ) ; lh . addEntry ( entry . array ( ) ) ; LOG . debug ( "Checking that it is still empty even after writing one entry" ) ; readLastConfirmed = lhOpen . readLastConfirmed ( ) ; assertTrue ( readLastConfirmed == LedgerHandle . INVALID_ENTRY_ID ) ; entry = ByteBuffer . allocate ( 4 ) ; entry . putInt ( rng . nextInt ( maxInt ) ) ; entry . position ( 0 ) ; entries . add ( entry . array ( ) ) ; entriesSize . add ( entry . array ( ) . length ) ; lh . addEntry ( entry . array ( ) ) ; LOG . info ( "Checking that it has an entry" ) ; readLastConfirmed = lhOpen . readLastConfirmed ( ) ; assertTrue ( readLastConfirmed == 0L ) ; lh . close ( ) ; lhOpen . close ( ) ; } catch ( BKException e ) { LOG . error ( "Test failed" , e ) ; fail ( "Test failed due to BookKeeper exception" ) ; } catch ( InterruptedException e ) { LOG . error ( "Test failed" , e ) ; fail ( "Test failed due to interruption" ) ; } }
public void test() { try { lh = bkc . createLedger ( digestType , ledgerPassword ) ; ledgerId = lh . getId ( ) ; LOG . info ( "Ledger ID: " + lh . getId ( ) ) ; LedgerHandle lhOpen = bkc . openLedgerNoRecovery ( ledgerId , digestType , ledgerPassword ) ; LOG . debug ( "Checking that it is empty" ) ; long readLastConfirmed = lhOpen . readLastConfirmed ( ) ; assertTrue ( "Last confirmed has the wrong value" , readLastConfirmed == LedgerHandle . INVALID_ENTRY_ID ) ; LOG . debug ( "Going to write one entry" ) ; ByteBuffer entry = ByteBuffer . allocate ( 4 ) ; entry . putInt ( rng . nextInt ( maxInt ) ) ; entry . position ( 0 ) ; entries . add ( entry . array ( ) ) ; entriesSize . add ( entry . array ( ) . length ) ; lh . addEntry ( entry . array ( ) ) ; LOG . debug ( "Checking that it is still empty even after writing one entry" ) ; readLastConfirmed = lhOpen . readLastConfirmed ( ) ; assertTrue ( readLastConfirmed == LedgerHandle . INVALID_ENTRY_ID ) ; entry = ByteBuffer . allocate ( 4 ) ; entry . putInt ( rng . nextInt ( maxInt ) ) ; entry . position ( 0 ) ; entries . add ( entry . array ( ) ) ; entriesSize . add ( entry . array ( ) . length ) ; lh . addEntry ( entry . array ( ) ) ; LOG . info ( "Checking that it has an entry" ) ; readLastConfirmed = lhOpen . readLastConfirmed ( ) ; assertTrue ( readLastConfirmed == 0L ) ; lh . close ( ) ; lhOpen . close ( ) ; } catch ( BKException e ) { LOG . error ( "Test failed" , e ) ; fail ( "Test failed due to BookKeeper exception" ) ; } catch ( InterruptedException e ) { LOG . error ( "Test failed" , e ) ; fail ( "Test failed due to interruption" ) ; } }
public void test() { try { lh = bkc . createLedger ( digestType , ledgerPassword ) ; ledgerId = lh . getId ( ) ; LOG . info ( "Ledger ID: " + lh . getId ( ) ) ; LedgerHandle lhOpen = bkc . openLedgerNoRecovery ( ledgerId , digestType , ledgerPassword ) ; LOG . debug ( "Checking that it is empty" ) ; long readLastConfirmed = lhOpen . readLastConfirmed ( ) ; assertTrue ( "Last confirmed has the wrong value" , readLastConfirmed == LedgerHandle . INVALID_ENTRY_ID ) ; LOG . debug ( "Going to write one entry" ) ; ByteBuffer entry = ByteBuffer . allocate ( 4 ) ; entry . putInt ( rng . nextInt ( maxInt ) ) ; entry . position ( 0 ) ; entries . add ( entry . array ( ) ) ; entriesSize . add ( entry . array ( ) . length ) ; lh . addEntry ( entry . array ( ) ) ; LOG . debug ( "Checking that it is still empty even after writing one entry" ) ; readLastConfirmed = lhOpen . readLastConfirmed ( ) ; assertTrue ( readLastConfirmed == LedgerHandle . INVALID_ENTRY_ID ) ; entry = ByteBuffer . allocate ( 4 ) ; entry . putInt ( rng . nextInt ( maxInt ) ) ; entry . position ( 0 ) ; entries . add ( entry . array ( ) ) ; entriesSize . add ( entry . array ( ) . length ) ; lh . addEntry ( entry . array ( ) ) ; LOG . info ( "Checking that it has an entry" ) ; readLastConfirmed = lhOpen . readLastConfirmed ( ) ; assertTrue ( readLastConfirmed == 0L ) ; lh . close ( ) ; lhOpen . close ( ) ; } catch ( BKException e ) { LOG . error ( "Test failed" , e ) ; fail ( "Test failed due to BookKeeper exception" ) ; } catch ( InterruptedException e ) { LOG . error ( "Test failed" , e ) ; fail ( "Test failed due to interruption" ) ; } }
public void test() { try { lh = bkc . createLedger ( digestType , ledgerPassword ) ; ledgerId = lh . getId ( ) ; LOG . info ( "Ledger ID: " + lh . getId ( ) ) ; LedgerHandle lhOpen = bkc . openLedgerNoRecovery ( ledgerId , digestType , ledgerPassword ) ; LOG . debug ( "Checking that it is empty" ) ; long readLastConfirmed = lhOpen . readLastConfirmed ( ) ; assertTrue ( "Last confirmed has the wrong value" , readLastConfirmed == LedgerHandle . INVALID_ENTRY_ID ) ; LOG . debug ( "Going to write one entry" ) ; ByteBuffer entry = ByteBuffer . allocate ( 4 ) ; entry . putInt ( rng . nextInt ( maxInt ) ) ; entry . position ( 0 ) ; entries . add ( entry . array ( ) ) ; entriesSize . add ( entry . array ( ) . length ) ; lh . addEntry ( entry . array ( ) ) ; LOG . debug ( "Checking that it is still empty even after writing one entry" ) ; readLastConfirmed = lhOpen . readLastConfirmed ( ) ; assertTrue ( readLastConfirmed == LedgerHandle . INVALID_ENTRY_ID ) ; entry = ByteBuffer . allocate ( 4 ) ; entry . putInt ( rng . nextInt ( maxInt ) ) ; entry . position ( 0 ) ; entries . add ( entry . array ( ) ) ; entriesSize . add ( entry . array ( ) . length ) ; lh . addEntry ( entry . array ( ) ) ; LOG . info ( "Checking that it has an entry" ) ; readLastConfirmed = lhOpen . readLastConfirmed ( ) ; assertTrue ( readLastConfirmed == 0L ) ; lh . close ( ) ; lhOpen . close ( ) ; } catch ( BKException e ) { LOG . error ( "Test failed" , e ) ; fail ( "Test failed due to BookKeeper exception" ) ; } catch ( InterruptedException e ) { LOG . error ( "Test failed" , e ) ; fail ( "Test failed due to interruption" ) ; } }
public void test() { try { lh = bkc . createLedger ( digestType , ledgerPassword ) ; ledgerId = lh . getId ( ) ; LOG . info ( "Ledger ID: " + lh . getId ( ) ) ; LedgerHandle lhOpen = bkc . openLedgerNoRecovery ( ledgerId , digestType , ledgerPassword ) ; LOG . debug ( "Checking that it is empty" ) ; long readLastConfirmed = lhOpen . readLastConfirmed ( ) ; assertTrue ( "Last confirmed has the wrong value" , readLastConfirmed == LedgerHandle . INVALID_ENTRY_ID ) ; LOG . debug ( "Going to write one entry" ) ; ByteBuffer entry = ByteBuffer . allocate ( 4 ) ; entry . putInt ( rng . nextInt ( maxInt ) ) ; entry . position ( 0 ) ; entries . add ( entry . array ( ) ) ; entriesSize . add ( entry . array ( ) . length ) ; lh . addEntry ( entry . array ( ) ) ; LOG . debug ( "Checking that it is still empty even after writing one entry" ) ; readLastConfirmed = lhOpen . readLastConfirmed ( ) ; assertTrue ( readLastConfirmed == LedgerHandle . INVALID_ENTRY_ID ) ; entry = ByteBuffer . allocate ( 4 ) ; entry . putInt ( rng . nextInt ( maxInt ) ) ; entry . position ( 0 ) ; entries . add ( entry . array ( ) ) ; entriesSize . add ( entry . array ( ) . length ) ; lh . addEntry ( entry . array ( ) ) ; LOG . info ( "Checking that it has an entry" ) ; readLastConfirmed = lhOpen . readLastConfirmed ( ) ; assertTrue ( readLastConfirmed == 0L ) ; lh . close ( ) ; lhOpen . close ( ) ; } catch ( BKException e ) { LOG . error ( "Test failed" , e ) ; fail ( "Test failed due to BookKeeper exception" ) ; } catch ( InterruptedException e ) { LOG . error ( "Test failed" , e ) ; fail ( "Test failed due to interruption" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Collapsing shards to " + date + " down to just the date with " + bits . cardinality ( ) + " of " + bits . length ( ) + " shards marked" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Appending " + bits . cardinality ( ) + " of " + bits . length ( ) + " shards to " + date ) ; } }
public void test() { if ( ! transactionCoordinatorBuilder . isJta ( ) ) { log . trace ( "TransactionFactory does not require a TransactionManager: don't wrap in a JTA transaction" ) ; return false ; } }
public void test() { if ( transactionManager == null ) { log . trace ( "No TransactionManager found, do not start a surrounding JTA transaction" ) ; return false ; } }
@ Override public List < LwM2mObject > findLwM2mObjectPage ( TenantId tenantId , String sortProperty , String sortOrder , PageLink pageLink ) { log . trace ( "Executing findByTenantId [{}]" , tenantId ) ; validateId ( tenantId , INCORRECT_TENANT_ID + tenantId ) ; PageData < TbResource > resourcePageData = resourceService . findTenantResourcesByResourceTypeAndPageLink ( tenantId , ResourceType . LWM2M_MODEL , pageLink ) ; return resourcePageData . getData ( ) . stream ( ) . flatMap ( s -> Stream . ofNullable ( toLwM2mObject ( s , false ) ) ) . sorted ( getComparator ( sortProperty , sortOrder ) ) . collect ( Collectors . toList ( ) ) ; }
public void test() { try { result = eval . evaluate ( eventsPerStream , true , null ) ; } catch ( Throwable t ) { log . error ( "Failed at expression " + expected . getKey ( ) + " at event " + assertionNumber , t ) ; fail ( ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( NumberFormatException nfe ) { log . error ( "GC_GRACE_SECONDS should be numeric type, Caused by: ." , nfe ) ; throw new SchemaGenerationException ( nfe ) ; } }
private Response getSamlpPostLogoutRequest ( String relayState , LogoutWrapper < LogoutRequest > logoutRequest ) throws SignatureException , WSSecurityException { LOGGER . debug ( "Configuring SAML LogoutRequest for POST." ) ; String encodedSamlRequest = encodeSaml ( logoutRequest ) ; String singleLogoutLocation = idpMetadata . getSingleLogoutLocation ( ) ; String submitFormUpdated = String . format ( submitForm , singleLogoutLocation , SAML_REQUEST , encodedSamlRequest , relayState ) ; Response . ResponseBuilder ok = Response . ok ( submitFormUpdated ) ; return ok . build ( ) ; }
public void test() { try { client . createTable ( table . getTTable ( ) ) ; } catch ( NoSuchObjectException e ) { throw new HiveMetaStoreException ( "Hive table not found: " + table . getDbName ( ) + "." + table . getTableName ( ) ) ; } catch ( AlreadyExistsException e ) { log . warn ( "Hive table already exists: {}.{}" , table . getDbName ( ) , table . getTableName ( ) ) ; } catch ( InvalidObjectException e ) { throw new HiveMetaStoreException ( "Invalid table" , e ) ; } }
public static void deleteMessage ( String relativeQueueUrl , String receiptHandle ) throws Exception { long ts1 = System . currentTimeMillis ( ) ; code_block = IfStatement ; long ts2 = System . currentTimeMillis ( ) ; CMBControllerServlet . valueAccumulator . addToCounter ( AccumulatorName . CNSCQSTime , ts2 - ts1 ) ; logger . debug ( "event=delete_message receipt_handle=" + receiptHandle ) ; }
public void test() { try { UserCache userCache = ( UserCache ) ctx . getBean ( "userCache" ) ; code_block = IfStatement ; } catch ( NoSuchBeanDefinitionException exc ) { log . debug ( "No userCache bean in context" , exc ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Throwable e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { getModel ( ) . removeNode ( element ) ; } catch ( NullPointerException e ) { LOG . error ( "Trying to delete an already removed node" , e ) ; } }
public void test() { if ( ! this . disposed ) { this . logger . error ( "Failed to execute job" , e ) ; } }
public void test() { -> { log . info ( "Rebalance happened " + partition . topic ( ) + ":" + partition . partition ( ) ) ; } }
public void test() { try { Set < String > beanProps = new HashSet < String > ( ) ; PropertyDescriptor [ ] propDescriptors = getBeanInfo ( clazz ) . getPropertyDescriptors ( ) ; code_block = ForStatement ; return beanProps ; } catch ( IntrospectionException e ) { LOG . warn ( "Failed to get bean properties on ({})" , clazz , e ) ; } }
public void test() { if ( _log . isWarnEnabled ( ) ) { _log . warn ( exception , exception ) ; } }
@ PUT @ Path ( "/changePassword" ) @ SubmarineApi public Response changePassword ( SysUser sysUser ) { LOG . info ( "changePassword({})" , sysUser . toString ( ) ) ; code_block = TryStatement ;  return new JsonResponse . Builder < > ( Response . Status . OK ) . success ( true ) . message ( "delete  user successfully!" ) . build ( ) ; }
public void test() { try { userService . changePassword ( sysUser ) ; } catch ( Exception e ) { LOG . error ( e . getMessage ( ) , e ) ; return new JsonResponse . Builder < > ( Response . Status . OK ) . success ( false ) . message ( "delete user failed!" ) . build ( ) ; } }
@ AfterClass public static void destroy ( ) throws IOException { logger . debug ( "Closing embedded node" ) ; embeddedNode . stop ( ) ; FileUtils . deleteDirectory ( new File ( ".\\data" ) ) ; }
public void test() { while ( it . hasNext ( ) ) { Map . Entry entry = ( Map . Entry ) it . next ( ) ; Index index = ( Index ) entry . getValue ( ) ; logger . info ( "The partitioned index created on this region " + " " + index ) ; logger . info ( "Current number of buckets indexed : " + "" + ( ( PartitionedIndex ) index ) . getNumberOfIndexedBuckets ( ) ) ; } }
@ Override public Response toResponse ( JobRunningException jobRunningException ) { LOG . error ( "Job Running" , jobRunningException ) ; return Response . status ( Status . INTERNAL_SERVER_ERROR ) . entity ( new JobRunningExceptionInfo ( jobRunningException ) ) . build ( ) ; }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "Excluding {} because it is not a valid Java " + "project" , project ) ; } }
public String setNamespace ( String futureNamespace ) { String previousNamespace = namespace ; LOGGER . info ( "Changing to {} namespace" , futureNamespace ) ; namespace = futureNamespace ; return previousNamespace ; }
public void test() { try { LOGGER . debug ( "Trying to connect to address {}." , address ) ; SocketChannel socketChannel = SocketChannel . open ( ) ; Socket socket = socketChannel . socket ( ) ; socket . connect ( address , CONNECT_TIMEOUT ) ; code_block = IfStatement ; LOGGER . debug ( "Connected to address {}." , socket . getRemoteSocketAddress ( ) ) ; ByteBuffer tokenBuffer = ByteBuffer . wrap ( token ) ; do code_block = "" ; while ( tokenBuffer . remaining ( ) > 0 ) ; LOGGER . debug ( "Exchanged token successfully" ) ; return new DaemonConnection ( socketChannel ) ; } catch ( DaemonException . ConnectException e ) { throw e ; } catch ( Exception e ) { throw new DaemonException . ConnectException ( String . format ( "Could not connect to server %s." , address ) , e ) ; } }
public void test() { try { LOGGER . debug ( "Trying to connect to address {}." , address ) ; SocketChannel socketChannel = SocketChannel . open ( ) ; Socket socket = socketChannel . socket ( ) ; socket . connect ( address , CONNECT_TIMEOUT ) ; code_block = IfStatement ; LOGGER . debug ( "Connected to address {}." , socket . getRemoteSocketAddress ( ) ) ; ByteBuffer tokenBuffer = ByteBuffer . wrap ( token ) ; do code_block = "" ; while ( tokenBuffer . remaining ( ) > 0 ) ; LOGGER . debug ( "Exchanged token successfully" ) ; return new DaemonConnection ( socketChannel ) ; } catch ( DaemonException . ConnectException e ) { throw e ; } catch ( Exception e ) { throw new DaemonException . ConnectException ( String . format ( "Could not connect to server %s." , address ) , e ) ; } }
public void test() { try { File file = new File ( ConfigCore . getKitodoConfigDirectory ( ) + "kitodo_exportXml.xml" ) ; code_block = IfStatement ; } catch ( ConfigurationException | RuntimeException e ) { logger . debug ( e . getMessage ( ) , e ) ; nss = new HashMap < > ( ) ; } }
public void test() { try { req . setCharacterEncoding ( "UTF-8" ) ; } catch ( UnsupportedEncodingException e ) { log . error ( "Encoding not supported" , e ) ; } }
public void test() { if ( debug ) { log . debug ( "Serving request path=" + portalPath + ", parameters=" + parameters + " with handler " + handler ) ; } }
public void test() { if ( debug ) { log . debug ( "Serving request path=" + portalPath + ", parameters=" + parameters + " with handler " + handler ) ; } }
public void test() { if ( debug ) { log . debug ( "Serving request path=" + portalPath + ", parameters=" + parameters + " with handler " + handler ) ; } }
public void test() { if ( debug ) { log . debug ( "Serving request path=" + portalPath + ", parameters=" + parameters + " with handler " + handler ) ; } }
public void test() { if ( router != null ) { Iterator < Map < QualifiedName , String > > matcher = router . matcher ( portalPath , req . getParameterMap ( ) ) ; boolean started = false ; boolean processed = false ; code_block = TryStatement ;  code_block = IfStatement ; } else { log . error ( "Missing valid router configuration " + configurationPathRef . get ( ) ) ; res . sendError ( HttpServletResponse . SC_INTERNAL_SERVER_ERROR ) ; } }
public void onDeviceConnected ( final NodeId nodeId ) { LOG . debug ( "FRS service registered for: {}" , nodeId . getValue ( ) ) ; final DeviceMastership mastership = new DeviceMastership ( nodeId , reconciliationRegistry , clusterSingletonService ) ; deviceMasterships . put ( nodeId , mastership ) ; }
public void test() { { log . debug ( "Received exchange: " + e . getIn ( ) ) ; latch . countDown ( ) ; } }
public void exceptionCaught ( ChannelHandlerContext ctx , ExceptionEvent e ) throws Exception { logger . error ( e . getCause ( ) . getMessage ( ) ) ; }
public void test() { try ( InputStream inputStream = jarFile . getInputStream ( jarEntry ) ) { pomInformation . parsePom ( inputStream ) ; } catch ( IOException e ) { LOGGER . error ( "Error while opening POM file" , e ) ; } }
public void test() { try { fieldAces . set ( acl , aces ) ; } catch ( Exception e ) { LOG . error ( "Could not set AccessControlEntries in the ACL" , e ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "Closing connection" ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( DepotEntryGroupRelServiceUtil . class , "updateDDMStructuresAvailable" , _updateDDMStructuresAvailableParameterTypes5 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , depotEntryGroupRelId , ddmStructuresAvailable ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . depot . model . DepotEntryGroupRel ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( null == availableConnection ) { code_block = IfStatement ; LOG . debug ( "Cache miss for table {}" , mTableName ) ; availableConnection = new PooledKijiTable ( mTableFactory . openTable ( mTableName ) , this ) ; mPoolSize ++ ; code_block = IfStatement ; } else { LOG . debug ( "Cache hit for table {}" , mTableName ) ; } }
public void test() { if ( null == availableConnection ) { code_block = IfStatement ; LOG . debug ( "Cache miss for table {}" , mTableName ) ; availableConnection = new PooledKijiTable ( mTableFactory . openTable ( mTableName ) , this ) ; mPoolSize ++ ; code_block = IfStatement ; } else { LOG . debug ( "Cache hit for table {}" , mTableName ) ; } }
@ Override @ AdapterDelegationEvent ( beforeBuilder = PRPAIN201305UV02EventDescriptionBuilder . class , afterReturningBuilder = PRPAIN201305UV02AdapterEventDescBuilder . class , serviceType = "Patient Discovery MPI" , version = "1.0" ) public PRPAIN201306UV02 findCandidates ( PRPAIN201305UV02 findCandidatesRequest , AssertionType assertion ) { LOG . trace ( "Entering AdapterMpiProxyJavaImpl.findCandidates" ) ; return findCandidatesMpi ( findCandidatesRequest , assertion ) ; }
public void test() { if ( _log . isWarnEnabled ( ) ) { _log . warn ( exception , exception ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CPDefinitionServiceUtil . class , "updateSubscriptionInfo" , _updateSubscriptionInfoParameterTypes25 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , cpDefinitionId , subscriptionEnabled , subscriptionLength , subscriptionType , subscriptionTypeSettingsUnicodeProperties , maxSubscriptionCycles , serviceContext ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . commerce . product . model . CPDefinition ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public static void main ( String [ ] args ) { String path = Paths . get ( "." ) . toAbsolutePath ( ) . normalize ( ) . toString ( ) ; LOG . info ( "path: {}" , path ) ; }
public void test() { try { MethodKey methodKey = new MethodKey ( BookmarksEntryServiceUtil . class , "getGroupEntriesCount" , _getGroupEntriesCountParameterTypes11 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( ( Integer ) returnObj ) . intValue ( ) ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { LOGGER . info ( "Received message of type: {}" , message . getJMSType ( ) ) ; final ObjectMessage objectMessage = ( ObjectMessage ) message ; final MessageProcessor processor = this . messageProcessorMap . getMessageProcessor ( objectMessage ) ; processor . processMessage ( objectMessage ) ; } catch ( final Exception e ) { LOGGER . error ( "Exception while handling a request from OSGP-Core: " , e ) ; } }
public void test() { try { LOGGER . info ( "Received message of type: {}" , message . getJMSType ( ) ) ; final ObjectMessage objectMessage = ( ObjectMessage ) message ; final MessageProcessor processor = this . messageProcessorMap . getMessageProcessor ( objectMessage ) ; processor . processMessage ( objectMessage ) ; } catch ( final Exception e ) { LOGGER . error ( "Exception while handling a request from OSGP-Core: " , e ) ; } }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; } catch ( Exception e ) { LOG . debug ( "failed to clean empty index directory" , e ) ; } }
public void test() { try { LOGGER . info ( f1 . getAbsolutePath ( ) ) ; LOGGER . info ( f2 . getAbsolutePath ( ) ) ; final Diff diff = new AstComparator ( ) . compare ( f1 , f2 ) ; code_block = IfStatement ; } catch ( Exception e ) { e . printStackTrace ( ) ; LOGGER . error ( "Error when trying to compare " + f1 + " and " + f2 ) ; } }
public void test() { try { LOGGER . info ( f1 . getAbsolutePath ( ) ) ; LOGGER . info ( f2 . getAbsolutePath ( ) ) ; final Diff diff = new AstComparator ( ) . compare ( f1 , f2 ) ; code_block = IfStatement ; } catch ( Exception e ) { e . printStackTrace ( ) ; LOGGER . error ( "Error when trying to compare " + f1 + " and " + f2 ) ; } }
public void test() { try { LOGGER . info ( f1 . getAbsolutePath ( ) ) ; LOGGER . info ( f2 . getAbsolutePath ( ) ) ; final Diff diff = new AstComparator ( ) . compare ( f1 , f2 ) ; code_block = IfStatement ; } catch ( Exception e ) { e . printStackTrace ( ) ; LOGGER . error ( "Error when trying to compare " + f1 + " and " + f2 ) ; } }
public void test() { try { pair = pairRemoteService . mirrorSequencingObject ( pair ) ; syncSequencingObject ( pair , sample , fileStatus ) ; } catch ( Exception e ) { logger . error ( "Error transferring file: " + pair . getRemoteStatus ( ) . getURL ( ) , e ) ; throw new ProjectSynchronizationException ( "Could not synchronize pair " + pair . getRemoteStatus ( ) . getURL ( ) , e ) ; } }
public void test() { if ( consumerKafkaGroup != null ) { consumerKafkaGroup . setKafkaSourceState ( null ) ; consumerKafkaGroup . shutdown ( ) ; LOG . info ( "Kafka Adapter disconnected for topic(s): " + optionHolder . validateAndGetStaticValue ( ADAPTOR_SUBSCRIBER_TOPIC ) ) ; } }
public void test() { try { session . deleteData ( paths , time ) ; putBack ( session ) ; return ; } catch ( IoTDBConnectionException e ) { logger . warn ( "deleteData failed" , e ) ; cleanSessionAndMayThrowConnectionException ( session , i , e ) ; } catch ( StatementExecutionException | RuntimeException e ) { putBack ( session ) ; throw e ; } }
public void test() { try { if ( previousConsentToView == null ) return ( getAllRemoteFacilities ( ) ) ; else return ( getPreviousConsentFacilities ( loggedInInfo ) ) ; } catch ( Exception e ) { integratorServerError = true ; logger . error ( "unexpected error" , e ) ; return ( null ) ; } }
public void test() { try { LOGGER . info ( "Stopping the SCMSecurityProtocolServer." ) ; metrics . unregister ( ) ; getRpcServer ( ) . stop ( ) ; } catch ( Exception ex ) { LOGGER . error ( "SCMSecurityProtocolServer stop failed." , ex ) ; } }
public void test() { try { LOGGER . info ( "Stopping the SCMSecurityProtocolServer." ) ; metrics . unregister ( ) ; getRpcServer ( ) . stop ( ) ; } catch ( Exception ex ) { LOGGER . error ( "SCMSecurityProtocolServer stop failed." , ex ) ; } }
public void test() { try { ctx . rollbackTransaction ( ) ; state . counter . rolled ++ ; logger . fatal ( name + ": Exception in txn " + state . counter , txnException ) ; } catch ( Exception rollException ) { state . counter . failedRollbacks ++ ; logger . fatal ( name + ": Exception in roll " + state . counter , rollException ) ; } }
public void test() { try { TransactionContext ctx = targetInstance . newTransactionContext ( ) ; code_block = TryStatement ;  } catch ( Exception e ) { logger . fatal ( name + ": outer Exception" + state . counter , e ) ; } finally { firstLock . unlock ( ) ; } }
public void test() { try { logger . warn ( e ) ; } catch ( Throwable ignored ) { } }
public void test() { try { BatchStatement batch = new BatchStatement ( BatchStatement . Type . UNLOGGED ) ; code_block = ForStatement ; Session session = DatastaxIO . getSession ( ) ; session . execute ( batch ) ; } catch ( Exception ex ) { Instrumentation . markWriteError ( ) ; LOG . error ( String . format ( "error writing locator batch of size %s, granularity %s" , writeContexts . size ( ) , writeContexts . get ( 0 ) . getGranularity ( ) ) , ex ) ; } finally { ctx . stop ( ) ; } }
public void test() { try { Thread . sleep ( 2000 ) ; } catch ( InterruptedException e ) { LOG . error ( "Failed to sleep TaskTracker thread" , e ) ; } }
public void test() { try { groomServer . run ( ) ; driver . sendStatusUpdate ( TaskStatus . newBuilder ( ) . setTaskId ( task . getTaskId ( ) ) . setState ( TaskState . TASK_FINISHED ) . build ( ) ) ; code_block = TryStatement ;  driver . stop ( ) ; } catch ( Throwable t ) { LOG . error ( "Caught exception, committing suicide." , t ) ; driver . stop ( ) ; System . exit ( 1 ) ; } }
public void test() { try { deleteWriteFile ( ) ; } catch ( IOException e ) { logger . error ( e . toString ( ) , e ) ; } }
public void test() { try { persistDefaultAuthResources ( ) ; } catch ( AuthServerException e ) { LOGGER . error ( "Error occurred while persisting auth resources." , e ) ; } }
public void test() { if ( this . gtidPositioning ) { LOGGER . warn ( "replicator stopped at position: {} -- restarting" , client . getGtidSet ( ) ) ; client . setBinlogFilename ( "" ) ; client . setBinlogPosition ( 4L ) ; client . connect ( 5000 ) ; throw new ClientReconnectedException ( ) ; } else { LOGGER . warn ( "replicator stopped at position: {} -- restarting" , client . getBinlogFilename ( ) + ":" + client . getBinlogPosition ( ) ) ; client . connect ( 5000 ) ; } }
public void test() { if ( this . gtidPositioning ) { LOGGER . warn ( "replicator stopped at position: {} -- restarting" , client . getGtidSet ( ) ) ; client . setBinlogFilename ( "" ) ; client . setBinlogPosition ( 4L ) ; client . connect ( 5000 ) ; throw new ClientReconnectedException ( ) ; } else { LOGGER . warn ( "replicator stopped at position: {} -- restarting" , client . getBinlogFilename ( ) + ":" + client . getBinlogPosition ( ) ) ; client . connect ( 5000 ) ; } }
@ Override public void initialize ( ) { logger . debug ( "Initializing the Lutron HomeWorks RS232 bridge handler" ) ; HwSerialBridgeConfig configuration = getConfigAs ( HwSerialBridgeConfig . class ) ; serialPortName = configuration . getSerialPort ( ) ; updateTime = configuration . getUpdateTime ( ) ; code_block = IfStatement ; code_block = IfStatement ; logger . debug ( "Lutron HomeWorks RS232 Bridge Handler Initializing." ) ; logger . debug ( "   Serial Port: {}," , serialPortName ) ; logger . debug ( "   Baud:        {}," , baudRate ) ; scheduler . execute ( ( ) -> openConnection ( ) ) ; }
@ Override public void initialize ( ) { logger . debug ( "Initializing the Lutron HomeWorks RS232 bridge handler" ) ; HwSerialBridgeConfig configuration = getConfigAs ( HwSerialBridgeConfig . class ) ; serialPortName = configuration . getSerialPort ( ) ; updateTime = configuration . getUpdateTime ( ) ; code_block = IfStatement ; code_block = IfStatement ; logger . debug ( "Lutron HomeWorks RS232 Bridge Handler Initializing." ) ; logger . debug ( "   Serial Port: {}," , serialPortName ) ; logger . debug ( "   Baud:        {}," , baudRate ) ; scheduler . execute ( ( ) -> openConnection ( ) ) ; }
@ Override public void initialize ( ) { logger . debug ( "Initializing the Lutron HomeWorks RS232 bridge handler" ) ; HwSerialBridgeConfig configuration = getConfigAs ( HwSerialBridgeConfig . class ) ; serialPortName = configuration . getSerialPort ( ) ; updateTime = configuration . getUpdateTime ( ) ; code_block = IfStatement ; code_block = IfStatement ; logger . debug ( "Lutron HomeWorks RS232 Bridge Handler Initializing." ) ; logger . debug ( "   Serial Port: {}," , serialPortName ) ; logger . debug ( "   Baud:        {}," , baudRate ) ; scheduler . execute ( ( ) -> openConnection ( ) ) ; }
public void test() { if ( filter == null || filter . accept ( obj ) ) { handler . handle ( obj ) ; } else { LOG . debug ( "Found but not passing the provided filter {}: {}" , filter , obj ) ; } }
public void test() { if ( tokenCredential != null ) { LOGGER . debug ( "Connecting to " + endpoint + " using AppConfigurationCredentialProvider." ) ; builder . credential ( tokenCredential ) ; } else-if ( ( connection . getClientId ( ) != null && StringUtils . isNotEmpty ( connection . getClientId ( ) ) ) && connection . getEndpoint ( ) != null ) { LOGGER . debug ( "Connecting to " + endpoint + " using Client ID from configuration file." ) ; ManagedIdentityCredentialBuilder micBuilder = new ManagedIdentityCredentialBuilder ( ) . clientId ( connection . getClientId ( ) ) ; builder . credential ( micBuilder . build ( ) ) ; } else-if ( StringUtils . isNotEmpty ( connection . getConnectionString ( ) ) ) { LOGGER . debug ( "Connecting to " + endpoint + " using Connecting String." ) ; builder . connectionString ( connection . getConnectionString ( ) ) ; } else-if ( connection . getEndpoint ( ) != null ) { LOGGER . debug ( "Connecting to " + endpoint + " using Azure System Assigned Identity or Azure User Assigned Identity." ) ; ManagedIdentityCredentialBuilder micBuilder = new ManagedIdentityCredentialBuilder ( ) ; builder . credential ( micBuilder . build ( ) ) ; } else { throw new IllegalArgumentException ( "No Configuration method was set for connecting to App Configuration" ) ; } }
public void test() { if ( tokenCredential != null ) { LOGGER . debug ( "Connecting to " + endpoint + " using AppConfigurationCredentialProvider." ) ; builder . credential ( tokenCredential ) ; } else-if ( ( connection . getClientId ( ) != null && StringUtils . isNotEmpty ( connection . getClientId ( ) ) ) && connection . getEndpoint ( ) != null ) { LOGGER . debug ( "Connecting to " + endpoint + " using Client ID from configuration file." ) ; ManagedIdentityCredentialBuilder micBuilder = new ManagedIdentityCredentialBuilder ( ) . clientId ( connection . getClientId ( ) ) ; builder . credential ( micBuilder . build ( ) ) ; } else-if ( StringUtils . isNotEmpty ( connection . getConnectionString ( ) ) ) { LOGGER . debug ( "Connecting to " + endpoint + " using Connecting String." ) ; builder . connectionString ( connection . getConnectionString ( ) ) ; } else-if ( connection . getEndpoint ( ) != null ) { LOGGER . debug ( "Connecting to " + endpoint + " using Azure System Assigned Identity or Azure User Assigned Identity." ) ; ManagedIdentityCredentialBuilder micBuilder = new ManagedIdentityCredentialBuilder ( ) ; builder . credential ( micBuilder . build ( ) ) ; } else { throw new IllegalArgumentException ( "No Configuration method was set for connecting to App Configuration" ) ; } }
public void test() { if ( tokenCredential != null ) { LOGGER . debug ( "Connecting to " + endpoint + " using AppConfigurationCredentialProvider." ) ; builder . credential ( tokenCredential ) ; } else-if ( ( connection . getClientId ( ) != null && StringUtils . isNotEmpty ( connection . getClientId ( ) ) ) && connection . getEndpoint ( ) != null ) { LOGGER . debug ( "Connecting to " + endpoint + " using Client ID from configuration file." ) ; ManagedIdentityCredentialBuilder micBuilder = new ManagedIdentityCredentialBuilder ( ) . clientId ( connection . getClientId ( ) ) ; builder . credential ( micBuilder . build ( ) ) ; } else-if ( StringUtils . isNotEmpty ( connection . getConnectionString ( ) ) ) { LOGGER . debug ( "Connecting to " + endpoint + " using Connecting String." ) ; builder . connectionString ( connection . getConnectionString ( ) ) ; } else-if ( connection . getEndpoint ( ) != null ) { LOGGER . debug ( "Connecting to " + endpoint + " using Azure System Assigned Identity or Azure User Assigned Identity." ) ; ManagedIdentityCredentialBuilder micBuilder = new ManagedIdentityCredentialBuilder ( ) ; builder . credential ( micBuilder . build ( ) ) ; } else { throw new IllegalArgumentException ( "No Configuration method was set for connecting to App Configuration" ) ; } }
public void test() { if ( tokenCredential != null ) { LOGGER . debug ( "Connecting to " + endpoint + " using AppConfigurationCredentialProvider." ) ; builder . credential ( tokenCredential ) ; } else-if ( ( connection . getClientId ( ) != null && StringUtils . isNotEmpty ( connection . getClientId ( ) ) ) && connection . getEndpoint ( ) != null ) { LOGGER . debug ( "Connecting to " + endpoint + " using Client ID from configuration file." ) ; ManagedIdentityCredentialBuilder micBuilder = new ManagedIdentityCredentialBuilder ( ) . clientId ( connection . getClientId ( ) ) ; builder . credential ( micBuilder . build ( ) ) ; } else-if ( StringUtils . isNotEmpty ( connection . getConnectionString ( ) ) ) { LOGGER . debug ( "Connecting to " + endpoint + " using Connecting String." ) ; builder . connectionString ( connection . getConnectionString ( ) ) ; } else-if ( connection . getEndpoint ( ) != null ) { LOGGER . debug ( "Connecting to " + endpoint + " using Azure System Assigned Identity or Azure User Assigned Identity." ) ; ManagedIdentityCredentialBuilder micBuilder = new ManagedIdentityCredentialBuilder ( ) ; builder . credential ( micBuilder . build ( ) ) ; } else { throw new IllegalArgumentException ( "No Configuration method was set for connecting to App Configuration" ) ; } }
public boolean isServiceSettingTrue ( String serviceparameter ) { String value = getServiceSetting ( serviceparameter ) ; boolean isTrue = ( value != null ? value . equalsIgnoreCase ( "true" ) : false ) ; logger . debug ( serviceparameter + ": " + isTrue ) ; return isTrue ; }
public void test() { if ( map . containsKey ( key ) ) { LOG . warn ( "A duplicate variable name found with name: {} and value: {}." , key , value ) ; } }
public void test() { if ( dialect != null ) { return dialect ; } }
public void test() { try { File destination = new File ( chooser . getDirectory ( ) + chooser . getFile ( ) ) ; jTextFieldExportPath . setText ( destination . getAbsolutePath ( ) ) ; } catch ( Exception ex ) { logger . error ( ex ) ; } }
public void test() { try { jTextFieldExportPath . setText ( chooser . getSelectedFile ( ) . getAbsolutePath ( ) ) ; } catch ( Exception ex ) { logger . error ( "BeobPfad.actionPerformed" , ex ) ; } }
@ Override public void punsubscribeAll ( ) { log . warn ( "unsupported" ) ; }
public void test() { if ( ! ( target instanceof JavaBrooklynClassLoadingContext ) ) { log . warn ( "Only Java classloaders should be secondary" ) ; } }
@ Override public Response setCollaborators ( CollaboratorsRepresentation collaborators , String entityId , String entityType ) { this . logger . debug ( "Setting {} collaborators to entity record [{}] via REST" , collaborators . getCollaborators ( ) . size ( ) , entityId ) ; return this . setCollaborators ( collaborators . getCollaborators ( ) , entityId , entityType , true ) ; }
public void test() { try { jt . update ( "UPDATE config_info_beta SET content=?, md5 = ?, src_ip=?,src_user=?,gmt_modified=?,app_name=? WHERE " + "data_id=? AND group_id=? AND tenant_id=?" , configInfo . getContent ( ) , md5 , srcIp , srcUser , time , appNameTmp , configInfo . getDataId ( ) , configInfo . getGroup ( ) , tenantTmp ) ; } catch ( CannotGetJdbcConnectionException e ) { LogUtil . FATAL_LOG . error ( "[db-error] " + e . toString ( ) , e ) ; throw e ; } }
@ Override public void sendNCBIUploadExceptionEmail ( String adminEmailAddress , Exception rootCause , Long submissionId ) throws MailSendException { logger . info ( "TestEmailControllersendNCBIUploadExceptionEmail called." ) ; }
public void test() { if ( matrixPath . isEmpty ( ) || vectorPath . isEmpty ( ) || outputPath . isEmpty ( ) ) { LOG . info ( "Please setup input path for vector and matrix and output path for result." ) ; return ; } }
public void test() { if ( writeDelay > this . writeDelayLimit && currentTime - timestamps . lastChannelWriteAttempt ( ) > writeHangGracePeriod ) { final Optional < RntbdContext > rntbdContext = requestManager . rntbdContext ( ) ; final int pendingRequestCount = requestManager . pendingRequestCount ( ) ; logger . warn ( "{} health check failed due to hung write: {lastChannelWriteAttempt: {}, lastChannelWrite: {}, " + "writeDelay: {}, writeDelayLimit: {}, rntbdContext: {}, pendingRequestCount: {}}" , channel , timestamps . lastChannelWriteAttempt ( ) , timestamps . lastChannelWrite ( ) , writeDelay , this . writeDelayLimit , rntbdContext , pendingRequestCount ) ; return promise . setSuccess ( Boolean . FALSE ) ; } }
public void test() { if ( readDelay > this . readDelayLimit && currentTime - timestamps . lastChannelWrite ( ) > readHangGracePeriod ) { final Optional < RntbdContext > rntbdContext = requestManager . rntbdContext ( ) ; final int pendingRequestCount = requestManager . pendingRequestCount ( ) ; logger . warn ( "{} health check failed due to hung read: {lastChannelWrite: {}, lastChannelRead: {}, " + "readDelay: {}, readDelayLimit: {}, rntbdContext: {}, pendingRequestCount: {}}" , channel , timestamps . lastChannelWrite ( ) , timestamps . lastChannelRead ( ) , readDelay , this . readDelayLimit , rntbdContext , pendingRequestCount ) ; return promise . setSuccess ( Boolean . FALSE ) ; } }
public void test() { try { return getConvertedPropertyValue ( securityContext , graphObject , key ) . getBytes ( Charset . forName ( "utf-8" ) ) . length ; } catch ( FrameworkException fex ) { logger . warn ( "" , fex ) ; } }
public void test() { try { c = getCachedClass ( this . className ) ; } catch ( ClassNotFoundException ex ) { logger . warn ( "Could not load data serializer class {} so both clients of this server and this server will not have this data serializer. Load failed because: {}" , this . className , getFullMessage ( ex ) ) ; return ; } }
public void test() { try { c = getCachedClass ( this . className ) ; } catch ( ClassNotFoundException ex ) { logger . warn ( "Could not load data serializer class {} so both clients of this server and this server will not have this data serializer. Load failed because: {}" , this . className , getFullMessage ( ex ) ) ; return ; } }
public void test() { try { c = getCachedClass ( this . className ) ; } catch ( ClassNotFoundException ex ) { logger . warn ( "Could not load data serializer class {} so both clients of this server and this server will not have this data serializer. Load failed because: {}" , this . className , getFullMessage ( ex ) ) ; return ; } }
public void test() { try { c = getCachedClass ( this . className ) ; } catch ( ClassNotFoundException ex ) { logger . warn ( "Could not load data serializer class {} so both clients of this server and this server will not have this data serializer. Load failed because: {}" , this . className , getFullMessage ( ex ) ) ; return ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( MessageFormat . format ( "Launching application: {0}{1}" , tool . getClass ( ) . getName ( ) , Arrays . toString ( args ) ) ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Resolve component metadata FINISHED for component: {}" , componentModel . getName ( ) ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Resolve component metadata FINISHED for component: {}" , componentModel . getName ( ) ) ; } }
public void test() { if ( ! metadataKeyResult . isComplete ( ) ) { return failure ( newFailure ( ) . withMessage ( metadataKeyResult . getPartialReason ( ) ) . withFailureCode ( INVALID_METADATA_KEY ) . onComponent ( ) ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Resolve component metadata FINISHED for component: {}" , componentModel . getName ( ) ) ; } }
public void test() { if ( ! metadataKeyResult . isComplete ( ) ) { return failure ( newFailure ( ) . withMessage ( metadataKeyResult . getPartialReason ( ) ) . withFailureCode ( INVALID_METADATA_KEY ) . onComponent ( ) ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Resolve component metadata FINISHED for component: {}" , componentModel . getName ( ) ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Resolve component metadata FINISHED for component: {}" , componentModel . getName ( ) ) ; } }
public void test() { if ( ! metadataKeyResult . isComplete ( ) ) { return failure ( newFailure ( ) . withMessage ( metadataKeyResult . getPartialReason ( ) ) . withFailureCode ( INVALID_METADATA_KEY ) . onComponent ( ) ) ; } }
public void test() { if ( _log . isWarnEnabled ( ) ) { _log . warn ( StringBundler . concat ( "Unable to find key " , key , " in theme " , _themeId ) , missingResourceException ) ; } }
public static void log ( String s ) { logger . info ( s ) ; }
public void test() { try { br = new BufferedReader ( new InputStreamReader ( new FileInputStream ( dispatchFile ) , "UTF-8" ) ) ; final String line = br . readLine ( ) ; return line ; } catch ( Exception continued ) { _log . info ( "Failed to read the dispatch file: " + dispatchFile ) ; return defaultValue ; } finally { code_block = IfStatement ; } }
public void test() { if ( handler == null ) { logger . warn ( "Onkyo Action service ThingHandler is null!" ) ; return ; } }
public void test() { try { code_block = ForStatement ; } catch ( Exception e ) { logger . warn ( "Failed to handle AgentStatBatch={}" , tAgentStatBatch , e ) ; } }
public void test() { try { m_transactedSession . rollback ( ) ; } catch ( final JMSException e ) { LOG . error ( "Failed to rollback transacted session [session={}]" , m_transactedSession , e ) ; } }
public void test() { if ( ! maybeEndDate . isPresent ( ) ) { LOGGER . error ( "Pattern used for bucketName used in deletedMessageVault is invalid and end date cannot be parsed {}" , bucketName ) ; } }
@ Override public void initialize ( final Subject subject , final CallbackHandler callbackHandler , final Map < String , ? > sharedState , final Map < String , ? > options ) { super . initialize ( subject , callbackHandler , sharedState , options ) ; code_block = ForStatement ; code_block = IfStatement ; logger . trace ( "roleResolverFactory = {}, roleFilter = {}, roleAttribute = {}, noResultsIsError = {}" , roleResolverFactory , roleFilter , Arrays . toString ( roleAttribute ) , noResultsIsError ) ; roleResolver = roleResolverFactory . createRoleResolver ( options ) ; logger . debug ( "Retrieved role resolver from factory: {}" , roleResolver ) ; searchRequest = roleResolverFactory . createSearchRequest ( options ) ; searchRequest . setReturnAttributes ( roleAttribute ) ; logger . debug ( "Retrieved search request from factory: {}" , searchRequest ) ; }
@ Override public void initialize ( final Subject subject , final CallbackHandler callbackHandler , final Map < String , ? > sharedState , final Map < String , ? > options ) { super . initialize ( subject , callbackHandler , sharedState , options ) ; code_block = ForStatement ; code_block = IfStatement ; logger . trace ( "roleResolverFactory = {}, roleFilter = {}, roleAttribute = {}, noResultsIsError = {}" , roleResolverFactory , roleFilter , Arrays . toString ( roleAttribute ) , noResultsIsError ) ; roleResolver = roleResolverFactory . createRoleResolver ( options ) ; logger . debug ( "Retrieved role resolver from factory: {}" , roleResolver ) ; searchRequest = roleResolverFactory . createSearchRequest ( options ) ; searchRequest . setReturnAttributes ( roleAttribute ) ; logger . debug ( "Retrieved search request from factory: {}" , searchRequest ) ; }
@ Override public void initialize ( final Subject subject , final CallbackHandler callbackHandler , final Map < String , ? > sharedState , final Map < String , ? > options ) { super . initialize ( subject , callbackHandler , sharedState , options ) ; code_block = ForStatement ; code_block = IfStatement ; logger . trace ( "roleResolverFactory = {}, roleFilter = {}, roleAttribute = {}, noResultsIsError = {}" , roleResolverFactory , roleFilter , Arrays . toString ( roleAttribute ) , noResultsIsError ) ; roleResolver = roleResolverFactory . createRoleResolver ( options ) ; logger . debug ( "Retrieved role resolver from factory: {}" , roleResolver ) ; searchRequest = roleResolverFactory . createSearchRequest ( options ) ; searchRequest . setReturnAttributes ( roleAttribute ) ; logger . debug ( "Retrieved search request from factory: {}" , searchRequest ) ; }
public void test() { try { Iterator < Record > result = this . neo4jConnectionManager . execute ( cypherQuery , interpreterContext ) . iterator ( ) ; Set < Node > nodes = new HashSet < > ( ) ; Set < Relationship > relationships = new HashSet < > ( ) ; List < String > columns = new ArrayList < > ( ) ; List < List < String > > lines = new ArrayList < List < String > > ( ) ; code_block = WhileStatement ; code_block = IfStatement ; } catch ( Exception e ) { LOGGER . error ( "Exception while interpreting cypher query" , e ) ; return new InterpreterResult ( Code . ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( _queryLogLevelInfo ) { _log . info ( sql ) ; } else { _log . debug ( sql ) ; } }
public void test() { if ( _queryLogLevelInfo ) { _log . info ( sql ) ; } else { _log . debug ( sql ) ; } }
private Location getVisionOffsets ( Head head , Location pickLocation ) throws Exception { Logger . debug ( "getVisionOffsets({}, {})" , head . getName ( ) , pickLocation ) ; Camera camera = null ; code_block = ForStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; head . moveToSafeZ ( ) ; Logger . debug ( "Move camera to pick location." ) ; camera . moveTo ( pickLocation ) ; VisionProvider visionProvider = camera . getVisionProvider ( ) ; Rectangle aoi = getVision ( ) . getAreaOfInterest ( ) ; Logger . debug ( "Perform template match." ) ; Point [ ] matchingPoints = visionProvider . locateTemplateMatches ( aoi . getX ( ) , aoi . getY ( ) , aoi . getWidth ( ) , aoi . getHeight ( ) , 0 , 0 , vision . getTemplateImage ( ) ) ; Point match = matchingPoints [ 0 ] ; double imageWidth = camera . getWidth ( ) ; double imageHeight = camera . getHeight ( ) ; double templateWidth = vision . getTemplateImage ( ) . getWidth ( ) ; double templateHeight = vision . getTemplateImage ( ) . getHeight ( ) ; double matchX = match . x ; double matchY = match . y ; Logger . debug ( "matchX {}, matchY {}" , matchX , matchY ) ; matchX += ( templateWidth / 2 ) ; matchY += ( templateHeight / 2 ) ; Logger . debug ( "centered matchX {}, matchY {}" , matchX , matchY ) ; double offsetX = ( imageWidth / 2 ) - matchX ; double offsetY = ( imageHeight / 2 ) - matchY ; Logger . debug ( "offsetX {}, offsetY {}" , offsetX , offsetY ) ; offsetY *= - 1 ; Logger . debug ( "negated offsetX {}, offsetY {}" , offsetX , offsetY ) ; Location unitsPerPixel = camera . getUnitsPerPixel ( ) ; offsetX *= unitsPerPixel . getX ( ) ; offsetY *= unitsPerPixel . getY ( ) ; Logger . debug ( "final, in camera units offsetX {}, offsetY {}" , offsetX , offsetY ) ; return new Location ( unitsPerPixel . getUnits ( ) , offsetX , offsetY , 0 , 0 ) ; }
private Location getVisionOffsets ( Head head , Location pickLocation ) throws Exception { Logger . debug ( "getVisionOffsets({}, {})" , head . getName ( ) , pickLocation ) ; Camera camera = null ; code_block = ForStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; head . moveToSafeZ ( ) ; Logger . debug ( "Move camera to pick location." ) ; camera . moveTo ( pickLocation ) ; VisionProvider visionProvider = camera . getVisionProvider ( ) ; Rectangle aoi = getVision ( ) . getAreaOfInterest ( ) ; Logger . debug ( "Perform template match." ) ; Point [ ] matchingPoints = visionProvider . locateTemplateMatches ( aoi . getX ( ) , aoi . getY ( ) , aoi . getWidth ( ) , aoi . getHeight ( ) , 0 , 0 , vision . getTemplateImage ( ) ) ; Point match = matchingPoints [ 0 ] ; double imageWidth = camera . getWidth ( ) ; double imageHeight = camera . getHeight ( ) ; double templateWidth = vision . getTemplateImage ( ) . getWidth ( ) ; double templateHeight = vision . getTemplateImage ( ) . getHeight ( ) ; double matchX = match . x ; double matchY = match . y ; Logger . debug ( "matchX {}, matchY {}" , matchX , matchY ) ; matchX += ( templateWidth / 2 ) ; matchY += ( templateHeight / 2 ) ; Logger . debug ( "centered matchX {}, matchY {}" , matchX , matchY ) ; double offsetX = ( imageWidth / 2 ) - matchX ; double offsetY = ( imageHeight / 2 ) - matchY ; Logger . debug ( "offsetX {}, offsetY {}" , offsetX , offsetY ) ; offsetY *= - 1 ; Logger . debug ( "negated offsetX {}, offsetY {}" , offsetX , offsetY ) ; Location unitsPerPixel = camera . getUnitsPerPixel ( ) ; offsetX *= unitsPerPixel . getX ( ) ; offsetY *= unitsPerPixel . getY ( ) ; Logger . debug ( "final, in camera units offsetX {}, offsetY {}" , offsetX , offsetY ) ; return new Location ( unitsPerPixel . getUnits ( ) , offsetX , offsetY , 0 , 0 ) ; }
private Location getVisionOffsets ( Head head , Location pickLocation ) throws Exception { Logger . debug ( "getVisionOffsets({}, {})" , head . getName ( ) , pickLocation ) ; Camera camera = null ; code_block = ForStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; head . moveToSafeZ ( ) ; Logger . debug ( "Move camera to pick location." ) ; camera . moveTo ( pickLocation ) ; VisionProvider visionProvider = camera . getVisionProvider ( ) ; Rectangle aoi = getVision ( ) . getAreaOfInterest ( ) ; Logger . debug ( "Perform template match." ) ; Point [ ] matchingPoints = visionProvider . locateTemplateMatches ( aoi . getX ( ) , aoi . getY ( ) , aoi . getWidth ( ) , aoi . getHeight ( ) , 0 , 0 , vision . getTemplateImage ( ) ) ; Point match = matchingPoints [ 0 ] ; double imageWidth = camera . getWidth ( ) ; double imageHeight = camera . getHeight ( ) ; double templateWidth = vision . getTemplateImage ( ) . getWidth ( ) ; double templateHeight = vision . getTemplateImage ( ) . getHeight ( ) ; double matchX = match . x ; double matchY = match . y ; Logger . debug ( "matchX {}, matchY {}" , matchX , matchY ) ; matchX += ( templateWidth / 2 ) ; matchY += ( templateHeight / 2 ) ; Logger . debug ( "centered matchX {}, matchY {}" , matchX , matchY ) ; double offsetX = ( imageWidth / 2 ) - matchX ; double offsetY = ( imageHeight / 2 ) - matchY ; Logger . debug ( "offsetX {}, offsetY {}" , offsetX , offsetY ) ; offsetY *= - 1 ; Logger . debug ( "negated offsetX {}, offsetY {}" , offsetX , offsetY ) ; Location unitsPerPixel = camera . getUnitsPerPixel ( ) ; offsetX *= unitsPerPixel . getX ( ) ; offsetY *= unitsPerPixel . getY ( ) ; Logger . debug ( "final, in camera units offsetX {}, offsetY {}" , offsetX , offsetY ) ; return new Location ( unitsPerPixel . getUnits ( ) , offsetX , offsetY , 0 , 0 ) ; }
private Location getVisionOffsets ( Head head , Location pickLocation ) throws Exception { Logger . debug ( "getVisionOffsets({}, {})" , head . getName ( ) , pickLocation ) ; Camera camera = null ; code_block = ForStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; head . moveToSafeZ ( ) ; Logger . debug ( "Move camera to pick location." ) ; camera . moveTo ( pickLocation ) ; VisionProvider visionProvider = camera . getVisionProvider ( ) ; Rectangle aoi = getVision ( ) . getAreaOfInterest ( ) ; Logger . debug ( "Perform template match." ) ; Point [ ] matchingPoints = visionProvider . locateTemplateMatches ( aoi . getX ( ) , aoi . getY ( ) , aoi . getWidth ( ) , aoi . getHeight ( ) , 0 , 0 , vision . getTemplateImage ( ) ) ; Point match = matchingPoints [ 0 ] ; double imageWidth = camera . getWidth ( ) ; double imageHeight = camera . getHeight ( ) ; double templateWidth = vision . getTemplateImage ( ) . getWidth ( ) ; double templateHeight = vision . getTemplateImage ( ) . getHeight ( ) ; double matchX = match . x ; double matchY = match . y ; Logger . debug ( "matchX {}, matchY {}" , matchX , matchY ) ; matchX += ( templateWidth / 2 ) ; matchY += ( templateHeight / 2 ) ; Logger . debug ( "centered matchX {}, matchY {}" , matchX , matchY ) ; double offsetX = ( imageWidth / 2 ) - matchX ; double offsetY = ( imageHeight / 2 ) - matchY ; Logger . debug ( "offsetX {}, offsetY {}" , offsetX , offsetY ) ; offsetY *= - 1 ; Logger . debug ( "negated offsetX {}, offsetY {}" , offsetX , offsetY ) ; Location unitsPerPixel = camera . getUnitsPerPixel ( ) ; offsetX *= unitsPerPixel . getX ( ) ; offsetY *= unitsPerPixel . getY ( ) ; Logger . debug ( "final, in camera units offsetX {}, offsetY {}" , offsetX , offsetY ) ; return new Location ( unitsPerPixel . getUnits ( ) , offsetX , offsetY , 0 , 0 ) ; }
private Location getVisionOffsets ( Head head , Location pickLocation ) throws Exception { Logger . debug ( "getVisionOffsets({}, {})" , head . getName ( ) , pickLocation ) ; Camera camera = null ; code_block = ForStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; head . moveToSafeZ ( ) ; Logger . debug ( "Move camera to pick location." ) ; camera . moveTo ( pickLocation ) ; VisionProvider visionProvider = camera . getVisionProvider ( ) ; Rectangle aoi = getVision ( ) . getAreaOfInterest ( ) ; Logger . debug ( "Perform template match." ) ; Point [ ] matchingPoints = visionProvider . locateTemplateMatches ( aoi . getX ( ) , aoi . getY ( ) , aoi . getWidth ( ) , aoi . getHeight ( ) , 0 , 0 , vision . getTemplateImage ( ) ) ; Point match = matchingPoints [ 0 ] ; double imageWidth = camera . getWidth ( ) ; double imageHeight = camera . getHeight ( ) ; double templateWidth = vision . getTemplateImage ( ) . getWidth ( ) ; double templateHeight = vision . getTemplateImage ( ) . getHeight ( ) ; double matchX = match . x ; double matchY = match . y ; Logger . debug ( "matchX {}, matchY {}" , matchX , matchY ) ; matchX += ( templateWidth / 2 ) ; matchY += ( templateHeight / 2 ) ; Logger . debug ( "centered matchX {}, matchY {}" , matchX , matchY ) ; double offsetX = ( imageWidth / 2 ) - matchX ; double offsetY = ( imageHeight / 2 ) - matchY ; Logger . debug ( "offsetX {}, offsetY {}" , offsetX , offsetY ) ; offsetY *= - 1 ; Logger . debug ( "negated offsetX {}, offsetY {}" , offsetX , offsetY ) ; Location unitsPerPixel = camera . getUnitsPerPixel ( ) ; offsetX *= unitsPerPixel . getX ( ) ; offsetY *= unitsPerPixel . getY ( ) ; Logger . debug ( "final, in camera units offsetX {}, offsetY {}" , offsetX , offsetY ) ; return new Location ( unitsPerPixel . getUnits ( ) , offsetX , offsetY , 0 , 0 ) ; }
private Location getVisionOffsets ( Head head , Location pickLocation ) throws Exception { Logger . debug ( "getVisionOffsets({}, {})" , head . getName ( ) , pickLocation ) ; Camera camera = null ; code_block = ForStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; head . moveToSafeZ ( ) ; Logger . debug ( "Move camera to pick location." ) ; camera . moveTo ( pickLocation ) ; VisionProvider visionProvider = camera . getVisionProvider ( ) ; Rectangle aoi = getVision ( ) . getAreaOfInterest ( ) ; Logger . debug ( "Perform template match." ) ; Point [ ] matchingPoints = visionProvider . locateTemplateMatches ( aoi . getX ( ) , aoi . getY ( ) , aoi . getWidth ( ) , aoi . getHeight ( ) , 0 , 0 , vision . getTemplateImage ( ) ) ; Point match = matchingPoints [ 0 ] ; double imageWidth = camera . getWidth ( ) ; double imageHeight = camera . getHeight ( ) ; double templateWidth = vision . getTemplateImage ( ) . getWidth ( ) ; double templateHeight = vision . getTemplateImage ( ) . getHeight ( ) ; double matchX = match . x ; double matchY = match . y ; Logger . debug ( "matchX {}, matchY {}" , matchX , matchY ) ; matchX += ( templateWidth / 2 ) ; matchY += ( templateHeight / 2 ) ; Logger . debug ( "centered matchX {}, matchY {}" , matchX , matchY ) ; double offsetX = ( imageWidth / 2 ) - matchX ; double offsetY = ( imageHeight / 2 ) - matchY ; Logger . debug ( "offsetX {}, offsetY {}" , offsetX , offsetY ) ; offsetY *= - 1 ; Logger . debug ( "negated offsetX {}, offsetY {}" , offsetX , offsetY ) ; Location unitsPerPixel = camera . getUnitsPerPixel ( ) ; offsetX *= unitsPerPixel . getX ( ) ; offsetY *= unitsPerPixel . getY ( ) ; Logger . debug ( "final, in camera units offsetX {}, offsetY {}" , offsetX , offsetY ) ; return new Location ( unitsPerPixel . getUnits ( ) , offsetX , offsetY , 0 , 0 ) ; }
private Location getVisionOffsets ( Head head , Location pickLocation ) throws Exception { Logger . debug ( "getVisionOffsets({}, {})" , head . getName ( ) , pickLocation ) ; Camera camera = null ; code_block = ForStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; head . moveToSafeZ ( ) ; Logger . debug ( "Move camera to pick location." ) ; camera . moveTo ( pickLocation ) ; VisionProvider visionProvider = camera . getVisionProvider ( ) ; Rectangle aoi = getVision ( ) . getAreaOfInterest ( ) ; Logger . debug ( "Perform template match." ) ; Point [ ] matchingPoints = visionProvider . locateTemplateMatches ( aoi . getX ( ) , aoi . getY ( ) , aoi . getWidth ( ) , aoi . getHeight ( ) , 0 , 0 , vision . getTemplateImage ( ) ) ; Point match = matchingPoints [ 0 ] ; double imageWidth = camera . getWidth ( ) ; double imageHeight = camera . getHeight ( ) ; double templateWidth = vision . getTemplateImage ( ) . getWidth ( ) ; double templateHeight = vision . getTemplateImage ( ) . getHeight ( ) ; double matchX = match . x ; double matchY = match . y ; Logger . debug ( "matchX {}, matchY {}" , matchX , matchY ) ; matchX += ( templateWidth / 2 ) ; matchY += ( templateHeight / 2 ) ; Logger . debug ( "centered matchX {}, matchY {}" , matchX , matchY ) ; double offsetX = ( imageWidth / 2 ) - matchX ; double offsetY = ( imageHeight / 2 ) - matchY ; Logger . debug ( "offsetX {}, offsetY {}" , offsetX , offsetY ) ; offsetY *= - 1 ; Logger . debug ( "negated offsetX {}, offsetY {}" , offsetX , offsetY ) ; Location unitsPerPixel = camera . getUnitsPerPixel ( ) ; offsetX *= unitsPerPixel . getX ( ) ; offsetY *= unitsPerPixel . getY ( ) ; Logger . debug ( "final, in camera units offsetX {}, offsetY {}" , offsetX , offsetY ) ; return new Location ( unitsPerPixel . getUnits ( ) , offsetX , offsetY , 0 , 0 ) ; }
private Location getVisionOffsets ( Head head , Location pickLocation ) throws Exception { Logger . debug ( "getVisionOffsets({}, {})" , head . getName ( ) , pickLocation ) ; Camera camera = null ; code_block = ForStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; head . moveToSafeZ ( ) ; Logger . debug ( "Move camera to pick location." ) ; camera . moveTo ( pickLocation ) ; VisionProvider visionProvider = camera . getVisionProvider ( ) ; Rectangle aoi = getVision ( ) . getAreaOfInterest ( ) ; Logger . debug ( "Perform template match." ) ; Point [ ] matchingPoints = visionProvider . locateTemplateMatches ( aoi . getX ( ) , aoi . getY ( ) , aoi . getWidth ( ) , aoi . getHeight ( ) , 0 , 0 , vision . getTemplateImage ( ) ) ; Point match = matchingPoints [ 0 ] ; double imageWidth = camera . getWidth ( ) ; double imageHeight = camera . getHeight ( ) ; double templateWidth = vision . getTemplateImage ( ) . getWidth ( ) ; double templateHeight = vision . getTemplateImage ( ) . getHeight ( ) ; double matchX = match . x ; double matchY = match . y ; Logger . debug ( "matchX {}, matchY {}" , matchX , matchY ) ; matchX += ( templateWidth / 2 ) ; matchY += ( templateHeight / 2 ) ; Logger . debug ( "centered matchX {}, matchY {}" , matchX , matchY ) ; double offsetX = ( imageWidth / 2 ) - matchX ; double offsetY = ( imageHeight / 2 ) - matchY ; Logger . debug ( "offsetX {}, offsetY {}" , offsetX , offsetY ) ; offsetY *= - 1 ; Logger . debug ( "negated offsetX {}, offsetY {}" , offsetX , offsetY ) ; Location unitsPerPixel = camera . getUnitsPerPixel ( ) ; offsetX *= unitsPerPixel . getX ( ) ; offsetY *= unitsPerPixel . getY ( ) ; Logger . debug ( "final, in camera units offsetX {}, offsetY {}" , offsetX , offsetY ) ; return new Location ( unitsPerPixel . getUnits ( ) , offsetX , offsetY , 0 , 0 ) ; }
public void test() { try { validatorResults = validator . validate ( ) ; } catch ( ValidatorException e ) { log . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { channel . close ( ) ; } catch ( IOException e ) { LOGGER . error ( ( ) -> "Error closing channel at: " + remoteAddress , e ) ; } }
public void test() { if ( ! routingContext . response ( ) . closed ( ) && ! routingContext . response ( ) . ended ( ) ) { routingContext . response ( ) . setStatusCode ( statusCode ) ; log . debug ( "[{}] Response: filename = {}" , routingContext . get ( "request-id" ) , filename ) ; routingContext . response ( ) . putHeader ( HttpHeaderNames . CONTENT_TYPE , contentType ) . sendFile ( filename ) ; } else-if ( routingContext . response ( ) . ended ( ) ) { log . warn ( "[{}] Response: already ended!" , routingContext . get ( "request-id" ) . toString ( ) ) ; } }
public void waitSensorActive ( @ Nonnull Sensor [ ] mSensors ) { log . debug ( "waitSensorActive[] starts" ) ; waitSensorState ( mSensors , Sensor . ACTIVE ) ; }
public void test() { try { File cacheDir = new File ( failoverDir ) ; code_block = IfStatement ; File [ ] files = cacheDir . listFiles ( ) ; code_block = IfStatement ; } catch ( Throwable e ) { NAMING_LOGGER . error ( "[NA] failed to backup file on startup." , e ) ; } }
public void createView ( String name , String tableName , String ... columnNames ) throws SQLException { code_block = IfStatement ; StringBuilder statement = new StringBuilder ( ) ; statement . append ( "CREATE VIEW " ) ; statement . append ( quoteCaseSensitive ( name ) ) ; statement . append ( " (" ) ; statement . append ( Arrays . stream ( columnNames ) . collect ( Collectors . joining ( "\", \"" , "\"" , "\"" ) ) ) ; statement . append ( ") AS SELECT " ) ; statement . append ( Arrays . stream ( columnNames ) . collect ( Collectors . joining ( "\", \"" , "\"" , "\"" ) ) ) ; statement . append ( " FROM " ) ; statement . append ( quoteCaseSensitive ( tableName ) ) ; Statement stmt = conn . createStatement ( ) ; String statementStr = statement . toString ( ) ; log . debug ( "Creating view {} in {} with statement {}" , name , this . name , statementStr ) ; stmt . execute ( statementStr ) ; }
public void test() { try { iam . projects ( ) . serviceAccounts ( ) . keys ( ) . get ( keyName ) . execute ( ) ; return true ; } catch ( GoogleJsonResponseException e ) { LOG . info ( "Couldn't check existence of key {} {} {}" , keyName , e . getStatusCode ( ) , e . getDetails ( ) . toPrettyString ( ) , e ) ; code_block = IfStatement ; throw e ; } }
public void test() { try { inputAsJSON = writer . writeValueAsString ( message ) ; logger . info ( "LCM Kit input message follows: {}" , inputAsJSON ) ; } catch ( JsonProcessingException e ) { logger . error ( "Error in logging LCM Message" , e ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "AtlasLdapAuthenticationProvider{" + "ldapURL='" + ldapURL + '\'' + ", ldapUserDNPattern='" + ldapUserDNPattern + '\'' + ", ldapGroupSearchBase='" + ldapGroupSearchBase + '\'' + ", ldapGroupSearchFilter='" + ldapGroupSearchFilter + '\'' + ", ldapGroupRoleAttribute='" + ldapGroupRoleAttribute + '\'' + ", ldapBindDN='" + ldapBindDN + '\'' + ", ldapDefaultRole='" + ldapDefaultRole + '\'' + ", ldapUserSearchFilter='" + ldapUserSearchFilter + '\'' + ", ldapReferral='" + ldapReferral + '\'' + ", ldapBase='" + ldapBase + '\'' + ", groupsFromUGI=" + groupsFromUGI + '}' ) ; } }
public void test() { try { Configuration configuration = ApplicationProperties . get ( ) ; Properties properties = ConfigurationConverter . getProperties ( configuration . subset ( "atlas.authentication.method.ldap" ) ) ; ldapURL = properties . getProperty ( "url" ) ; ldapUserDNPattern = properties . getProperty ( "userDNpattern" ) ; ldapGroupSearchBase = properties . getProperty ( "groupSearchBase" ) ; ldapGroupSearchFilter = properties . getProperty ( "groupSearchFilter" ) ; ldapGroupRoleAttribute = properties . getProperty ( "groupRoleAttribute" ) ; ldapBindDN = properties . getProperty ( "bind.dn" ) ; ldapBindPassword = properties . getProperty ( "bind.password" ) ; ldapDefaultRole = properties . getProperty ( "default.role" ) ; ldapUserSearchFilter = properties . getProperty ( "user.searchfilter" ) ; ldapReferral = properties . getProperty ( "referral" ) ; ldapBase = properties . getProperty ( "base.dn" ) ; groupsFromUGI = configuration . getBoolean ( "atlas.authentication.method.ldap.ugi-groups" , true ) ; code_block = IfStatement ; } catch ( Exception e ) { LOG . error ( "Exception while setLdapProperties" , e ) ; } }
public void test() { switch ( problem . getSeverity ( ) ) { case ERROR : case FATAL : throw new BootstrapMavenException ( "Settings problem encountered at " + problem . getLocation ( ) , problem . getException ( ) ) ; default : log . warn ( "Settings problem encountered at " + problem . getLocation ( ) , problem . getException ( ) ) ; } }
@ Override public < K , V > Map < K , V > createLRUWeakCache ( int maximumCacheSize ) { LOG . trace ( "Creating LRUWeakCache with maximumCacheSize: {}" , maximumCacheSize ) ; return new SimpleLRUCache < > ( maximumCacheSize ) ; }
public void test() { if ( handler == null ) { logger . debug ( HANDLER_IS_NULL ) ; return ; } }
@ Override public List < AccountReport > listReports ( String accountId , String report ) throws IOException { code_block = IfStatement ; LOG . debug ( "Retrieving information about all " + report + " reports for account " + accountId ) ; String url = buildCanvasUrl ( "accounts/" + accountId + "/reports/" + report , Collections . emptyMap ( ) ) ; return getListFromCanvas ( url ) ; }
public void test() { if ( fieldBean != null ) { columnNames . add ( fieldBean . getColumnName ( ) ) ; } else-if ( keyBean != null ) { columnNames . add ( keyBean . getColumnName ( ) ) ; } else { LOG . warn ( "{} field is ignored, couldn't find relevant field in the persistent mapping" , field ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "setProtocolManagerFactoryStr(" + protocolManagerFactoryStr + ")" ) ; } }
public void test() { if ( configExpirationTime != tokenDefinedExirationTime ) { String msg = String . format ( "The configured expiration time for the token = '%s' changed from when the token was created." , token . getId ( ) ) ; logger . warn ( msg ) ; } }
public boolean pkgShow ( List < String > packages ) { boolean cmdOk = true ; code_block = IfStatement ; StringBuilder sb = new StringBuilder ( ) ; sb . append ( "****************************************" ) ; code_block = ForStatement ; log . info ( sb . toString ( ) ) ; return cmdOk ; }
public List findByExample ( NmbNotiz instance ) { log . debug ( "finding NmbNotiz instance by example" ) ; code_block = TryStatement ;  }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.NmbNotiz" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.NmbNotiz" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
public void test() { try { SerializingTranscoder transcoder = new SerializingTranscoder ( config . getMaxObjectSize ( ) ) ; transcoder . setCompressionThreshold ( Integer . MAX_VALUE ) ; OperationQueueFactory opQueueFactory ; int maxQueueSize = config . getMaxOperationQueueSize ( ) ; code_block = IfStatement ; String hostsStr = config . getHosts ( ) ; ConnectionFactory connectionFactory = new MemcachedConnectionFactoryBuilder ( ) . setProtocol ( ConnectionFactoryBuilder . Protocol . BINARY ) . setHashAlg ( DefaultHashAlgorithm . FNV1A_64_HASH ) . setLocatorType ( ConnectionFactoryBuilder . Locator . CONSISTENT ) . setDaemon ( true ) . setFailureMode ( FailureMode . Redistribute ) . setTranscoder ( transcoder ) . setShouldOptimize ( true ) . setOpQueueMaxBlockTime ( config . getTimeout ( ) ) . setOpTimeout ( config . getTimeout ( ) ) . setReadBufferSize ( config . getReadBufferSize ( ) ) . setOpQueueFactory ( opQueueFactory ) . build ( ) ; return new MemcachedCache ( new MemcachedClient ( new MemcachedConnectionFactory ( connectionFactory ) , getResolvedAddrList ( hostsStr ) ) , config , memcachedPrefix , timeToLive ) ; } catch ( IOException e ) { logger . error ( "Unable to create MemcachedCache instance." , e ) ; throw Throwables . propagate ( e ) ; } }
public void test() { if ( ! ( resultSeq . isEmpty ( ) || Type . subTypeOf ( resultSeq . getItemType ( ) , Type . NODE ) ) ) { throw new EXistException ( "select expression should evaluate to a node-set; got " + Type . getTypeName ( resultSeq . getItemType ( ) ) ) ; } }
public void test() { { final String startTime = TimeUtil . getTimeWrtSystemTime ( - 20 ) ; String endTime = TimeUtil . getTimeWrtSystemTime ( 4000 ) ; bundles [ 1 ] . setProcessPeriodicity ( 1 , TimeUnit . days ) ; bundles [ 1 ] . setOutputFeedPeriodicity ( 1 , TimeUnit . days ) ; bundles [ 1 ] . setProcessValidity ( startTime , endTime ) ; bundles [ 1 ] . submitBundle ( prism ) ; AssertUtil . assertSucceeded ( cluster3 . getProcessHelper ( ) . schedule ( bundles [ 1 ] . getProcessData ( ) ) ) ; InstanceUtil . waitTillInstancesAreCreated ( cluster3OC , bundles [ 1 ] . getProcessData ( ) , 0 , 10 ) ; String oldBundleId = OozieUtil . getLatestBundleID ( cluster3OC , bundles [ 1 ] . getProcessName ( ) , EntityType . PROCESS ) ; waitForProcessToReachACertainState ( cluster3OC , bundles [ 1 ] , Job . Status . RUNNING ) ; List < String > oldNominalTimes = OozieUtil . getActionsNominalTime ( cluster3OC , oldBundleId , EntityType . PROCESS ) ; LOGGER . info ( "original process: " + Util . prettyPrintXml ( bundles [ 1 ] . getProcessData ( ) ) ) ; ProcessMerlin updatedProcess = new ProcessMerlin ( bundles [ 1 ] . getProcessObject ( ) ) ; updatedProcess . setFrequency ( new Frequency ( "5" , TimeUnit . minutes ) ) ; LOGGER . info ( "updated process: " + updatedProcess ) ; ServiceResponse response = prism . getProcessHelper ( ) . update ( bundles [ 1 ] . getProcessData ( ) , updatedProcess . toString ( ) ) ; AssertUtil . assertSucceeded ( response ) ; InstanceUtil . waitTillInstancesAreCreated ( cluster3OC , bundles [ 1 ] . getProcessData ( ) , 1 , 10 ) ; String prismString = dualComparison ( prism , cluster2 , bundles [ 1 ] . getProcessData ( ) ) ; Assert . assertEquals ( new ProcessMerlin ( prismString ) . getFrequency ( ) , new Frequency ( "" + 5 , TimeUnit . minutes ) ) ; dualComparison ( prism , cluster3 , bundles [ 1 ] . getProcessData ( ) ) ; OozieUtil . verifyNewBundleCreation ( cluster3OC , oldBundleId , oldNominalTimes , bundles [ 1 ] . getProcessData ( ) , true , true ) ; AssertUtil . checkNotStatus ( cluster2OC , EntityType . PROCESS , bundles [ 1 ] , Job . Status . RUNNING ) ; } }
public void test() { { final String startTime = TimeUtil . getTimeWrtSystemTime ( - 20 ) ; String endTime = TimeUtil . getTimeWrtSystemTime ( 4000 ) ; bundles [ 1 ] . setProcessPeriodicity ( 1 , TimeUnit . days ) ; bundles [ 1 ] . setOutputFeedPeriodicity ( 1 , TimeUnit . days ) ; bundles [ 1 ] . setProcessValidity ( startTime , endTime ) ; bundles [ 1 ] . submitBundle ( prism ) ; AssertUtil . assertSucceeded ( cluster3 . getProcessHelper ( ) . schedule ( bundles [ 1 ] . getProcessData ( ) ) ) ; InstanceUtil . waitTillInstancesAreCreated ( cluster3OC , bundles [ 1 ] . getProcessData ( ) , 0 , 10 ) ; String oldBundleId = OozieUtil . getLatestBundleID ( cluster3OC , bundles [ 1 ] . getProcessName ( ) , EntityType . PROCESS ) ; waitForProcessToReachACertainState ( cluster3OC , bundles [ 1 ] , Job . Status . RUNNING ) ; List < String > oldNominalTimes = OozieUtil . getActionsNominalTime ( cluster3OC , oldBundleId , EntityType . PROCESS ) ; LOGGER . info ( "original process: " + Util . prettyPrintXml ( bundles [ 1 ] . getProcessData ( ) ) ) ; ProcessMerlin updatedProcess = new ProcessMerlin ( bundles [ 1 ] . getProcessObject ( ) ) ; updatedProcess . setFrequency ( new Frequency ( "5" , TimeUnit . minutes ) ) ; LOGGER . info ( "updated process: " + updatedProcess ) ; ServiceResponse response = prism . getProcessHelper ( ) . update ( bundles [ 1 ] . getProcessData ( ) , updatedProcess . toString ( ) ) ; AssertUtil . assertSucceeded ( response ) ; InstanceUtil . waitTillInstancesAreCreated ( cluster3OC , bundles [ 1 ] . getProcessData ( ) , 1 , 10 ) ; String prismString = dualComparison ( prism , cluster2 , bundles [ 1 ] . getProcessData ( ) ) ; Assert . assertEquals ( new ProcessMerlin ( prismString ) . getFrequency ( ) , new Frequency ( "" + 5 , TimeUnit . minutes ) ) ; dualComparison ( prism , cluster3 , bundles [ 1 ] . getProcessData ( ) ) ; OozieUtil . verifyNewBundleCreation ( cluster3OC , oldBundleId , oldNominalTimes , bundles [ 1 ] . getProcessData ( ) , true , true ) ; AssertUtil . checkNotStatus ( cluster2OC , EntityType . PROCESS , bundles [ 1 ] , Job . Status . RUNNING ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( noSuchFolderException , noSuchFolderException ) ; } }
@ Override public void stsRevocationRegistryInstantiationError ( ) { logger . warn ( "Error instantiating revocation registry class - using default registry" ) ; }
public void test() { try { Release release = componentClient . getReleaseById ( releaseId , user ) ; JsonNode input = OBJECT_MAPPER . readValue ( request . getParameter ( SPDX_LICENSE_INFO ) , JsonNode . class ) ; JsonNode licenesIdsNode = input . get ( LICENSE_IDS ) ; code_block = IfStatement ; licenesIdsNode = input . get ( "otherLicenseIds" ) ; code_block = IfStatement ; result = componentClient . updateRelease ( release , user ) ; } catch ( TException | IOException e ) { log . error ( "Cannot write license info into release " + releaseId + "." , e ) ; response . setProperty ( ResourceResponse . HTTP_STATUS_CODE , "500" ) ; } }
public void test() { try { response = new ResponseDTO ( faqService . getFAQSByWidget ( widgetId , domainId ) ) ; } catch ( ServiceException e ) { LOGGER . error ( e . getMessage ( ) ) ; return complianceService . formatException ( e ) ; } }
public void test() { try { int count = 1 ; boolean reachable = isGithubReachable ( containerId ) ; code_block = WhileStatement ; String cmd = String . format ( "cd %s; bash get_unzip.sh %s" , TMP_DIR , repoLink ) ; runCmd ( containerId , cmd ) ; } catch ( Exception e ) { log . error ( "Error configuring environment" , e ) ; } }
public void test() { if ( ! responses . containsKey ( requestId ) ) { logger . warn ( String . format ( "No queue found in the response map: %s" , requestId ) ) ; return ; } }
public void test() { if ( queue != null ) { queue . put ( response ) ; } else { logger . error ( String . format ( "No queue found in the response map: %s" , requestId ) ) ; } }
public void test() { try { final ArrayBlockingQueue < Object > queue = responses . get ( requestId ) ; code_block = IfStatement ; } catch ( InterruptedException e ) { logger . error ( "Error reading the queue in the response map." , e ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "addDoubleField fieldName: {}; value: {}" , fieldName , value ) ; } }
@ Override public void move ( MoveOperationContext moveContext ) throws LdapException { LOG . debug ( ">>> Entering into the Administrative Interceptor, moveRequest" ) ; Entry entry = moveContext . getOriginalEntry ( ) ; Attribute adminPoint = entry . get ( directoryService . getAtProvider ( ) . getAdministrativeRole ( ) ) ; code_block = IfStatement ; String message = "Cannot move an Administrative Point in the current version" ; LOG . error ( message ) ; throw new LdapUnwillingToPerformException ( message ) ; }
public void test() { if ( componentId . missing ( ) ) { LOGGER . warn ( "Component Id not found from disk component metadata" ) ; } }
public void test() { if ( verbose ) { LOG . debug ( "-" ) ; } }
public void test() { if ( verbose ) { LOG . debug ( "rstate:" + shard . getValue ( ) . getStr ( ZkStateReader . STATE_PROP ) + " live:" + clusterState . liveNodesContain ( shard . getValue ( ) . getNodeName ( ) ) ) ; } }
public void test() { if ( verbose ) { LOG . debug ( "no one is recovering" ) ; } }
public void test() { if ( verbose ) { LOG . debug ( "no one is recovering" ) ; } }
public void test() { try { boolean cont = true ; int cnt = 0 ; code_block = WhileStatement ; } finally { LOG . info ( "Exiting solr wait" ) ; } }
@ Override public void setup ( ProfilerPluginSetupContext context ) { DubboConfiguration config = new DubboConfiguration ( context . getConfig ( ) ) ; code_block = IfStatement ; logger . info ( "{} config:{}" , this . getClass ( ) . getSimpleName ( ) , config ) ; code_block = IfStatement ; logger . info ( "Adding Dubbo transformers" ) ; this . addTransformers ( ) ; }
public void test() { if ( ! context . registerApplicationType ( DubboConstants . DUBBO_PROVIDER_SERVICE_TYPE ) ) { logger . info ( "Application type [{}] already set, skipping [{}] registration." , context . getApplicationType ( ) , DubboConstants . DUBBO_PROVIDER_SERVICE_TYPE ) ; } }
public void test() { if ( pm != null && propertyMappings . contains ( pm ) ) { propertyMappings . remove ( pm ) ; LOG . debug ( "Removed mapping to term " + pm . getTerm ( ) . qualifiedName ( ) ) ; } }
public void test() { if ( addProp . getLocalName ( ) . equals ( INVOCATION_TYPE ) ) { final String invocationType = addProp . getTextContent ( ) . trim ( ) ; LOG . debug ( "InvocationType property: {}" , invocationType ) ; return invocationType ; } }
private void checkSystemRequestDTO ( final SystemRequestDTO system , final String origin ) { logger . debug ( "checkSystemRequestDTO started..." ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; final int validatedPort = system . getPort ( ) . intValue ( ) ; code_block = IfStatement ; }
private void adjustDhModulus ( DHEServerKeyExchangeMessage message ) { tlsContext . setServerDhModulus ( new BigInteger ( 1 , message . getModulus ( ) . getValue ( ) ) ) ; LOGGER . debug ( "Dh Modulus: " + tlsContext . getServerDhModulus ( ) ) ; }
public void test() { try { cis . close ( ) ; } catch ( final IOException ioe ) { LOG . warn ( ioe . getMessage ( ) , ioe ) ; } }
@ Override public Boolean isDuplicateFile ( String filepath ) { logger . info ( "No metadata connection" ) ; return false ; }
public void test() { if ( accuracyThreshold > 0 ) { logger . debug ( "Location accuracy is below required threshold: {}<={}" , accuracy , accuracyThreshold ) ; } else { logger . debug ( "Location accuracy threshold check is disabled." ) ; } }
public void test() { if ( accuracyThreshold > 0 ) { logger . debug ( "Location accuracy is below required threshold: {}<={}" , accuracy , accuracyThreshold ) ; } else { logger . debug ( "Location accuracy threshold check is disabled." ) ; } }
public void test() { if ( accuracyThreshold >= accuracy || accuracyThreshold . intValue ( ) == 0 ) { code_block = IfStatement ; String regionName = ConfigHelper . getRegionName ( currentConfig ) ; PointType center = ConfigHelper . getRegionCenterLocation ( currentConfig ) ; State newLocation = message . getTrackerLocation ( ) ; code_block = IfStatement ; } else { logger . debug ( "Skip update as location accuracy is above required threshold: {}>{}" , accuracy , accuracyThreshold ) ; } }
public void test() { if ( resource == null || resource . isEmpty ( ) ) { logger . warn ( "This resource has no resource identifier in the xacml response results!" ) ; } else-if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Checking: {}" , resource ) ; } }
public void test() { if ( resource == null || resource . isEmpty ( ) ) { logger . warn ( "This resource has no resource identifier in the xacml response results!" ) ; } else-if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Checking: {}" , resource ) ; } }
public void test() { if ( ! conflictingNodeIds . isEmpty ( ) ) { final Set < String > fullNodeIdDescriptions = conflictingNodeIds . stream ( ) . map ( NodeIdentifier :: getFullDescription ) . collect ( Collectors . toSet ( ) ) ; logger . warn ( "New Node {} was registered for this cluster, but this Node Identifier conflicts with {} others: {}; " + "each of these conflicting Node Identifiers will be removed from the cluster" , nodeIdentifier . getFullDescription ( ) , fullNodeIdDescriptions . size ( ) , fullNodeIdDescriptions ) ; conflictingNodeIds . forEach ( uuid -> removeNode ( uuid ) ) ; } }
public void test() { for ( @ SuppressWarnings ( "rawtypes" ) Metric metric : record . metrics ( ) ) { LOG . debug ( "Metric name:" + metric . name ( ) + " metric value:" + metric . value ( ) ) ; } }
public void test() { try { MetricsRecord record = ( MetricsRecord ) future . get ( ) ; code_block = IfStatement ; } catch ( InterruptedException ie ) { LOG . warn ( ie ) ; Thread . currentThread ( ) . interrupt ( ) ; } catch ( ExecutionException ee ) { LOG . warn ( ee . getCause ( ) ) ; } }
public void test() { try { MetricsRecord record = ( MetricsRecord ) future . get ( ) ; code_block = IfStatement ; } catch ( InterruptedException ie ) { LOG . warn ( ie ) ; Thread . currentThread ( ) . interrupt ( ) ; } catch ( ExecutionException ee ) { LOG . warn ( ee . getCause ( ) ) ; } }
public void test() { try { MetaServerConsoleService . PreviousPrimaryDcMessage previousPrimaryDcMessage = commandFuture . get ( ) ; logger . info ( "[doPrevPrimaryDcMigrate][result]{},{},{},{}" , cluster , shard , dc , previousPrimaryDcMessage ) ; shardMigrationResult . setPreviousPrimaryDcMessage ( previousPrimaryDcMessage ) ; shardMigrationResult . updateStepResult ( ShardMigrationStep . MIGRATE_PREVIOUS_PRIMARY_DC , true , previousPrimaryDcMessage == null ? LogUtils . info ( "Succeed, return message null" ) : previousPrimaryDcMessage . getMessage ( ) ) ; } catch ( Exception e ) { logger . error ( "[doPrevPrimaryDcMigrate][fail]" , e ) ; shardMigrationResult . updateStepResult ( ShardMigrationStep . MIGRATE_PREVIOUS_PRIMARY_DC , true , LogUtils . error ( "Ignored:" + e . getMessage ( ) ) ) ; } }
public void test() { try ( Checkpoint checkpoint = Checkpoint . create ( rocksdb ) ) { String tempPath = targetPath + "_temp" ; File tempFile = new File ( tempPath ) ; FileUtils . deleteDirectory ( tempFile ) ; LOG . debug ( "Deleted temp directory {}" , tempFile ) ; FileUtils . forceMkdir ( tempFile . getParentFile ( ) ) ; checkpoint . createCheckpoint ( tempPath ) ; File snapshotFile = new File ( targetPath ) ; FileUtils . deleteDirectory ( snapshotFile ) ; LOG . debug ( "Deleted stale directory {}" , snapshotFile ) ; code_block = IfStatement ; } catch ( Exception e ) { throw new BackendException ( "Failed to create checkpoint at path %s" , e , targetPath ) ; } }
public void test() { try ( Checkpoint checkpoint = Checkpoint . create ( rocksdb ) ) { String tempPath = targetPath + "_temp" ; File tempFile = new File ( tempPath ) ; FileUtils . deleteDirectory ( tempFile ) ; LOG . debug ( "Deleted temp directory {}" , tempFile ) ; FileUtils . forceMkdir ( tempFile . getParentFile ( ) ) ; checkpoint . createCheckpoint ( tempPath ) ; File snapshotFile = new File ( targetPath ) ; FileUtils . deleteDirectory ( snapshotFile ) ; LOG . debug ( "Deleted stale directory {}" , snapshotFile ) ; code_block = IfStatement ; } catch ( Exception e ) { throw new BackendException ( "Failed to create checkpoint at path %s" , e , targetPath ) ; } }
public void test() { try { checkpointIds [ i ] = Integer . valueOf ( status [ i ] . getPath ( ) . getName ( ) ) ; } catch ( Throwable x ) { LOG . warn ( "Path " + status [ i ] . getPath ( ) . toString ( ) + " is not a valid checkpoint path, just remove it" ) ; } }
public void test() { if ( ! fs . delete ( status [ i ] . getPath ( ) , true ) ) { LOG . warn ( "Delete path " + status [ i ] . getPath ( ) + " failed " ) ; } else { LOG . info ( "Delete old checkpoint " + status [ i ] . getPath ( ) + " failed " ) ; } }
public void test() { if ( ! fs . delete ( status [ i ] . getPath ( ) , true ) ) { LOG . warn ( "Delete path " + status [ i ] . getPath ( ) + " failed " ) ; } else { LOG . info ( "Delete old checkpoint " + status [ i ] . getPath ( ) + " failed " ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Throwable x ) { LOG . warn ( "Delete path " + status [ i ] . getPath ( ) + " failed " , x ) ; } }
public void test() { if ( receiveBufferSize != expectedRecvBufferSize ) { LOG . warn ( "receiveBufferSize (SO_RCVBUF) for input {} (channel {}) should be {} but is {}." , input , channel , expectedRecvBufferSize , receiveBufferSize ) ; } }
public void test() { if ( future . isSuccess ( ) ) { final Channel channel = future . channel ( ) ; channelReference . set ( channel ) ; LOG . debug ( "Started channel {}" , channel ) ; final ServerSocketChannelConfig channelConfig = ( ServerSocketChannelConfig ) channel . config ( ) ; final int receiveBufferSize = channelConfig . getReceiveBufferSize ( ) ; code_block = IfStatement ; } else { LOG . warn ( "Failed to start channel for input {}" , input , future . cause ( ) ) ; } }
public void test() { try { registerMBean ( bean , mBeanServer , objectName ) ; } catch ( Exception e ) { LOG . error ( "Failed to register in JMX bean " + bean + " at " + objectName + ". " + e , e ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( AssetCategoryServiceUtil . class , "getVocabularyCategoriesDisplay" , _getVocabularyCategoriesDisplayParameterTypes20 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , vocabularyId , start , end , orderByComparator ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . asset . kernel . model . AssetCategoryDisplay ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( StringUtils . isEmpty ( organisationIdentification ) ) { LOGGER . error ( "organisationIdentification is empty or null" ) ; } }
public void test() { if ( StringUtils . isEmpty ( userName ) ) { LOGGER . error ( "userName is empty or null" ) ; } }
public void test() { if ( StringUtils . isEmpty ( applicationName ) ) { LOGGER . error ( "applicationName is empty or null" ) ; } }
public void test() { switch ( jainmgcpresponseevent . getObjectIdentifier ( ) ) { case Constants . RESP_ENDPOINT_CONFIGURATION : responseReceived = true ; break ; default : logger . warn ( "This RESPONSE is unexpected " + jainmgcpresponseevent ) ; break ; } }
@ Override public void init ( ) throws Exception { this . checkTrashedResourceDiskFolder ( this . getResourceTrashRootDiskSubFolder ( ) ) ; _logger . debug ( "{} ready" , this . getClass ( ) . getName ( ) ) ; _logger . debug ( "Folder trashed resources: {}" , this . getResourceTrashRootDiskSubFolder ( ) ) ; }
@ Override public void init ( ) throws Exception { this . checkTrashedResourceDiskFolder ( this . getResourceTrashRootDiskSubFolder ( ) ) ; _logger . debug ( "{} ready" , this . getClass ( ) . getName ( ) ) ; _logger . debug ( "Folder trashed resources: {}" , this . getResourceTrashRootDiskSubFolder ( ) ) ; }
public void test() { if ( dueTime > maximumDueTime ) { logger . warn ( "The customer ({})'s dueTime ({}) was automatically reduced" + " to maximumDueTime ({}) because of the depot's dueTime ({})." , customer , dueTime , maximumDueTime , depot . getDueTime ( ) ) ; dueTime = maximumDueTime ; } }
public void test() { try { Collection < ? > c = ( Collection ) literal ; Iterator iterator = c . iterator ( ) ; List < Object > cast = new ArrayList < Object > ( ) ; code_block = WhileStatement ; return cast ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; } }
public void test() { { jcloudsLocation = ( JcloudsLocation ) managementContext . getLocationRegistry ( ) . getLocationManaged ( AWS_EC2_LOCATION_SPEC ) ; machine = createEc2Machine ( ImmutableMap . < String , Object > of ( ) ) ; assertSshable ( machine ) ; String locationAddress = machine . getAddress ( ) . getHostName ( ) ; InetAddress address = machine . getAddress ( ) ; Set < String > publicAddresses = machine . getPublicAddresses ( ) ; Set < String > privateAddresses = machine . getPrivateAddresses ( ) ; String subnetIp = machine . getSubnetIp ( ) ; String subnetHostname = machine . getSubnetHostname ( ) ; String hostname = machine . getHostname ( ) ; String msg = "locationAddress=" + locationAddress + "; address=" + address + "; publicAddrs=" + publicAddresses + "; privateAddrs=" + privateAddresses + "; subnetIp=" + subnetIp + "; hostname=" + hostname + "; subnetHostname=" + subnetHostname ; LOG . info ( "node: " + msg ) ; assertReachable ( machine , locationAddress , msg ) ; assertReachableFromMachine ( machine , locationAddress , msg ) ; assertReachable ( machine , address , msg ) ; assertTrue ( publicAddresses . size ( ) > 0 , msg ) ; code_block = ForStatement ; assertTrue ( privateAddresses . size ( ) > 0 , msg ) ; code_block = ForStatement ; assertNotNull ( subnetIp , msg ) ; assertReachableFromMachine ( machine , subnetIp , msg ) ; assertNotNull ( hostname , msg ) ; assertReachableFromMachine ( machine , hostname , msg ) ; assertNotNull ( subnetHostname , msg ) ; assertReachable ( machine , subnetHostname , msg ) ; assertReachableFromMachine ( machine , subnetHostname , msg ) ; } }
public void test() { try ( final Collection coll = broker . openCollection ( XmldbURI . createInternal ( protectColl ) , collectionLockMode ) ) { docs = new DefaultDocumentSet ( ) ; coll . allDocs ( broker , docs , true , lockedDocuments , documentLockMode ) ; return lockedDocuments ; } catch ( final LockException e ) { LOG . warn ( "Deadlock detected. Starting over again. Docs: {}; locked: {}. Cause: {}" , docs . getDocumentCount ( ) , lockedDocuments . size ( ) , e . getMessage ( ) ) ; lockedDocuments . unlock ( ) ; } }
public void test() { try { session = sessionFactory . openSession ( ) ; tokenDto = ( EnrollmentTokenDto ) session . createQuery ( "from EnrollmentTokenDto " + " where enrollmentToken= :enrollmentToken" + " ORDER BY id DESC" ) . setString ( "enrollmentToken" , token ) . setMaxResults ( 1 ) . uniqueResult ( ) ; code_block = IfStatement ; } catch ( Exception e ) { LOGGER . error ( "StudyMetaDataDao - isValidToken() :: ERROR" , e ) ; } finally { code_block = IfStatement ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Parameter '" + paramName + "' rejected in component '" + inputComponent . getClientId ( ) + "' in ComponentValidator '" + this . getClass ( ) + "'" ) ; } }
public Structure update ( final Structure structure ) { log . debug ( "update() - structure: {}" , structure ) ; code_block = IfStatement ; validateStructure ( structure ) ; return structureRepository . save ( structure ) ; }
@ Override public void init ( ) { super . init ( ) ; Integer portStartingPoint ; Object rawPort = getAllConfigBag ( ) . getStringKey ( PORT_FORWARD_MANAGER_STARTING_PORT . getName ( ) ) ; code_block = IfStatement ; portReserved . set ( portStartingPoint ) ; log . debug ( this + " set initial port to " + portStartingPoint ) ; }
@ Override public void stop ( ) { LOGGER . warn ( "Plugin::" + getClass ( ) . getSimpleName ( ) + "::stop" ) ; springCtxs . forEach ( context code_block = LoopStatement ; ) ; LOGGER . warn ( "Plugin:SpringAppLoader stoped.." ) ; }
public void test() { if ( protocol . equals ( "HTTP" ) ) { return Protocol . HTTP ; } else-if ( protocol . equals ( "HTTPS" ) ) { return Protocol . HTTPS ; } else { LOG . warn ( "Invalid protocol type {}." + "Avaiable proxy protocol type HTTP and HTTPS." , protocol ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( testName + ": status = " + statusCode ) ; } }
@ ApiOperation ( value = "get history by owner's id" ) @ GetMapping ( CommonConstants . PATH_LOGBOOK ) public LogbookOperationsResponseDto findHistoryById ( final @ PathVariable String id ) { LOGGER . debug ( "get logbook for owner with id :{}" , id ) ; ParameterChecker . checkParameter ( "Identifier is mandatory : " , id ) ; return service . findHistoryById ( buildUiHttpContext ( ) , id ) ; }
@ RequestMapping ( value = "/{id}" , method = RequestMethod . GET ) public RoleDto find ( @ PathVariable Long id ) { log . debug ( "find() - id: {}" , id ) ; return roleManagementService . findRole ( id ) ; }
public void test() { if ( connected . compareAndSet ( false , true ) ) { code_block = TryStatement ;  } else { logger . error ( "the channel can't be connected twice." ) ; } }
public void test() { try { catalogs = catalogService . latestInfraTemplates ( ) ; break ; } catch ( IOException e ) { log . info ( "Waiting for catalog service" ) ; code_block = TryStatement ;  } }
public void test() { try { code_block = IfStatement ; LOG . info ( "Starting gRPC server on address {}" , mRpcConnectAddress ) ; GrpcServerBuilder serverBuilder = GrpcServerBuilder . forAddress ( GrpcServerAddress . create ( mRpcConnectAddress . getHostName ( ) , mRpcBindAddress ) , ServerConfiguration . global ( ) , ServerUserState . global ( ) ) ; code_block = ForStatement ; mGrpcServer = serverBuilder . build ( ) . start ( ) ; LOG . info ( "Started gRPC server on address {}" , mRpcConnectAddress ) ; mGrpcServer . awaitTermination ( ) ; } catch ( IOException e ) { throw new RuntimeException ( e ) ; } }
public void test() { try { code_block = IfStatement ; LOG . info ( "Starting gRPC server on address {}" , mRpcConnectAddress ) ; GrpcServerBuilder serverBuilder = GrpcServerBuilder . forAddress ( GrpcServerAddress . create ( mRpcConnectAddress . getHostName ( ) , mRpcBindAddress ) , ServerConfiguration . global ( ) , ServerUserState . global ( ) ) ; code_block = ForStatement ; mGrpcServer = serverBuilder . build ( ) . start ( ) ; LOG . info ( "Started gRPC server on address {}" , mRpcConnectAddress ) ; mGrpcServer . awaitTermination ( ) ; } catch ( IOException e ) { throw new RuntimeException ( e ) ; } }
public void test() { try { KeyPairGenerator keyPairGenerator = null ; keyPairGenerator = KeyPairGenerator . getInstance ( alg ) ; keyPairGenerator . initialize ( keySize ) ; return keyPairGenerator . generateKeyPair ( ) ; } catch ( NoSuchAlgorithmException e ) { log . error ( "The provided algorithm is not found " , e ) ; } }
public void test() { try { MerkleEntityId id = fromContractId ( txn . getContractUpdateInstance ( ) . getContractID ( ) ) ; Timestamp expiry = lookupAccountExpiry ( id , view . accounts ( ) ) ; return usageEstimator . getContractUpdateTxFeeMatrices ( txn , expiry , sigUsage ) ; } catch ( Exception e ) { log . debug ( "Unable to deduce ContractUpdate usage for {}, using defaults" , txn . getTransactionID ( ) , e ) ; return FeeData . getDefaultInstance ( ) ; } }
public void test() { if ( e instanceof org . apache . airavata . model . error . InvalidRequestException ) { result . ire = ( org . apache . airavata . model . error . InvalidRequestException ) e ; result . setIreIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AiravataClientException ) { result . ace = ( org . apache . airavata . model . error . AiravataClientException ) e ; result . setAceIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AiravataSystemException ) { result . ase = ( org . apache . airavata . model . error . AiravataSystemException ) e ; result . setAseIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AuthorizationException ) { result . ae = ( org . apache . airavata . model . error . AuthorizationException ) e ; result . setAeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof org . apache . airavata . model . error . InvalidRequestException ) { result . ire = ( org . apache . airavata . model . error . InvalidRequestException ) e ; result . setIreIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AiravataClientException ) { result . ace = ( org . apache . airavata . model . error . AiravataClientException ) e ; result . setAceIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AiravataSystemException ) { result . ase = ( org . apache . airavata . model . error . AiravataSystemException ) e ; result . setAseIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AuthorizationException ) { result . ae = ( org . apache . airavata . model . error . AuthorizationException ) e ; result . setAeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof org . apache . airavata . model . error . InvalidRequestException ) { result . ire = ( org . apache . airavata . model . error . InvalidRequestException ) e ; result . setIreIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AiravataClientException ) { result . ace = ( org . apache . airavata . model . error . AiravataClientException ) e ; result . setAceIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AiravataSystemException ) { result . ase = ( org . apache . airavata . model . error . AiravataSystemException ) e ; result . setAseIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AuthorizationException ) { result . ae = ( org . apache . airavata . model . error . AuthorizationException ) e ; result . setAeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { try { fcall . sendResponse ( fb , msg , msgType , seqid ) ; } catch ( java . lang . Exception ex ) { _LOGGER . error ( "Exception writing to internal frame buffer" , ex ) ; fb . close ( ) ; } }
public void test() { if ( elementsOrder [ i ] . equals ( elementName ) ) { currentElementIndex = i ; break ; } }
public void test() { if ( lastElementIndex != - 1 && ( currentElementIndex < lastElementIndex ) ) { logger . warn ( "ElementOrderValidationFunction - '" + elementName + "' is placed before '" + lastElementName + "'" ) ; return false ; } }
public void test() { try { if ( ! Services . getInstance ( ) . isShuttingDown ( ) && ! Services . getInstance ( ) . isShutdownDone ( ) ) info . messageCallback ( topic , message ) ; } catch ( FrameworkException e ) { logger . error ( "Error during MQTT message callback: " + e . getMessage ( ) ) ; } }
public void test() { if ( update > 0 ) { logger . info ( "update retry message success by retryQueryCondition:{}, status:{}, sendStartTime:{}, sendEndTime:{}" , retryQueryCondition , status ) ; } else { logger . error ( "update retry message error by retryQueryCondition:{}, status:{}, sendStartTime:{}, sendEndTime:{}" , retryQueryCondition , status ) ; } }
public void test() { if ( update > 0 ) { logger . info ( "update retry message success by retryQueryCondition:{}, status:{}, sendStartTime:{}, sendEndTime:{}" , retryQueryCondition , status ) ; } else { logger . error ( "update retry message error by retryQueryCondition:{}, status:{}, sendStartTime:{}, sendEndTime:{}" , retryQueryCondition , status ) ; } }
public void test() { try { com . liferay . commerce . model . CommerceOrder returnValue = CommerceOrderServiceUtil . updateCustomFields ( commerceOrderId , serviceContext ) ; return com . liferay . commerce . model . CommerceOrderSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( SegmentsEntryRelServiceUtil . class , "getSegmentsEntryRelsCount" , _getSegmentsEntryRelsCountParameterTypes7 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , classNameId , classPK ) ; Object returnObj = null ; code_block = TryStatement ;  return ( ( Integer ) returnObj ) . intValue ( ) ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( head != null ) { head . moveToBlocking ( neck , rothead , eyeX , eyeY , jaw , rollNeck ) ; } else { log . error ( "I have a null head" ) ; } }
public void test() { try { file = new File ( url . toURI ( ) ) ; } catch ( URISyntaxException e ) { file = new File ( url . getPath ( ) ) ; } catch ( Exception e ) { logger . error ( "Couldn't convert url '" + url + "' to a file" ) ; file = null ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Using authorization provider " + authProviderName + " with resource " + resourceName + ", policy engine " + policyEngineName + ", provider backend " + providerBackendName ) ; } }
@ Test public void copyAllTranslationsWithMD5MatchBetweenRepositoriesNameRegex ( ) throws InterruptedException , ExecutionException , RepositoryNameAlreadyUsedException , AssetWithIdNotFoundException , RepositoryWithIdNotFoundException { TMTestData tmTestDataSource = new TMTestData ( testIdWatcher ) ; Repository sourceRepository = tmTestDataSource . repository ; logger . debug ( "Create the target repository" ) ; Repository targetRepository = repositoryService . createRepository ( testIdWatcher . getEntityName ( "targetRepository" ) ) ; TM tm = targetRepository . getTm ( ) ; Asset asset = assetService . createAssetWithContent ( targetRepository . getId ( ) , "fake_for_test" , "fake code_block = ForStatement ; ; Iterator < TMTextUnitVariant > itSource = Iterables . filter ( sourceTranslations , filterZuora ) . iterator ( ) ; Iterator < TMTextUnitVariant > itTarget = targetTranslations . iterator ( ) ; code_block = WhileStatement ; Assert . assertFalse ( itSource . hasNext ( ) ) ; }
public void test() { try { this . addFile ( ) ; this . outputStream . close ( ) ; this . outputStream = null ; } catch ( Exception ex ) { ex . printStackTrace ( ) ; LOGGER . warn ( "There was an error adding the temporaryFile to S3" ) ; } finally { this . isClosed = true ; } }
public void test() { if ( persistentLogin . getTimestamp ( ) . getTime ( ) + getTokenValiditySeconds ( ) * 1000L < currentTimeMillis ( ) ) { throw new RememberMeAuthenticationException ( "Remember-me login has expired" ) ; } }
public void test() { try { EntityManager . instance . insertFeedback ( username , title , type , message , email , host , cal . getTime ( ) ) ; } catch ( Exception e ) { log . fatal ( e ) ; } }
public void test() { if ( logger . isDebugable ( ) ) { logger . debug ( "REDIS INVOKE START: " + targetURL + " action: " + redisAction , null ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Closing JDBC Connection [" + connection + "]" ) ; } }
public void test() { if ( segmentImp == null ) { _log . error ( "Segment " + segmentName + "Not found." ) ; return DEFAULT_CHANGE_NUMBER ; } }
public void test() { try { OperationFuture < Boolean > future = memcachedClient ( ) . delete ( key ) ; return getReturnCode ( future ) ; } catch ( Exception e ) { logger . error ( "Error deleting value" , e ) ; return Status . ERROR ; } }
public void test() { if ( mainRenderContext == null ) { LOGGER . error ( "Cannot render sub-geometries without a render context." ) ; return ; } }
public void test() { if ( mainRenderContext == null ) { LOGGER . error ( "Cannot render sub-geometries without a render context." ) ; return ; } }
@ Override public void run ( ) { HazelcastInstance instance = factory . newHazelcastInstance ( config ) ; instances . set ( 0 , instance ) ; IMap < String , String > map = instance . getMap ( MAP_NAME ) ; node1MapLoadingAboutToStart . countDown ( ) ; LOGGER . info ( "Getting the size of the map on node1 -> load is triggered" ) ; int sizeOnNode1 = map . size ( ) ; LOGGER . info ( "Map loading has been completed by now" ) ; LOGGER . info ( "Map size on node 1: " + sizeOnNode1 ) ; node1FinishedLoading . countDown ( ) ; }
@ Override public void run ( ) { HazelcastInstance instance = factory . newHazelcastInstance ( config ) ; instances . set ( 0 , instance ) ; IMap < String , String > map = instance . getMap ( MAP_NAME ) ; node1MapLoadingAboutToStart . countDown ( ) ; LOGGER . info ( "Getting the size of the map on node1 -> load is triggered" ) ; int sizeOnNode1 = map . size ( ) ; LOGGER . info ( "Map loading has been completed by now" ) ; LOGGER . info ( "Map size on node 1: " + sizeOnNode1 ) ; node1FinishedLoading . countDown ( ) ; }
@ Override public void run ( ) { HazelcastInstance instance = factory . newHazelcastInstance ( config ) ; instances . set ( 0 , instance ) ; IMap < String , String > map = instance . getMap ( MAP_NAME ) ; node1MapLoadingAboutToStart . countDown ( ) ; LOGGER . info ( "Getting the size of the map on node1 -> load is triggered" ) ; int sizeOnNode1 = map . size ( ) ; LOGGER . info ( "Map loading has been completed by now" ) ; LOGGER . info ( "Map size on node 1: " + sizeOnNode1 ) ; node1FinishedLoading . countDown ( ) ; }
public void test() { try { code_block = IfStatement ; } catch ( LifecycleException exception ) { logger . warn ( "Cannot Stop Tomcat" + exception . getMessage ( ) ) ; } }
public void test() { if ( me . getClass ( ) . getName ( ) . endsWith ( ".SMTPAddressFailedException" ) || me . getClass ( ) . getName ( ) . endsWith ( ".SMTPAddressSucceededException" ) ) { LOGGER . debug ( "ADDRESS :[{}] Address:[{}] Command : [{}] RetCode[{}] Response [{}]" , enhancedMessagingException . computeAction ( ) , me , enhancedMessagingException . computeAddress ( ) , enhancedMessagingException . computeCommand ( ) , enhancedMessagingException . getReturnCode ( ) ) ; } }
public void test() { if ( pref . isPresent ( ) ) { String json = pref . get ( ) . getTraits ( ) ; T result = JSONUtil . fromJsonString ( aKey . getTraitClass ( ) , json ) ; LOGGER . info ( "Loaded preferences for key {} and user {} and project {}: [{}]" , aKey , aUser , aProject , result ) ; return result ; } else { LOGGER . debug ( "No preferences found for key {} and user {}" , aKey , aUser ) ; return buildDefault ( aKey . getTraitClass ( ) ) ; } }
public void test() { if ( pref . isPresent ( ) ) { String json = pref . get ( ) . getTraits ( ) ; T result = JSONUtil . fromJsonString ( aKey . getTraitClass ( ) , json ) ; LOGGER . info ( "Loaded preferences for key {} and user {} and project {}: [{}]" , aKey , aUser , aProject , result ) ; return result ; } else { LOGGER . debug ( "No preferences found for key {} and user {}" , aKey , aUser ) ; return buildDefault ( aKey . getTraitClass ( ) ) ; } }
public void test() { try { Optional < UserProjectPreference > pref = getUserProjectPreference ( aKey , aUser , aProject ) ; code_block = IfStatement ; } catch ( IOException e ) { LOGGER . error ( "Error while loading traits, returning default" , e ) ; return buildDefault ( aKey . getTraitClass ( ) ) ; } }
public void test() { if ( CollectionUtils . isEmpty ( ini ) ) { log . warn ( "ServletContext INI resource '" + servletContextPath + "' exists, but it did not contain " + "any data." ) ; } }
public void test() { if ( ! isTestSuccess . get ( ) ) { LOG . error ( "Test case has exceeded the maximum allotted time to run of: " + getMaxTestTime ( ) + " ms." ) ; dumpAllThreads ( getName ( ) ) ; System . exit ( EXIT_ERROR ) ; } }
public void test() { if ( jdbcUrl . isPresent ( ) ) { connection = DriverManager . getConnection ( jdbcUrl . get ( ) ) ; } else { LOG . warn ( "Container is not running. Connection is not created." ) ; } }
public void test() { if ( sNumber . charAt ( 0 ) == '@' ) { sNumber = sNumber . substring ( 1 ) ; } }
@ Before public void setup ( ) throws Exception { File agentDir = StagedInstall . getInstance ( ) . getStageDir ( ) ; LOGGER . debug ( "Using agent stage dir: {}" , agentDir ) ; File testDir = new File ( agentDir , TestSpooldirSource . class . getName ( ) ) ; assertTrue ( testDir . mkdirs ( ) ) ; File spoolParentDir = new File ( testDir , "spools" ) ; assertTrue ( "Unable to create sink output dir: " + spoolParentDir . getPath ( ) , spoolParentDir . mkdir ( ) ) ; final int NUM_SOURCES = 100 ; agentProps = new Properties ( ) ; List < String > spooldirSrcNames = Lists . newArrayList ( ) ; String channelName = "mem-01" ; code_block = ForStatement ; agentProps . put ( "agent.channels.mem-01.type" , "MEMORY" ) ; agentProps . put ( "agent.channels.mem-01.capacity" , String . valueOf ( 100000 ) ) ; sinkOutputDir = new File ( testDir , "out" ) ; assertTrue ( "Unable to create sink output dir: " + sinkOutputDir . getPath ( ) , sinkOutputDir . mkdir ( ) ) ; agentProps . put ( "agent.sinks.roll-01.channel" , channelName ) ; agentProps . put ( "agent.sinks.roll-01.type" , "FILE_ROLL" ) ; agentProps . put ( "agent.sinks.roll-01.sink.directory" , sinkOutputDir . getPath ( ) ) ; agentProps . put ( "agent.sinks.roll-01.sink.rollInterval" , "0" ) ; agentProps . put ( "agent.sources" , Joiner . on ( " " ) . join ( spooldirSrcNames ) ) ; agentProps . put ( "agent.channels" , channelName ) ; agentProps . put ( "agent.sinks" , "roll-01" ) ; }
public void test() { try { com . liferay . expando . kernel . model . ExpandoValue returnValue = ExpandoValueServiceUtil . addValue ( companyId , className , tableName , columnName , classPK , data ) ; return com . liferay . expando . kernel . model . ExpandoValueSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( filters . size ( ) > 0 ) { NeutralQuery query = new NeutralQuery ( ) ; code_block = ForStatement ; securityEventIds = Lists . newArrayList ( ( repository . findAllIds ( RESOURCE_NAME , query ) ) ) ; } else { LOG . info ( "Cannot access SecurityEvents!" ) ; } }
public void test() { try { LOG . debug ( "Adding exchange with key {}" , key ) ; boolean present = jdbcTemplate . queryForObject ( "SELECT COUNT(1) FROM " + getRepositoryName ( ) + " WHERE " + ID + " = ?" , Integer . class , key ) != 0 ; code_block = IfStatement ; code_block = IfStatement ; } catch ( Exception e ) { throw new RuntimeException ( "Error adding to repository " + repositoryName + " with key " + key , e ) ; } }
public void test() { if ( present ) { long version = exchange . getProperty ( VERSION_PROPERTY , Long . class ) ; LOG . debug ( "Updating record with key {} and version {}" , key , version ) ; update ( camelContext , correlationId , exchange , getRepositoryName ( ) , version ) ; } else { LOG . debug ( "Inserting record with key {}" , key ) ; insert ( camelContext , correlationId , exchange , getRepositoryName ( ) , 1L ) ; } }
public void test() { if ( present ) { long version = exchange . getProperty ( VERSION_PROPERTY , Long . class ) ; LOG . debug ( "Updating record with key {} and version {}" , key , version ) ; update ( camelContext , correlationId , exchange , getRepositoryName ( ) , version ) ; } else { LOG . debug ( "Inserting record with key {}" , key ) ; insert ( camelContext , correlationId , exchange , getRepositoryName ( ) , 1L ) ; } }
public void test() { if ( clazzExtension != null ) { return clazzExtension ; } }
public void test() { if ( currentLevel > params . length ) { log . warn ( ".removeFromNode (" + Thread . currentThread ( ) . getId ( ) + ") Current level exceed parameter length, node=" + currentNode + "  filterCallback=" + filterCallback ) ; return false ; } }
public void test() { if ( indexFound != null ) { break ; } }
public void test() { try { configurator . setInputStream ( new BufferedInputStream ( Files . newInputStream ( cacheConfig ) ) ) ; configurator . configure ( pluginName ) ; } catch ( Exception e ) { Log . error ( "An exception occurred while trying to configure caches for plugin '{}':" , pluginName , e ) ; } }
public void test() { try { parameters . add ( URLDecoder . decode ( homeFile . getAbsolutePath ( ) , StandardCharsets . UTF_8 . name ( ) ) ) ; return commandService . runCommand ( new File ( command ) , parameters ) . isSuccessful ( ) ; } catch ( FileNotFoundException e ) { logger . error ( "FileNotFoundException in deleteSymLink" , e ) ; return false ; } catch ( IOException e ) { logger . error ( "IOException in deleteSymLink" , e ) ; return false ; } }
public void test() { try { parameters . add ( URLDecoder . decode ( homeFile . getAbsolutePath ( ) , StandardCharsets . UTF_8 . name ( ) ) ) ; return commandService . runCommand ( new File ( command ) , parameters ) . isSuccessful ( ) ; } catch ( FileNotFoundException e ) { logger . error ( "FileNotFoundException in deleteSymLink" , e ) ; return false ; } catch ( IOException e ) { logger . error ( "IOException in deleteSymLink" , e ) ; return false ; } }
public void test() { if ( rs . next ( ) ) { Long id = rs . getLong ( "id" ) ; LOGGER . debug ( "findSchemaForPositionSHA: found schema_id: {} for sha: {}" , id , sha ) ; return id ; } else { return null ; } }
@ Override public void generate ( Model model , MolgenisOptions options ) throws Exception { Template template = createTemplate ( "/" + getClass ( ) . getSimpleName ( ) + ".java.ftl" ) ; Map < String , Object > templateArgs = createTemplateArguments ( options ) ; File target = new File ( this . getDocumentationPath ( options ) + "/tabledoc.html" ) ; boolean created = target . getParentFile ( ) . mkdirs ( ) ; code_block = IfStatement ; templateArgs . put ( "model" , model ) ; OutputStream targetOut = new FileOutputStream ( target ) ; template . process ( templateArgs , new OutputStreamWriter ( targetOut , Charset . forName ( "UTF-8" ) ) ) ; targetOut . close ( ) ; logger . info ( "generated " + target ) ; }
public void test() { if ( logger . isWarnEnabled ( ) ) { logger . warn ( "Unsupported method " + method , e ) ; } }
public void test() { try { Configuration configuration = new ConfigurationImpl ( ) ; configuration . setPersistenceEnabled ( false ) ; configuration . setJournalDirectory ( DEFAULT_DATA_DIRECTORY ) ; configuration . setSecurityEnabled ( false ) ; configuration . addAcceptorConfiguration ( "amqp" , "tcp://127.0.0.1:5672?protocols=AMQP" ) ; configuration . addConnectorConfiguration ( "connector" , "tcp://127.0.0.1:5672" ) ; JMSConfiguration jmsConfig = new JMSConfigurationImpl ( ) ; ConnectionFactoryConfiguration cfConfig = new ConnectionFactoryConfigurationImpl ( ) . setName ( "cf" ) . setConnectorNames ( Arrays . asList ( "connector" ) ) . setBindings ( "cf" ) ; jmsConfig . getConnectionFactoryConfigurations ( ) . add ( cfConfig ) ; jmsServer = new EmbeddedJMS ( ) . setConfiguration ( configuration ) . setJmsConfiguration ( jmsConfig ) . start ( ) ; code_block = IfStatement ; } catch ( RuntimeException e ) { throw e ; } catch ( Exception e ) { logger . error ( "Failed to start Event Broker" , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Set disposition as of date for next action '" + nextAction . getName ( ) + "' (" + nextAction . getNodeRef ( ) + ") to: " + newAsOfDate ) ; } }
public void test() { try { currentStream = streamProvider . openStream ( ) ; currentWriter = streamSupport . createWriter ( currentPath , currentStream ) ; } catch ( IOException e ) { WGLOG . error ( e , "E04001" , resourceName , processName , currentPath ) ; throw e ; } }
public void test() { try { connected = connect ( ip_board , port_board ) ; } catch ( ArrayIndexOutOfBoundsException outEx ) { LOG . error ( "The object address '" + c . getProperty ( "address" ) + "' is not properly formatted. Check it!" ) ; throw new UnableToExecuteException ( ) ; } catch ( NumberFormatException numberFormatException ) { LOG . error ( port_board + " is not a valid ethernet port to connect to" ) ; throw new UnableToExecuteException ( ) ; } }
public void test() { try { connected = connect ( ip_board , port_board ) ; } catch ( ArrayIndexOutOfBoundsException outEx ) { LOG . error ( "The object address '" + c . getProperty ( "address" ) + "' is not properly formatted. Check it!" ) ; throw new UnableToExecuteException ( ) ; } catch ( NumberFormatException numberFormatException ) { LOG . error ( port_board + " is not a valid ethernet port to connect to" ) ; throw new UnableToExecuteException ( ) ; } }
public void test() { try { String reply = sendToBoard ( message ) ; code_block = IfStatement ; } catch ( IOException iOException ) { setDescription ( "Unable to send the message to host " + address [ 0 ] + " on port " + address [ 1 ] ) ; LOG . error ( "Unable to send the message to host " + address [ 0 ] + " on port " + address [ 1 ] ) ; throw new UnableToExecuteException ( ) ; } finally { disconnect ( ) ; } }
public void startMongoServer ( Runtime runtime , String startMongoServerCommand ) throws IOException , InterruptedException { logger . info ( "Starting mongo server at ..........." + startMongoServerCommand ) ; runtime . exec ( startMongoServerCommand ) ; logger . info ( "started.............." ) ; Thread . sleep ( 90000 ) ; }
public void startMongoServer ( Runtime runtime , String startMongoServerCommand ) throws IOException , InterruptedException { logger . info ( "Starting mongo server at ..........." + startMongoServerCommand ) ; runtime . exec ( startMongoServerCommand ) ; logger . info ( "started.............." ) ; Thread . sleep ( 90000 ) ; }
@ Test public void testDebugShortCircuit ( ) { inner . setLevel ( Level . OFF ) ; logger . debug ( "hello" ) ; assertTrue ( handler . isEmpty ( ) ) ; }
public void test() { try { client . advance ( ) ; } catch ( RuntimeException e ) { log . debug ( e , "error printing status" ) ; } }
public void test() { if ( dist > 100 ) { LOGGER . warn ( "Player {} tried to reach {} blocks wide" , player . name ( ) , dist ) ; return ; } }
public void test() { switch ( evt . getEventType ( ) ) { case MCREvent . CREATE_EVENT : handleFileCreated ( evt , file ) ; break ; case MCREvent . UPDATE_EVENT : handleFileUpdated ( evt , file ) ; break ; case MCREvent . DELETE_EVENT : handleFileDeleted ( evt , file ) ; break ; case MCREvent . REPAIR_EVENT : handleFileRepaired ( evt , file ) ; break ; case MCREvent . INDEX_EVENT : updateFileIndex ( evt , file ) ; break ; default : LOGGER . warn ( "Can't find method for file data handler for event type {}" , evt . getEventType ( ) ) ; break ; } }
public void test() { try { listener . onStart ( context ) ; } catch ( Throwable t ) { LOG . error ( "Error in listener {}" , listener . getClass ( ) . getName ( ) , t ) ; } }
public void test() { if ( currentResource == null ) { log . info ( "Creating {} {}" , newResource . getKind ( ) , newResource . getMetadata ( ) . getName ( ) ) ; operation . create ( newResource ) ; } else-if ( comparator . compare ( currentResource , newResource ) != 0 ) { log . info ( "Updating {} {}" , newResource . getKind ( ) , newResource . getMetadata ( ) . getName ( ) ) ; operation . createOrReplace ( newResource ) ; } }
public void test() { if ( currentResource == null ) { log . info ( "Creating {} {}" , newResource . getKind ( ) , newResource . getMetadata ( ) . getName ( ) ) ; operation . create ( newResource ) ; } else-if ( comparator . compare ( currentResource , newResource ) != 0 ) { log . info ( "Updating {} {}" , newResource . getKind ( ) , newResource . getMetadata ( ) . getName ( ) ) ; operation . createOrReplace ( newResource ) ; } }
public void test() { try { handle . reset ( ) ; } catch ( Throwable e ) { EeLogger . ROOT_LOGGER . debug ( "failed to reset handle" , e ) ; } }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; final StringBuilder stringBuilder = new StringBuilder ( ) ; String buffer ; code_block = WhileStatement ; return stringBuilder . toString ( ) ; } catch ( Exception e ) { LOGGER . error ( "Error occurred when extracting HTTP response body" , e ) ; throw e ; } finally { code_block = IfStatement ; } }
public void test() { try { bufferedReader . close ( ) ; } catch ( IOException exception ) { LOGGER . error ( "Unable to close buffered reader" , exception ) ; } }
public ConnectionBuilder db ( String dbName ) { this . dbName = dbName ; LOGGER . info ( "Using database name: " + dbName ) ; return this ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Getting deployment policy: [deployment-policy_id] " + deploymentPolicyID ) ; } }
public void test() { try { DLAppServiceUtil . cancelCheckOut ( fileEntryId ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
@ Override protected void shutDown ( ) { log . info ( "Elastic Job: Stop {}" , serviceName ( ) ) ; }
public void test() { try { LOG . debug ( "start writing Solr Update XML for export" ) ; writer . writeStartElement ( ADD_IDENTIFIER ) ; } catch ( final XMLStreamException e ) { final String message = "couldn't finish Solr Update XML export successfully" ; final DMPConverterException converterException = new DMPConverterException ( message , e ) ; throw DMPConverterError . wrap ( converterException ) ; } }
private void echoStartWorkers ( ) { started = System . nanoTime ( ) ; LOGGER . info ( HORIZONTAL_RULER ) ; LOGGER . info ( "Starting Workers..." ) ; LOGGER . info ( HORIZONTAL_RULER ) ; LOGGER . info ( format ( "Starting %d Workers (%d members, %d clients)..." , count ( memberDeploymentPlan ) + count ( clientDeploymentPlan ) , count ( memberDeploymentPlan ) , count ( clientDeploymentPlan ) ) ) ; }
private void echoStartWorkers ( ) { started = System . nanoTime ( ) ; LOGGER . info ( HORIZONTAL_RULER ) ; LOGGER . info ( "Starting Workers..." ) ; LOGGER . info ( HORIZONTAL_RULER ) ; LOGGER . info ( format ( "Starting %d Workers (%d members, %d clients)..." , count ( memberDeploymentPlan ) + count ( clientDeploymentPlan ) , count ( memberDeploymentPlan ) , count ( clientDeploymentPlan ) ) ) ; }
public void test() { try { StringBuilder query = new StringBuilder ( isAdd ? ADD_PAGE_METADATA_START : UPDATE_PAGE_METADATA_START ) ; query . append ( tableName ) . append ( isAdd ? ADD_PAGE_METADATA_END : UPDATE_PAGE_METADATA_END ) ; stat = conn . prepareStatement ( query . toString ( ) ) ; int index = 1 ; code_block = IfStatement ; stat . setString ( index ++ , pageMetadata . getGroup ( ) ) ; stat . setString ( index ++ , pageMetadata . getTitles ( ) . toXml ( ) ) ; stat . setString ( index ++ , pageMetadata . getModel ( ) . getCode ( ) ) ; code_block = IfStatement ; String extraConfig = this . getExtraConfig ( pageMetadata ) ; stat . setString ( index ++ , extraConfig ) ; Date updatedAt = pageMetadata . getUpdatedAt ( ) != null ? pageMetadata . getUpdatedAt ( ) : new Date ( ) ; stat . setTimestamp ( index ++ , updatedAt != null ? new java . sql . Timestamp ( updatedAt . getTime ( ) ) : null ) ; code_block = IfStatement ; stat . executeUpdate ( ) ; } catch ( Throwable t ) { _logger . error ( "Error while saving the page metadata record for table {}" , tableName , t ) ; throw new RuntimeException ( "Error while saving the page metadata record for table " + tableName , t ) ; } finally { closeDaoResources ( null , stat ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Requesting unsubscribe: " + topic ) ; } }
public void test() { for ( Logger logger : this . loggers ) { logger . debug ( format , arguments ) ; } }
public void test() { if ( getBasicProperties ( ) . isSuppressGenerateTask ( ) ) { _log . info ( "...Suppressing Generate task as basicInfoMap.dfprop" ) ; return false ; } }
@ Override protected boolean begin ( ) { code_block = IfStatement ; _log . info ( "+------------------------------------------+" ) ; _log . info ( "|                                          |" ) ; _log . info ( "|                 Generate                 |" ) ; _log . info ( "|                                          |" ) ; _log . info ( "+------------------------------------------+" ) ; DfDBFluteTaskStatus . getInstance ( ) . setTaskType ( TaskType . Generate ) ; return true ; }
public void syncHive ( ) { HiveSyncConfig hiveSyncConfig = DataSourceUtils . buildHiveSyncConfig ( props , cfg . targetBasePath , cfg . baseFileFormat ) ; LOG . info ( "Syncing target hoodie table with hive table(" + hiveSyncConfig . tableName + "). Hive metastore URL :" + hiveSyncConfig . jdbcUrl + ", basePath :" + cfg . targetBasePath ) ; HiveConf hiveConf = new HiveConf ( conf , HiveConf . class ) ; LOG . info ( "Hive Conf => " + hiveConf . getAllProperties ( ) . toString ( ) ) ; LOG . info ( "Hive Sync Conf => " + hiveSyncConfig . toString ( ) ) ; new HiveSyncTool ( hiveSyncConfig , hiveConf , fs ) . syncHoodieTable ( ) ; }
public void syncHive ( ) { HiveSyncConfig hiveSyncConfig = DataSourceUtils . buildHiveSyncConfig ( props , cfg . targetBasePath , cfg . baseFileFormat ) ; LOG . info ( "Syncing target hoodie table with hive table(" + hiveSyncConfig . tableName + "). Hive metastore URL :" + hiveSyncConfig . jdbcUrl + ", basePath :" + cfg . targetBasePath ) ; HiveConf hiveConf = new HiveConf ( conf , HiveConf . class ) ; LOG . info ( "Hive Conf => " + hiveConf . getAllProperties ( ) . toString ( ) ) ; LOG . info ( "Hive Sync Conf => " + hiveSyncConfig . toString ( ) ) ; new HiveSyncTool ( hiveSyncConfig , hiveConf , fs ) . syncHoodieTable ( ) ; }
public void syncHive ( ) { HiveSyncConfig hiveSyncConfig = DataSourceUtils . buildHiveSyncConfig ( props , cfg . targetBasePath , cfg . baseFileFormat ) ; LOG . info ( "Syncing target hoodie table with hive table(" + hiveSyncConfig . tableName + "). Hive metastore URL :" + hiveSyncConfig . jdbcUrl + ", basePath :" + cfg . targetBasePath ) ; HiveConf hiveConf = new HiveConf ( conf , HiveConf . class ) ; LOG . info ( "Hive Conf => " + hiveConf . getAllProperties ( ) . toString ( ) ) ; LOG . info ( "Hive Sync Conf => " + hiveSyncConfig . toString ( ) ) ; new HiveSyncTool ( hiveSyncConfig , hiveConf , fs ) . syncHoodieTable ( ) ; }
public void test() { try { RecordMetadata metadata = result . get ( 10 , TimeUnit . MILLISECONDS ) ; LOGGER . info ( "send success metadata={}" , metadata ) ; } catch ( Throwable t ) { LOGGER . error ( "failed to send the message to Kafka" , t ) ; } }
public void test() { -> { LOG . trace ( "setting connectTimeout: {}" , s ) ; super . setConnectTimeout ( s ) ; } }
@ Override public void recordEvent ( Event e ) { logger . info ( e . toString ( ) ) ; }
public void test() { if ( ab != null ) { list . add ( ab . getTitle ( ) ) ; } else { log . warn ( "AddressbookDO with id '" + id + "' not found. abIds string was: " + abIds ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( sb . toString ( ) ) ; } }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; } catch ( SQLException sqle ) { logger . debug ( "SQL Exception closing statement/connection in executeQuery: " + sqle . getLocalizedMessage ( ) ) ; return null ; } }
public void test() { if ( cachedBeanMetaDataInstances < totalCreatedMetaDataInstances ) { log . debug ( "Garbage collection occurred and some metadata instances got garbage collected!" ) ; log . debug ( "totalCreatedMetaDataInstances:" + totalCreatedMetaDataInstances ) ; log . debug ( "cachedBeanMetaDataInstances:" + cachedBeanMetaDataInstances ) ; break ; } }
public void test() { try { List < Object > memoryConsumer = new ArrayList < > ( ) ; code_block = ForStatement ; } catch ( OutOfMemoryError e ) { log . debug ( "Out of memory error occurred." ) ; log . debug ( "totalCreatedMetaDataInstances:" + totalCreatedMetaDataInstances ) ; log . debug ( "cachedBeanMetaDataInstances:" + cachedBeanMetaDataInstances ) ; } }
@ Override public boolean connect ( ) { InfluxDBClientOptions . Builder optionsBuilder = InfluxDBClientOptions . builder ( ) . url ( configuration . getUrl ( ) ) . org ( configuration . getDatabaseName ( ) ) . bucket ( configuration . getRetentionPolicy ( ) ) ; char [ ] token = configuration . getTokenAsCharArray ( ) ; code_block = IfStatement ; InfluxDBClientOptions clientOptions = optionsBuilder . build ( ) ; final InfluxDBClient createdClient = InfluxDBClientFactory . create ( clientOptions ) ; this . client = createdClient ; logger . debug ( "Succesfully connected to InfluxDB. Instance ready={}" , createdClient . ready ( ) ) ; queryAPI = createdClient . getQueryApi ( ) ; writeAPI = createdClient . getWriteApi ( ) ; return checkConnectionStatus ( ) ; }
private void handleException ( FailureReason aReason , String aMessage ) { myURLDataSource . setFailureReason ( aReason ) ; myURLDataSource . setErrorMessage ( aMessage ) ; LOGGER . warn ( aMessage ) ; }
public void test() { try { String value = URLDecoder . decode ( cookie . getValue ( ) , "UTF-8" ) ; LOGGER . debug ( "Cookie: " + value + " (" + httpRequest . getRequestURI ( ) + ")" ) ; if ( StringUtils . isNotBlank ( value ) ) return value ; } catch ( UnsupportedEncodingException e ) { LOGGER . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { String value = URLDecoder . decode ( cookie . getValue ( ) , "UTF-8" ) ; LOGGER . debug ( "Cookie: " + value + " (" + httpRequest . getRequestURI ( ) + ")" ) ; if ( StringUtils . isNotBlank ( value ) ) return value ; } catch ( UnsupportedEncodingException e ) { LOGGER . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { type = Class . forName ( typeClass ) . asSubclass ( DiscreteIndexType . class ) . newInstance ( ) ; } catch ( Exception e ) { log . warn ( "Unable to create DiscreteIndexType for class name: " + typeClass ) ; } }
public void test() { try { connection = startRequestToUrl ( appendPath ( serverUrl , path ) ) ; return connectionHandler . withConnection ( connection ) ; } catch ( Exception e ) { expectedErrorCount = incrementAndGetErrorCount ( expectedErrorCount ) ; logger . debug ( "Exception while interacting with APM Server, trying next one." ) ; code_block = IfStatement ; previousException = e ; } finally { HttpUtils . consumeAndClose ( connection ) ; } }
public void test() { try { VersionInfo version = client . getVersion ( ) ; code_block = IfStatement ; log . info ( "Kubernetes API Server at '" + masterURL + "' successfully contacted." ) ; log . debugf ( "Kubernetes Version: %s.%s" , version . getMajor ( ) , version . getMinor ( ) ) ; serverFound = true ; return Result . enabled ( ) ; } catch ( Exception e ) { code_block = IfStatement ; } finally { client . close ( ) ; } }
public void test() { if ( ! displays . containsKey ( source ) ) { log . error ( "cannot remove VideoDisplayPanel " + source ) ; return ; } }
@ Override public void run ( ) { code_block = IfStatement ; code_block = TryStatement ;  code_block = IfStatement ; nextSchedule = getNextSchedule ( nextSchedule , cronExpression ) ; final long delay = getDelay ( nextSchedule ) ; logger . debug ( "Finished task for {}; next scheduled time is at {} after a delay of {} milliseconds" , connectable , nextSchedule , delay ) ; flowEngine . schedule ( this , delay , TimeUnit . MILLISECONDS ) ; }
public void test() { try { MerkleTopic merkleTopic = view . topics ( ) . get ( MerkleEntityId . fromTopicId ( txn . getConsensusUpdateTopic ( ) . getTopicID ( ) ) ) ; long rbsIncrease = getUpdateTopicRbsIncrease ( txn . getTransactionID ( ) . getTransactionValidStart ( ) , JKey . mapJKey ( merkleTopic . getAdminKey ( ) ) , JKey . mapJKey ( merkleTopic . getSubmitKey ( ) ) , merkleTopic . getMemo ( ) , merkleTopic . hasAutoRenewAccountId ( ) , lookupExpiry ( merkleTopic ) , txn . getConsensusUpdateTopic ( ) ) ; return getConsensusUpdateTopicFee ( txn , rbsIncrease , sigUsage ) ; } catch ( Exception illegal ) { log . warn ( "Usage estimation unexpectedly failed for {}!" , txn , illegal ) ; throw new InvalidTxBodyException ( illegal ) ; } }
public void test() { if ( attr instanceof Attributes ) { Attributes attrs = ( Attributes ) attr ; delta = attrs . size ( ) ; log . trace ( "delta to " + attrs + " is " + delta ) ; } else { log . trace ( "delta to " + attr + " is " + delta ) ; } }
public void test() { if ( attr instanceof Attributes ) { Attributes attrs = ( Attributes ) attr ; delta = attrs . size ( ) ; log . trace ( "delta to " + attrs + " is " + delta ) ; } else { log . trace ( "delta to " + attr + " is " + delta ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception ex ) { String msg = "An Exception was caught while trying to load the driver. %s" ; String msgArg = ex . getLocalizedMessage ( ) ; logger . error ( String . format ( msg , msgArg ) , ex ) ; throw new SQLException ( String . format ( msg , msgArg ) ) ; } }
public void test() { try { managedChannel . shutdownNow ( ) ; managedChannel . awaitTermination ( 1000L , TimeUnit . MILLISECONDS ) ; } catch ( Exception e ) { LOG . info ( "Shutting down ManagedChannel failed." , e ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "field ('{}') has no attributes, cannot parse an attribute from it" , ToStringBuilder . reflectionToString ( field ) ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "field ('{}') has no name XML attribute, cannot parse an attribute from it" , ToStringBuilder . reflectionToString ( field ) ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "field ('{}') has no name, cannot parse an attribute from it" , ToStringBuilder . reflectionToString ( field ) ) ; } }
@ Transactional ( rollbackFor = ArrowheadException . class ) public void removePlanEntryById ( final long id ) { logger . debug ( "removePlanEntryById started..." ) ; code_block = TryStatement ;  }
public void test() { try { code_block = IfStatement ; choreographerPlanRepository . deleteById ( id ) ; choreographerPlanRepository . flush ( ) ; } catch ( final InvalidParameterException ex ) { throw ex ; } catch ( final Exception ex ) { logger . debug ( ex . getMessage ( ) , ex ) ; throw new ArrowheadException ( CoreCommonConstants . DATABASE_OPERATION_EXCEPTION_MSG ) ; } }
public void test() { try { logTimeseriesDeleted ( user , entityId , keys , t ) ; } catch ( ThingsboardException e ) { log . error ( "Failed to log timeseries delete" , e ) ; } }
public void test() { if ( sizeInBytes > logRequestSizeLimit ) { LOGGER . warn ( "Large document detected (id: %s). Size in bytes: %d" , item . getVertexiumObjectId ( ) , sizeInBytes ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception ex ) { getLogger ( ) . debug ( Messages . getInstance ( ) . getString ( "ERROR.OverridingExistingUser" , user . getUsername ( ) ) , ex ) ; } }
public void test() { try { roleDao . createUser ( tenant , user . getUsername ( ) , password , null , userRoles ) ; } catch ( AlreadyExistsException e ) { getLogger ( ) . info ( Messages . getInstance ( ) . getString ( "USER.Already.Exists" , user . getUsername ( ) ) ) ; code_block = TryStatement ;  } catch ( Exception e ) { getLogger ( ) . error ( Messages . getInstance ( ) . getString ( "ERROR.OverridingExistingUser" , user . getUsername ( ) ) , e ) ; } }
@ Override public void lifeCycleStopping ( @ Nullable LifeCycle arg0 ) { logger . trace ( "WebSocketClient stopping" ) ; }
@ Override public void runEx ( RunnableEx runnable ) throws Exception { log . debug ( "running in TestTransactionUtil" ) ; startTransactionIfNeeded ( ) ; runnable . run ( ) ; flushAndClear ( ) ; }
@ Override public void onNext ( final TaskStop event ) { LOG . info ( "NoopTask.TaskStopHandler.send() invoked." ) ; NoopTask . this . stopTask ( ) ; }
public void test() { if ( AdminUtils . topicExists ( zkUtils , topic ) ) { log . info ( "Deleting topic {}" , topic ) ; AdminUtils . deleteTopic ( zkUtils , topic ) ; log . info ( "Deleted Zookeeper topic {}" , topic ) ; } else { log . info ( "No need to delete topic {} as it does not exist" , topic ) ; } }
public void test() { try { checkNotNull ( embeddedAgentModule ) ; embeddedAgentModule . waitForSimpleRepoModule ( ) ; embeddedAgentModule . initEmbeddedServer ( ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { executeSendCommandWithNoResponse ( task ) ; } catch ( PDUException e ) { logger . warn ( "PDU String should be always valid" , e ) ; } }
public void test() { if ( printInBackground ) { LOG . debug ( String . format ( "Executing non-blocking process %s" , commandLine . toString ( ) ) ) ; resultHandler = new PrintResultHandler ( watchdog ) ; executor . execute ( commandLine , resultHandler ) ; } else { LOG . debug ( String . format ( "Executing blocking process %s" , commandLine . toString ( ) ) ) ; successExitValue = executor . execute ( commandLine ) ; resultHandler = new PrintResultHandler ( successExitValue ) ; } }
public void test() { if ( printInBackground ) { LOG . debug ( String . format ( "Executing non-blocking process %s" , commandLine . toString ( ) ) ) ; resultHandler = new PrintResultHandler ( watchdog ) ; executor . execute ( commandLine , resultHandler ) ; } else { LOG . debug ( String . format ( "Executing blocking process %s" , commandLine . toString ( ) ) ) ; successExitValue = executor . execute ( commandLine ) ; resultHandler = new PrintResultHandler ( successExitValue ) ; } }
private static void logAndPrint ( String s ) { System . out . println ( s ) ; log . info ( s ) ; }
@ Bean @ ConditionalOnProperty ( "sap.security.services.xsuaa.uaadomain" ) public JwtDecoder hybridJwtDecoder ( XsuaaServiceConfiguration xsuaaConfig , IdentityServiceConfiguration identityConfig ) { LOGGER . debug ( "auto-configures HybridJwtDecoder." ) ; return new JwtDecoderBuilder ( ) . withIasServiceConfiguration ( identityConfig ) . withXsuaaServiceConfiguration ( xsuaaConfig ) . build ( ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { String s = ".encodeClose(): Raw Bytes - " + getHexDump ( allocated ) ; logger . debug ( CLASS_NAME + s ) ; } }
public void test() { if ( scheduledEventslogger . isTraceEnabled ( ) ) { scheduledEventslogger . trace ( "Establishing a session timeout of " + sessionTimeout + " seconds for WebSocket session (" + getId ( ) + ")." ) ; } }
public void test() { if ( getManagementSupport ( ) . isNoLongerManaged ( ) ) { log . debug ( "Entity {} no longer managed; ignoring scheduled connect " + "sensors on rebind" , OpenShiftEntityImpl . this ) ; return ; } }
public void test() { try { code_block = IfStatement ; connectSensors ( ) ; } catch ( Throwable e ) { log . warn ( "Problem connecting sensors on rebind of " + OpenShiftEntityImpl . this , e ) ; Exceptions . propagateIfFatal ( e ) ; } }
@ Override public void postProcess ( DataSetLookup lookup , DataSet dataSet ) { LOGGER . debug ( "On Data Set Post Process: {}" , dataSet . getUUID ( ) ) ; numberOfRunningDataSetLookups . labels ( dataSet . getUUID ( ) ) . dec ( ) ; dataSetExecution . labels ( dataSet . getUUID ( ) ) . inc ( ) ; final Long start = ( Long ) lookup . getMetadata ( PROMETHEUS_META ) ; code_block = IfStatement ; }
public void test() { try { String inputPath = "../../data/a9a/a9a_123d_train.libsvm" ; String savePath = LOCAL_FS + TMP_PATH + "/FMmodel" ; String logPath = LOCAL_FS + TMP_PATH + "/FMlog" ; conf . setInt ( AngelConf . ANGEL_PS_NUMBER , 4 ) ; conf . set ( AngelConf . ANGEL_TRAIN_DATA_PATH , inputPath ) ; conf . set ( AngelConf . ANGEL_SAVE_MODEL_PATH , savePath ) ; conf . set ( AngelConf . ANGEL_LOG_PATH , logPath ) ; conf . set ( AngelConf . ANGEL_ACTION_TYPE , MLConf . ANGEL_ML_TRAIN ( ) ) ; GraphRunner runner = new GraphRunner ( ) ; runner . train ( conf ) ; } catch ( Exception x ) { LOG . error ( "run trainOnLocalClusterTest failed " , x ) ; throw x ; } }
public void test() { if ( new File ( pathToWrite ) . exists ( ) ) { LOGGER . debug ( "Collection file " + pathToWrite + " already written" ) ; } else { code_block = TryStatement ;  } }
public void test() { try ( PrintWriter writer = new PrintWriter ( pathToWrite , "UTF-8" ) ; ) { code_block = ForStatement ; } catch ( Exception e ) { LOGGER . error ( "Error writting collection to file" , e ) ; e . printStackTrace ( ) ; } }
public void test() { try { code_block = WhileStatement ; } catch ( IOException e ) { LOG . error ( "Failed to execute the command due the exception " + e ) ; } }
public void test() { try { var extension = context . getBean ( LogLevelOverrideExtension . class ) ; LOGGER . info ( "{} activated" , LogLevelOverrideExtension . class . getSimpleName ( ) ) ; return Optional . of ( extension ) ; } catch ( NoSuchBeanDefinitionException e ) { return Optional . empty ( ) ; } }
public void test() { try { savedSock . close ( ) ; } catch ( IOException ex ) { log . warn ( "Exception while interrupting channel: " , ex ) ; return false ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "setStat({})[{}]: {}" , getClientChannel ( ) , handle , attributes ) ; } }
@ Override public void commit ( ) throws SailException { super . commit ( ) ; logger . debug ( "Added {} unique statements, deduped {}" , addedStmts . size ( ) , dedupCount ) ; resetDedupBuffer ( ) ; }
public void test() { try { java . util . List < com . liferay . journal . model . JournalArticle > returnValue = JournalArticleServiceUtil . getArticlesByLayoutUuid ( groupId , layoutUuid , start , end ) ; return com . liferay . journal . model . JournalArticleSoap . toSoapModels ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try { final MailboxSession mailboxSession = session . getMailboxSession ( ) ; final SelectedMailbox selectedMailbox = session . getSelected ( ) ; final boolean isSelectedMailbox = selectedMailbox != null && selectedMailbox . getMailboxId ( ) . equals ( mailbox . getId ( ) ) ; final ComposedMessageId messageId = mailbox . appendMessage ( MessageManager . AppendCommand . builder ( ) . withInternalDate ( datetime ) . withFlags ( flagsToBeSet ) . isRecent ( ! isSelectedMailbox ) . build ( message ) , mailboxSession ) . getId ( ) ; code_block = IfStatement ; UidValidity uidValidity = mailbox . getMailboxEntity ( ) . getUidValidity ( ) ; unsolicitedResponses ( session , responder , false ) ; okComplete ( request , ResponseCode . appendUid ( uidValidity , new UidRange [ ] code_block = "" ; ) , responder ) ; } catch ( MailboxNotFoundException e ) { tryCreate ( request , responder , e ) ; } catch ( MailboxException e ) { LOGGER . error ( "Unable to append message to mailbox {}" , mailboxPath , e ) ; no ( request , responder , HumanReadableText . SAVE_FAILED ) ; } }
public void test() { if ( ! file . delete ( ) ) { LOGGER . debug ( "File was unable to be deleted: {}" , file . getAbsolutePath ( ) ) ; } }
public void test() { if ( entityName . endsWith ( "DO" ) ) { log . error ( msg ) ; } else { log . info ( msg ) ; } }
public void test() { if ( entityName . endsWith ( "DO" ) ) { log . error ( msg ) ; } else { log . info ( msg ) ; } }
public void test() { if ( length != null ) { return length ; } }
@ Then ( "^the \"([^\"]*)\" meter reads gas result should be returned$" ) public void theMeterReadsGasResultShouldBeReturned ( final String periodType , final Map < String , String > settings ) throws Throwable { final PeriodicMeterReadsGasAsyncRequest asyncRequest = PeriodicMeterReadsGasRequestFactory . fromScenarioContext ( ) ; LOGGER . warn ( "Asyncrequest: {} " , asyncRequest ) ; final PeriodicMeterReadsGasResponse response = this . responseClient . getResponse ( asyncRequest ) ; assertThat ( response ) . as ( "PeriodicMeterReadsGasResponse should not be null" ) . isNotNull ( ) ; assertThat ( response . getPeriodType ( ) ) . as ( "PeriodType should match" ) . isEqualTo ( PeriodType . fromValue ( periodType ) ) ; assertThat ( response . getPeriodicMeterReadsGas ( ) ) . as ( "Expected periodic meter reads gas" ) . isNotNull ( ) ; }
public void test() { try ( Connection con = getDatasource ( ) . getConnection ( ) ; PreparedStatement stmt = con . prepareStatement ( query ) ; ) { stmt . setString ( 1 , status . name ( ) ) ; stmt . setString ( 2 , instanceId ) ; stmt . executeUpdate ( ) ; } catch ( SQLException e ) { logger . error ( "Failed to set subscription end state for instanceId " + instanceId , e ) ; throw e ; } }
private static void loadTpchTopic ( EmbeddedKafka embeddedKafka , TestingPrestoClient prestoClient , TpchTable < ? > table ) { long start = System . nanoTime ( ) ; log . info ( "Running import for %s" , table . getTableName ( ) ) ; TestUtils . loadTpchTopic ( embeddedKafka , prestoClient , kafkaTopicName ( table ) , new QualifiedObjectName ( "tpch" , TINY_SCHEMA_NAME , table . getTableName ( ) . toLowerCase ( ENGLISH ) ) ) ; log . info ( "Imported %s in %s" , 0 , table . getTableName ( ) , nanosSince ( start ) . convertToMostSuccinctTimeUnit ( ) ) ; }
private static void loadTpchTopic ( EmbeddedKafka embeddedKafka , TestingPrestoClient prestoClient , TpchTable < ? > table ) { long start = System . nanoTime ( ) ; log . info ( "Running import for %s" , table . getTableName ( ) ) ; TestUtils . loadTpchTopic ( embeddedKafka , prestoClient , kafkaTopicName ( table ) , new QualifiedObjectName ( "tpch" , TINY_SCHEMA_NAME , table . getTableName ( ) . toLowerCase ( ENGLISH ) ) ) ; log . info ( "Imported %s in %s" , 0 , table . getTableName ( ) , nanosSince ( start ) . convertToMostSuccinctTimeUnit ( ) ) ; }
@ Override public void warn ( Marker marker , String message ) { getLogger ( ) . warn ( marker , message ) ; }
public void test() { if ( warnWhenFull ) { logger . warn ( "Cache is being evicted because it exceeded max capacity {}. Consider increasing cache size with the {} system property, or using an unbound cache with the {} system property" , upperBound , SystemProperties . BOUNDED_QUERY_CACHE_SIZE , SystemProperties . ENABLE_BOUNDED_QUERY_CACHE ) ; } }
public void test() { if ( event . isPre ( ) ) { log . trace ( "Going to evict " + event . getEntries ( ) . size ( ) + " entries from the cache " + event . getCache ( ) . getName ( ) ) ; } else { log . trace ( "Evicted " + event . getEntries ( ) . size ( ) + " entries from the cache " + event . getCache ( ) . getName ( ) ) ; } }
public void test() { if ( event . isPre ( ) ) { log . trace ( "Going to evict " + event . getEntries ( ) . size ( ) + " entries from the cache " + event . getCache ( ) . getName ( ) ) ; } else { log . trace ( "Evicted " + event . getEntries ( ) . size ( ) + " entries from the cache " + event . getCache ( ) . getName ( ) ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( MBMessageServiceUtil . class , "addMessage" , _addMessageParameterTypes3 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , categoryId , subject , body , serviceContext ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . message . boards . model . MBMessage ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { long createdTime = setAndGetCreatedTime ( new Path ( file . toString ( ) ) , tableId . toString ( ) ) ; stat = Status . newBuilder ( stat ) . setCreatedTime ( createdTime ) . build ( ) ; value = ProtobufUtil . toValue ( stat ) ; log . debug ( "Status was lacking createdTime, set to {} for {}" , createdTime , file ) ; } catch ( IOException e ) { log . warn ( "Failed to get file status, will retry" , e ) ; return false ; } catch ( MutationsRejectedException e ) { log . warn ( "Failed to write status mutation for replication, will retry" , e ) ; return false ; } }
public void test() { try { long createdTime = setAndGetCreatedTime ( new Path ( file . toString ( ) ) , tableId . toString ( ) ) ; stat = Status . newBuilder ( stat ) . setCreatedTime ( createdTime ) . build ( ) ; value = ProtobufUtil . toValue ( stat ) ; log . debug ( "Status was lacking createdTime, set to {} for {}" , createdTime , file ) ; } catch ( IOException e ) { log . warn ( "Failed to get file status, will retry" , e ) ; return false ; } catch ( MutationsRejectedException e ) { log . warn ( "Failed to write status mutation for replication, will retry" , e ) ; return false ; } }
public void runStep ( ChoreographerStep step , long sessionId ) throws InterruptedException { logger . debug ( "Running " + step . getId ( ) + "     " + step . getName ( ) + "       sessionId: " + sessionId + "!" ) ; ChoreographerRunningStep runningStep = insertInitiatedRunningStep ( step . getId ( ) , sessionId ) ; ServiceQueryFormDTO serviceQuery = new ServiceQueryFormDTO ( ) ; serviceQuery . setServiceDefinitionRequirement ( step . getServiceName ( ) . toLowerCase ( ) ) ; code_block = IfStatement ; final OrchestrationFormRequestDTO orchestrationForm = new OrchestrationFormRequestDTO . Builder ( requesterSystem ) . requestedService ( serviceQuery ) . flag ( OrchestrationFlags . Flag . MATCHMAKING , true ) . flag ( OrchestrationFlags . Flag . OVERRIDE_STORE , true ) . flag ( OrchestrationFlags . Flag . TRIGGER_INTER_CLOUD , false ) . build ( ) ; final OrchestrationResponseDTO orchestrationResponse = queryOrchestrator ( orchestrationForm ) ; List < OrchestrationResultDTO > orchestrationResultList = orchestrationResponse . getResponse ( ) ; logger . debug ( orchestrationResultList ) ; ChoreographerSessionRunningStepDataDTO runningStepDataDTO = new ChoreographerSessionRunningStepDataDTO ( sessionId , runningStep . getId ( ) ) ; code_block = IfStatement ; }
public void test() { try { Home home = client . getHome ( serialNumber , sessionId ) ; code_block = IfStatement ; Map < String , String > map = home . getSerializedMap ( ) ; code_block = IfStatement ; OneTouch [ ] oneTouches = client . getOneTouch ( serialNumber , sessionId ) ; Auxiliary [ ] auxes = client . getAux ( serialNumber , sessionId ) ; code_block = IfStatement ; code_block = ForStatement ; code_block = ForStatement ; code_block = IfStatement ; } catch ( IOException e ) { logger . debug ( "Exception polling" , e ) ; code_block = IfStatement ; } catch ( NotAuthorizedException e ) { logger . debug ( "Authorization Exception during polling" , e ) ; clearPolling ( ) ; configure ( ) ; } }
public void test() { if ( duration > MAX_DURATION ) { getLogger ( ) . info ( "Max animation time. " + duration ) ; duration = MAX_DURATION ; } }
public void test() { if ( dataSourceClassName != null ) { LOGGER . warn ( "{} - using dataSource and ignoring dataSourceClassName." , poolName ) ; } }
public void test() { if ( driverClassName != null ) { LOGGER . error ( "{} - cannot use driverClassName and dataSourceClassName together." , poolName ) ; throw new IllegalStateException ( "cannot use driverClassName and dataSourceClassName together." ) ; } else-if ( jdbcUrl != null ) { LOGGER . warn ( "{} - using dataSourceClassName and ignoring jdbcUrl." , poolName ) ; } }
public void test() { if ( dataSource != null ) { code_block = IfStatement ; } else-if ( dataSourceClassName != null ) { code_block = IfStatement ; } else-if ( jdbcUrl != null || dataSourceJndiName != null ) { } else-if ( driverClassName != null ) { LOGGER . error ( "{} - jdbcUrl is required with driverClassName." , poolName ) ; throw new IllegalArgumentException ( "jdbcUrl is required with driverClassName." ) ; } else { LOGGER . error ( "{} - dataSource or dataSourceClassName or jdbcUrl is required." , poolName ) ; throw new IllegalArgumentException ( "dataSource or dataSourceClassName or jdbcUrl is required." ) ; } }
public void test() { if ( dataSource != null ) { code_block = IfStatement ; } else-if ( dataSourceClassName != null ) { code_block = IfStatement ; } else-if ( jdbcUrl != null || dataSourceJndiName != null ) { } else-if ( driverClassName != null ) { LOGGER . error ( "{} - jdbcUrl is required with driverClassName." , poolName ) ; throw new IllegalArgumentException ( "jdbcUrl is required with driverClassName." ) ; } else { LOGGER . error ( "{} - dataSource or dataSourceClassName or jdbcUrl is required." , poolName ) ; throw new IllegalArgumentException ( "dataSource or dataSourceClassName or jdbcUrl is required." ) ; } }
private HttpURLConnection connect ( URL url ) throws IOException { LOGGER . debug ( "Searching Artifactory url {}" , url ) ; final URLConnectionFactory factory = new URLConnectionFactory ( settings ) ; final HttpURLConnection conn = factory . createHttpURLConnection ( url , useProxy ) ; conn . setDoOutput ( true ) ; conn . addRequestProperty ( "X-Result-Detail" , "info" ) ; final String username = settings . getString ( Settings . KEYS . ANALYZER_ARTIFACTORY_API_USERNAME ) ; final String apiToken = settings . getString ( Settings . KEYS . ANALYZER_ARTIFACTORY_API_TOKEN ) ; code_block = IfStatement ; conn . connect ( ) ; return conn ; }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "Switchover occured where fromJvmRoute=" + fromJvmRoute + " and toJvmRoute=" + toJvmRoute + " with " + updatedRoutes + " updated routes." ) ; } }
public void test() { if ( e instanceof org . apache . airavata . service . profile . user . cpi . exception . UserProfileServiceException ) { result . upe = ( org . apache . airavata . service . profile . user . cpi . exception . UserProfileServiceException ) e ; result . setUpeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AuthorizationException ) { result . ae = ( org . apache . airavata . model . error . AuthorizationException ) e ; result . setAeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof org . apache . airavata . service . profile . user . cpi . exception . UserProfileServiceException ) { result . upe = ( org . apache . airavata . service . profile . user . cpi . exception . UserProfileServiceException ) e ; result . setUpeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AuthorizationException ) { result . ae = ( org . apache . airavata . model . error . AuthorizationException ) e ; result . setAeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof org . apache . airavata . service . profile . user . cpi . exception . UserProfileServiceException ) { result . upe = ( org . apache . airavata . service . profile . user . cpi . exception . UserProfileServiceException ) e ; result . setUpeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . airavata . model . error . AuthorizationException ) { result . ae = ( org . apache . airavata . model . error . AuthorizationException ) e ; result . setAeIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { try { fcall . sendResponse ( fb , msg , msgType , seqid ) ; } catch ( java . lang . Exception ex ) { _LOGGER . error ( "Exception writing to internal frame buffer" , ex ) ; fb . close ( ) ; } }
public void test() { try { List < String > protocolStrings = objectMapper . readValue ( protocols , new TypeReference < List < String > > ( ) code_block = "" ; ) ; code_block = ForStatement ; return protocolList ; } catch ( Exception e ) { logger . error ( "INVALID configured test protocols [{}]." , protocols ) ; throw new IllegalStateException ( "INVALID configured test protocols " + protocols ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Authenticating with " + authscope ) ; } }
public void test() { if ( ! cleanAuthHeaders ( method , WWW_AUTH_RESP ) ) { return ; } }
public void test() { if ( authstring != null ) { method . addRequestHeader ( new Header ( WWW_AUTH_RESP , authstring , true ) ) ; } }
public void test() { if ( InstanceStatus . Status . STOPPED != instanceStatus . status ( ) ) { logger . debug ( ">> powering off %s ..." , RegionAndId . slashEncodeRegionAndId ( regionAndId ) ) ; api . instanceApi ( ) . powerOff ( instanceId ) ; instanceSuspendedPredicate . apply ( RegionAndId . slashEncodeRegionAndId ( regionAndId ) ) ; } }
public boolean cleanupNode ( final RegionAndId regionAndId ) { String instanceId = regionAndId . id ( ) ; InstanceStatus instanceStatus = Iterables . tryFind ( api . instanceApi ( ) . listInstanceStatus ( regionAndId . regionId ( ) ) . concat ( ) , new InstanceStatusPredicate ( instanceId ) ) . orNull ( ) ; if ( instanceStatus == null ) return true ; code_block = IfStatement ; logger . debug ( ">> destroying %s ..." , RegionAndId . slashEncodeRegionAndId ( regionAndId ) ) ; api . instanceApi ( ) . delete ( instanceId ) ; return instanceTerminatedPredicate . apply ( RegionAndId . slashEncodeRegionAndId ( regionAndId ) ) ; }
public void test() { if ( localCoordinator != null ) { localCoordinator . audioSink . process ( audioStream ) ; } else { logger . debug ( "Cannot process audioStream. No coordinator has been initialized." ) ; } }
public void onFailure ( Throwable failure ) { submitDialog . hide ( ) ; failure . printStackTrace ( System . err ) ; Dialog . error ( "Server-side Production Error" , failure . getMessage ( ) ) ; }
public void test() { try { final DeflaterOutputStream dzip = new DeflaterOutputStream ( out ) ; dzip . write ( stringified . getBytes ( ) ) ; dzip . close ( ) ; encodedResult = Base64 . encodeBase64String ( out . toByteArray ( ) ) ; } catch ( final IOException e ) { LOGGER . warn ( "Exception while compressing security group rules" ) ; } }
public List < GenPolynomial < C > > GB ( int modv , List < GenPolynomial < C > > F ) { List < GenPolynomial < C > > G = normalizeZerosOnes ( F ) ; G = PolyUtil . < C > monic ( G ) ; code_block = IfStatement ; GenPolynomialRing < C > ring = G . get ( 0 ) . ring ; code_block = IfStatement ; PairList < C > pairlist = strategy . create ( modv , ring ) ; pairlist . put ( G ) ; logger . info ( "start " + pairlist ) ; Terminator fin = new Terminator ( threads ) ; code_block = ForStatement ; fin . waitDone ( ) ; code_block = IfStatement ; logger . debug ( "parallel list = " + G . size ( ) ) ; G = minimalGB ( G ) ; logger . info ( "end   " + pairlist ) ; return G ; }
public List < GenPolynomial < C > > GB ( int modv , List < GenPolynomial < C > > F ) { List < GenPolynomial < C > > G = normalizeZerosOnes ( F ) ; G = PolyUtil . < C > monic ( G ) ; code_block = IfStatement ; GenPolynomialRing < C > ring = G . get ( 0 ) . ring ; code_block = IfStatement ; PairList < C > pairlist = strategy . create ( modv , ring ) ; pairlist . put ( G ) ; logger . info ( "start " + pairlist ) ; Terminator fin = new Terminator ( threads ) ; code_block = ForStatement ; fin . waitDone ( ) ; code_block = IfStatement ; logger . debug ( "parallel list = " + G . size ( ) ) ; G = minimalGB ( G ) ; logger . info ( "end   " + pairlist ) ; return G ; }
public List < GenPolynomial < C > > GB ( int modv , List < GenPolynomial < C > > F ) { List < GenPolynomial < C > > G = normalizeZerosOnes ( F ) ; G = PolyUtil . < C > monic ( G ) ; code_block = IfStatement ; GenPolynomialRing < C > ring = G . get ( 0 ) . ring ; code_block = IfStatement ; PairList < C > pairlist = strategy . create ( modv , ring ) ; pairlist . put ( G ) ; logger . info ( "start " + pairlist ) ; Terminator fin = new Terminator ( threads ) ; code_block = ForStatement ; fin . waitDone ( ) ; code_block = IfStatement ; logger . debug ( "parallel list = " + G . size ( ) ) ; G = minimalGB ( G ) ; logger . info ( "end   " + pairlist ) ; return G ; }
@ Test public void testLoginButton ( ) { tester . startPage ( GeoServerHomePage . class ) ; String html = tester . getLastResponseAsString ( ) ; LOGGER . info ( "Last page HTML:\n" + html ) ; assertTrue ( html . contains ( "<form style=\"display: inline-block;\" method=\"post\" action=\"../web/j_spring_oauth2_geonode_login\">" ) ) ; assertTrue ( html . contains ( "<img src=\"./wicket/resource/org.geoserver.web.security.oauth2.GeoNodeOAuth2AuthProviderPanel/geonode" ) ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Returning edges of element: " + source . getTitle ( ) + ", target type is: " + elementTypeId + "..." ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Edge found, source: " + source . getTitle ( ) + ", target: " + edge . getTarget ( ) . getTitle ( ) + ", edge type: " + edge . getType ( ) ) ; } }
public void test() { try ( BufferedReader upgradeLogReader = new BufferedReader ( new FileReader ( FSFactoryProducer . getFSFactory ( ) . getFile ( UpgradeLog . getUpgradeLogPath ( ) ) ) ) ) { String line = null ; code_block = WhileStatement ; } catch ( IOException e ) { logger . error ( "meet error when recover upgrade process, file path:{}" , UpgradeLog . getUpgradeLogPath ( ) , e ) ; } finally { FSFactoryProducer . getFSFactory ( ) . getFile ( UpgradeLog . getUpgradeLogPath ( ) ) . delete ( ) ; } }
public String getClientID ( ) { logger . debug ( "Getting clientID [" + this . clientID + "]" ) ; return this . clientID ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( this . getClass ( ) . getName ( ) + "/handleRequest!" ) ; } }
public void test() { try { String [ ] parts = getPathParts ( request ) ; String pid = parts [ 1 ] ; resAttr = ResourceAttributes . getResources ( parts ) ; code_block = IfStatement ; code_block = IfStatement ; actions . put ( Constants . ACTION . ID . getURI ( ) , Constants . ACTION . EXPORT . getStringAttribute ( ) ) ; actions . put ( Constants . ACTION . API . getURI ( ) , Constants . ACTION . APIM . getStringAttribute ( ) ) ; req = getContextHandler ( ) . buildRequest ( getSubjects ( request ) , actions , resAttr , getEnvironment ( request ) ) ; LogUtil . statLog ( request . getRemoteUser ( ) , Constants . ACTION . EXPORT . uri , pid , null ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; throw new ServletException ( e . getMessage ( ) , e ) ; } }
protected List < String > createEntity ( ArrayNode entities ) throws AtlasServiceException { LOG . debug ( "Creating entities: {}" , entities ) ; ObjectNode response = callAPIWithBody ( API_V1 . CREATE_ENTITY , entities . toString ( ) ) ; List < String > results = extractEntityResult ( response ) . getCreatedEntities ( ) ; LOG . debug ( "Create entities returned results: {}" , results ) ; return results ; }
protected List < String > createEntity ( ArrayNode entities ) throws AtlasServiceException { LOG . debug ( "Creating entities: {}" , entities ) ; ObjectNode response = callAPIWithBody ( API_V1 . CREATE_ENTITY , entities . toString ( ) ) ; List < String > results = extractEntityResult ( response ) . getCreatedEntities ( ) ; LOG . debug ( "Create entities returned results: {}" , results ) ; return results ; }
public void test() { try { LocalDate actionDate = LocalDate . from ( DateUtils . LRS_ACTIONS_DATE . parse ( billActionMatcher . group ( 1 ) ) ) ; String actionText = billActionMatcher . group ( 2 ) ; Chamber actionChamber = StringUtils . isAllUpperCase ( actionText . replaceAll ( "[^a-zA-Z]+" , "" ) ) ? Chamber . SENATE : Chamber . ASSEMBLY ; billActions . add ( new BillAction ( actionDate , actionText , actionChamber , ++ sequenceNo , daybreakBill . getBaseBillId ( ) ) ) ; } catch ( DateTimeParseException ex ) { logger . error ( "Could not parse date " + billActionMatcher . group ( 1 ) + " to " + daybreakBill . getDaybreakBillId ( ) ) ; logger . error ( ex . getMessage ( ) ) ; } }
public void test() { try { lastFlow = flow ; flow . setCandidates ( result ) ; flow . setSpec ( allocationSpec ) ; flow . setTrigger ( this ) ; flow . setPaginationInfo ( paginationInfo ) ; flow . allocate ( ) ; } catch ( OperationFailureException ofe ) { code_block = IfStatement ; } catch ( Throwable t ) { logger . warn ( "unhandled throwable" , t ) ; completion . fail ( inerr ( t . getMessage ( ) ) ) ; } }
public void test() { try { ifaces = NetworkInterface . getNetworkInterfaces ( ) ; } catch ( SocketException e ) { log . error ( "Failed to get host address" , e ) ; } }
public void test() { try { _changesetCollectionLocalService . deleteChangesetCollection ( changesetCollection . getChangesetCollectionId ( ) ) ; } catch ( PortalException portalException ) { _log . error ( portalException , portalException ) ; } }
public void test() { switch ( _dataType ) { case BOOL : final char first = _value . charAt ( 0 ) ; return first == '0' ? "false" : "true" ; case ERROR : logger . warn ( "Error-cell occurred: {}" , _value ) ; return _value . toString ( ) ; case FORMULA : return _value . toString ( ) ; case INLINESTR : final XSSFRichTextString rtsi = new XSSFRichTextString ( _value . toString ( ) ) ; return rtsi . toString ( ) ; case SSTINDEX : final String sstIndex = _value . toString ( ) ; final int idx = Integer . parseInt ( sstIndex ) ; final RichTextString item = _sharedStringTable . getItemAt ( idx ) ; return item . getString ( ) ; case NUMBER : final String numberString = _value . toString ( ) ; code_block = IfStatement ; default : logger . error ( "Unsupported data type: {}" , _dataType ) ; return "" ; } }
@ Override public void onSuccess ( @ Nullable final Collection < SgtInfo > result ) { final Integer counter = Optional . ofNullable ( result ) . map ( Collection :: size ) . orElse ( 0 ) ; LOG . debug ( "ise harvest finished, outcome: {}" , counter ) ; storeOutcome ( true , counter , null ) ; }
public void test() { try { code_block = IfStatement ; } catch ( ParseException pe ) { logger . warn ( "Could not parse '{}' to a valid date" , value ) ; } }
@ Test public void testSlowAppendWithoutTimeout ( ) throws InterruptedException , LifecycleException , EventDeliveryException , IOException { LOG . debug ( "Starting..." ) ; slowAppendTestHelper ( 0 ) ; }
public void test() { try { return new BufferedWriter ( new OutputStreamWriter ( fs . create ( new Path ( filePath ) ) ) ) ; } catch ( IOException e ) { logger . error ( "Failed to get buffered writer for {}. " , filePath , e ) ; return null ; } }
public void test() { if ( t instanceof RuntimeException ) { logger . error ( "Unexpected error happened while executing the task " + "(if the error is known to happen it should be caught and wrapped " + "into an checked exception to stop logging it as an error)" , t ) ; exceptionHolder . setExpected ( false ) ; exceptionHolder . setException ( ( Exception ) t ) ; } else-if ( t instanceof Exception ) { logger . debug ( "Error happened during task execution" , t ) ; exceptionHolder . setExpected ( true ) ; exceptionHolder . setException ( ( Exception ) t ) ; } else { String msg = "A throwable was thrown while executing the task, this is most likely a severe issue." ; logger . error ( msg , t ) ; exceptionHolder . setExpected ( false ) ; exceptionHolder . setException ( new Exception ( msg , t ) ) ; } }
public void test() { if ( t instanceof RuntimeException ) { logger . error ( "Unexpected error happened while executing the task " + "(if the error is known to happen it should be caught and wrapped " + "into an checked exception to stop logging it as an error)" , t ) ; exceptionHolder . setExpected ( false ) ; exceptionHolder . setException ( ( Exception ) t ) ; } else-if ( t instanceof Exception ) { logger . debug ( "Error happened during task execution" , t ) ; exceptionHolder . setExpected ( true ) ; exceptionHolder . setException ( ( Exception ) t ) ; } else { String msg = "A throwable was thrown while executing the task, this is most likely a severe issue." ; logger . error ( msg , t ) ; exceptionHolder . setExpected ( false ) ; exceptionHolder . setException ( new Exception ( msg , t ) ) ; } }
public void test() { if ( t instanceof RuntimeException ) { logger . error ( "Unexpected error happened while executing the task " + "(if the error is known to happen it should be caught and wrapped " + "into an checked exception to stop logging it as an error)" , t ) ; exceptionHolder . setExpected ( false ) ; exceptionHolder . setException ( ( Exception ) t ) ; } else-if ( t instanceof Exception ) { logger . debug ( "Error happened during task execution" , t ) ; exceptionHolder . setExpected ( true ) ; exceptionHolder . setException ( ( Exception ) t ) ; } else { String msg = "A throwable was thrown while executing the task, this is most likely a severe issue." ; logger . error ( msg , t ) ; exceptionHolder . setExpected ( false ) ; exceptionHolder . setException ( new Exception ( msg , t ) ) ; } }
private void waitUntilServiceIsReady ( ) throws Exception { getProvisionedServiceItem ( ) . expandServiceItem ( ) ; log . info ( "Waiting until provisioned service will be completed" ) ; selenium . takeScreenShot ( ) ; selenium . getDriverWait ( ) . withTimeout ( Duration . ofMinutes ( 5 ) ) . until ( ExpectedConditions . numberOfElementsToBe ( By . className ( "alert-info" ) , 0 ) ) ; selenium . takeScreenShot ( ) ; }
public void test() { if ( isEnabled ( LogLevel . DEBUG ) ) { this . logger . debug ( buildMessage ( message ) , e ) ; } }
public void run ( ) { log . info ( "Started event send for read" ) ; code_block = TryStatement ;  log . info ( "Completed event send for read" ) ; }
public void test() { try { code_block = WhileStatement ; } catch ( RuntimeException ex ) { log . error ( "Exception encountered: " + ex . getMessage ( ) , ex ) ; exception = ex ; } }
public void run ( ) { log . info ( "Started event send for read" ) ; code_block = TryStatement ;  log . info ( "Completed event send for read" ) ; }
public void test() { if ( log . isInfoEnabled ( ) ) { log . info ( String . format ( "Cache distribution %s" , distribution ) ) ; } }
public void test() { while ( mat . find ( ) ) { String found = mat . group ( ) ; LOGGER . debug ( "******** mailfound=\"{}\"" , found ) ; found = "mailto://" + found ; LOGGER . debug ( "*******6 mailfoundfound=\"{}\" after cleanup 6" , found ) ; String host = hostFromUriStr ( found ) ; code_block = IfStatement ; } }
@ RequestMapping ( value = Constants . UDR_GET_BY_AUTHOR_TITLE , method = RequestMethod . GET ) public JsonServiceResponse getUDR ( @ PathVariable String authorId , @ PathVariable String title ) { logger . debug ( "getUDR() user =" + authorId + ", title=" + title ) ; UdrSpecification resp = udrService . fetchUdr ( authorId , title ) ; return new JsonServiceResponse ( Status . SUCCESS , "GET UDR was successful" , resp ) ; }
@ Test @ Order ( 2 ) void testWhenCaptured ( ) { LOGGER . error ( "captured error" ) ; assertEquals ( "captured error" , logCapture . getMessage ( 3 ) ) ; }
public void test() { try { code_block = IfStatement ; } catch ( Throwable e ) { logger . error ( e , e ) ; } }
public void test() { try { dataSources = OBJECT_MAPPER . readValue ( dataSourcesUrl , DataSources . class ) ; } catch ( IOException e ) { LOG . error ( "Exception in reading data sources file {}" , dataSourcesUrl , e ) ; } }
@ Override public void startBundles ( String symbolicName , String ... additionalSymbolicNames ) { Set < String > toStart = toSet ( symbolicName , additionalSymbolicNames ) ; String bundlesNames = String . join ( ", " , toStart ) ; LOGGER . info ( "Starting the following bundles:[{}]" , bundlesNames ) ; long startTime = System . currentTimeMillis ( ) ; code_block = ForStatement ; LOGGER . info ( "Finished starting bundles in [{}] ms" , ( System . currentTimeMillis ( ) - startTime ) ) ; }
@ Override public void startBundles ( String symbolicName , String ... additionalSymbolicNames ) { Set < String > toStart = toSet ( symbolicName , additionalSymbolicNames ) ; String bundlesNames = String . join ( ", " , toStart ) ; LOGGER . info ( "Starting the following bundles:[{}]" , bundlesNames ) ; long startTime = System . currentTimeMillis ( ) ; code_block = ForStatement ; LOGGER . info ( "Finished starting bundles in [{}] ms" , ( System . currentTimeMillis ( ) - startTime ) ) ; }
public void test() { try { List < ApiDefinitionEntity > apis = sentinelApiClient . fetchApis ( app , ip , port ) . get ( ) ; repository . saveAll ( apis ) ; return Result . ofSuccess ( apis ) ; } catch ( Throwable throwable ) { logger . error ( "queryApis error:" , throwable ) ; return Result . ofThrowable ( - 1 , throwable ) ; } }
public void test() { try { String fieldName = fieldNameNode . getIdentifier ( ) ; String txtIndex = fieldName . substring ( fieldName . indexOf ( '(' ) + 1 , fieldName . indexOf ( ')' ) ) ; return Integer . parseInt ( txtIndex ) ; } catch ( Exception e ) { LOG . debug ( "Ignored error: " , e ) ; return null ; } }
public void test() { if ( paths . isEmpty ( ) ) { LOG . warn ( "No paths specified on command line" ) ; System . exit ( - 1 ) ; } }
public void test() { try { demo . run ( paths ) ; } catch ( Throwable t ) { LOG . warn ( "Failed to run paths: {}" , paths . stream ( ) . map ( Objects :: toString ) . collect ( Collectors . joining ( ", " , "[" , "]" ) ) , t ) ; } }
@ Test public void testSendTextMessage ( ) throws Exception { String expectedBody = "Hello there!" ; template . sendBodyAndHeader ( startEndpointUri , expectedBody , "cheese" , 123 ) ; listener . assertMessagesArrived ( 1 , 5000 ) ; List < Message > list = listener . flushMessages ( ) ; assertFalse ( list . isEmpty ( ) , "Should have received some messages!" ) ; Message message = list . get ( 0 ) ; LOG . debug ( "Received: {}" , message ) ; TextMessage textMessage = assertIsInstanceOf ( TextMessage . class , message ) ; assertEquals ( expectedBody , textMessage . getText ( ) , "Text message body: " + textMessage ) ; }
public static void errorLogging ( Logger logger , short responseCommand ) { logger . trace ( "setResponse(): cannot handle response {} ({})." , Command . get ( responseCommand ) . toString ( ) , new CommandNumber ( responseCommand ) . toString ( ) ) ; logger . debug ( "Gateway response {} ({}) cannot be handled at this point of interaction." , Command . get ( responseCommand ) . toString ( ) , new CommandNumber ( responseCommand ) . toString ( ) ) ; }
public static void errorLogging ( Logger logger , short responseCommand ) { logger . trace ( "setResponse(): cannot handle response {} ({})." , Command . get ( responseCommand ) . toString ( ) , new CommandNumber ( responseCommand ) . toString ( ) ) ; logger . debug ( "Gateway response {} ({}) cannot be handled at this point of interaction." , Command . get ( responseCommand ) . toString ( ) , new CommandNumber ( responseCommand ) . toString ( ) ) ; }
public void test() { switch ( next ) { case NORMAL : logger . info ( LogMarker . DISK_STORE_MONITOR_MARKER , "The disk volume {} for log files has returned to normal usage levels and is {} full." , args ) ; break ; case WARN : case CRITICAL : logger . warn ( LogMarker . DISK_STORE_MONITOR_MARKER , "The disk volume {} for log files has exceeded the warning usage threshold and is {} full." , args ) ; break ; } }
public void test() { switch ( next ) { case NORMAL : logger . info ( LogMarker . DISK_STORE_MONITOR_MARKER , "The disk volume {} for log files has returned to normal usage levels and is {} full." , args ) ; break ; case WARN : case CRITICAL : logger . warn ( LogMarker . DISK_STORE_MONITOR_MARKER , "The disk volume {} for log files has exceeded the warning usage threshold and is {} full." , args ) ; break ; } }
public void test() { if ( isTrace ) { StringBuilder txt = new StringBuilder ( ) ; txt . append ( "Ending dispatch." ) ; txt . append ( " dispatched type: " ) . append ( oldde . type ) ; txt . append ( ",   remaining nesting levels: " ) . append ( dispatches . size ( ) ) ; code_block = IfStatement ; LOG . debug ( txt . toString ( ) ) ; } }
public void test() { if ( _log . isWarnEnabled ( ) ) { _log . warn ( fieldName + " is not of type ThreadLocal. Skip binding." ) ; } }
public void test() { if ( _log . isWarnEnabled ( ) ) { _log . warn ( fieldName + " is not a static ThreadLocal. Skip binding." ) ; } }
public void test() { if ( _log . isWarnEnabled ( ) ) { _log . warn ( fieldName + " is not initialized. Skip binding." ) ; } }
public void test() { try { LoggingFactory . init ( Level . INFO ) ; log . info ( "{}" , Serial . getPorts ( ) ) ; Platform . setVirtual ( false ) ; String port = "COM7" ; int pin = 22 ; boolean useHobbyServo = true ; SwingGui gui = ( SwingGui ) Runtime . start ( "gui" , "SwingGui" ) ; Arduino mega = ( Arduino ) Runtime . start ( "mega" , "Arduino" ) ; ServoControl servo = null ; servo = ( ServoControl ) Runtime . start ( "servo" , "Servo" ) ; Service . sleep ( 500 ) ; gui . setActiveTab ( "servo" ) ; code_block = IfStatement ; mega . connect ( port ) ; servo . setPin ( pin ) ; log . info ( "rest is {}" , servo . getRest ( ) ) ; servo . attach ( mega ) ; servo . moveTo ( 10.3 ) ; servo . moveTo ( 110.3 ) ; servo . moveToBlocking ( 113.0 ) ; servo . setSpeed ( 2.0 ) ; servo . moveTo ( 140.0 ) ; TestCatcher catcher = ( TestCatcher ) Runtime . start ( "catcher" , "TestCatcher" ) ; catcher . exportAll ( "export.py" ) ; } catch ( Exception e ) { log . error ( "main threw" , e ) ; } }
public void test() { try { LoggingFactory . init ( Level . INFO ) ; log . info ( "{}" , Serial . getPorts ( ) ) ; Platform . setVirtual ( false ) ; String port = "COM7" ; int pin = 22 ; boolean useHobbyServo = true ; SwingGui gui = ( SwingGui ) Runtime . start ( "gui" , "SwingGui" ) ; Arduino mega = ( Arduino ) Runtime . start ( "mega" , "Arduino" ) ; ServoControl servo = null ; servo = ( ServoControl ) Runtime . start ( "servo" , "Servo" ) ; Service . sleep ( 500 ) ; gui . setActiveTab ( "servo" ) ; code_block = IfStatement ; mega . connect ( port ) ; servo . setPin ( pin ) ; log . info ( "rest is {}" , servo . getRest ( ) ) ; servo . attach ( mega ) ; servo . moveTo ( 10.3 ) ; servo . moveTo ( 110.3 ) ; servo . moveToBlocking ( 113.0 ) ; servo . setSpeed ( 2.0 ) ; servo . moveTo ( 140.0 ) ; TestCatcher catcher = ( TestCatcher ) Runtime . start ( "catcher" , "TestCatcher" ) ; catcher . exportAll ( "export.py" ) ; } catch ( Exception e ) { log . error ( "main threw" , e ) ; } }
public void test() { try { LoggingFactory . init ( Level . INFO ) ; log . info ( "{}" , Serial . getPorts ( ) ) ; Platform . setVirtual ( false ) ; String port = "COM7" ; int pin = 22 ; boolean useHobbyServo = true ; SwingGui gui = ( SwingGui ) Runtime . start ( "gui" , "SwingGui" ) ; Arduino mega = ( Arduino ) Runtime . start ( "mega" , "Arduino" ) ; ServoControl servo = null ; servo = ( ServoControl ) Runtime . start ( "servo" , "Servo" ) ; Service . sleep ( 500 ) ; gui . setActiveTab ( "servo" ) ; code_block = IfStatement ; mega . connect ( port ) ; servo . setPin ( pin ) ; log . info ( "rest is {}" , servo . getRest ( ) ) ; servo . attach ( mega ) ; servo . moveTo ( 10.3 ) ; servo . moveTo ( 110.3 ) ; servo . moveToBlocking ( 113.0 ) ; servo . setSpeed ( 2.0 ) ; servo . moveTo ( 140.0 ) ; TestCatcher catcher = ( TestCatcher ) Runtime . start ( "catcher" , "TestCatcher" ) ; catcher . exportAll ( "export.py" ) ; } catch ( Exception e ) { log . error ( "main threw" , e ) ; } }
public void test() { try { d = _cache . load ( id ) ; } catch ( Exception e ) { LOG . warn ( "Unable to load id {}" , id , e ) ; } }
public void test() { if ( m != null ) { return m . object ; } else { log . trace ( "Could not find jar with key: " + entryKey ) ; return null ; } }
@ Test public void testGetResource ( ) throws Exception { ResourcesResourceTest . LOG . debug ( "start get resource test" ) ; final String resourceJSON = resourceUploadInteral ( resourceFile , expectedResource ) ; ResourcesResourceTest . LOG . debug ( "created resource = '{}'" , resourceJSON ) ; final Resource resource = objectMapper . readValue ( resourceJSON , Resource . class ) ; Assert . assertNotNull ( "resource shouldn't be null" , resource ) ; Assert . assertNotNull ( "resource id shouldn't be null" , resource . getUuid ( ) ) ; ResourcesResourceTest . LOG . debug ( "try to retrieve resource '{}'" , resource . getUuid ( ) ) ; final Response response = target ( String . valueOf ( resource . getUuid ( ) ) ) . request ( ) . accept ( MediaType . APPLICATION_JSON_TYPE ) . get ( Response . class ) ; final String responseResource = response . readEntity ( String . class ) ; Assert . assertEquals ( "200 OK was expected" , 200 , response . getStatus ( ) ) ; Assert . assertEquals ( "resource JSONs are not equal" , resourceJSON , responseResource ) ; ResourcesResourceTest . LOG . debug ( "end get resource test" ) ; }
@ Test public void testGetResource ( ) throws Exception { ResourcesResourceTest . LOG . debug ( "start get resource test" ) ; final String resourceJSON = resourceUploadInteral ( resourceFile , expectedResource ) ; ResourcesResourceTest . LOG . debug ( "created resource = '{}'" , resourceJSON ) ; final Resource resource = objectMapper . readValue ( resourceJSON , Resource . class ) ; Assert . assertNotNull ( "resource shouldn't be null" , resource ) ; Assert . assertNotNull ( "resource id shouldn't be null" , resource . getUuid ( ) ) ; ResourcesResourceTest . LOG . debug ( "try to retrieve resource '{}'" , resource . getUuid ( ) ) ; final Response response = target ( String . valueOf ( resource . getUuid ( ) ) ) . request ( ) . accept ( MediaType . APPLICATION_JSON_TYPE ) . get ( Response . class ) ; final String responseResource = response . readEntity ( String . class ) ; Assert . assertEquals ( "200 OK was expected" , 200 , response . getStatus ( ) ) ; Assert . assertEquals ( "resource JSONs are not equal" , resourceJSON , responseResource ) ; ResourcesResourceTest . LOG . debug ( "end get resource test" ) ; }
@ Test public void testGetResource ( ) throws Exception { ResourcesResourceTest . LOG . debug ( "start get resource test" ) ; final String resourceJSON = resourceUploadInteral ( resourceFile , expectedResource ) ; ResourcesResourceTest . LOG . debug ( "created resource = '{}'" , resourceJSON ) ; final Resource resource = objectMapper . readValue ( resourceJSON , Resource . class ) ; Assert . assertNotNull ( "resource shouldn't be null" , resource ) ; Assert . assertNotNull ( "resource id shouldn't be null" , resource . getUuid ( ) ) ; ResourcesResourceTest . LOG . debug ( "try to retrieve resource '{}'" , resource . getUuid ( ) ) ; final Response response = target ( String . valueOf ( resource . getUuid ( ) ) ) . request ( ) . accept ( MediaType . APPLICATION_JSON_TYPE ) . get ( Response . class ) ; final String responseResource = response . readEntity ( String . class ) ; Assert . assertEquals ( "200 OK was expected" , 200 , response . getStatus ( ) ) ; Assert . assertEquals ( "resource JSONs are not equal" , resourceJSON , responseResource ) ; ResourcesResourceTest . LOG . debug ( "end get resource test" ) ; }
@ Test public void testGetResource ( ) throws Exception { ResourcesResourceTest . LOG . debug ( "start get resource test" ) ; final String resourceJSON = resourceUploadInteral ( resourceFile , expectedResource ) ; ResourcesResourceTest . LOG . debug ( "created resource = '{}'" , resourceJSON ) ; final Resource resource = objectMapper . readValue ( resourceJSON , Resource . class ) ; Assert . assertNotNull ( "resource shouldn't be null" , resource ) ; Assert . assertNotNull ( "resource id shouldn't be null" , resource . getUuid ( ) ) ; ResourcesResourceTest . LOG . debug ( "try to retrieve resource '{}'" , resource . getUuid ( ) ) ; final Response response = target ( String . valueOf ( resource . getUuid ( ) ) ) . request ( ) . accept ( MediaType . APPLICATION_JSON_TYPE ) . get ( Response . class ) ; final String responseResource = response . readEntity ( String . class ) ; Assert . assertEquals ( "200 OK was expected" , 200 , response . getStatus ( ) ) ; Assert . assertEquals ( "resource JSONs are not equal" , resourceJSON , responseResource ) ; ResourcesResourceTest . LOG . debug ( "end get resource test" ) ; }
public void test() { try { List < WidgetType > types = this . getWidgetManager ( ) . getWidgetTypes ( ) ; List < WidgetDto > dtoList = dtoBuilder . convert ( types ) ; List < WidgetDto > resultList = new WidgetTypeListProcessor ( restListReq , dtoList ) . filterAndSort ( ) . toList ( ) ; List < WidgetDto > sublist = restListReq . getSublist ( resultList ) ; SearcherDaoPaginatedResult < WidgetDto > paginatedResult = new SearcherDaoPaginatedResult < > ( resultList . size ( ) , sublist ) ; PagedMetadata < WidgetDto > pagedMetadata = new PagedMetadata < > ( restListReq , paginatedResult ) ; pagedMetadata . setBody ( sublist ) ; return pagedMetadata ; } catch ( Throwable t ) { logger . error ( "error in get widgets" , t ) ; throw new RestServerError ( "error in get widgets" , t ) ; } }
public void test() { try { listener . componentSecretUpdated ( subdomain , secret ) ; } catch ( Exception e ) { Log . warn ( "An exception occurred while dispatching a 'componentSecretUpdated' event!" , e ) ; } }
public void test() { if ( StringUtils . isEmpty ( serviceName ) || StringUtils . isEmpty ( checksum ) ) { Loggers . SRV_LOG . warn ( "[DOMAIN-CHECKSUM] serviceName or checksum is empty,serviceName: {}, checksum: {}" , serviceName , checksum ) ; return ; } }
public void test() { try { code_block = WhileStatement ; Thread . sleep ( 10L ) ; } catch ( IOException ignore1 ) { } catch ( InterruptedException ignore2 ) { } catch ( Throwable t ) { logger . warn ( "" , t ) ; } }
public void test() { try { com . liferay . blogs . model . BlogsEntry returnValue = BlogsEntryServiceUtil . getEntry ( entryId ) ; return com . liferay . blogs . model . BlogsEntrySoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try { bean . reloadConfiguration ( ) ; } catch ( Exception e ) { logger . warn ( "while reloading configuration of " + bean , e ) ; } }
public void test() { try { id = Integer . parseInt ( strId ) ; } catch ( NumberFormatException e ) { LOG . warn ( "Unable to parse pool size: " + strId + ". Applying default value" ) ; id = 0 ; } }
public List < Long > doInHibernate ( final Session session ) throws HibernateException { StringBuilder sb = new StringBuilder ( ) ; sb . append ( "SELECT DISTINCT child.id AS child_id FROM quota AS father" ) ; sb . append ( " JOIN quota AS child" ) ; sb . append ( " ON child.domain_parent_id = father.domain_id" ) ; sb . append ( " AND child.quota_type = :domainType " ) ; sb . append ( " AND father.domain_parent_id = :domainId " ) ; sb . append ( " AND father.default_max_file_size_override = false" ) ; sb . append ( " WHERE father.quota_type = :domainType" ) ; code_block = IfStatement ; sb . append ( " AND child.default_max_file_size_override = false" ) ; sb . append ( ";" ) ; @ SuppressWarnings ( "unchecked" ) final NativeQuery < Long > query = session . createNativeQuery ( sb . toString ( ) ) ; query . setParameter ( "domainId" , domain . getPersistenceId ( ) ) ; query . addScalar ( "child_id" , LongType . INSTANCE ) ; query . setParameter ( "domainType" , type . name ( ) ) ; code_block = IfStatement ; List < Long > res = query . list ( ) ; logger . debug ( "child_ids :" + res ) ; return res ; }
public void test() { try { listener . gotUserDetail ( user ) ; } catch ( Exception e ) { logger . warn ( "Exception at showUser" , e ) ; } }
@ Override public void ping ( TCredentials credentials ) { log . info ( "Manager reports: I just got pinged!" ) ; }
public void test() { try { SnappyOutputStream snappyOutputStream = new SnappyOutputStream ( baos ) ; snappyOutputStream . write ( uncompressedBytes ) ; snappyOutputStream . close ( ) ; baos . close ( ) ; } catch ( IOException e ) { logger . error ( "Could not compress sensei request " , e ) ; } }
public void test() { try { GitAccess . getInstance ( ) . setBranch ( branchName ) ; } catch ( CheckoutConflictException ex ) { logger . debug ( ex , ex ) ; restoreCurrentBranchSelectionInMenu ( ) ; BranchesUtil . showBranchSwitchErrorMessage ( ) ; } catch ( GitAPIException | JGitInternalException ex ) { restoreCurrentBranchSelectionInMenu ( ) ; PluginWorkspaceProvider . getPluginWorkspace ( ) . showErrorMessage ( ex . getMessage ( ) , ex ) ; } }
public void test() { if ( zoo . exists ( ZKUserPath ) ) { zoo . recursiveDelete ( ZKUserPath , NodeMissingPolicy . SKIP ) ; log . info ( "Removed {}/ from zookeeper" , ZKUserPath ) ; } }
public void test() { try { final Boolean skipDevDependencies = getSettings ( ) . getBoolean ( Settings . KEYS . ANALYZER_NODE_AUDIT_SKIPDEV , false ) ; final JsonObject lockJson = fetchYarnAuditJson ( dependency , skipDevDependencies ) ; final JsonReader packageReader = Json . createReader ( FileUtils . openInputStream ( packageFile ) ) ; final JsonObject packageJson = packageReader . readObject ( ) ; final JsonObject payload = NpmPayloadBuilder . build ( lockJson , packageJson , dependencyMap , skipDevDependencies ) ; return getSearcher ( ) . submitPackage ( payload ) ; } catch ( URLConnectionFailureException e ) { this . setEnabled ( false ) ; throw new AnalysisException ( "Failed to connect to the NPM Audit API (YarnAuditAnalyzer); the analyzer " + "is being disabled and may result in false negatives." , e ) ; } catch ( IOException e ) { LOGGER . debug ( "Error reading dependency or connecting to NPM Audit API" , e ) ; this . setEnabled ( false ) ; throw new AnalysisException ( "Failed to read results from the NPM Audit API (YarnAuditAnalyzer); " + "the analyzer is being disabled and may result in false negatives." , e ) ; } catch ( JsonException e ) { throw new AnalysisException ( String . format ( "Failed to parse %s file from the NPM Audit API " + "(YarnAuditAnalyzer)." , lockFile . getPath ( ) ) , e ) ; } catch ( SearchException ex ) { LOGGER . error ( "YarnAuditAnalyzer failed on {}" , dependency . getActualFilePath ( ) ) ; throw ex ; } }
public void test() { try { final Boolean skipDevDependencies = getSettings ( ) . getBoolean ( Settings . KEYS . ANALYZER_NODE_AUDIT_SKIPDEV , false ) ; final JsonObject lockJson = fetchYarnAuditJson ( dependency , skipDevDependencies ) ; final JsonReader packageReader = Json . createReader ( FileUtils . openInputStream ( packageFile ) ) ; final JsonObject packageJson = packageReader . readObject ( ) ; final JsonObject payload = NpmPayloadBuilder . build ( lockJson , packageJson , dependencyMap , skipDevDependencies ) ; return getSearcher ( ) . submitPackage ( payload ) ; } catch ( URLConnectionFailureException e ) { this . setEnabled ( false ) ; throw new AnalysisException ( "Failed to connect to the NPM Audit API (YarnAuditAnalyzer); the analyzer " + "is being disabled and may result in false negatives." , e ) ; } catch ( IOException e ) { LOGGER . debug ( "Error reading dependency or connecting to NPM Audit API" , e ) ; this . setEnabled ( false ) ; throw new AnalysisException ( "Failed to read results from the NPM Audit API (YarnAuditAnalyzer); " + "the analyzer is being disabled and may result in false negatives." , e ) ; } catch ( JsonException e ) { throw new AnalysisException ( String . format ( "Failed to parse %s file from the NPM Audit API " + "(YarnAuditAnalyzer)." , lockFile . getPath ( ) ) , e ) ; } catch ( SearchException ex ) { LOGGER . error ( "YarnAuditAnalyzer failed on {}" , dependency . getActualFilePath ( ) ) ; throw ex ; } }
private void executeStatement ( String sql , Statement stmt ) throws SQLException { LOG . info ( "Executing: " + sql ) ; stmt . execute ( sql ) ; }
@ Override public void onStatusChanged ( final UaSubscription subscription , final StatusCode status ) { LOG . info ( "Subscription status changed {} : {}" , subscription . getSubscriptionId ( ) , status ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Executing lucene query: {}, on region {}" , query , region . getFullPath ( ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Executing search on repo: " + repo . toString ( ) ) ; } }
public void test() { try { date1 = format . parse ( dateString1 ) ; date2 = format . parse ( dateString2 ) ; } catch ( ParseException e ) { log . error ( e ) ; return 0 ; } }
protected void leaked ( LeakDetector . LeakInfo leakInfo ) { LOG . info ( "Connection {} leaked at:" , leakInfo . getResourceDescription ( ) , leakInfo . getStackFrames ( ) ) ; }
public void test() { try { return propertiesProvider . getConfiguration ( RABBITMQ_CONFIGURATION_NAME ) ; } catch ( FileNotFoundException e ) { LOGGER . error ( "Could not find " + RABBITMQ_CONFIGURATION_NAME + " configuration file." ) ; throw new RuntimeException ( e ) ; } }
public void test() { if ( StringAttribute . identifier . equals ( type ) ) { return StringAttribute . getInstance ( value ) ; } else-if ( AnyURIAttribute . identifier . equals ( type ) ) { return AnyURIAttribute . getInstance ( value ) ; } else-if ( DateTimeAttribute . identifier . equals ( type ) ) { return DateTimeAttribute . getInstance ( value ) ; } else-if ( DateAttribute . identifier . equals ( type ) ) { return DateAttribute . getInstance ( value ) ; } else-if ( TimeAttribute . identifier . equals ( type ) ) { return TimeAttribute . getInstance ( value ) ; } else-if ( IntegerAttribute . identifier . equals ( type ) ) { return IntegerAttribute . getInstance ( value ) ; } else-if ( BooleanAttribute . identifier . equals ( type ) ) { return BooleanAttribute . getInstance ( value ) ; } else { logger . warn ( "Unknown type requested {}, returning StringAttribute" , type ) ; return StringAttribute . getInstance ( value ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { logger . warn ( "Error parsing attribute value {} of type {}: {}" , value , type , e . getMessage ( ) ) ; return null ; } }
public void test() { try { this . oslpChannelHandler . send ( this . createAddress ( ipAddress ) , oslpRequest , oslpResponseHandler , deviceRequest . getDeviceIdentification ( ) ) ; } catch ( final RuntimeException e ) { LOGGER . error ( "Exception during sendMessage()" , e ) ; throw new IOException ( e . getMessage ( ) ) ; } }
public void test() { try { profiler . getAttribute ( ) ; LOG . info ( "Profiling Agent found. Per-step profiling is enabled." ) ; return ProfilingState . PROFILING_PRESENT ; } catch ( UnsatisfiedLinkError e ) { LOG . info ( "Profiling Agent not found. Profiles will not be available from this worker." ) ; return ProfilingState . PROFILING_ABSENT ; } }
public void test() { try { profiler . getAttribute ( ) ; LOG . info ( "Profiling Agent found. Per-step profiling is enabled." ) ; return ProfilingState . PROFILING_PRESENT ; } catch ( UnsatisfiedLinkError e ) { LOG . info ( "Profiling Agent not found. Profiles will not be available from this worker." ) ; return ProfilingState . PROFILING_ABSENT ; } }
public void test() { try { String path = getResourceRoot ( ) + fs + serviceType + ".png" ; return Files . readAllBytes ( Paths . get ( path ) ) ; } catch ( Exception e ) { log . warn ( "getServiceIcon threw" , e ) ; } }
public void deactivate ( ) { httpService . unregister ( path ) ; logger . debug ( "Netatmo webhook servlet stopped" ) ; this . bridgeHandler = null ; }
public void test() { while ( iter . hasNext ( ) ) { Map . Entry pairs = ( Map . Entry ) iter . next ( ) ; logger . debug ( "key: " + pairs . getKey ( ) + " value: " + pairs . getValue ( ) ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "==> RangerBasePlugin.grantAccess(" + request + ")" ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "<== RangerBasePlugin.grantAccess(" + request + ")" ) ; } }
@ Override public void service ( final WebdavRequest request , final WebdavResponse response ) throws WebdavException , IOException { logger . debug ( "Setting request in RequestInfo: {}" , requestInfo ) ; requestInfo . setRequest ( request ) ; super . service ( request , response ) ; }
public org . springframework . data . domain . Page < ModuleDto > findAllModules ( final Pageable pageable , final ModuleSearchForm moduleSearchForm ) { log . debug ( "findAllModules() - pageable: {}, moduleSearchForm: {}" , pageable , moduleSearchForm ) ; final PlatformUser platformAuthorizedUser = securityService . getAuthorizedUser ( ) ; final User authorizedUser = userService . find ( platformAuthorizedUser . getId ( ) ) ; final org . springframework . data . domain . Page < Module > modules = moduleService . findAll ( pageable , moduleSearchForm , authorizedUser ) ; return moduleToModuleDtoConverter . convertToPage ( modules ) ; }
public void test() { if ( lock == null ) { s_logger . debug ( "Couldn't get the global lock" ) ; return ; } }
public void test() { if ( ! lock . lock ( 30 ) ) { s_logger . debug ( "Couldn't lock the db" ) ; return ; } }
public void test() { try { scanStalledVMInTransitionStateOnDisconnectedHosts ( ) ; final List < VMInstanceVO > instances = _vmDao . findVMInTransition ( new Date ( DateUtil . currentGMTTime ( ) . getTime ( ) - AgentManager . Wait . value ( ) * 1000 ) , State . Starting , State . Stopping ) ; code_block = ForStatement ; } catch ( final Exception e ) { s_logger . warn ( "Caught the following exception on transition checking" , e ) ; } finally { lock . unlock ( ) ; } }
private void getDropletActions ( Exchange exchange ) throws Exception { Actions actions = getEndpoint ( ) . getDigitalOceanClient ( ) . getAvailableDropletActions ( dropletId , configuration . getPage ( ) , configuration . getPerPage ( ) ) ; LOG . trace ( "Actions for Droplet {} : page {} / {} per page [{}] " , dropletId , configuration . getPage ( ) , configuration . getPerPage ( ) , actions . getActions ( ) ) ; exchange . getMessage ( ) . setBody ( actions . getActions ( ) ) ; }
public void test() { try { slackApi . call ( message ) ; } catch ( SlackChannelNotFoundException ex ) { logger . warn ( "Attempt to post slack message to invalid channel: " + ex . getChannelName ( ) ) ; } }
@ Test public void testHeartBeat4 ( ) throws Exception { connection . close ( ) ; ClientStompFrame frame = conn . createFrame ( "CONNECT" ) ; frame . addHeader ( "host" , "127.0.0.1" ) ; frame . addHeader ( "login" , this . defUser ) ; frame . addHeader ( "passcode" , this . defPass ) ; frame . addHeader ( "heart-beat" , "500,500" ) ; frame . addHeader ( "accept-version" , "1.1,1.2" ) ; ClientStompFrame reply = conn . sendFrame ( frame ) ; instanceLog . debug ( "Reply: " + reply . toString ( ) ) ; assertEquals ( "CONNECTED" , reply . getCommand ( ) ) ; RemotingConnection remotingConnection = null ; StompConnection stompConnection = null ; Iterator < RemotingConnection > iterator = server . getRemotingService ( ) . getConnections ( ) . iterator ( ) ; code_block = WhileStatement ; StompFrameHandlerV11 stompFrameHandler = ( StompFrameHandlerV11 ) stompConnection . getStompVersionHandler ( ) ; instanceLog . debug ( "========== start pinger!" ) ; conn . startPinger ( 100 ) ; ClientStompFrame subFrame = conn . createFrame ( "SUBSCRIBE" ) ; subFrame . addHeader ( "destination" , getTopicPrefix ( ) + getTopicName ( ) ) ; subFrame . addHeader ( "id" , "0" ) ; ClientStompFrame f = conn . sendFrame ( subFrame ) ; f = conn . sendFrame ( subFrame ) ; f = conn . sendFrame ( subFrame ) ; f = conn . receiveFrame ( 1000 ) ; instanceLog . debug ( "Received " + f . toString ( ) ) ; Assert . assertTrue ( f . getCommand ( ) . equals ( "ERROR" ) ) ; conn . stopPinger ( ) ; Thread . sleep ( 2000 ) ; Wait . waitFor ( ( ) code_block = LoopStatement ; ) ; Assert . assertFalse ( "HeartBeater is still running!!" , stompFrameHandler . getHeartBeater ( ) . isStarted ( ) ) ; }
public void persist ( CmMasState transientInstance ) { log . debug ( "persisting CmMasState instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . persist ( transientInstance ) ; log . debug ( "persist successful" ) ; } catch ( RuntimeException re ) { log . error ( "persist failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . persist ( transientInstance ) ; log . debug ( "persist successful" ) ; } catch ( RuntimeException re ) { log . error ( "persist failed" , re ) ; throw re ; } }
@ Test public void testTimedFlush ( ) throws Exception { logger . info ( "CounterIT.testCounters" ) ; EntityManager em = app . getEntityManager ( ) ; assertNotNull ( em ) ; UUID user1 = UUID . randomUUID ( ) ; UUID user2 = UUID . randomUUID ( ) ; Event event ; code_block = ForStatement ; Thread . sleep ( 30000 ) ; final long totalCount = returnCounts ( em , "visits" ) ; assertEquals ( 200 , totalCount ) ; }
private void addToPollQueue ( InsteonDevice d , long time ) { long texp = findNextExpirationTime ( d , time ) ; PQEntry ne = new PQEntry ( d , texp ) ; logger . trace ( "added entry {} originally aimed at time {}" , ne , String . format ( "%tc" , new Date ( time ) ) ) ; pollQueue . add ( ne ) ; }
private void handleOnCompleted ( PinpointGrpcServer pinpointGrpcServer , AgentInfo agentInfo ) { Objects . requireNonNull ( pinpointGrpcServer , "pinpointGrpcServer" ) ; Objects . requireNonNull ( agentInfo , "agentInfo" ) ; logger . info ( "{} => local. onCompleted" , getAgentInfo ( ) . getAgentKey ( ) ) ; pinpointGrpcServer . disconnected ( ) ; }
public void test() { if ( indexRef == null ) { reset ( ) ; } else { Log . info ( "Loaded " + idCounter + " documents from index at " + indexKey ( ) ) ; } }
public void test() { try { logger . debug ( "JarDeployer deployedJars list before remove: {}" , Arrays . toString ( deployedJars . keySet ( ) . toArray ( ) ) ) ; DeployedJar deployedJar = deployedJars . remove ( artifactId ) ; code_block = IfStatement ; logger . debug ( "JarDeployer deployedJars list after remove: {}" , Arrays . toString ( deployedJars . keySet ( ) . toArray ( ) ) ) ; ClassPathLoader . getLatest ( ) . unloadClassloaderForArtifact ( artifactId ) ; deleteAllVersionsOfJar ( deployedJar . getFile ( ) . getName ( ) ) ; return deployedJar . getFileCanonicalPath ( ) ; } finally { lock . unlock ( ) ; } }
public void test() { try { logger . debug ( "JarDeployer deployedJars list before remove: {}" , Arrays . toString ( deployedJars . keySet ( ) . toArray ( ) ) ) ; DeployedJar deployedJar = deployedJars . remove ( artifactId ) ; code_block = IfStatement ; logger . debug ( "JarDeployer deployedJars list after remove: {}" , Arrays . toString ( deployedJars . keySet ( ) . toArray ( ) ) ) ; ClassPathLoader . getLatest ( ) . unloadClassloaderForArtifact ( artifactId ) ; deleteAllVersionsOfJar ( deployedJar . getFile ( ) . getName ( ) ) ; return deployedJar . getFileCanonicalPath ( ) ; } finally { lock . unlock ( ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( JournalArticleServiceUtil . class , "getLatestArticle" , _getLatestArticleParameterTypes47 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , className , classPK ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . journal . model . JournalArticle ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public ArrayList < FileMatch > getTargetFileMatches ( CommandDirectories commandDirectories , FileType fileType , String sourceLocale , String sourcePathFilterRegex ) throws CommandException { logger . debug ( "Search for target assets that are already localized" ) ; FileFinder fileFinder = getFileFinder ( commandDirectories , fileType , sourceLocale , sourcePathFilterRegex ) ; return fileFinder . getTargets ( ) ; }
public Long cascadeDefaultAccountQuotaToSubDomainsAccountQuota ( AbstractDomain domain , Long accountQuota , List < Long > quotaIdList ) { code_block = IfStatement ; HibernateCallback < Long > action = new HibernateCallback < Long > ( ) code_block = "" ; ; long updatedCounter = getHibernateTemplate ( ) . execute ( action ) ; logger . debug ( " {} account_quota of ContainerQuota have been updated." , updatedCounter ) ; return updatedCounter ; }
public void test() { if ( log . isInfoEnabled ( ) ) { log . info ( "Started monitoring http server.. Port: " + options . getPort ( ) ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( String . format ( "[one-after-another algorithm] [scale-up] [partition] %s has space to create " + "members. [non terminated count] %s [max] %s" , partitionContext . getPartitionId ( ) , partitionContext . getNonTerminatedMemberCount ( ) , partitionContext . getMax ( ) ) ) ; } }
public void test() { try { X500Name x500name = new JcaX509CertificateHolder ( certificate ) . getSubject ( ) ; RDN cn = x500name . getRDNs ( BCStyle . CN ) [ 0 ] ; String cnStr = IETFUtils . valueToString ( cn . getFirst ( ) . getValue ( ) ) ; trustStore . setCertificateEntry ( cnStr , certificate ) ; resultList . add ( Collections . singletonMap ( "success" , true ) ) ; } catch ( CertificateEncodingException e ) { resultList . add ( Collections . singletonMap ( "success" , false ) ) ; LOGGER . info ( "Unable to store certificate: {}" , certificate , e ) ; } }
public void test() { try { decodedUrl = new String ( Base64 . getDecoder ( ) . decode ( url ) , "UTF-8" ) ; socket = createNonVerifyingSslSocket ( decodedUrl ) ; socket . startHandshake ( ) ; X509Certificate [ ] peerCertificateChain = ( X509Certificate [ ] ) socket . getSession ( ) . getPeerCertificates ( ) ; code_block = ForStatement ; Path trustStoreFile = Paths . get ( SecurityConstants . getTruststorePath ( ) ) ; code_block = IfStatement ; String keyStorePassword = SecurityConstants . getTruststorePassword ( ) ; fos = Files . newOutputStream ( trustStoreFile ) ; trustStore . store ( fos , keyStorePassword . toCharArray ( ) ) ; } catch ( IOException | GeneralSecurityException e ) { LOGGER . info ( "Unable to add certificate(s) to trust store from URL: {}" , ( decodedUrl != null ) ? decodedUrl : url , e ) ; } finally { IOUtils . closeQuietly ( socket ) ; IOUtils . closeQuietly ( fos ) ; } }
public void test() { switch ( permission ) { case ADMINISTER : break ; case CREATE : case DELETE : case UPDATE : hasPermission = isRegionOwner ( authentication , region , trustedRegionContainer , trustedDomainObject ) || isRegionMember ( authentication , region , trustedRegionContainer , trustedDomainObject , true ) ; break ; case READ : hasPermission = isRegionOwner ( authentication , region , trustedRegionContainer , trustedDomainObject ) || isRegionMember ( authentication , region , trustedRegionContainer , trustedDomainObject , false ) ; default : log . warn ( "unknown permission: " + permission ) ; break ; } }
public void test() { try { String value = dpr . getValue ( key ) . toString ( ) ; log . debug ( "= " + value ) ; return FormattingSwitchHelper . applyFormattingSwitch ( context . getWmlPackage ( ) , model , value ) ; } catch ( FieldValueException e ) { if ( e . getMessage ( ) . contains ( "No value found code_block = ForStatement ; throw new TransformerException ( e ) ; } catch ( Docx4JException e ) { throw new TransformerException ( e ) ; } }
public void test() { try { LogManager . resetConfiguration ( ) ; PaxLoggingConfigurator configurator = new PaxLoggingConfigurator ( m_bundleContext ) ; configurator . doConfigure ( extracted , LogManager . getLoggerRepository ( ) ) ; proxies = configurator . getProxies ( ) ; emptyConfiguration . set ( false ) ; } catch ( Exception e ) { LogLog . error ( "Configuration problem: " + e . getMessage ( ) , e ) ; problem = e ; } }
public void test() { if ( auth == null ) { log . debug ( "Cannot handle protected node " + protectedParent + ". It nor one of its parents represent a valid Authorizable." ) ; return false ; } else { currentMembership = new Membership ( auth . getID ( ) ) ; return true ; } }
public void test() { -> { int originalSize = transactions . size ( ) ; logger . trace ( "Transaction maintenance task started." ) ; code_block = TryStatement ;  logger . debug ( "Transaction maintenance task finished. originalSize={}, currentSize={}" , originalSize , transactions . size ( ) ) ; } }
public void test() { try { code_block = ForStatement ; } catch ( Exception e ) { logger . error ( "An exception occurred while maintaining transactions" , e ) ; } }
public void test() { -> { int originalSize = transactions . size ( ) ; logger . trace ( "Transaction maintenance task started." ) ; code_block = TryStatement ;  logger . debug ( "Transaction maintenance task finished. originalSize={}, currentSize={}" , originalSize , transactions . size ( ) ) ; } }
@ Override public void onFailure ( Throwable caught ) { caught . printStackTrace ( System . err ) ; Dialog . error ( "Server-side Error" , caught . getMessage ( ) ) ; productionRequests = new DtoProductionRequest [ 0 ] ; fillRequestList ( ) ; updateRequestDetails ( ) ; }
public void test() { try { setVolume ( oldVolume , sinkId ) ; } catch ( IOException e ) { logger . debug ( "An exception occurred while setting the volume of sink '{}' : {}" , sink . getId ( ) , e . getMessage ( ) , e ) ; } }
public void test() { try { setVolume ( oldVolume , sinkId ) ; } catch ( IOException e ) { logger . debug ( "An exception occurred while setting the volume of sink '{}' : {}" , sink . getId ( ) , e . getMessage ( ) , e ) ; } }
public void test() { try { sink . process ( audioStream ) ; } catch ( UnsupportedAudioFormatException | UnsupportedAudioStreamException e ) { logger . warn ( "Error playing '{}': {}" , audioStream , e . getMessage ( ) , e ) ; } finally { code_block = IfStatement ; } }
public void test() { try { setVolume ( oldVolume , sinkId ) ; } catch ( IOException e ) { logger . debug ( "An exception occurred while setting the volume of sink '{}' : {}" , sink . getId ( ) , e . getMessage ( ) , e ) ; } }
public void test() { if ( sink != null ) { PercentType oldVolume = null ; code_block = TryStatement ;  code_block = IfStatement ; code_block = TryStatement ;  } else { logger . warn ( "Failed playing audio stream '{}' as no audio sink was found." , audioStream ) ; } }
public void test() { try { MojitoAppUserInfo result = new MojitoAppUserInfo ( ) ; BoxAPIConnection apiConnection = boxAPIConnectionProvider . getConnection ( ) ; BoxFolder parentFolder = new BoxFolder ( apiConnection , BoxFolder . getRootFolder ( apiConnection ) . getID ( ) ) ; BoxFolder . Info mojitoFolder = parentFolder . createFolder ( MOJITO_FOLDER_NAME ) ; logger . debug ( "Created Mojito Folder: " + mojitoFolder . getID ( ) ) ; result . setRootFolderId ( mojitoFolder . getID ( ) ) ; BoxFolder . Info projectRequestFolder = mojitoFolder . getResource ( ) . createFolder ( PROJECT_REQUESTS_FOLDER_NAME ) ; logger . debug ( "Created Project Requests Folder: " + projectRequestFolder . getID ( ) ) ; result . setDropsFolderId ( projectRequestFolder . getID ( ) ) ; return result ; } catch ( BoxAPIException e ) { throw new BoxSDKServiceException ( "Can't creating Mojito Folder Structure." , e ) ; } }
public void test() { try { MojitoAppUserInfo result = new MojitoAppUserInfo ( ) ; BoxAPIConnection apiConnection = boxAPIConnectionProvider . getConnection ( ) ; BoxFolder parentFolder = new BoxFolder ( apiConnection , BoxFolder . getRootFolder ( apiConnection ) . getID ( ) ) ; BoxFolder . Info mojitoFolder = parentFolder . createFolder ( MOJITO_FOLDER_NAME ) ; logger . debug ( "Created Mojito Folder: " + mojitoFolder . getID ( ) ) ; result . setRootFolderId ( mojitoFolder . getID ( ) ) ; BoxFolder . Info projectRequestFolder = mojitoFolder . getResource ( ) . createFolder ( PROJECT_REQUESTS_FOLDER_NAME ) ; logger . debug ( "Created Project Requests Folder: " + projectRequestFolder . getID ( ) ) ; result . setDropsFolderId ( projectRequestFolder . getID ( ) ) ; return result ; } catch ( BoxAPIException e ) { throw new BoxSDKServiceException ( "Can't creating Mojito Folder Structure." , e ) ; } }
@ Test public void testSet ( ) { Set < String > set = new HashSet < > ( ) ; set . add ( "1" ) ; logger . info ( "{}" , JsonCodec . INSTANCE . encode ( set ) ) ; }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Checking duplicate connections for newly discovered peer: {}." + " All connections:\n{}" , peer , connectionsWithSameAddress . stream ( ) . map ( Object :: toString ) . collect ( Collectors . joining ( "\n" ) ) ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Closing duplicate incoming connection with {}:{}" + " (established remote port {})" , incomingConnection . getRemotePeer ( ) . getInetAddress ( ) , outgoingConnection . getRemotePort ( ) , incomingConnection . getRemotePort ( ) ) ; } }
@ Override protected void shutDown ( ) { publisher . close ( ) ; listener . close ( ) ; curator . close ( ) ; log . info ( "Stopping pipelines-to-avro-from-dwca service" ) ; }
public synchronized void initServerInfo ( Id server , NodeRole role ) { E . checkArgument ( server != null && role != null , "The server id or role can't be null" ) ; this . selfServerId = server ; this . selfServerRole = role ; HugeServerInfo existed = this . serverInfo ( server ) ; E . checkArgument ( existed == null || ! existed . alive ( ) , "The server with name '%s' already in cluster" , server ) ; code_block = IfStatement ; HugeServerInfo serverInfo = new HugeServerInfo ( server , role ) ; serverInfo . maxLoad ( this . calcMaxLoad ( ) ) ; this . save ( serverInfo ) ; LOG . info ( "Init server info: {}" , serverInfo ) ; }
public void run ( ) { Thread . currentThread ( ) . setName ( "ChannelBufferManager Redis subscription Thread" ) ; logger . info ( "New thread <" + Thread . currentThread ( ) . getName ( ) + "> created for subscribing to redis channel: " + channelRegEx ) ; code_block = TryStatement ;  Thread . currentThread ( ) . interrupt ( ) ; logger . info ( "Exiting thread: " + Thread . currentThread ( ) . getName ( ) ) ; }
public void test() { try { subscriberJedis . psubscribe ( aidrSubscriber , channelRegEx ) ; } catch ( JedisConnectionException e ) { logger . error ( "AIDR Predict Channel pSubscribing failed for channel = " + channelRegEx , e ) ; stopSubscription ( ) ; Thread . currentThread ( ) . interrupt ( ) ; } }
public void run ( ) { Thread . currentThread ( ) . setName ( "ChannelBufferManager Redis subscription Thread" ) ; logger . info ( "New thread <" + Thread . currentThread ( ) . getName ( ) + "> created for subscribing to redis channel: " + channelRegEx ) ; code_block = TryStatement ;  Thread . currentThread ( ) . interrupt ( ) ; logger . info ( "Exiting thread: " + Thread . currentThread ( ) . getName ( ) ) ; }
@ Override public void runCompareTest ( ) throws Exception { logger . info ( "Diffs saved in: {}" , targetDir . getAbsolutePath ( ) ) ; assertFalse ( "Dir should not exist: " + targetDir . getAbsolutePath ( ) , targetDir . exists ( ) ) ; assertTrue ( "Unable to create: " + targetDir . getAbsolutePath ( ) , targetDir . mkdirs ( ) ) ; assertTrue ( new File ( targetDir , "0_deltas_go_here.txt" ) . createNewFile ( ) ) ; preRunHook ( ) ; int failedQueries = 0 ; code_block = ForStatement ; assertThat ( "Number of failed queries" , failedQueries , is ( 0 ) ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "insert trace: {}" , spanBo ) ; } }
public String getInstanceState ( String instanceId ) { LOGGER . debug ( "getInstanceState('{}') entered" , instanceId ) ; DescribeInstancesResult result = getEC2 ( ) . describeInstances ( new DescribeInstancesRequest ( ) . withInstanceIds ( instanceId ) ) ; List < Reservation > reservations = result . getReservations ( ) ; Set < Instance > instances = new HashSet < Instance > ( ) ; code_block = ForStatement ; LOGGER . debug ( "getInstanceState('{}') left" , instanceId ) ; return null ; }
public void test() { if ( instances . size ( ) > 0 ) { String state = instances . iterator ( ) . next ( ) . getState ( ) . getName ( ) ; LOGGER . debug ( "  InstanceState: {}" , state ) ; return state ; } }
public String getInstanceState ( String instanceId ) { LOGGER . debug ( "getInstanceState('{}') entered" , instanceId ) ; DescribeInstancesResult result = getEC2 ( ) . describeInstances ( new DescribeInstancesRequest ( ) . withInstanceIds ( instanceId ) ) ; List < Reservation > reservations = result . getReservations ( ) ; Set < Instance > instances = new HashSet < Instance > ( ) ; code_block = ForStatement ; LOGGER . debug ( "getInstanceState('{}') left" , instanceId ) ; return null ; }
public void test() { if ( Log . isErrorEnabled ( ) ) { Log . error ( "[execute] The history cannot be fetched." , e ) ; } }
public void test() { if ( sentTime == null ) { logger . warn ( "Request has not been sent for {}." , this ) ; } }
@ Override public void update ( Object args , Observable observable ) { logger . info ( "[update]{}" , args ) ; this . currentState . refresh ( ) ; }
@ ExceptionHandler ( InvalidTokenException . class ) public ResponseEntity < OAuth2Exception > handleException ( Exception e ) throws Exception { logger . info ( "Handling error: " + e . getClass ( ) . getSimpleName ( ) + ", " + e . getMessage ( ) ) ; InvalidTokenException e400 = new InvalidTokenException ( e . getMessage ( ) ) code_block = "" ; ; return exceptionTranslator . translate ( e400 ) ; }
@ Test public void runBothDates ( ) throws IOException { File createTempDir = Files . createTempDir ( ) ; String randomString = UUID . randomUUID ( ) . toString ( ) ; File testOutFile = new File ( createTempDir , randomString + ".txt" ) ; String listCommand = "-p net.sourceforge.seqware.pipeline.plugins.WorkflowRunReporter " + "-- --output-filename " + testOutFile . getName ( ) + " --workflow-accession 2861 --time-period 2012-01-01:2012-01-15 " ; String listOutput = ITUtility . runSeqWareJar ( listCommand , ReturnValue . SUCCESS , createTempDir ) ; Log . info ( listOutput ) ; File retrievedFile = new File ( createTempDir , testOutFile . getName ( ) ) ; Assert . assertTrue ( "output file does not exist" , retrievedFile . exists ( ) ) ; List < String > readLines = FileUtils . readLines ( testOutFile , StandardCharsets . UTF_8 ) ; Assert . assertTrue ( "incorrect number of lines " , readLines . size ( ) == 4 ) ; long checksumCRC32 = FileUtils . checksumCRC32 ( testOutFile ) ; Assert . assertTrue ( "incorrect output checksum " + checksumCRC32 + " " + FileUtils . readFileToString ( retrievedFile , StandardCharsets . UTF_8 ) , checksumCRC32 == 562223107L || checksumCRC32 == 4072825873L ) ; }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Unregistering RESTEasy servlet with an alias '" + _alias + "'" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Blob s3://" + bucketName + "/" + bucketKey + " already exists" ) ; } }
public void test() { if ( _log . isInfoEnabled ( ) ) { _log . info ( "Skipping index settings contributor" ) ; } }
public void test() { try { ReturnT returnT = GsonTool . fromJson ( resultJson , ReturnT . class , returnTargClassOfT ) ; return returnT ; } catch ( Exception e ) { logger . error ( "xxl-rpc remoting (url=" + url + ") response content invalid(" + resultJson + ")." , e ) ; return new ReturnT < String > ( ReturnT . FAIL_CODE , "xxl-rpc remoting (url=" + url + ") response content invalid(" + resultJson + ")." ) ; } }
public void test() { try { URL realUrl = new URL ( url ) ; connection = ( HttpURLConnection ) realUrl . openConnection ( ) ; boolean useHttps = url . startsWith ( "https" ) ; code_block = IfStatement ; connection . setRequestMethod ( "POST" ) ; connection . setDoOutput ( true ) ; connection . setDoInput ( true ) ; connection . setUseCaches ( false ) ; connection . setReadTimeout ( timeout * 1000 ) ; connection . setConnectTimeout ( 3 * 1000 ) ; connection . setRequestProperty ( "connection" , "Keep-Alive" ) ; connection . setRequestProperty ( "Content-Type" , "application/json;charset=UTF-8" ) ; connection . setRequestProperty ( "Accept-Charset" , "application/json;charset=UTF-8" ) ; code_block = IfStatement ; connection . connect ( ) ; code_block = IfStatement ; int statusCode = connection . getResponseCode ( ) ; code_block = IfStatement ; bufferedReader = new BufferedReader ( new InputStreamReader ( connection . getInputStream ( ) , "UTF-8" ) ) ; StringBuilder result = new StringBuilder ( ) ; String line ; code_block = WhileStatement ; String resultJson = result . toString ( ) ; code_block = TryStatement ;  } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; return new ReturnT < String > ( ReturnT . FAIL_CODE , "xxl-rpc remoting error(" + e . getMessage ( ) + "), for url : " + url ) ; } finally { code_block = TryStatement ;  } }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; } catch ( Exception e2 ) { logger . error ( e2 . getMessage ( ) , e2 ) ; } }
public void test() { try { futureTimeToAlertTs = System . currentTimeMillis ( ) + interval ; info ( "starting watchdog timer %s - expecting checkpoint in %d ms" , name , interval ) ; code_block = WhileStatement ; } catch ( Exception e2 ) { log . error ( "watchdog threw" , e2 ) ; deactivate ( ) ; } }
public void test() { -> { CreateDirectoryPOptions mergedOptions = FileSystemOptions . createDirectoryDefaults ( mFsContext . getPathConf ( path ) ) . toBuilder ( ) . mergeFrom ( options ) . build ( ) ; client . createDirectory ( path , mergedOptions ) ; LOG . debug ( "Created directory {}, options: {}" , path . getPath ( ) , mergedOptions ) ; return null ; } }
public void test() { try { LOGGER . info ( "Trying to send query : {}" , query ) ; runQuery ( query , Collections . singletonList ( query . replace ( "'mytable'" , "mytable" ) ) ) ; } catch ( Exception e ) { LOGGER . error ( "Getting erro for query : {}" , query ) ; } }
public void test() { try { LOGGER . info ( "Trying to send query : {}" , query ) ; runQuery ( query , Collections . singletonList ( query . replace ( "'mytable'" , "mytable" ) ) ) ; } catch ( Exception e ) { LOGGER . error ( "Getting erro for query : {}" , query ) ; } }
@ Test public void testGenerateJson ( ) { BandInfoParameters object = new BandInfoParameters ( ) ; List < BandInfoParameters > objects = new ArrayList < BandInfoParameters > ( ) ; objects . add ( object ) ; LOGGER . info ( new JsonUtils ( ) . convertBandInfos ( objects ) ) ; }
public void test() { try { return _assetEntryService . getEntriesCount ( assetEntryQuery ) ; } catch ( Exception exception ) { _log . error ( "Unable to get asset entries count" , exception ) ; } }
public void test() { if ( log . isErrorEnabled ( ) ) { log . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( terminationEarlySuccessful ) { logger . info ( "Terminating solver early." ) ; } }
public void test() { try { subEquipmentConfiguration . setAliveTagId ( Long . parseLong ( getTagValue ( subEquipmentElement , ALIVE_TAG_ID_ELEMENT ) ) ) ; } catch ( NullPointerException e ) { log . debug ( "SubEquipment has no alive tag id." ) ; } }
public void test() { try { subEquipmentConfiguration . setAliveInterval ( Long . parseLong ( getTagValue ( subEquipmentElement , ALIVE_INTERVAL_ELEMENT ) ) ) ; } catch ( NullPointerException e ) { log . debug ( "SubEquipment has no alive tag interval." ) ; } }
public void test() { try { PhaseWorkerCall call = new PhaseWorkerCall ( ) ; do code_block = "" ; while ( LIFECYCLE_EXECUTOR . getActiveCount ( ) <= getMinThreads ( ) ) ; } catch ( Throwable t ) { LOG . fatal ( "Fatal error in View Lifecycle worker" , t ) ; } }
public void handleSetRandomisationSettingsResponse ( final DeviceMessageMetadata deviceMessageMetadata , final ResponseMessageResultType deviceResult , final OsgpException exception ) { LOGGER . info ( "handle SetRandomisationSettings response for MessageType: {}" , deviceMessageMetadata . getMessageType ( ) ) ; ResponseMessageResultType result = deviceResult ; code_block = IfStatement ; final ResponseMessage responseMessage = ResponseMessage . newResponseMessageBuilder ( ) . withCorrelationUid ( deviceMessageMetadata . getCorrelationUid ( ) ) . withOrganisationIdentification ( deviceMessageMetadata . getOrganisationIdentification ( ) ) . withDeviceIdentification ( deviceMessageMetadata . getDeviceIdentification ( ) ) . withResult ( result ) . withOsgpException ( exception ) . withMessagePriority ( deviceMessageMetadata . getMessagePriority ( ) ) . build ( ) ; this . webServiceResponseMessageSender . send ( responseMessage , deviceMessageMetadata . getMessageType ( ) ) ; }
public void test() { if ( exception != null ) { LOGGER . error ( DEVICE_RESPONSE_NOT_OK_LOG_MSG , exception ) ; result = ResponseMessageResultType . NOT_OK ; } }
public void test() { if ( debug ) { logger . debug ( "after comma: " + tt ) ; } }
public void test() { if ( ex != null ) { logger . error ( "Python operation encountered an exception: " + ex . toString ( ) ) ; } }
public void test() { try { link = new URI ( uri ) ; entry . setLink ( link ) ; } catch ( URISyntaxException e ) { logger . info ( "Feed contains illegal 'link' element content:" + uri ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "Got geometry from feed: " + where ) ; } }
@ Override public ListIterator < Object > listIterator ( ) { Collections . shuffle ( cachedEntityList , workingRandom ) ; logger . trace ( "    Shuffled cachedEntityList with size ({}) in entitySelector({})." , cachedEntityList . size ( ) , this ) ; return cachedEntityList . listIterator ( ) ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( sb . toString ( ) ) ; } }
@ POST @ JacksonSerialized @ Produces ( MediaType . APPLICATION_JSON ) public Response addWorkerContainer ( ConnectWorkerContainer connectWorkerContainer ) { LOG . info ( "Worker container: " + connectWorkerContainer . getEndpointUrl ( ) + " was detected" ) ; this . workerAdministrationManagement . register ( connectWorkerContainer ) ; return ok ( Notifications . success ( "Worker Container sucessfully added" ) ) ; }
public void test() { if ( retries == 0 ) { LOG . info ( "Creating Schema History table " + table + ( baseline ? " with baseline" : "" ) + " ..." ) ; } }
public void test() { try { LOG . debug ( "Schema History table creation failed. Retrying in 1 sec ..." ) ; Thread . sleep ( 1000 ) ; } catch ( InterruptedException e1 ) { } }
public void test() { try { UserGroupInformation ugi = UgiFactory . getUgi ( user ) ; final long startTime = System . nanoTime ( ) ; String id = queueAsUser ( ugi , args ) ; long elapsed = ( ( System . nanoTime ( ) - startTime ) / ( ( int ) 1e6 ) ) ; LOG . debug ( "queued job " + id + " in " + elapsed + " ms" ) ; if ( id == null ) throw new QueueException ( "Unable to get job id" ) ; registerJob ( id , user , callback , userArgs ) ; return new EnqueueBean ( id ) ; } catch ( InterruptedException e ) { throw new QueueException ( "Unable to launch job " + e ) ; } }
@ PUT public Response put ( @ PathParam ( "path" ) final String externalPath ) throws Exception { final FedoraId id = identifierConverter ( ) . pathToInternalId ( externalPath ) ; LOGGER . trace ( "PUT: {}" , id . getFullIdPath ( ) ) ; return doRequest ( id ) ; }
public void test() { try ( InputStream is = new ByteArrayInputStream ( readAttempt . result ( ) . getBytes ( ) ) ) { networkConfig . load ( is ) ; } catch ( final IOException e ) { LOG . warn ( "skipping malformed NetworkConfig properties [{}]" , fileName ) ; } }
public void test() { if ( readAttempt . succeeded ( ) ) { code_block = TryStatement ;  } else { LOG . warn ( "error reading NetworkConfig file [{}]" , fileName , readAttempt . cause ( ) ) ; } }
public void test() { if ( debug ) { logger . info ( "rootBound = " + M ) ; } }
@ Override public void save ( Device profile ) { LOGGER . info ( "Save: " + profile ) ; deviceService . save ( profile ) ; Audit . logSave ( profile ) ; }
public void test() { try { task . run ( ) ; } catch ( Exception e ) { log . error ( "Failed to execute shutdown hook" , e ) ; } }
public void setConfigFile ( String file ) { _log . info ( "config service override of configFile=" + file ) ; configFile = file ; }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Removing completion key: {}" , key ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( interruptedException , interruptedException ) ; } }
public void test() { try { KeyStoreManager keyStoreManager = KeyStoreManager . getInstance ( MultitenantConstants . SUPER_TENANT_ID ) ; this . publicCert = keyStoreManager . getDefaultPrimaryCertificate ( ) ; } catch ( Exception e ) { String error = "Error in obtaining keystore" ; log . debug ( error , e ) ; } }
@ Test public void TestCreateVfModuleFailure_5000 ( ) { new MockAAIGenericVnfSearch ( wireMockServer ) ; MockAAICreateGenericVnf ( wireMockServer ) ; MockAAIVfModulePUT ( wireMockServer , true ) ; Map < String , Object > variables = new HashMap < > ( ) ; variables . put ( "isDebugLogEnabled" , "true" ) ; variables . put ( "isVidRequest" , "false" ) ; variables . put ( "vnfId" , "a27ce5a9-29c4-4c22-a017-6615ac73c721" ) ; variables . put ( "serviceId" , "99999999-9999-9999-9999-999999999999" ) ; variables . put ( "personaModelId" , "973ed047-d251-4fb9-bf1a-65b8949e0a73" ) ; variables . put ( "personaModelVersion" , "1.0" ) ; variables . put ( "vfModuleName" , "STMTN5MMSC21-PCRF::module-1-0" ) ; variables . put ( "vfModuleModelName" , "STMTN5MMSC21-PCRF::model-1-0" ) ; variables . put ( "mso-request-id" , UUID . randomUUID ( ) . toString ( ) ) ; String processId = invokeSubProcess ( "CreateAAIVfModule" , variables ) ; WorkflowException exception = BPMNUtil . getRawVariable ( processEngine , "CreateAAIVfModule" , "WorkflowException" , processId ) ; Assert . assertEquals ( 5000 , exception . getErrorCode ( ) ) ; Assert . assertEquals ( true , exception . getErrorMessage ( ) . contains ( "<messageId>SVC3002</messageId>" ) ) ; logger . debug ( exception . getErrorMessage ( ) ) ; }
public void attachDirty ( MBstnStatus instance ) { log . debug ( "attaching dirty MBstnStatus instance" ) ; code_block = TryStatement ;  }
public void test() { try { getSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { getSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
@ Test public void findProjectsByFuzzyOwnerTest ( ) { LOGGER . info ( "FindProjects by owner fuzzy search test start................................" ) ; code_block = IfStatement ; GetUser getUserRequest = GetUser . newBuilder ( ) . setEmail ( authClientInterceptor . getClient1Email ( ) ) . build ( ) ; UserInfo testUser1 = uacServiceStub . getUser ( getUserRequest ) ; String testUser1UserName = testUser1 . getVertaInfo ( ) . getUsername ( ) ; Value stringValue = Value . newBuilder ( ) . setStringValue ( testUser1UserName . substring ( 0 , 2 ) ) . build ( ) ; KeyValueQuery keyValueQuery = KeyValueQuery . newBuilder ( ) . setKey ( "owner" ) . setValue ( stringValue ) . setOperator ( OperatorEnum . Operator . CONTAIN ) . build ( ) ; FindProjects findProjects = FindProjects . newBuilder ( ) . addPredicates ( keyValueQuery ) . build ( ) ; FindProjects . Response response = projectServiceStub . findProjects ( findProjects ) ; LOGGER . info ( "FindProjects Response : " + response . getProjectsList ( ) ) ; assertEquals ( "Project count not match with expected project count" , 4 , response . getProjectsList ( ) . size ( ) ) ; assertEquals ( "Total records count not matched with expected records count" , 4 , response . getTotalRecords ( ) ) ; keyValueQuery = KeyValueQuery . newBuilder ( ) . setKey ( "owner" ) . setValue ( stringValue ) . setOperator ( OperatorEnum . Operator . NOT_CONTAIN ) . build ( ) ; findProjects = FindProjects . newBuilder ( ) . addPredicates ( keyValueQuery ) . build ( ) ; response = projectServiceStub . findProjects ( findProjects ) ; assertEquals ( "Total records count not matched with expected records count" , 0 , response . getTotalRecords ( ) ) ; assertEquals ( "Project count not match with expected project count" , 0 , response . getProjectsCount ( ) ) ; stringValue = Value . newBuilder ( ) . setStringValue ( "asdasdasd" ) . build ( ) ; keyValueQuery = KeyValueQuery . newBuilder ( ) . setKey ( "owner" ) . setValue ( stringValue ) . setOperator ( OperatorEnum . Operator . CONTAIN ) . build ( ) ; findProjects = FindProjects . newBuilder ( ) . addPredicates ( keyValueQuery ) . build ( ) ; response = projectServiceStub . findProjects ( findProjects ) ; LOGGER . info ( "FindProjects Response : " + response . getProjectsList ( ) ) ; assertEquals ( "Project count not match with expected project count" , 0 , response . getProjectsList ( ) . size ( ) ) ; LOGGER . info ( "FindProjects by owner fuzzy search test stop ................................" ) ; }
@ Test public void findProjectsByFuzzyOwnerTest ( ) { LOGGER . info ( "FindProjects by owner fuzzy search test start................................" ) ; code_block = IfStatement ; GetUser getUserRequest = GetUser . newBuilder ( ) . setEmail ( authClientInterceptor . getClient1Email ( ) ) . build ( ) ; UserInfo testUser1 = uacServiceStub . getUser ( getUserRequest ) ; String testUser1UserName = testUser1 . getVertaInfo ( ) . getUsername ( ) ; Value stringValue = Value . newBuilder ( ) . setStringValue ( testUser1UserName . substring ( 0 , 2 ) ) . build ( ) ; KeyValueQuery keyValueQuery = KeyValueQuery . newBuilder ( ) . setKey ( "owner" ) . setValue ( stringValue ) . setOperator ( OperatorEnum . Operator . CONTAIN ) . build ( ) ; FindProjects findProjects = FindProjects . newBuilder ( ) . addPredicates ( keyValueQuery ) . build ( ) ; FindProjects . Response response = projectServiceStub . findProjects ( findProjects ) ; LOGGER . info ( "FindProjects Response : " + response . getProjectsList ( ) ) ; assertEquals ( "Project count not match with expected project count" , 4 , response . getProjectsList ( ) . size ( ) ) ; assertEquals ( "Total records count not matched with expected records count" , 4 , response . getTotalRecords ( ) ) ; keyValueQuery = KeyValueQuery . newBuilder ( ) . setKey ( "owner" ) . setValue ( stringValue ) . setOperator ( OperatorEnum . Operator . NOT_CONTAIN ) . build ( ) ; findProjects = FindProjects . newBuilder ( ) . addPredicates ( keyValueQuery ) . build ( ) ; response = projectServiceStub . findProjects ( findProjects ) ; assertEquals ( "Total records count not matched with expected records count" , 0 , response . getTotalRecords ( ) ) ; assertEquals ( "Project count not match with expected project count" , 0 , response . getProjectsCount ( ) ) ; stringValue = Value . newBuilder ( ) . setStringValue ( "asdasdasd" ) . build ( ) ; keyValueQuery = KeyValueQuery . newBuilder ( ) . setKey ( "owner" ) . setValue ( stringValue ) . setOperator ( OperatorEnum . Operator . CONTAIN ) . build ( ) ; findProjects = FindProjects . newBuilder ( ) . addPredicates ( keyValueQuery ) . build ( ) ; response = projectServiceStub . findProjects ( findProjects ) ; LOGGER . info ( "FindProjects Response : " + response . getProjectsList ( ) ) ; assertEquals ( "Project count not match with expected project count" , 0 , response . getProjectsList ( ) . size ( ) ) ; LOGGER . info ( "FindProjects by owner fuzzy search test stop ................................" ) ; }
@ Test public void testQuery003 ( ) throws Exception { log . info ( "------ Test a*.* AND b ------" ) ; Set < String > expected = new HashSet < > ( ) ; String query = CarField . COLOR . name ( ) + " =~ 'bl.*s.*' and " + CarField . WHEELS . name ( ) + " == '4'" ; runTest ( query , expected ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "page complete: " + complete ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( format ( "Preparing static statements for entity of type %s" , entityClass . getCanonicalName ( ) ) ) ; } }
public void test() { try { Foreach internalForeach = createForeach ( ) ; Processor nestedEventProcessor = event code_block = LoopStatement ; ; internalForeach . setMessageProcessors ( singletonList ( nestedEventProcessor ) ) ; Foreach nestedForeach = createForeach ( ) ; nestedForeach . setMessageProcessors ( singletonList ( internalForeach ) ) ; initialiseIfNeeded ( nestedForeach , muleContext ) ; nestedForeach . process ( nestedForeachEvent ) ; } catch ( Throwable t ) { LOGGER . error ( "Unexpected error on nestedForeach" , t ) ; } }
public void test() { try { return helper . fetchPage ( sqlCount . toString ( ) , sql . toString ( ) , paramList . toArray ( ) , pageNo , pageSize , CONFIG_INFO_ROW_MAPPER ) ; } catch ( CannotGetJdbcConnectionException e ) { LogUtil . FATAL_LOG . error ( "[db-error] " + e . toString ( ) , e ) ; throw e ; } }
public void test() { if ( debug ) { log . debug ( "Rollback: " + task . getDescription ( ) ) ; } }
public void test() { try { code_block = IfStatement ; task . rollback ( ) ; } catch ( Exception e ) { log . error ( "Exception during NodeChangeListener's rollback task: " + task . getDescription ( ) , e ) ; errors = true ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( "Deploying " + servletContextName + " from queue" ) ; } }
public void test() { try { hotDeployListener . invokeDeploy ( hotDeployEvent ) ; } catch ( HotDeployException hotDeployException ) { _log . error ( hotDeployException , hotDeployException ) ; } finally { PortletClassLoaderUtil . setServletContextName ( null ) ; } }
public void test() { try { handleSendFailure ( e , Event . Type . SEND_EVENT_REQUEST , buffer ) ; } catch ( Exception e1 ) { logger . error ( "Failure handling buffer send failure" , e1 ) ; } }
public void test() { try { this . handleEvent ( eventQueue . remove ( 0 ) ) ; } catch ( Exception e ) { logger . error ( "Failure handling queued event" , e ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "==> RangerTagEnricher.preCleanup()" ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "<== RangerTagEnricher.preCleanup() : result=" + true ) ; } }
public void test() { { log . info ( Color . BLUE + "4-Network-1 pattern lowercase" + Color . NORMAL ) ; Context context = initValidatorContext ( ) ; Assert . assertNotNull ( fullparameters , "no parameters for test" ) ; context . put ( VALIDATION_REPORT , new ValidationReport ( ) ) ; bean1 . setRegistrationNumber ( "azerty" ) ; bean2 . setRegistrationNumber ( "az234ZDER" ) ; fullparameters . getNetwork ( ) . getRegistrationNumber ( ) . setPattern ( AbstractValidation . PATTERN_OPTION . lower . ordinal ( ) ) ; context . put ( VALIDATION , fullparameters ) ; ValidationData data = new ValidationData ( ) ; data . getNetworks ( ) . addAll ( beansFor4 ) ; context . put ( VALIDATION_DATA , data ) ; checkPoint . validate ( context , null ) ; ValidationReport report = ( ValidationReport ) context . get ( VALIDATION_REPORT ) ; checkReportForTest ( report , "4-Network-1" , 1 ) ; } }
public void test() { try code_block = "" ; catch ( RuntimeException e1 ) { log . error ( e1 , "Exception thrown when closing maker.  Logging and ignoring." ) ; } }
public void test() { try { LOG . info ( "closing connection" ) ; m_impl . close ( ) ; LOG . info ( "connection closed" ) ; } catch ( JMSException e ) { BEANS . get ( MomExceptionHandler . class ) . handle ( e ) ; } finally { m_impl = null ; } }
public void test() { try { LOG . info ( "closing connection" ) ; m_impl . close ( ) ; LOG . info ( "connection closed" ) ; } catch ( JMSException e ) { BEANS . get ( MomExceptionHandler . class ) . handle ( e ) ; } finally { m_impl = null ; } }
public void test() { try { return migration . migrate ( s ) ; } catch ( Exception e ) { LOGGER . error ( "Unable to migrate input '{}' (keep previous value)." , s ) ; return s ; } }
public void test() { try { if ( rwFileChannel != null ) rwFileChannel . close ( ) ; rwFileChannel = null ; if ( rwRaf != null ) rwRaf . close ( ) ; rwRaf = null ; rwFile = null ; File parent = new File ( baseDir ) ; code_block = IfStatement ; rwFile = FileUtils . getFile ( baseDir , SystemClock . now ( ) + "" ) ; rwRaf = new RandomAccessFile ( rwFile , "rw" ) ; rwFileChannel = rwRaf . getChannel ( ) ; rwMap = rwFileChannel . map ( FileChannel . MapMode . READ_WRITE , 0 , pageSize ) ; } catch ( Exception ex ) { logger . error ( "create and mapped file error." , ex ) ; } }
public void test() { if ( StringUtils . isNotBlank ( defaultFSProp ) ) { Map < String , String > fsRelatedProps = PropertyUtils . getPrefixedProperties ( "fs." ) ; configuration . set ( Constants . FS_DEFAULTFS , defaultFSProp ) ; fsRelatedProps . forEach ( ( key , value ) -> configuration . set ( key , value ) ) ; } else { logger . error ( "property:{} can not to be empty, please set!" , Constants . FS_DEFAULTFS ) ; throw new RuntimeException ( String . format ( "property: %s can not to be empty, please set!" , Constants . FS_DEFAULTFS ) ) ; } }
public void test() { if ( defaultFS . startsWith ( "file" ) ) { String defaultFSProp = PropertyUtils . getString ( Constants . FS_DEFAULTFS ) ; code_block = IfStatement ; } else { logger . info ( "get property:{} -> {}, from core-site.xml hdfs-site.xml " , Constants . FS_DEFAULTFS , defaultFS ) ; } }
public void test() { try { configuration = new HdfsConfiguration ( ) ; String resourceStorageType = PropertyUtils . getUpperCaseString ( Constants . RESOURCE_STORAGE_TYPE ) ; ResUploadType resUploadType = ResUploadType . valueOf ( resourceStorageType ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; } }
@ Test public void wildcardResourcesAreOrderedAlphabetically ( ) { final WroModel model = new WroModel ( ) ; final String uri = String . format ( ClasspathUriLocator . PREFIX + "%s/expander/order/**.js" , WroUtil . toPackageAsFolder ( getClass ( ) ) ) ; model . addGroup ( new Group ( "group" ) . addResource ( Resource . create ( uri , ResourceType . JS ) ) ) ; Mockito . when ( decoratedFactory . create ( ) ) . thenReturn ( model ) ; final WroModel changedModel = transformer . transform ( model ) ; LOG . debug ( "model: {}" , changedModel ) ; Assert . assertEquals ( 7 , changedModel . getGroupByName ( "group" ) . getResources ( ) . size ( ) ) ; final List < Resource > resources = changedModel . getGroupByName ( "group" ) . getResources ( ) ; Assert . assertEquals ( "01-xyc.js" , FilenameUtils . getName ( resources . get ( 0 ) . getUri ( ) ) ) ; Assert . assertEquals ( "02-xyc.js" , FilenameUtils . getName ( resources . get ( 1 ) . getUri ( ) ) ) ; Assert . assertEquals ( "03-jquery-ui.js" , FilenameUtils . getName ( resources . get ( 2 ) . getUri ( ) ) ) ; Assert . assertEquals ( "04-xyc.js" , FilenameUtils . getName ( resources . get ( 3 ) . getUri ( ) ) ) ; Assert . assertEquals ( "05-xyc.js" , FilenameUtils . getName ( resources . get ( 4 ) . getUri ( ) ) ) ; Assert . assertEquals ( "06-xyc.js" , FilenameUtils . getName ( resources . get ( 5 ) . getUri ( ) ) ) ; Assert . assertEquals ( "07-jquery-impromptu.js" , FilenameUtils . getName ( resources . get ( 6 ) . getUri ( ) ) ) ; }
public void test() { -> { UfsJournalProgressLogger progressLogger = new UfsJournalProgressLogger ( mJournal , finalSN , ( ) -> mLastAppliedSN ) ; code_block = WhileStatement ; } }
@ Override public void contextDestroyed ( ServletContextEvent servletContextEvent ) { LOGGER . info ( "Destroying Web application" ) ; WebApplicationContext ac = WebApplicationContextUtils . getRequiredWebApplicationContext ( servletContextEvent . getServletContext ( ) ) ; ConfigurableApplicationContext gwac = ( ConfigurableApplicationContext ) ac ; gwac . close ( ) ; LOGGER . log ( Level . FINE , "Web application destroyed" ) ; }
public void test() { if ( log . isWarnEnabled ( ) ) { log . warn ( "Message could not be built. [exception=({})]" , e . getMessage ( ) , e ) ; } }
public void test() { if ( ! unsanitizedDn . equals ( decodedDn ) ) { logger . warn ( "The provided DN [" + sanitizedDn + "] had been escaped, and was reconstituted to the dangerous DN [" + unsanitizedDn + "]" ) ; } }
public void test() { if ( t != null ) { LOG . error ( "Exception when executing " + r , t ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "BEGIN createAcl: objectIdentity: " + objectIdentity ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "END createAcl: acl: " + acl ) ; } }
public void test() { switch ( status ) { case FAILURE : LOGGER . info ( "Failure device message status received: {}" , status ) ; throw new TechnicalException ( ComponentType . PROTOCOL_OSLP , "Device reports failure" ) ; case REJECTED : LOGGER . info ( "Rejected device message status received: {}" , status ) ; throw new TechnicalException ( ComponentType . PROTOCOL_OSLP , "Device reports rejected" ) ; case OK : LOGGER . info ( "OK device message status received: {}" , status ) ; break ; default : LOGGER . warn ( "Unknown device message status received: {}" , status ) ; break ; } }
public void test() { switch ( status ) { case FAILURE : LOGGER . info ( "Failure device message status received: {}" , status ) ; throw new TechnicalException ( ComponentType . PROTOCOL_OSLP , "Device reports failure" ) ; case REJECTED : LOGGER . info ( "Rejected device message status received: {}" , status ) ; throw new TechnicalException ( ComponentType . PROTOCOL_OSLP , "Device reports rejected" ) ; case OK : LOGGER . info ( "OK device message status received: {}" , status ) ; break ; default : LOGGER . warn ( "Unknown device message status received: {}" , status ) ; break ; } }
public void test() { switch ( status ) { case FAILURE : LOGGER . info ( "Failure device message status received: {}" , status ) ; throw new TechnicalException ( ComponentType . PROTOCOL_OSLP , "Device reports failure" ) ; case REJECTED : LOGGER . info ( "Rejected device message status received: {}" , status ) ; throw new TechnicalException ( ComponentType . PROTOCOL_OSLP , "Device reports rejected" ) ; case OK : LOGGER . info ( "OK device message status received: {}" , status ) ; break ; default : LOGGER . warn ( "Unknown device message status received: {}" , status ) ; break ; } }
public void test() { switch ( status ) { case FAILURE : LOGGER . info ( "Failure device message status received: {}" , status ) ; throw new TechnicalException ( ComponentType . PROTOCOL_OSLP , "Device reports failure" ) ; case REJECTED : LOGGER . info ( "Rejected device message status received: {}" , status ) ; throw new TechnicalException ( ComponentType . PROTOCOL_OSLP , "Device reports rejected" ) ; case OK : LOGGER . info ( "OK device message status received: {}" , status ) ; break ; default : LOGGER . warn ( "Unknown device message status received: {}" , status ) ; break ; } }
public void test() { if ( file . createNewFile ( ) ) { logger . debug ( "File created: {}" , file . getName ( ) ) ; final boolean append = true ; BufferedWriter headerWriter = new BufferedWriter ( new OutputStreamWriter ( new FileOutputStream ( csvFile , append ) , StandardCharsets . UTF_8 ) ) ; final String headerToAppend = "ContainerId,Timestamp[uts],ReceivedMessages[msgs/sec],SentMessages[msgs/sec]" ; headerWriter . write ( headerToAppend ) ; headerWriter . newLine ( ) ; headerWriter . close ( ) ; } else { logger . debug ( "File already exists. Writting data to: {}" , csvFile ) ; } }
public void test() { if ( file . createNewFile ( ) ) { logger . debug ( "File created: {}" , file . getName ( ) ) ; final boolean append = true ; BufferedWriter headerWriter = new BufferedWriter ( new OutputStreamWriter ( new FileOutputStream ( csvFile , append ) , StandardCharsets . UTF_8 ) ) ; final String headerToAppend = "ContainerId,Timestamp[uts],ReceivedMessages[msgs/sec],SentMessages[msgs/sec]" ; headerWriter . write ( headerToAppend ) ; headerWriter . newLine ( ) ; headerWriter . close ( ) ; } else { logger . debug ( "File already exists. Writting data to: {}" , csvFile ) ; } }
@ Override public void exitLegacy_change_substitution ( Legacy_change_substitutionContext ctx ) { LOGGER . debug ( "Leaving legacy_change_substitution" ) ; LegacyLocation location = ( LegacyLocation ) getValue ( ctx . legacy_point_location ( ) ) ; String from = ctx . NT_STRING ( 0 ) . getText ( ) ; String to = ctx . NT_STRING ( 1 ) . getText ( ) ; setValue ( ctx , new LegacySubstitution ( location , from , to ) ) ; }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "calling setuseWorkflowPessimisticLocking '" + useWorkflowPessimisticLocking + "'" ) ; } }
public void test() { try { String bootstrapHostnames = podNames . stream ( ) . map ( podName -> KafkaCluster . podDnsName ( this . namespace , this . cluster , podName ) + ":" + KafkaCluster . REPLICATION_PORT ) . collect ( Collectors . joining ( "," ) ) ; log . debug ( "{}: Creating AdminClient for {}" , reconciliation , bootstrapHostnames ) ; return adminClientProvider . createAdminClient ( bootstrapHostnames , this . clusterCaCertSecret , this . coKeySecret , "cluster-operator" ) ; } catch ( KafkaException e ) { code_block = IfStatement ; } catch ( RuntimeException e ) { throw new ForceableProblem ( "An error while try to create an admin client with bootstrap brokers " + podNames , e ) ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( "Sending query to bigquery standard sql: {}" , translatedQuery ) ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( "Result of query {} is {}" , translatedQuery , result . toString ( ) ) ; } }
public void test() { if ( resource == null ) { ourLog . warn ( "Unable to find resource {}/{}/_history/{} in database" , next . getResourceType ( ) , next . getIdDt ( ) . getIdPart ( ) , next . getVersion ( ) ) ; continue ; } }
public void test() { if ( index == null ) { ourLog . warn ( "Got back unexpected resource PID {}" , resourceId ) ; continue ; } }
public void test() { try { configDescriptionProvider . add ( bundle , configDescription ) ; } catch ( RuntimeException e ) { logger . error ( "Could not register ConfigDescription: {}" , configDescription . getUID ( ) , e ) ; } }
public void replay ( RemoveTopicRecord record ) { TopicControlInfo topic = topics . remove ( record . topicId ( ) ) ; code_block = IfStatement ; topicsByName . remove ( topic . name ) ; configurationControl . deleteTopicConfigs ( topic . name ) ; code_block = ForStatement ; brokersToIsrs . removeTopicEntryForBroker ( topic . id , NO_LEADER ) ; controllerMetrics . setGlobalTopicsCount ( topics . size ( ) ) ; controllerMetrics . setGlobalPartitionCount ( globalPartitionCount . get ( ) ) ; controllerMetrics . setOfflinePartitionCount ( brokersToIsrs . offlinePartitionCount ( ) ) ; controllerMetrics . setPreferredReplicaImbalanceCount ( preferredReplicaImbalanceCount . get ( ) ) ; log . info ( "Removed topic {} with ID {}." , topic . name , record . topicId ( ) ) ; }
@ Override public void onFailedConnection ( Address address ) { logger . info ( "Failed connection: " + address ) ; }
public void test() { try { code_block = IfStatement ; } catch ( IllegalStateException e ) { } catch ( AccessControlException e ) { logger . warn ( e . toString ( ) , e ) ; } }
public void test() { if ( ! equal ) { log . error ( "Expected " + JexlStringBuildingVisitor . buildQuery ( expectedScript ) ) ; log . error ( "Actual   " + JexlStringBuildingVisitor . buildQuery ( actualScript ) ) ; log . error ( "Expected " + PrintingVisitor . formattedQueryString ( expectedScript ) ) ; log . error ( "Actual   " + PrintingVisitor . formattedQueryString ( actualScript ) ) ; } }
public void test() { if ( ! equal ) { log . error ( "Expected " + JexlStringBuildingVisitor . buildQuery ( expectedScript ) ) ; log . error ( "Actual   " + JexlStringBuildingVisitor . buildQuery ( actualScript ) ) ; log . error ( "Expected " + PrintingVisitor . formattedQueryString ( expectedScript ) ) ; log . error ( "Actual   " + PrintingVisitor . formattedQueryString ( actualScript ) ) ; } }
public void test() { if ( ! equal ) { log . error ( "Expected " + JexlStringBuildingVisitor . buildQuery ( expectedScript ) ) ; log . error ( "Actual   " + JexlStringBuildingVisitor . buildQuery ( actualScript ) ) ; log . error ( "Expected " + PrintingVisitor . formattedQueryString ( expectedScript ) ) ; log . error ( "Actual   " + PrintingVisitor . formattedQueryString ( actualScript ) ) ; } }
public void test() { if ( ! equal ) { log . error ( "Expected " + JexlStringBuildingVisitor . buildQuery ( expectedScript ) ) ; log . error ( "Actual   " + JexlStringBuildingVisitor . buildQuery ( actualScript ) ) ; log . error ( "Expected " + PrintingVisitor . formattedQueryString ( expectedScript ) ) ; log . error ( "Actual   " + PrintingVisitor . formattedQueryString ( actualScript ) ) ; } }
public void test() { try { final ReferenceTerm referenceTerm = new ReferenceTermImpl ( new URL ( referenceValue . getReference ( ) ) , Set . copyOf ( referenceValue . getEntityTypes ( ) ) ) ; return persistentEntityResolver . resolveByUri ( Set . of ( referenceTerm ) ) . getOrDefault ( referenceTerm , Collections . emptyList ( ) ) ; } catch ( MalformedURLException e ) { LOGGER . debug ( "There was a problem converting the input to ReferenceTermType" ) ; throw new IllegalArgumentException ( "The input values are invalid" , e ) ; } }
public void test() { if ( cause != null ) { LOG . error ( cause . getMessage ( ) , cause ) ; } }
public void test() { try { String inputPath = "../../data/census/census_148d_train.dummy" ; conf . set ( AngelConf . ANGEL_TRAIN_DATA_PATH , inputPath ) ; conf . set ( AngelConf . ANGEL_ACTION_TYPE , MLConf . ANGEL_ML_TRAIN ( ) ) ; conf . set ( AngelConf . ANGEL_LOAD_MODEL_PATH , LOCAL_FS + TMP_PATH + "/model/deepFM" ) ; conf . set ( AngelConf . ANGEL_SAVE_MODEL_PATH , LOCAL_FS + TMP_PATH + "/model/deepFM_new" ) ; GraphRunner runner = new GraphRunner ( ) ; runner . train ( conf ) ; } catch ( Exception x ) { LOG . error ( "run trainOnLocalClusterTest failed " , x ) ; throw x ; } }
public void test() { -> { LOG . error ( "error in here" , e ) ; } }
public void test() { try { final ConnectableObservable < Response > writeResponse = internalServiceFactory . getInternalGDMGraphService ( ) . updateObject ( dataModel . getUuid ( ) , connectableResult2 . observeOn ( GDM_SCHEDULER ) . onBackpressureBuffer ( 100000 ) , updateFormat , enableVersioning ) . onBackpressureBuffer ( 100000 ) . doOnSubscribe ( ( ) -> LOG . debug ( "subscribed to write response observable" ) ) . publish ( ) ; connectableResult2 . connect ( ) ; writeResponse . ignoreElements ( ) . cast ( Void . class ) . doOnError ( e code_block = LoopStatement ; ) ; final BlockingObservable < Response > blockingObservable = writeResponse . toBlocking ( ) ; writeResponse . connect ( ) ; connectableSource . connect ( ) ; blockingObservable . firstOrDefault ( null ) ; LOG . debug ( "processed {} data resource into data model '{}'" , type , dataModel . getUuid ( ) ) ; } catch ( final DMPPersistenceException e ) { final String message = String . format ( "couldn't persist the converted data of data model '%s'" , dataModel . getUuid ( ) ) ; ConverterEventRecorder . LOG . error ( message , e ) ; throw new DMPControllerException ( String . format ( "%s %s" , message , e . getMessage ( ) ) , e ) ; } }
public void test() { try { final ConnectableObservable < Response > writeResponse = internalServiceFactory . getInternalGDMGraphService ( ) . updateObject ( dataModel . getUuid ( ) , connectableResult2 . observeOn ( GDM_SCHEDULER ) . onBackpressureBuffer ( 100000 ) , updateFormat , enableVersioning ) . onBackpressureBuffer ( 100000 ) . doOnSubscribe ( ( ) -> LOG . debug ( "subscribed to write response observable" ) ) . publish ( ) ; connectableResult2 . connect ( ) ; writeResponse . ignoreElements ( ) . cast ( Void . class ) . doOnError ( e code_block = LoopStatement ; ) ; final BlockingObservable < Response > blockingObservable = writeResponse . toBlocking ( ) ; writeResponse . connect ( ) ; connectableSource . connect ( ) ; blockingObservable . firstOrDefault ( null ) ; LOG . debug ( "processed {} data resource into data model '{}'" , type , dataModel . getUuid ( ) ) ; } catch ( final DMPPersistenceException e ) { final String message = String . format ( "couldn't persist the converted data of data model '%s'" , dataModel . getUuid ( ) ) ; ConverterEventRecorder . LOG . error ( message , e ) ; throw new DMPControllerException ( String . format ( "%s %s" , message , e . getMessage ( ) ) , e ) ; } }
public void test() { if ( forceRegistration ) { LOG . info ( "ForceRegistration enabled, unregistering existing MBean with ObjectName: {}" , name ) ; server . unregisterMBean ( name ) ; } else { LOG . debug ( "MBean already registered with ObjectName: {}" , name ) ; } }
public void test() { if ( forceRegistration ) { LOG . info ( "ForceRegistration enabled, unregistering existing MBean with ObjectName: {}" , name ) ; server . unregisterMBean ( name ) ; } else { LOG . debug ( "MBean already registered with ObjectName: {}" , name ) ; } }
public void test() { if ( this . deregisterJDBCDriver ) { cleanupDrivers ( this . providedDrivers ) ; } else { LOGGER . debug ( "Deregistering of JDBC driver(s) is disabled!" ) ; } }
public boolean doPairing ( ) throws InterruptedException { logger . trace ( "Starting pairing openHAB Client with Bosch Smart Home Controller!" ) ; logger . trace ( "Please press the Bosch Smart Home Controller button until LED starts blinking" ) ; ContentResponse contentResponse ; code_block = TryStatement ;  }
public void test() { try { String publicCert = getCertFromSslContextFactory ( ) ; logger . trace ( "Pairing with SHC {}" , ipAddress ) ; Map < String , String > items = new HashMap < > ( ) ; items . put ( "@type" , "client" ) ; items . put ( "id" , BoschSslUtil . getBoschShcClientId ( ) ) ; items . put ( "name" , "oss_OpenHAB_Binding" ) ; items . put ( "primaryRole" , "ROLE_RESTRICTED_CLIENT" ) ; items . put ( "certificate" , "-----BEGIN CERTIFICATE-----\r" + publicCert + "\r-----END CERTIFICATE-----" ) ; String url = this . getPairingUrl ( ) ; Request request = this . createRequest ( url , HttpMethod . POST , items ) . header ( "Systempassword" , Base64 . getEncoder ( ) . encodeToString ( this . systemPassword . getBytes ( StandardCharsets . UTF_8 ) ) ) ; contentResponse = request . send ( ) ; logger . trace ( "Pairing response complete: {} - return code: {}" , contentResponse . getContentAsString ( ) , contentResponse . getStatus ( ) ) ; code_block = IfStatement ; } catch ( TimeoutException | CertificateEncodingException | KeyStoreException | NullPointerException e ) { logger . warn ( "Pairing failed with exception {}" , e . getMessage ( ) ) ; return false ; } catch ( ExecutionException e ) { logger . trace ( "Pairing failed - Details: {}" , e . getMessage ( ) ) ; logger . warn ( "Pairing failed. Was the Bosch Smart Home Controller button pressed?" ) ; return false ; } }
public void test() { try { String publicCert = getCertFromSslContextFactory ( ) ; logger . trace ( "Pairing with SHC {}" , ipAddress ) ; Map < String , String > items = new HashMap < > ( ) ; items . put ( "@type" , "client" ) ; items . put ( "id" , BoschSslUtil . getBoschShcClientId ( ) ) ; items . put ( "name" , "oss_OpenHAB_Binding" ) ; items . put ( "primaryRole" , "ROLE_RESTRICTED_CLIENT" ) ; items . put ( "certificate" , "-----BEGIN CERTIFICATE-----\r" + publicCert + "\r-----END CERTIFICATE-----" ) ; String url = this . getPairingUrl ( ) ; Request request = this . createRequest ( url , HttpMethod . POST , items ) . header ( "Systempassword" , Base64 . getEncoder ( ) . encodeToString ( this . systemPassword . getBytes ( StandardCharsets . UTF_8 ) ) ) ; contentResponse = request . send ( ) ; logger . trace ( "Pairing response complete: {} - return code: {}" , contentResponse . getContentAsString ( ) , contentResponse . getStatus ( ) ) ; code_block = IfStatement ; } catch ( TimeoutException | CertificateEncodingException | KeyStoreException | NullPointerException e ) { logger . warn ( "Pairing failed with exception {}" , e . getMessage ( ) ) ; return false ; } catch ( ExecutionException e ) { logger . trace ( "Pairing failed - Details: {}" , e . getMessage ( ) ) ; logger . warn ( "Pairing failed. Was the Bosch Smart Home Controller button pressed?" ) ; return false ; } }
public void test() { if ( 201 == contentResponse . getStatus ( ) ) { logger . debug ( "Pairing successful." ) ; return true ; } else { logger . info ( "Pairing failed with response status {}." , contentResponse . getStatus ( ) ) ; return false ; } }
public void test() { if ( 201 == contentResponse . getStatus ( ) ) { logger . debug ( "Pairing successful." ) ; return true ; } else { logger . info ( "Pairing failed with response status {}." , contentResponse . getStatus ( ) ) ; return false ; } }
public void test() { try { String publicCert = getCertFromSslContextFactory ( ) ; logger . trace ( "Pairing with SHC {}" , ipAddress ) ; Map < String , String > items = new HashMap < > ( ) ; items . put ( "@type" , "client" ) ; items . put ( "id" , BoschSslUtil . getBoschShcClientId ( ) ) ; items . put ( "name" , "oss_OpenHAB_Binding" ) ; items . put ( "primaryRole" , "ROLE_RESTRICTED_CLIENT" ) ; items . put ( "certificate" , "-----BEGIN CERTIFICATE-----\r" + publicCert + "\r-----END CERTIFICATE-----" ) ; String url = this . getPairingUrl ( ) ; Request request = this . createRequest ( url , HttpMethod . POST , items ) . header ( "Systempassword" , Base64 . getEncoder ( ) . encodeToString ( this . systemPassword . getBytes ( StandardCharsets . UTF_8 ) ) ) ; contentResponse = request . send ( ) ; logger . trace ( "Pairing response complete: {} - return code: {}" , contentResponse . getContentAsString ( ) , contentResponse . getStatus ( ) ) ; code_block = IfStatement ; } catch ( TimeoutException | CertificateEncodingException | KeyStoreException | NullPointerException e ) { logger . warn ( "Pairing failed with exception {}" , e . getMessage ( ) ) ; return false ; } catch ( ExecutionException e ) { logger . trace ( "Pairing failed - Details: {}" , e . getMessage ( ) ) ; logger . warn ( "Pairing failed. Was the Bosch Smart Home Controller button pressed?" ) ; return false ; } }
public void test() { try { String publicCert = getCertFromSslContextFactory ( ) ; logger . trace ( "Pairing with SHC {}" , ipAddress ) ; Map < String , String > items = new HashMap < > ( ) ; items . put ( "@type" , "client" ) ; items . put ( "id" , BoschSslUtil . getBoschShcClientId ( ) ) ; items . put ( "name" , "oss_OpenHAB_Binding" ) ; items . put ( "primaryRole" , "ROLE_RESTRICTED_CLIENT" ) ; items . put ( "certificate" , "-----BEGIN CERTIFICATE-----\r" + publicCert + "\r-----END CERTIFICATE-----" ) ; String url = this . getPairingUrl ( ) ; Request request = this . createRequest ( url , HttpMethod . POST , items ) . header ( "Systempassword" , Base64 . getEncoder ( ) . encodeToString ( this . systemPassword . getBytes ( StandardCharsets . UTF_8 ) ) ) ; contentResponse = request . send ( ) ; logger . trace ( "Pairing response complete: {} - return code: {}" , contentResponse . getContentAsString ( ) , contentResponse . getStatus ( ) ) ; code_block = IfStatement ; } catch ( TimeoutException | CertificateEncodingException | KeyStoreException | NullPointerException e ) { logger . warn ( "Pairing failed with exception {}" , e . getMessage ( ) ) ; return false ; } catch ( ExecutionException e ) { logger . trace ( "Pairing failed - Details: {}" , e . getMessage ( ) ) ; logger . warn ( "Pairing failed. Was the Bosch Smart Home Controller button pressed?" ) ; return false ; } }
public void test() { try { String publicCert = getCertFromSslContextFactory ( ) ; logger . trace ( "Pairing with SHC {}" , ipAddress ) ; Map < String , String > items = new HashMap < > ( ) ; items . put ( "@type" , "client" ) ; items . put ( "id" , BoschSslUtil . getBoschShcClientId ( ) ) ; items . put ( "name" , "oss_OpenHAB_Binding" ) ; items . put ( "primaryRole" , "ROLE_RESTRICTED_CLIENT" ) ; items . put ( "certificate" , "-----BEGIN CERTIFICATE-----\r" + publicCert + "\r-----END CERTIFICATE-----" ) ; String url = this . getPairingUrl ( ) ; Request request = this . createRequest ( url , HttpMethod . POST , items ) . header ( "Systempassword" , Base64 . getEncoder ( ) . encodeToString ( this . systemPassword . getBytes ( StandardCharsets . UTF_8 ) ) ) ; contentResponse = request . send ( ) ; logger . trace ( "Pairing response complete: {} - return code: {}" , contentResponse . getContentAsString ( ) , contentResponse . getStatus ( ) ) ; code_block = IfStatement ; } catch ( TimeoutException | CertificateEncodingException | KeyStoreException | NullPointerException e ) { logger . warn ( "Pairing failed with exception {}" , e . getMessage ( ) ) ; return false ; } catch ( ExecutionException e ) { logger . trace ( "Pairing failed - Details: {}" , e . getMessage ( ) ) ; logger . warn ( "Pairing failed. Was the Bosch Smart Home Controller button pressed?" ) ; return false ; } }
public void test() { try { LOG . debug ( "Safe close on stream using {}" , onClose ) ; onClose . run ( ) ; isClosed = true ; } catch ( Exception e ) { LOG . error ( "Unable to invoke onClose closure." , e ) ; } }
public void test() { if ( temperatureFormat . isPresent ( ) ) { updateState ( CHANNEL_AUX_TEMP , new QuantityType < > ( temperatureFormat . get ( ) . omniToFormat ( status . getTemperature ( ) ) , temperatureFormat . get ( ) . getFormatNumber ( ) == 1 ? ImperialUnits . FAHRENHEIT : SIUnits . CELSIUS ) ) ; updateState ( CHANNEL_AUX_LOW_SETPOINT , new QuantityType < > ( temperatureFormat . get ( ) . omniToFormat ( status . getCoolSetpoint ( ) ) , temperatureFormat . get ( ) . getFormatNumber ( ) == 1 ? ImperialUnits . FAHRENHEIT : SIUnits . CELSIUS ) ) ; updateState ( CHANNEL_AUX_HIGH_SETPOINT , new QuantityType < > ( temperatureFormat . get ( ) . omniToFormat ( status . getHeatSetpoint ( ) ) , temperatureFormat . get ( ) . getFormatNumber ( ) == 1 ? ImperialUnits . FAHRENHEIT : SIUnits . CELSIUS ) ) ; } else { logger . warn ( "Receieved null temperature format, could not update Temperature Sensor channels!" ) ; } }
public void test() { if ( bridgeHandler != null ) { Optional < TemperatureFormat > temperatureFormat = bridgeHandler . getTemperatureFormat ( ) ; code_block = IfStatement ; } else { logger . debug ( "Received null bridge while updating Temperature Sensor channels!" ) ; } }
public void test() { if ( serviceName . equals ( "jndi/serviceA" ) ) { LOGGER . info ( "Looking up service A and creating new service for A" ) ; return new ServiceImpl ( "jndi/serviceA" ) ; } else-if ( serviceName . equals ( "jndi/serviceB" ) ) { LOGGER . info ( "Looking up service B and creating new service for B" ) ; return new ServiceImpl ( "jndi/serviceB" ) ; } else { return null ; } }
public void test() { if ( serviceName . equals ( "jndi/serviceA" ) ) { LOGGER . info ( "Looking up service A and creating new service for A" ) ; return new ServiceImpl ( "jndi/serviceA" ) ; } else-if ( serviceName . equals ( "jndi/serviceB" ) ) { LOGGER . info ( "Looking up service B and creating new service for B" ) ; return new ServiceImpl ( "jndi/serviceB" ) ; } else { return null ; } }
@ SuppressWarnings ( "deprecation" ) @ Test public void testAddNewLocationDefinition ( ) { String yaml = Joiner . on ( "\n" ) . join ( ImmutableList . of ( "brooklyn.catalog:" , "  symbolicName: " + locationName , "  version: " + locationVersion , "" , "brooklyn.locations:" , "- type: " + "aws-ec2:us-east-1" , "  brooklyn.config:" , "    identity: bob" , "    credential: CR3dential" ) ) ; ClientResponse response = client ( ) . resource ( "/v1/catalog" ) . post ( ClientResponse . class , yaml ) ; assertEquals ( response . getStatus ( ) , Response . Status . CREATED . getStatusCode ( ) ) ; URI addedCatalogItemUri = response . getLocation ( ) ; log . info ( "added, at: " + addedCatalogItemUri ) ; CatalogLocationSummary locationItem = client ( ) . resource ( "/v1/catalog/locations/" + locationName + "/" + locationVersion ) . get ( CatalogLocationSummary . class ) ; log . info ( " item: " + locationItem ) ; LocationSummary locationSummary = client ( ) . resource ( URI . create ( "/v1/locations/" + locationName + "/" ) ) . get ( LocationSummary . class ) ; log . info ( " summary: " + locationSummary ) ; Assert . assertEquals ( locationSummary . getSpec ( ) , "brooklyn.catalog:" + locationName + ":" + locationVersion ) ; JcloudsLocation l = ( JcloudsLocation ) getManagementContext ( ) . getLocationRegistry ( ) . resolve ( locationName ) ; Assert . assertEquals ( l . getProvider ( ) , "aws-ec2" ) ; Assert . assertEquals ( l . getRegion ( ) , "us-east-1" ) ; Assert . assertEquals ( l . getIdentity ( ) , "bob" ) ; Assert . assertEquals ( l . getCredential ( ) , "CR3dential" ) ; }
@ SuppressWarnings ( "deprecation" ) @ Test public void testAddNewLocationDefinition ( ) { String yaml = Joiner . on ( "\n" ) . join ( ImmutableList . of ( "brooklyn.catalog:" , "  symbolicName: " + locationName , "  version: " + locationVersion , "" , "brooklyn.locations:" , "- type: " + "aws-ec2:us-east-1" , "  brooklyn.config:" , "    identity: bob" , "    credential: CR3dential" ) ) ; ClientResponse response = client ( ) . resource ( "/v1/catalog" ) . post ( ClientResponse . class , yaml ) ; assertEquals ( response . getStatus ( ) , Response . Status . CREATED . getStatusCode ( ) ) ; URI addedCatalogItemUri = response . getLocation ( ) ; log . info ( "added, at: " + addedCatalogItemUri ) ; CatalogLocationSummary locationItem = client ( ) . resource ( "/v1/catalog/locations/" + locationName + "/" + locationVersion ) . get ( CatalogLocationSummary . class ) ; log . info ( " item: " + locationItem ) ; LocationSummary locationSummary = client ( ) . resource ( URI . create ( "/v1/locations/" + locationName + "/" ) ) . get ( LocationSummary . class ) ; log . info ( " summary: " + locationSummary ) ; Assert . assertEquals ( locationSummary . getSpec ( ) , "brooklyn.catalog:" + locationName + ":" + locationVersion ) ; JcloudsLocation l = ( JcloudsLocation ) getManagementContext ( ) . getLocationRegistry ( ) . resolve ( locationName ) ; Assert . assertEquals ( l . getProvider ( ) , "aws-ec2" ) ; Assert . assertEquals ( l . getRegion ( ) , "us-east-1" ) ; Assert . assertEquals ( l . getIdentity ( ) , "bob" ) ; Assert . assertEquals ( l . getCredential ( ) , "CR3dential" ) ; }
@ SuppressWarnings ( "deprecation" ) @ Test public void testAddNewLocationDefinition ( ) { String yaml = Joiner . on ( "\n" ) . join ( ImmutableList . of ( "brooklyn.catalog:" , "  symbolicName: " + locationName , "  version: " + locationVersion , "" , "brooklyn.locations:" , "- type: " + "aws-ec2:us-east-1" , "  brooklyn.config:" , "    identity: bob" , "    credential: CR3dential" ) ) ; ClientResponse response = client ( ) . resource ( "/v1/catalog" ) . post ( ClientResponse . class , yaml ) ; assertEquals ( response . getStatus ( ) , Response . Status . CREATED . getStatusCode ( ) ) ; URI addedCatalogItemUri = response . getLocation ( ) ; log . info ( "added, at: " + addedCatalogItemUri ) ; CatalogLocationSummary locationItem = client ( ) . resource ( "/v1/catalog/locations/" + locationName + "/" + locationVersion ) . get ( CatalogLocationSummary . class ) ; log . info ( " item: " + locationItem ) ; LocationSummary locationSummary = client ( ) . resource ( URI . create ( "/v1/locations/" + locationName + "/" ) ) . get ( LocationSummary . class ) ; log . info ( " summary: " + locationSummary ) ; Assert . assertEquals ( locationSummary . getSpec ( ) , "brooklyn.catalog:" + locationName + ":" + locationVersion ) ; JcloudsLocation l = ( JcloudsLocation ) getManagementContext ( ) . getLocationRegistry ( ) . resolve ( locationName ) ; Assert . assertEquals ( l . getProvider ( ) , "aws-ec2" ) ; Assert . assertEquals ( l . getRegion ( ) , "us-east-1" ) ; Assert . assertEquals ( l . getIdentity ( ) , "bob" ) ; Assert . assertEquals ( l . getCredential ( ) , "CR3dential" ) ; }
@ Override public void initialize ( ) { LOGGER . debug ( "Creating a new JAXB context for the following class: " + entityClass . getSimpleName ( ) ) ; code_block = TryStatement ;  LOGGER . debug ( "Start the initialize phase for the type " + entityClass . getSimpleName ( ) ) ; final Collection < T > loadedFiles = loadFiles ( ) ; code_block = ForStatement ; postInitiate ( ) ; }
public void test() { try { this . jaxbContext = JAXBContext . newInstance ( entityClass ) ; } catch ( JAXBException e ) { LOGGER . error ( "Unable to create a new JAXB instance" , e ) ; throw new IllegalStateException ( "Unable to create a new JAXB instance" , e ) ; } }
public void test() { try { final RevisionInfo revisionInfo = policyToDelete . getRevision ( ) ; final Long version = revisionInfo == null ? 0 : revisionInfo . getVersion ( ) ; client . target ( createURL ( "policies/" + policyToDelete . getIdentifier ( ) ) ) . queryParam ( "version" , version . longValue ( ) ) . request ( ) . header ( "Authorization" , "Bearer " + adminAuthToken ) . delete ( ) ; } catch ( Exception e ) { LOGGER . error ( "Error cleaning up policies after test due to: " + e . getMessage ( ) , e ) ; } }
public void test() { if ( ! failedReporter . getFailedTests ( ) . isEmpty ( ) ) { log . error ( "Failures: " + failedReporter . getFailedTests ( ) ) ; System . exit ( 1 ) ; } }
public void test() { if ( context . getElement ( ) . isPresent ( ) ) { AnnotatedElement e = context . getElement ( ) . get ( ) ; Description description = Description . createSuiteDescription ( "LDAP" , e . getAnnotations ( ) ) ; log . trace ( "Creating directory service" ) ; directoryService = DSAnnotationProcessor . getDirectoryService ( description ) ; DSAnnotationProcessor . applyLdifs ( description , directoryService ) ; log . trace ( "Creating ldap server" ) ; ldapServer = ServerAnnotationProcessor . createLdapServer ( description , directoryService ) ; } }
public void test() { if ( context . getElement ( ) . isPresent ( ) ) { AnnotatedElement e = context . getElement ( ) . get ( ) ; Description description = Description . createSuiteDescription ( "LDAP" , e . getAnnotations ( ) ) ; log . trace ( "Creating directory service" ) ; directoryService = DSAnnotationProcessor . getDirectoryService ( description ) ; DSAnnotationProcessor . applyLdifs ( description , directoryService ) ; log . trace ( "Creating ldap server" ) ; ldapServer = ServerAnnotationProcessor . createLdapServer ( description , directoryService ) ; } }
private void forwardRpcRequestToDeviceActor ( ToDeviceRpcRequest request , Consumer < FromDeviceRpcResponse > responseConsumer ) { log . trace ( "[{}][{}] Processing local rpc call to device actor [{}]" , request . getTenantId ( ) , request . getId ( ) , request . getDeviceId ( ) ) ; UUID requestId = request . getId ( ) ; toDeviceRpcRequests . put ( requestId , responseConsumer ) ; sendRpcRequestToDevice ( request ) ; scheduleTimeout ( request , requestId ) ; }
private void encryptVPNPassword ( Connection conn ) { s_logger . debug ( "Encrypting vpn_users password" ) ; List < PreparedStatement > pstmt2Close = new ArrayList < PreparedStatement > ( ) ; PreparedStatement pstmt = null ; ResultSet rs = null ; code_block = TryStatement ;  s_logger . debug ( "Done encrypting vpn_users password" ) ; }
public void attachDirty ( FilterCol instance ) { log . debug ( "attaching dirty FilterCol instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { LOGGER . info ( "Received message" ) ; final ObjectMessage objectMessage = ( ObjectMessage ) message ; final String messageType = objectMessage . getJMSType ( ) ; final RequestMessage requestMessage = ( RequestMessage ) objectMessage . getObject ( ) ; this . messageProcessor . processMessage ( requestMessage , messageType ) ; } catch ( final JMSException e ) { LOGGER . error ( "Exception: {}, StackTrace: {}" , e . getMessage ( ) , e . getStackTrace ( ) , e ) ; } catch ( final UnknownMessageTypeException e ) { LOGGER . error ( "UnknownMessageTypeException" , e ) ; } }
public void test() { try { LOGGER . info ( "Received message" ) ; final ObjectMessage objectMessage = ( ObjectMessage ) message ; final String messageType = objectMessage . getJMSType ( ) ; final RequestMessage requestMessage = ( RequestMessage ) objectMessage . getObject ( ) ; this . messageProcessor . processMessage ( requestMessage , messageType ) ; } catch ( final JMSException e ) { LOGGER . error ( "Exception: {}, StackTrace: {}" , e . getMessage ( ) , e . getStackTrace ( ) , e ) ; } catch ( final UnknownMessageTypeException e ) { LOGGER . error ( "UnknownMessageTypeException" , e ) ; } }
public void test() { try { LOGGER . info ( "Received message" ) ; final ObjectMessage objectMessage = ( ObjectMessage ) message ; final String messageType = objectMessage . getJMSType ( ) ; final RequestMessage requestMessage = ( RequestMessage ) objectMessage . getObject ( ) ; this . messageProcessor . processMessage ( requestMessage , messageType ) ; } catch ( final JMSException e ) { LOGGER . error ( "Exception: {}, StackTrace: {}" , e . getMessage ( ) , e . getStackTrace ( ) , e ) ; } catch ( final UnknownMessageTypeException e ) { LOGGER . error ( "UnknownMessageTypeException" , e ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Loading database script from :" + scriptName ) ; } }
public void test() { if ( ! CollectionUtils . isEmpty ( listServerCertificatesMetadata ) ) { code_block = ForStatement ; iamCertificateVH . put ( account + delimiter + accountName , iamCerttList ) ; } else { log . info ( "List is empty" ) ; } }
public void test() { try { amazonIdentityManagement = AmazonIdentityManagementClientBuilder . standard ( ) . withCredentials ( new AWSStaticCredentialsProvider ( temporaryCredentials ) ) . withRegion ( InventoryConstants . REGION_US_WEST_2 ) . build ( ) ; listServerCertificatesMetadata = amazonIdentityManagement . listServerCertificates ( new ListServerCertificatesRequest ( ) ) . getServerCertificateMetadataList ( ) ; List < IAMCertificateVH > iamCerttList = new ArrayList < > ( ) ; code_block = IfStatement ; } catch ( Exception e ) { log . error ( expPrefix + InventoryConstants . ERROR_CAUSE + e . getMessage ( ) + "\"}" ) ; ErrorManageUtil . uploadError ( account , "" , "IAMCertificate" , e . getMessage ( ) ) ; } }
public org . talend . mdm . webservice . WSViewPK putView ( org . talend . mdm . webservice . WSPutView arg0 ) { LOG . info ( "Executing operation putView" ) ; System . out . println ( arg0 ) ; code_block = TryStatement ;  }
public void test() { try { return repositoriesContainer . getRepositoryNames ( Collections . unmodifiableMap ( factoryParams ) ) ; } catch ( RepositoryException e ) { LOGGER . error ( e , WebJcrI18n . cannotLoadRepositoryNames . text ( ) ) ; return Collections . emptySet ( ) ; } }
public void test() { if ( ! ObjectUtils . isEmpty ( errors ) ) { return handleRequestValidationErrors ( messageContext , errors ) ; } else-if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Request message validated" ) ; } }
public void test() { try { currentTransaction . success ( ) ; currentTransaction . close ( ) ; } catch ( Throwable t ) { logger . warn ( "" , t ) ; } finally { currentTransaction = null ; } }
public void test() { try { tryDestroy ( ) ; } catch ( Exception e ) { logger . error ( e ) ; } }
public void test() { if ( statusCode != HttpURLConnection . HTTP_OK ) { logger . debug ( "An unexpected status code was returned: '{}'" , statusCode ) ; } }
public void test() { if ( null == startKey && null == endKey ) { Preconditions . checkState ( 1 == locations . size ( ) ) ; logger . debug ( "HBase table {} has a single region {}" , tableName , regionInfo ) ; return b . put ( new KeyRange ( FOUR_ZERO_BYTES , FOUR_ZERO_BYTES ) , serverName ) . build ( ) ; } else-if ( null == startKey ) { logger . debug ( "Found HRegionInfo with null startKey on server {}: {}" , serverName , regionInfo ) ; Preconditions . checkState ( null == nullStart ) ; nullStart = location ; StaticBuffer endBuf = StaticArrayBuffer . of ( zeroExtend ( endKey ) ) ; b . put ( new KeyRange ( FOUR_ZERO_BYTES , endBuf ) , serverName ) ; } else-if ( null == endKey ) { logger . debug ( "Found HRegionInfo with null endKey on server {}: {}" , serverName , regionInfo ) ; Preconditions . checkState ( null == nullEnd ) ; nullEnd = location ; b . put ( new KeyRange ( StaticArrayBuffer . of ( zeroExtend ( startKey ) ) , FOUR_ZERO_BYTES ) , serverName ) ; } else { Preconditions . checkState ( null != startKey ) ; Preconditions . checkState ( null != endKey ) ; StaticBuffer startBuf = StaticArrayBuffer . of ( zeroExtend ( startKey ) ) ; StaticBuffer endBuf = StaticArrayBuffer . of ( zeroExtend ( endKey ) ) ; KeyRange kr = new KeyRange ( startBuf , endBuf ) ; b . put ( kr , serverName ) ; logger . debug ( "Found HRegionInfo with non-null end and start keys on server {}: {}" , serverName , regionInfo ) ; } }
public void test() { if ( null == startKey && null == endKey ) { Preconditions . checkState ( 1 == locations . size ( ) ) ; logger . debug ( "HBase table {} has a single region {}" , tableName , regionInfo ) ; return b . put ( new KeyRange ( FOUR_ZERO_BYTES , FOUR_ZERO_BYTES ) , serverName ) . build ( ) ; } else-if ( null == startKey ) { logger . debug ( "Found HRegionInfo with null startKey on server {}: {}" , serverName , regionInfo ) ; Preconditions . checkState ( null == nullStart ) ; nullStart = location ; StaticBuffer endBuf = StaticArrayBuffer . of ( zeroExtend ( endKey ) ) ; b . put ( new KeyRange ( FOUR_ZERO_BYTES , endBuf ) , serverName ) ; } else-if ( null == endKey ) { logger . debug ( "Found HRegionInfo with null endKey on server {}: {}" , serverName , regionInfo ) ; Preconditions . checkState ( null == nullEnd ) ; nullEnd = location ; b . put ( new KeyRange ( StaticArrayBuffer . of ( zeroExtend ( startKey ) ) , FOUR_ZERO_BYTES ) , serverName ) ; } else { Preconditions . checkState ( null != startKey ) ; Preconditions . checkState ( null != endKey ) ; StaticBuffer startBuf = StaticArrayBuffer . of ( zeroExtend ( startKey ) ) ; StaticBuffer endBuf = StaticArrayBuffer . of ( zeroExtend ( endKey ) ) ; KeyRange kr = new KeyRange ( startBuf , endBuf ) ; b . put ( kr , serverName ) ; logger . debug ( "Found HRegionInfo with non-null end and start keys on server {}: {}" , serverName , regionInfo ) ; } }
public void test() { if ( null == startKey && null == endKey ) { Preconditions . checkState ( 1 == locations . size ( ) ) ; logger . debug ( "HBase table {} has a single region {}" , tableName , regionInfo ) ; return b . put ( new KeyRange ( FOUR_ZERO_BYTES , FOUR_ZERO_BYTES ) , serverName ) . build ( ) ; } else-if ( null == startKey ) { logger . debug ( "Found HRegionInfo with null startKey on server {}: {}" , serverName , regionInfo ) ; Preconditions . checkState ( null == nullStart ) ; nullStart = location ; StaticBuffer endBuf = StaticArrayBuffer . of ( zeroExtend ( endKey ) ) ; b . put ( new KeyRange ( FOUR_ZERO_BYTES , endBuf ) , serverName ) ; } else-if ( null == endKey ) { logger . debug ( "Found HRegionInfo with null endKey on server {}: {}" , serverName , regionInfo ) ; Preconditions . checkState ( null == nullEnd ) ; nullEnd = location ; b . put ( new KeyRange ( StaticArrayBuffer . of ( zeroExtend ( startKey ) ) , FOUR_ZERO_BYTES ) , serverName ) ; } else { Preconditions . checkState ( null != startKey ) ; Preconditions . checkState ( null != endKey ) ; StaticBuffer startBuf = StaticArrayBuffer . of ( zeroExtend ( startKey ) ) ; StaticBuffer endBuf = StaticArrayBuffer . of ( zeroExtend ( endKey ) ) ; KeyRange kr = new KeyRange ( startBuf , endBuf ) ; b . put ( kr , serverName ) ; logger . debug ( "Found HRegionInfo with non-null end and start keys on server {}: {}" , serverName , regionInfo ) ; } }
public void test() { if ( null == startKey && null == endKey ) { Preconditions . checkState ( 1 == locations . size ( ) ) ; logger . debug ( "HBase table {} has a single region {}" , tableName , regionInfo ) ; return b . put ( new KeyRange ( FOUR_ZERO_BYTES , FOUR_ZERO_BYTES ) , serverName ) . build ( ) ; } else-if ( null == startKey ) { logger . debug ( "Found HRegionInfo with null startKey on server {}: {}" , serverName , regionInfo ) ; Preconditions . checkState ( null == nullStart ) ; nullStart = location ; StaticBuffer endBuf = StaticArrayBuffer . of ( zeroExtend ( endKey ) ) ; b . put ( new KeyRange ( FOUR_ZERO_BYTES , endBuf ) , serverName ) ; } else-if ( null == endKey ) { logger . debug ( "Found HRegionInfo with null endKey on server {}: {}" , serverName , regionInfo ) ; Preconditions . checkState ( null == nullEnd ) ; nullEnd = location ; b . put ( new KeyRange ( StaticArrayBuffer . of ( zeroExtend ( startKey ) ) , FOUR_ZERO_BYTES ) , serverName ) ; } else { Preconditions . checkState ( null != startKey ) ; Preconditions . checkState ( null != endKey ) ; StaticBuffer startBuf = StaticArrayBuffer . of ( zeroExtend ( startKey ) ) ; StaticBuffer endBuf = StaticArrayBuffer . of ( zeroExtend ( endKey ) ) ; KeyRange kr = new KeyRange ( startBuf , endBuf ) ; b . put ( kr , serverName ) ; logger . debug ( "Found HRegionInfo with non-null end and start keys on server {}: {}" , serverName , regionInfo ) ; } }
@ Override public IApsEntity extractEntityType ( String typeCode , Class entityClass , String configItemName , IEntityTypeDOM entityTypeDom , String entityManagerName , IApsEntityDOM entityDom ) throws ApsSystemException { String xml = this . getConfigManager ( ) . getConfigItem ( configItemName ) ; logger . debug ( "{} : {}" , configItemName , xml ) ; return entityTypeDom . extractEntityType ( typeCode , xml , entityClass , entityDom , entityManagerName ) ; }
public void test() { try { xceiverClientManager . close ( ) ; } catch ( Exception ex ) { LOG . error ( "Can't close " + this . getClass ( ) . getSimpleName ( ) , ex ) ; } }
public void attachClean ( MbPrioritaet instance ) { log . debug ( "attaching clean MbPrioritaet instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . lock ( instance , LockMode . NONE ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( _NO_SUCH_ENTITY_WITH_PRIMARY_KEY + primaryKey ) ; } }
public void test() { try { pem = SshKeyGen . writePublicKey ( entry . getValue ( ) ) ; break ; } catch ( Exception e ) { log . error ( "Failed to write PEM" , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Successfully published message for token:" + token ) ; } }
public void test() { try ( BufferedWriter writer = new BufferedWriter ( new FileWriter ( idFile ) ) ) { writer . write ( String . valueOf ( identifier ) ) ; } catch ( IOException e ) { logger . error ( "Cannot save the node identifier" , e ) ; } }
public void test() { try { return resp . getBody ( ComponentTypesHash . class ) ; } catch ( Exception e ) { log . error ( "Recieved Message Exception." , e ) ; return null ; } }
protected void doDependency ( Index index , final EntryStatistics es [ ] , final IterablePosting ips [ ] , ResultSet rs , final double [ ] phraseTermWeights , boolean SD ) throws IOException { final int numPhraseTerms = phraseTerms . length ; final boolean [ ] postingListFinished = new boolean [ numPhraseTerms ] ; code_block = ForStatement ; this . setCollectionStatistics ( index . getCollectionStatistics ( ) , index ) ; determineGlobalStatistics ( phraseTerms , es , SD ) ; final int [ ] docids = rs . getDocids ( ) ; final double [ ] scores = rs . getScores ( ) ; final short [ ] occurrences = rs . getOccurrences ( ) ; int altered = 0 ; MultiSort . ascendingHeapSort ( docids , scores , occurrences , docids . length ) ; final int docidsLength = docids . length ; boolean allZero = true ; code_block = ForStatement ; DOC : code_block = ForStatement ; code_block = ForStatement ; logger . info ( this . getClass ( ) . getSimpleName ( ) + " altered scores to " + altered + " documents" ) ; }
private void parseSaltLength ( PasswordSaltExtensionMessage msg ) { msg . setSaltLength ( parseIntField ( ExtensionByteLength . PASSWORD_SALT ) ) ; LOGGER . debug ( "SaltLength: " + msg . getSaltLength ( ) . getValue ( ) ) ; }
public void test() { try { code_block = ForStatement ; input . put ( InputConverterUnitTest . END_ROW ) ; } catch ( InterruptedException e ) { logger . warn ( "Fail to produce records into BlockingQueue due to: " + e ) ; } }
public void test() { try { consumer . join ( ) ; } catch ( InterruptedException e ) { logger . warn ( "Fail to join consumer thread: " + e ) ; } }
public void test() { try { closeWriter ( ) ; commitTransaction ( ) ; } catch ( EventDeliveryException ex ) { rollbackTransaction ( ) ; LOG . warn ( "Closing the writer failed: " + ex . getLocalizedMessage ( ) ) ; LOG . debug ( "Exception follows." , ex ) ; } }
@ Test public void testInvalidConfigStatus ( ) { ThingMock t = new ThingMock ( ) ; HashMap < String , Object > properties = new HashMap < String , Object > ( ) ; properties . put ( "sensorid" , - 1 ) ; t . setConfiguration ( properties ) ; PMHandlerExtension pmHandler = new PMHandlerExtension ( t ) ; pmHandler . initialize ( ) ; logger . info ( "LC status: {}" , pmHandler . getLifecycleStatus ( ) ) ; int retryCount = 0 ; code_block = WhileStatement ; assertEquals ( ConfigStatus . SENSOR_ID_NEGATIVE , pmHandler . getConfigStatus ( ) , "Handler Configuration status" ) ; }
public void test() { try { logger . info ( "LC running not reached - wait" ) ; Thread . sleep ( 500 ) ; retryCount ++ ; } catch ( InterruptedException e ) { } }
public void test() { if ( _Employee . LOG . isDebugEnabled ( ) ) { _Employee . LOG . debug ( "updating hireDate from " + hireDate ( ) + " to " + value ) ; } }
public void test() { if ( content . length > smallCellMetadataWarningThreshold ) { logger . warn ( "A JSON metadata entry's size is not supposed to exceed kylin.metadata.jdbc.small-cell-meta-size-warning-threshold({}), resPath: {}, actual size: {}" , smallCellMetadataWarningThreshold , resPath , content . length ) ; } }
public void test() { try { int hueId = Integer . parseInt ( metadata . getValue ( ) ) ; code_block = IfStatement ; } catch ( NumberFormatException e ) { logger . warn ( "A non numeric hue ID '{}' was assigned. Ignoring!" , metadata . getValue ( ) ) ; } }
@ Test public void testMarker ( ) { Logger logger = LoggerFactory . getLogger ( "testMarker" ) ; Marker blue = MarkerFactory . getMarker ( "BLUE" ) ; logger . debug ( blue , "hello" ) ; logger . info ( blue , "hello" ) ; logger . warn ( blue , "hello" ) ; logger . error ( blue , "hello" ) ; logger . debug ( blue , "hello {}" , "world" ) ; logger . info ( blue , "hello {}" , "world" ) ; logger . warn ( blue , "hello {}" , "world" ) ; logger . error ( blue , "hello {}" , "world" ) ; logger . debug ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . info ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . warn ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . error ( blue , "hello {} and {} " , "world" , "universe" ) ; }
@ Test public void testMarker ( ) { Logger logger = LoggerFactory . getLogger ( "testMarker" ) ; Marker blue = MarkerFactory . getMarker ( "BLUE" ) ; logger . debug ( blue , "hello" ) ; logger . info ( blue , "hello" ) ; logger . warn ( blue , "hello" ) ; logger . error ( blue , "hello" ) ; logger . debug ( blue , "hello {}" , "world" ) ; logger . info ( blue , "hello {}" , "world" ) ; logger . warn ( blue , "hello {}" , "world" ) ; logger . error ( blue , "hello {}" , "world" ) ; logger . debug ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . info ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . warn ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . error ( blue , "hello {} and {} " , "world" , "universe" ) ; }
@ Test public void testMarker ( ) { Logger logger = LoggerFactory . getLogger ( "testMarker" ) ; Marker blue = MarkerFactory . getMarker ( "BLUE" ) ; logger . debug ( blue , "hello" ) ; logger . info ( blue , "hello" ) ; logger . warn ( blue , "hello" ) ; logger . error ( blue , "hello" ) ; logger . debug ( blue , "hello {}" , "world" ) ; logger . info ( blue , "hello {}" , "world" ) ; logger . warn ( blue , "hello {}" , "world" ) ; logger . error ( blue , "hello {}" , "world" ) ; logger . debug ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . info ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . warn ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . error ( blue , "hello {} and {} " , "world" , "universe" ) ; }
@ Test public void testMarker ( ) { Logger logger = LoggerFactory . getLogger ( "testMarker" ) ; Marker blue = MarkerFactory . getMarker ( "BLUE" ) ; logger . debug ( blue , "hello" ) ; logger . info ( blue , "hello" ) ; logger . warn ( blue , "hello" ) ; logger . error ( blue , "hello" ) ; logger . debug ( blue , "hello {}" , "world" ) ; logger . info ( blue , "hello {}" , "world" ) ; logger . warn ( blue , "hello {}" , "world" ) ; logger . error ( blue , "hello {}" , "world" ) ; logger . debug ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . info ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . warn ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . error ( blue , "hello {} and {} " , "world" , "universe" ) ; }
@ Test public void testMarker ( ) { Logger logger = LoggerFactory . getLogger ( "testMarker" ) ; Marker blue = MarkerFactory . getMarker ( "BLUE" ) ; logger . debug ( blue , "hello" ) ; logger . info ( blue , "hello" ) ; logger . warn ( blue , "hello" ) ; logger . error ( blue , "hello" ) ; logger . debug ( blue , "hello {}" , "world" ) ; logger . info ( blue , "hello {}" , "world" ) ; logger . warn ( blue , "hello {}" , "world" ) ; logger . error ( blue , "hello {}" , "world" ) ; logger . debug ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . info ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . warn ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . error ( blue , "hello {} and {} " , "world" , "universe" ) ; }
@ Test public void testMarker ( ) { Logger logger = LoggerFactory . getLogger ( "testMarker" ) ; Marker blue = MarkerFactory . getMarker ( "BLUE" ) ; logger . debug ( blue , "hello" ) ; logger . info ( blue , "hello" ) ; logger . warn ( blue , "hello" ) ; logger . error ( blue , "hello" ) ; logger . debug ( blue , "hello {}" , "world" ) ; logger . info ( blue , "hello {}" , "world" ) ; logger . warn ( blue , "hello {}" , "world" ) ; logger . error ( blue , "hello {}" , "world" ) ; logger . debug ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . info ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . warn ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . error ( blue , "hello {} and {} " , "world" , "universe" ) ; }
@ Test public void testMarker ( ) { Logger logger = LoggerFactory . getLogger ( "testMarker" ) ; Marker blue = MarkerFactory . getMarker ( "BLUE" ) ; logger . debug ( blue , "hello" ) ; logger . info ( blue , "hello" ) ; logger . warn ( blue , "hello" ) ; logger . error ( blue , "hello" ) ; logger . debug ( blue , "hello {}" , "world" ) ; logger . info ( blue , "hello {}" , "world" ) ; logger . warn ( blue , "hello {}" , "world" ) ; logger . error ( blue , "hello {}" , "world" ) ; logger . debug ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . info ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . warn ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . error ( blue , "hello {} and {} " , "world" , "universe" ) ; }
@ Test public void testMarker ( ) { Logger logger = LoggerFactory . getLogger ( "testMarker" ) ; Marker blue = MarkerFactory . getMarker ( "BLUE" ) ; logger . debug ( blue , "hello" ) ; logger . info ( blue , "hello" ) ; logger . warn ( blue , "hello" ) ; logger . error ( blue , "hello" ) ; logger . debug ( blue , "hello {}" , "world" ) ; logger . info ( blue , "hello {}" , "world" ) ; logger . warn ( blue , "hello {}" , "world" ) ; logger . error ( blue , "hello {}" , "world" ) ; logger . debug ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . info ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . warn ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . error ( blue , "hello {} and {} " , "world" , "universe" ) ; }
@ Test public void testMarker ( ) { Logger logger = LoggerFactory . getLogger ( "testMarker" ) ; Marker blue = MarkerFactory . getMarker ( "BLUE" ) ; logger . debug ( blue , "hello" ) ; logger . info ( blue , "hello" ) ; logger . warn ( blue , "hello" ) ; logger . error ( blue , "hello" ) ; logger . debug ( blue , "hello {}" , "world" ) ; logger . info ( blue , "hello {}" , "world" ) ; logger . warn ( blue , "hello {}" , "world" ) ; logger . error ( blue , "hello {}" , "world" ) ; logger . debug ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . info ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . warn ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . error ( blue , "hello {} and {} " , "world" , "universe" ) ; }
@ Test public void testMarker ( ) { Logger logger = LoggerFactory . getLogger ( "testMarker" ) ; Marker blue = MarkerFactory . getMarker ( "BLUE" ) ; logger . debug ( blue , "hello" ) ; logger . info ( blue , "hello" ) ; logger . warn ( blue , "hello" ) ; logger . error ( blue , "hello" ) ; logger . debug ( blue , "hello {}" , "world" ) ; logger . info ( blue , "hello {}" , "world" ) ; logger . warn ( blue , "hello {}" , "world" ) ; logger . error ( blue , "hello {}" , "world" ) ; logger . debug ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . info ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . warn ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . error ( blue , "hello {} and {} " , "world" , "universe" ) ; }
@ Test public void testMarker ( ) { Logger logger = LoggerFactory . getLogger ( "testMarker" ) ; Marker blue = MarkerFactory . getMarker ( "BLUE" ) ; logger . debug ( blue , "hello" ) ; logger . info ( blue , "hello" ) ; logger . warn ( blue , "hello" ) ; logger . error ( blue , "hello" ) ; logger . debug ( blue , "hello {}" , "world" ) ; logger . info ( blue , "hello {}" , "world" ) ; logger . warn ( blue , "hello {}" , "world" ) ; logger . error ( blue , "hello {}" , "world" ) ; logger . debug ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . info ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . warn ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . error ( blue , "hello {} and {} " , "world" , "universe" ) ; }
@ Test public void testMarker ( ) { Logger logger = LoggerFactory . getLogger ( "testMarker" ) ; Marker blue = MarkerFactory . getMarker ( "BLUE" ) ; logger . debug ( blue , "hello" ) ; logger . info ( blue , "hello" ) ; logger . warn ( blue , "hello" ) ; logger . error ( blue , "hello" ) ; logger . debug ( blue , "hello {}" , "world" ) ; logger . info ( blue , "hello {}" , "world" ) ; logger . warn ( blue , "hello {}" , "world" ) ; logger . error ( blue , "hello {}" , "world" ) ; logger . debug ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . info ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . warn ( blue , "hello {} and {} " , "world" , "universe" ) ; logger . error ( blue , "hello {} and {} " , "world" , "universe" ) ; }
public void test() { if ( document == null ) { logger . debug ( "Received Provenance Event {} to index but it contained no information that should be indexed, so skipping it" , event . getEventId ( ) ) ; } else { final File indexDir ; code_block = IfStatement ; final IndexableDocument doc = new IndexableDocument ( document , summary , indexDir ) ; indexableDocs . add ( doc ) ; } }
public void test() { try { indexTask . reIndex ( indexableDocs , CommitPreference . PREVENT_COMMIT ) ; } catch ( final IOException ioe ) { logger . error ( "Failed to reindex some Provenance Events" , ioe ) ; eventReporter . reportEvent ( Severity . ERROR , EVENT_CATEGORY , "Failed to re-index some Provenance Events. " + "Some Provenance Events may not be available for querying. See logs for more information." ) ; } }
@ Override @ SuppressWarnings ( "deprecation" ) public RequestCtx handleRequest ( SOAPMessageContext context ) throws OperationHandlerException { logger . debug ( "GetDisseminationHandler/handleRequest!" ) ; RequestCtx req = null ; Object oMap = null ; String pid = null ; String sDefPid = null ; String methodName = null ; String asOfDateTime = null ; code_block = TryStatement ;  code_block = TryStatement ;  logger . debug ( "Extracted SOAP Request Objects" ) ; Map < URI , AttributeValue > actions = new HashMap < URI , AttributeValue > ( ) ; Map < URI , AttributeValue > resAttr ; code_block = TryStatement ;  return req ; }
public void test() { try { oMap = getSOAPRequestObjects ( context ) ; logger . debug ( "Retrieved SOAP Request Objects" ) ; } catch ( SoapFault af ) { logger . error ( "Error obtaining SOAP Request Objects" , af ) ; throw new OperationHandlerException ( "Error obtaining SOAP Request Objects" , af ) ; } }
public void test() { try { pid = ( String ) callGetter ( "getPid" , oMap ) ; sDefPid = ( String ) callGetter ( "getServiceDefinitionPid" , oMap ) ; methodName = ( String ) callGetter ( "getMethodName" , oMap ) ; asOfDateTime = ( String ) callGetter ( "getAsOfDateTime" , oMap ) ; } catch ( Exception e ) { logger . error ( "Error obtaining parameters" , e ) ; throw new OperationHandlerException ( "Error obtaining parameters." , e ) ; } }
public void test() { try { resAttr = ResourceAttributes . getResources ( pid ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; actions . put ( Constants . ACTION . ID . getURI ( ) , Constants . ACTION . GET_DISSEMINATION . getStringAttribute ( ) ) ; actions . put ( Constants . ACTION . API . getURI ( ) , Constants . ACTION . APIA . getStringAttribute ( ) ) ; req = getContextHandler ( ) . buildRequest ( getSubjects ( context ) , actions , resAttr , getEnvironment ( context ) ) ; LogUtil . statLog ( getUser ( context ) , Constants . ACTION . GET_DISSEMINATION . uri , pid , null ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; throw new OperationHandlerException ( e . getMessage ( ) , e ) ; } }
@ Override public void unRegisterService ( String serviceType , String baseUrl ) throws ServiceRegistryException { logger . info ( "Unregistering Service {}@{} and cleaning its running jobs" , serviceType , baseUrl ) ; setOnlineStatus ( serviceType , baseUrl , null , false , null ) ; cleanRunningJobs ( serviceType , baseUrl ) ; }
public synchronized void producersConnected ( final Wire [ ] wires ) { logger . info ( "Producers connected - {}" , ( Object ) wires ) ; this . wireSupport . producersConnected ( wires ) ; }
private void logCallInfo ( URI _uri , Map < String , String > _params ) { final StringBuilder b = new StringBuilder ( ) ; b . append ( "Calling [" ) . append ( _uri . toString ( ) ) . append ( "]" ) ; code_block = IfStatement ; log . info ( b . toString ( ) + "]" ) ; }
@ Override public Reader loadResource ( String relativePath ) { LOG . info ( "Loading resource. RelativePath = {}." , relativePath ) ; InputStream is = this . classLoader . getResourceAsStream ( relativePath ) ; return is != null ? new InputStreamReader ( is ) : null ; }
public List < String > lookupAssigningAuthorities ( String homeCommunityId ) { LOG . trace ( "converting homeCommunityId [" + homeCommunityId + "] to assigning authority" ) ; return mappingDao . getAssigningAuthoritiesByHomeCommunity ( homeCommunityId ) ; }
public void test() { try { m3uaManagementEventListener . onAsCreated ( as ) ; } catch ( Throwable ee ) { logger . error ( "Exception while invoking onAsCreated" , ee ) ; } }
@ Test public void deleteNotExistingUserExpectError ( ) throws Exception { LOG . info ( "delete specific not existing user" ) ; assertEquals ( deleteUser ( adminConnectionCorrect , NEW_USER_SDN ) . getStatus ( ) , HttpStatus . NOT_FOUND_404 ) ; }
public void test() { try { code_block = IfStatement ; MessageResourcesFactory factory = ( MessageResourcesFactory ) clazz . newInstance ( ) ; return ( factory ) ; } catch ( Throwable t ) { LOG . error ( "MessageResourcesFactory.createFactory" , t ) ; return ( null ) ; } }
public void test() { try { fileChannel . close ( ) ; } catch ( IOException e ) { LOG . warn ( "Exception while closing channel for log file:" + logId ) ; } }
public void test() { if ( n1 == null ) { LOG . warn ( "The cluster does not contain node: {}" , NodeBase . getPath ( node1 ) ) ; return Integer . MAX_VALUE ; } }
public void test() { if ( n2 == null ) { LOG . warn ( "The cluster does not contain node: {}" , NodeBase . getPath ( node2 ) ) ; return Integer . MAX_VALUE ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Starting the job with job id {}" , execution . getJobId ( ) ) ; } }
public void test() { try { job . dead ( execution ) ; jobAccessor . save ( execution ) ; } catch ( Exception t ) { logger . error ( "Unable to invoke dead event on job" , t ) ; } }
public void test() { if ( value == null ) { return null ; } }
public void test() { try { code_block = TryStatement ;  } catch ( final ValidationUserException e ) { final UiMessageStack uiMessageStack = routeContext . getUiMessageStack ( ) ; e . flushToUiMessageStack ( uiMessageStack ) ; return sendJsonUiMessageStack ( SC_UNPROCESSABLE_ENTITY , uiMessageStack , response ) ; } catch ( final VUserException e ) { final UiMessageStack uiMessageStack = routeContext . getUiMessageStack ( ) ; uiMessageStack . error ( e . getMessage ( ) ) ; return sendJsonUiMessageStack ( SC_UNPROCESSABLE_ENTITY , uiMessageStack , response ) ; } catch ( final SessionException e ) { return sendJsonError ( HttpServletResponse . SC_UNAUTHORIZED , e , response ) ; } catch ( final VSecurityException e ) { return sendJsonError ( HttpServletResponse . SC_FORBIDDEN , e , response ) ; } catch ( final JsonSyntaxException e ) { LOGGER . info ( "JsonSyntaxException" , e ) ; return sendJsonError ( HttpServletResponse . SC_BAD_REQUEST , e , response ) ; } catch ( final TooManyRequestException e ) { return sendJsonError ( SC_TOO_MANY_REQUEST , e , response ) ; } catch ( final Throwable e ) { LOGGER . error ( "Internal Server Error" , e ) ; return sendJsonError ( HttpServletResponse . SC_INTERNAL_SERVER_ERROR , e , response ) ; } }
public void test() { try { code_block = TryStatement ;  } catch ( final ValidationUserException e ) { final UiMessageStack uiMessageStack = routeContext . getUiMessageStack ( ) ; e . flushToUiMessageStack ( uiMessageStack ) ; return sendJsonUiMessageStack ( SC_UNPROCESSABLE_ENTITY , uiMessageStack , response ) ; } catch ( final VUserException e ) { final UiMessageStack uiMessageStack = routeContext . getUiMessageStack ( ) ; uiMessageStack . error ( e . getMessage ( ) ) ; return sendJsonUiMessageStack ( SC_UNPROCESSABLE_ENTITY , uiMessageStack , response ) ; } catch ( final SessionException e ) { return sendJsonError ( HttpServletResponse . SC_UNAUTHORIZED , e , response ) ; } catch ( final VSecurityException e ) { return sendJsonError ( HttpServletResponse . SC_FORBIDDEN , e , response ) ; } catch ( final JsonSyntaxException e ) { LOGGER . info ( "JsonSyntaxException" , e ) ; return sendJsonError ( HttpServletResponse . SC_BAD_REQUEST , e , response ) ; } catch ( final TooManyRequestException e ) { return sendJsonError ( SC_TOO_MANY_REQUEST , e , response ) ; } catch ( final Throwable e ) { LOGGER . error ( "Internal Server Error" , e ) ; return sendJsonError ( HttpServletResponse . SC_INTERNAL_SERVER_ERROR , e , response ) ; } }
public void test() { if ( jobCounter . get ( ) != currentAlertCount ) { LOGGER . info ( "alerts evaluated so far: {}" , jobCounter . get ( ) ) ; } }
public void test() { try { int currentAlertCount = jobCounter . get ( ) ; jobCounter . addAndGet ( service . executeScheduledAlerts ( 50 , timeout ) ) ; code_block = IfStatement ; Thread . sleep ( POLL_INTERVAL_MS ) ; } catch ( InterruptedException ex ) { LOGGER . info ( "Execution was interrupted." ) ; Thread . currentThread ( ) . interrupt ( ) ; break ; } catch ( Throwable ex ) { LOGGER . error ( "Exception in alerter: {}" , ExceptionUtils . getFullStackTrace ( ex ) ) ; } }
public void test() { try { int currentAlertCount = jobCounter . get ( ) ; jobCounter . addAndGet ( service . executeScheduledAlerts ( 50 , timeout ) ) ; code_block = IfStatement ; Thread . sleep ( POLL_INTERVAL_MS ) ; } catch ( InterruptedException ex ) { LOGGER . info ( "Execution was interrupted." ) ; Thread . currentThread ( ) . interrupt ( ) ; break ; } catch ( Throwable ex ) { LOGGER . error ( "Exception in alerter: {}" , ExceptionUtils . getFullStackTrace ( ex ) ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( exception , exception ) ; } }
public void test() { try { v = Short . parseShort ( value . trim ( ) ) ; } catch ( NumberFormatException e ) { Logger logger = Logger . getLogger ( StringTools . class ) ; logger . warn ( "Could not create a short from the string " + value ) ; throw new IllegalArgumentException ( "Must be of type short!" , e ) ; } }
@ Override public void lifeCycleStarted ( LifeCycle event ) { final Set < SslReload > reloaders = new HashSet < > ( ) ; reloaders . addAll ( getReloaders ( environment . getApplicationContext ( ) ) ) ; reloaders . addAll ( getReloaders ( environment . getAdminContext ( ) ) ) ; LOGGER . info ( "{} ssl reloaders registered" , reloaders . size ( ) ) ; reloadTask . setReloaders ( reloaders ) ; }
public void test() { if ( totalError > lastTotalError ) { logger . warn ( "Exception in dict\n {}" , msg ) ; lastTotalError = totalError ; } else { logger . info ( msg ) ; } }
public void test() { if ( totalError > lastTotalError ) { logger . warn ( "Exception in dict\n {}" , msg ) ; lastTotalError = totalError ; } else { logger . info ( msg ) ; } }
@ Override public void onConnectGPRSRequest ( ConnectGPRSRequest ind ) { this . logger . debug ( "ConnectGPRSRequest" ) ; TestEvent te = TestEvent . createReceivedEvent ( EventType . ConnectGPRSRequest , ind , sequence ++ ) ; this . observerdEvents . add ( te ) ; }
@ Override public void onError ( final Task task , final Throwable throwable ) { logger . error ( "Exception thrown while initializing consumers." , throwable ) ; errorReference . compareAndSet ( null , throwable ) ; }
public void test() { try { return file . toURI ( ) . toURL ( ) ; } catch ( MalformedURLException e ) { LOG . debug ( String . format ( "File '%s' can't be converted to URL" , file ) , e ) ; return null ; } }
public void test() { { final String user = SecurityContextHolder . getContext ( ) . getAuthentication ( ) . getName ( ) ; logger . info ( "{} try to add Node {} To ReplicaSet {}" , user , nodeID , replicaSetID ) ; streamingService . addNodeToReplicaSet ( replicaSetID , nodeID ) ; } }
public void test() { if ( read != slotFile . length ( ) && logger . isWarnEnabled ( ) ) { logger . warn ( "SlotManager in {} read size does not equal to file size: {}/{}" , slotFilePath , read , slotFile . length ( ) ) ; } }
public void test() { try ( FileInputStream fileInputStream = new FileInputStream ( slotFile ) ; BufferedInputStream bufferedInputStream = new BufferedInputStream ( fileInputStream ) ) { byte [ ] bytes = new byte [ ( int ) slotFile . length ( ) ] ; int read = bufferedInputStream . read ( bytes ) ; code_block = IfStatement ; deserialize ( ByteBuffer . wrap ( bytes ) ) ; return true ; } catch ( Exception e ) { logger . warn ( "Cannot deserialize slotManager from {}" , slotFilePath , e ) ; return false ; } }
public void test() { try { Log . debug ( "Annotating WorkflowRun " + workflowSWID + " with skip=" + skip + ", Att = " + att ) ; Workflow obj = ll . findWorkflow ( "/" + workflowSWID ) ; code_block = IfStatement ; code_block = IfStatement ; ll . updateWorkflow ( "/" + workflowSWID , obj ) ; } catch ( IOException ex ) { Log . error ( "IOException while updating study " + workflowSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } catch ( JAXBException ex ) { Log . error ( "JAXBException while updating study " + workflowSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } catch ( ResourceException ex ) { Log . error ( "ResourceException while updating study " + workflowSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } }
public void test() { if ( skip != null ) { Log . info ( "Processing does not have a skip column!" ) ; } }
public void test() { try { Log . debug ( "Annotating WorkflowRun " + workflowSWID + " with skip=" + skip + ", Att = " + att ) ; Workflow obj = ll . findWorkflow ( "/" + workflowSWID ) ; code_block = IfStatement ; code_block = IfStatement ; ll . updateWorkflow ( "/" + workflowSWID , obj ) ; } catch ( IOException ex ) { Log . error ( "IOException while updating study " + workflowSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } catch ( JAXBException ex ) { Log . error ( "JAXBException while updating study " + workflowSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } catch ( ResourceException ex ) { Log . error ( "ResourceException while updating study " + workflowSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } }
public void test() { try { Log . debug ( "Annotating WorkflowRun " + workflowSWID + " with skip=" + skip + ", Att = " + att ) ; Workflow obj = ll . findWorkflow ( "/" + workflowSWID ) ; code_block = IfStatement ; code_block = IfStatement ; ll . updateWorkflow ( "/" + workflowSWID , obj ) ; } catch ( IOException ex ) { Log . error ( "IOException while updating study " + workflowSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } catch ( JAXBException ex ) { Log . error ( "JAXBException while updating study " + workflowSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } catch ( ResourceException ex ) { Log . error ( "ResourceException while updating study " + workflowSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } }
public void test() { try { Log . debug ( "Annotating WorkflowRun " + workflowSWID + " with skip=" + skip + ", Att = " + att ) ; Workflow obj = ll . findWorkflow ( "/" + workflowSWID ) ; code_block = IfStatement ; code_block = IfStatement ; ll . updateWorkflow ( "/" + workflowSWID , obj ) ; } catch ( IOException ex ) { Log . error ( "IOException while updating study " + workflowSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } catch ( JAXBException ex ) { Log . error ( "JAXBException while updating study " + workflowSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } catch ( ResourceException ex ) { Log . error ( "ResourceException while updating study " + workflowSWID + " " + ex . getMessage ( ) ) ; wrapAsRuntimeException ( ex ) ; } }
public void test() { if ( pTO . getIdentifiers ( ) . size ( ) == 1 ) { IdentifierTO ito = new IdentifierTO ( ) ; ito . setValue ( pTO . getIdentifiers ( ) . get ( 0 ) . getValue ( ) ) ; person . setIdentifier ( ito . getValue ( ) ) ; } else-if ( pTO . getIdentifiers ( ) . size ( ) > 1 ) { LOGGER . warn ( "Multiple identifiers found for Person: " + pTO . getId ( ) ) ; } }
public void test() { try { detector = getDefaultLanguageDetector ( ) ; detector . loadModels ( ) ; } catch ( IOException e ) { logger . warn ( "Can not load lang detector models" , e ) ; } }
public void test() { if ( entityDTO == null || entityDTO . getId ( ) == null ) { throw new UnsupportedOperationException ( "Updated reminder entity DTO is invalid." ) ; } }
public void test() { if ( ! view . getForm ( ) . isValid ( ) ) { return ; } }
@ Override protected void tearDown ( ) throws Exception { connection . close ( ) ; service . stop ( ) ; context . close ( ) ; super . tearDown ( ) ; LOG . info ( " closed down the test case: " + getName ( ) ) ; }
protected final void initOutput ( BatchSinkContext context , BigQuery bigQuery , String outputName , String tableName , @ Nullable Schema tableSchema , String bucket , FailureCollector collector ) throws IOException { LOG . debug ( "Init output for table '{}' with schema: {}" , tableName , tableSchema ) ; List < BigQueryTableFieldSchema > fields = getBigQueryTableFields ( bigQuery , tableName , tableSchema , getConfig ( ) . isAllowSchemaRelaxation ( ) , collector ) ; Configuration configuration = new Configuration ( baseConfiguration ) ; String temporaryGcsPath = BigQuerySinkUtils . getTemporaryGcsPath ( bucket , runUUID . toString ( ) , tableName ) ; BigQuerySinkUtils . configureOutput ( configuration , getConfig ( ) . getDatasetProject ( ) , getConfig ( ) . getDataset ( ) , tableName , temporaryGcsPath , fields ) ; List < String > fieldNames = fields . stream ( ) . map ( BigQueryTableFieldSchema :: getName ) . collect ( Collectors . toList ( ) ) ; recordLineage ( context , outputName , tableSchema , fieldNames ) ; context . addOutput ( Output . of ( outputName , getOutputFormatProvider ( configuration , tableName , tableSchema ) ) ) ; }
public void test() { if ( ! authorizationResponseStr . contains ( "" ) ) { Wait < WebDriver > wait = new FluentWait < WebDriver > ( driver ) . withTimeout ( Duration . ofSeconds ( WAIT_OPERATION_TIMEOUT ) ) . pollingEvery ( Duration . ofMillis ( 500 ) ) . ignoring ( NoSuchElementException . class ) ; WebElement allowButton = wait . until ( new Function < WebDriver , WebElement > ( ) code_block = "" ; ) ; JavascriptExecutor jse = ( JavascriptExecutor ) driver ; jse . executeScript ( "scroll(0, 1000)" ) ; String previousURL = driver . getCurrentUrl ( ) ; Actions actions = new Actions ( driver ) ; actions . click ( allowButton ) . perform ( ) ; authorizationResponseStr = driver . getCurrentUrl ( ) ; authorizationResponse = new AuthorizationResponse ( authorizationResponseStr ) ; LOG . info ( "Authorization Response url is: " + driver . getCurrentUrl ( ) ) ; } else { fail ( "The authorization form was expected to be shown." ) ; } }
public void test() { try { AtlasAuthorizer authorizer = AtlasAuthorizerFactory . getAtlasAuthorizer ( ) ; request . setUser ( getCurrentUserName ( ) , getCurrentUserGroups ( ) ) ; request . setClientIPAddress ( RequestContext . get ( ) . getClientIPAddress ( ) ) ; request . setForwardedAddresses ( RequestContext . get ( ) . getForwardedAddresses ( ) ) ; request . setRemoteIPAddress ( RequestContext . get ( ) . getClientIPAddress ( ) ) ; authorizer . filterTypesDef ( request ) ; } catch ( AtlasAuthorizationException e ) { LOG . error ( "Unable to obtain AtlasAuthorizer" , e ) ; } }
@ Test void parse ( ) throws Exception { JavaClassSource clazz = ( JavaClassSource ) Roaster . parse ( new File ( "src/test/java/org/apache/camel/parser/java/MySimpleToDRoute.java" ) ) ; MethodSource < JavaClassSource > method = CamelJavaParserHelper . findConfigureMethod ( clazz ) ; List < CamelEndpointDetails > details = new ArrayList < > ( ) ; RouteBuilderParser . parseRouteBuilderEndpoints ( clazz , "." , "src/test/java/org/apache/camel/parser/java/MySimpleToDRoute.java" , details ) ; LOG . info ( "{}" , details ) ; List < ParserResult > list = CamelJavaParserHelper . parseCamelConsumerUris ( method , true , true ) ; code_block = ForStatement ; assertEquals ( "direct:start" , list . get ( 0 ) . getElement ( ) ) ; list = CamelJavaParserHelper . parseCamelProducerUris ( method , true , true ) ; code_block = ForStatement ; assertEquals ( "toD" , list . get ( 0 ) . getNode ( ) ) ; assertEquals ( "log:a" , list . get ( 0 ) . getElement ( ) ) ; assertEquals ( "to" , list . get ( 1 ) . getNode ( ) ) ; assertEquals ( "log:b" , list . get ( 1 ) . getElement ( ) ) ; assertEquals ( "to" , list . get ( 2 ) . getNode ( ) ) ; assertEquals ( "log:c" , list . get ( 2 ) . getElement ( ) ) ; assertEquals ( 3 , list . size ( ) ) ; assertEquals ( 4 , details . size ( ) ) ; assertEquals ( "direct:start" , details . get ( 0 ) . getEndpointUri ( ) ) ; assertEquals ( "log:a" , details . get ( 1 ) . getEndpointUri ( ) ) ; assertEquals ( "log:b" , details . get ( 2 ) . getEndpointUri ( ) ) ; assertEquals ( "log:c" , details . get ( 3 ) . getEndpointUri ( ) ) ; }
public void test() { for ( ParserResult result : list ) { LOG . info ( "Consumer: " + result . getElement ( ) ) ; } }
public void test() { for ( ParserResult result : list ) { LOG . info ( "Producer: " + result . getElement ( ) ) ; } }
public void test() { try { session . writeBuffer ( buffer ) ; } catch ( IOException e ) { log . error ( "Failed ({}) to send channel open packet for {}: {}" , e . getClass ( ) . getSimpleName ( ) , channel , e . getMessage ( ) ) ; throw new IllegalStateException ( "Failed to send packet" , e ) ; } }
public Object call ( ) throws Exception { LOG . info ( "Simulating a task which takes " + getEndpoint ( ) . getDelay ( ) + " millis to reply" ) ; Thread . sleep ( getEndpoint ( ) . getDelay ( ) ) ; int count = counter . incrementAndGet ( ) ; code_block = IfStatement ; LOG . info ( "Callback done(false)" ) ; callback . done ( false ) ; return null ; }
public void test() { if ( getEndpoint ( ) . getFailFirstAttempts ( ) >= count ) { LOG . info ( "Simulating a failure at attempt " + count ) ; exchange . setException ( new CamelExchangeException ( "Simulated error at attempt " + count , exchange ) ) ; } else { String reply = getEndpoint ( ) . getReply ( ) ; exchange . getMessage ( ) . setBody ( reply ) ; exchange . getMessage ( ) . setHeaders ( exchange . getIn ( ) . getHeaders ( ) ) ; LOG . info ( "Setting reply " + reply ) ; } }
public void test() { if ( getEndpoint ( ) . getFailFirstAttempts ( ) >= count ) { LOG . info ( "Simulating a failure at attempt " + count ) ; exchange . setException ( new CamelExchangeException ( "Simulated error at attempt " + count , exchange ) ) ; } else { String reply = getEndpoint ( ) . getReply ( ) ; exchange . getMessage ( ) . setBody ( reply ) ; exchange . getMessage ( ) . setHeaders ( exchange . getIn ( ) . getHeaders ( ) ) ; LOG . info ( "Setting reply " + reply ) ; } }
public void test() { if ( label . getLang ( ) . equals ( Locale . getDefault ( ) . getLanguage ( ) ) ) { final String labelText = label . getLabel ( ) ; LoggerFactory . getLogger ( this . getClass ( ) ) . debug ( labelText ) ; if ( labelText != null ) return labelText ; } }
public void test() { try { Message prototype = ( Message ) super . getEntityPrototype ( formTypeCode ) ; Integer version = 1 ; code_block = IfStatement ; prototype . setVersionType ( version ) ; String modelXml = new FormTypeDOM ( ) . getXml ( prototype ) ; StepsConfig stepsConfig = this . getStepsConfig ( formTypeCode ) ; String stepsXml = new StepConfigsDOM ( ) . createConfigXml ( stepsConfig ) ; TypeVersionGuiConfig versionConfig = new TypeVersionGuiConfig ( ) ; versionConfig . setFormTypeCode ( formTypeCode ) ; versionConfig . setPrototypeXml ( modelXml ) ; versionConfig . setPrototype ( prototype ) ; versionConfig . setStepsConfigXml ( stepsXml ) ; versionConfig . setStepsConfig ( stepsConfig ) ; Map < String , StepGuiConfig > workGuiConfigs = this . getFormTypeGuiDAO ( ) . getWorkGuiConfigs ( formTypeCode ) ; List < StepGuiConfig > guiConfigs = new ArrayList < StepGuiConfig > ( ) ; guiConfigs . addAll ( workGuiConfigs . values ( ) ) ; versionConfig . setGuiConfigs ( guiConfigs ) ; versionConfig . setVersion ( prototype . getVersionType ( ) ) ; this . generateLabels ( versionConfig ) ; this . getFormTypeGuiDAO ( ) . addTypeVersionGui ( versionConfig ) ; this . getLastVersionConfigs ( ) . put ( formTypeCode , versionConfig ) ; this . updateEntityPrototype ( prototype ) ; } catch ( Throwable t ) { _logger . error ( "Error generating new version type - {}" , formTypeCode , t ) ; throw new ApsSystemException ( "Error generating new version type - " + formTypeCode , t ) ; } }
private void sendRequest ( Socket socket , Integer port , String request , String arguments , Logger logger ) throws IOException { logger . debug ( "Connecting to NiFi instance" ) ; socket . setSoTimeout ( 60000 ) ; socket . connect ( new InetSocketAddress ( "localhost" , port ) ) ; logger . debug ( "Established connection to NiFi instance." ) ; socket . setSoTimeout ( 60000 ) ; logger . debug ( "Sending {} Command to port {}" , request , port ) ; final OutputStream socketOut = socket . getOutputStream ( ) ; final Properties nifiProps = loadProperties ( logger ) ; final String secretKey = nifiProps . getProperty ( "secret.key" ) ; code_block = IfStatement ; socketOut . flush ( ) ; }
private void sendRequest ( Socket socket , Integer port , String request , String arguments , Logger logger ) throws IOException { logger . debug ( "Connecting to NiFi instance" ) ; socket . setSoTimeout ( 60000 ) ; socket . connect ( new InetSocketAddress ( "localhost" , port ) ) ; logger . debug ( "Established connection to NiFi instance." ) ; socket . setSoTimeout ( 60000 ) ; logger . debug ( "Sending {} Command to port {}" , request , port ) ; final OutputStream socketOut = socket . getOutputStream ( ) ; final Properties nifiProps = loadProperties ( logger ) ; final String secretKey = nifiProps . getProperty ( "secret.key" ) ; code_block = IfStatement ; socketOut . flush ( ) ; }
public void test() { if ( db . isFile ( ) ) { final File temp = settings . getTempDirectory ( ) ; final File tempDB = new File ( temp , db . getName ( ) ) ; LOGGER . debug ( "copying database {} to {}" , db . toPath ( ) , temp . toPath ( ) ) ; Files . copy ( db . toPath ( ) , tempDB . toPath ( ) ) ; settings . setString ( Settings . KEYS . H2_DATA_DIRECTORY , temp . getPath ( ) ) ; final String connStr = settings . getString ( Settings . KEYS . DB_CONNECTION_STRING ) ; code_block = IfStatement ; database = new CveDB ( settings ) ; } else { throw new DatabaseException ( "Unable to open database - configured database file does not exist: " + db . toString ( ) ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Closing cache for feneric compiler " + compiler . getId ( ) ) ; } }
public Job run ( ) throws Exception { Job job = Job . getInstance ( getConf ( ) ) ; job . setJobName ( name ) ; job . setJarByClass ( RollupPhaseFourJob . class ) ; job . setMapperClass ( RollupPhaseFourMapper . class ) ; job . setInputFormatClass ( SequenceFileInputFormat . class ) ; job . setMapOutputKeyClass ( BytesWritable . class ) ; job . setMapOutputValueClass ( BytesWritable . class ) ; job . setCombinerClass ( RollupPhaseFourReducer . class ) ; job . setReducerClass ( RollupPhaseFourReducer . class ) ; job . setOutputKeyClass ( BytesWritable . class ) ; job . setOutputValueClass ( BytesWritable . class ) ; job . setOutputFormatClass ( SequenceFileOutputFormat . class ) ; String numReducers = props . getProperty ( "num.reducers" ) ; job . setNumReduceTasks ( 1 ) ; LOGGER . info ( "Setting number of reducers : " + job . getNumReduceTasks ( ) ) ; Configuration configuration = job . getConfiguration ( ) ; String inputPathDir = getAndSetConfiguration ( configuration , ROLLUP_PHASE4_INPUT_PATH ) ; getAndSetConfiguration ( configuration , ROLLUP_PHASE4_CONFIG_PATH ) ; getAndSetConfiguration ( configuration , ROLLUP_PHASE4_OUTPUT_PATH ) ; LOGGER . info ( "Input path dir: " + inputPathDir ) ; code_block = ForStatement ; FileOutputFormat . setOutputPath ( job , new Path ( getAndCheck ( ROLLUP_PHASE4_OUTPUT_PATH . toString ( ) ) ) ) ; job . waitForCompletion ( true ) ; return job ; }
public Job run ( ) throws Exception { Job job = Job . getInstance ( getConf ( ) ) ; job . setJobName ( name ) ; job . setJarByClass ( RollupPhaseFourJob . class ) ; job . setMapperClass ( RollupPhaseFourMapper . class ) ; job . setInputFormatClass ( SequenceFileInputFormat . class ) ; job . setMapOutputKeyClass ( BytesWritable . class ) ; job . setMapOutputValueClass ( BytesWritable . class ) ; job . setCombinerClass ( RollupPhaseFourReducer . class ) ; job . setReducerClass ( RollupPhaseFourReducer . class ) ; job . setOutputKeyClass ( BytesWritable . class ) ; job . setOutputValueClass ( BytesWritable . class ) ; job . setOutputFormatClass ( SequenceFileOutputFormat . class ) ; String numReducers = props . getProperty ( "num.reducers" ) ; job . setNumReduceTasks ( 1 ) ; LOGGER . info ( "Setting number of reducers : " + job . getNumReduceTasks ( ) ) ; Configuration configuration = job . getConfiguration ( ) ; String inputPathDir = getAndSetConfiguration ( configuration , ROLLUP_PHASE4_INPUT_PATH ) ; getAndSetConfiguration ( configuration , ROLLUP_PHASE4_CONFIG_PATH ) ; getAndSetConfiguration ( configuration , ROLLUP_PHASE4_OUTPUT_PATH ) ; LOGGER . info ( "Input path dir: " + inputPathDir ) ; code_block = ForStatement ; FileOutputFormat . setOutputPath ( job , new Path ( getAndCheck ( ROLLUP_PHASE4_OUTPUT_PATH . toString ( ) ) ) ) ; job . waitForCompletion ( true ) ; return job ; }
public void test() { for ( String inputPath : inputPathDir . split ( "," ) ) { LOGGER . info ( "Adding input:" + inputPath ) ; Path input = new Path ( inputPath ) ; FileInputFormat . addInputPath ( job , input ) ; } }
private void getHouseStatusCommsJob ( ) { logger . trace ( "getHouseStatusCommsJob() initiated by {} will process HouseStatus." , Thread . currentThread ( ) ) ; code_block = IfStatement ; logger . trace ( "getHouseStatusCommsJob() initiated by {} has finished." , Thread . currentThread ( ) ) ; }
public void test() { if ( new VeluxBridgeGetHouseStatus ( ) . evaluateState ( thisBridge ) ) { logger . trace ( "getHouseStatusCommsJob(): => GetHouseStatus() => updates received => synchronizing" ) ; syncChannelsWithProducts ( ) ; } else { logger . trace ( "getHouseStatusCommsJob(): => GetHouseStatus() => no updates" ) ; } }
public void test() { if ( new VeluxBridgeGetHouseStatus ( ) . evaluateState ( thisBridge ) ) { logger . trace ( "getHouseStatusCommsJob(): => GetHouseStatus() => updates received => synchronizing" ) ; syncChannelsWithProducts ( ) ; } else { logger . trace ( "getHouseStatusCommsJob(): => GetHouseStatus() => no updates" ) ; } }
public void test() { if ( this . logger . isDebugEnabled ( ) ) { this . logger . debug ( "Finalized classloader " + toString ( ) ) ; } }
public void test() { try { componentCSVRecord = new ComponentCSVRecordBuilder ( input ) . build ( ) ; } catch ( Exception e ) { log . error ( "Bad record " + input , e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( KBCommentServiceUtil . class , "getKBCommentsCount" , _getKBCommentsCountParameterTypes9 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( ( Integer ) returnObj ) . intValue ( ) ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( log . isErrorEnabled ( ) ) { log . error ( "Print error." , ex ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "results of getAllAuthorities1(): " + res ) ; } }
private static boolean isFieldQueried ( Path field , ValueComparisonExpression q , Path context ) { LOGGER . debug ( "Checking if field {} is queried by value comparison {}" , field , q ) ; return isFieldQueried ( field , q . getField ( ) , context ) ; }
public void test() { try { log . debug ( "cls classloader: " + cls . getClass ( ) . getClassLoader ( ) + " uid: " + cls . getClass ( ) . getClassLoader ( ) . hashCode ( ) ) ; } catch ( Exception e ) { } }
public void test() { if ( state == null ) { log . debug ( "Skipping reset of partition {} since it is no longer assigned" , tp ) ; } else-if ( ! state . awaitingReset ( ) ) { log . debug ( "Skipping reset of partition {} since reset is no longer needed" , tp ) ; } else-if ( requestedResetStrategy != state . resetStrategy ) { log . debug ( "Skipping reset of partition {} since an alternative reset has been requested" , tp ) ; } else { log . info ( "Resetting offset for partition {} to position {}." , tp , position ) ; state . seekUnvalidated ( position ) ; } }
public void test() { if ( state == null ) { log . debug ( "Skipping reset of partition {} since it is no longer assigned" , tp ) ; } else-if ( ! state . awaitingReset ( ) ) { log . debug ( "Skipping reset of partition {} since reset is no longer needed" , tp ) ; } else-if ( requestedResetStrategy != state . resetStrategy ) { log . debug ( "Skipping reset of partition {} since an alternative reset has been requested" , tp ) ; } else { log . info ( "Resetting offset for partition {} to position {}." , tp , position ) ; state . seekUnvalidated ( position ) ; } }
public void test() { if ( state == null ) { log . debug ( "Skipping reset of partition {} since it is no longer assigned" , tp ) ; } else-if ( ! state . awaitingReset ( ) ) { log . debug ( "Skipping reset of partition {} since reset is no longer needed" , tp ) ; } else-if ( requestedResetStrategy != state . resetStrategy ) { log . debug ( "Skipping reset of partition {} since an alternative reset has been requested" , tp ) ; } else { log . info ( "Resetting offset for partition {} to position {}." , tp , position ) ; state . seekUnvalidated ( position ) ; } }
public void test() { if ( state == null ) { log . debug ( "Skipping reset of partition {} since it is no longer assigned" , tp ) ; } else-if ( ! state . awaitingReset ( ) ) { log . debug ( "Skipping reset of partition {} since reset is no longer needed" , tp ) ; } else-if ( requestedResetStrategy != state . resetStrategy ) { log . debug ( "Skipping reset of partition {} since an alternative reset has been requested" , tp ) ; } else { log . info ( "Resetting offset for partition {} to position {}." , tp , position ) ; state . seekUnvalidated ( position ) ; } }
public void test() { try { stopSyncInternal ( syncPoint ) ; } catch ( InvalidPathException e ) { LOG . warn ( "stop: InvalidPathException resolving syncPoint {}, exception {}" , syncPoint , e ) ; return ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Determined user DN prefix [{}] and suffix [{}]" , prefix , suffix ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "ServerToClientFunctionResultSender setting exception {} " , exception ) ; } }
public void test() { try { code_block = IfStatement ; String exceptionMessage = exception . getMessage ( ) != null ? exception . getMessage ( ) : "Exception occurred during function execution" ; logger . warn ( String . format ( "Exception on server while executing function : %s" , this . fn ) , exception ) ; code_block = IfStatement ; writeFunctionExceptionResponse ( msg , exceptionMessage , exception ) ; this . lastResultReceived = true ; } catch ( IOException ignoreAsSocketIsClosed ) { } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "ServerToClientFunctionResultSender sending Function Exception : " ) ; } }
public void test() { try { flatPushMessageInformationDao . update ( pushMessageInformation ) ; } catch ( Exception e ) { logger . info ( "Failed to save pushMessageInformation: {}" , e . getMessage ( ) ) ; logger . debug ( "Details:" , e ) ; } }
public void test() { try { flatPushMessageInformationDao . update ( pushMessageInformation ) ; } catch ( Exception e ) { logger . info ( "Failed to save pushMessageInformation: {}" , e . getMessage ( ) ) ; logger . debug ( "Details:" , e ) ; } }
public void test() { try { LOG . info ( "A dead datanode is detected. {}" , datanodeDetails ) ; destroyPipelines ( datanodeDetails ) ; closeContainers ( datanodeDetails , publisher ) ; code_block = IfStatement ; } catch ( NodeNotFoundException ex ) { LOG . error ( "DeadNode event for a unregistered node: {}!" , datanodeDetails ) ; } }
public void test() { if ( exists == null ) { logger . error ( "Cannot find delivery Tag in path:" + deliveryTagPath + " for this experiment" ) ; return - 1 ; } }
public void test() { if ( snapshotString == null || snapshotString . size ( ) == 0 ) { LOG . info ( "SnapshotString in AlertSnapshotBean {} is empty, return an empty map instead" , getId ( ) ) ; } else { code_block = ForStatement ; } }
public void test() { try { AnomalyNotifiedStatus status = OBJECT_MAPPER . readValue ( statusString , AnomalyNotifiedStatus . class ) ; snapshot . put ( entry . getKey ( ) , status ) ; } catch ( IOException e ) { LOG . error ( "Unable to parse String {} to Status" , statusString ) ; } }
public void test() { try { ManagementFactory . getRuntimeMXBean ( ) . getInputArguments ( ) . stream ( ) . forEach ( s -> logger . debug ( "Parameter: {}" , s ) ) ; System . getProperties ( ) . entrySet ( ) . stream ( ) . forEach ( e -> logger . debug ( "Property: {}={}" , e . getKey ( ) , e . getValue ( ) ) ) ; System . getenv ( ) . entrySet ( ) . forEach ( e -> logger . debug ( "Env: {}={}" , e . getKey ( ) , e . getValue ( ) ) ) ; logger . debug ( "Option: {}" , options ) ; } catch ( final Exception e ) { } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Destroying LaContainer.." ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "SuggestCreator is stopped." , e ) ; } else-if ( logger . isInfoEnabled ( ) ) { logger . info ( "SuggestCreator is stopped." ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "SuggestCreator is stopped." , e ) ; } else-if ( logger . isInfoEnabled ( ) ) { logger . info ( "SuggestCreator is stopped." ) ; } }
public void test() { try { SingletonLaContainerFactory . setConfigPath ( "app.xml" ) ; SingletonLaContainerFactory . setExternalContext ( new GenericExternalContext ( ) ) ; SingletonLaContainerFactory . setExternalContextComponentDefRegister ( new GenericExternalContextComponentDefRegister ( ) ) ; SingletonLaContainerFactory . init ( ) ; final Thread shutdownCallback = new Thread ( "ShutdownHook" ) code_block = "" ; ; Runtime . getRuntime ( ) . addShutdownHook ( shutdownCallback ) ; systemMonitorTask = TimeoutManager . getInstance ( ) . addTimeoutTarget ( new SystemMonitorTarget ( ) , ComponentUtil . getFessConfig ( ) . getSuggestSystemMonitorIntervalAsInteger ( ) , true ) ; exitCode = process ( options ) ; } catch ( final ContainerNotAvailableException e ) { code_block = IfStatement ; exitCode = Constants . EXIT_FAIL ; } catch ( final Throwable t ) { logger . error ( "Suggest creator does not work correctly." , t ) ; exitCode = Constants . EXIT_FAIL ; } finally { code_block = IfStatement ; destroyContainer ( ) ; } }
private void cancelAllJobs ( ) { logger . debug ( "Cancel all jobs." ) ; JOBS . keySet ( ) . forEach ( this :: cancelJob ) ; }
@ Secured ( ServicesData . ROLE_GET_AGENCIES ) @ GetMapping ( path = RestApi . PATH_REFERENTIAL_ID ) public AgencyDto getOne ( final @ PathVariable ( "identifier" ) String identifier ) { LOGGER . debug ( "get agency identifier={}" ) ; ParameterChecker . checkParameter ( "Identifier is mandatory : " , identifier ) ; return agencyExternalService . getOne ( identifier ) ; }
public void test() { try { st . push ( new FlowVariable ( Scope . Local . getPrefix ( ) + "(drop) " + f , child . toURI ( ) . toURL ( ) . toString ( ) , Scope . Local ) ) ; } catch ( MalformedURLException mue ) { LOGGER . warn ( "Unable to process drop file" , mue ) ; } }
public void test() { try { checkNotNull ( "file" , file ) ; checkNotNull ( "attrs" , attrs ) ; code_block = IfStatement ; } catch ( final Exception e ) { logger . error ( "Authz policy file VFS read error: " + file . getFileName ( ) , e ) ; return FileVisitResult . TERMINATE ; } }
public void test() { if ( systemMetadata . has ( "id" ) ) { this . id = systemMetadata . getInt ( "id" ) ; metadata . setId ( this . id ) ; } else { log . error ( "SHERPA internal ID missing for API response item" ) ; } }
public void test() { if ( systemMetadata . has ( "id" ) ) { this . id = systemMetadata . getInt ( "id" ) ; metadata . setId ( this . id ) ; } else { log . error ( "SHERPA internal ID missing for API response item" ) ; } }
public void test() { if ( System . currentTimeMillis ( ) >= stopTime ) { log . info ( "Request to terminate work. Max duration reached" ) ; return true ; } }
private static void print ( String content ) { LOG . info ( content ) ; }
public void test() { try { AttachmentId attachmentId = AttachmentId . from ( attachment . getBlobId ( ) . getRawValue ( ) ) ; return OptionalUtils . executeIfEmpty ( Optional . ofNullable ( attachmentsById . get ( attachmentId ) ) . map ( attachmentMetadata -> MessageAttachmentMetadata . builder ( ) . attachment ( attachmentMetadata ) . name ( attachment . getName ( ) . orElse ( null ) ) . cid ( attachment . getCid ( ) . map ( Cid :: from ) . orElse ( null ) ) . isInline ( attachment . isIsInline ( ) ) . build ( ) ) , ( ) -> LOGGER . error ( String . format ( "Attachment %s not found" , attachment . getBlobId ( ) ) ) ) ; } catch ( IllegalStateException e ) { LOGGER . error ( String . format ( "Attachment %s is not well-formed" , attachment . getBlobId ( ) ) , e ) ; return Optional . empty ( ) ; } }
private void updateBrokerState ( final BrokerState newState ) { LOG . info ( "update broker state from {} to {}" , BrokerState . codeOf ( brokerState ) , newState ) ; brokerState = newState . getCode ( ) ; heartbeat ( ) ; }
public void test() { if ( recoveryValue . getObjectName ( ) == null || recoveryValue . getObjectName ( ) . isEmpty ( ) ) { this . lowerBoundIdValue = selectLowerBoundValue ( sourceConnect ) ; log . debug ( "START object '{}' from = {}" , operationParams . tableName , recoveryValue ) ; } else-if ( recoveryValue . getProcessedObject ( ) != null && ! recoveryValue . getProcessedObject ( ) . isEmpty ( ) && isContain ( recoveryValue . getProcessedObject ( ) , currentTableName ) ) { log . debug ( "SKIP object '{}' by = {}" , operationParams . tableName , recoveryValue ) ; return true ; } else { code_block = IfStatement ; } }
public void test() { if ( recoveryValue . getObjectName ( ) != null && recoveryValue . getObjectName ( ) . equalsIgnoreCase ( currentTableName ) && ! isContain ( recoveryValue . getProcessedObject ( ) , currentTableName ) ) { this . lowerBoundIdValue = recoveryValue . getLastColumnValue ( ) ; log . debug ( "RESTORED object '{}' from = {}" , operationParams . tableName , recoveryValue ) ; } else { this . lowerBoundIdValue = selectLowerBoundValue ( sourceConnect ) ; log . debug ( "GO NEXT object '{}' by = {}" , operationParams . tableName , recoveryValue ) ; } }
public void test() { if ( recoveryValue . getObjectName ( ) != null && recoveryValue . getObjectName ( ) . equalsIgnoreCase ( currentTableName ) && ! isContain ( recoveryValue . getProcessedObject ( ) , currentTableName ) ) { this . lowerBoundIdValue = recoveryValue . getLastColumnValue ( ) ; log . debug ( "RESTORED object '{}' from = {}" , operationParams . tableName , recoveryValue ) ; } else { this . lowerBoundIdValue = selectLowerBoundValue ( sourceConnect ) ; log . debug ( "GO NEXT object '{}' by = {}" , operationParams . tableName , recoveryValue ) ; } }
public void test() { try ( PreparedStatement query = prepareStatement ( "SELECT commits.id, commits.elasticId, commits.refid, commits.timestamp, committype.name, creator FROM commits JOIN committype ON commits.committype = committype.id WHERE elasticId = ?" ) ) { query . setString ( 1 , commitId ) ; code_block = TryStatement ;  } catch ( Exception e ) { logger . warn ( String . format ( "%s" , LogUtil . getStackTrace ( e ) ) ) ; } finally { close ( ) ; } }
public void test() { try { DeleteResult response = collection . deleteMany ( new BasicDBObject ( ) ) ; return response . getDeletedCount ( ) ; } catch ( MongoException e ) { LOGGER . error ( "Error while delete documents" , e ) ; throw new DatabaseException ( "Error while delete documents" , e ) ; } }
private String bratRenderLaterCommand ( ) { LOG . trace ( "[{}][{}] bratRenderLaterCommand" , getMarkupId ( ) , vis . getMarkupId ( ) ) ; return "Wicket.$('" + vis . getMarkupId ( ) + "').dispatcher.post('current', " + "[" + toJson ( getCollection ( ) ) + ", '1234', {}, true]);" ; }
public void skip ( String containerId , Number taskId , String userId ) { containerId = context . getContainerId ( containerId , new ByTaskIdContainerLocator ( taskId . longValue ( ) ) ) ; userId = getUser ( userId ) ; logger . debug ( "About to skip task with id '{}' as user '{}'" , taskId , userId ) ; userTaskService . skip ( containerId , taskId . longValue ( ) , userId ) ; }
public void test() { try { newMessage = builder . buildQueryResponse ( results , ( String ) task . getParameters ( ) . get ( TaskRequestsConstants . P_QUERY_NUMBER ) , this . plugin . getName ( ) ) ; } catch ( IOException ex ) { LoggerFactory . getLogger ( QueryHandler . class ) . error ( ex . getMessage ( ) , ex ) ; } }
public void test() { try { String t = Text . decode ( s . getBytes ( ) , 0 , s . getBytes ( ) . length ) ; return t . getBytes ( ) ; } catch ( CharacterCodingException e ) { LOG . warn ( "Error generating json binary type from object." , e ) ; return null ; } }
public void test() { if ( null == topicConfig ) { log . warn ( "[cloneGroupOffset], topic config not exist, {}" , topic ) ; continue ; } }
@ Override public void progress ( float percent ) { synchronized ( this ) code_block = "" ; assertTrue ( percent >= 0 ) ; assertTrue ( percent <= 100 ) ; adaptee . progress ( percent ) ; LOGGER . info ( "progress:" + percent ) ; }
public void test() { try { File hookDir = Controller . HOOKS_DIR . resolve ( subdir ) . toFile ( ) ; code_block = IfStatement ; } catch ( IOException e ) { log . error ( "Failed to list hooks." , e ) ; } }
public void test() { try { java . util . List < com . liferay . commerce . product . model . CPDefinitionLink > returnValue = CPDefinitionLinkServiceUtil . getCPDefinitionLinks ( cpDefinitionId ) ; return com . liferay . commerce . product . model . CPDefinitionLinkSoap . toSoapModels ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Mem: QUERY " + key1 + " = " + value1 + " AND " + key2 + " = " + value2 ) ; } }
public void test() { if ( ! ( value1 . equals ( state . get ( key1 ) ) && value2 . equals ( state . get ( key2 ) ) ) ) { continue ; } }
public void test() { if ( glVCB != - 1 && tId != - 1 ) { int textureUnitId = JOGLUtils . getTextureUnitConst ( i ) ; gl . glClientActiveTexture ( textureUnitId ) ; gl . glActiveTexture ( textureUnitId ) ; gl . glEnable ( GL . GL_TEXTURE_2D ) ; gl . glEnableClientState ( GL . GL_TEXTURE_COORD_ARRAY ) ; gl . glBindBufferARB ( GL . GL_ARRAY_BUFFER_ARB , glVCB ) ; gl . glTexCoordPointer ( 2 , GL . GL_FLOAT , 0 , 0 ) ; gl . glBindTexture ( GL . GL_TEXTURE_2D , tId ) ; gl . glTexParameteri ( GL . GL_TEXTURE_2D , GL . GL_TEXTURE_WRAP_S , GL . GL_CLAMP_TO_EDGE ) ; gl . glTexParameteri ( GL . GL_TEXTURE_2D , GL . GL_TEXTURE_WRAP_T , GL . GL_CLAMP_TO_EDGE ) ; } else { LOG . warn ( "No texture id or no texture coordinates for texture(" + i + "), ignoring this texture." ) ; } }
public void test() { if ( glVCB != - 1 && tId != - 1 ) { gl . glClientActiveTexture ( GL . GL_TEXTURE0 ) ; gl . glActiveTexture ( GL . GL_TEXTURE0 ) ; gl . glEnable ( GL . GL_TEXTURE_2D ) ; gl . glEnableClientState ( GL . GL_TEXTURE_COORD_ARRAY ) ; gl . glBindBufferARB ( GL . GL_ARRAY_BUFFER_ARB , glVCB ) ; gl . glTexCoordPointer ( 2 , GL . GL_FLOAT , 0 , 0 ) ; gl . glBindTexture ( GL . GL_TEXTURE_2D , tId ) ; gl . glTexParameteri ( GL . GL_TEXTURE_2D , GL . GL_TEXTURE_WRAP_S , GL . GL_CLAMP_TO_EDGE ) ; gl . glTexParameteri ( GL . GL_TEXTURE_2D , GL . GL_TEXTURE_WRAP_T , GL . GL_CLAMP_TO_EDGE ) ; code_block = ForStatement ; shaderProgram . useProgram ( gl ) ; code_block = ForStatement ; } else { LOG . warn ( "No texture id or no texture coordinates for texture(0), this fragments textures." ) ; } }
public void test() { try { messageProcessor . writeHeader ( transactionContext ) ; messageProcessor . writeMessageEnd ( ) ; transport . flush ( ) ; String infoLog = "response[seqId:" + transactionContext . seqId ( ) + ", respCode:" + soaHeader . getRespCode ( ) . get ( ) + "]:" + "service[" + soaHeader . getServiceName ( ) + "]:version[" + soaHeader . getVersionName ( ) + "]:method[" + soaHeader . getMethodName ( ) + "]" + ( soaHeader . getOperatorId ( ) . isPresent ( ) ? " operatorId:" + soaHeader . getOperatorId ( ) . get ( ) : "" ) + ( soaHeader . getUserId ( ) . isPresent ( ) ? " userId:" + soaHeader . getUserId ( ) . get ( ) : "" ) ; LOGGER . info ( getClass ( ) + " " + infoLog + ", payload:\n" + soaException . getMessage ( ) ) ; } catch ( Throwable e ) { LOGGER . error ( e . getMessage ( ) , e ) ; } finally { container . requestCounter ( ) . decrementAndGet ( ) ; MDC . remove ( SoaSystemEnvProperties . KEY_LOGGER_SESSION_TID ) ; } }
public void test() { try { messageProcessor . writeHeader ( transactionContext ) ; messageProcessor . writeMessageEnd ( ) ; transport . flush ( ) ; String infoLog = "response[seqId:" + transactionContext . seqId ( ) + ", respCode:" + soaHeader . getRespCode ( ) . get ( ) + "]:" + "service[" + soaHeader . getServiceName ( ) + "]:version[" + soaHeader . getVersionName ( ) + "]:method[" + soaHeader . getMethodName ( ) + "]" + ( soaHeader . getOperatorId ( ) . isPresent ( ) ? " operatorId:" + soaHeader . getOperatorId ( ) . get ( ) : "" ) + ( soaHeader . getUserId ( ) . isPresent ( ) ? " userId:" + soaHeader . getUserId ( ) . get ( ) : "" ) ; LOGGER . info ( getClass ( ) + " " + infoLog + ", payload:\n" + soaException . getMessage ( ) ) ; } catch ( Throwable e ) { LOGGER . error ( e . getMessage ( ) , e ) ; } finally { container . requestCounter ( ) . decrementAndGet ( ) ; MDC . remove ( SoaSystemEnvProperties . KEY_LOGGER_SESSION_TID ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceCurrencyServiceUtil . class , "updateExchangeRate" , _updateExchangeRateParameterTypes12 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , commerceCurrencyId , exchangeRateProviderKey ) ; code_block = TryStatement ;  } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { code_block = IfStatement ; } catch ( ClassCastException e ) { log . debug ( "createGroup - Failed to create the group {}. The exception occure" , groupNameValue . getKey ( ) , e ) ; rollbackWithException ( ActionStatus . INVALID_YAML ) ; } }
@ Test public void testSetupWorked ( ) throws IOException { log . info ( "Checking input file(s) is/are present..." ) ; Path [ ] inputFiles = FileUtil . stat2Paths ( getFileSystem ( ) . listStatus ( input , new OutputLogFilter ( ) ) ) ; Assert . assertEquals ( testWarcs . length , inputFiles . length ) ; }
public void test() { try { org . apache . commons . io . FileUtils . deleteDirectory ( new File ( execLocalPath ) ) ; logger . info ( "exec local path: {} cleared." , execLocalPath ) ; } catch ( IOException e ) { logger . error ( "delete exec dir failed : {}" , e . getMessage ( ) , e ) ; } }
public void test() { if ( dv . getArchivalCopyLocation ( ) != null ) { logger . info ( "DatasetVersion id=" + ds . getGlobalId ( ) . toString ( ) + " v" + versionNumber + " submitted to Archive at: " + dv . getArchivalCopyLocation ( ) ) ; } else { logger . severe ( "Error submitting version due to conflict/error at Archive" ) ; } }
public void test() { { long start = System . currentTimeMillis ( ) ; code_block = ForStatement ; long end = System . currentTimeMillis ( ) ; long duration = end - start ; LOG . info ( "Thread " + count + " execution time: " + duration ) ; totalTime . addAndGet ( duration ) ; activeCount . decrementAndGet ( ) ; } }
public static void main ( String [ ] args ) throws InterruptedException { CouchbaseSampleEntryManager couchbaseSampleEntryManager = new CouchbaseSampleEntryManager ( ) ; final CouchbaseEntryManager couchbaseEntryManager = couchbaseSampleEntryManager . createCouchbaseEntryManager ( ) ; int countUsers = 1000000 ; int threadCount = 200 ; int threadIterationCount = 10 ; long totalStart = System . currentTimeMillis ( ) ; code_block = TryStatement ;  long totalEnd = System . currentTimeMillis ( ) ; long duration = totalEnd - totalStart ; LOG . info ( "Total execution time: " + duration + " after execution: " + ( threadCount * threadIterationCount ) ) ; System . out . println ( String . format ( "successResult: '%d', failedResult: '%d', errorResult: '%d'" , successResult . get ( ) , failedResult . get ( ) , errorResult . get ( ) ) ) ; }
public void test() { switch ( operation ) { case ASSIGN_CONTENT : assignContent ( in ) ; break ; case ASSIGN_EXPRESSION : assignExpression ( in ) ; break ; case EVAL : eval ( in ) ; break ; case VOID_EVAL : voidEval ( in ) ; break ; case PARSE_AND_EVAL : parseAndEval ( in ) ; break ; default : LOGGER . info ( "Nothing to do since operation has not been configured yet" ) ; break ; } }
public void test() { if ( awaked ) { log . warn ( String . format ( "Bundle %s already loaded" , bundleName ) ) ; return ; } }
public void test() { if ( log . isInfoEnabled ( ) ) { log . info ( String . format ( "Loading bundle %s" , bundleName ) ) ; } }
private void prepareServerNameListLength ( ClientEsniInner msg ) { msg . setServerNameListLength ( msg . getServerNameListBytes ( ) . getValue ( ) . length ) ; LOGGER . debug ( "ServerNameListLength: " + msg . getServerNameListLength ( ) . getValue ( ) ) ; }
public void test() { try { DistributedLockService service = DistributedLockService . getServiceNamed ( serviceName ) ; code_block = WhileStatement ; } catch ( VirtualMachineError e ) { SystemFailure . initiateFailure ( e ) ; throw e ; } catch ( Throwable t ) { logger . warn ( t ) ; fail ( t . getMessage ( ) ) ; } finally { return lockCount . get ( ) ; } }
public void test() { if ( ! loggingOff ) { log . info ( Hex . toHexString ( actual . getContractCallResult ( ) . toByteArray ( ) ) ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { long start = System . nanoTime ( ) ; writer . addDocument ( document ) ; log . debug ( "Inverted a document in {}us" , ( System . nanoTime ( ) - start ) / 1000 ) ; } else { writer . addDocument ( document ) ; } }
public void test() { try { Document document = getFinishedDocument ( doc ) ; code_block = IfStatement ; } catch ( IOException e ) { log . warn ( "Exception while inverting a document" , e ) ; exceptions . add ( e ) ; } finally { latch . countDown ( ) ; } }
public void test() { try { return execute ( ) ; } catch ( LdapException e ) { code_block = IfStatement ; String host = hosts [ currentConnectionIndex ] ; LOG . error ( "Catch exception with ldap host[" + host + "]." , e ) ; performFailover ( ) ; host = hosts [ currentConnectionIndex ] ; LOG . info ( "Failover to ldap host:" + host + "." ) ; return execute ( ) ; } }
@ Override public void accept ( String propertyName , Object value ) { var stateCommand = typeConverter . toStateCommand ( value ) ; channelHandler . updateItemState ( channel . getUID ( ) , stateCommand ) ; logger . debug ( "channel {} updated with {} ({})" , channel . getUID ( ) . getAsString ( ) , value , channel . getAcceptedItemType ( ) ) ; }
public void test() { if ( _log . isInfoEnabled ( ) ) { _log . info ( "Creating a new SAML keystore at " + samlKeyStoreFile ) ; } }
private boolean download ( final File jarFile ) throws IOException { final File tempFile = File . createTempFile ( "JFlex" , "zip" ) ; tempFile . deleteOnExit ( ) ; code_block = IfStatement ; log . info ( "downloading JFlex 1.4.3 from '" + DOWNLOAD_URL + "' ..." ) ; copyIntoFileAndCloseStream ( new URL ( DOWNLOAD_URL ) . openStream ( ) , tempFile ) ; log . info ( "finished downloading. Now extracting to " + downloadTo ) ; final ZipFile zipFile = new ZipFile ( tempFile ) ; code_block = TryStatement ;  return true ; }
public boolean teamExists ( Integer teamId ) { boolean exists = getHibernateTemplate ( ) . get ( ProgramTeam . class , teamId ) != null ; log . debug ( "teamExists: " + exists ) ; return exists ; }
public void test() { try { previousLocalListing = WatcherCommon . initStorage ( currentLocalListingFile , config . localDir ) ; } catch ( IOException e ) { updateStatus ( ThingStatus . OFFLINE , ThingStatusDetail . CONFIGURATION_ERROR , e . getMessage ( ) ) ; logger . debug ( "Can't write file {}: {}" , currentLocalListingFile , e . getMessage ( ) ) ; return ; } }
public void test() { if ( i % 1000000 == 0 ) { LOGGER . debug ( "checking: %s" , element . getId ( ) ) ; } }
public void test() { try { code_block = IfStatement ; headerSplitInfoRef . set ( splitInfo ) ; } catch ( IllegalStateException e ) { error . set ( true ) ; getLogger ( ) . error ( e . getMessage ( ) + " Routing to failure." , e ) ; } }
public void test() { if ( getLogger ( ) . isDebugEnabled ( ) ) { getLogger ( ) . debug ( "Computed splits in " + ( stop - start ) + " milliseconds." ) ; } }
private void recordAudit ( HttpServletRequest httpRequest , String whenISO9601 ) { final String who = getUserFromRequest ( httpRequest ) ; final String fromHost = httpRequest . getRemoteHost ( ) ; final String fromAddress = httpRequest . getRemoteAddr ( ) ; final String whatURL = Servlets . getRequestURL ( httpRequest ) ; final String whatAddrs = httpRequest . getLocalAddr ( ) ; LOG . debug ( "Audit: {}/{} performed request {} ({}) at time {}" , who , fromAddress , whatURL , whatAddrs , whenISO9601 ) ; GenericAlert . audit ( who , fromAddress , fromHost , whatURL , whatAddrs , whenISO9601 ) ; }
@ Path ( "/getMetadataShouldSucceed" ) @ POST public void getMetadataShouldSucceed ( ) { LOG . debug ( "Calling OpenstackSwiftContainerResource.getMetadataShouldSucceed()" ) ; String uri = String . format ( URI_FORMAT , SwiftConstants . GET_METADATA ) ; Map < ? , ? > metadata = template . requestBodyAndHeader ( uri , null , SwiftConstants . CONTAINER_NAME , CONTAINER_NAME , Map . class ) ; assertNotNull ( metadata ) ; assertEquals ( "2000" , metadata . get ( NAME_YEAR ) ) ; assertEquals ( "TestBook" , metadata . get ( NAME_BOOK ) ) ; }
public void test() { try { path = new PartialPath ( pathString ) ; } catch ( IllegalPathException e ) { logger . error ( "Failed to deserialize device {} from buffer" , pathString ) ; } }
public void test() { try { Mongo mongo = this . createNewMongo ( getProperties ( ) ) ; return mongo != null ; } catch ( Exception e ) { log . error ( "Error in checking Mongo config availability" , e ) ; return false ; } }
public void testGetFile ( ) throws Exception { ByteArrayContainerResponseWriter writer = new ByteArrayContainerResponseWriter ( ) ; String requestPath = SERVICE_URI + "item/" + fileId ; ContainerResponse response = launcher . service ( HttpMethod . GET , requestPath , BASE_URI , null , null , writer , null ) ; log . info ( new String ( writer . getBody ( ) ) ) ; assertEquals ( "Error: " + response . getEntity ( ) , 200 , response . getStatus ( ) ) ; Item item = ( Item ) response . getEntity ( ) ; assertEquals ( ItemType . FILE , item . getItemType ( ) ) ; assertEquals ( fileId , item . getId ( ) ) ; assertEquals ( filePath , item . getPath ( ) ) ; validateLinks ( item ) ; }
public void test() { if ( values . length == 2 ) { securityLogger . audit ( "Adding mapping: {} = {} to matchAllMap." , values [ 1 ] . trim ( ) , values [ 0 ] . trim ( ) ) ; matchAllMap . put ( values [ 1 ] . trim ( ) , values [ 0 ] . trim ( ) ) ; } else { LOGGER . warn ( "Match all mapping ignored: {} doesn't match expected format of metacardAttribute=userAttribute" , mapping ) ; } }
public void test() { { String targetPackageId = "target2" ; addTargetPackage ( targetPackageId ) ; ResourceIdentifier id = ResourceIdentifier . create ( ResourceType . ENTITY_TYPE , ENTITY_TYPE_A ) ; TestProgress progress = new TestProgress ( ) ; copyService . copy ( singletonList ( id ) , targetPackageId , progress ) ; await ( ) . atMost ( 5 , TimeUnit . SECONDS ) . until ( copyJobFinished ( progress ) ) ; LOG . info ( "Copy job progress: {}/{}" , progress . getProgress ( ) , progress . getProgressMax ( ) ) ; waitForWorkToBeFinished ( indexService , LOG ) ; Package targetPackage = metadataService . getPackage ( targetPackageId ) . get ( ) ; List < Package > packages = newArrayList ( targetPackage . getChildren ( ) ) ; List < EntityType > entityTypes = newArrayList ( targetPackage . getEntityTypes ( ) ) ; assertEquals ( 0 , packages . size ( ) ) ; assertEquals ( 1 , entityTypes . size ( ) ) ; EntityType entityTypeACopy = entityTypes . get ( 0 ) ; assertEquals ( "EntityType A" , entityTypeACopy . getLabel ( ) ) ; assertEquals ( ENTITY_TYPE_B , entityTypeA . getAttribute ( "xref_attr" ) . getRefEntity ( ) . getId ( ) ) ; assertEquals ( ENTITY_TYPE_B , entityTypeACopy . getAttribute ( "xref_attr" ) . getRefEntity ( ) . getId ( ) ) ; assertEquals ( 1 , progress . getProgress ( ) ) ; List < Object > entitiesOfA = dataService . findAll ( entityTypeACopy . getId ( ) ) . map ( Entity :: getIdValue ) . collect ( toList ( ) ) ; assertEquals ( asList ( "0" , "1" , "2" ) , entitiesOfA ) ; cleanupTargetPackage ( targetPackageId ) ; } }
public void test() { try { listener . destroyedFavorite ( status ) ; } catch ( Exception e ) { logger . warn ( "Exception at destroyFavorite" , e ) ; } }
@ Override public void processMessage ( final ObjectMessage message ) throws JMSException { LOGGER . debug ( "Processing public lighting set transition response message" ) ; String correlationUid = null ; String messageType = null ; int messagePriority = MessagePriorityEnum . DEFAULT . getPriority ( ) ; String organisationIdentification = null ; String deviceIdentification = null ; ResponseMessage responseMessage ; ResponseMessageResultType responseMessageResultType = null ; OsgpException osgpException = null ; code_block = TryStatement ;  code_block = TryStatement ;  }
public void test() { try { correlationUid = message . getJMSCorrelationID ( ) ; messageType = message . getJMSType ( ) ; messagePriority = message . getJMSPriority ( ) ; organisationIdentification = message . getStringProperty ( Constants . ORGANISATION_IDENTIFICATION ) ; deviceIdentification = message . getStringProperty ( Constants . DEVICE_IDENTIFICATION ) ; responseMessage = ( ResponseMessage ) message . getObject ( ) ; responseMessageResultType = responseMessage . getResult ( ) ; osgpException = responseMessage . getOsgpException ( ) ; } catch ( final JMSException e ) { LOGGER . error ( "UNRECOVERABLE ERROR, unable to read ObjectMessage instance, giving up." , e ) ; LOGGER . debug ( "correlationUid: {}" , correlationUid ) ; LOGGER . debug ( "messageType: {}" , messageType ) ; LOGGER . debug ( "messagePriority: {}" , messagePriority ) ; LOGGER . debug ( "organisationIdentification: {}" , organisationIdentification ) ; LOGGER . debug ( "deviceIdentification: {}" , deviceIdentification ) ; LOGGER . debug ( "responseMessageResultType: {}" , responseMessageResultType ) ; LOGGER . debug ( "deviceIdentification: {}" , deviceIdentification ) ; LOGGER . debug ( "osgpException" , osgpException ) ; return ; } }
public void test() { try { correlationUid = message . getJMSCorrelationID ( ) ; messageType = message . getJMSType ( ) ; messagePriority = message . getJMSPriority ( ) ; organisationIdentification = message . getStringProperty ( Constants . ORGANISATION_IDENTIFICATION ) ; deviceIdentification = message . getStringProperty ( Constants . DEVICE_IDENTIFICATION ) ; responseMessage = ( ResponseMessage ) message . getObject ( ) ; responseMessageResultType = responseMessage . getResult ( ) ; osgpException = responseMessage . getOsgpException ( ) ; } catch ( final JMSException e ) { LOGGER . error ( "UNRECOVERABLE ERROR, unable to read ObjectMessage instance, giving up." , e ) ; LOGGER . debug ( "correlationUid: {}" , correlationUid ) ; LOGGER . debug ( "messageType: {}" , messageType ) ; LOGGER . debug ( "messagePriority: {}" , messagePriority ) ; LOGGER . debug ( "organisationIdentification: {}" , organisationIdentification ) ; LOGGER . debug ( "deviceIdentification: {}" , deviceIdentification ) ; LOGGER . debug ( "responseMessageResultType: {}" , responseMessageResultType ) ; LOGGER . debug ( "deviceIdentification: {}" , deviceIdentification ) ; LOGGER . debug ( "osgpException" , osgpException ) ; return ; } }
public void test() { try { correlationUid = message . getJMSCorrelationID ( ) ; messageType = message . getJMSType ( ) ; messagePriority = message . getJMSPriority ( ) ; organisationIdentification = message . getStringProperty ( Constants . ORGANISATION_IDENTIFICATION ) ; deviceIdentification = message . getStringProperty ( Constants . DEVICE_IDENTIFICATION ) ; responseMessage = ( ResponseMessage ) message . getObject ( ) ; responseMessageResultType = responseMessage . getResult ( ) ; osgpException = responseMessage . getOsgpException ( ) ; } catch ( final JMSException e ) { LOGGER . error ( "UNRECOVERABLE ERROR, unable to read ObjectMessage instance, giving up." , e ) ; LOGGER . debug ( "correlationUid: {}" , correlationUid ) ; LOGGER . debug ( "messageType: {}" , messageType ) ; LOGGER . debug ( "messagePriority: {}" , messagePriority ) ; LOGGER . debug ( "organisationIdentification: {}" , organisationIdentification ) ; LOGGER . debug ( "deviceIdentification: {}" , deviceIdentification ) ; LOGGER . debug ( "responseMessageResultType: {}" , responseMessageResultType ) ; LOGGER . debug ( "deviceIdentification: {}" , deviceIdentification ) ; LOGGER . debug ( "osgpException" , osgpException ) ; return ; } }
public void test() { try { correlationUid = message . getJMSCorrelationID ( ) ; messageType = message . getJMSType ( ) ; messagePriority = message . getJMSPriority ( ) ; organisationIdentification = message . getStringProperty ( Constants . ORGANISATION_IDENTIFICATION ) ; deviceIdentification = message . getStringProperty ( Constants . DEVICE_IDENTIFICATION ) ; responseMessage = ( ResponseMessage ) message . getObject ( ) ; responseMessageResultType = responseMessage . getResult ( ) ; osgpException = responseMessage . getOsgpException ( ) ; } catch ( final JMSException e ) { LOGGER . error ( "UNRECOVERABLE ERROR, unable to read ObjectMessage instance, giving up." , e ) ; LOGGER . debug ( "correlationUid: {}" , correlationUid ) ; LOGGER . debug ( "messageType: {}" , messageType ) ; LOGGER . debug ( "messagePriority: {}" , messagePriority ) ; LOGGER . debug ( "organisationIdentification: {}" , organisationIdentification ) ; LOGGER . debug ( "deviceIdentification: {}" , deviceIdentification ) ; LOGGER . debug ( "responseMessageResultType: {}" , responseMessageResultType ) ; LOGGER . debug ( "deviceIdentification: {}" , deviceIdentification ) ; LOGGER . debug ( "osgpException" , osgpException ) ; return ; } }
public void test() { try { correlationUid = message . getJMSCorrelationID ( ) ; messageType = message . getJMSType ( ) ; messagePriority = message . getJMSPriority ( ) ; organisationIdentification = message . getStringProperty ( Constants . ORGANISATION_IDENTIFICATION ) ; deviceIdentification = message . getStringProperty ( Constants . DEVICE_IDENTIFICATION ) ; responseMessage = ( ResponseMessage ) message . getObject ( ) ; responseMessageResultType = responseMessage . getResult ( ) ; osgpException = responseMessage . getOsgpException ( ) ; } catch ( final JMSException e ) { LOGGER . error ( "UNRECOVERABLE ERROR, unable to read ObjectMessage instance, giving up." , e ) ; LOGGER . debug ( "correlationUid: {}" , correlationUid ) ; LOGGER . debug ( "messageType: {}" , messageType ) ; LOGGER . debug ( "messagePriority: {}" , messagePriority ) ; LOGGER . debug ( "organisationIdentification: {}" , organisationIdentification ) ; LOGGER . debug ( "deviceIdentification: {}" , deviceIdentification ) ; LOGGER . debug ( "responseMessageResultType: {}" , responseMessageResultType ) ; LOGGER . debug ( "deviceIdentification: {}" , deviceIdentification ) ; LOGGER . debug ( "osgpException" , osgpException ) ; return ; } }
public void test() { try { correlationUid = message . getJMSCorrelationID ( ) ; messageType = message . getJMSType ( ) ; messagePriority = message . getJMSPriority ( ) ; organisationIdentification = message . getStringProperty ( Constants . ORGANISATION_IDENTIFICATION ) ; deviceIdentification = message . getStringProperty ( Constants . DEVICE_IDENTIFICATION ) ; responseMessage = ( ResponseMessage ) message . getObject ( ) ; responseMessageResultType = responseMessage . getResult ( ) ; osgpException = responseMessage . getOsgpException ( ) ; } catch ( final JMSException e ) { LOGGER . error ( "UNRECOVERABLE ERROR, unable to read ObjectMessage instance, giving up." , e ) ; LOGGER . debug ( "correlationUid: {}" , correlationUid ) ; LOGGER . debug ( "messageType: {}" , messageType ) ; LOGGER . debug ( "messagePriority: {}" , messagePriority ) ; LOGGER . debug ( "organisationIdentification: {}" , organisationIdentification ) ; LOGGER . debug ( "deviceIdentification: {}" , deviceIdentification ) ; LOGGER . debug ( "responseMessageResultType: {}" , responseMessageResultType ) ; LOGGER . debug ( "deviceIdentification: {}" , deviceIdentification ) ; LOGGER . debug ( "osgpException" , osgpException ) ; return ; } }
public void test() { try { correlationUid = message . getJMSCorrelationID ( ) ; messageType = message . getJMSType ( ) ; messagePriority = message . getJMSPriority ( ) ; organisationIdentification = message . getStringProperty ( Constants . ORGANISATION_IDENTIFICATION ) ; deviceIdentification = message . getStringProperty ( Constants . DEVICE_IDENTIFICATION ) ; responseMessage = ( ResponseMessage ) message . getObject ( ) ; responseMessageResultType = responseMessage . getResult ( ) ; osgpException = responseMessage . getOsgpException ( ) ; } catch ( final JMSException e ) { LOGGER . error ( "UNRECOVERABLE ERROR, unable to read ObjectMessage instance, giving up." , e ) ; LOGGER . debug ( "correlationUid: {}" , correlationUid ) ; LOGGER . debug ( "messageType: {}" , messageType ) ; LOGGER . debug ( "messagePriority: {}" , messagePriority ) ; LOGGER . debug ( "organisationIdentification: {}" , organisationIdentification ) ; LOGGER . debug ( "deviceIdentification: {}" , deviceIdentification ) ; LOGGER . debug ( "responseMessageResultType: {}" , responseMessageResultType ) ; LOGGER . debug ( "deviceIdentification: {}" , deviceIdentification ) ; LOGGER . debug ( "osgpException" , osgpException ) ; return ; } }
public SysExportNotiz findById ( sernet . gs . reveng . SysExportNotizId id ) { log . debug ( "getting SysExportNotiz instance with id: " + id ) ; code_block = TryStatement ;  }
public void test() { if ( instance == null ) { log . debug ( "get successful, no instance found" ) ; } else { log . debug ( "get successful, instance found" ) ; } }
public void test() { if ( instance == null ) { log . debug ( "get successful, no instance found" ) ; } else { log . debug ( "get successful, instance found" ) ; } }
public void test() { try { SysExportNotiz instance = ( SysExportNotiz ) sessionFactory . getCurrentSession ( ) . get ( "sernet.gs.reveng.SysExportNotiz" , id ) ; code_block = IfStatement ; return instance ; } catch ( RuntimeException re ) { log . error ( "get failed" , re ) ; throw re ; } }
public void test() { -> { LOG . info ( "Exiting process now." ) ; System . exit ( 1 ) ; } }
public void test() { if ( SRV_LOG . isDebugEnabled ( ) ) { SRV_LOG . debug ( "mysql check, ip is marked as to skip health check, ip: {}" , ip . getIp ( ) ) ; } }
public void test() { if ( ! ip . markChecking ( ) ) { SRV_LOG . warn ( "mysql check started before last one finished, service: {}:{}:{}" , task . getCluster ( ) . getService ( ) . getName ( ) , task . getCluster ( ) . getName ( ) , ip . getIp ( ) ) ; healthCheckCommon . reEvaluateCheckRT ( task . getCheckRtNormalized ( ) * 2 , task , switchDomain . getMysqlHealthParams ( ) ) ; continue ; } }
public void agentLost ( Protos . AgentID agentId ) { LOG . warn ( "Lost a agent {}" , agentId ) ; agentAndRackManager . agentLost ( agentId ) ; }
public void test() { if ( channel == null ) { logger . debug ( "channel is null" ) ; return null ; } }
public void test() { if ( channel == null ) { logger . debug ( "channel is null" ) ; return null ; } }
public void test() { if ( channel == null ) { logger . debug ( "channel is null" ) ; return null ; } }
public void test() { try { String response = HttpJsonHelper . requestString ( apiEndPoint + "/internal/sso/server/" + token , "GET" , null , Pair . of ( "clienturl" , URLEncoder . encode ( apiEndPoint , "UTF-8" ) ) ) ; JsonValue value = JsonHelper . parseJson ( response ) ; return new UserImpl ( value . getElement ( "name" ) . getStringValue ( ) , value . getElement ( "id" ) . getStringValue ( ) , value . getElement ( "token" ) . getStringValue ( ) , Collections . < String > emptySet ( ) , value . getElement ( "temporary" ) . getBooleanValue ( ) ) ; } catch ( ForbiddenException | UnauthorizedException | ServerException un ) { return null ; } catch ( ConflictException | NotFoundException | IOException | JsonParseException e ) { LOG . warn ( e . getLocalizedMessage ( ) ) ; throw new ServletException ( e . getMessage ( ) , e ) ; } }
public void test() { if ( loginConfig != null ) { loginConfig . setAuthMethod ( "KEYCLOAK-SAML" ) ; } else { log . warn ( "Failed to set up KEYCLOAK-SAML auth method for WAR: " + deploymentUnit . getName ( ) + " (loginConfig == null)" ) ; } }
@ ApiOperation ( value = "get history by profile's id" ) @ GetMapping ( CommonConstants . PATH_LOGBOOK ) public LogbookOperationsResponseDto findHistoryById ( final @ PathVariable String id ) { LOGGER . debug ( "get logbook for profile with id :{}" , id ) ; ParameterChecker . checkParameter ( "The Identifier is a mandatory parameter: " , id ) ; return service . findHistoryById ( buildUiHttpContext ( ) , id ) ; }
protected void deleteTestTable ( String tableName ) { LOG . trace ( "Removing table " + tableName + "." ) ; KuduClient client = ikc . getClient ( ) ; code_block = TryStatement ;  LOG . trace ( "Table " + tableName + " removed." ) ; }
public void test() { try { client . deleteTable ( tableName ) ; } catch ( Exception e ) { LOG . error ( e . getMessage ( ) , e ) ; } }
@ Override public Set < String > queryNames ( ) { long start = System . currentTimeMillis ( ) ; Set < String > names = withAllReams ( realm code_block = LoopStatement ; ) . collect ( Collectors . toSet ( ) ) ; LOGGER . debug ( type . getSimpleName ( ) + "-queryNames took " + ( System . currentTimeMillis ( ) - start ) + "ms" ) ; return names ; }
private void createOrUpdateIndexerSearchEnties ( SearchCacheEntry searchCacheEntry ) { Stopwatch stopwatch = Stopwatch . createStarted ( ) ; int countEntities = 0 ; code_block = ForStatement ; logger . debug ( LoggingMarkers . PERFORMANCE , "Saving {} indexer search entities took {}ms" , countEntities , stopwatch . elapsed ( TimeUnit . MILLISECONDS ) ) ; }
public void test() { try { final Collection < ValidationResult > instanceResults = instance . validate ( context ) ; code_block = IfStatement ; } catch ( final Exception e ) { final ComponentLog logger = getLogger ( ) ; final String message = "Unable to validate the script Processor: " + e ; logger . error ( message , e ) ; final Collection < ValidationResult > results = new HashSet < > ( ) ; results . add ( new ValidationResult . Builder ( ) . subject ( "Validation" ) . valid ( false ) . explanation ( "An error occurred calling validate in the configured script Processor." ) . input ( context . getProperty ( ScriptingComponentUtils . SCRIPT_FILE ) . getValue ( ) ) . build ( ) ) ; return results ; } }
public void test() { while ( budget . timeLeft ( ) >= 0 && ! isUserActive ( foundUser ) ) { Thread . sleep ( 1000 ) ; log . info ( "Checking if user {} is active: {}" , name , foundUser ) ; foundUser = client . withName ( name ) . get ( ) ; } }
public void test() { try { localhost = InetAddress . getLocalHost ( ) . getCanonicalHostName ( ) ; } catch ( UnknownHostException e ) { LOG . warn ( "Unable to determine local hostname " + "-falling back to \"" + LOCALHOST + "\"" , e ) ; localhost = LOCALHOST ; } }
public void test() { if ( LOGGER . isTraceEnabled ( ) ) { LOGGER . trace ( "Received initial result partition read request for JobId: " + jobId + " partition: " + partition + " on channel: " + ccb ) ; } }
public void test() { try { partitionManager . initializeResultPartitionReader ( jobId , rsId , partition , noc ) ; } catch ( HyracksException e ) { LOGGER . warn ( "Failed to initialize result partition reader" , e ) ; noc . abort ( AbstractChannelWriteInterface . REMOTE_ERROR_CODE ) ; } }
@ Override String createOid ( String ... keys ) { String oid = "IG_" ; String crfName = keys [ 0 ] ; String itemGroupLabel = keys [ 1 ] ; logger . debug ( crfName ) ; logger . debug ( itemGroupLabel ) ; crfName = truncateToXChars ( capitalize ( stripNonAlphaNumeric ( crfName ) ) , 5 ) ; itemGroupLabel = truncateToXChars ( capitalize ( stripNonAlphaNumeric ( itemGroupLabel ) ) , 26 ) ; oid = oid + crfName + "_" + itemGroupLabel ; code_block = IfStatement ; logger . debug ( "OID : " + oid ) ; return oid ; }
@ Override String createOid ( String ... keys ) { String oid = "IG_" ; String crfName = keys [ 0 ] ; String itemGroupLabel = keys [ 1 ] ; logger . debug ( crfName ) ; logger . debug ( itemGroupLabel ) ; crfName = truncateToXChars ( capitalize ( stripNonAlphaNumeric ( crfName ) ) , 5 ) ; itemGroupLabel = truncateToXChars ( capitalize ( stripNonAlphaNumeric ( itemGroupLabel ) ) , 26 ) ; oid = oid + crfName + "_" + itemGroupLabel ; code_block = IfStatement ; logger . debug ( "OID : " + oid ) ; return oid ; }
@ Override String createOid ( String ... keys ) { String oid = "IG_" ; String crfName = keys [ 0 ] ; String itemGroupLabel = keys [ 1 ] ; logger . debug ( crfName ) ; logger . debug ( itemGroupLabel ) ; crfName = truncateToXChars ( capitalize ( stripNonAlphaNumeric ( crfName ) ) , 5 ) ; itemGroupLabel = truncateToXChars ( capitalize ( stripNonAlphaNumeric ( itemGroupLabel ) ) , 26 ) ; oid = oid + crfName + "_" + itemGroupLabel ; code_block = IfStatement ; logger . debug ( "OID : " + oid ) ; return oid ; }
public void test() { if ( timeUnit == null || ! SUPPORTED_TIME_UNITS . contains ( timeUnit ) ) { logger . debug ( "Given time unit {} is not supported. Will keep current configuration." , timeUnit ) ; return false ; } }
public void test() { try { configDescriptionValidator . validate ( config , new URI ( CONFIG_DESC_URI_KEY ) ) ; } catch ( URISyntaxException | ConfigValidationException e ) { logger . debug ( "Validation of new configuration values failed. Will keep current configuration." , e ) ; return false ; } }
public void test() { for ( String h : headers . getRequestHeader ( key ) ) { log . debug ( "  " + h ) ; } }
public void test() { for ( Commit commit : list ) { LOG . info ( commit . toString ( ) ) ; } }
private void writeSessionIdHit ( SSL2ServerHelloMessage message ) { appendByte ( message . getSessionIdHit ( ) . getValue ( ) ) ; LOGGER . debug ( "SessionIdHit: " + message . getSessionIdHit ( ) . getValue ( ) ) ; }
public void test() { try { EntityParam recipient = new EntityParam ( entityId ) ; String entityName = getEntityLabel ( recipient ) ; Map < String , String > params = new HashMap < > ( ) ; params . put ( UserNotificationTemplateDef . USER , entityName == null ? "" : entityName ) ; notificationProducer . sendNotification ( recipient , templateId , params , cfg . getDefaultLocale ( ) . toString ( ) , null , false ) ; } catch ( Exception e ) { log . warn ( "Unable to send notification using template " + templateId + " to entity " + entityId , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "BucketRegionQueue: mark event {} as possible duplicate due to" + " change of primary bucket." , object ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "BucketRegionQueue: mark event {} as possible duplicate due to" + " change of primary bucket." , object ) ; } }
public void test() { try { list = this . getIdeaDAO ( ) . searchIdea ( instanceCode , status , text , category , order ) ; } catch ( Throwable t ) { _logger . error ( "Error in searchIdeas" , t ) ; throw new ApsSystemException ( "Error in searchIdeas" , t ) ; } }
public void test() { -> { LOG . warn ( "Failed to get bundle for topic - [{}] {}" , topicName , ex . getMessage ( ) ) ; return null ; } }
public void test() { -> { LOG . warn ( "Failed to get bundle for topic - [{}] {}" , topicName , ex . getMessage ( ) ) ; return null ; } }
public void test() { try { EndpointBuilder instance ; code_block = IfStatement ; return Optional . of ( instance ) ; } catch ( CitrusRuntimeException e ) { LOG . warn ( String . format ( "Failed to resolve endpoint builder from resource '%s/%s'" , RESOURCE_PATH , builder ) ) ; } }
public void test() { try { String canonicalURL = _portal . getCanonicalURL ( _getCompleteURL ( themeDisplay ) , themeDisplay , layout , false , false ) ; LayoutSEOLink layoutSEOLink = _layoutSEOLinkManager . getCanonicalLayoutSEOLink ( layout , locale , canonicalURL , _portal . getAlternateURLs ( canonicalURL , themeDisplay , layout ) ) ; return layoutSEOLink . getHref ( ) ; } catch ( PortalException portalException ) { _log . error ( portalException , portalException ) ; return StringPool . BLANK ; } }
public void test() { try { String authString = clientId . concat ( ":" ) . concat ( oauth2ClientSecret ) ; byte [ ] authEncBytes = Base64 . encodeBase64 ( authString . getBytes ( ) ) ; clientCredentials = "Basic " + new String ( authEncBytes ) ; } catch ( Exception exception ) { log . error ( "Exception in getClientAuthorization: " + exception . getMessage ( ) ) ; return response ( false , "Client Validation Failed!!!" ) ; } }
public void test() { try { String url = System . getenv ( DOMAIN_URL ) + "/oauth/token" ; Map < String , String > headers = Maps . newHashMap ( ) ; headers . put ( HttpHeaders . CONTENT_TYPE , MediaType . APPLICATION_FORM_URLENCODED_VALUE ) ; String clientCredentials = null ; code_block = TryStatement ;  headers . put ( HttpHeaders . AUTHORIZATION , clientCredentials ) ; String accessToken = doHttpPost ( url , requestBodyUrl , headers ) ; accessTokenDetails = mapper . readValue ( accessToken , new TypeReference < HashMap < String , Object > > ( ) code_block = "" ; ) ; code_block = IfStatement ; } catch ( Exception exception ) { log . error ( "Exception in loginProxy: " + exception . getMessage ( ) ) ; return response ( false , "Unexpected Error Occured!!!" ) ; } }
public void test() { if ( offHeapQueue == null ) { int remaining = inMemoryQueue . remainingCapacity ( ) ; LOG . trace ( "Checked if full and remaining capacity is {}" , remaining ) ; return remaining <= 0 ; } }
public void test() { try { Thread . sleep ( 1000 ) ; } catch ( InterruptedException e ) { logger . warn ( "Thread was interrupted" ) ; } }
private void waitUntilServerIsInitialized ( ) { int i = 0 ; code_block = ForStatement ; logger . error ( "Can't recover from the Journal - " + "the server hasn't initialized after " + i + " seconds." ) ; shutdown = true ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Checking sip Servlet Mapping for following request : " + sipServletRequest ) ; } }
public void test() { if ( sipServletMapping . getMatchingRule ( ) . matches ( sipServletRequest ) ) { return sipServletMapping ; } else { logger . debug ( "Following mapping rule didn't match : servletName => " + sipServletMapping . getServletName ( ) + " | expression = " + sipServletMapping . getMatchingRule ( ) . getExpression ( ) ) ; } }
@ Override public ConnectionTestResult runTests ( final List < TestType > testTypes ) throws JargonException { log . info ( "runTests{}" ) ; code_block = IfStatement ; ConnectionTestResult testResult = new ConnectionTestResult ( ) ; testResult . setConfiguration ( connectionTesterConfiguration ) ; testResult . setIrodsAccount ( getIrodsAccount ( ) ) ; code_block = ForStatement ; return testResult ; }
public void test() { try { Log . debug ( "JDBCAuthProvider: Automatically creating new user account to " + username ) ; UserManager . getUserProvider ( ) . createUser ( username , StringUtils . randomString ( 8 ) , null , null ) ; } catch ( UserAlreadyExistsException uaee ) { } }
public void test() { try { final List vStatusList = appConfigValuesService . getConfigValuesByModuleAndKey ( "EGF" , "APPROVEDVOUCHERSTATUS" ) ; code_block = IfStatement ; else throw new ApplicationRuntimeException ( "APPROVEDVOUCHERSTATUS" + MISSINGMSG ) ; createVoucher . createVoucherFromPreApprovedVoucher ( vouhcerheaderid , voucherStatus ) ; } catch ( final ApplicationRuntimeException e ) { LOGGER . error ( e . getMessage ( ) ) ; throw new ApplicationRuntimeException ( e . getMessage ( ) ) ; } }
public void test() { try { serverSocket = new ServerSocket ( seyrenConfig . getGraphiteCarbonPicklePort ( ) ) ; serverSocket . setReuseAddress ( true ) ; serverSocket . setReceiveBufferSize ( 1024 * 1024 ) ; code_block = WhileStatement ; } catch ( IOException e ) { LOGGER . warn ( "Error: " , e ) ; } finally { code_block = IfStatement ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Condition " + statement . getCondition ( ) + " evaluated to " + result + " to " + context . get ( ContextKey . entityId . name ( ) ) + " in " + context . get ( ContextKey . groupName . name ( ) ) ) ; } }
public void test() { try { quantumClock . stopDeamon ( ) ; registryUpdateDeamon . stopDeamon ( ) ; dataTransferManagerDTP . stopDataTransferServer ( ) ; ManagementUtil . unregisterMBean ( "Container" ) ; super . unregister ( ) ; log . info ( "Container succesfully stopped!" ) ; executor . terminate ( ) ; } catch ( RemoteException e ) { log . debug ( "Stop Container" , e ) ; throw new ServerException ( "Cannot stop container" , e ) ; } }
public void test() { try { quantumClock . stopDeamon ( ) ; registryUpdateDeamon . stopDeamon ( ) ; dataTransferManagerDTP . stopDataTransferServer ( ) ; ManagementUtil . unregisterMBean ( "Container" ) ; super . unregister ( ) ; log . info ( "Container succesfully stopped!" ) ; executor . terminate ( ) ; } catch ( RemoteException e ) { log . debug ( "Stop Container" , e ) ; throw new ServerException ( "Cannot stop container" , e ) ; } }
public void test() { try { final CliFrontend cli = new CliFrontend ( configuration , customCommandLines ) ; SecurityUtils . install ( new SecurityConfiguration ( cli . configuration ) ) ; retCode = SecurityUtils . getInstalledContext ( ) . runSecured ( ( ) -> cli . parseAndRun ( args ) ) ; } catch ( Throwable t ) { final Throwable strippedThrowable = ExceptionUtils . stripException ( t , UndeclaredThrowableException . class ) ; LOG . error ( "Fatal error while running command line interface." , strippedThrowable ) ; strippedThrowable . printStackTrace ( ) ; } finally { System . exit ( retCode ) ; } }
public void test() { try { CLIENT . getService ( SyncopeService . class ) ; LOG . info ( getName ( ) + " completed successfully!" ) ; } catch ( Exception e ) { LOG . error ( getName ( ) + " did not complete" , e ) ; } }
public void test() { try { propValue = getPropAccessor ( ) . getProperty ( NhincConstants . GATEWAY_PROPERTY_FILE , propertyName ) ; } catch ( PropertyAccessException ex ) { LOG . warn ( "Unable to read property: {})" , propertyName , ex ) ; } }
public void test() { if ( eofSenderFuture == null ) { log . error ( "EOF ACK received but EOF not sent" ) ; } else { log . info ( "CFDP received EOF ACK" ) ; eofSenderFuture . cancel ( true ) ; } }
public void test() { if ( eofSenderFuture == null ) { log . error ( "EOF ACK received but EOF not sent" ) ; } else { log . info ( "CFDP received EOF ACK" ) ; eofSenderFuture . cancel ( true ) ; } }
public void test() { try { String [ ] resultFieldsArray = resultFields . getItem ( ) . toArray ( EMPTY_STRING_ARRAY ) ; org . fcrepo . server . search . FieldSearchResult result = m_access . findObjects ( context , resultFieldsArray , maxResults . intValue ( ) , TypeUtility . convertGenFieldSearchQueryToFieldSearchQuery ( query ) ) ; return TypeUtility . convertFieldSearchResultToGenFieldSearchResult ( result ) ; } catch ( Throwable th ) { LOG . error ( "Error finding objects" , th ) ; throw CXFUtility . getFault ( th ) ; } }
public void test() { try { onFinish ( ) ; } catch ( final Exception e ) { logger . error ( "Failed to perform tasks when enumerator was finished" , e ) ; } }
public void test() { if ( encoding == null ) { encoding = Charset . defaultCharset ( ) . name ( ) ; configuration . setEncoding ( encoding ) ; LOG . debug ( "{}: No encoding parameter using default charset: {}" , type , encoding ) ; } }
public void test() { if ( nextToProcess == - 1 ) { nextToProcess = seqNum ; LOG . debug ( "{}: got seq={} (first request), set nextToProcess in {}" , requests . getName ( ) , seqNum , this ) ; } else { LOG . debug ( "{}: got seq={} in {}" , requests . getName ( ) , seqNum , this ) ; } }
public void test() { if ( nextToProcess == - 1 ) { nextToProcess = seqNum ; LOG . debug ( "{}: got seq={} (first request), set nextToProcess in {}" , requests . getName ( ) , seqNum , this ) ; } else { LOG . debug ( "{}: got seq={} in {}" , requests . getName ( ) , seqNum , this ) ; } }
public void test() { try { WatchdogServiceModVerIdLookup lookup = watchdogModVerIdLookupRepository . findOneByDistributionId ( distributionId ) ; String serviceModelVersionId = "" ; code_block = IfStatement ; logger . debug ( "Executed RequestDB getWatchdogServiceModVerIdLookup with distributionId: {} " + "and serviceModelVersionId: {}" , distributionId , serviceModelVersionId ) ; logger . debug ( "ASDC Notification ServiceModelInvariantUUID : {}" , serviceModelInvariantUUID ) ; code_block = IfStatement ; AAIResourceUri aaiUri = AAIUriFactory . createResourceUri ( AAIFluentTypeBuilder . serviceDesignAndCreation ( ) . model ( serviceModelInvariantUUID ) . modelVer ( serviceModelVersionId ) ) ; aaiUri . depth ( Depth . ZERO ) ; logger . debug ( "Target A&AI Resource URI: {}" , aaiUri . build ( ) . toString ( ) ) ; Map < String , String > payload = new HashMap < > ( ) ; payload . put ( "distribution-status" , distributionStatus ) ; getAaiClient ( ) . update ( aaiUri , payload ) ; logger . debug ( "A&AI UPDATE MODEL Version is success!" ) ; } catch ( Exception e ) { logger . debug ( "Exception occurred on executePatchAAI : {}" , e . getMessage ( ) ) ; logger . error ( "Exception occurred" , e ) ; throw new Exception ( e ) ; } }
public void test() { try { WatchdogServiceModVerIdLookup lookup = watchdogModVerIdLookupRepository . findOneByDistributionId ( distributionId ) ; String serviceModelVersionId = "" ; code_block = IfStatement ; logger . debug ( "Executed RequestDB getWatchdogServiceModVerIdLookup with distributionId: {} " + "and serviceModelVersionId: {}" , distributionId , serviceModelVersionId ) ; logger . debug ( "ASDC Notification ServiceModelInvariantUUID : {}" , serviceModelInvariantUUID ) ; code_block = IfStatement ; AAIResourceUri aaiUri = AAIUriFactory . createResourceUri ( AAIFluentTypeBuilder . serviceDesignAndCreation ( ) . model ( serviceModelInvariantUUID ) . modelVer ( serviceModelVersionId ) ) ; aaiUri . depth ( Depth . ZERO ) ; logger . debug ( "Target A&AI Resource URI: {}" , aaiUri . build ( ) . toString ( ) ) ; Map < String , String > payload = new HashMap < > ( ) ; payload . put ( "distribution-status" , distributionStatus ) ; getAaiClient ( ) . update ( aaiUri , payload ) ; logger . debug ( "A&AI UPDATE MODEL Version is success!" ) ; } catch ( Exception e ) { logger . debug ( "Exception occurred on executePatchAAI : {}" , e . getMessage ( ) ) ; logger . error ( "Exception occurred" , e ) ; throw new Exception ( e ) ; } }
public void test() { if ( serviceModelInvariantUUID == null || "" . equals ( serviceModelVersionId ) ) { String error = "No Service found with serviceModelInvariantUUID: " + serviceModelInvariantUUID ; logger . debug ( error ) ; throw new Exception ( error ) ; } }
public void test() { try { WatchdogServiceModVerIdLookup lookup = watchdogModVerIdLookupRepository . findOneByDistributionId ( distributionId ) ; String serviceModelVersionId = "" ; code_block = IfStatement ; logger . debug ( "Executed RequestDB getWatchdogServiceModVerIdLookup with distributionId: {} " + "and serviceModelVersionId: {}" , distributionId , serviceModelVersionId ) ; logger . debug ( "ASDC Notification ServiceModelInvariantUUID : {}" , serviceModelInvariantUUID ) ; code_block = IfStatement ; AAIResourceUri aaiUri = AAIUriFactory . createResourceUri ( AAIFluentTypeBuilder . serviceDesignAndCreation ( ) . model ( serviceModelInvariantUUID ) . modelVer ( serviceModelVersionId ) ) ; aaiUri . depth ( Depth . ZERO ) ; logger . debug ( "Target A&AI Resource URI: {}" , aaiUri . build ( ) . toString ( ) ) ; Map < String , String > payload = new HashMap < > ( ) ; payload . put ( "distribution-status" , distributionStatus ) ; getAaiClient ( ) . update ( aaiUri , payload ) ; logger . debug ( "A&AI UPDATE MODEL Version is success!" ) ; } catch ( Exception e ) { logger . debug ( "Exception occurred on executePatchAAI : {}" , e . getMessage ( ) ) ; logger . error ( "Exception occurred" , e ) ; throw new Exception ( e ) ; } }
public void test() { try { WatchdogServiceModVerIdLookup lookup = watchdogModVerIdLookupRepository . findOneByDistributionId ( distributionId ) ; String serviceModelVersionId = "" ; code_block = IfStatement ; logger . debug ( "Executed RequestDB getWatchdogServiceModVerIdLookup with distributionId: {} " + "and serviceModelVersionId: {}" , distributionId , serviceModelVersionId ) ; logger . debug ( "ASDC Notification ServiceModelInvariantUUID : {}" , serviceModelInvariantUUID ) ; code_block = IfStatement ; AAIResourceUri aaiUri = AAIUriFactory . createResourceUri ( AAIFluentTypeBuilder . serviceDesignAndCreation ( ) . model ( serviceModelInvariantUUID ) . modelVer ( serviceModelVersionId ) ) ; aaiUri . depth ( Depth . ZERO ) ; logger . debug ( "Target A&AI Resource URI: {}" , aaiUri . build ( ) . toString ( ) ) ; Map < String , String > payload = new HashMap < > ( ) ; payload . put ( "distribution-status" , distributionStatus ) ; getAaiClient ( ) . update ( aaiUri , payload ) ; logger . debug ( "A&AI UPDATE MODEL Version is success!" ) ; } catch ( Exception e ) { logger . debug ( "Exception occurred on executePatchAAI : {}" , e . getMessage ( ) ) ; logger . error ( "Exception occurred" , e ) ; throw new Exception ( e ) ; } }
public void test() { try { WatchdogServiceModVerIdLookup lookup = watchdogModVerIdLookupRepository . findOneByDistributionId ( distributionId ) ; String serviceModelVersionId = "" ; code_block = IfStatement ; logger . debug ( "Executed RequestDB getWatchdogServiceModVerIdLookup with distributionId: {} " + "and serviceModelVersionId: {}" , distributionId , serviceModelVersionId ) ; logger . debug ( "ASDC Notification ServiceModelInvariantUUID : {}" , serviceModelInvariantUUID ) ; code_block = IfStatement ; AAIResourceUri aaiUri = AAIUriFactory . createResourceUri ( AAIFluentTypeBuilder . serviceDesignAndCreation ( ) . model ( serviceModelInvariantUUID ) . modelVer ( serviceModelVersionId ) ) ; aaiUri . depth ( Depth . ZERO ) ; logger . debug ( "Target A&AI Resource URI: {}" , aaiUri . build ( ) . toString ( ) ) ; Map < String , String > payload = new HashMap < > ( ) ; payload . put ( "distribution-status" , distributionStatus ) ; getAaiClient ( ) . update ( aaiUri , payload ) ; logger . debug ( "A&AI UPDATE MODEL Version is success!" ) ; } catch ( Exception e ) { logger . debug ( "Exception occurred on executePatchAAI : {}" , e . getMessage ( ) ) ; logger . error ( "Exception occurred" , e ) ; throw new Exception ( e ) ; } }
public void test() { try { WatchdogServiceModVerIdLookup lookup = watchdogModVerIdLookupRepository . findOneByDistributionId ( distributionId ) ; String serviceModelVersionId = "" ; code_block = IfStatement ; logger . debug ( "Executed RequestDB getWatchdogServiceModVerIdLookup with distributionId: {} " + "and serviceModelVersionId: {}" , distributionId , serviceModelVersionId ) ; logger . debug ( "ASDC Notification ServiceModelInvariantUUID : {}" , serviceModelInvariantUUID ) ; code_block = IfStatement ; AAIResourceUri aaiUri = AAIUriFactory . createResourceUri ( AAIFluentTypeBuilder . serviceDesignAndCreation ( ) . model ( serviceModelInvariantUUID ) . modelVer ( serviceModelVersionId ) ) ; aaiUri . depth ( Depth . ZERO ) ; logger . debug ( "Target A&AI Resource URI: {}" , aaiUri . build ( ) . toString ( ) ) ; Map < String , String > payload = new HashMap < > ( ) ; payload . put ( "distribution-status" , distributionStatus ) ; getAaiClient ( ) . update ( aaiUri , payload ) ; logger . debug ( "A&AI UPDATE MODEL Version is success!" ) ; } catch ( Exception e ) { logger . debug ( "Exception occurred on executePatchAAI : {}" , e . getMessage ( ) ) ; logger . error ( "Exception occurred" , e ) ; throw new Exception ( e ) ; } }
public void test() { { long ts1 = System . currentTimeMillis ( ) ; logger . info ( "event=lp_receive vto=120 wt=20" ) ; List < Message > messages = cqs1 . receiveMessage ( receiveMessageRequest ) . getMessages ( ) ; assertTrue ( "Expected 1 message, instead found " + messages . size ( ) , messages . size ( ) == 1 ) ; long ts2 = System . currentTimeMillis ( ) ; logger . info ( "event=message_found duration=" + ( ts2 - ts1 ) ) ; } }
public void test() { try { final String queueUrl = getQueueUrl ( 1 , USR . USER1 ) ; Thread . sleep ( 1000 ) ; final ReceiveMessageRequest receiveMessageRequest = new ReceiveMessageRequest ( queueUrl ) ; receiveMessageRequest . setVisibilityTimeout ( 120 ) ; receiveMessageRequest . setMaxNumberOfMessages ( 10 ) ; receiveMessageRequest . setWaitTimeSeconds ( 20 ) ; ( new Thread ( ) code_block = "" ; ) . start ( ) ; Thread . sleep ( 100 ) ; logger . info ( "event=send_message queue_url=" + queueUrl ) ; cqs1 . sendMessage ( new SendMessageRequest ( queueUrl , "This is my message text. " + ( new Random ( ) ) . nextInt ( ) ) ) ; Thread . sleep ( 100 ) ; List < Message > messages = null ; long ts = System . currentTimeMillis ( ) ; code_block = ForStatement ; code_block = ForStatement ; fail ( "message not found any more" ) ; } catch ( AmazonServiceException ase ) { logger . error ( "test failed" , ase ) ; fail ( ase . getMessage ( ) ) ; } }
public void test() { try { final String queueUrl = getQueueUrl ( 1 , USR . USER1 ) ; Thread . sleep ( 1000 ) ; final ReceiveMessageRequest receiveMessageRequest = new ReceiveMessageRequest ( queueUrl ) ; receiveMessageRequest . setVisibilityTimeout ( 120 ) ; receiveMessageRequest . setMaxNumberOfMessages ( 10 ) ; receiveMessageRequest . setWaitTimeSeconds ( 20 ) ; ( new Thread ( ) code_block = "" ; ) . start ( ) ; Thread . sleep ( 100 ) ; logger . info ( "event=send_message queue_url=" + queueUrl ) ; cqs1 . sendMessage ( new SendMessageRequest ( queueUrl , "This is my message text. " + ( new Random ( ) ) . nextInt ( ) ) ) ; Thread . sleep ( 100 ) ; List < Message > messages = null ; long ts = System . currentTimeMillis ( ) ; code_block = ForStatement ; code_block = ForStatement ; fail ( "message not found any more" ) ; } catch ( AmazonServiceException ase ) { logger . error ( "test failed" , ase ) ; fail ( ase . getMessage ( ) ) ; } }
public void test() { if ( member != getDistributionManagerId ( ) ) { String relationship = areInSameZone ( getDistributionManagerId ( ) , member ) ? "" : "not " ; logger . info ( "Member {} is {}equivalent or in the same redundancy zone." , member , relationship ) ; } }
@ Override public RuleResult execute ( Map < String , String > ruleParam , Map < String , String > resourceAttributes ) { logger . debug ( "======== Azure Policy Evaluation Rule started =========" ) ; MDC . put ( "executionId" , ruleParam . get ( "executionId" ) ) ; MDC . put ( "ruleId" , ruleParam . get ( PacmanSdkConstants . RULE_ID ) ) ; String severity = ruleParam . get ( PacmanRuleConstants . SEVERITY ) ; String category = ruleParam . get ( PacmanRuleConstants . CATEGORY ) ; String resourceId = resourceAttributes . get ( PacmanRuleConstants . RESOURCE_ID ) . toLowerCase ( ) ; String pacmanHost = PacmanUtils . getPacmanHost ( PacmanRuleConstants . ES_URI ) ; String policyDefinitionName = ruleParam . get ( "policyDefinitionName" ) ; String azurePolicyEvaluationResultsURl = ruleParam . get ( "azurePolicyEvaluationResults" ) ; Map < String , Object > policyEvaluationResultsMap = new HashMap < > ( ) ; code_block = TryStatement ;  logger . debug ( "======== Azure Policy Evaluation Rule ended=========" ) ; return new RuleResult ( PacmanSdkConstants . STATUS_SUCCESS , PacmanRuleConstants . SUCCESS_MESSAGE ) ; }
public void test() { if ( ! isCompliant == true ) { List < LinkedHashMap < String , Object > > issueList = new ArrayList < > ( ) ; LinkedHashMap < String , Object > issue = new LinkedHashMap < > ( ) ; Annotation annotation = null ; annotation = Annotation . buildAnnotation ( ruleParam , Annotation . Type . ISSUE ) ; annotation . put ( PacmanSdkConstants . DESCRIPTION , policyEvaluationResultsMap . get ( "policyDescription" ) . toString ( ) ) ; annotation . put ( PacmanRuleConstants . SEVERITY , severity ) ; annotation . put ( PacmanRuleConstants . CATEGORY , category ) ; annotation . put ( PacmanRuleConstants . AZURE_SUBSCRIPTION , resourceAttributes . get ( PacmanRuleConstants . AZURE_SUBSCRIPTION ) ) ; annotation . put ( PacmanRuleConstants . AZURE_SUBSCRIPTION_NAME , resourceAttributes . get ( PacmanRuleConstants . AZURE_SUBSCRIPTION_NAME ) ) ; issue . put ( "resourceId" , resourceId ) ; issue . put ( "policyDescription" , policyEvaluationResultsMap . get ( "policyDescription" ) . toString ( ) ) ; issue . put ( "policyName" , policyEvaluationResultsMap . get ( "policyName" ) . toString ( ) ) ; issueList . add ( issue ) ; annotation . put ( PacmanRuleConstants . ISSUE_DETAILS , issueList . toString ( ) ) ; logger . debug ( "======== Azure Policy Evaluation Rule ended with annotation {} : =========" , annotation ) ; return new RuleResult ( PacmanSdkConstants . STATUS_FAILURE , PacmanRuleConstants . FAILURE_MESSAGE , annotation ) ; } }
public void test() { try { policyEvaluationResultsMap = PacmanUtils . getAzurePolicyEvaluationResults ( pacmanHost + azurePolicyEvaluationResultsURl , resourceId , policyDefinitionName ) ; code_block = IfStatement ; } catch ( Exception exception ) { logger . error ( "error: " , exception ) ; throw new RuleExecutionFailedExeption ( exception . getMessage ( ) ) ; } }
@ Override public RuleResult execute ( Map < String , String > ruleParam , Map < String , String > resourceAttributes ) { logger . debug ( "======== Azure Policy Evaluation Rule started =========" ) ; MDC . put ( "executionId" , ruleParam . get ( "executionId" ) ) ; MDC . put ( "ruleId" , ruleParam . get ( PacmanSdkConstants . RULE_ID ) ) ; String severity = ruleParam . get ( PacmanRuleConstants . SEVERITY ) ; String category = ruleParam . get ( PacmanRuleConstants . CATEGORY ) ; String resourceId = resourceAttributes . get ( PacmanRuleConstants . RESOURCE_ID ) . toLowerCase ( ) ; String pacmanHost = PacmanUtils . getPacmanHost ( PacmanRuleConstants . ES_URI ) ; String policyDefinitionName = ruleParam . get ( "policyDefinitionName" ) ; String azurePolicyEvaluationResultsURl = ruleParam . get ( "azurePolicyEvaluationResults" ) ; Map < String , Object > policyEvaluationResultsMap = new HashMap < > ( ) ; code_block = TryStatement ;  logger . debug ( "======== Azure Policy Evaluation Rule ended=========" ) ; return new RuleResult ( PacmanSdkConstants . STATUS_SUCCESS , PacmanRuleConstants . SUCCESS_MESSAGE ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( String . format ( "Invalidating StorageEntry stored at key %s form L2 cache" , key ) ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( String . format ( "Invalidating StorageEntry stored at key %s form L1 cache" , key ) ) ; } }
public void test() { if ( enabled ) { logger . debug ( "LIVE IS STOPPING?!? message=" + finalMessage + " " + enabled ) ; return sendReplicatePacket ( new ReplicationLiveIsStoppingMessage ( finalMessage ) ) ; } }
public void test() { if ( ! DISK_OUT_OF_SPACE . equals ( e . getResult ( ) ) ) { LOG . info ( "Unexpected error during container creation" , e ) ; } }
public void test() { if ( defaultVariant != null ) { getLogger ( ) . info ( "Using default variant [" + defaultVariant . getShortCodeWithCountryAndVariant ( ) + "] for language [" + aJCas . getDocumentLanguage ( ) + "]" ) ; lang = defaultVariant ; } }
@ Override public List < CommunicationChannel > getCommunicationChannelsForUser ( String userId ) throws IOException { LOG . debug ( "Retrieving communication channels for user id " + userId ) ; String url = buildCanvasUrl ( String . format ( "users/%s/communication_channels" , userId ) , emptyMap ( ) ) ; return getListFromCanvas ( url ) ; }
public void test() { try { servletDescriptor . register ( httpService ) ; } catch ( RuntimeException e ) { LOG . error ( "Registration of ServletDescriptor under mountpoint {} fails with unexpected RuntimeException!" , servletDescriptor . getAlias ( ) , e ) ; } catch ( ServletException e ) { LOG . error ( "Unable to mount servlet on mount point '{}', either it was already registered under the same alias or the init method throws an exception" , servletDescriptor . getAlias ( ) , e ) ; } catch ( NamespaceException e ) { LOG . error ( "Unable to mount servlet on mount point '{}', another resource is already bound to this alias" , servletDescriptor . getAlias ( ) , e ) ; } }
public void test() { try { servletDescriptor . register ( httpService ) ; } catch ( RuntimeException e ) { LOG . error ( "Registration of ServletDescriptor under mountpoint {} fails with unexpected RuntimeException!" , servletDescriptor . getAlias ( ) , e ) ; } catch ( ServletException e ) { LOG . error ( "Unable to mount servlet on mount point '{}', either it was already registered under the same alias or the init method throws an exception" , servletDescriptor . getAlias ( ) , e ) ; } catch ( NamespaceException e ) { LOG . error ( "Unable to mount servlet on mount point '{}', another resource is already bound to this alias" , servletDescriptor . getAlias ( ) , e ) ; } }
public void test() { try { servletDescriptor . register ( httpService ) ; } catch ( RuntimeException e ) { LOG . error ( "Registration of ServletDescriptor under mountpoint {} fails with unexpected RuntimeException!" , servletDescriptor . getAlias ( ) , e ) ; } catch ( ServletException e ) { LOG . error ( "Unable to mount servlet on mount point '{}', either it was already registered under the same alias or the init method throws an exception" , servletDescriptor . getAlias ( ) , e ) ; } catch ( NamespaceException e ) { LOG . error ( "Unable to mount servlet on mount point '{}', another resource is already bound to this alias" , servletDescriptor . getAlias ( ) , e ) ; } }
@ Override public GetDataSystemIdentifierDto getData ( final SystemFilterDto systemFilter , final Iec61850Client client , final DeviceConnection connection ) throws NodeException { final int logicalDeviceIndex = systemFilter . getId ( ) ; LOGGER . info ( "Get data called for logical device {}{}" , DEVICE . getDescription ( ) , logicalDeviceIndex ) ; final List < MeasurementDto > measurements = new ArrayList < > ( ) ; code_block = ForStatement ; final List < ProfileDto > profiles = new ArrayList < > ( ) ; code_block = ForStatement ; return new GetDataSystemIdentifierDto ( systemFilter . getId ( ) , systemFilter . getSystemType ( ) , measurements , profiles ) ; }
public void terminate ( ) { goon = false ; logger . info ( "terminating" ) ; code_block = IfStatement ; int svs = 0 ; List < DHTBroadcaster < K > > scopy = null ; code_block = IfStatement ; logger . debug ( "DHTBroadcasters terminated" ) ; long enc = DHTTransport . etime - etime ; long dec = DHTTransport . dtime - dtime ; long encr = DHTTransport . ertime - ertime ; long decr = DHTTransport . drtime - drtime ; long drest = ( encr * dec ) / ( enc + 1 ) ; logger . info ( "DHT time: encode = " + enc + ", decode = " + dec + ", enc raw = " + encr + ", dec raw wait = " + decr + ", dec raw est = " + drest + ", sum est = " + ( enc + dec + encr + drest ) ) ; code_block = IfStatement ; code_block = TryStatement ;  mythread = null ; logger . info ( "terminated" ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . info ( "server+ " + br + " terminated" ) ; } }
public void terminate ( ) { goon = false ; logger . info ( "terminating" ) ; code_block = IfStatement ; int svs = 0 ; List < DHTBroadcaster < K > > scopy = null ; code_block = IfStatement ; logger . debug ( "DHTBroadcasters terminated" ) ; long enc = DHTTransport . etime - etime ; long dec = DHTTransport . dtime - dtime ; long encr = DHTTransport . ertime - ertime ; long decr = DHTTransport . drtime - drtime ; long drest = ( encr * dec ) / ( enc + 1 ) ; logger . info ( "DHT time: encode = " + enc + ", decode = " + dec + ", enc raw = " + encr + ", dec raw wait = " + decr + ", dec raw est = " + drest + ", sum est = " + ( enc + dec + encr + drest ) ) ; code_block = IfStatement ; code_block = TryStatement ;  mythread = null ; logger . info ( "terminated" ) ; }
public void test() { if ( logger . isWarnEnabled ( ) ) { logger . warn ( "server terminated " + mythread ) ; } }
public void test() { if ( CompanyThreadLocal . isDeleteInProcess ( ) ) { _log . debug ( "Skip indexing because company delete is in process" ) ; } else-if ( IndexWriterHelperUtil . isIndexReadOnly ( ) ) { _log . debug ( "Skip indexing because the index is read only" ) ; } }
public void test() { if ( CompanyThreadLocal . isDeleteInProcess ( ) ) { _log . debug ( "Skip indexing because company delete is in process" ) ; } else-if ( IndexWriterHelperUtil . isIndexReadOnly ( ) ) { _log . debug ( "Skip indexing because the index is read only" ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( "Skipping indexing read only index to " + indexer . getClassName ( ) ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "checking if group '" + potentialGroupName + "' is an element which has quality requirements" ) ; } }
private String getDateBeforeDays ( int numberOfDays ) { ZonedDateTime dateBeforeTwoDays = ZonedDateTime . now ( ) . minus ( Duration . ofDays ( numberOfDays ) ) ; String result = DateTimeFormatter . ofPattern ( "yyyy-MM-dd" ) . format ( dateBeforeTwoDays ) ; LOGGER . info ( MessageFormat . format ( Messages . PURGE_DELETE_REQUEST_SPACE_FROM_CONFIGURATION_TABLES , result ) ) ; return result ; }
public void test() { if ( LOG . isDebugEnabled ( ) && ! ( command instanceof WaitIOCommand ) ) { LOG . debug ( "call: thread " + diskId + "'s next IO command is: " + command ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) && ! ( command instanceof WaitIOCommand ) ) { LOG . debug ( "call: thread " + diskId + "'s command " + command + " completed: bytes= " + bytes + ", duration=" + duration + ", " + "bandwidth=" + String . format ( "%.2f" , ( double ) bytes / duration * 1000 / 1024 / 1024 ) + ( ( command instanceof WaitIOCommand ) ? "" : ( ", bandwidth (excluding GC time)=" + String . format ( "%.2f" , ( double ) bytes / ( duration - timeInGC ) * 1000 / 1024 / 1024 ) ) ) ) ; } }
public void test() { if ( LOG . isInfoEnabled ( ) ) { LOG . info ( "call: out-of-core IO thread " + diskId + " terminating!" ) ; } }
protected void onUnexpectedStatusCode ( String urlStr , int statusCode , String contentType , String description ) { logger . warn ( "Skipping URL: {}, StatusCode: {}, {}, {}" , urlStr , statusCode , contentType , description ) ; }
@ Override protected AmazonSQSClient createClient ( final ProcessContext context , final AWSCredentialsProvider credentialsProvider , final ClientConfiguration config ) { getLogger ( ) . info ( "Creating client using aws credentials provider " ) ; return new AmazonSQSClient ( credentialsProvider , config ) ; }
public void test() { try { Set < URI > unassignFrom = new HashSet < URI > ( ) ; unassignFrom . add ( id ) ; _log . info ( "No Errors found proceeding further {}, {}, {}" , new Object [ ] code_block = "" ; ) ; controller . unassignFilePolicy ( filePolicyUri , unassignFrom , task ) ; auditOp ( OperationTypeEnum . UNASSIGN_FILE_POLICY , true , "BEGIN" , fp . getId ( ) . toString ( ) , fp . getFilePolicyName ( ) ) ; } catch ( BadRequestException e ) { op = _dbClient . error ( FilePolicy . class , fp . getId ( ) , task , e ) ; _log . error ( "Error Unassigning File policy {}, {}" , e . getMessage ( ) , e ) ; throw e ; } catch ( Exception e ) { _log . error ( "Error Unassigning Filesystem policy {}, {}" , e . getMessage ( ) , e ) ; throw APIException . badRequests . unableToProcessRequest ( e . getMessage ( ) ) ; } }
private void addKeyFile ( String username , String keyPairName , String filePath ) { ProviderParams params = providerParamsDao . getByUser ( username ) ; LOG . info ( "{}" , params ) ; params . getKeys ( ) . put ( keyPairName , filePath ) ; save ( params ) ; }
@ Test public void testBasicApi ( ) { LogListener lsnr = LogListener . matches ( Pattern . compile ( "a[a-z]+" ) ) . andMatches ( "Exception message." ) . andMatches ( ".java:" ) . build ( ) ; log . registerListener ( lsnr ) ; log . info ( "Something new." ) ; assertFalse ( lsnr . check ( ) ) ; log . error ( "There was an error." , new RuntimeException ( "Exception message." ) ) ; assertTrue ( lsnr . check ( ) ) ; }
@ Test public void testBasicApi ( ) { LogListener lsnr = LogListener . matches ( Pattern . compile ( "a[a-z]+" ) ) . andMatches ( "Exception message." ) . andMatches ( ".java:" ) . build ( ) ; log . registerListener ( lsnr ) ; log . info ( "Something new." ) ; assertFalse ( lsnr . check ( ) ) ; log . error ( "There was an error." , new RuntimeException ( "Exception message." ) ) ; assertTrue ( lsnr . check ( ) ) ; }
public void test() { try { entity = this . createEntityType ( element , entityClass ) ; entity . setEntityDOM ( entityDom ) ; this . fillEntityType ( entity , element ) ; entity . setDefaultLang ( this . getLangManager ( ) . getDefaultLang ( ) . getCode ( ) ) ; _logger . debug ( "Entity Type '{}' defined" , entity . getTypeCode ( ) ) ; } catch ( Throwable t ) { _logger . error ( "Error extracting entity type" , t ) ; throw new ApsSystemException ( "Configuration error of the Entity Type detected" , t ) ; } }
public void test() { try { code_block = IfStatement ; final String json = command . getCommand ( this ) ; logger . debug ( "Sent: {}" , command ) ; websocketEndpoint . sendMessage ( json ) ; } catch ( final BitfinexCommandException e ) { logger . error ( "Got Exception while sending command" , e ) ; } }
public void test() { try { code_block = IfStatement ; final String json = command . getCommand ( this ) ; logger . debug ( "Sent: {}" , command ) ; websocketEndpoint . sendMessage ( json ) ; } catch ( final BitfinexCommandException e ) { logger . error ( "Got Exception while sending command" , e ) ; } }
public void test() { try { PSAgentContext . get ( ) . getMasterClient ( ) . updateClock ( request . getTaskIndex ( ) , request . getMatrixId ( ) , request . getClock ( ) ) ; } catch ( ServiceException e ) { LOG . warn ( "update clock to master failed. task=" + request . getTaskIndex ( ) + ", matrix=" + request . getMatrixId ( ) + ", clock=" + request . getClock ( ) ) ; } }
public void test() { if ( debugEnabled ) { log . debug ( "tryNext({}) starting authentication mechanisms: client={}, server={}" , session , clientMethods , serverMethods ) ; } }
public void test() { if ( debugEnabled ) { log . debug ( "tryNext({}) no initial request sent by method={}" , session , userAuth . getName ( ) ) ; } }
public void test() { if ( debugEnabled ) { log . debug ( "tryNext({}) successfully processed initial buffer by method={}" , session , userAuth . getName ( ) ) ; } }
public void test() { if ( debugEnabled ) { log . debug ( "tryNext({}) exhausted all methods - client={}, server={}" , session , clientMethods , serverMethods ) ; } }
public void test() { if ( debugEnabled ) { log . debug ( "tryNext({}) attempting method={}" , session , method ) ; } }
public void test() { try { String [ ] objArr = child . getTextTrim ( ) . split ( "/" ) ; String objStr = objArr [ objArr . length - 1 ] . toUpperCase ( ) ; BaseObjectType . valueOf ( objStr ) ; myModule . setObjectType ( objStr ) ; } catch ( IllegalArgumentException ex ) { logger . debug ( "ActivityObject not valid" ) ; myModule . setObjectType ( "NOTE" ) ; } }
public void test() { try { File fileToDiscard = new File ( selectedRepository , file . getFileLocation ( ) ) ; FileUtils . forceDelete ( fileToDiscard ) ; deletedFilesParentDirs . add ( fileToDiscard . getParentFile ( ) ) ; } catch ( IOException e1 ) { logger . error ( e1 , e1 ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( exception , exception ) ; } }
public void test() { if ( migrationFolderFile == null ) { LOG . debug ( "Skip migration because migration source folder is not specified or otherwise invalid." ) ; return ; } }
public void test() { if ( fileExt . equalsIgnoreCase ( "json" ) ) { String reportTemplateStr = readFileAndCreateJson ( file ) ; JsonObject reportTemplateJson = jsonParser . parse ( reportTemplateStr ) . getAsJsonObject ( ) ; int reportId = saveTemplateReport ( reportTemplateJson ) ; returnMessage = "Report Template Id created " + reportId ; } else { log . error ( "Invalid Report Template file format. " ) ; throw new InsightsCustomException ( "Invalid Report Template file format." ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception ex ) { log . error ( "Error in Report Template file {} " , ex . getMessage ( ) ) ; throw new InsightsCustomException ( ex . getMessage ( ) ) ; } }
public void test() { if ( potentialModification ) { int transId = msg != null ? msg . getTransactionId ( ) : Integer . MIN_VALUE ; logger . warn ( String . format ( "%s: Unexpected IOException during operation for region: %s key: %s messId: %s" , serverConnection . getName ( ) , serverConnection . getModRegion ( ) , serverConnection . getModKey ( ) , transId ) , e ) ; } else { logger . warn ( String . format ( "%s: Unexpected IOException: " , serverConnection . getName ( ) ) , e ) ; } }
public void test() { if ( potentialModification ) { int transId = msg != null ? msg . getTransactionId ( ) : Integer . MIN_VALUE ; logger . warn ( String . format ( "%s: Unexpected IOException during operation for region: %s key: %s messId: %s" , serverConnection . getName ( ) , serverConnection . getModRegion ( ) , serverConnection . getModKey ( ) , transId ) , e ) ; } else { logger . warn ( String . format ( "%s: Unexpected IOException: " , serverConnection . getName ( ) ) , e ) ; } }
public void test() { try { serviceInstance = extractPojosForBB . extractByKey ( execution , ResourceKey . SERVICE_INSTANCE_ID ) ; vnf = extractPojosForBB . extractByKey ( execution , ResourceKey . GENERIC_VNF_ID ) ; vfModule = extractPojosForBB . extractByKey ( execution , ResourceKey . VF_MODULE_ID ) ; Customer customer = gBBInput . getCustomer ( ) ; CloudRegion cloudRegion = gBBInput . getCloudRegion ( ) ; SDNCRequest sdncRequest = new SDNCRequest ( ) ; GenericResourceApiVfModuleOperationInformation req = sdncVfModuleResources . activateVfModule ( vfModule , vnf , serviceInstance , customer , cloudRegion , requestContext , buildCallbackURI ( sdncRequest ) ) ; sdncRequest . setSDNCPayload ( req ) ; sdncRequest . setTopology ( SDNCTopology . VFMODULE ) ; execution . setVariable ( SDNC_REQUEST , sdncRequest ) ; } catch ( Exception ex ) { logger . error ( "Exception occurred in SDNCActivateTasks activateVfModule process" , ex ) ; exceptionUtil . buildAndThrowWorkflowException ( execution , 7000 , ex ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( layoutFriendlyURLException , layoutFriendlyURLException ) ; } }
public void test() { if ( this . agentStatus == AgentStatus . INITIALIZING ) { changeStatus ( AgentStatus . RUNNING ) ; } else { logger . warn ( "Agent already started." ) ; return ; } }
@ Override public String getWorkflowRunReport ( int workflowRunSWID ) { logger . info ( "No metadata connection" ) ; return "" ; }
public void test() { try { JsonNode crNode = OBJECT_MAPPER . readValue ( request . getParameter ( CLEARING_REQUEST ) , JsonNode . class ) ; clearingRequest = OBJECT_MAPPER . convertValue ( crNode , ClearingRequest . class ) ; clearingRequest . setRequestingUser ( user . getEmail ( ) ) ; clearingRequest . setClearingState ( ClearingRequestState . NEW ) ; LiferayPortletURL projectUrl = createDetailLinkTemplate ( request ) ; projectUrl . setParameter ( PROJECT_ID , clearingRequest . getProjectId ( ) ) ; projectUrl . setParameter ( PAGENAME , PAGENAME_DETAIL ) ; ProjectService . Iface client = thriftClients . makeProjectClient ( ) ; requestSummary = client . createClearingRequest ( clearingRequest , user , projectUrl . toString ( ) ) ; } catch ( IOException | TException e ) { log . error ( "Error creating clearing request for project: " + clearingRequest . getProjectId ( ) , e ) ; response . setProperty ( ResourceResponse . HTTP_STATUS_CODE , "500" ) ; } }
public void test() { try { JsonGenerator jsonGenerator = JSON_FACTORY . createGenerator ( response . getWriter ( ) ) ; jsonGenerator . writeStartObject ( ) ; jsonGenerator . writeStringField ( RESULT , requestSummary . getRequestStatus ( ) . toString ( ) ) ; code_block = IfStatement ; jsonGenerator . writeEndObject ( ) ; jsonGenerator . close ( ) ; } catch ( IOException e ) { log . error ( "Cannot write JSON response for clearing request id " + requestSummary . getId ( ) + " in project " + clearingRequest . getProjectId ( ) + "." , e ) ; response . setProperty ( ResourceResponse . HTTP_STATUS_CODE , "500" ) ; } }
public void test() { try { lifeCycle . manager = this ; lifeCycle . start ( ) ; lifeCycles . put ( lifeCycle . getDomainName ( ) , lifeCycle ) ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; } }
private void getTableLineage ( ASTNode queryNode ) { ASTNode fromNode = ( ASTNode ) queryNode . getFirstChildWithType ( HiveParser . TOK_FROM ) ; ASTNode insertNode = ( ASTNode ) queryNode . getFirstChildWithType ( HiveParser . TOK_INSERT ) ; ASTNode insertDestinationNode = ( ASTNode ) insertNode . getFirstChildWithType ( HiveParser . TOK_DESTINATION ) ; if ( insertDestinationNode == null ) insertDestinationNode = ( ASTNode ) insertNode . getFirstChildWithType ( HiveParser . TOK_INSERT_INTO ) ; ASTNode nextToDestinationNode = ( ASTNode ) insertDestinationNode . getChild ( 0 ) ; ASTNode selectNode = ( ASTNode ) insertNode . getFirstChildWithType ( HiveParser . TOK_SELECT ) ; if ( selectNode == null ) selectNode = ( ASTNode ) insertNode . getFirstChildWithType ( HiveParser . TOK_SELECTDI ) ; ASTNode nextNodeOfFrom = ( ASTNode ) fromNode . getChild ( 0 ) ; LOGGER . info ( "nextNodeOfFrom to process : " + nextNodeOfFrom . toString ( ) ) ; if ( nextNodeOfFrom . getType ( ) == HiveParser . TOK_SUBQUERY ) handleTokSubQuery ( nextNodeOfFrom ) ; else-if ( nextNodeOfFrom . getType ( ) == HiveParser . TOK_TABREF ) handleTokTabRef ( nextNodeOfFrom ) ; else-if ( nextNodeOfFrom . getType ( ) == HiveParser . TOK_JOIN || nextNodeOfFrom . getType ( ) == HiveParser . TOK_LEFTOUTERJOIN || nextNodeOfFrom . getType ( ) == HiveParser . TOK_RIGHTOUTERJOIN || nextNodeOfFrom . getType ( ) == HiveParser . TOK_FULLOUTERJOIN || nextNodeOfFrom . getType ( ) == HiveParser . TOK_LEFTSEMIJOIN || nextNodeOfFrom . getType ( ) == HiveParser . TOK_UNIQUEJOIN ) handleTokJoin ( nextNodeOfFrom ) ; LOGGER . info ( "nextToDestinationNode to process : " + nextToDestinationNode . toString ( ) ) ; if ( nextToDestinationNode . getType ( ) == HiveParser . TOK_DIR ) handleTokDir ( queryNode ) ; else-if ( nextToDestinationNode . getType ( ) == HiveParser . TOK_TAB ) handleTokTab ( nextToDestinationNode ) ; LOGGER . info ( "selectNode to process : " + selectNode . toString ( ) ) ; code_block = ForStatement ; }
private void getTableLineage ( ASTNode queryNode ) { ASTNode fromNode = ( ASTNode ) queryNode . getFirstChildWithType ( HiveParser . TOK_FROM ) ; ASTNode insertNode = ( ASTNode ) queryNode . getFirstChildWithType ( HiveParser . TOK_INSERT ) ; ASTNode insertDestinationNode = ( ASTNode ) insertNode . getFirstChildWithType ( HiveParser . TOK_DESTINATION ) ; if ( insertDestinationNode == null ) insertDestinationNode = ( ASTNode ) insertNode . getFirstChildWithType ( HiveParser . TOK_INSERT_INTO ) ; ASTNode nextToDestinationNode = ( ASTNode ) insertDestinationNode . getChild ( 0 ) ; ASTNode selectNode = ( ASTNode ) insertNode . getFirstChildWithType ( HiveParser . TOK_SELECT ) ; if ( selectNode == null ) selectNode = ( ASTNode ) insertNode . getFirstChildWithType ( HiveParser . TOK_SELECTDI ) ; ASTNode nextNodeOfFrom = ( ASTNode ) fromNode . getChild ( 0 ) ; LOGGER . info ( "nextNodeOfFrom to process : " + nextNodeOfFrom . toString ( ) ) ; if ( nextNodeOfFrom . getType ( ) == HiveParser . TOK_SUBQUERY ) handleTokSubQuery ( nextNodeOfFrom ) ; else-if ( nextNodeOfFrom . getType ( ) == HiveParser . TOK_TABREF ) handleTokTabRef ( nextNodeOfFrom ) ; else-if ( nextNodeOfFrom . getType ( ) == HiveParser . TOK_JOIN || nextNodeOfFrom . getType ( ) == HiveParser . TOK_LEFTOUTERJOIN || nextNodeOfFrom . getType ( ) == HiveParser . TOK_RIGHTOUTERJOIN || nextNodeOfFrom . getType ( ) == HiveParser . TOK_FULLOUTERJOIN || nextNodeOfFrom . getType ( ) == HiveParser . TOK_LEFTSEMIJOIN || nextNodeOfFrom . getType ( ) == HiveParser . TOK_UNIQUEJOIN ) handleTokJoin ( nextNodeOfFrom ) ; LOGGER . info ( "nextToDestinationNode to process : " + nextToDestinationNode . toString ( ) ) ; if ( nextToDestinationNode . getType ( ) == HiveParser . TOK_DIR ) handleTokDir ( queryNode ) ; else-if ( nextToDestinationNode . getType ( ) == HiveParser . TOK_TAB ) handleTokTab ( nextToDestinationNode ) ; LOGGER . info ( "selectNode to process : " + selectNode . toString ( ) ) ; code_block = ForStatement ; }
private void getTableLineage ( ASTNode queryNode ) { ASTNode fromNode = ( ASTNode ) queryNode . getFirstChildWithType ( HiveParser . TOK_FROM ) ; ASTNode insertNode = ( ASTNode ) queryNode . getFirstChildWithType ( HiveParser . TOK_INSERT ) ; ASTNode insertDestinationNode = ( ASTNode ) insertNode . getFirstChildWithType ( HiveParser . TOK_DESTINATION ) ; if ( insertDestinationNode == null ) insertDestinationNode = ( ASTNode ) insertNode . getFirstChildWithType ( HiveParser . TOK_INSERT_INTO ) ; ASTNode nextToDestinationNode = ( ASTNode ) insertDestinationNode . getChild ( 0 ) ; ASTNode selectNode = ( ASTNode ) insertNode . getFirstChildWithType ( HiveParser . TOK_SELECT ) ; if ( selectNode == null ) selectNode = ( ASTNode ) insertNode . getFirstChildWithType ( HiveParser . TOK_SELECTDI ) ; ASTNode nextNodeOfFrom = ( ASTNode ) fromNode . getChild ( 0 ) ; LOGGER . info ( "nextNodeOfFrom to process : " + nextNodeOfFrom . toString ( ) ) ; if ( nextNodeOfFrom . getType ( ) == HiveParser . TOK_SUBQUERY ) handleTokSubQuery ( nextNodeOfFrom ) ; else-if ( nextNodeOfFrom . getType ( ) == HiveParser . TOK_TABREF ) handleTokTabRef ( nextNodeOfFrom ) ; else-if ( nextNodeOfFrom . getType ( ) == HiveParser . TOK_JOIN || nextNodeOfFrom . getType ( ) == HiveParser . TOK_LEFTOUTERJOIN || nextNodeOfFrom . getType ( ) == HiveParser . TOK_RIGHTOUTERJOIN || nextNodeOfFrom . getType ( ) == HiveParser . TOK_FULLOUTERJOIN || nextNodeOfFrom . getType ( ) == HiveParser . TOK_LEFTSEMIJOIN || nextNodeOfFrom . getType ( ) == HiveParser . TOK_UNIQUEJOIN ) handleTokJoin ( nextNodeOfFrom ) ; LOGGER . info ( "nextToDestinationNode to process : " + nextToDestinationNode . toString ( ) ) ; if ( nextToDestinationNode . getType ( ) == HiveParser . TOK_DIR ) handleTokDir ( queryNode ) ; else-if ( nextToDestinationNode . getType ( ) == HiveParser . TOK_TAB ) handleTokTab ( nextToDestinationNode ) ; LOGGER . info ( "selectNode to process : " + selectNode . toString ( ) ) ; code_block = ForStatement ; }
public void test() { if ( this . serialNumber == null ) { logger . debug ( "sendCommand getSerialNumber :: {}" , TelitModemAtCommands . getSerialNumber . getCommand ( ) ) ; byte [ ] reply ; CommConnection commAtConnection = openSerialPort ( getAtPort ( ) ) ; code_block = IfStatement ; code_block = TryStatement ;  closeSerialPort ( commAtConnection ) ; code_block = IfStatement ; } }
public void test() { try { long totalBases = sequencingObject . getFiles ( ) . stream ( ) . mapToLong ( f code_block = LoopStatement ; ) . sum ( ) ; CoverageQCEntry coverageQCEntry = new CoverageQCEntry ( sequencingObject , totalBases ) ; qcEntryRepository . save ( coverageQCEntry ) ; } catch ( EntityNotFoundException e ) { logger . warn ( "Not running coverage as not all files have fastqc results.  Object ID: " + sequencingObject . getId ( ) ) ; } }
public void test() { try { final Response response = remoteWebResource . request ( MediaType . TEXT_PLAIN ) . post ( Entity . text ( "" ) ) ; checkResponseStatus ( response ) ; } catch ( final Exception c ) { LOG . warn ( "[onStart] Erreur de connexion au serveur {} ({})" , remoteWebResource . getUri ( ) , c . getMessage ( ) ) ; } }
public void test() { if ( irodsDescriptionValue . getTagData ( ) . equals ( currentIrodsDescriptionValue . getTagData ( ) ) ) { log . info ( "no change in description, no update" ) ; return ; } }
@ Override public void checkAndUpdateDescriptionOnCollection ( final String collectionAbsolutePath , final IRODSTagValue irodsDescriptionValue ) throws JargonException { IRODSTagValue currentIrodsDescriptionValue = getDescriptionOnCollectionForLoggedInUser ( collectionAbsolutePath ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; deleteDescriptionFromCollection ( collectionAbsolutePath , currentIrodsDescriptionValue ) ; addDescriptionToCollection ( collectionAbsolutePath , irodsDescriptionValue ) ; log . info ( "update done" ) ; }
public void test() { try { insertPreparedStmt = connection . prepareStatement ( insertQuery ) ; } catch ( SQLException e ) { LOG . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( message . toString ( ) . contains ( "ERROR" ) ) { logger . error ( "Got error message: {}" , message . toString ( ) ) ; } }
public void test() { if ( message . toString ( ) . contains ( "ERROR" ) ) { logger . error ( "Got error message: {}" , message . toString ( ) ) ; } }
public void test() { try { handler . handleChannelData ( action , message ) ; } catch ( final BitfinexClientException e ) { logger . error ( "Got exception while handling callback" , e ) ; } }
public void test() { try { com . liferay . commerce . price . list . model . CommercePriceListChannelRel returnValue = CommercePriceListChannelRelServiceUtil . addCommercePriceListChannelRel ( commercePriceListId , commerceChannelId , order , serviceContext ) ; return com . liferay . commerce . price . list . model . CommercePriceListChannelRelSoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try { InsightsContentConfig existingContentConfig = reportConfigDAL . getContentConfig ( contentId ) ; code_block = IfStatement ; } catch ( Exception e ) { log . error ( e ) ; } }
public void test() { try { boolean interrupted = false ; code_block = WhileStatement ; code_block = IfStatement ; } catch ( JMSException e ) { log . error ( "Error while receiving messages " , e ) ; throw new RuntimeException ( "JMSException : Error while listening to messages" , e ) ; } catch ( IOException e ) { log . error ( "Error while writing message to file" , e ) ; throw new RuntimeException ( "IOException : Error while writing message to file\"" , e ) ; } }
public void test() { try { boolean interrupted = false ; code_block = WhileStatement ; code_block = IfStatement ; } catch ( JMSException e ) { log . error ( "Error while receiving messages " , e ) ; throw new RuntimeException ( "JMSException : Error while listening to messages" , e ) ; } catch ( IOException e ) { log . error ( "Error while writing message to file" , e ) ; throw new RuntimeException ( "IOException : Error while writing message to file\"" , e ) ; } }
@ Override public void run ( ) { log . debug ( "Producing event for custom resource id: {}" , customResourceUid ) ; eventHandler . handleEvent ( new TimerEvent ( customResourceUid , TimerEventSource . this ) ) ; }
public void test() { try { long checksum = 0 ; long remaining = length ; code_block = WhileStatement ; return new WriteSummary ( length , checksum ) ; } catch ( Throwable throwable ) { LOG . error ( "Failed to write file." , throwable ) ; fail ( "Failed to write file." + throwable . getMessage ( ) ) ; throw throwable ; } }
public void test() { if ( nextEvent != null ) { logger . debug ( "Return existing event, type: {}" , nextEvent . getEventType ( ) . toString ( ) ) ; event = nextEvent ; nextEvent = null ; } else-if ( textUnitsIterator . hasNext ( ) ) { logger . debug ( "There are TextUnitDTOs available, create a text unit and return the text unit event" ) ; event = new Event ( EventType . TEXT_UNIT , getNextTextUnit ( ) ) ; } else { logger . debug ( "No more TextUnitDTO, create end document event and return it" ) ; event = new Event ( EventType . END_DOCUMENT ) ; finished = true ; } }
public void test() { if ( nextEvent != null ) { logger . debug ( "Return existing event, type: {}" , nextEvent . getEventType ( ) . toString ( ) ) ; event = nextEvent ; nextEvent = null ; } else-if ( textUnitsIterator . hasNext ( ) ) { logger . debug ( "There are TextUnitDTOs available, create a text unit and return the text unit event" ) ; event = new Event ( EventType . TEXT_UNIT , getNextTextUnit ( ) ) ; } else { logger . debug ( "No more TextUnitDTO, create end document event and return it" ) ; event = new Event ( EventType . END_DOCUMENT ) ; finished = true ; } }
public void test() { if ( nextEvent != null ) { logger . debug ( "Return existing event, type: {}" , nextEvent . getEventType ( ) . toString ( ) ) ; event = nextEvent ; nextEvent = null ; } else-if ( textUnitsIterator . hasNext ( ) ) { logger . debug ( "There are TextUnitDTOs available, create a text unit and return the text unit event" ) ; event = new Event ( EventType . TEXT_UNIT , getNextTextUnit ( ) ) ; } else { logger . debug ( "No more TextUnitDTO, create end document event and return it" ) ; event = new Event ( EventType . END_DOCUMENT ) ; finished = true ; } }
@ Test public void testUserHostAndPortSerialization ( ) throws Exception { String result = checkSerializesAs ( UserAndHostAndPort . fromParts ( "testHostUser" , "1.2.3.4" , 22 ) , null ) ; log . info ( "UserHostAndPort serialized as: " + result ) ; Assert . assertFalse ( result . contains ( "error" ) , "Shouldn't have had an error, instead got: " + result ) ; Assert . assertEquals ( Strings . collapseWhitespace ( result , "" ) , "{\"user\":\"testHostUser\",\"hostAndPort\":{\"host\":\"1.2.3.4\",\"port\":22,\"hasBracketlessColons\":false}}" ) ; }
public void test() { if ( traceEnabled ) { log . trace ( "receive({}) check iteration {} for id={} remain time={}" , this , count , id , idleTimeout ) ; } }
public void test() { if ( buffer != null ) { return buffer ; } }
public List findByExample ( CmZosState instance ) { log . debug ( "finding CmZosState instance by example" ) ; code_block = TryStatement ;  }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.CmZosState" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.CmZosState" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
public void processClaimedUser ( ShadowUser shadowUser , RequestContext context ) { logger . info ( context , "ShadowUserProcessor:processClaimedUser:started claming shadow user with processId: " + shadowUser . getProcessId ( ) ) ; String orgId = getOrgId ( shadowUser , context ) ; Map < String , Object > esUser = ( Map < String , Object > ) ElasticSearchHelper . getResponseFromFuture ( elasticSearchService . getDataByIdentifier ( ProjectUtil . EsType . user . getTypeName ( ) , shadowUser . getUserId ( ) , context ) ) ; String userId = ( String ) esUser . get ( JsonKey . ID ) ; String rootOrgId = ( String ) esUser . get ( JsonKey . ROOT_ORG_ID ) ; logger . info ( context , "ShadowUserProcessor:processClaimedUser:started: flag value got from es " + esUser . get ( JsonKey . FLAGS_VALUE ) ) ; int flagsValue = null != esUser . get ( JsonKey . FLAGS_VALUE ) ? ( int ) esUser . get ( JsonKey . FLAGS_VALUE ) : 0 ; logger . info ( context , "ShadowUserProcessor:processClaimedUser:Got Flag Value " + flagsValue ) ; code_block = IfStatement ; deleteUserFromOrganisations ( shadowUser , rootOrgId , ( List < Map < String , Object > > ) esUser . get ( JsonKey . ORGANISATIONS ) , context ) ; code_block = IfStatement ; syncUserToES ( userId , context ) ; updateUserInShadowDb ( userId , shadowUser , ClaimStatus . CLAIMED . getValue ( ) , null , context ) ; }
public void processClaimedUser ( ShadowUser shadowUser , RequestContext context ) { logger . info ( context , "ShadowUserProcessor:processClaimedUser:started claming shadow user with processId: " + shadowUser . getProcessId ( ) ) ; String orgId = getOrgId ( shadowUser , context ) ; Map < String , Object > esUser = ( Map < String , Object > ) ElasticSearchHelper . getResponseFromFuture ( elasticSearchService . getDataByIdentifier ( ProjectUtil . EsType . user . getTypeName ( ) , shadowUser . getUserId ( ) , context ) ) ; String userId = ( String ) esUser . get ( JsonKey . ID ) ; String rootOrgId = ( String ) esUser . get ( JsonKey . ROOT_ORG_ID ) ; logger . info ( context , "ShadowUserProcessor:processClaimedUser:started: flag value got from es " + esUser . get ( JsonKey . FLAGS_VALUE ) ) ; int flagsValue = null != esUser . get ( JsonKey . FLAGS_VALUE ) ? ( int ) esUser . get ( JsonKey . FLAGS_VALUE ) : 0 ; logger . info ( context , "ShadowUserProcessor:processClaimedUser:Got Flag Value " + flagsValue ) ; code_block = IfStatement ; deleteUserFromOrganisations ( shadowUser , rootOrgId , ( List < Map < String , Object > > ) esUser . get ( JsonKey . ORGANISATIONS ) , context ) ; code_block = IfStatement ; syncUserToES ( userId , context ) ; updateUserInShadowDb ( userId , shadowUser , ClaimStatus . CLAIMED . getValue ( ) , null , context ) ; }
public void processClaimedUser ( ShadowUser shadowUser , RequestContext context ) { logger . info ( context , "ShadowUserProcessor:processClaimedUser:started claming shadow user with processId: " + shadowUser . getProcessId ( ) ) ; String orgId = getOrgId ( shadowUser , context ) ; Map < String , Object > esUser = ( Map < String , Object > ) ElasticSearchHelper . getResponseFromFuture ( elasticSearchService . getDataByIdentifier ( ProjectUtil . EsType . user . getTypeName ( ) , shadowUser . getUserId ( ) , context ) ) ; String userId = ( String ) esUser . get ( JsonKey . ID ) ; String rootOrgId = ( String ) esUser . get ( JsonKey . ROOT_ORG_ID ) ; logger . info ( context , "ShadowUserProcessor:processClaimedUser:started: flag value got from es " + esUser . get ( JsonKey . FLAGS_VALUE ) ) ; int flagsValue = null != esUser . get ( JsonKey . FLAGS_VALUE ) ? ( int ) esUser . get ( JsonKey . FLAGS_VALUE ) : 0 ; logger . info ( context , "ShadowUserProcessor:processClaimedUser:Got Flag Value " + flagsValue ) ; code_block = IfStatement ; deleteUserFromOrganisations ( shadowUser , rootOrgId , ( List < Map < String , Object > > ) esUser . get ( JsonKey . ORGANISATIONS ) , context ) ; code_block = IfStatement ; syncUserToES ( userId , context ) ; updateUserInShadowDb ( userId , shadowUser , ClaimStatus . CLAIMED . getValue ( ) , null , context ) ; }
public void test() { try { messageMetadata = MessageMetadata . fromMessage ( message ) ; publicKey = ( String ) message . getObject ( ) ; } catch ( final JMSException e ) { LOGGER . error ( "UNRECOVERABLE ERROR, unable to read ObjectMessage instance, giving up." , e ) ; return ; } }
public void test() { try { final CloseableHttpClient httpclient = HttpClients . createDefault ( ) ; uri = "http://search.maven.org/solrsearch/select?q=1:<SHA1>&rows=20&wt=json" . replaceAll ( "<SHA1>" , _digest ) ; final HttpGet method = new HttpGet ( uri ) ; if ( ConnectionUtil . getProxyConfig ( ) != null ) method . setConfig ( ConnectionUtil . getProxyConfig ( ) ) ; final CloseableHttpResponse response = httpclient . execute ( method ) ; code_block = TryStatement ;  } catch ( ClientProtocolException e ) { log . error ( "HTTP GET [uri=" + uri + "] caused an exception: " + e . getMessage ( ) ) ; } catch ( Exception e ) { log . error ( "HTTP GET [uri=" + uri + "] caused an exception: " + e . getMessage ( ) , e ) ; } }
public void test() { try { final CloseableHttpClient httpclient = HttpClients . createDefault ( ) ; uri = "http://search.maven.org/solrsearch/select?q=1:<SHA1>&rows=20&wt=json" . replaceAll ( "<SHA1>" , _digest ) ; final HttpGet method = new HttpGet ( uri ) ; if ( ConnectionUtil . getProxyConfig ( ) != null ) method . setConfig ( ConnectionUtil . getProxyConfig ( ) ) ; final CloseableHttpResponse response = httpclient . execute ( method ) ; code_block = TryStatement ;  } catch ( ClientProtocolException e ) { log . error ( "HTTP GET [uri=" + uri + "] caused an exception: " + e . getMessage ( ) ) ; } catch ( Exception e ) { log . error ( "HTTP GET [uri=" + uri + "] caused an exception: " + e . getMessage ( ) , e ) ; } }
public void execute ( ) throws IOException { log . info ( "Finding matches" ) ; Set < Map < String , String > > resultSet = match ( this . matchThreshold , this . scoreJena ) ; code_block = ForStatement ; log . info ( "Found " + resultSet . size ( ) + " links between Vivo and the Input model" ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; this . inputJena . sync ( ) ; }
public void test() { for ( Map < String , String > entry : resultSet ) { String sInputURI = entry . get ( "sInputURI" ) ; log . trace ( "input: " + sInputURI ) ; String sVivoURI = entry . get ( "sVivoURI" ) ; log . trace ( "vivo: " + sVivoURI ) ; String score = entry . get ( "score" ) ; log . trace ( "score: " + score ) ; log . debug ( "Match found: <" + sInputURI + "> in Input matched with <" + sVivoURI + "> in Vivo" ) ; } }
public void test() { for ( Map < String , String > entry : resultSet ) { String sInputURI = entry . get ( "sInputURI" ) ; log . trace ( "input: " + sInputURI ) ; String sVivoURI = entry . get ( "sVivoURI" ) ; log . trace ( "vivo: " + sVivoURI ) ; String score = entry . get ( "score" ) ; log . trace ( "score: " + score ) ; log . debug ( "Match found: <" + sInputURI + "> in Input matched with <" + sVivoURI + "> in Vivo" ) ; } }
public void test() { for ( Map < String , String > entry : resultSet ) { String sInputURI = entry . get ( "sInputURI" ) ; log . trace ( "input: " + sInputURI ) ; String sVivoURI = entry . get ( "sVivoURI" ) ; log . trace ( "vivo: " + sVivoURI ) ; String score = entry . get ( "score" ) ; log . trace ( "score: " + score ) ; log . debug ( "Match found: <" + sInputURI + "> in Input matched with <" + sVivoURI + "> in Vivo" ) ; } }
public void execute ( ) throws IOException { log . info ( "Finding matches" ) ; Set < Map < String , String > > resultSet = match ( this . matchThreshold , this . scoreJena ) ; code_block = ForStatement ; log . info ( "Found " + resultSet . size ( ) + " links between Vivo and the Input model" ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; this . inputJena . sync ( ) ; }
public void test() { try { conn . close ( ) ; } catch ( SQLException e ) { logger . error ( e . getMessage ( ) , e ) ; } finally { conn = null ; } }
public void test() { try { Webcam webcamx = ( Webcam ) Runtime . start ( "webcam" , "Webcam" ) ; webcamx . capture ( "/dev/video0" , "jpg" , 30 , 640 , 480 , 0.5 ) ; WebGui webgui = ( WebGui ) Runtime . create ( "webgui" , "WebGui" ) ; webgui . autoStartBrowser ( false ) ; webgui . startService ( ) ; } catch ( Exception e ) { log . error ( "main threw" , e ) ; } }
public void test() { if ( client . getSecurityGroupServices ( ) . describeSecurityGroupsInRegion ( region , groupName ) . size ( ) > 0 ) { logger . debug ( ">> deleting securityGroup(%s)" , groupName ) ; client . getSecurityGroupServices ( ) . deleteSecurityGroupInRegion ( region , groupName ) ; securityGroupMap . invalidate ( new RegionNameAndIngressRules ( region , groupName , null , false ) ) ; logger . debug ( "<< deleted securityGroup(%s)" , groupName ) ; } }
public void test() { if ( client . getSecurityGroupServices ( ) . describeSecurityGroupsInRegion ( region , groupName ) . size ( ) > 0 ) { logger . debug ( ">> deleting securityGroup(%s)" , groupName ) ; client . getSecurityGroupServices ( ) . deleteSecurityGroupInRegion ( region , groupName ) ; securityGroupMap . invalidate ( new RegionNameAndIngressRules ( region , groupName , null , false ) ) ; logger . debug ( "<< deleted securityGroup(%s)" , groupName ) ; } }
@ Test public void testMultiMapSerialization ( ) throws Exception { Multimap < String , Integer > m = MultimapBuilder . hashKeys ( ) . arrayListValues ( ) . build ( ) ; m . put ( "bob" , 24 ) ; m . put ( "bob" , 25 ) ; String result = checkSerializesAs ( m , null ) ; log . info ( "multimap serialized as: " + result ) ; Assert . assertFalse ( result . contains ( "error" ) , "Shouldn't have had an error, instead got: " + result ) ; Assert . assertEquals ( Strings . collapseWhitespace ( result , "" ) , "{\"bob\":[24,25]}" ) ; }
public void test() { try { super . close ( ) ; } catch ( IOException e ) { LOGGER . warn ( "Could not close plugin classloader" , e ) ; } }
public void test() { if ( token . getJwt ( ) . getJWTClaimsSet ( ) . getIssueTime ( ) . before ( validToDate ) ) { logger . info ( "Rotating the registration access token to " + client . getClientId ( ) ) ; tokenService . revokeAccessToken ( token ) ; OAuth2AccessTokenEntity newToken = connectTokenService . createResourceAccessToken ( client ) ; tokenService . saveAccessToken ( newToken ) ; return newToken ; } else { return token ; } }
public void test() { try { Date validToDate = new Date ( System . currentTimeMillis ( ) - config . getRegTokenLifeTime ( ) * 1000 ) ; code_block = IfStatement ; } catch ( ParseException e ) { logger . error ( "Couldn't parse a known-valid token?" , e ) ; return token ; } }
public void test() { if ( retryCounter > 0 ) { LOG . warn ( "Netconf WRITE transaction failed to {}. Restarting transaction ... " , e . getMessage ( ) ) ; return write ( mountpoint , data , -- retryCounter ) ; } else { LOG . warn ( "Netconf WRITE transaction unsuccessful. Maximal number of attempts reached. Trace: {}" , e ) ; return false ; } }
private void validateExchange ( final PkiMessage < ? > req , final CertRep res ) throws TransactionException { LOGGER . debug ( "Validating SCEP message exchange" ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; LOGGER . debug ( "SCEP message exchange validated successfully" ) ; }
public void test() { if ( ! res . getTransactionId ( ) . equals ( req . getTransactionId ( ) ) ) { throw new TransactionException ( "Transaction ID Mismatch" ) ; } else { LOGGER . debug ( "Matched transaction IDs" ) ; } }
public void test() { if ( ! res . getRecipientNonce ( ) . equals ( req . getSenderNonce ( ) ) ) { throw new InvalidNonceException ( req . getSenderNonce ( ) , res . getRecipientNonce ( ) ) ; } else { LOGGER . debug ( "Matched request senderNonce and response recipientNonce" ) ; } }
public void test() { if ( res . getSenderNonce ( ) == null ) { LOGGER . warn ( "Response senderNonce is null" ) ; return ; } }
public ManagedConnectionFactory getManagedConnectionFactory ( ) { logger . debug ( "getManagedConnectionFactory()..." ) ; return this . managedConnectionFactory ; }
public void test() { try { clientResp = restClient . get ( relativeURL , queryParams ) ; } catch ( Exception e ) { LOG . error ( "Failed to get response, Error is : " + e . getMessage ( ) ) ; } }
public void test() { try { response = kv . get ( storeKey ) . get ( ) ; } catch ( InterruptedException | ExecutionException e ) { log . error ( "read(key:{}) error." , key , e ) ; } }
public void test() { try { this . writeValue ( from ? HeliosEasyControlsBindingConstants . BYPASS_FROM_DAY : HeliosEasyControlsBindingConstants . BYPASS_TO_DAY , Integer . toString ( bypassDate . getDay ( ) ) ) ; this . writeValue ( from ? HeliosEasyControlsBindingConstants . BYPASS_FROM_MONTH : HeliosEasyControlsBindingConstants . BYPASS_TO_MONTH , Integer . toString ( bypassDate . getMonth ( ) ) ) ; } catch ( HeliosException e ) { logger . warn ( "{} encountered Exception when trying to set bypass period: {}" , HeliosEasyControlsHandler . class . getSimpleName ( ) , e . getMessage ( ) ) ; } }
public void test() { if ( dropCommandStrategy . shouldDropCommand ( commandId , address , redelivery ) ) { writeBuffer . flip ( ) ; LOG . info ( "Dropping datagram with command: " + commandId ) ; getReplayBuffer ( ) . addBuffer ( commandId , writeBuffer ) ; } else { super . sendWriteBuffer ( commandId , address , writeBuffer , redelivery ) ; } }
@ PostConstruct public void initialise ( ) { logger . info ( "START initialisation of vehicle service proxy." ) ; service = serviceLocator . builder ( VehicleStateStatelessAsync . class , "io.joynr.examples.statelessasync.carsim" ) . withUseCase ( "jee-consumer-test" ) . build ( ) ; logger . info ( "FINISHED initialisation vehicle service proxy." ) ; }
@ PostConstruct public void initialise ( ) { logger . info ( "START initialisation of vehicle service proxy." ) ; service = serviceLocator . builder ( VehicleStateStatelessAsync . class , "io.joynr.examples.statelessasync.carsim" ) . withUseCase ( "jee-consumer-test" ) . build ( ) ; logger . info ( "FINISHED initialisation vehicle service proxy." ) ; }
@ Transactional ( readOnly = false ) public int undoSoftDeleteOnCascade ( int id , Set < CardType > types , Set < EventType > filteredEvents ) { LOG . debug ( "undoSoftDeleteOnCascade: {id: {}}" , id ) ; return queries . undoSoftDeleteOnCascade ( id , toStringList ( types ) , toStringList ( filteredEvents ) ) ; }
public void test() { try { return delegate . next ( ) ; } finally { log . debug ( sb . a ( ", after=" + toString ( ) + ']' ) . toString ( ) ) ; } }
public void test() { if ( ! ( rawHookId instanceof String ) ) { logger . warn ( "Found hook with invalid {}, not registering" , ConfigurationService . KURA_SERVICE_PID ) ; return ; } }
public void test() { if ( registeredHooks . containsKey ( hookId ) ) { logger . warn ( "Found duplicated hook with id {}, not registering" , ConfigurationService . KURA_SERVICE_PID ) ; return ; } }
public synchronized void bindHook ( ServiceReference < DeploymentHook > hook ) { final Object rawHookId = hook . getProperty ( ConfigurationService . KURA_SERVICE_PID ) ; code_block = IfStatement ; final String hookId = ( String ) rawHookId ; code_block = IfStatement ; this . registeredHooks . put ( hookId , getBundleContext ( ) . getService ( hook ) ) ; logger . info ( "Hook registered: {}" , hookId ) ; updateAssociations ( ) ; }
public void test() { if ( result . getError ( ) . isPresent ( ) ) { LOGGER . warn ( "HealthCheck is unstable for {} : {}" , result . getComponentName ( ) . getName ( ) , result . getCause ( ) . orElse ( "" ) , result . getError ( ) . get ( ) ) ; } else { LOGGER . warn ( "HealthCheck is unstable for {} : {}" , result . getComponentName ( ) . getName ( ) , result . getCause ( ) . orElse ( "" ) ) ; } }
public void test() { if ( result . getError ( ) . isPresent ( ) ) { LOGGER . warn ( "HealthCheck is unstable for {} : {}" , result . getComponentName ( ) . getName ( ) , result . getCause ( ) . orElse ( "" ) , result . getError ( ) . get ( ) ) ; } else { LOGGER . warn ( "HealthCheck is unstable for {} : {}" , result . getComponentName ( ) . getName ( ) , result . getCause ( ) . orElse ( "" ) ) ; } }
public void test() { if ( result . getError ( ) . isPresent ( ) ) { LOGGER . warn ( "HealthCheck is unstable for {} : {}" , result . getComponentName ( ) . getName ( ) , result . getCause ( ) . orElse ( "" ) , result . getError ( ) . get ( ) ) ; } else { LOGGER . warn ( "HealthCheck is unstable for {} : {}" , result . getComponentName ( ) . getName ( ) , result . getCause ( ) . orElse ( "" ) ) ; } }
public void test() { if ( result . getError ( ) . isPresent ( ) ) { LOGGER . warn ( "HealthCheck is unstable for {} : {}" , result . getComponentName ( ) . getName ( ) , result . getCause ( ) . orElse ( "" ) , result . getError ( ) . get ( ) ) ; } else { LOGGER . warn ( "HealthCheck is unstable for {} : {}" , result . getComponentName ( ) . getName ( ) , result . getCause ( ) . orElse ( "" ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( LogMarker . PERSIST_ADVISOR_VERBOSE ) ) { logger . debug ( LogMarker . PERSIST_ADVISOR_VERBOSE , "{}-{}: Member offine. id={}, persistentID={}" , shortDiskStoreId ( ) , regionPath , distributedMember , persistentID ) ; } }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; } catch ( DiskAccessException e ) { logger . warn ( "Unable to persist membership change" , e ) ; } }
private void cleanupTemporaryFile ( String temporaryFile ) { s_logger . debug ( "Cleaning up temporary certificate file" ) ; Script . runSimpleBashScript ( "rm -f " + temporaryFile ) ; }
public void test() { if ( ! dhcpServiceProvider . removeDhcpSupportForSubnet ( network ) ) { s_logger . warn ( "Failed to remove the ip alias on the router, marking it as removed in db and freed the allocated ip " + ipAlias . getIp4Address ( ) ) ; } }
public void test() { try { final NicIpAliasVO ipAlias = _nicIpAliasDao . findByGatewayAndNetworkIdAndState ( nic . getIPv4Gateway ( ) , network . getId ( ) , NicIpAlias . State . active ) ; code_block = IfStatement ; } catch ( final ResourceUnavailableException e ) { s_logger . info ( "Unable to delete the ip alias due to unable to contact the virtualrouter." ) ; } }
public void test() { try { String url = getTDMURL ( ) + "/api/bundle/deploy/status/" + id + "/list" ; _log . debug ( "requesting:" + url ) ; String json = _remoteConnectionService . getContent ( url ) ; _log . debug ( "response:" + json ) ; return Response . ok ( json ) . build ( ) ; } catch ( Exception e ) { return Response . serverError ( ) . build ( ) ; } }
public void test() { try { String url = getTDMURL ( ) + "/api/bundle/deploy/status/" + id + "/list" ; _log . debug ( "requesting:" + url ) ; String json = _remoteConnectionService . getContent ( url ) ; _log . debug ( "response:" + json ) ; return Response . ok ( json ) . build ( ) ; } catch ( Exception e ) { return Response . serverError ( ) . build ( ) ; } }
public void test() { if ( ! ( reservation . getState ( ) . equals ( lastState ) ) ) { lastState = reservation . getState ( ) ; log . debug ( "Service " + serviceId + " state updated to " + lastState ) ; } }
public void test() { if ( connectedShardDataSourceMap . containsKey ( shardId ) ) { TransactionalDataSource dataSource = connectedShardDataSourceMap . get ( shardId ) ; dataSource . update ( dbVersion ) ; log . debug ( "Init existing SHARD using db version'{}' in {} ms" , dbVersion , System . currentTimeMillis ( ) - start ) ; return dataSource ; } else { return createShardDatasource ( shardId , dbVersion ) ; } }
@ Override public void assign ( byte [ ] regionName ) throws MasterNotRunningException , ZooKeeperConnectionException , IOException { LOG . info ( "assign is a no-op" ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Fetching the details of all ${rootArtifactId} devices" ) ; } }
public void test() { try { code_block = IfStatement ; devices = deviceTypeDAO . getDeviceTypeDAO ( ) . getAllDevices ( ) ; } catch ( DeviceMgtPluginException e ) { String msg = "Error while fetching all ${rootArtifactId} devices." ; log . error ( msg , e ) ; throw new DeviceManagementException ( msg , e ) ; } }
@ AfterClass public static void finalization ( ) { logger . info ( "Stopping LB!" ) ; balancer . stop ( ) ; logger . info ( "Stopping SMPP server!" ) ; server . destroy ( ) ; logger . info ( "SMPP server stopped!" ) ; executor . shutdownNow ( ) ; monitorExecutor . shutdownNow ( ) ; balancer . stop ( ) ; logger . info ( "Done. Exiting" ) ; }
@ AfterClass public static void finalization ( ) { logger . info ( "Stopping LB!" ) ; balancer . stop ( ) ; logger . info ( "Stopping SMPP server!" ) ; server . destroy ( ) ; logger . info ( "SMPP server stopped!" ) ; executor . shutdownNow ( ) ; monitorExecutor . shutdownNow ( ) ; balancer . stop ( ) ; logger . info ( "Done. Exiting" ) ; }
@ AfterClass public static void finalization ( ) { logger . info ( "Stopping LB!" ) ; balancer . stop ( ) ; logger . info ( "Stopping SMPP server!" ) ; server . destroy ( ) ; logger . info ( "SMPP server stopped!" ) ; executor . shutdownNow ( ) ; monitorExecutor . shutdownNow ( ) ; balancer . stop ( ) ; logger . info ( "Done. Exiting" ) ; }
@ AfterClass public static void finalization ( ) { logger . info ( "Stopping LB!" ) ; balancer . stop ( ) ; logger . info ( "Stopping SMPP server!" ) ; server . destroy ( ) ; logger . info ( "SMPP server stopped!" ) ; executor . shutdownNow ( ) ; monitorExecutor . shutdownNow ( ) ; balancer . stop ( ) ; logger . info ( "Done. Exiting" ) ; }
public void parseStatus ( int bus , int address , int value ) throws java . io . IOException { log . debug ( "parse Status called with bus {} address {} and value {}" , bus , address , value ) ; java . util . List < SystemConnectionMemo > list = InstanceManager . getList ( SystemConnectionMemo . class ) ; SystemConnectionMemo memo ; code_block = TryStatement ;  String sensorName = memo . getSystemPrefix ( ) + "S" + address ; this . initSensor ( sensorName ) ; code_block = IfStatement ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Setting Sensor INACTIVE" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Setting Sensor ACTIVE" ) ; } }
public void test() { try { configRepository . updateUserConfig ( userConfig , version ) ; } catch ( DuplicateUsernameException e ) { logger . debug ( e . getMessage ( ) , e ) ; throw new JsonServiceException ( CONFLICT , "username" ) ; } }
public final void openModule ( WorkbenchModule module ) { code_block = IfStatement ; LOGGER . trace ( "openModule - set active module to " + module ) ; activeModule . setValue ( module ) ; }
public void test() { { LOGGER . info ( "About to reactivate record, orcid={}" , orcid ) ; String primaryEmailTrim = primaryEmail . trim ( ) ; emailManager . reactivatePrimaryEmail ( orcid , primaryEmailTrim ) ; code_block = IfStatement ; ProfileEntity profileEntity = profileDao . find ( orcid ) ; profileEntity . setDeactivationDate ( null ) ; profileEntity . setClaimed ( true ) ; profileEntity . setIndexingStatus ( IndexingStatus . PENDING ) ; code_block = IfStatement ; profileDao . merge ( profileEntity ) ; code_block = IfStatement ; LOGGER . info ( "Record orcid={} successfully reactivated" , orcid ) ; return true ; } }
public void test() { { LOGGER . info ( "About to reactivate record, orcid={}" , orcid ) ; String primaryEmailTrim = primaryEmail . trim ( ) ; emailManager . reactivatePrimaryEmail ( orcid , primaryEmailTrim ) ; code_block = IfStatement ; ProfileEntity profileEntity = profileDao . find ( orcid ) ; profileEntity . setDeactivationDate ( null ) ; profileEntity . setClaimed ( true ) ; profileEntity . setIndexingStatus ( IndexingStatus . PENDING ) ; code_block = IfStatement ; profileDao . merge ( profileEntity ) ; code_block = IfStatement ; LOGGER . info ( "Record orcid={} successfully reactivated" , orcid ) ; return true ; } }
public void test() { try { stopRejectingRpcServer ( ) ; LOG . info ( "Starting Alluxio job master gRPC server on address {}" , mRpcBindAddress ) ; GrpcServerBuilder serverBuilder = GrpcServerBuilder . forAddress ( GrpcServerAddress . create ( mRpcConnectAddress . getHostName ( ) , mRpcBindAddress ) , ServerConfiguration . global ( ) , ServerUserState . global ( ) ) ; registerServices ( serverBuilder , mJobMaster . getServices ( ) ) ; serverBuilder . addService ( alluxio . grpc . ServiceType . JOURNAL_MASTER_CLIENT_SERVICE , new GrpcService ( new JournalMasterClientServiceHandler ( new DefaultJournalMaster ( JournalDomain . JOB_MASTER , mJournalSystem ) ) ) ) ; mGrpcServer = serverBuilder . build ( ) . start ( ) ; LOG . info ( "Started Alluxio job master gRPC server on address {}" , mRpcConnectAddress ) ; mGrpcServer . awaitTermination ( ) ; } catch ( IOException e ) { throw new RuntimeException ( e ) ; } }
public void test() { try { stopRejectingRpcServer ( ) ; LOG . info ( "Starting Alluxio job master gRPC server on address {}" , mRpcBindAddress ) ; GrpcServerBuilder serverBuilder = GrpcServerBuilder . forAddress ( GrpcServerAddress . create ( mRpcConnectAddress . getHostName ( ) , mRpcBindAddress ) , ServerConfiguration . global ( ) , ServerUserState . global ( ) ) ; registerServices ( serverBuilder , mJobMaster . getServices ( ) ) ; serverBuilder . addService ( alluxio . grpc . ServiceType . JOURNAL_MASTER_CLIENT_SERVICE , new GrpcService ( new JournalMasterClientServiceHandler ( new DefaultJournalMaster ( JournalDomain . JOB_MASTER , mJournalSystem ) ) ) ) ; mGrpcServer = serverBuilder . build ( ) . start ( ) ; LOG . info ( "Started Alluxio job master gRPC server on address {}" , mRpcConnectAddress ) ; mGrpcServer . awaitTermination ( ) ; } catch ( IOException e ) { throw new RuntimeException ( e ) ; } }
public void test() { switch ( responseStatus ) { case CACHE_HIT : LOGGER . debug ( "A response was generated from the cache with " + "no requests sent upstream" ) ; break ; case CACHE_MODULE_RESPONSE : LOGGER . debug ( "The response was generated directly by the " + "caching module" ) ; break ; case CACHE_MISS : LOGGER . debug ( "The response came from an upstream server" ) ; break ; case VALIDATED : LOGGER . debug ( "The response was generated from the cache " + "after validating the entry with the origin server" ) ; break ; } }
public void test() { switch ( responseStatus ) { case CACHE_HIT : LOGGER . debug ( "A response was generated from the cache with " + "no requests sent upstream" ) ; break ; case CACHE_MODULE_RESPONSE : LOGGER . debug ( "The response was generated directly by the " + "caching module" ) ; break ; case CACHE_MISS : LOGGER . debug ( "The response came from an upstream server" ) ; break ; case VALIDATED : LOGGER . debug ( "The response was generated from the cache " + "after validating the entry with the origin server" ) ; break ; } }
public void test() { switch ( responseStatus ) { case CACHE_HIT : LOGGER . debug ( "A response was generated from the cache with " + "no requests sent upstream" ) ; break ; case CACHE_MODULE_RESPONSE : LOGGER . debug ( "The response was generated directly by the " + "caching module" ) ; break ; case CACHE_MISS : LOGGER . debug ( "The response came from an upstream server" ) ; break ; case VALIDATED : LOGGER . debug ( "The response was generated from the cache " + "after validating the entry with the origin server" ) ; break ; } }
public void test() { switch ( responseStatus ) { case CACHE_HIT : LOGGER . debug ( "A response was generated from the cache with " + "no requests sent upstream" ) ; break ; case CACHE_MODULE_RESPONSE : LOGGER . debug ( "The response was generated directly by the " + "caching module" ) ; break ; case CACHE_MISS : LOGGER . debug ( "The response came from an upstream server" ) ; break ; case VALIDATED : LOGGER . debug ( "The response was generated from the cache " + "after validating the entry with the origin server" ) ; break ; } }
@ Override public void entryAdded ( EntryEvent < Long , Whiteboards > event ) { log . trace ( "WbListener::Add" ) ; onlineWbs . put ( event . getKey ( ) , event . getValue ( ) ) ; }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; } catch ( IOException e ) { logger . debug ( "Error closing {} port for thing {} at IP {}: exception={}" , conn . getName ( ) , thingID ( ) , conn . getIP ( ) , e . getMessage ( ) ) ; } }
public void test() { try { new ExtGithub ( this . farm ) . value ( ) . repos ( ) . get ( new Coordinates . Simple ( "zerocracy/farm" ) ) . json ( ) ; this . output . set ( "OK" ) ; } catch ( final IOException err ) { Logger . error ( this , "Failed to find farm repo: %[exception]s" , err ) ; this . output . set ( err . getMessage ( ) ) ; } }
public void test() { if ( LOGGER . isTraceEnabled ( ) ) { LOGGER . trace ( "columnData from current stack: {}" , Arrays . toString ( columnData ) ) ; } }
@ Test public void testFindUsersByBadHeaderValueThenReturnBadRequest ( ) { LOGGER . debug ( "testFindUsersByBadHeaderValueThenReturnBadRequest" ) ; final HttpHeaders headers = new HttpHeaders ( ) ; headers . add ( CommonConstants . X_TENANT_ID_HEADER , "%ds</><!-sdq" ) ; super . performGet ( "/archive" , ImmutableMap . of ( "page" , 1 , "size" , 20 , "orderBy" , "id" ) , headers , status ( ) . isBadRequest ( ) ) ; }
public void test() { try { logger . debug ( "Sending transformed FCM payload: {}" , fcmMessage ) ; final ConfigurableFCMSender sender = new ConfigurableFCMSender ( androidVariant . getGoogleKey ( ) ) ; PrometheusExporter . instance ( ) . increasetotalPushAndroidRequests ( ) ; processFCM ( androidVariant , pushTargets , fcmMessage , sender ) ; logger . debug ( "Message batch to FCM has been submitted" ) ; callback . onSuccess ( ) ; } catch ( Exception e ) { callback . onError ( String . format ( "Error sending payload to FCM server: %s" , e . getMessage ( ) ) ) ; } }
public void test() { try { logger . debug ( "Sending transformed FCM payload: {}" , fcmMessage ) ; final ConfigurableFCMSender sender = new ConfigurableFCMSender ( androidVariant . getGoogleKey ( ) ) ; PrometheusExporter . instance ( ) . increasetotalPushAndroidRequests ( ) ; processFCM ( androidVariant , pushTargets , fcmMessage , sender ) ; logger . debug ( "Message batch to FCM has been submitted" ) ; callback . onSuccess ( ) ; } catch ( Exception e ) { callback . onError ( String . format ( "Error sending payload to FCM server: %s" , e . getMessage ( ) ) ) ; } }
public void test() { try { runTestAsSubject ( new TestOperation ( ) code_block = "" ; ) ; } catch ( Exception e ) { LOGGER . warn ( "Exception happened after test case." , e ) ; } finally { policyFilePath . delete ( ) ; } }
public static Process exec ( String ... cmd ) throws IOException { log . info ( "Runtime exec {}" , Arrays . toString ( cmd ) ) ; Process p = java . lang . Runtime . getRuntime ( ) . exec ( cmd ) ; return p ; }
public void test() { if ( dependentValues != null && ! dependentValues . contains ( dependencyValue ) ) { logger . debug ( "Dependency for {} is not satisfied because it depends on {}, which has a value of {}. Dependent values = {}" , propertyDescriptor , dependencyName , dependencyValue , dependentValues ) ; return false ; } }
public void test() { if ( dependentValues != null && ! dependentValues . contains ( dependencyValue ) ) { logger . debug ( "Dependency for {} is not satisfied because it depends on {}, which has a value of {}. Dependent values = {}" , propertyDescriptor , dependencyName , dependencyValue , dependentValues ) ; return false ; } }
public void test() { if ( dependentValues != null && ! dependentValues . contains ( dependencyValue ) ) { logger . debug ( "Dependency for {} is not satisfied because it depends on {}, which has a value of {}. Dependent values = {}" , propertyDescriptor , dependencyName , dependencyValue , dependentValues ) ; return false ; } }
public void test() { if ( dependentValues != null && ! dependentValues . contains ( dependencyValue ) ) { logger . debug ( "Dependency for {} is not satisfied because it depends on {}, which has a value of {}. Dependent values = {}" , propertyDescriptor , dependencyName , dependencyValue , dependentValues ) ; return false ; } }
public void test() { try { cswSubscribe . deleteRecordsSubscription ( filterlessSubscriptionId ) ; } catch ( CswException e ) { LOGGER . info ( "Failed to remove filterless subscription registered for id {} for csw source with id of {}" , filterlessSubscriptionId , this . getId ( ) ) ; } }
public void test() { try { conn = this . getConnection ( ) ; StringBuffer sbBuffer = new StringBuffer ( ) ; Iterator < String > it = groupNames . iterator ( ) ; boolean appendWhere = true ; String q = SEARCH_IDEAINSTANCES_ID ; code_block = IfStatement ; code_block = IfStatement ; stat = conn . prepareStatement ( q ) ; int index = 1 ; code_block = IfStatement ; code_block = IfStatement ; res = stat . executeQuery ( ) ; code_block = WhileStatement ; } catch ( Throwable t ) { _logger . error ( "error in searchIdeaInstances" , t ) ; throw new RuntimeException ( "error in searchIdeaInstances" , t ) ; } finally { closeDaoResources ( res , stat , conn ) ; } }
public String createSecurityGroup ( String clusterName , String groupName , Nova nova , Set < String > ports ) { String securityGroupUniqueName = NovaSetting . NOVA_UNIQUE_GROUP_NAME ( clusterName , groupName ) ; logger . info ( String . format ( "Creating security group '%s' ..." , securityGroupUniqueName ) ) ; SecGroupExtension group = this . novaContext . getCompute ( ) . securityGroups ( ) . create ( securityGroupUniqueName , String . format ( "Security group for hops cluster %s, node group %s" , clusterName , groupName ) ) ; code_block = IfStatement ; logger . info ( String . format ( "Security group '%s' was created :)" , securityGroupUniqueName ) ) ; return group . getId ( ) ; }
public String createSecurityGroup ( String clusterName , String groupName , Nova nova , Set < String > ports ) { String securityGroupUniqueName = NovaSetting . NOVA_UNIQUE_GROUP_NAME ( clusterName , groupName ) ; logger . info ( String . format ( "Creating security group '%s' ..." , securityGroupUniqueName ) ) ; SecGroupExtension group = this . novaContext . getCompute ( ) . securityGroups ( ) . create ( securityGroupUniqueName , String . format ( "Security group for hops cluster %s, node group %s" , clusterName , groupName ) ) ; code_block = IfStatement ; logger . info ( String . format ( "Security group '%s' was created :)" , securityGroupUniqueName ) ) ; return group . getId ( ) ; }
@ Test public void testAggregateProcessInstancesEmpty ( ) throws Exception { String xml1 = read ( this . getClass ( ) . getResourceAsStream ( "/jaxb/process-instance-empty.xml" ) ) ; String xml2 = read ( this . getClass ( ) . getResourceAsStream ( "/jaxb/process-instance-empty.xml" ) ) ; JaxbXMLResponseAggregator aggregate = new JaxbXMLResponseAggregator ( ) ; List < String > data = new ArrayList < > ( ) ; data . add ( xml1 ) ; data . add ( xml2 ) ; String result = aggregate . aggregate ( data ) ; logger . debug ( result ) ; Document xml = toXml ( result ) ; assertNotNull ( xml ) ; NodeList processes = xml . getElementsByTagName ( "process-instance-list" ) ; assertNotNull ( processes ) ; assertEquals ( 1 , processes . getLength ( ) ) ; NodeList processInstances = xml . getElementsByTagName ( "process-instance" ) ; assertNotNull ( processInstances ) ; assertEquals ( 0 , processInstances . getLength ( ) ) ; }
public void test() { try { return helper . fetchPageLimit ( sqlCountRows , sqlFetchRows , new Object [ ] code_block = "" ; , pageNo , pageSize , CONFIG_INFO_ROW_MAPPER ) ; } catch ( CannotGetJdbcConnectionException e ) { LogUtil . FATAL_LOG . error ( "[db-error] " + e . toString ( ) , e ) ; throw e ; } }
public void doSendServiceCheck ( String serviceCheckName , String status , String message , String [ ] tags ) { String tagString = "" ; code_block = IfStatement ; log . info ( serviceCheckName + tagString + " - " + System . currentTimeMillis ( ) / 1000 + " = " + status ) ; Map < String , Object > sc = new HashMap < String , Object > ( ) ; sc . put ( "name" , serviceCheckName ) ; sc . put ( "status" , status ) ; sc . put ( "message" , message ) ; sc . put ( "tags" , tags ) ; serviceChecks . add ( sc ) ; }
public void test() { try { MethodKey methodKey = new MethodKey ( CPTaxCategoryServiceUtil . class , "getCPTaxCategory" , _getCPTaxCategoryParameterTypes8 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , cpTaxCategoryId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . commerce . product . model . CPTaxCategory ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void onClusterChange ( ) { ILogger logger = loggingService . getLogger ( HazelcastInstance . class ) ; logger . info ( "Resetting local state of the client, because of a cluster change." ) ; dispose ( onClusterChangeDisposables ) ; clusterService . reset ( ) ; partitionService . reset ( ) ; connectionManager . reset ( ) ; }
public void test() { if ( fileName == null ) { LOG . info ( "Test properties was not provided, therefore not loading any test properties" ) ; return ; } }
public void test() { try ( InputStream stream = new FileInputStream ( fileName ) ) { properties . load ( stream ) ; System . getProperties ( ) . putAll ( properties ) ; } catch ( FileNotFoundException e ) { LOG . error ( "Test properties provided at {} does not exist, therefore aborting the test execution" , fileName ) ; fail ( "The given test properties file does not exist" ) ; } catch ( IOException e ) { LOG . error ( "I/O error reading the test properties at {}: {}" , fileName , e . getMessage ( ) , e ) ; fail ( "Unable to read the test properties file" ) ; } }
public void test() { try ( InputStream stream = new FileInputStream ( fileName ) ) { properties . load ( stream ) ; System . getProperties ( ) . putAll ( properties ) ; } catch ( FileNotFoundException e ) { LOG . error ( "Test properties provided at {} does not exist, therefore aborting the test execution" , fileName ) ; fail ( "The given test properties file does not exist" ) ; } catch ( IOException e ) { LOG . error ( "I/O error reading the test properties at {}: {}" , fileName , e . getMessage ( ) , e ) ; fail ( "Unable to read the test properties file" ) ; } }
public void test() { if ( timerFuture . status != HashedWheelTimerFuture . WAITING ) { log . warn ( "[HashedWheelTimer] impossible, please fix the bug" ) ; return true ; } }
public void test() { if ( timerFuture . totalTicks < currentTick ) { log . warn ( "[HashedWheelTimer] timerFuture.totalTicks < currentTick, please fix the bug" ) ; } }
private void autoStart ( ) throws TException { log . info ( "Auto-starting scheduling tasks in schedule service..." ) ; String [ ] servicesToSchedule = ScheduleConstants . autostartServices ; code_block = ForStatement ; log . info ( "Auto-start completed." ) ; }
private void autoStart ( ) throws TException { log . info ( "Auto-starting scheduling tasks in schedule service..." ) ; String [ ] servicesToSchedule = ScheduleConstants . autostartServices ; code_block = ForStatement ; log . info ( "Auto-start completed." ) ; }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceChannelServiceUtil . class , "fetchCommerceChannel" , _fetchCommerceChannelParameterTypes3 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , commerceChannelId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . commerce . product . model . CommerceChannel ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( rc != BKException . Code . OK ) { LOG . error ( "Error processing auth message, closing connection" ) ; channel . close ( ) ; return ; } }
public void test() { try { final FileSystem fileSystem = zip . getFileSystem ( configuration ) ; long size = 0L ; final byte [ ] buffer = new byte [ 1 << 13 ] ; progressable . progress ( ) ; code_block = TryStatement ;  progressable . progress ( ) ; return size ; } catch ( IOException | RuntimeException exception ) { log . error ( exception , "Exception in unzip retry loop" ) ; throw exception ; } }
public void test() { try { autoPingManager . release ( ) ; bookmarkManager . release ( ) ; mediaFileManager . release ( ) ; fileContentManager . release ( ) ; pingTargetManager . release ( ) ; pingQueueManager . release ( ) ; pluginManager . release ( ) ; threadManager . release ( ) ; userManager . release ( ) ; weblogManager . release ( ) ; } catch ( Exception e ) { log . error ( "Error calling Roller.release()" , e ) ; } }
@ ExceptionHandler ( InvalidSearchParamException . class ) @ ResponseStatus ( value = HttpStatus . BAD_REQUEST ) public ViewObjectErrorResponse invalidSearchParamExceptionHandler ( InvalidSearchParamException ex ) { logger . debug ( "Invalid Search Param Exception: " , ex ) ; return new ViewObjectErrorResponse ( ErrorCode . SEARCH_ERROR , ex . getMessage ( ) ) ; }
public void attachClean ( TmpItvSel instance ) { log . debug ( "attaching clean TmpItvSel instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . lock ( instance , LockMode . NONE ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
@ Override public MigrationPublishResult doMigrationPublish ( String clusterName , String shardName , String primaryDcName , InetSocketAddress newMaster ) throws OuterClientException { logger . info ( "[doMigrationPublish]Cluster:{}, Shard:{}, NewPrimaryDc:{}, NewMaster:{}" , clusterName , shardName , primaryDcName , newMaster ) ; String startTime = DateTimeUtils . currentTimeAsString ( ) ; MigrationPublishResult res = new MigrationPublishResult ( "default-addr" , clusterName , primaryDcName , Arrays . asList ( newMaster ) ) ; String endTime = DateTimeUtils . currentTimeAsString ( ) ; res . setSuccess ( true ) ; res . setMessage ( "default-success" ) ; res . setStartTime ( startTime ) ; res . setEndTime ( endTime ) ; return res ; }
public void test() { try { return mapper . writeValueAsString ( mapper . createObjectNode ( ) . put ( "message" , getMessage ( ) ) ) ; } catch ( JsonProcessingException e ) { log . warn ( "Failed to serialize exception " , e ) ; throw new RuntimeException ( e ) ; } }
private void writePSKIdentityHintLength ( PskEcDheServerKeyExchangeMessage msg ) { appendInt ( msg . getIdentityHintLength ( ) . getValue ( ) , HandshakeByteLength . PSK_IDENTITY_LENGTH ) ; LOGGER . debug ( "SerializedPSKIdentityLength: " + msg . getIdentityHintLength ( ) . getValue ( ) ) ; }
public void test() { try { return this . createEntityFromAPI ( getApiUrl ( ) + SESSION_URL + id , token ) ; } catch ( JsonSyntaxException e ) { LOGGER . warn ( "Error occured while getting session" , e ) ; return new GenericEntity ( ) ; } }
@ ApiOperation ( value = "get history by user's id" ) @ GetMapping ( CommonConstants . PATH_LOGBOOK ) public LogbookOperationsResponseDto findHistoryById ( final @ PathVariable String id ) { LOGGER . debug ( "get logbook for users with id :{}" , id ) ; SanityChecker . check ( id ) ; return service . findHistoryById ( buildUiHttpContext ( ) , id ) ; }
public void test() { try { return InetAddress . getByName ( ip ) . getCanonicalHostName ( ) ; } catch ( UnknownHostException ex ) { LOGGER . warn ( "Could not perform reverse DNS for \"" + ip + "\"" , ex ) ; return ip ; } }
public void test() { try { SwingUtilities . invokeAndWait ( this :: createFrame ) ; } catch ( InterruptedException | InvocationTargetException ex ) { log . error ( "Exception creating system console frame: {}" , ex ) ; } }
public void testDatasourceUpdate ( ) throws Exception { TestContext context = new TestContext ( ) ; Map < String , String > overlay = context . getUniqueOverlay ( ) ; String filePath = TestContext . overlayParametersOverTemplate ( TestContext . CLUSTER_TEMPLATE , overlay ) ; context . setCluster ( filePath ) ; LOG . info ( "entity -submit -type cluster -file " + filePath ) ; Assert . assertEquals ( TestContext . executeWithURL ( "entity -submit -type cluster -file " + filePath ) , 0 ) ; String dsName = "datasource-test-1" ; overlay . put ( DATASOURCE_NAME_KEY , dsName ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . DATASOURCE_TEMPLATE1 , overlay ) ; LOG . info ( "Submit datatsource entity {} via entity -submit -type datasource -file {}" , dsName , filePath ) ; Assert . assertEquals ( TestContext . executeWithURL ( "entity -submit -type datasource -file " + filePath ) , 0 ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . FEED_TEMPLATE3 , overlay ) ; LOG . info ( "Submit feed with datasource {} via entity -submitAndSchedule -type feed -file {}" , dsName , filePath ) ; Assert . assertEquals ( 0 , TestContext . executeWithURL ( "entity -submitAndSchedule -type feed -file " + filePath ) ) ; overlay . put ( DATASOURCE_NAME_KEY , dsName ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . DATASOURCE_TEMPLATE5 , overlay ) ; LOG . info ( "update datasource {} via -update -type datasource -file {}" , dsName , filePath ) ; Assert . assertEquals ( 0 , TestContext . executeWithURL ( "entity -update -type datasource -file " + filePath ) ) ; }
public void testDatasourceUpdate ( ) throws Exception { TestContext context = new TestContext ( ) ; Map < String , String > overlay = context . getUniqueOverlay ( ) ; String filePath = TestContext . overlayParametersOverTemplate ( TestContext . CLUSTER_TEMPLATE , overlay ) ; context . setCluster ( filePath ) ; LOG . info ( "entity -submit -type cluster -file " + filePath ) ; Assert . assertEquals ( TestContext . executeWithURL ( "entity -submit -type cluster -file " + filePath ) , 0 ) ; String dsName = "datasource-test-1" ; overlay . put ( DATASOURCE_NAME_KEY , dsName ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . DATASOURCE_TEMPLATE1 , overlay ) ; LOG . info ( "Submit datatsource entity {} via entity -submit -type datasource -file {}" , dsName , filePath ) ; Assert . assertEquals ( TestContext . executeWithURL ( "entity -submit -type datasource -file " + filePath ) , 0 ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . FEED_TEMPLATE3 , overlay ) ; LOG . info ( "Submit feed with datasource {} via entity -submitAndSchedule -type feed -file {}" , dsName , filePath ) ; Assert . assertEquals ( 0 , TestContext . executeWithURL ( "entity -submitAndSchedule -type feed -file " + filePath ) ) ; overlay . put ( DATASOURCE_NAME_KEY , dsName ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . DATASOURCE_TEMPLATE5 , overlay ) ; LOG . info ( "update datasource {} via -update -type datasource -file {}" , dsName , filePath ) ; Assert . assertEquals ( 0 , TestContext . executeWithURL ( "entity -update -type datasource -file " + filePath ) ) ; }
public void testDatasourceUpdate ( ) throws Exception { TestContext context = new TestContext ( ) ; Map < String , String > overlay = context . getUniqueOverlay ( ) ; String filePath = TestContext . overlayParametersOverTemplate ( TestContext . CLUSTER_TEMPLATE , overlay ) ; context . setCluster ( filePath ) ; LOG . info ( "entity -submit -type cluster -file " + filePath ) ; Assert . assertEquals ( TestContext . executeWithURL ( "entity -submit -type cluster -file " + filePath ) , 0 ) ; String dsName = "datasource-test-1" ; overlay . put ( DATASOURCE_NAME_KEY , dsName ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . DATASOURCE_TEMPLATE1 , overlay ) ; LOG . info ( "Submit datatsource entity {} via entity -submit -type datasource -file {}" , dsName , filePath ) ; Assert . assertEquals ( TestContext . executeWithURL ( "entity -submit -type datasource -file " + filePath ) , 0 ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . FEED_TEMPLATE3 , overlay ) ; LOG . info ( "Submit feed with datasource {} via entity -submitAndSchedule -type feed -file {}" , dsName , filePath ) ; Assert . assertEquals ( 0 , TestContext . executeWithURL ( "entity -submitAndSchedule -type feed -file " + filePath ) ) ; overlay . put ( DATASOURCE_NAME_KEY , dsName ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . DATASOURCE_TEMPLATE5 , overlay ) ; LOG . info ( "update datasource {} via -update -type datasource -file {}" , dsName , filePath ) ; Assert . assertEquals ( 0 , TestContext . executeWithURL ( "entity -update -type datasource -file " + filePath ) ) ; }
public void testDatasourceUpdate ( ) throws Exception { TestContext context = new TestContext ( ) ; Map < String , String > overlay = context . getUniqueOverlay ( ) ; String filePath = TestContext . overlayParametersOverTemplate ( TestContext . CLUSTER_TEMPLATE , overlay ) ; context . setCluster ( filePath ) ; LOG . info ( "entity -submit -type cluster -file " + filePath ) ; Assert . assertEquals ( TestContext . executeWithURL ( "entity -submit -type cluster -file " + filePath ) , 0 ) ; String dsName = "datasource-test-1" ; overlay . put ( DATASOURCE_NAME_KEY , dsName ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . DATASOURCE_TEMPLATE1 , overlay ) ; LOG . info ( "Submit datatsource entity {} via entity -submit -type datasource -file {}" , dsName , filePath ) ; Assert . assertEquals ( TestContext . executeWithURL ( "entity -submit -type datasource -file " + filePath ) , 0 ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . FEED_TEMPLATE3 , overlay ) ; LOG . info ( "Submit feed with datasource {} via entity -submitAndSchedule -type feed -file {}" , dsName , filePath ) ; Assert . assertEquals ( 0 , TestContext . executeWithURL ( "entity -submitAndSchedule -type feed -file " + filePath ) ) ; overlay . put ( DATASOURCE_NAME_KEY , dsName ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . DATASOURCE_TEMPLATE5 , overlay ) ; LOG . info ( "update datasource {} via -update -type datasource -file {}" , dsName , filePath ) ; Assert . assertEquals ( 0 , TestContext . executeWithURL ( "entity -update -type datasource -file " + filePath ) ) ; }
public void test() { try { executeStmt ( nodeStateDeleteSQL , new Object [ ] code_block = "" ; ) ; } catch ( Exception e ) { String msg = "failed to delete node state: " + state . getNodeId ( ) ; log . error ( msg , e ) ; throw new ItemStateException ( msg , e ) ; } }
public void test() { if ( isReservedWord ( name ) ) { LOGGER . warn ( name + " (reserved word) cannot be used as parameter name. Renamed to " + name + "_" ) ; name = name + "_" ; } }
public void test() { try { String loginToken = session . login ( session . getAccessToken ( ) ) ; LOG . info ( "First token " + loginToken ) ; assertTrue ( onLoginTriggered , "SalesforceSessionListener onLogin NOT called" ) ; onLoginTriggered = false ; loginToken = session . login ( loginToken ) ; LOG . info ( "Refreshed token " + loginToken ) ; assertTrue ( onLogoutTriggered , "SalesforceSessionListener onLogout NOT called" ) ; assertTrue ( onLoginTriggered , "SalesforceSessionListener onLogin NOT called" ) ; } finally { session . logout ( ) ; } }
public void test() { try { String loginToken = session . login ( session . getAccessToken ( ) ) ; LOG . info ( "First token " + loginToken ) ; assertTrue ( onLoginTriggered , "SalesforceSessionListener onLogin NOT called" ) ; onLoginTriggered = false ; loginToken = session . login ( loginToken ) ; LOG . info ( "Refreshed token " + loginToken ) ; assertTrue ( onLogoutTriggered , "SalesforceSessionListener onLogout NOT called" ) ; assertTrue ( onLoginTriggered , "SalesforceSessionListener onLogin NOT called" ) ; } finally { session . logout ( ) ; } }
public void test() { if ( protectionSystems . isEmpty ( ) ) { _log . info ( String . format ( "RP Placement: Storage pool %s does not have connectivity to a protection system." , storagePool . getLabel ( ) ) ) ; } }
public void test() { try { Thread . sleep ( 2000 ) ; log . debug ( "saving {} positions of {} servos" , filename , positions . size ( ) ) ; FileIO . toFile ( filename , CodecUtils . toJson ( positions ) . getBytes ( ) ) ; } catch ( Exception e ) { log . error ( "could not save servo positions" , e ) ; } }
public void test() { try { Thread . sleep ( 2000 ) ; log . debug ( "saving {} positions of {} servos" , filename , positions . size ( ) ) ; FileIO . toFile ( filename , CodecUtils . toJson ( positions ) . getBytes ( ) ) ; } catch ( Exception e ) { log . error ( "could not save servo positions" , e ) ; } }
public void test() { if ( hls ) { return new Transcoding ( null , "hls" , mediaFile . getFormat ( ) , "ts" , settingsService . getHlsCommand ( ) , null , null , true ) ; } }
public void test() { if ( FORMAT_RAW . equals ( preferredTargetFormat ) ) { return null ; } }
public void test() { if ( LOGGER . isTraceEnabled ( ) ) { LOGGER . trace ( format ( "Select async with execution info : %s" , statementWrapper . getBoundStatement ( ) . preparedStatement ( ) . getQueryString ( ) ) ) ; } }
@ Test public void testQuery ( ) throws Exception { com . google . api . services . calendar . model . FreeBusyRequest request = new FreeBusyRequest ( ) ; List < FreeBusyRequestItem > items = new ArrayList < > ( ) ; items . add ( new FreeBusyRequestItem ( ) . setId ( getCalendar ( ) . getId ( ) ) ) ; request . setItems ( items ) ; request . setTimeMin ( DateTime . parseRfc3339 ( "2014-11-10T20:45:30-00:00" ) ) ; request . setTimeMax ( DateTime . parseRfc3339 ( "2014-11-10T21:45:30-00:00" ) ) ; final com . google . api . services . calendar . model . FreeBusyResponse result = requestBody ( "direct://QUERY" , request ) ; assertNotNull ( result , "query result" ) ; LOG . debug ( "query: " + result ) ; }
protected void processAffinityGroup ( AffinityGroupVMMapVO vmGroupMapping , DeploymentPlan plan , VirtualMachine vm ) { AffinityGroupVO group = _affinityGroupDao . findById ( vmGroupMapping . getAffinityGroupId ( ) ) ; s_logger . debug ( "Processing affinity group " + group . getName ( ) + " for VM Id: " + vm . getId ( ) ) ; List < Long > groupVMIds = _affinityGroupVMMapDao . listVmIdsByAffinityGroup ( group . getId ( ) ) ; groupVMIds . remove ( vm . getId ( ) ) ; List < Long > preferredHosts = getPreferredHostsFromGroupVMIds ( groupVMIds ) ; plan . setPreferredHosts ( preferredHosts ) ; }
public void test() { try { BaremetalRctResponse rsp = vlanMgr . addRct ( this ) ; this . setResponseObject ( rsp ) ; } catch ( Exception e ) { s_logger . warn ( String . format ( "unable to add baremetal RCT[%s]" , getRctUrl ( ) ) , e ) ; throw new ServerApiException ( ApiErrorCode . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { try { Thread . sleep ( 5000 ) ; } catch ( Exception e ) { LOGGER . error ( "error trying to sleep waiting for external view to change : " , e ) ; } }
public void test() { if ( duration > 1000 ) { LOG . debug ( "{} - Unlocking {} after {}ms" , name , requestId , duration ) ; } else { LOG . trace ( "{} - Unlocking {} after {}ms" , name , requestId , duration ) ; } }
public void test() { if ( duration > 1000 ) { LOG . debug ( "{} - Unlocking {} after {}ms" , name , requestId , duration ) ; } else { LOG . trace ( "{} - Unlocking {} after {}ms" , name , requestId , duration ) ; } }
public void test() { if ( ! nonEmptyDirs . isEmpty ( ) ) { LOG . error ( "Not all the new directories are empty. New directories that are not empty are: " + nonEmptyDirs ) ; throw new InvalidCookieException ( ) ; } }
public void test() { try { code_block = TryStatement ;  } catch ( IOException e ) { String errMsg = "Trouble copying inputstream " + in + " to outputstream " + out ; log . warn ( errMsg , e ) ; throw new IOFailure ( errMsg , e ) ; } }
public void test() { if ( ! file . delete ( ) ) { logger . warn ( "Failed to delete: {}" , file ) ; } }
public void test() { try { VALUE value = TypeCastUtility . castValue ( XmlUtility . getObjectAttribute ( x , "value" ) , getHolderType ( ) ) ; setValue ( value ) ; } catch ( Exception e ) { LOG . warn ( "Could not load form XML [{}]" , getClass ( ) . getName ( ) , e ) ; } }
public void test() { try { return new URL ( defaultUrl . get ( ) ) ; } catch ( IllegalArgumentException e1 ) { log . warn ( "Parameter `default` for to_url() is not a valid URL: {}" , defaultUrl . get ( ) ) ; throw Throwables . propagate ( e1 ) ; } }
public void test() { try { luceneQuery = inQueryParser . parse ( nativeSearchString ) ; } catch ( ParseException e ) { String message = "Unable to parse query: '" + nativeSearchString + "'" ; log . error ( message ) ; throw new RuntimeException ( message ) ; } }
@ Override public synchronized void run ( ) { log . debug ( "** Failing connection" ) ; session . getConnection ( ) . fail ( new ActiveMQNotConnectedException ( "oops" ) ) ; log . debug ( "** Fail complete" ) ; cancel ( ) ; executed = true ; }
@ Override public synchronized void run ( ) { log . debug ( "** Failing connection" ) ; session . getConnection ( ) . fail ( new ActiveMQNotConnectedException ( "oops" ) ) ; log . debug ( "** Fail complete" ) ; cancel ( ) ; executed = true ; }
public void test() { try { testUser = TestUtils . setupUser ( "wtTestUser" ) ; testWeblog = TestUtils . setupWeblog ( "wtTestWeblog" , testUser ) ; TestUtils . endSession ( true ) ; } catch ( Exception ex ) { log . error ( ex ) ; throw new Exception ( "Test setup failed" , ex ) ; } }
public void test() { try { CopyWriter copyWriter = sourceBlob . copyTo ( destination ) ; copied = copyWriter . getResult ( ) ; LOG . debug ( "Successfully copied {} to {}." , toPath ( sourceBlob . getBlobId ( ) ) , toPath ( destination ) ) ; } catch ( StorageException e ) { throw new RuntimeException ( String . format ( "Unable to access source bucket %s. " , destPath . getBucket ( ) ) + "Ensure you entered the correct bucket path." , e ) ; } }
public void test() { try { return Optional . of ( generateReport ( metacard ) ) ; } catch ( ValidationExceptionImpl e ) { LOGGER . warn ( "Exception validating metacard ID {}" , metacard . getId ( ) , e ) ; return Optional . empty ( ) ; } }
public void test() { if ( currentOp . getState ( ) == OperationState . COMPLETE ) { getLogger ( ) . debug ( "Completed read op: %s and giving the next %d " + "bytes" , currentOp , rbuf . remaining ( ) ) ; Operation op = node . removeCurrentReadOp ( ) ; assert op == currentOp : "Expected to pop " + currentOp + " got " + op ; code_block = IfStatement ; } else-if ( currentOp . getState ( ) == OperationState . RETRY ) { handleRetryInformation ( currentOp . getErrorMsg ( ) ) ; getLogger ( ) . debug ( "Reschedule read op due to NOT_MY_VBUCKET error: " + "%s " , currentOp ) ; ( ( VBucketAware ) currentOp ) . addNotMyVbucketNode ( currentOp . getHandlingNode ( ) ) ; Operation op = node . removeCurrentReadOp ( ) ; assert op == currentOp : "Expected to pop " + currentOp + " got " + op ; retryOperation ( currentOp ) ; metrics . markMeter ( OVERALL_RESPONSE_RETRY_METRIC ) ; } }
public void test() { -> { logger . warn ( "Checkout conflicts warning" ) ; EventHandler handler = event -> quickStashSave ( ) ; this . notificationPaneController . addNotification ( "You can't switch to that branch because there would be a merge conflict. " + "Stash your changes or resolve conflicts first." , "stash" , handler ) ; } }
@ Override public void processRecord ( ARCRecord sar , OutputStream os ) { log . trace ( "Processing ARCRecord with offset: {}" , sar . getMetaData ( ) . getOffset ( ) ) ; Map < String , String > fieldsread = new HashMap < String , String > ( ) ; fieldsread . put ( "A" , sar . getMetaData ( ) . getUrl ( ) ) ; fieldsread . put ( "e" , sar . getMetaData ( ) . getIp ( ) ) ; fieldsread . put ( "b" , sar . getMetaData ( ) . getDate ( ) ) ; fieldsread . put ( "m" , sar . getMetaData ( ) . getMimetype ( ) ) ; fieldsread . put ( "n" , Long . toString ( sar . getMetaData ( ) . getLength ( ) ) ) ; fieldsread . put ( "v" , Long . toString ( sar . getMetaData ( ) . getOffset ( ) ) ) ; fieldsread . put ( "g" , sar . getMetaData ( ) . getArcFile ( ) . getName ( ) ) ; code_block = IfStatement ; printFields ( fieldsread , os ) ; }
public void test() { try { Map < String , Object > params = new HashMap < > ( ) ; params . put ( "tableName" , tableName ) ; params . put ( "fieldName" , fieldName ) ; params . put ( "fieldVal" , fieldVal ) ; params . put ( "equalFieldName" , equalFieldName ) ; params . put ( "equalFieldVal" , equalFieldVal ) ; params . put ( "dataId" , dataId ) ; count = systemMapper . duplicateCheck ( params ) ; } catch ( Exception e ) { LOG . error ( e . getMessage ( ) , e ) ; } finally { sqlSession . close ( ) ; } }
public void test() { if ( count == null || count == 0 ) { LOG . info ( "This value is available" ) ; return new JsonResponse . Builder < > ( Response . Status . OK ) . message ( "This value is available!" ) . success ( true ) . build ( ) ; } else { LOG . info ( "This value already exists is not available!" ) ; return new JsonResponse . Builder < > ( Response . Status . OK ) . message ( "This value already exists is not available!" ) . success ( false ) . build ( ) ; } }
public void test() { if ( count == null || count == 0 ) { LOG . info ( "This value is available" ) ; return new JsonResponse . Builder < > ( Response . Status . OK ) . message ( "This value is available!" ) . success ( true ) . build ( ) ; } else { LOG . info ( "This value already exists is not available!" ) ; return new JsonResponse . Builder < > ( Response . Status . OK ) . message ( "This value already exists is not available!" ) . success ( false ) . build ( ) ; } }
public void test() { if ( resolvedMoveThreadCount > availableProcessorCount ) { LOGGER . warn ( "The resolvedMoveThreadCount ({}) is higher " + "than the availableProcessorCount ({}), which is counter-efficient." , resolvedMoveThreadCount , availableProcessorCount ) ; } }
public void test() { try { String body = message . getBody ( ) ; code_block = IfStatement ; String from = message . getFrom ( ) ; code_block = IfStatement ; long fromUserId = UserLocalServiceUtil . getUserIdByScreenName ( _companyId , JabberUtil . getScreenName ( from ) ) ; EntryLocalServiceUtil . addEntry ( fromUserId , _userId , body ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; } }
public void test() { if ( getPort ( ) == getPortDefaultValue ( ) ) { log . info ( "server listens on standard secure port [{}:{}]" , bindAddress , server . actualPort ( ) ) ; } else { log . warn ( "server listens on non-standard secure port [{}:{}], default is {}" , bindAddress , server . actualPort ( ) , getPortDefaultValue ( ) ) ; } }
public void test() { if ( getPort ( ) == getPortDefaultValue ( ) ) { log . info ( "server listens on standard secure port [{}:{}]" , bindAddress , server . actualPort ( ) ) ; } else { log . warn ( "server listens on non-standard secure port [{}:{}], default is {}" , bindAddress , server . actualPort ( ) , getPortDefaultValue ( ) ) ; } }
public void test() { try { MDC . put ( "destination" , destination ) ; applyResult = applySnapshotToDB ( lastPosition , false ) ; } catch ( Throwable e ) { logger . error ( "scheudle applySnapshotToDB faield" , e ) ; } }
public void test() { try { MDC . put ( "destination" , destination ) ; code_block = IfStatement ; } catch ( Throwable e ) { logger . error ( "scheudle snapshotExpire faield" , e ) ; } }
public void test() { try { ConfigurationRequest . Builder builder = ConfigurationRequest . newBuilder ( ) . setClusterName ( settings . getClusterName ( ) ) ; code_block = IfStatement ; ConfigurationResponse response = stub . call ( builder . build ( ) ) ; String responseUuid = response . getUuid ( ) ; code_block = IfStatement ; response . getConfigTableList ( ) . forEach ( config code_block = LoopStatement ; ) ; this . uuid = responseUuid ; } catch ( Exception e ) { LOGGER . error ( "Remote config center [" + settings + "] is not available." , e ) ; } }
@ Test public void testRangeOpsInDiffSubTree ( ) throws Exception { log . info ( "------  testRangeOpsInDiffSubTree  ------" ) ; String city = TestCities . rome . name ( ) ; String query = "(" + CityField . NUM . name ( ) + LTE_OP + "100" + AND_OP + CityField . CITY . name ( ) + EQ_OP + "'" + city + "')" + AND_OP + CityField . NUM . name ( ) + GTE_OP + "100" ; String expected = CityField . NUM . name ( ) + LTE_OP + "'+cE1'" + JEXL_AND_OP + CityField . CITY . name ( ) + EQ_OP + "'" + city + "'" + JEXL_AND_OP + CityField . NUM . name ( ) + GTE_OP + "'+cE1'" ; String plan = getPlan ( query , true , true ) ; assertPlanEquals ( expected , plan ) ; expected = CityField . NUM . name ( ) + LTE_OP + "'+cE1'" + JEXL_AND_OP + CityField . CITY . name ( ) + EQ_OP + "'" + city + "'" + JEXL_AND_OP + CityField . NUM . name ( ) + GTE_OP + "'+cE1'" ; plan = getPlan ( query , true , false ) ; assertPlanEquals ( expected , plan ) ; expected = CityField . NUM . name ( ) + LTE_OP + "'+cE1'" + JEXL_AND_OP + CityField . CITY . name ( ) + EQ_OP + "'" + city + "'" + JEXL_AND_OP + CityField . NUM . name ( ) + GTE_OP + "'+cE1'" ; plan = getPlan ( query , false , true ) ; assertPlanEquals ( expected , plan ) ; expected = query ; runTest ( query , expected ) ; }
public void test() { if ( LOGGER . isTraceEnabled ( ) ) { LOGGER . trace ( String . format ( "Is '%s' Update statement ? " , statement . toString ( ) ) ) ; } }
public void test() { try { String result = this . checkMethod ( ) ; if ( null != result ) return result ; this . resetMethodStatus ( this . extractMethod ( ) ) ; } catch ( Throwable t ) { _logger . error ( "Error resetting method status" , t ) ; return FAILURE ; } }
public void test() { if ( joinRecords . size ( ) > 0 ) { AsyncJobJoinMapVO joinRecord = joinRecords . get ( 0 ) ; code_block = ForStatement ; } else { s_logger . warn ( "job-" + job . getId ( ) + " is scheduled for wakeup run, but there is no joining info anymore" ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Animation plan set to " + plan ) ; } }
public void shutdown ( ) { log . info ( "Shutting down thread monitoring tables." ) ; shutdownLatch . countDown ( ) ; }
public static void destroyInstance ( ) { checkState ( instance != null , "createInstance() must be called prior to destroyInstance()" ) ; instance . stop ( ) ; instance = null ; LOG . info ( "Destroyed WebSocketServer." ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Invoke a Change Listener" ) ; log . debugf ( "Previous: %s" , previous ) ; log . debugf ( "Current: %s" , current ) ; log . debugf ( "Partition: %s" , partition ) ; log . debugf ( "Added: %s" , addedRemoved . get ( "added" ) ) ; log . debugf ( "Removed: %s" , addedRemoved . get ( "removed" ) ) ; } }
public void test() { try { jwt = jwt . substring ( jwt . indexOf ( ' ' ) + 1 ) ; Jws < Claims > claims = Jwts . parser ( ) . setSigningKey ( ShiroJwtProvider . SIGNING_KEY ) . parseClaimsJws ( jwt ) ; String user = claims . getBody ( ) . getSubject ( ) ; return Strings . hasText ( user ) ? Optional . of ( user ) : Optional . empty ( ) ; } catch ( JwtException | IllegalArgumentException e ) { LOG . error ( "Failed validating JWT {} from {}" , jwt , WebUtils . toHttp ( req ) . getRemoteAddr ( ) ) ; LOG . debug ( "exception" , e ) ; } }
public void test() { try { jwt = jwt . substring ( jwt . indexOf ( ' ' ) + 1 ) ; Jws < Claims > claims = Jwts . parser ( ) . setSigningKey ( ShiroJwtProvider . SIGNING_KEY ) . parseClaimsJws ( jwt ) ; String user = claims . getBody ( ) . getSubject ( ) ; return Strings . hasText ( user ) ? Optional . of ( user ) : Optional . empty ( ) ; } catch ( JwtException | IllegalArgumentException e ) { LOG . error ( "Failed validating JWT {} from {}" , jwt , WebUtils . toHttp ( req ) . getRemoteAddr ( ) ) ; LOG . debug ( "exception" , e ) ; } }
private void updateTwcsDtcsGcSeconds ( ) throws Exception { logger . info ( "updating gc_grace_seconds on TWCS/DTCS tables..." ) ; code_block = ForStatement ; logger . info ( "updating gc_grace_seconds on TWCS/DTCS tables - complete" ) ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( sb . toString ( ) ) ; } }
public void test() { if ( datasetWithEtag . isDeleted ( ) ) { logger . debug ( "sending status 410 GONE" ) ; return new ResponseEntity < > ( headers , HttpStatus . GONE ) ; } else-if ( datasetWithEtag . isNotFound ( ) ) { logger . debug ( "sending status 404 NOT FOUND" ) ; return new ResponseEntity < > ( headers , HttpStatus . NOT_FOUND ) ; } else-if ( datasetWithEtag . isForbidden ( ) ) { int statusCode = HttpStatus . FORBIDDEN . value ( ) ; Optional < AclEvalResult > accResult = WonAclRequestHelper . getWonAclEvaluationContext ( request ) . getCombinedResults ( ) ; code_block = IfStatement ; HttpStatus status = HttpStatus . valueOf ( statusCode ) ; logger . debug ( "sending status {}" , status ) ; return new ResponseEntity < > ( headers , status ) ; } else-if ( datasetWithEtag . isChanged ( ) ) { logger . debug ( "sending status 200 OK" ) ; return new ResponseEntity < > ( datasetWithEtag . getData ( ) , headers , HttpStatus . OK ) ; } else { logger . debug ( "sending status 304 NOT MODIFIED" ) ; return new ResponseEntity < > ( headers , HttpStatus . NOT_MODIFIED ) ; } }
public void test() { if ( datasetWithEtag . isDeleted ( ) ) { logger . debug ( "sending status 410 GONE" ) ; return new ResponseEntity < > ( headers , HttpStatus . GONE ) ; } else-if ( datasetWithEtag . isNotFound ( ) ) { logger . debug ( "sending status 404 NOT FOUND" ) ; return new ResponseEntity < > ( headers , HttpStatus . NOT_FOUND ) ; } else-if ( datasetWithEtag . isForbidden ( ) ) { int statusCode = HttpStatus . FORBIDDEN . value ( ) ; Optional < AclEvalResult > accResult = WonAclRequestHelper . getWonAclEvaluationContext ( request ) . getCombinedResults ( ) ; code_block = IfStatement ; HttpStatus status = HttpStatus . valueOf ( statusCode ) ; logger . debug ( "sending status {}" , status ) ; return new ResponseEntity < > ( headers , status ) ; } else-if ( datasetWithEtag . isChanged ( ) ) { logger . debug ( "sending status 200 OK" ) ; return new ResponseEntity < > ( datasetWithEtag . getData ( ) , headers , HttpStatus . OK ) ; } else { logger . debug ( "sending status 304 NOT MODIFIED" ) ; return new ResponseEntity < > ( headers , HttpStatus . NOT_MODIFIED ) ; } }
public void test() { if ( datasetWithEtag . isDeleted ( ) ) { logger . debug ( "sending status 410 GONE" ) ; return new ResponseEntity < > ( headers , HttpStatus . GONE ) ; } else-if ( datasetWithEtag . isNotFound ( ) ) { logger . debug ( "sending status 404 NOT FOUND" ) ; return new ResponseEntity < > ( headers , HttpStatus . NOT_FOUND ) ; } else-if ( datasetWithEtag . isForbidden ( ) ) { int statusCode = HttpStatus . FORBIDDEN . value ( ) ; Optional < AclEvalResult > accResult = WonAclRequestHelper . getWonAclEvaluationContext ( request ) . getCombinedResults ( ) ; code_block = IfStatement ; HttpStatus status = HttpStatus . valueOf ( statusCode ) ; logger . debug ( "sending status {}" , status ) ; return new ResponseEntity < > ( headers , status ) ; } else-if ( datasetWithEtag . isChanged ( ) ) { logger . debug ( "sending status 200 OK" ) ; return new ResponseEntity < > ( datasetWithEtag . getData ( ) , headers , HttpStatus . OK ) ; } else { logger . debug ( "sending status 304 NOT MODIFIED" ) ; return new ResponseEntity < > ( headers , HttpStatus . NOT_MODIFIED ) ; } }
public void test() { if ( datasetWithEtag . isDeleted ( ) ) { logger . debug ( "sending status 410 GONE" ) ; return new ResponseEntity < > ( headers , HttpStatus . GONE ) ; } else-if ( datasetWithEtag . isNotFound ( ) ) { logger . debug ( "sending status 404 NOT FOUND" ) ; return new ResponseEntity < > ( headers , HttpStatus . NOT_FOUND ) ; } else-if ( datasetWithEtag . isForbidden ( ) ) { int statusCode = HttpStatus . FORBIDDEN . value ( ) ; Optional < AclEvalResult > accResult = WonAclRequestHelper . getWonAclEvaluationContext ( request ) . getCombinedResults ( ) ; code_block = IfStatement ; HttpStatus status = HttpStatus . valueOf ( statusCode ) ; logger . debug ( "sending status {}" , status ) ; return new ResponseEntity < > ( headers , status ) ; } else-if ( datasetWithEtag . isChanged ( ) ) { logger . debug ( "sending status 200 OK" ) ; return new ResponseEntity < > ( datasetWithEtag . getData ( ) , headers , HttpStatus . OK ) ; } else { logger . debug ( "sending status 304 NOT MODIFIED" ) ; return new ResponseEntity < > ( headers , HttpStatus . NOT_MODIFIED ) ; } }
public void test() { if ( datasetWithEtag . isDeleted ( ) ) { logger . debug ( "sending status 410 GONE" ) ; return new ResponseEntity < > ( headers , HttpStatus . GONE ) ; } else-if ( datasetWithEtag . isNotFound ( ) ) { logger . debug ( "sending status 404 NOT FOUND" ) ; return new ResponseEntity < > ( headers , HttpStatus . NOT_FOUND ) ; } else-if ( datasetWithEtag . isForbidden ( ) ) { int statusCode = HttpStatus . FORBIDDEN . value ( ) ; Optional < AclEvalResult > accResult = WonAclRequestHelper . getWonAclEvaluationContext ( request ) . getCombinedResults ( ) ; code_block = IfStatement ; HttpStatus status = HttpStatus . valueOf ( statusCode ) ; logger . debug ( "sending status {}" , status ) ; return new ResponseEntity < > ( headers , status ) ; } else-if ( datasetWithEtag . isChanged ( ) ) { logger . debug ( "sending status 200 OK" ) ; return new ResponseEntity < > ( datasetWithEtag . getData ( ) , headers , HttpStatus . OK ) ; } else { logger . debug ( "sending status 304 NOT MODIFIED" ) ; return new ResponseEntity < > ( headers , HttpStatus . NOT_MODIFIED ) ; } }
@ BeforeClass public static void setUp ( ) throws Exception { log . warn ( "Spinning up test cluster..." ) ; new File ( "target/test-logs" ) . mkdirs ( ) ; System . setProperty ( "hadoop.log.dir" , "target/test-logs" ) ; System . setProperty ( "javax.xml.parsers.SAXParserFactory" , "com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl" ) ; Configuration conf = new Configuration ( ) ; dfsCluster = new MiniDFSCluster ( conf , 1 , true , null ) ; dfsCluster . getFileSystem ( ) . makeQualified ( input ) ; dfsCluster . getFileSystem ( ) . makeQualified ( output ) ; mrCluster = new MiniMRCluster ( 1 , getFileSystem ( ) . getUri ( ) . toString ( ) , 1 ) ; code_block = ForStatement ; log . warn ( "Spun up test cluster." ) ; }
@ BeforeClass public static void setUp ( ) throws Exception { log . warn ( "Spinning up test cluster..." ) ; new File ( "target/test-logs" ) . mkdirs ( ) ; System . setProperty ( "hadoop.log.dir" , "target/test-logs" ) ; System . setProperty ( "javax.xml.parsers.SAXParserFactory" , "com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl" ) ; Configuration conf = new Configuration ( ) ; dfsCluster = new MiniDFSCluster ( conf , 1 , true , null ) ; dfsCluster . getFileSystem ( ) . makeQualified ( input ) ; dfsCluster . getFileSystem ( ) . makeQualified ( output ) ; mrCluster = new MiniMRCluster ( 1 , getFileSystem ( ) . getUri ( ) . toString ( ) , 1 ) ; code_block = ForStatement ; log . warn ( "Spun up test cluster." ) ; }
public void test() { try { AbstractOutputWriter writer = getWriter ( ) ; code_block = IfStatement ; String [ ] tok = new String [ fileToken . length ] ; code_block = ForStatement ; code_block = ForStatement ; StringBuilder sb = new StringBuilder ( ) ; code_block = ForStatement ; log . debug ( "send {} to {}" , bundle , sb ) ; writer . writeLine ( sb . toString ( ) , bundle ) ; } catch ( Exception ex ) { throw DataChannelError . promote ( ex ) ; } }
public ModelAndView getOntologyOwlApi ( HttpServletRequest request , HttpServletResponse response ) throws Exception { log . debug ( "Controller for transforming POJO object to resources of semantic web" ) ; String syntax ; OutputStream out = null ; InputStream is = null ; int headerBufferSize = 8096 ; syntax = request . getParameter ( "type" ) ; response . setHeader ( "Content-Type" , "application/rdf+xml" ) ; response . setContentType ( "application/rdf+xml" ) ; response . setHeader ( "Content-Disposition" , "attachment;filename=eegdatabase.owl" ) ; log . debug ( "Creating output stream" ) ; response . setStatus ( HttpServletResponse . SC_OK ) ; response . setBufferSize ( headerBufferSize ) ; out = response . getOutputStream ( ) ; log . debug ( "Generating OWL" ) ; is = semanticFactory . getOntologyOwlApi ( syntax ) ; copy ( is , out ) ; out . flush ( ) ; out . close ( ) ; return null ; }
public ModelAndView getOntologyOwlApi ( HttpServletRequest request , HttpServletResponse response ) throws Exception { log . debug ( "Controller for transforming POJO object to resources of semantic web" ) ; String syntax ; OutputStream out = null ; InputStream is = null ; int headerBufferSize = 8096 ; syntax = request . getParameter ( "type" ) ; response . setHeader ( "Content-Type" , "application/rdf+xml" ) ; response . setContentType ( "application/rdf+xml" ) ; response . setHeader ( "Content-Disposition" , "attachment;filename=eegdatabase.owl" ) ; log . debug ( "Creating output stream" ) ; response . setStatus ( HttpServletResponse . SC_OK ) ; response . setBufferSize ( headerBufferSize ) ; out = response . getOutputStream ( ) ; log . debug ( "Generating OWL" ) ; is = semanticFactory . getOntologyOwlApi ( syntax ) ; copy ( is , out ) ; out . flush ( ) ; out . close ( ) ; return null ; }
public ModelAndView getOntologyOwlApi ( HttpServletRequest request , HttpServletResponse response ) throws Exception { log . debug ( "Controller for transforming POJO object to resources of semantic web" ) ; String syntax ; OutputStream out = null ; InputStream is = null ; int headerBufferSize = 8096 ; syntax = request . getParameter ( "type" ) ; response . setHeader ( "Content-Type" , "application/rdf+xml" ) ; response . setContentType ( "application/rdf+xml" ) ; response . setHeader ( "Content-Disposition" , "attachment;filename=eegdatabase.owl" ) ; log . debug ( "Creating output stream" ) ; response . setStatus ( HttpServletResponse . SC_OK ) ; response . setBufferSize ( headerBufferSize ) ; out = response . getOutputStream ( ) ; log . debug ( "Generating OWL" ) ; is = semanticFactory . getOntologyOwlApi ( syntax ) ; copy ( is , out ) ; out . flush ( ) ; out . close ( ) ; return null ; }
public void partition ( ) { log . trace ( "Partition forming" ) ; disableDiscovery ( ) ; installNewView ( ) ; assertPartitionFormed ( ) ; log . trace ( "New views installed" ) ; }
public void partition ( ) { log . trace ( "Partition forming" ) ; disableDiscovery ( ) ; installNewView ( ) ; assertPartitionFormed ( ) ; log . trace ( "New views installed" ) ; }
public void test() { try { IPage newPage = null ; code_block = IfStatement ; return this . getDtoBuilder ( ) . convert ( newPage ) ; } catch ( ValidationGenericException e ) { throw e ; } catch ( ApsSystemException e ) { logger . error ( "Error updating page {} status" , pageCode , e ) ; throw new RestServerError ( "error in update page status" , e ) ; } }
public void test() { if ( cachedRowSet . size ( ) == 0 ) { return Optional . empty ( ) ; } else-if ( cachedRowSet . size ( ) > 1 ) { LOGGER . warn ( MessageFormat . format ( "Found multiple implementations for ActionTrace {0}. Returning first implementation" , scriptParameterDesignTraceKey . toString ( ) ) ) ; } }
private void prepareUsername ( PWDProtectExtensionMessage msg ) throws CryptoException { Config config = chooser . getConfig ( ) ; EllipticCurve curve = CurveFactory . getCurve ( config . getDefaultPWDProtectGroup ( ) ) ; Point generator = curve . getBasePoint ( ) ; Point serverPublicKey = config . getDefaultServerPWDProtectPublicKey ( ) ; HKDFAlgorithm hkdfAlgorithm ; code_block = IfStatement ; BigInteger clientPublicKey = curve . mult ( config . getDefaultServerPWDProtectRandomSecret ( ) , generator ) . getX ( ) . getData ( ) ; BigInteger sharedSecret = curve . mult ( config . getDefaultServerPWDProtectRandomSecret ( ) , serverPublicKey ) . getX ( ) . getData ( ) ; byte [ ] key = HKDFunction . expand ( hkdfAlgorithm , HKDFunction . extract ( hkdfAlgorithm , null , ArrayConverter . bigIntegerToByteArray ( sharedSecret ) ) , new byte [ 0 ] , curve . getModulus ( ) . bitLength ( ) / Bits . IN_A_BYTE ) ; LOGGER . debug ( "Username encryption key: " + ArrayConverter . bytesToHexString ( key ) ) ; byte [ ] ctrKey = Arrays . copyOfRange ( key , 0 , key . length / 2 ) ; byte [ ] macKey = Arrays . copyOfRange ( key , key . length / 2 , key . length ) ; SivMode AES_SIV = new SivMode ( ) ; byte [ ] protectedUsername = AES_SIV . encrypt ( ctrKey , macKey , chooser . getClientPWDUsername ( ) . getBytes ( ) ) ; msg . setUsername ( ArrayConverter . concatenate ( ArrayConverter . bigIntegerToByteArray ( clientPublicKey , curve . getModulus ( ) . bitLength ( ) / Bits . IN_A_BYTE , true ) , protectedUsername ) ) ; LOGGER . debug ( "Username: " + ArrayConverter . bytesToHexString ( msg . getUsername ( ) ) ) ; }
private void prepareUsername ( PWDProtectExtensionMessage msg ) throws CryptoException { Config config = chooser . getConfig ( ) ; EllipticCurve curve = CurveFactory . getCurve ( config . getDefaultPWDProtectGroup ( ) ) ; Point generator = curve . getBasePoint ( ) ; Point serverPublicKey = config . getDefaultServerPWDProtectPublicKey ( ) ; HKDFAlgorithm hkdfAlgorithm ; code_block = IfStatement ; BigInteger clientPublicKey = curve . mult ( config . getDefaultServerPWDProtectRandomSecret ( ) , generator ) . getX ( ) . getData ( ) ; BigInteger sharedSecret = curve . mult ( config . getDefaultServerPWDProtectRandomSecret ( ) , serverPublicKey ) . getX ( ) . getData ( ) ; byte [ ] key = HKDFunction . expand ( hkdfAlgorithm , HKDFunction . extract ( hkdfAlgorithm , null , ArrayConverter . bigIntegerToByteArray ( sharedSecret ) ) , new byte [ 0 ] , curve . getModulus ( ) . bitLength ( ) / Bits . IN_A_BYTE ) ; LOGGER . debug ( "Username encryption key: " + ArrayConverter . bytesToHexString ( key ) ) ; byte [ ] ctrKey = Arrays . copyOfRange ( key , 0 , key . length / 2 ) ; byte [ ] macKey = Arrays . copyOfRange ( key , key . length / 2 , key . length ) ; SivMode AES_SIV = new SivMode ( ) ; byte [ ] protectedUsername = AES_SIV . encrypt ( ctrKey , macKey , chooser . getClientPWDUsername ( ) . getBytes ( ) ) ; msg . setUsername ( ArrayConverter . concatenate ( ArrayConverter . bigIntegerToByteArray ( clientPublicKey , curve . getModulus ( ) . bitLength ( ) / Bits . IN_A_BYTE , true ) , protectedUsername ) ) ; LOGGER . debug ( "Username: " + ArrayConverter . bytesToHexString ( msg . getUsername ( ) ) ) ; }
@ MCRCommand ( syntax = "build google sitemap" , help = "Creates the google sitemap(s) in webapps directory." , order = 10 ) public static void buildSitemap ( ) throws Exception { LOGGER . debug ( "Build Google sitemap start." ) ; final long start = System . currentTimeMillis ( ) ; File webappBaseDir = new File ( MCRConfiguration2 . getStringOrThrow ( "MCR.WebApplication.basedir" ) ) ; MCRGoogleSitemapCommon common = new MCRGoogleSitemapCommon ( webappBaseDir ) ; common . removeSitemapFiles ( ) ; int number = common . checkSitemapFile ( ) ; LOGGER . debug ( "Build Google number of URL files {}." , Integer . toString ( number ) ) ; code_block = IfStatement ; LOGGER . debug ( "Google sitemap request took {}ms." , System . currentTimeMillis ( ) - start ) ; }
@ MCRCommand ( syntax = "build google sitemap" , help = "Creates the google sitemap(s) in webapps directory." , order = 10 ) public static void buildSitemap ( ) throws Exception { LOGGER . debug ( "Build Google sitemap start." ) ; final long start = System . currentTimeMillis ( ) ; File webappBaseDir = new File ( MCRConfiguration2 . getStringOrThrow ( "MCR.WebApplication.basedir" ) ) ; MCRGoogleSitemapCommon common = new MCRGoogleSitemapCommon ( webappBaseDir ) ; common . removeSitemapFiles ( ) ; int number = common . checkSitemapFile ( ) ; LOGGER . debug ( "Build Google number of URL files {}." , Integer . toString ( number ) ) ; code_block = IfStatement ; LOGGER . debug ( "Google sitemap request took {}ms." , System . currentTimeMillis ( ) - start ) ; }
@ MCRCommand ( syntax = "build google sitemap" , help = "Creates the google sitemap(s) in webapps directory." , order = 10 ) public static void buildSitemap ( ) throws Exception { LOGGER . debug ( "Build Google sitemap start." ) ; final long start = System . currentTimeMillis ( ) ; File webappBaseDir = new File ( MCRConfiguration2 . getStringOrThrow ( "MCR.WebApplication.basedir" ) ) ; MCRGoogleSitemapCommon common = new MCRGoogleSitemapCommon ( webappBaseDir ) ; common . removeSitemapFiles ( ) ; int number = common . checkSitemapFile ( ) ; LOGGER . debug ( "Build Google number of URL files {}." , Integer . toString ( number ) ) ; code_block = IfStatement ; LOGGER . debug ( "Google sitemap request took {}ms." , System . currentTimeMillis ( ) - start ) ; }
public void test() { try { return renderService . fromMetadata ( byteArrayInputStream , RenderFormat . JSON ) ; } catch ( IOException ex ) { String message = "Exception occurred while trying to do JSON conversion while parsing class list map" ; log . error ( message ) ; throw new MetadataSyncServiceException ( message , ex ) ; } catch ( Exception ex ) { throw new MetadataSyncServiceException ( ex . getMessage ( ) , ex ) ; } }
protected void runIngestTest ( long defaultRunTime , long keysPerServerPerIter , int colsPerKey , int recordSize , int writeThreads , int readThreads ) throws Exception { LOG . info ( "Running ingest" ) ; LOG . info ( "Cluster size:" + util . getHBaseClusterInterface ( ) . getClusterMetrics ( ) . getLiveServerMetrics ( ) . size ( ) ) ; long start = System . currentTimeMillis ( ) ; String runtimeKey = String . format ( RUN_TIME_KEY , this . getClass ( ) . getSimpleName ( ) ) ; long runtime = util . getConfiguration ( ) . getLong ( runtimeKey , defaultRunTime ) ; long startKey = 0 ; long numKeys = getNumKeys ( keysPerServerPerIter ) ; code_block = WhileStatement ; }
public void test() { if ( 0 != ret ) { String errorMsg = "Verification failed with error code " + ret ; LOG . error ( errorMsg + " Rerunning verification after 1 minute for debugging" ) ; Threads . sleep ( 1000 * 60 ) ; ret = loadTool . run ( getArgsForLoadTestTool ( "-read" , String . format ( "100:%d" , readThreads ) , startKey , numKeys ) ) ; code_block = IfStatement ; Assert . fail ( errorMsg ) ; } }
public void test() { if ( 0 != ret ) { LOG . error ( "Rerun of Verification failed with error code " + ret ) ; } }
public void test() { try { uninstall = DOMCommandsParser . parse ( df ) ; } catch ( Exception e ) { log . error ( "Failed to set uninstall commands" ) ; } }
public void test() { try { Map < String , Object > parameters = new HashMap < > ( ) ; parameters . put ( "contentId" , contentId ) ; return getUniqueResult ( "FROM InsightsContentConfig CC WHERE CC.contentId = :contentId " , InsightsContentConfig . class , parameters ) ; } catch ( Exception e ) { log . error ( e ) ; throw e ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Received message [" + data + "]" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Found a matching contract with an output message. Will send it to the [" + destination + "] destination" ) ; } }
public void test() { try { code_block = TryStatement ;  } catch ( final IOException e ) { LOG . warn ( "exception while writing counters data to files" , e ) ; } finally { code_block = TryStatement ;  stopped = true ; } }
public void test() { try { listener . gotUserListMemberships ( lists ) ; } catch ( Exception e ) { logger . warn ( "Exception at getUserListMemberships" , e ) ; } }
@ Override public boolean matches ( final ConditionContext context , final AnnotatedTypeMetadata metadata ) { final boolean enabled = SQS . equals ( RpcStrategy . getRpcStrategy ( ) ) ; LOG . debug ( "Enable SQS RPC: {}" , enabled ) ; return enabled ; }
public void test() { if ( kuduClient . tableExists ( userTable ) ) { kuduClient . deleteTable ( userTable ) ; LOG . info ( String . format ( "table  %s has been deleted." , userTable ) ) ; } }
public void test() { if ( options . basicIndexOptions . getPartitionStrategy ( ) . equals ( PartitionStrategy . NONE ) ) { LOGGER . warn ( "Partition strategy is necessary when using more than 1 partition, defaulting to 'hash' partitioning." ) ; } }
public void test() { try { logger . debug ( "Searching index using custom query '" + query + "'" ) ; return solrRequester . getByQuery ( query , limit , offset ) ; } catch ( SolrServerException e ) { throw new SearchException ( e ) ; } }
public void test() { if ( queryProfile . isDetailProfileEnable ( ) ) { String stepName = getQueryStepName ( ) ; StreamingQueryProfile . ProfileStep profileStep = queryProfile . finishStep ( stepName ) ; profileStep . stepInfo ( "scan_count" , String . valueOf ( scanCnt ) ) . stepInfo ( "filter_count" , String . valueOf ( filterCnt ) ) ; logger . info ( "query-{}: segment-{} memory store scan finished, take {} ms" , queryProfile . getQueryId ( ) , segmentName , profileStep . getDuration ( ) ) ; } }
public void attachDirty ( StgMPersbezTxt instance ) { log . debug ( "attaching dirty StgMPersbezTxt instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { lifeCycles = checkAndUpdateLifeCycle ( lifeCycles , type ) ; validateNotEmpty ( "entityName" , entity ) ; validateInstanceFilterByClause ( filterBy ) ; Entity entityObject = EntityUtil . getEntity ( type , entity ) ; AbstractWorkflowEngine wfEngine = getWorkflowEngine ( entityObject ) ; return getInstanceResultSubset ( wfEngine . getRunningInstances ( entityObject , lifeCycles ) , filterBy , orderBy , sortOrder , offset , numResults , "" ) ; } catch ( Throwable e ) { LOG . error ( "Failed to get running instances" , e ) ; throw FalconWebException . newAPIException ( e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceApplicationBrandServiceUtil . class , "deleteCommerceApplicationBrand" , _deleteCommerceApplicationBrandParameterTypes1 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , commerceApplicationBrandId ) ; code_block = TryStatement ;  } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( ActiveMQRALogger . LOGGER . isTraceEnabled ( ) ) { ActiveMQRALogger . LOGGER . trace ( "getBooleanProperty(" + name + ")" ) ; } }
public void test() { try { RenderRequest renderRequest = ( RenderRequest ) httpServletRequest . getAttribute ( JavaConstants . JAVAX_PORTLET_REQUEST ) ; CommerceVirtualOrderItemEditDisplayContext commerceVirtualOrderItemEditDisplayContext = new CommerceVirtualOrderItemEditDisplayContext ( _commerceOrderService , _commerceOrderItemService , getCommerceVirtualOrderItem ( httpServletRequest ) , _dlAppService , _itemSelector , renderRequest ) ; httpServletRequest . setAttribute ( WebKeys . PORTLET_DISPLAY_CONTEXT , commerceVirtualOrderItemEditDisplayContext ) ; } catch ( PortalException portalException ) { _log . error ( portalException , portalException ) ; } }
public void test() { if ( ! ( exception instanceof BulkValidationException ) ) { LoggerFactory . getLogger ( DefaultQualityControlService . class ) . error ( "Bulk save failed" , exception ) ; } }
public void test() { if ( iterationCondition == null ) { return null ; } else-if ( iterationCondition instanceof Text ) { return iterationCondition . toString ( ) ; } else { LOGGER . warn ( MessageFormat . format ( this . getActionExecution ( ) . getAction ( ) . getType ( ) + " does not accept {0} as type for iterationCondition" , iterationCondition . getClass ( ) ) ) ; return iterationCondition . toString ( ) ; } }
public void test() { if ( message . getError ( ) != null ) { log . trace ( "Not bouncing a stanza that included an error (to prevent never-ending loops of bounces-of-bounces)." ) ; return ; } }
public void test() { try { log . trace ( "Bouncing a message stanza." ) ; final Message errorResponse = message . createCopy ( ) ; errorResponse . setError ( PacketError . Condition . service_unavailable ) ; errorResponse . setFrom ( message . getTo ( ) ) ; errorResponse . setTo ( message . getFrom ( ) ) ; route ( errorResponse ) ; } catch ( Exception e ) { log . error ( "An exception occurred while trying to bounce a message." , e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceDiscountRelServiceUtil . class , "getCPDefinitionsByCommerceDiscountId" , _getCPDefinitionsByCommerceDiscountIdParameterTypes12 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , commerceDiscountId , name , languageId , start , end ) ; Object returnObj = null ; code_block = TryStatement ;  return ( java . util . List < com . liferay . commerce . discount . model . CommerceDiscountRel > ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( debug ) { logger . info ( "GCDProxy done e1 " + e1 . getClass ( ) . getName ( ) ) ; } }
public void test() { try { GenPolynomial < C > g = e1 . baseGcd ( P , S ) ; code_block = IfStatement ; return g ; } catch ( PreemptingException e ) { throw new RuntimeException ( "GCDProxy e1 pre " + e ) ; } catch ( Exception e ) { logger . info ( "GCDProxy e1 " + e ) ; logger . info ( "GCDProxy P = " + P ) ; logger . info ( "GCDProxy S = " + S ) ; throw new RuntimeException ( "GCDProxy e1 " + e ) ; } }
public void test() { if ( debug ) { logger . info ( "GCDProxy done e2 " + e2 . getClass ( ) . getName ( ) ) ; } }
public void test() { try { GenPolynomial < C > g = e2 . baseGcd ( P , S ) ; code_block = IfStatement ; return g ; } catch ( PreemptingException e ) { throw new RuntimeException ( "GCDProxy e2 pre " + e ) ; } catch ( Exception e ) { logger . info ( "GCDProxy e2 " + e ) ; logger . info ( "GCDProxy P = " + P ) ; logger . info ( "GCDProxy S = " + S ) ; throw new RuntimeException ( "GCDProxy e2 " + e ) ; } }
public void test() { try { g = pool . invokeAny ( cs ) ; } catch ( InterruptedException ignored ) { logger . info ( "InterruptedException " + ignored ) ; Thread . currentThread ( ) . interrupt ( ) ; } catch ( ExecutionException e ) { logger . info ( "ExecutionException " + e ) ; Thread . currentThread ( ) . interrupt ( ) ; } }
public void test() { try { g = pool . invokeAny ( cs ) ; } catch ( InterruptedException ignored ) { logger . info ( "InterruptedException " + ignored ) ; Thread . currentThread ( ) . interrupt ( ) ; } catch ( ExecutionException e ) { logger . info ( "ExecutionException " + e ) ; Thread . currentThread ( ) . interrupt ( ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( portalException , portalException ) ; } }
public void test() { try { final String stringValue = getConvertedPropertyValue ( securityContext , graphObject , key ) ; final byte [ ] bytes = stringValue . getBytes ( Charset . forName ( "utf-8" ) ) ; dst . put ( bytes ) ; return bytes . length ; } catch ( FrameworkException fex ) { logger . warn ( "" , fex ) ; } }
@ Test public void testMovingTarget ( ) throws Exception { logger . info ( "GeoIT.testMovingTarget" ) ; EntityManager em = app . getEntityManager ( ) ; assertNotNull ( em ) ; Map < String , Object > properties = new LinkedHashMap < String , Object > ( ) code_block = "" ; ; Entity user = em . create ( "user" , properties ) ; assertNotNull ( user ) ; app . waitForQueueDrainAndRefreshIndex ( ) ; final double lat = 37.776753 ; final double lon = - 122.407846 ; Query query = Query . fromQL ( "select * where location within 100 of " + lat + "," + lon ) ; Results listResults = em . searchCollection ( em . getApplicationRef ( ) , "users" , query ) ; assertEquals ( 1 , listResults . size ( ) ) ; updatePos ( em , user , 37.428526 , - 122.140916 ) ; listResults = em . searchCollection ( em . getApplicationRef ( ) , "users" , query ) ; assertEquals ( 0 , listResults . size ( ) ) ; em . delete ( user ) ; }
public void test() { if ( key != null ) { code_block = IfStatement ; } else { Log . warn ( "Ignoring null property key for group: " + name ) ; } }
public void test() { try { con = DbConnectionManager . getConnection ( ) ; pstmt = con . prepareStatement ( LOAD_PROPERTIES ) ; pstmt . setString ( 1 , name ) ; rs = pstmt . executeQuery ( ) ; code_block = WhileStatement ; } catch ( SQLException sqle ) { Log . error ( sqle . getMessage ( ) , sqle ) ; } finally { DbConnectionManager . closeConnection ( rs , pstmt , con ) ; } }
@ Override public void run ( ) { int count = 0 ; long start = System . currentTimeMillis ( ) ; log . info ( "Starting the DL4J classifier thread..." ) ; running = true ; code_block = WhileStatement ; }
public void test() { try { Thread . sleep ( 10 ) ; } catch ( InterruptedException e ) { log . info ( "Dl4j classifier thread interrupted" , e ) ; } }
public void test() { if ( count % 100 == 0 ) { double rate = 1000.0 * count / ( System . currentTimeMillis ( ) - start ) ; log . info ( "DL4J Filter Rate: {}" , rate ) ; } }
public void test() { try { count ++ ; lastResult = dl4j . classifyImageMiniEXCEPTION ( lastImage , confidence ) ; code_block = IfStatement ; invoke ( "publishClassification" , lastResult ) ; if ( lastResult != null && lastResult . size ( ) > 0 ) log . info ( formatResultString ( lastResult ) ) ; } catch ( IOException e ) { log . warn ( "Exception classifying image!" , e ) ; } }
public void handle ( PortletContainerException e ) { log . error ( "The portlet could not be loaded. Check if properly deployed." , ExceptionUtil . getRootCause ( e ) ) ; }
@ Test public void retrievePutConcurrent ( ) throws Exception { LOG . info ( "Started retrievePutConcurrent" ) ; final File f = createFile ( 0 , loader , cache , folder ) ; File f2 = copyToFile ( randomStream ( 1 , 4 * 1024 ) , folder . newFile ( ) ) ; ListeningExecutorService executorService = MoreExecutors . listeningDecorator ( Executors . newFixedThreadPool ( 2 ) ) ; closer . register ( new ExecutorCloser ( executorService , 5 , TimeUnit . MILLISECONDS ) ) ; CountDownLatch thread1Start = new CountDownLatch ( 1 ) ; SettableFuture < File > future1 = retrieveThread ( executorService , ID_PREFIX + 0 , cache , thread1Start ) ; CountDownLatch thread2Start = new CountDownLatch ( 1 ) ; SettableFuture < Boolean > future2 = putThread ( executorService , 1 , f2 , cache , thread2Start ) ; thread1Start . countDown ( ) ; thread2Start . countDown ( ) ; future1 . get ( ) ; future2 . get ( ) ; LOG . info ( "Async tasks finished" ) ; assertCacheIfPresent ( 0 , cache , f ) ; assertCacheIfPresent ( 1 , cache , copyToFile ( randomStream ( 1 , 4 * 1024 ) , folder . newFile ( ) ) ) ; assertCacheStats ( cache , 2 , 8 * 1024 , 1 , 1 ) ; LOG . info ( "Finished retrievePutConcurrent" ) ; }
@ Test public void retrievePutConcurrent ( ) throws Exception { LOG . info ( "Started retrievePutConcurrent" ) ; final File f = createFile ( 0 , loader , cache , folder ) ; File f2 = copyToFile ( randomStream ( 1 , 4 * 1024 ) , folder . newFile ( ) ) ; ListeningExecutorService executorService = MoreExecutors . listeningDecorator ( Executors . newFixedThreadPool ( 2 ) ) ; closer . register ( new ExecutorCloser ( executorService , 5 , TimeUnit . MILLISECONDS ) ) ; CountDownLatch thread1Start = new CountDownLatch ( 1 ) ; SettableFuture < File > future1 = retrieveThread ( executorService , ID_PREFIX + 0 , cache , thread1Start ) ; CountDownLatch thread2Start = new CountDownLatch ( 1 ) ; SettableFuture < Boolean > future2 = putThread ( executorService , 1 , f2 , cache , thread2Start ) ; thread1Start . countDown ( ) ; thread2Start . countDown ( ) ; future1 . get ( ) ; future2 . get ( ) ; LOG . info ( "Async tasks finished" ) ; assertCacheIfPresent ( 0 , cache , f ) ; assertCacheIfPresent ( 1 , cache , copyToFile ( randomStream ( 1 , 4 * 1024 ) , folder . newFile ( ) ) ) ; assertCacheStats ( cache , 2 , 8 * 1024 , 1 , 1 ) ; LOG . info ( "Finished retrievePutConcurrent" ) ; }
@ Test public void retrievePutConcurrent ( ) throws Exception { LOG . info ( "Started retrievePutConcurrent" ) ; final File f = createFile ( 0 , loader , cache , folder ) ; File f2 = copyToFile ( randomStream ( 1 , 4 * 1024 ) , folder . newFile ( ) ) ; ListeningExecutorService executorService = MoreExecutors . listeningDecorator ( Executors . newFixedThreadPool ( 2 ) ) ; closer . register ( new ExecutorCloser ( executorService , 5 , TimeUnit . MILLISECONDS ) ) ; CountDownLatch thread1Start = new CountDownLatch ( 1 ) ; SettableFuture < File > future1 = retrieveThread ( executorService , ID_PREFIX + 0 , cache , thread1Start ) ; CountDownLatch thread2Start = new CountDownLatch ( 1 ) ; SettableFuture < Boolean > future2 = putThread ( executorService , 1 , f2 , cache , thread2Start ) ; thread1Start . countDown ( ) ; thread2Start . countDown ( ) ; future1 . get ( ) ; future2 . get ( ) ; LOG . info ( "Async tasks finished" ) ; assertCacheIfPresent ( 0 , cache , f ) ; assertCacheIfPresent ( 1 , cache , copyToFile ( randomStream ( 1 , 4 * 1024 ) , folder . newFile ( ) ) ) ; assertCacheStats ( cache , 2 , 8 * 1024 , 1 , 1 ) ; LOG . info ( "Finished retrievePutConcurrent" ) ; }
public void test() { try { registerResources ( pluginAnnotation , classLoader ) ; } catch ( IOException e ) { LOGGER . error ( "Unable to register resources for annotation {} on method {} class {}" , e , pluginAnnotation . getAnnotation ( ) , pluginAnnotation . getMethod ( ) . getName ( ) , pluginAnnotation . getMethod ( ) . getDeclaringClass ( ) . getName ( ) ) ; return false ; } }
public void test() { try { return catalogsCache . get ( "" , ( ) code_block = LoopStatement ; ) ; } catch ( ExecutionException executionException ) { log . debug ( executionException . getCause ( ) , "Error while caching all catalogs metadata. Falling back to default flow" ) ; return delegate . getCatalogs ( ) ; } }
public void test() { try { url = new URL ( posterimageUrlOpt ) ; } catch ( Exception e ) { logger . debug ( "Given poster image URI '{}' is not valid" , posterimageUrlOpt ) ; } }
public void test() { try { File coverImageFile = getWorkspace ( ) . get ( url . toURI ( ) ) ; return coverImageFile . getPath ( ) ; } catch ( NotFoundException e ) { logger . warn ( "Poster image could not be found at '{}'" , url ) ; return null ; } catch ( IOException e ) { logger . warn ( "Error getting poster image: {}" , e . getMessage ( ) ) ; return null ; } catch ( URISyntaxException e ) { logger . warn ( "Given URL '{}' is not a valid URI" , url ) ; return null ; } }
public void test() { try { File coverImageFile = getWorkspace ( ) . get ( url . toURI ( ) ) ; return coverImageFile . getPath ( ) ; } catch ( NotFoundException e ) { logger . warn ( "Poster image could not be found at '{}'" , url ) ; return null ; } catch ( IOException e ) { logger . warn ( "Error getting poster image: {}" , e . getMessage ( ) ) ; return null ; } catch ( URISyntaxException e ) { logger . warn ( "Given URL '{}' is not a valid URI" , url ) ; return null ; } }
public void test() { try { File coverImageFile = getWorkspace ( ) . get ( url . toURI ( ) ) ; return coverImageFile . getPath ( ) ; } catch ( NotFoundException e ) { logger . warn ( "Poster image could not be found at '{}'" , url ) ; return null ; } catch ( IOException e ) { logger . warn ( "Error getting poster image: {}" , e . getMessage ( ) ) ; return null ; } catch ( URISyntaxException e ) { logger . warn ( "Given URL '{}' is not a valid URI" , url ) ; return null ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceOrderServiceUtil . class , "getPendingCommerceOrdersCount" , _getPendingCommerceOrdersCountParameterTypes21 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , commerceAccountId , keywords ) ; Object returnObj = null ; code_block = TryStatement ;  return ( ( Integer ) returnObj ) . intValue ( ) ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( throwable , throwable ) ; } }
public void test() { try { return terminalInfoAvp . getGrouped ( ) . getAvp ( Avp . TGPP2_MEID ) != null ; } catch ( AvpDataException ex ) { logger . debug ( "Failure trying to obtain (Terminal-Information) MEID AVP value" , ex ) ; } }
public void test() { try { return Class . forName ( interfaceName ) ; } catch ( ClassNotFoundException e1 ) { logger . debug ( "Could not find class, but will keep looking" , e1 ) ; } }
public void test() { if ( value != null ) { LOG . warn ( "Configuration value overridden by system property: {}, with value: {}" , property , value ) ; } else { value = element . getAttribute ( attributeName ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( "Synchronizing all folders for accountId " + account . getAccountId ( ) ) ; } }
private ReportResult generateReport ( Path userCsvFolder , Profile profile , DashBoard dash , long now ) throws Exception { int fetchCount = ( int ) report . reportType . getFetchCount ( report . granularityType ) ; long startFrom = now - TimeUnit . DAYS . toMillis ( report . reportType . getDuration ( ) ) ; startFrom = ( startFrom / report . granularityType . period ) * report . granularityType . period ; Path output = Paths . get ( userCsvFolder . toString ( ) + ".zip" ) ; boolean hasData = generateReport ( output , profile , dash , fetchCount , startFrom ) ; code_block = IfStatement ; log . info ( "No data for report for user {} and reportId {}." , key . user . email , report . id ) ; return ReportResult . NO_DATA ; }
public void test() { if ( ! createDirectories ( destination ) ) { LOGGER . error ( "Directory copy failed, could not create destination directory: " + destination . getAbsolutePath ( ) ) ; return false ; } }
public void test() { if ( ERROR . equals ( when ) && log . isErrorEnabled ( ) ) { log . error ( sb . toString ( ) , t ) ; } else-if ( log . isTraceEnabled ( ) ) { log . trace ( sb . toString ( ) ) ; } }
public void test() { if ( ERROR . equals ( when ) && log . isErrorEnabled ( ) ) { log . error ( sb . toString ( ) , t ) ; } else-if ( log . isTraceEnabled ( ) ) { log . trace ( sb . toString ( ) ) ; } }
public void test() { try { out = new PrintWriter ( new BufferedWriter ( new FileWriter ( logFile , true ) ) ) ; out . println ( sb . toString ( ) ) ; } catch ( Exception e ) { log . debug ( "Unable to write message to file" , e ) ; } finally { IOUtils . closeQuietly ( out ) ; } }
public void test() { try { StringBuilder sb = new StringBuilder ( ) ; appendToLine ( sb , Long . toString ( context . getEvaluationId ( ) ) ) ; appendToLine ( sb , DateUtil . formatDate ( new Date ( logTime ) , "yyyy-MM-dd HH:mm:ss.S" ) ) ; appendToLine ( sb , StringUtils . repeat ( ">" , context . getEvaluationLevel ( ) ) ) ; appendToLine ( sb , when ) ; appendToLine ( sb , eventCode ) ; code_block = IfStatement ; sb . append ( message ) ; code_block = IfStatement ; code_block = IfStatement ; } catch ( Exception e ) { log . trace ( "An error occurred logging an evaluation event" , e ) ; } }
private static String [ ] expandFileNames ( String fileName ) throws IOException { code_block = IfStatement ; code_block = IfStatement ; fileName = new File ( fileName ) . getCanonicalPath ( ) ; LOG . debug ( "Canonical path: {}" , fileName ) ; DirectoryScanner scanner = new DirectoryScanner ( ) ; scanner . setIncludes ( new String [ ] code_block = "" ; ) ; scanner . scan ( ) ; return scanner . getIncludedFiles ( ) ; }
public void test() { if ( IS_DEBUG ) { LOG . debug ( "KrbSafe created" ) ; } }
public void test() { while ( true ) { Thread . sleep ( 1000 ) ; logger . info ( "Tuple row key: " , output . getReceivedTuples ( ) ) ; logger . info ( "Received tuple number {}, instance is {}." , output . getReceivedTuples ( ) == null ? 0 : output . getReceivedTuples ( ) . size ( ) , System . identityHashCode ( output ) ) ; code_block = IfStatement ; code_block = IfStatement ; } }
public void testOrFilter2 ( ) throws Exception { logger . info ( "executing test case testOrFilter2" ) ; String req = "{\"filter\":{\"or\":[{\"term\":{\"color\":\"blue\",\"_noOptimize\":false}},{\"term\":{\"color\":\"red\",\"_noOptimize\":false}}]}}" ; JSONObject res = search ( new JSONObject ( req ) ) ; assertEquals ( "numhits is wrong" , 3264 , res . getInt ( "numhits" ) ) ; }
public void test() { try { value = parseItemValue ( item . getObject ( ) , item . getClassType ( ) ) ; } catch ( Exception ex ) { RecordLog . warn ( "[ParamFlowRuleUtil] Failed to parse value for item: " + item , ex ) ; continue ; } }
public void test() { if ( ! cacheEnabled ( region ) ) { Cache cache = buildCache ( region , cacheProperties ) ; code_block = IfStatement ; } else { CacheManager . logger . warn ( Messages . getInstance ( ) . getString ( "CacheManager.WARN_0002_REGION_ALREADY_EXIST" , region ) ) ; } }
public void test() { if ( cacheEnabled ) { code_block = IfStatement ; } else { CacheManager . logger . warn ( Messages . getInstance ( ) . getString ( "CacheManager.WARN_0001_CACHE_NOT_ENABLED" ) ) ; } }
@ Override public void addIP ( LogicalDevice logicalDevice , IPProtocolEndpoint ip ) throws CapabilityException { log . info ( "Start of addIP call" ) ; if ( ( ip . getIPv4Address ( ) != null ) && ( ip . getSubnetMask ( ) != null ) && ! ( ip . getIPv4Address ( ) . isEmpty ( ) ) && ! ( ip . getSubnetMask ( ) . isEmpty ( ) ) ) addIPv4 ( logicalDevice , ip ) ; else-if ( ip . getIPv6Address ( ) != null && ! ip . getIPv6Address ( ) . isEmpty ( ) ) addIPv6 ( logicalDevice , ip ) ; else throw new CapabilityException ( "IP address not set." ) ; log . info ( "End of addIP call" ) ; }
@ Override public void addIP ( LogicalDevice logicalDevice , IPProtocolEndpoint ip ) throws CapabilityException { log . info ( "Start of addIP call" ) ; if ( ( ip . getIPv4Address ( ) != null ) && ( ip . getSubnetMask ( ) != null ) && ! ( ip . getIPv4Address ( ) . isEmpty ( ) ) && ! ( ip . getSubnetMask ( ) . isEmpty ( ) ) ) addIPv4 ( logicalDevice , ip ) ; else-if ( ip . getIPv6Address ( ) != null && ! ip . getIPv6Address ( ) . isEmpty ( ) ) addIPv6 ( logicalDevice , ip ) ; else throw new CapabilityException ( "IP address not set." ) ; log . info ( "End of addIP call" ) ; }
public void test() { if ( group != null && "api" . equals ( group . getName ( ) ) ) { LOGGER . info ( "Skipping test: " + tcCtx . toString ( ) ) ; return true ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Data Groups Changed: Request Tree Rebuild" ) ; } }
public void test() { try { writeToLog ( new Checkpoint ( txnId ) ) ; } catch ( final JournalException e ) { LOG . error ( "An error occurred whilst writing a checkpoint to the Journal: {}" , e . getMessage ( ) , e ) ; } }
public void test() { try { channel . close ( ) ; } catch ( final IOException e ) { LOG . error ( "Unable to close Journal file: {}" , e . getMessage ( ) , e ) ; } }
public void test() { try { String taskId = taskFile . getName ( ) ; TaskAnnouncement taskAnnouncement = jsonMapper . readValue ( taskFile , TaskAnnouncement . class ) ; code_block = IfStatement ; } catch ( IOException ex ) { log . error ( ex , "Failed to read completed task from disk at [%s]. Ignored." , taskFile . getAbsoluteFile ( ) ) ; } }
public void test() { if ( ! completedTaskDir . isDirectory ( ) ) { throw new ISE ( "Completed Tasks Dir [%s] does not exist/not-a-directory." , completedTaskDir ) ; } }
public void test() { try { checkNodes ( expCnt ) ; return true ; } catch ( AssertionError e ) { log . info ( "Check failed, will retry: " + e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( DLFileEntryServiceUtil . class , "deleteFileEntry" , _deleteFileEntryParameterTypes8 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , fileEntryId ) ; code_block = TryStatement ;  } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { server . stop ( ) ; } catch ( Exception ex ) { logger . warn ( "Failed to stop web server" , ex ) ; } }
public static List < CoordinatorAction > getProcessInstanceListFromAllBundles ( OozieClient oozieClient , String processName , EntityType entityType ) throws OozieClientException { List < CoordinatorAction > list = new ArrayList < > ( ) ; final List < String > bundleIds = OozieUtil . getBundles ( oozieClient , processName , entityType ) ; LOGGER . info ( "bundle size for process is " + bundleIds . size ( ) ) ; code_block = ForStatement ; String coordId = OozieUtil . getLatestCoordinatorID ( oozieClient , processName , entityType ) ; LOGGER . info ( "default coordID: " + coordId ) ; return list ; }
public static List < CoordinatorAction > getProcessInstanceListFromAllBundles ( OozieClient oozieClient , String processName , EntityType entityType ) throws OozieClientException { List < CoordinatorAction > list = new ArrayList < > ( ) ; final List < String > bundleIds = OozieUtil . getBundles ( oozieClient , processName , entityType ) ; LOGGER . info ( "bundle size for process is " + bundleIds . size ( ) ) ; code_block = ForStatement ; String coordId = OozieUtil . getLatestCoordinatorID ( oozieClient , processName , entityType ) ; LOGGER . info ( "default coordID: " + coordId ) ; return list ; }
public void test() { try { return some ( IOUtils . toString ( is ) ) ; } catch ( Exception e ) { logger . warn ( "Cannot load resource " + resource + " from classpath" ) ; return none ( ) ; } }
public void test() { if ( fieldExpType == null ) { log . error ( "Could not find field type" ) ; } }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; } catch ( ClassCastException ex ) { logger . error ( "Given target type for mounted files or folders was not extending AbstractFile." , ex ) ; } }
public void test() { if ( mountFolderPath != null ) { final Path relativePathParent = relativePath . getParent ( ) ; code_block = IfStatement ; } else { logger . warn ( "Cannot handle watch event, folder {} has no path" , folder . getUuid ( ) ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { LoggerFactory . getLogger ( ) . error ( "Error occurred re-adding node watcherfor node " + nodePath , e ) ; } }
public void test() { try { admin . createTopicIfNotExists ( topic ) ; ProjectTopicName topicName = ProjectTopicName . of ( pubSubSettings . getProjectId ( ) , topic ) ; Publisher publisher = Publisher . newBuilder ( topicName ) . setCredentialsProvider ( pubSubSettings . getCredentialsProvider ( ) ) . build ( ) ; publisherMap . put ( topic , publisher ) ; return publisher ; } catch ( IOException e ) { log . error ( "Failed to create Publisher for the topic [{}]." , topic , e ) ; throw new RuntimeException ( "Failed to create Publisher for the topic." , e ) ; } }
public void test() { try { code_block = IfStatement ; acceptComponentRequestCall = true ; RequestLifeCycle . begin ( this ) ; super . start ( ) ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; } finally { RequestLifeCycle . end ( ) ; } }
public void test() { if ( Boolean . TRUE . equals ( getAttribute ( BrooklynNode . WEB_CONSOLE_ACCESSIBLE ) ) ) { queueShutdownTask ( ) ; queueWaitExitTask ( ) ; } else { log . info ( "Skipping graceful shutdown call, because web-console not up for {}" , this ) ; } }
public void test() { if ( _log . isInfoEnabled ( ) ) { _log . info ( "Importing portlet settings..." ) ; } }
public void test() { if ( _log . isInfoEnabled ( ) ) { _log . info ( "Portlet settings successfully imported" ) ; } }
public void test() { try { outputPlans = initialOptimizer . optimize ( appModel , MMtoOptModelTransformer . transformModel ( suitableCloudOffer ) , benchmarkPlatforms , NUMBER_OF_PLANS_GENERATED , HYSTERESIS_PROPORTION ) ; previousPlans = outputPlans ; } catch ( Exception exc ) { log . warn ( "Optimizer did not work in its expected input. Exception name was " + exc . getClass ( ) . getName ( ) + " Trying with the assumption of former versions of Input " ) ; outputPlans = initialOptimizer . optimize ( appModel , suitableCloudOffer , benchmarkPlatforms , NUMBER_OF_PLANS_GENERATED , HYSTERESIS_PROPORTION ) ; previousPlans = outputPlans ; } catch ( Error E ) { log . error ( "Error optimizing the initial deployment" ) ; E . printStackTrace ( ) ; } }
public void test() { try { outputPlans = initialOptimizer . optimize ( appModel , MMtoOptModelTransformer . transformModel ( suitableCloudOffer ) , benchmarkPlatforms , NUMBER_OF_PLANS_GENERATED , HYSTERESIS_PROPORTION ) ; previousPlans = outputPlans ; } catch ( Exception exc ) { log . warn ( "Optimizer did not work in its expected input. Exception name was " + exc . getClass ( ) . getName ( ) + " Trying with the assumption of former versions of Input " ) ; outputPlans = initialOptimizer . optimize ( appModel , suitableCloudOffer , benchmarkPlatforms , NUMBER_OF_PLANS_GENERATED , HYSTERESIS_PROPORTION ) ; previousPlans = outputPlans ; } catch ( Error E ) { log . error ( "Error optimizing the initial deployment" ) ; E . printStackTrace ( ) ; } }
public void method ( String string ) { LOGGER . debug ( "Do something thing by Sub Generic Controller" ) ; }
@ Override public void elementOpened ( OpenAction action ) { log . info ( "HANDLE: OpenAction for element: " + action . getElementId ( ) ) ; }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( this + "::Acking message " + message ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Resource " + name + " not found with classloader: " + delegate + ". Trying other delegates" ) ; } }
public void test() { if ( this . proxyRequests ) { log . info ( "Will proxy requests to backend that are not getmap or getcapabilities." ) ; } else { log . info ( "Will NOT proxy non-getMap requests to backend." ) ; } }
public void test() { if ( this . proxyRequests ) { log . info ( "Will proxy requests to backend that are not getmap or getcapabilities." ) ; } else { log . info ( "Will NOT proxy non-getMap requests to backend." ) ; } }
public void test() { try { sizeofByType . putIfAbsent ( genericType , SIZE_RECURSIVE_MARKER ) ; Sizeof newSizeof = computeSizeof ( type , genericType ) ; return sizeofByType . compute ( genericType , ( key , value ) -> value == null || value == SIZE_RECURSIVE_MARKER ? newSizeof : value ) ; } catch ( Exception | StackOverflowError t ) { log . error ( "sizeof: Failed to compute function to " + type + ": " , t ) ; return Sizeof . constant ( objectHeaderSize ) ; } }
private OrgDisambiguatedEntity createDisambiguatedOrg ( RDFOrganization organization ) { LOGGER . info ( "Creating disambiguated org {}" , organization . name ) ; String orgType = getOrgType ( organization ) ; Iso3166Country country = StringUtils . isNotBlank ( organization . country ) ? Iso3166Country . fromValue ( organization . country ) : null ; OrgDisambiguatedEntity orgDisambiguatedEntity = new OrgDisambiguatedEntity ( ) ; orgDisambiguatedEntity . setName ( organization . name ) ; orgDisambiguatedEntity . setCountry ( country == null ? null : country . name ( ) ) ; orgDisambiguatedEntity . setCity ( organization . city ) ; orgDisambiguatedEntity . setRegion ( organization . stateCode ) ; orgDisambiguatedEntity . setOrgType ( orgType ) ; orgDisambiguatedEntity . setSourceId ( organization . doi ) ; orgDisambiguatedEntity . setSourceUrl ( organization . doi ) ; code_block = IfStatement ; code_block = IfStatement ; orgDisambiguatedEntity . setSourceType ( OrgDisambiguatedSourceType . FUNDREF . name ( ) ) ; orgDisambiguatedManager . createOrgDisambiguated ( orgDisambiguatedEntity ) ; return orgDisambiguatedEntity ; }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "addLongField fieldName: {}; value: {}" , fieldName , value ) ; } }
public void test() { try { paths . add ( pathElement . toURI ( ) . toURL ( ) ) ; } catch ( MalformedURLException e ) { LOG . debug ( "Malformed URL from " + pathName , e ) ; } }
public void test() { if ( cache . getCacheConfiguration ( ) . jmxStatistics ( ) . enabled ( ) ) { return cache . getAdvancedCache ( ) . getStats ( ) ; } else { log . debug ( "Statistics are not enabled for cache \'{}\'" , cacheName ) ; return new UnavailableStats ( - 1 ) ; } }
public void test() { try { Cache < Object , Object > cache = cacheManager . getCache ( cacheName ) ; code_block = IfStatement ; } catch ( Exception e ) { log . warn ( "Error getting Stats object - are statistics enabled for cache \'{}\'?" , cacheName , e ) ; return new UnavailableStats ( - 2 ) ; } }
@ Override public void process ( Channel channel , Command command ) { Preconditions . checkArgument ( CommandType . TASK_EXECUTE_ACK == command . getType ( ) , String . format ( "invalid command type : %s" , command . getType ( ) ) ) ; TaskExecuteAckCommand taskAckCommand = JSONUtils . parseObject ( command . getBody ( ) , TaskExecuteAckCommand . class ) ; logger . info ( "taskAckCommand : {}" , taskAckCommand ) ; taskInstanceCacheManager . cacheTaskInstance ( taskAckCommand ) ; String workerAddress = ChannelUtils . toAddress ( channel ) . getAddress ( ) ; ExecutionStatus ackStatus = ExecutionStatus . of ( taskAckCommand . getStatus ( ) ) ; TaskResponseEvent taskResponseEvent = TaskResponseEvent . newAck ( ackStatus , taskAckCommand . getStartTime ( ) , workerAddress , taskAckCommand . getExecutePath ( ) , taskAckCommand . getLogPath ( ) , taskAckCommand . getTaskInstanceId ( ) , channel ) ; taskResponseService . addResponse ( taskResponseEvent ) ; }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Failed to recover JMS Connection. Will try again in " + task . getCurrentDelay ( ) + " millis" , e ) ; } }
public void test() { try { SearchExecutor executor = new SearchExecutor ( ) ; executor . setBaseDn ( this . userSearchBaseDn ) ; log . info ( "Running search '{}'" , filter ) ; result = executor . search ( cf , filter ) . getResult ( ) ; log . info ( "User query successful: {}" , result ) ; } catch ( LdapException e ) { log . error ( "Error running {}" , filter , e ) ; throw new IllegalStateException ( e ) ; } }
public void test() { try { SearchExecutor executor = new SearchExecutor ( ) ; executor . setBaseDn ( this . userSearchBaseDn ) ; log . info ( "Running search '{}'" , filter ) ; result = executor . search ( cf , filter ) . getResult ( ) ; log . info ( "User query successful: {}" , result ) ; } catch ( LdapException e ) { log . error ( "Error running {}" , filter , e ) ; throw new IllegalStateException ( e ) ; } }
public void test() { try { new LdifWriter ( writer ) . write ( result ) ; } catch ( IOException e ) { log . error ( "Error generating LDIF file" , e ) ; throw new IllegalStateException ( e ) ; } }
@ Override public String exportAsLdif ( @ NonNull Account account ) { log . info ( "Exporting personal user data for account {}" , account . getUid ( ) ) ; ConnectionConfig connConfig = new ConnectionConfig ( this . ldapUrl ) ; connConfig . setConnectionInitializer ( new BindConnectionInitializer ( this . ldapUserName , new Credential ( this . ldapPassword ) ) ) ; ConnectionFactory cf = new DefaultConnectionFactory ( connConfig ) ; SearchResult result ; final String filter = String . format ( "(uid=%s)" , account . getUid ( ) ) ; code_block = TryStatement ;  StringWriter writer = new StringWriter ( ) ; code_block = TryStatement ;  String ldifContents = writer . toString ( ) ; log . info ( "Returning LDIF: {}" , ldifContents ) ; return ldifContents ; }
public void test() { try { logger . info ( "Getting storage path for segment [%s]" , segment . getSegment ( ) . getId ( ) ) ; Path path = new Path ( JobHelper . getURIFromSegment ( segment . getSegment ( ) ) ) ; logger . info ( "Fetch segment files from [%s]" , path ) ; File dir = FileUtils . createTempDir ( ) ; tmpSegmentDirs . add ( dir ) ; logger . info ( "Locally storing fetched segment at [%s]" , dir ) ; JobHelper . unzipNoGuava ( path , context . getConfiguration ( ) , dir , context , null ) ; logger . info ( "finished fetching segment files" ) ; QueryableIndex index = HadoopDruidIndexerConfig . INDEX_IO . loadIndex ( dir ) ; indexes . add ( index ) ; numRows += index . getNumRows ( ) ; return new WindowedStorageAdapter ( new QueryableIndexStorageAdapter ( index ) , segment . getInterval ( ) ) ; } catch ( IOException ex ) { throw new RuntimeException ( ex ) ; } }
public void test() { try { logger . info ( "Getting storage path for segment [%s]" , segment . getSegment ( ) . getId ( ) ) ; Path path = new Path ( JobHelper . getURIFromSegment ( segment . getSegment ( ) ) ) ; logger . info ( "Fetch segment files from [%s]" , path ) ; File dir = FileUtils . createTempDir ( ) ; tmpSegmentDirs . add ( dir ) ; logger . info ( "Locally storing fetched segment at [%s]" , dir ) ; JobHelper . unzipNoGuava ( path , context . getConfiguration ( ) , dir , context , null ) ; logger . info ( "finished fetching segment files" ) ; QueryableIndex index = HadoopDruidIndexerConfig . INDEX_IO . loadIndex ( dir ) ; indexes . add ( index ) ; numRows += index . getNumRows ( ) ; return new WindowedStorageAdapter ( new QueryableIndexStorageAdapter ( index ) , segment . getInterval ( ) ) ; } catch ( IOException ex ) { throw new RuntimeException ( ex ) ; } }
public void test() { try { logger . info ( "Getting storage path for segment [%s]" , segment . getSegment ( ) . getId ( ) ) ; Path path = new Path ( JobHelper . getURIFromSegment ( segment . getSegment ( ) ) ) ; logger . info ( "Fetch segment files from [%s]" , path ) ; File dir = FileUtils . createTempDir ( ) ; tmpSegmentDirs . add ( dir ) ; logger . info ( "Locally storing fetched segment at [%s]" , dir ) ; JobHelper . unzipNoGuava ( path , context . getConfiguration ( ) , dir , context , null ) ; logger . info ( "finished fetching segment files" ) ; QueryableIndex index = HadoopDruidIndexerConfig . INDEX_IO . loadIndex ( dir ) ; indexes . add ( index ) ; numRows += index . getNumRows ( ) ; return new WindowedStorageAdapter ( new QueryableIndexStorageAdapter ( index ) , segment . getInterval ( ) ) ; } catch ( IOException ex ) { throw new RuntimeException ( ex ) ; } }
public void test() { try { logger . info ( "Getting storage path for segment [%s]" , segment . getSegment ( ) . getId ( ) ) ; Path path = new Path ( JobHelper . getURIFromSegment ( segment . getSegment ( ) ) ) ; logger . info ( "Fetch segment files from [%s]" , path ) ; File dir = FileUtils . createTempDir ( ) ; tmpSegmentDirs . add ( dir ) ; logger . info ( "Locally storing fetched segment at [%s]" , dir ) ; JobHelper . unzipNoGuava ( path , context . getConfiguration ( ) , dir , context , null ) ; logger . info ( "finished fetching segment files" ) ; QueryableIndex index = HadoopDruidIndexerConfig . INDEX_IO . loadIndex ( dir ) ; indexes . add ( index ) ; numRows += index . getNumRows ( ) ; return new WindowedStorageAdapter ( new QueryableIndexStorageAdapter ( index ) , segment . getInterval ( ) ) ; } catch ( IOException ex ) { throw new RuntimeException ( ex ) ; } }
public void test() { try { return keycloakclient . isUserExist ( authzToken . getAccessToken ( ) , gatewayId , username ) ; } catch ( Exception ex ) { String msg = "Error while checking if user account exists, reason: " + ex . getMessage ( ) ; logger . error ( msg , ex ) ; throw new IamAdminServicesException ( msg ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( "User agent " + userAgent ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( "Invalid WebDAV path " + httpServletRequest . getPathInfo ( ) ) ; } }
public void test() { if ( logError ) { _log . error ( webDAVException , webDAVException ) ; } else-if ( _log . isWarnEnabled ( ) ) { _log . warn ( webDAVException , webDAVException ) ; } }
public void test() { if ( logError ) { _log . error ( webDAVException , webDAVException ) ; } else-if ( _log . isWarnEnabled ( ) ) { _log . warn ( webDAVException , webDAVException ) ; } }
public void test() { try { code_block = IfStatement ; WebDAVStorage storage = getStorage ( httpServletRequest ) ; code_block = IfStatement ; code_block = IfStatement ; PermissionChecker permissionChecker = null ; String remoteUser = httpServletRequest . getRemoteUser ( ) ; code_block = IfStatement ; MethodFactory methodFactory = storage . getMethodFactory ( ) ; Method method = methodFactory . create ( httpServletRequest ) ; code_block = TryStatement ;  } catch ( Exception exception ) { _log . error ( exception , exception ) ; } finally { httpServletResponse . setStatus ( status ) ; code_block = IfStatement ; } }
@ Override public void connectionClosed ( final IOException e ) { LOGGER . info ( "Connection ({}) closed." , this . connection , e ) ; this . iec60870ConnectionRegistry . unregisterConnection ( this . connection ) ; }
public void test() { try { addBranchIfNeeded ( clientDn ) ; permission . setDn ( getDn ( clientDn , permission . getTicket ( ) ) ) ; ldapEntryManager . persist ( permission ) ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { SipProvider p = null ; Boolean isIpv6 = false ; InetAddress address = InetAddress . getByName ( messageChannel . getHost ( ) ) ; code_block = IfStatement ; ResponseEvent event = new ResponseEvent ( new BalancerAppContent ( p , isIpv6 ) , null , null , response ) ; balancerRunner . balancerContext . forwarder . processResponse ( event ) ; } catch ( Exception e ) { logger . error ( "A Problem happened in the BalancerValve on response " + response , e ) ; return false ; } }
public void enforce ( IAgreement agreement , Date since , boolean isLastExecution ) { logger . debug ( "enforce(agreement={},since={})" , agreement . getAgreementId ( ) , since ) ; final Date now = new Date ( ) ; checkInitialized ( true ) ; Map < IGuaranteeTerm , IMetricsRetrieverV2 . RetrievalItem > retrievalItems = buildRetrievalItems ( agreement , since , now , isLastExecution ) ; Map < IGuaranteeTerm , List < IMonitoringMetric > > metricsMap ; code_block = IfStatement ; enforce ( agreement , metricsMap ) ; }
public void test() { try { String vmIp = getFirstReachableAddress ( node , setup ) ; int port = node . getLoginPort ( ) ; inferredHostAndPort = HostAndPort . fromParts ( vmIp , port ) ; } catch ( Exception e ) { LOG . warn ( "Error reaching aws-ec2 instance " + node . getId ( ) + "@" + node . getLocation ( ) + " on port " + node . getLoginPort ( ) + "; falling back to jclouds metadata for address" , e ) ; } }
public void test() { if ( isWindows ( node , setup ) ) { LOG . warn ( "Cannot query aws-ec2 Windows instance " + node . getId ( ) + "@" + node . getLocation ( ) + " over ssh for its hostname; falling back to jclouds metadata for address" ) ; } else { HostAndPort hostAndPortToUse = sshHostAndPort . isPresent ( ) ? sshHostAndPort . get ( ) : inferredHostAndPort ; code_block = TryStatement ;  } }
public void test() { try { String vmIp = getFirstReachableAddress ( node , setup ) ; int port = node . getLoginPort ( ) ; inferredHostAndPort = HostAndPort . fromParts ( vmIp , port ) ; } catch ( Exception e ) { LOG . warn ( "Error reaching aws-ec2 instance " + node . getId ( ) + "@" + node . getLocation ( ) + " on port " + node . getLoginPort ( ) + "; falling back to jclouds metadata for address" , e ) ; } }
public void test() { try { application = new AccessInternalMain ( ACCESS_CONF , AccessInternalResourceImplTest . class , null ) ; application . start ( ) ; RestAssured . port = port ; RestAssured . basePath = ACCESS_RESOURCE_URI ; LOGGER . debug ( "Beginning tests" ) ; } catch ( final VitamApplicationServerException e ) { LOGGER . error ( e ) ; throw new IllegalStateException ( "Cannot start the Access Application Server" , e ) ; } }
public void test() { try { application = new AccessInternalMain ( ACCESS_CONF , AccessInternalResourceImplTest . class , null ) ; application . start ( ) ; RestAssured . port = port ; RestAssured . basePath = ACCESS_RESOURCE_URI ; LOGGER . debug ( "Beginning tests" ) ; } catch ( final VitamApplicationServerException e ) { LOGGER . error ( e ) ; throw new IllegalStateException ( "Cannot start the Access Application Server" , e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( DLAppServiceUtil . class , "checkOutFileEntry" , _checkOutFileEntryParameterTypes14 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , fileEntryId , owner , expirationTime , serviceContext ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . portal . kernel . repository . model . FileEntry ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( t == null ) { logger . debug ( msg ) ; } else { logger . debug ( msg , t ) ; } }
public void test() { if ( t == null ) { logger . debug ( msg ) ; } else { logger . debug ( msg , t ) ; } }
public void test() { if ( t == null ) { logger . info ( msg ) ; } else { logger . info ( msg , t ) ; } }
public void test() { if ( t == null ) { logger . info ( msg ) ; } else { logger . info ( msg , t ) ; } }
public void test() { if ( t == null ) { logger . warn ( msg ) ; } else { logger . warn ( msg , t ) ; } }
public void test() { if ( t == null ) { logger . warn ( msg ) ; } else { logger . warn ( msg , t ) ; } }
public void test() { if ( t == null ) { logger . error ( msg ) ; } else { logger . error ( msg , t ) ; } }
public void test() { if ( t == null ) { logger . error ( msg ) ; } else { logger . error ( msg , t ) ; } }
@ Override public void disposeModel ( ) { LOG . debug ( "Disposing Model on CamelContext" ) ; model = null ; }
public void test() { try { processor . processTimeout ( entry ) ; } catch ( Exception e ) { LOGGER . error ( "process sync timeout request error" , e ) ; } }
public void test() { try { String targetNode = this . getTargetNode ( ) ; Boolean open = this . getOpen ( ) ; code_block = IfStatement ; } catch ( Throwable t ) { _logger . error ( "error in buildTree" , t ) ; } }
public static void inoutdemo ( int in1 , int [ ] inout1 , int [ ] out1 ) { LOG . info ( "calling inoutdemo: {}, {}" , inout1 [ 0 ] , out1 [ 0 ] ) ; inout1 [ 0 ] = 1 ; out1 [ 0 ] = 2 ; }
@ Test public void testInsertEmptyWhere ( ) throws Exception { logger . debug ( "executing test testInsertEmptyWhere" ) ; StringBuilder update = new StringBuilder ( ) ; update . append ( getNamespaceDeclarations ( ) ) ; update . append ( "INSERT code_block = "" ; WHERE code_block = "" ;
@ Test @ Atomic public void test09 ( ) { TxIntrospector txIntrospector = FenixFramework . getTransaction ( ) . getTxIntrospector ( ) ; printTest ( "Create a new book and modify its 1-* relation with '" + LITTLE + "' twice\n\t" + "(New: [ '" + ECLIPSE + "']; DM: []; M: ['" + LITTLE + "']; RCL: ['PublisherWithBooks'])" ) ; VampireBook eclipse = createEclipse ( txIntrospector ) ; Publisher little = getPublisherByName ( LITTLE ) ; eclipse . setPublisher ( little ) ; little . addPublishedBook ( eclipse ) ; assertFalse ( txIntrospector . getModifiedObjects ( ) . contains ( eclipse ) ) ; assertFalse ( txIntrospector . getDirectlyModifiedObjects ( ) . contains ( eclipse ) ) ; assertTrue ( txIntrospector . getModifiedObjects ( ) . contains ( little ) ) ; assertFalse ( txIntrospector . getDirectlyModifiedObjects ( ) . contains ( little ) ) ; logger . trace ( txIntrospector . toString ( ) ) ; }
public void test() { try { lucenePartitionRepositoryManager . computeRepository ( bucketId ) ; } catch ( PrimaryBucketException | AlreadyClosedException e ) { logger . debug ( "Exception while cleaning up Lucene Index Repository" , e ) ; } }
@ ModelAttribute ( "researchGroupTitle" ) private String fillResearchGroupTitleForExperiment ( @ RequestParam ( "experimentId" ) String idString ) { log . debug ( "Loading experiment info" ) ; int experimentId = Integer . parseInt ( idString ) ; Experiment experiment = ( Experiment ) experimentDao . read ( experimentId ) ; return experiment . getResearchGroup ( ) . getTitle ( ) ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( sb . toString ( ) ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceCurrencyServiceUtil . class , "setActive" , _setActiveParameterTypes9 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , commerceCurrencyId , active ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . commerce . currency . model . CommerceCurrency ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( number != allocatedNumber ) { LOG . warn ( "Expect to allocate {} taskManagers. Actually allocate {} taskManagers." , number , allocatedNumber ) ; } }
@ ResponseStatus ( HttpStatus . OK ) @ RequestMapping ( value = "/bundles/{bundleId}/uninstallconfig" , method = RequestMethod . POST ) public void uninstallBundleWithConfig ( @ PathVariable long bundleId ) throws BundleException { moduleAdminService . uninstallBundle ( bundleId , true ) ; LOGGER . info ( "Bundle [{}] removed successfully" ) ; }
public void test() { try { String prefix = CONFIG_PROVIDERS_CONFIG + "." + entry . getKey ( ) + CONFIG_PROVIDERS_PARAM ; Map < String , ? > configProperties = configProviderProperties ( prefix , providerConfigProperties ) ; ConfigProvider provider = Utils . newInstance ( entry . getValue ( ) , ConfigProvider . class ) ; provider . configure ( configProperties ) ; configProviderInstances . put ( entry . getKey ( ) , provider ) ; } catch ( ClassNotFoundException e ) { log . error ( "ClassNotFoundException exception occurred: " + entry . getValue ( ) ) ; throw new ConfigException ( "Invalid config:" + entry . getValue ( ) + " ClassNotFoundException exception occurred" , e ) ; } }
public static void reset ( ) { freemarkerConfig = null ; freemarkerConfig = new Configuration ( ) ; freemarkerConfig . setObjectWrapper ( new DefaultObjectWrapper ( ) ) ; freemarkerConfig . clearTemplateCache ( ) ; logger . debug ( "Created new freemarker template processor for DocUtils" ) ; }
public void test() { try { code_block = WhileStatement ; message . saveChanges ( ) ; } catch ( MessagingException e ) { LOGGER . error ( "MessagingException in recoverAttachment" , e ) ; } catch ( IOException e ) { LOGGER . error ( "IOException in recoverAttachment" , e ) ; } }
public void test() { try { code_block = WhileStatement ; message . saveChanges ( ) ; } catch ( MessagingException e ) { LOGGER . error ( "MessagingException in recoverAttachment" , e ) ; } catch ( IOException e ) { LOGGER . error ( "IOException in recoverAttachment" , e ) ; } }
public void test() { if ( vm == null ) { logger . warn ( "Cannot find VM by VM path " + node . getVmName ( ) + " whose mobid is " + node . getVmMobId ( ) + "." ) ; } else { node . setVmMobId ( vm . getId ( ) ) ; } }
public void test() { if ( propertyMap instanceof Map ) { String excludePrefix = ServiceHelper . getContext ( ) . getPrefix ( ) ; Map < String , Object > properties = ( Map < String , Object > ) propertyMap ; Map < String , Object > filtered = properties . entrySet ( ) . stream ( ) . filter ( p -> ! ( p == null || p . getKey ( ) . startsWith ( excludePrefix ) ) ) . collect ( Collectors . toMap ( p -> p . getKey ( ) , p -> p . getValue ( ) ) ) ; ServiceHelper . getContext ( ) . addProperties ( ( Map ) propertyMap ) ; } else { log . warn ( "Unable to update service context properties." ) ; } }
public void test() { try { pstmt . executeQuery ( ) ; } catch ( SQLException e ) { s_logger . debug ( "Assuming that domain_id field doesn't exist in account_vlan_map table, no need to upgrade" ) ; return ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { try { fcall . sendResponse ( fb , msg , msgType , seqid ) ; } catch ( java . lang . Exception ex ) { _LOGGER . error ( "Exception writing to internal frame buffer" , ex ) ; fb . close ( ) ; } }
private void prepareDistinguishedNames ( CertificateRequestMessage msg ) { msg . setDistinguishedNames ( chooser . getConfig ( ) . getDistinguishedNames ( ) ) ; LOGGER . debug ( "DistinguishedNames: " + ArrayConverter . bytesToHexString ( msg . getDistinguishedNames ( ) . getValue ( ) ) ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Initialize {}" , this . getClass ( ) . getSimpleName ( ) ) ; } }
public void test() { try { k . cancel ( ) ; } catch ( Exception e ) { LOG . error ( "Error cancelling command selection key" , e ) ; } }
@ AfterGroups ( groups = "sharedHBase" ) public void afterGroups ( ITestContext context ) throws Exception { LOG . info ( "Tearing down OmidTestBase..." ) ; code_block = IfStatement ; getClient ( context ) . close ( ) . get ( ) ; getTSO ( context ) . stopAndWait ( ) ; TestUtils . waitForSocketNotListening ( "localhost" , 1234 , 1000 ) ; }
public void dataSource ( DataSource dataSource ) { ConnectionDataSourceConfig connectionConfig = new ConnectionDataSourceConfig ( dataSource ) ; connectionConfig . setDbName ( dbName ) ; connectionConfig . setEnvironment ( environment ) ; connectionConfig . setTesting ( testing ) ; DBConfiguration . addConnectionConfig ( connectionConfig ) ; LOGGER . info ( "Configuring JDBC connection using data source: {}" , dataSource . toString ( ) ) ; }
public void test() { if ( isClosed ( ) ) { return false ; } }
public void test() { try { closeableHttpClient . close ( ) ; } catch ( IOException e ) { log . warn ( "Can't close a http client" , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Starting thread to " + p2pReaderName ( ) ) ; } }
public void test() { try { String response = caseManagementServiceBase . startCase ( containerId , caseDefId , payload , type ) ; logger . debug ( "Returning CREATED response for start case with content '{}'" , response ) ; return createResponse ( response , v , Response . Status . CREATED , customHeaders ) ; } catch ( CaseDefinitionNotFoundException e ) { return notFound ( MessageFormat . format ( CASE_DEFINITION_NOT_FOUND , caseDefId , containerId ) , v , customHeaders ) ; } }
public void test() { try { helper . addTokenFromUserToJobConf ( ugi , jobConf ) ; } catch ( IOException e ) { log . info ( "Ignoring exception, likely coming from Hadoop 1" , e ) ; return ; } }
public void test() { try { logTimeseriesDeleted ( user , entityId , new ArrayList < > ( keys ) , null ) ; } catch ( ThingsboardException e ) { log . error ( "Failed to log timeseries delete" , e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( result . wasAcknowledged ( ) ) { log . debug ( "Removed user {} from {} tracks" , user , result . getN ( ) ) ; } else { log . error ( "Error removing user {} from tracks: {}" , user , result ) ; } }
public void test() { if ( result . wasAcknowledged ( ) ) { log . debug ( "Removed user {} from {} tracks" , user , result . getN ( ) ) ; } else { log . error ( "Error removing user {} from tracks: {}" , user , result ) ; } }
public void test() { if ( ! tadCache . get ( deviceId ) . containsKey ( descriptor ) ) { log . trace ( "Creating new TAD..." ) ; Pair < DataBuffer , DataBuffer > buffers = super . getTADOnlyShapeInfo ( array , dimension ) ; if ( buffers . getFirst ( ) != array . shapeInfoDataBuffer ( ) ) AtomicAllocator . getInstance ( ) . moveToConstant ( buffers . getFirst ( ) ) ; if ( buffers . getSecond ( ) != null ) AtomicAllocator . getInstance ( ) . moveToConstant ( buffers . getSecond ( ) ) ; tadCache . get ( deviceId ) . put ( descriptor , buffers ) ; bytes . addAndGet ( ( buffers . getFirst ( ) . length ( ) * 4 ) ) ; if ( buffers . getSecond ( ) != null ) bytes . addAndGet ( buffers . getSecond ( ) . length ( ) * 8 ) ; log . trace ( "Using TAD from cache..." ) ; } }
public void test() { if ( ! tadCache . get ( deviceId ) . containsKey ( descriptor ) ) { log . trace ( "Creating new TAD..." ) ; Pair < DataBuffer , DataBuffer > buffers = super . getTADOnlyShapeInfo ( array , dimension ) ; if ( buffers . getFirst ( ) != array . shapeInfoDataBuffer ( ) ) AtomicAllocator . getInstance ( ) . moveToConstant ( buffers . getFirst ( ) ) ; if ( buffers . getSecond ( ) != null ) AtomicAllocator . getInstance ( ) . moveToConstant ( buffers . getSecond ( ) ) ; tadCache . get ( deviceId ) . put ( descriptor , buffers ) ; bytes . addAndGet ( ( buffers . getFirst ( ) . length ( ) * 4 ) ) ; if ( buffers . getSecond ( ) != null ) bytes . addAndGet ( buffers . getSecond ( ) . length ( ) * 8 ) ; log . trace ( "Using TAD from cache..." ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Prefilled connection {} connection count is now {}" , connection , connectionAccounting . getCount ( ) ) ; } }
public void test() { try { connection = createPooledConnection ( ) ; code_block = IfStatement ; getPoolStats ( ) . incPrefillConnect ( ) ; availableConnectionManager . addLast ( connection , false ) ; code_block = IfStatement ; return true ; } catch ( ServerConnectivityException ex ) { logger . info ( String . format ( "Unable to prefill pool to minimum because: %s" , ex . getMessage ( ) ) ) ; return false ; } finally { code_block = IfStatement ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Unable to prefill pool to minimum, connection count is now {}" , this :: getConnectionCount ) ; } }
public void test() { try { BulkByScrollResponse bulkResponse = client . deleteByQuery ( request , RequestOptions . DEFAULT ) ; LOGGER . debug ( String . format ( "Deleted %d models in the index '%s'" , bulkResponse . getTotal ( ) , index ) ) ; } catch ( IOException e ) { throw new IndexingException ( String . format ( "Error deleting all models in the '%s' index." , index ) , e ) ; } }
public void test() { try { String filePath = base + "_" + WorkerContext . get ( ) . getWorkerAttemptId ( ) + "_" + taskIndex + "_" + currentWriteFileIndex ++ ; Path newPath = lDirAlloc . getLocalPathForWrite ( filePath , maxSizePerFile , WorkerContext . get ( ) . getConf ( ) ) ; LOG . info ( "KVDiskStorage create a new file, filePath = " + newPath ) ; return newPath . toUri ( ) . toString ( ) ; } catch ( Exception e ) { throw new IOException ( "task UnInitializtionException" , e ) ; } }
@ Test public void testStringResult ( ) throws ServiceFailureException { LOGGER . info ( "  testStringResult" ) ; ObservationDao doa = service . observations ( ) ; Observation b1 = new Observation ( "fourty two" , DATASTREAMS . get ( 0 ) ) ; doa . create ( b1 ) ; OBSERVATIONS . add ( b1 ) ; Observation found ; found = doa . find ( b1 . getId ( ) ) ; String message = "Expected result to be a String." ; Assert . assertEquals ( message , b1 . getResult ( ) , found . getResult ( ) ) ; }
public void test() { if ( ! java . util . Objects . deepEquals ( byteBufferOut , expectedByteBuffer ) ) { logger . info ( name . getMethodName ( ) + " - callback - invalid content" ) ; subscribeBroadcastWithSingleByteBufferParameterCallbackResult = false ; } else { logger . info ( name . getMethodName ( ) + " - callback - content OK" ) ; subscribeBroadcastWithSingleByteBufferParameterCallbackResult = true ; } }
public void test() { if ( ! java . util . Objects . deepEquals ( byteBufferOut , expectedByteBuffer ) ) { logger . info ( name . getMethodName ( ) + " - callback - invalid content" ) ; subscribeBroadcastWithSingleByteBufferParameterCallbackResult = false ; } else { logger . info ( name . getMethodName ( ) + " - callback - content OK" ) ; subscribeBroadcastWithSingleByteBufferParameterCallbackResult = true ; } }
public void test() { { logger . info ( name . getMethodName ( ) + " - callback - error" ) ; subscribeBroadcastWithSingleByteBufferParameterCallbackResult = false ; resultsAvailable . release ( ) ; } }
public void test() { try { subscriptionIdFuture = testInterfaceProxy . subscribeToBroadcastWithSingleByteBufferParameterBroadcast ( new BroadcastWithSingleByteBufferParameterBroadcastAdapter ( ) code_block = "" ; , new MulticastSubscriptionQos ( ) , partitions ) ; subscriptionId = subscriptionIdFuture . get ( 10000 ) ; logger . info ( name . getMethodName ( ) + " - subscription successful, subscriptionId = " + subscriptionId ) ; logger . info ( name . getMethodName ( ) + " - Invoking fire method" ) ; testInterfaceProxy . methodToFireBroadcastWithSingleByteBufferParameter ( expectedByteBuffer , partitions ) ; logger . info ( name . getMethodName ( ) + " - fire method invoked" ) ; Assert . assertTrue ( name . getMethodName ( ) + " - FAILED - callback was not received in time" , resultsAvailable . tryAcquire ( 2 , TimeUnit . SECONDS ) ) ; logger . info ( name . getMethodName ( ) + " - results received" ) ; Assert . assertTrue ( name . getMethodName ( ) + " - FAILED - callback got called but received unexpected error or publication event" , subscribeBroadcastWithSingleByteBufferParameterCallbackResult ) ; code_block = TryStatement ;  } catch ( Exception e ) { fail ( name . getMethodName ( ) + " - FAILED - caught unexpected exception: " + e . getMessage ( ) ) ; } }
public void test() { try { subscriptionIdFuture = testInterfaceProxy . subscribeToBroadcastWithSingleByteBufferParameterBroadcast ( new BroadcastWithSingleByteBufferParameterBroadcastAdapter ( ) code_block = "" ; , new MulticastSubscriptionQos ( ) , partitions ) ; subscriptionId = subscriptionIdFuture . get ( 10000 ) ; logger . info ( name . getMethodName ( ) + " - subscription successful, subscriptionId = " + subscriptionId ) ; logger . info ( name . getMethodName ( ) + " - Invoking fire method" ) ; testInterfaceProxy . methodToFireBroadcastWithSingleByteBufferParameter ( expectedByteBuffer , partitions ) ; logger . info ( name . getMethodName ( ) + " - fire method invoked" ) ; Assert . assertTrue ( name . getMethodName ( ) + " - FAILED - callback was not received in time" , resultsAvailable . tryAcquire ( 2 , TimeUnit . SECONDS ) ) ; logger . info ( name . getMethodName ( ) + " - results received" ) ; Assert . assertTrue ( name . getMethodName ( ) + " - FAILED - callback got called but received unexpected error or publication event" , subscribeBroadcastWithSingleByteBufferParameterCallbackResult ) ; code_block = TryStatement ;  } catch ( Exception e ) { fail ( name . getMethodName ( ) + " - FAILED - caught unexpected exception: " + e . getMessage ( ) ) ; } }
public void test() { try { subscriptionIdFuture = testInterfaceProxy . subscribeToBroadcastWithSingleByteBufferParameterBroadcast ( new BroadcastWithSingleByteBufferParameterBroadcastAdapter ( ) code_block = "" ; , new MulticastSubscriptionQos ( ) , partitions ) ; subscriptionId = subscriptionIdFuture . get ( 10000 ) ; logger . info ( name . getMethodName ( ) + " - subscription successful, subscriptionId = " + subscriptionId ) ; logger . info ( name . getMethodName ( ) + " - Invoking fire method" ) ; testInterfaceProxy . methodToFireBroadcastWithSingleByteBufferParameter ( expectedByteBuffer , partitions ) ; logger . info ( name . getMethodName ( ) + " - fire method invoked" ) ; Assert . assertTrue ( name . getMethodName ( ) + " - FAILED - callback was not received in time" , resultsAvailable . tryAcquire ( 2 , TimeUnit . SECONDS ) ) ; logger . info ( name . getMethodName ( ) + " - results received" ) ; Assert . assertTrue ( name . getMethodName ( ) + " - FAILED - callback got called but received unexpected error or publication event" , subscribeBroadcastWithSingleByteBufferParameterCallbackResult ) ; code_block = TryStatement ;  } catch ( Exception e ) { fail ( name . getMethodName ( ) + " - FAILED - caught unexpected exception: " + e . getMessage ( ) ) ; } }
public void test() { try { subscriptionIdFuture = testInterfaceProxy . subscribeToBroadcastWithSingleByteBufferParameterBroadcast ( new BroadcastWithSingleByteBufferParameterBroadcastAdapter ( ) code_block = "" ; , new MulticastSubscriptionQos ( ) , partitions ) ; subscriptionId = subscriptionIdFuture . get ( 10000 ) ; logger . info ( name . getMethodName ( ) + " - subscription successful, subscriptionId = " + subscriptionId ) ; logger . info ( name . getMethodName ( ) + " - Invoking fire method" ) ; testInterfaceProxy . methodToFireBroadcastWithSingleByteBufferParameter ( expectedByteBuffer , partitions ) ; logger . info ( name . getMethodName ( ) + " - fire method invoked" ) ; Assert . assertTrue ( name . getMethodName ( ) + " - FAILED - callback was not received in time" , resultsAvailable . tryAcquire ( 2 , TimeUnit . SECONDS ) ) ; logger . info ( name . getMethodName ( ) + " - results received" ) ; Assert . assertTrue ( name . getMethodName ( ) + " - FAILED - callback got called but received unexpected error or publication event" , subscribeBroadcastWithSingleByteBufferParameterCallbackResult ) ; code_block = TryStatement ;  } catch ( Exception e ) { fail ( name . getMethodName ( ) + " - FAILED - caught unexpected exception: " + e . getMessage ( ) ) ; } }
public void test() { try { testInterfaceProxy . unsubscribeFromBroadcastWithSingleByteBufferParameterBroadcast ( subscriptionId ) ; logger . info ( name . getMethodName ( ) + " - unsubscribe successful" ) ; } catch ( Exception e ) { fail ( name . getMethodName ( ) + " - FAILED - caught unexpected exception on unsubscribe: " + e . getMessage ( ) ) ; } }
public void test() { try { validator . validate ( xmlFile ) ; report . setValid ( errorHandler . getErrors ( ) . isEmpty ( ) ) ; code_block = ForStatement ; } catch ( SAXException e ) { LOGGER . error ( "Error validating preservation binary " + binary . getStoragePath ( ) , e ) ; report . setValid ( false ) ; code_block = ForStatement ; } }
public void test() { try { root = new File ( new URI ( location ) ) ; code_block = IfStatement ; } catch ( Exception e ) { log . error ( "Failed to locate bundle at " + location ) ; } }
public void test() { try { URL url = file . toURI ( ) . toURL ( ) ; log . info ( "Deploying external component: " + url ) ; deployment . urls = new ArrayList < URL > ( ) ; ctx . deploy ( url ) ; deployment . urls . add ( url ) ; } catch ( Exception e ) { log . error ( "Failed to deploy: " + file , e ) ; } }
public void test() { try { URL url = file . toURI ( ) . toURL ( ) ; log . info ( "Deploying external component: " + url ) ; deployment . urls = new ArrayList < URL > ( ) ; ctx . deploy ( url ) ; deployment . urls . add ( url ) ; } catch ( Exception e ) { log . error ( "Failed to deploy: " + file , e ) ; } }
public void test() { try { return JsonHandler . getFromString ( objectList , List . class , JsonNode . class ) ; } catch ( InvalidParseOperationException e ) { LOGGER . error ( e ) ; throw new IllegalArgumentException ( e ) ; } }
public void test() { if ( e . getCause ( ) instanceof NoClassDefFoundError ) { LOG . info ( "Could not load factory due to missing dependencies." ) ; } else { throw e ; } }
@ Override public void start ( ) { logger . info ( "{} - {}" , DESCRIPTION , MongoDBHelper . getRiverVersion ( ) ) ; Status status = MongoDBRiverHelper . getRiverStatus ( esClient , riverName . getName ( ) ) ; code_block = IfStatement ; code_block = IfStatement ; statusThread = EsExecutors . daemonThreadFactory ( settings . globalSettings ( ) , "mongodb_river_status:" + definition . getIndexName ( ) ) . newThread ( new StatusChecker ( this , definition , context ) ) ; statusThread . start ( ) ; }
public void test() { if ( status == Status . IMPORT_FAILED || status == Status . INITIAL_IMPORT_FAILED || status == Status . SCRIPT_IMPORT_FAILED || status == Status . START_FAILED ) { logger . error ( "Cannot start. Current status is {}" , status ) ; return ; } }
public void test() { if ( status == Status . STOPPED ) { context . setStatus ( Status . STOPPED ) ; logger . info ( "River is currently disabled and will not be started" ) ; } else { context . setStatus ( Status . START_PENDING ) ; MongoDBRiverHelper . setRiverStatus ( esClient , riverName . getName ( ) , Status . RUNNING ) ; logger . info ( "Startup pending" ) ; } }
public void test() { if ( status == Status . STOPPED ) { context . setStatus ( Status . STOPPED ) ; logger . info ( "River is currently disabled and will not be started" ) ; } else { context . setStatus ( Status . START_PENDING ) ; MongoDBRiverHelper . setRiverStatus ( esClient , riverName . getName ( ) , Status . RUNNING ) ; logger . info ( "Startup pending" ) ; } }
public void test() { if ( e instanceof SymjaMMANotFoundException ) { logger . warn ( e . getLocalizedMessage ( ) ) ; r = TryResult . createError ( e . getLocalizedMessage ( ) ) ; } else { e . printStackTrace ( ) ; r = TryResult . createError ( e . getLocalizedMessage ( ) ) ; } }
private void startZookeeper ( ) { client = CuratorUtils . newCuratorFrameworkClient ( connectString , logger ) ; client . start ( ) ; logger . info ( "Curator framework start operation invoked" ) ; int startupTimeOutMs = Integer . parseInt ( System . getProperty ( Constants . Properties . ZK_STARTUP_TIMEOUT , "30000" ) ) ; code_block = TryStatement ;  code_block = IfStatement ; logger . info ( "CuratorFramework client started successfully" ) ; }
public void test() { try { logger . info ( "Waiting to connect to zookeeper, startupTimeout : {}" , startupTimeOutMs ) ; client . blockUntilConnected ( startupTimeOutMs , TimeUnit . MILLISECONDS ) ; } catch ( InterruptedException ex ) { logger . error ( "Interrupted while waiting to connect zookeeper (connectString : {}) : {}" , ex , connectString ) ; } }
private void startZookeeper ( ) { client = CuratorUtils . newCuratorFrameworkClient ( connectString , logger ) ; client . start ( ) ; logger . info ( "Curator framework start operation invoked" ) ; int startupTimeOutMs = Integer . parseInt ( System . getProperty ( Constants . Properties . ZK_STARTUP_TIMEOUT , "30000" ) ) ; code_block = TryStatement ;  code_block = IfStatement ; logger . info ( "CuratorFramework client started successfully" ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Executing {} command..." , getName ( ) ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Getting deployment policy {}" , deploymentPolicyId ) ; } }
protected List < NodeRef > processNodes ( final Long batchSize , Long maxNodeId , final Pair < Long , QName > recordAspectPair , Long totalNumberOfRecordsToProcess , final BufferedWriter out , final boolean attach ) { final Long maxRecordsToProcess = totalNumberOfRecordsToProcess ; final List < NodeRef > processedNodes = new ArrayList < > ( ) ; logger . info ( MESSAGE_PROCESSING_BEGIN ) ; code_block = ForStatement ; logger . info ( MESSAGE_PROCESSING_END ) ; return processedNodes ; }
public void test() { for ( Long nodeId : nodeIds ) { code_block = IfStatement ; NodeRef record = nodeDAO . getNodePair ( nodeId ) . getSecond ( ) ; String recordName = ( String ) nodeService . getProperty ( record , ContentModel . PROP_NAME ) ; logger . info ( MessageFormat . format ( MESSAGE_PROCESSING_RECORD_BEGIN_TEMPLATE , recordName ) ) ; processNode ( record ) ; logger . info ( MessageFormat . format ( MESSAGE_PROCESSING_RECORD_END_TEMPLATE , recordName ) ) ; processedNodes . add ( record ) ; code_block = IfStatement ; } }
public void test() { for ( Long nodeId : nodeIds ) { code_block = IfStatement ; NodeRef record = nodeDAO . getNodePair ( nodeId ) . getSecond ( ) ; String recordName = ( String ) nodeService . getProperty ( record , ContentModel . PROP_NAME ) ; logger . info ( MessageFormat . format ( MESSAGE_PROCESSING_RECORD_BEGIN_TEMPLATE , recordName ) ) ; processNode ( record ) ; logger . info ( MessageFormat . format ( MESSAGE_PROCESSING_RECORD_END_TEMPLATE , recordName ) ) ; processedNodes . add ( record ) ; code_block = IfStatement ; } }
protected List < NodeRef > processNodes ( final Long batchSize , Long maxNodeId , final Pair < Long , QName > recordAspectPair , Long totalNumberOfRecordsToProcess , final BufferedWriter out , final boolean attach ) { final Long maxRecordsToProcess = totalNumberOfRecordsToProcess ; final List < NodeRef > processedNodes = new ArrayList < > ( ) ; logger . info ( MESSAGE_PROCESSING_BEGIN ) ; code_block = ForStatement ; logger . info ( MESSAGE_PROCESSING_END ) ; return processedNodes ; }
public void test() { try { code_block = IfStatement ; lastOffset = offset ; return true ; } catch ( EventHubException ex ) { logger . error ( "failed to open eventhub receiver: " + ex . getMessage ( ) ) ; return false ; } }
public void test() { if ( verbose > 0 ) { LOGGER . info ( "Resizing node table from {} to {}" , oldsize , newsize ) ; } }
public void test() { if ( currentEmailTemplate == null ) { LOG . error ( "Email template is null" ) ; return false ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "id=" + id ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Found conflicting container with uuid {" + conflictingContainer . getUuid ( ) + "} of node {" + conflictingNode . getUuid ( ) + "}" ) ; } }
@ GET @ Timed @ Produces ( APPLICATION_JSON_WITH_CHARSET ) public String get ( @ Context GraphManager manager , @ PathParam ( "graph" ) String graph , @ QueryParam ( "source" ) String sourceV , @ QueryParam ( "direction" ) String direction , @ QueryParam ( "label" ) String edgeLabel , @ QueryParam ( "max_depth" ) int depth , @ QueryParam ( "max_degree" ) @ DefaultValue ( DEFAULT_MAX_DEGREE ) long maxDegree , @ QueryParam ( "capacity" ) @ DefaultValue ( DEFAULT_CAPACITY ) long capacity , @ QueryParam ( "limit" ) @ DefaultValue ( DEFAULT_PATHS_LIMIT ) long limit ) { LOG . debug ( "Graph [{}] get rays paths from '{}' with " + "direction '{}', edge label '{}', max depth '{}', " + "max degree '{}', capacity '{}' and limit '{}'" , graph , sourceV , direction , edgeLabel , depth , maxDegree , capacity , limit ) ; Id source = VertexAPI . checkAndParseVertexId ( sourceV ) ; Directions dir = Directions . convert ( EdgeAPI . parseDirection ( direction ) ) ; HugeGraph g = graph ( manager , graph ) ; SubGraphTraverser traverser = new SubGraphTraverser ( g ) ; HugeTraverser . PathSet paths = traverser . rays ( source , dir , edgeLabel , depth , maxDegree , capacity , limit ) ; return manager . serializer ( g ) . writePaths ( "rays" , paths , false ) ; }
@ Override public void geoServerGetStyleCommand ( final org . locationtech . geowave . service . grpc . protobuf . GeoServerGetStyleCommandParametersProtos request , final StreamObserver < org . locationtech . geowave . service . grpc . protobuf . GeoWaveReturnTypesProtos . StringResponseProtos > responseObserver ) { final GeoServerGetStyleCommand cmd = new GeoServerGetStyleCommand ( ) ; final Map < FieldDescriptor , Object > m = request . getAllFields ( ) ; GeoWaveGrpcServiceCommandUtil . setGrpcToCommandFields ( m , cmd ) ; final File configFile = GeoWaveGrpcServiceOptions . geowaveConfigFile ; final OperationParams params = new ManualOperationParams ( ) ; params . getContext ( ) . put ( ConfigOptions . PROPERTIES_FILE_CONTEXT , configFile ) ; cmd . prepare ( params ) ; LOGGER . info ( "Executing GeoServerGetStyleCommand..." ) ; code_block = TryStatement ;  }
public void test() { try { final String result = cmd . computeResults ( params ) ; final StringResponseProtos resp = StringResponseProtos . newBuilder ( ) . setResponseValue ( result ) . build ( ) ; responseObserver . onNext ( resp ) ; responseObserver . onCompleted ( ) ; } catch ( final Exception e ) { LOGGER . error ( "Exception encountered executing command" , e ) ; responseObserver . onError ( e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( MDRActionServiceUtil . class , "updateAction" , _updateActionParameterTypes6 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , actionId , nameMap , descriptionMap , type , typeSettingsUnicodeProperties , serviceContext ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . mobile . device . rules . model . MDRAction ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { createQueue ( config . setTemporary ( false ) . setTransient ( false ) . setAutoCreated ( false ) . setConfigurationManaged ( true ) . setAutoCreateAddress ( true ) , false ) ; } catch ( ActiveMQQueueExistsException e ) { ActiveMQServerLogger . LOGGER . warn ( e . getMessage ( ) ) ; } }
public void test() { try { directProducer . sendBodyAndHeader ( "Message " + i , RabbitMQConstants . ROUTING_KEY , "rk3" ) ; } catch ( CamelExecutionException e ) { log . debug ( "Can not send message" , e ) ; failedMessages ++ ; } }
public void test() { if ( _log . isInfoEnabled ( ) ) { _log . info ( "...Locking the singleton world of the DB definition!" ) ; } }
public void test() { try { addresses = singletonList ( nominatimRestClient . exchange ( url , GET , getHttpEntity ( ) , Address . class ) . getBody ( ) ) ; LOG . debug ( "{} search result(s) found for URL {}" , addresses . size ( ) , url ) ; } catch ( RestClientException e ) { addresses = null ; } }
public void test() { try { conn = this . getConnection ( ) ; conn . setAutoCommit ( false ) ; this . insertIdeaInstance ( ideainstance , conn ) ; this . insertIdeaInstanceGroups ( ideainstance . getCode ( ) , ideainstance . getGroups ( ) , conn ) ; conn . commit ( ) ; } catch ( Throwable t ) { this . executeRollback ( conn ) ; _logger . error ( "Error creating ideainstance" , t ) ; throw new RuntimeException ( "Error creating ideainstance" , t ) ; } finally { this . closeDaoResources ( null , stat , conn ) ; } }
public void test() { if ( tlsTestsEnabled ) { LOG . info ( "TLS tests enabled so starting the TLS auth route" ) ; final String uri = "natsTlsAuth:test?sslContextParameters=ssl&secure=true" ; from ( uri ) . routeId ( "tls-auth" ) . bean ( natsResource , "storeMessage" ) ; } else { LOG . info ( "TLS tests NOT enabled, so NOT starting the TLS auth route" ) ; } }
public void test() { if ( tlsTestsEnabled ) { LOG . info ( "TLS tests enabled so starting the TLS auth route" ) ; final String uri = "natsTlsAuth:test?sslContextParameters=ssl&secure=true" ; from ( uri ) . routeId ( "tls-auth" ) . bean ( natsResource , "storeMessage" ) ; } else { LOG . info ( "TLS tests NOT enabled, so NOT starting the TLS auth route" ) ; } }
public void test() { try ( Transaction tx = ignite . transactions ( ) . txStart ( PESSIMISTIC , REPEATABLE_READ , timeout , 0 ) ) { int key1 = threadNum ; log . info ( ">>> Performs put [node=" + ( ( IgniteKernal ) ignite ) . localNode ( ) + ", tx=" + tx + ", key=" + key1 + ']' ) ; cache . put ( key1 , 0 ) ; barrier . await ( ) ; code_block = IfStatement ; tx . commit ( ) ; } catch ( Exception e ) { if ( hasCause ( e , TransactionTimeoutException . class ) ) timedOut . set ( true ) ; if ( hasCause ( e , TransactionDeadlockException . class ) ) deadlock . set ( true ) ; } }
public void test() { if ( threadNum == threads ) { log . info ( ">>> Performs sleep. [node=" + ( ( IgniteKernal ) ignite ) . localNode ( ) + ", tx=" + tx + ']' ) ; U . sleep ( timeout * 3 ) ; } else { int key2 = threadNum + 1 ; log . info ( ">>> Performs put [node=" + ( ( IgniteKernal ) ignite ) . localNode ( ) + ", tx=" + tx + ", key2=" + key2 + ']' ) ; cache . put ( key2 , 1 ) ; } }
public void test() { if ( threadNum == threads ) { log . info ( ">>> Performs sleep. [node=" + ( ( IgniteKernal ) ignite ) . localNode ( ) + ", tx=" + tx + ']' ) ; U . sleep ( timeout * 3 ) ; } else { int key2 = threadNum + 1 ; log . info ( ">>> Performs put [node=" + ( ( IgniteKernal ) ignite ) . localNode ( ) + ", tx=" + tx + ", key2=" + key2 + ']' ) ; cache . put ( key2 , 1 ) ; } }
public void test() { if ( ActiveMQRALogger . LOGGER . isTraceEnabled ( ) ) { ActiveMQRALogger . LOGGER . trace ( "getAcknowledgeMode()" ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( BookmarksEntryServiceUtil . class , "getEntry" , _getEntryParameterTypes6 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , entryId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . bookmarks . model . BookmarksEntry ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { URL url = new URL ( purlServerBaseURL + PURL_PATH + purl ) ; LOGGER . debug ( url . toString ( ) ) ; String data = "target=" + URLEncoder . encode ( target , StandardCharsets . UTF_8 ) ; data += "&maintainers=" + maintainers ; data += "&type=" + type ; LOGGER . debug ( data ) ; conn = ( HttpURLConnection ) url . openConnection ( ) ; conn . setRequestProperty ( COOKIE_HEADER_PARAM , cookie ) ; conn . setRequestMethod ( "POST" ) ; conn . setDoOutput ( true ) ; code_block = TryStatement ;  response = conn . getResponseCode ( ) ; code_block = IfStatement ; } catch ( Exception e ) { LOGGER . error ( e ) ; } finally { code_block = IfStatement ; } }
public void test() { while ( ( line = rd . readLine ( ) ) != null ) { LOGGER . error ( line ) ; } }
public void test() { try { URL url = new URL ( purlServerBaseURL + PURL_PATH + purl ) ; LOGGER . debug ( url . toString ( ) ) ; String data = "target=" + URLEncoder . encode ( target , StandardCharsets . UTF_8 ) ; data += "&maintainers=" + maintainers ; data += "&type=" + type ; LOGGER . debug ( data ) ; conn = ( HttpURLConnection ) url . openConnection ( ) ; conn . setRequestProperty ( COOKIE_HEADER_PARAM , cookie ) ; conn . setRequestMethod ( "POST" ) ; conn . setDoOutput ( true ) ; code_block = TryStatement ;  response = conn . getResponseCode ( ) ; code_block = IfStatement ; } catch ( Exception e ) { LOGGER . error ( e ) ; } finally { code_block = IfStatement ; } }
@ Override public void onCommunicationFailure ( BinaryLogClient client , Exception ex ) { logger . debug ( "A communication failure event arrived" , ex ) ; logReaderState ( ) ; code_block = TryStatement ;  BinlogReader . this . failed ( ex ) ; }
public void test() { try { client . disconnect ( ) ; } catch ( final Exception e ) { logger . debug ( "Exception while closing client" , e ) ; } }
public void test() { try { searchResult . close ( ) ; } catch ( IOException e ) { logger . warn ( "exception when close gtscanner" , e ) ; } }
@ Test public void testApiHTML ( ) throws Exception { MockHttpServletResponse response = getAsMockHttpServletResponse ( "ogc/images/api?f=text/html" , 200 ) ; assertEquals ( "text/html" , response . getContentType ( ) ) ; String html = response . getContentAsString ( ) ; LOGGER . info ( html ) ; assertThat ( html , containsString ( "<link rel=\"icon\" type=\"image/png\" href=\"http://localhost:8080/geoserver/swagger-ui/favicon-32x32.png\" sizes=\"32x32\" />" ) ) ; assertThat ( html , containsString ( "<link rel=\"icon\" type=\"image/png\" href=\"http://localhost:8080/geoserver/swagger-ui/favicon-16x16.png\" sizes=\"16x16\" />" ) ) ; assertThat ( html , containsString ( "<script src=\"http://localhost:8080/geoserver/swagger-ui/swagger-ui-bundle.js\">" ) ) ; assertThat ( html , containsString ( "<script src=\"http://localhost:8080/geoserver/swagger-ui/swagger-ui-standalone-preset.js\">" ) ) ; assertThat ( html , containsString ( "url: \"http://localhost:8080/geoserver/ogc/images/api?f=application%2Fvnd.oai.openapi%2Bjson%3Bversion%3D3.0\"" ) ) ; }
public void test() { try { Map < byte [ ] , byte [ ] > dataAll = jedisClient . hgetAll ( key . getBytes ( ) ) ; code_block = IfStatement ; int count = 0 ; code_block = ForStatement ; } catch ( JedisException e ) { LOGGER . error ( "removeByFilter occur a exception" , e ) ; } }
public void test() { -> { LOGGER . debug ( "Could not find a MetacardMapper for featureType {}." , featureType ) ; return null ; } }
public void test() { if ( schemaManager . add ( syntaxChecker ) ) { LOG . debug ( "Added {} into the enabled schema {}" , dn . getName ( ) , schemaName ) ; } else { String msg = I18n . err ( I18n . ERR_386 , entry . getDn ( ) . getName ( ) , Strings . listToString ( schemaManager . getErrors ( ) ) ) ; LOG . info ( msg ) ; throw new LdapUnwillingToPerformException ( ResultCodeEnum . UNWILLING_TO_PERFORM , msg ) ; } }
public void test() { if ( schemaManager . add ( syntaxChecker ) ) { LOG . debug ( "Added {} into the enabled schema {}" , dn . getName ( ) , schemaName ) ; } else { String msg = I18n . err ( I18n . ERR_386 , entry . getDn ( ) . getName ( ) , Strings . listToString ( schemaManager . getErrors ( ) ) ) ; LOG . info ( msg ) ; throw new LdapUnwillingToPerformException ( ResultCodeEnum . UNWILLING_TO_PERFORM , msg ) ; } }
public void test() { if ( schema . isEnabled ( ) && syntaxChecker . isEnabled ( ) ) { code_block = IfStatement ; } else { LOG . debug ( "The SyntaxChecker {} cannot be added in the disabled schema {}" , dn . getName ( ) , schemaName ) ; } }
public void test() { if ( log . isInfoEnabled ( ) ) { log . info ( "Publishing Cluster terminating event for [application] " + appId + " [cluster] " + getClusterId ( ) + " [instance] " + instanceId ) ; } }
public void test() { try { synchronized ( this ) code_block = "" ; } catch ( IOException ex ) { log . error ( Util . getMessage ( "FailedSubscribingTo" ) + destination + ": " , ex ) ; } }
public void test() { try { LiferayPortletURL portletURL = PortletURLFactoryUtil . create ( portletRequest , PortletConfigurationSharingPortletKeys . PORTLET_CONFIGURATION_SHARING , PortletRequest . RENDER_PHASE ) ; portletURL . setParameter ( "netvibesURL" , getWidgetURL ( portletRequest ) ) ; portletURL . setWindowState ( LiferayWindowState . POP_UP ) ; return portletURL . toString ( ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; return StringPool . BLANK ; } }
public void test() { if ( cachedSources . containsKey ( radio . getId ( ) ) ) { LOG . debug ( "Got cached sources for internet radio {}!" , radio . getStreamUrl ( ) ) ; sources = cachedSources . get ( radio . getId ( ) ) ; } else { LOG . debug ( "Retrieving sources for internet radio {}..." , radio . getStreamUrl ( ) ) ; code_block = TryStatement ;  cachedSources . put ( radio . getId ( ) , sources ) ; } }
public void test() { if ( cachedSources . containsKey ( radio . getId ( ) ) ) { LOG . debug ( "Got cached sources for internet radio {}!" , radio . getStreamUrl ( ) ) ; sources = cachedSources . get ( radio . getId ( ) ) ; } else { LOG . debug ( "Retrieving sources for internet radio {}..." , radio . getStreamUrl ( ) ) ; code_block = TryStatement ;  cachedSources . put ( radio . getId ( ) , sources ) ; } }
public void test() { if ( sources . isEmpty ( ) ) { LOG . warn ( "No entries found for internet radio {}." , radio . getStreamUrl ( ) ) ; } else { LOG . info ( "Retrieved playlist for internet radio {}, got {} sources." , radio . getStreamUrl ( ) , sources . size ( ) ) ; } }
public void test() { if ( sources . isEmpty ( ) ) { LOG . warn ( "No entries found for internet radio {}." , radio . getStreamUrl ( ) ) ; } else { LOG . info ( "Retrieved playlist for internet radio {}, got {} sources." , radio . getStreamUrl ( ) , sources . size ( ) ) ; } }
public void test() { try { sources = retrieveInternetRadioSources ( radio ) ; code_block = IfStatement ; } catch ( Exception e ) { LOG . error ( "Failed to retrieve sources for internet radio {}." , radio . getStreamUrl ( ) , e ) ; sources = new ArrayList < > ( ) ; } }
public void test() { if ( page . getContent ( ) != null ) { LOG . info ( "url:" + page . getUrl ( ) . toString ( ) ) ; LOG . info ( "limit:" + page . getContent ( ) . limit ( ) ) ; } else { assertNull ( page . getContent ( ) ) ; } }
public void test() { if ( page . getContent ( ) != null ) { LOG . info ( "url:" + page . getUrl ( ) . toString ( ) ) ; LOG . info ( "limit:" + page . getContent ( ) . limit ( ) ) ; } else { assertNull ( page . getContent ( ) ) ; } }
public void test() { try { messageHandler . handleMessage ( bytes ) ; } catch ( Throwable t ) { LOG . warn ( "Unexpected exception while handling framework message" , t ) ; } }
public void test() { -> { LOGGER . debug ( "    {}" , entityType ) ; MqttBatchResult < JSONObject > result = mqttHelper . executeRequests ( getUpdatePatchEntityAction ( entityType ) , mqttHelper . getTopic ( entityType , IDS . get ( entityType ) ) ) ; assertJsonEqualsWithLinkResolving ( result . getActionResult ( ) , result . getMessages ( ) . values ( ) . iterator ( ) . next ( ) , mqttHelper . getTopic ( entityType , IDS . get ( entityType ) ) ) ; } }
public void test() { try { port = Integer . parseInt ( portSubstring ) ; } catch ( NumberFormatException nfe ) { LOGGER . error ( "Can not parse port from substring {}" , portSubstring ) ; } }
@ SuppressWarnings ( "squid:S1126" ) private boolean checkSystemIfUniqueValidationNeeded ( final System system , final String validatedSystemName , final String validatedAddress , final Integer validatedPort ) { logger . debug ( "checkSystemIfUniqueValidationNeeded started..." ) ; final String actualSystemName = system . getSystemName ( ) ; final String actualAddress = system . getAddress ( ) ; final int actualPort = system . getPort ( ) ; code_block = IfStatement ; }
public void test() { try { node . nodeRemovedNotify ( ) ; code_block = IfStatement ; } catch ( Exception t ) { LOG . error ( "Error removing node" , t ) ; } }
public void test() { try { mdConsumer = new SimpleConsumer ( broker . split ( ":" ) [ 0 ] , Integer . parseInt ( broker . split ( ":" ) [ 1 ] ) , timeout , bufferSize , mdClientId ) ; List < String > topics = new ArrayList < String > ( 1 ) ; topics . add ( topic ) ; kafka . javaapi . TopicMetadataRequest req = new kafka . javaapi . TopicMetadataRequest ( topics ) ; TopicMetadataResponse resp = mdConsumer . send ( req ) ; List < TopicMetadata > metaData = resp . topicsMetadata ( ) ; code_block = ForStatement ; } catch ( NumberFormatException e ) { throw new IllegalArgumentException ( "Wrong format for broker url, should be \"broker1:port1\"" ) ; } catch ( Exception e ) { logger . warn ( "Broker {} is unavailable or in bad state!" , broker ) ; } }
List < VirtualAssetTextUnit > getLocalizedTextUnitsForTargetLocale ( Asset asset , RepositoryLocale repositoryLocale , InheritanceMode inheritanceMode ) { logger . debug ( "Get localized virtual asset for target locale" ) ; List < VirtualAssetTextUnit > virtualAssetTextUnits = new ArrayList < > ( ) ; Long lastSuccessfulAssetExtractionId = asset . getLastSuccessfulAssetExtraction ( ) . getId ( ) ; List < AssetTextUnitDTO > findByAssetExtractionAssetId = findByAssetExtractionIdAndDoNotTranslateFilter ( lastSuccessfulAssetExtractionId , null ) ; TranslatorWithInheritance translatorWithInheritance = new TranslatorWithInheritance ( asset , repositoryLocale , inheritanceMode ) ; code_block = ForStatement ; return virtualAssetTextUnits ; }
public void test() { if ( translation == null && InheritanceMode . REMOVE_UNTRANSLATED . equals ( inheritanceMode ) ) { logger . debug ( "Remove untranslated text unit" ) ; } else { logger . debug ( "Set translation for text unit with name: {}, translation: {}" , assetTextUnit . getName ( ) , translation ) ; VirtualAssetTextUnit virtualAssetTextUnit = convertAssetTextUnitDTOToVirtualAssetTextUnit ( assetTextUnit ) ; virtualAssetTextUnit . setContent ( translation ) ; virtualAssetTextUnits . add ( virtualAssetTextUnit ) ; } }
public void test() { if ( translation == null && InheritanceMode . REMOVE_UNTRANSLATED . equals ( inheritanceMode ) ) { logger . debug ( "Remove untranslated text unit" ) ; } else { logger . debug ( "Set translation for text unit with name: {}, translation: {}" , assetTextUnit . getName ( ) , translation ) ; VirtualAssetTextUnit virtualAssetTextUnit = convertAssetTextUnitDTOToVirtualAssetTextUnit ( assetTextUnit ) ; virtualAssetTextUnit . setContent ( translation ) ; virtualAssetTextUnits . add ( virtualAssetTextUnit ) ; } }
public void test() { try ( ResultSet rs = pstmt . executeQuery ( ) ; ) { code_block = WhileStatement ; } catch ( Exception e ) { s_logger . error ( "listPodIdsHavingVmsforAccount:Exception: " + e . getMessage ( ) ) ; throw new CloudRuntimeException ( "listPodIdsHavingVmsforAccount:Exception: " + e . getMessage ( ) , e ) ; } }
public void test() { try ( PreparedStatement pstmt = txn . prepareStatement ( sql ) ) { pstmt . setLong ( 1 , zoneId ) ; pstmt . setLong ( 2 , accountId ) ; code_block = TryStatement ;  txn . commit ( ) ; return result ; } catch ( Exception e ) { s_logger . error ( "listPodIdsHavingVmsforAccount:Exception : " + e . getMessage ( ) ) ; throw new CloudRuntimeException ( "listPodIdsHavingVmsforAccount:Exception: " + e . getMessage ( ) , e ) ; } finally { code_block = TryStatement ;  } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { s_logger . error ( "listPodIdsHavingVmsforAccount:Exception:" + e . getMessage ( ) ) ; } }
@ Test public void testDeepWalk ( ) throws Exception { Heartbeat . getInstance ( ) . disableHeartbeat ( ) ; AbstractCache < Blogger > vocabCache = new AbstractCache . Builder < Blogger > ( ) . build ( ) ; Graph < Blogger , Double > graph = buildGraph ( ) ; GraphWalker < Blogger > walker = new PopularityWalker . Builder < > ( graph ) . setNoEdgeHandling ( NoEdgeHandling . RESTART_ON_DISCONNECTED ) . setWalkLength ( 40 ) . setWalkDirection ( WalkDirection . FORWARD_UNIQUE ) . setRestartProbability ( 0.05 ) . setPopularitySpread ( 10 ) . setPopularityMode ( PopularityMode . MAXIMUM ) . setSpreadSpectrum ( SpreadSpectrum . PROPORTIONAL ) . build ( ) ; GraphTransformer < Blogger > graphTransformer = new GraphTransformer . Builder < > ( graph ) . setGraphWalker ( walker ) . shuffleOnReset ( true ) . setVocabCache ( vocabCache ) . build ( ) ; Blogger blogger = graph . getVertex ( 0 ) . getValue ( ) ; assertEquals ( 119 , blogger . getElementFrequency ( ) , 0.001 ) ; logger . info ( "Blogger: " + blogger ) ; AbstractSequenceIterator < Blogger > sequenceIterator = new AbstractSequenceIterator . Builder < > ( graphTransformer ) . build ( ) ; WeightLookupTable < Blogger > lookupTable = new InMemoryLookupTable . Builder < Blogger > ( ) . lr ( 0.025 ) . vectorLength ( 150 ) . useAdaGrad ( false ) . cache ( vocabCache ) . seed ( 42 ) . build ( ) ; lookupTable . resetWeights ( true ) ; SequenceVectors < Blogger > vectors = new SequenceVectors . Builder < Blogger > ( new VectorsConfiguration ( ) ) . lookupTable ( lookupTable ) . iterate ( sequenceIterator ) . vocabCache ( vocabCache ) . batchSize ( 1000 ) . iterations ( 1 ) . epochs ( 10 ) . resetModel ( false ) . trainElementsRepresentation ( true ) . trainSequencesRepresentation ( false ) . elementsLearningAlgorithm ( new SkipGram < Blogger > ( ) ) . learningRate ( 0.025 ) . layerSize ( 150 ) . sampling ( 0 ) . negativeSample ( 0 ) . windowSize ( 4 ) . workers ( 6 ) . seed ( 42 ) . build ( ) ; vectors . fit ( ) ; vectors . setModelUtils ( new FlatModelUtils ( ) ) ; double sim = vectors . similarity ( "12" , "72" ) ; Collection < String > list = vectors . wordsNearest ( "12" , 20 ) ; logger . info ( "12->72: " + sim ) ; printWords ( "12" , list , vectors ) ; assertTrue ( sim > 0.10 ) ; assertFalse ( Double . isNaN ( sim ) ) ; }
@ Test public void testDeepWalk ( ) throws Exception { Heartbeat . getInstance ( ) . disableHeartbeat ( ) ; AbstractCache < Blogger > vocabCache = new AbstractCache . Builder < Blogger > ( ) . build ( ) ; Graph < Blogger , Double > graph = buildGraph ( ) ; GraphWalker < Blogger > walker = new PopularityWalker . Builder < > ( graph ) . setNoEdgeHandling ( NoEdgeHandling . RESTART_ON_DISCONNECTED ) . setWalkLength ( 40 ) . setWalkDirection ( WalkDirection . FORWARD_UNIQUE ) . setRestartProbability ( 0.05 ) . setPopularitySpread ( 10 ) . setPopularityMode ( PopularityMode . MAXIMUM ) . setSpreadSpectrum ( SpreadSpectrum . PROPORTIONAL ) . build ( ) ; GraphTransformer < Blogger > graphTransformer = new GraphTransformer . Builder < > ( graph ) . setGraphWalker ( walker ) . shuffleOnReset ( true ) . setVocabCache ( vocabCache ) . build ( ) ; Blogger blogger = graph . getVertex ( 0 ) . getValue ( ) ; assertEquals ( 119 , blogger . getElementFrequency ( ) , 0.001 ) ; logger . info ( "Blogger: " + blogger ) ; AbstractSequenceIterator < Blogger > sequenceIterator = new AbstractSequenceIterator . Builder < > ( graphTransformer ) . build ( ) ; WeightLookupTable < Blogger > lookupTable = new InMemoryLookupTable . Builder < Blogger > ( ) . lr ( 0.025 ) . vectorLength ( 150 ) . useAdaGrad ( false ) . cache ( vocabCache ) . seed ( 42 ) . build ( ) ; lookupTable . resetWeights ( true ) ; SequenceVectors < Blogger > vectors = new SequenceVectors . Builder < Blogger > ( new VectorsConfiguration ( ) ) . lookupTable ( lookupTable ) . iterate ( sequenceIterator ) . vocabCache ( vocabCache ) . batchSize ( 1000 ) . iterations ( 1 ) . epochs ( 10 ) . resetModel ( false ) . trainElementsRepresentation ( true ) . trainSequencesRepresentation ( false ) . elementsLearningAlgorithm ( new SkipGram < Blogger > ( ) ) . learningRate ( 0.025 ) . layerSize ( 150 ) . sampling ( 0 ) . negativeSample ( 0 ) . windowSize ( 4 ) . workers ( 6 ) . seed ( 42 ) . build ( ) ; vectors . fit ( ) ; vectors . setModelUtils ( new FlatModelUtils ( ) ) ; double sim = vectors . similarity ( "12" , "72" ) ; Collection < String > list = vectors . wordsNearest ( "12" , 20 ) ; logger . info ( "12->72: " + sim ) ; printWords ( "12" , list , vectors ) ; assertTrue ( sim > 0.10 ) ; assertFalse ( Double . isNaN ( sim ) ) ; }
public void test() { try { setUpClass ( ) ; } catch ( RuntimeException | IOException | InterruptedException ex ) { LOGGER . error ( "Failed to initialise." , ex ) ; } }
public void test() { try { zigBeeGateway . processInputLine ( command , out ) ; } catch ( final Exception e ) { LOGGER . error ( "Error in ZigBeeConsole API execute command." , e ) ; out . println ( "Error: " + e . getMessage ( ) ) ; } }
public void test() { if ( ! this . rsaKeyFile . exists ( ) ) { log . info ( "Private key does not exist,filepath={}" , this . rsaKeyFile . getAbsolutePath ( ) ) ; return null ; } }
public void test() { try ( FileInputStream input = new FileInputStream ( rsaKeyFile ) ) { return readInputStream ( input ) ; } catch ( IOException e ) { log . error ( "errors while reading private key" , e ) ; String msg = String . format ( "Failed to read private key {%s}." , this . rsaKeyFile . getAbsolutePath ( ) ) ; throw new EncryptionException ( msg , e ) ; } }
public void test() { try ( OutputStream propertiesOutputStream = fileSystemClient . newOutputStream ( catalogPath . getPropertiesPath ( ) ) ; OutputStream metadataOutputStream = fileSystemClient . newOutputStream ( catalogPath . getMetadataPath ( ) ) ) { Map < String , CatalogFileInputStream . InputStreamWithType > inputStreams = configFiles . getInputStreams ( ) ; code_block = ForStatement ; Properties metadata = new Properties ( ) ; metadata . put ( "createdTime" , String . valueOf ( catalogInfo . getCreatedTime ( ) ) ) ; metadata . put ( "version" , String . valueOf ( catalogInfo . getVersion ( ) ) ) ; metadata . put ( "catalogFiles" , LIST_CODEC . toJson ( configFiles . getCatalogFileNames ( ) ) ) ; metadata . put ( "globalFiles" , LIST_CODEC . toJson ( configFiles . getGlobalFileNames ( ) ) ) ; metadata . store ( metadataOutputStream , "The metadata of dynamic catalog" ) ; Map < String , String > catalogProperties = rewriteFilePathProperties ( catalogName , catalogInfo . getProperties ( ) , configFiles . getCatalogFileNames ( ) , configFiles . getGlobalFileNames ( ) ) ; Properties properties = new Properties ( ) ; properties . putAll ( catalogProperties ) ; properties . put ( CATALOG_NAME_PROPERTY , catalogInfo . getConnectorName ( ) ) ; properties . store ( propertiesOutputStream , "The properties of dynamic catalog" ) ; } catch ( IOException ex ) { log . error ( ex , "Pull catalog files failed" ) ; deleteCatalog ( catalogName , false ) ; throw ex ; } }
public void close ( Socket s ) { LOG . info ( "Close connection {}" , s ) ; SafeClose . close ( s ) ; }
public void test() { try { currentStream . close ( ) ; currentStream = null ; } catch ( IOException ioe ) { logger . warn ( "IOException while closing file being read" , ioe ) ; } }
public void test() { try { in = Files . openFileStream ( thisFilename ) ; filename = thisFilename . replaceAll ( "\\.gz$" , "" ) ; } catch ( IOException ioe ) { logger . warn ( "Problem reading " + thisFilename + " in " + "SimpleFileCollection.getDocuent() : " , ioe ) ; } }
@ Override public void mapPort ( final int port , final String address , final PortMapProtocol protocol , final String mappingDescription ) { LOG . debug ( "Mapping port: [{}] for address: [{}] with description: [{}]." , port , address , mappingDescription ) ; final Protocol resolvedProtocol = resolveProtocol ( protocol ) ; final PortMapping portMapping = new PortMapping ( port , address , resolvedProtocol , mappingDescription ) ; mappingServicesRegistrar . registerPortMapping ( portMapping ) ; LOG . debug ( "Port: [{}] for address: [{}] with description: [{}] has been mapped." , port , address , mappingDescription ) ; }
@ Override public void mapPort ( final int port , final String address , final PortMapProtocol protocol , final String mappingDescription ) { LOG . debug ( "Mapping port: [{}] for address: [{}] with description: [{}]." , port , address , mappingDescription ) ; final Protocol resolvedProtocol = resolveProtocol ( protocol ) ; final PortMapping portMapping = new PortMapping ( port , address , resolvedProtocol , mappingDescription ) ; mappingServicesRegistrar . registerPortMapping ( portMapping ) ; LOG . debug ( "Port: [{}] for address: [{}] with description: [{}] has been mapped." , port , address , mappingDescription ) ; }
public void test() { try { SecorConfig config = SecorConfig . load ( ) ; String stagingDirectoryPath = config . getLocalPath ( ) + '/' + IdUtil . getLocalMessageDir ( ) ; ShutdownHookRegistry . registerHook ( 10 , new StagingDirectoryCleaner ( stagingDirectoryPath ) ) ; MetricCollector metricCollector = ReflectionUtil . createMetricCollector ( config . getMetricsCollectorClass ( ) ) ; metricCollector . initialize ( config ) ; OstrichAdminService ostrichService = new OstrichAdminService ( config ) ; ostrichService . start ( ) ; FileUtil . configure ( config ) ; LogFileDeleter logFileDeleter = new LogFileDeleter ( config ) ; logFileDeleter . deleteOldLogs ( ) ; RateLimitUtil . configure ( config ) ; LOG . info ( "starting {} consumer threads" , config . getConsumerThreads ( ) ) ; LinkedList < Consumer > consumers = new LinkedList < Consumer > ( ) ; code_block = ForStatement ; code_block = ForStatement ; } catch ( Throwable t ) { LOG . error ( "Consumer failed" , t ) ; System . exit ( 1 ) ; } }
public void test() { try { SecorConfig config = SecorConfig . load ( ) ; String stagingDirectoryPath = config . getLocalPath ( ) + '/' + IdUtil . getLocalMessageDir ( ) ; ShutdownHookRegistry . registerHook ( 10 , new StagingDirectoryCleaner ( stagingDirectoryPath ) ) ; MetricCollector metricCollector = ReflectionUtil . createMetricCollector ( config . getMetricsCollectorClass ( ) ) ; metricCollector . initialize ( config ) ; OstrichAdminService ostrichService = new OstrichAdminService ( config ) ; ostrichService . start ( ) ; FileUtil . configure ( config ) ; LogFileDeleter logFileDeleter = new LogFileDeleter ( config ) ; logFileDeleter . deleteOldLogs ( ) ; RateLimitUtil . configure ( config ) ; LOG . info ( "starting {} consumer threads" , config . getConsumerThreads ( ) ) ; LinkedList < Consumer > consumers = new LinkedList < Consumer > ( ) ; code_block = ForStatement ; code_block = ForStatement ; } catch ( Throwable t ) { LOG . error ( "Consumer failed" , t ) ; System . exit ( 1 ) ; } }
public void test() { try { interceptInitEntity ( ) ; } catch ( RuntimeException ex ) { LOG . error ( "entity {}" , this , ex ) ; } }
public void test() { try { a . initAttribute ( ) ; } catch ( RuntimeException ex ) { LOG . error ( "attribute {}/{}" , this , a , ex ) ; } }
public void test() { try { e . initEntity ( ) ; } catch ( RuntimeException ex ) { LOG . error ( "entity {}/{}" , this , e , ex ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "unbind branch type {}" , unbindBranchType ) ; } }
@ Override public void createShare ( final String irodsAbsolutePath , final String shareName ) throws ShareAlreadyExistsException , FileNotFoundException , JargonException { log . info ( "createShare()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "irodsAbsolutePath:{}" , irodsAbsolutePath ) ; log . info ( "deciding whether a file or collection..." ) ; ObjStat objStat = getObjStatForAbsolutePath ( irodsAbsolutePath ) ; log . info ( "seeing if share already present.." ) ; IRODSSharedFileOrCollection currentSharedFile = findSharedGivenObjStat ( irodsAbsolutePath , objStat ) ; code_block = IfStatement ; MetadataDomain metadataDomain ; code_block = IfStatement ; IRODSSharedFileOrCollection irodsSharedFileOrCollection = new IRODSSharedFileOrCollection ( metadataDomain , irodsAbsolutePath , shareName , irodsAccount . getUserName ( ) , irodsAccount . getZone ( ) , new ArrayList < ShareUser > ( ) ) ; log . info ( "adding share tag" ) ; AvuData avuData = buildAVUBasedOnShare ( irodsSharedFileOrCollection ) ; log . info ( "setting inheritance and ACL" ) ; code_block = IfStatement ; log . info ( "share created" ) ; }
@ Override public void createShare ( final String irodsAbsolutePath , final String shareName ) throws ShareAlreadyExistsException , FileNotFoundException , JargonException { log . info ( "createShare()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "irodsAbsolutePath:{}" , irodsAbsolutePath ) ; log . info ( "deciding whether a file or collection..." ) ; ObjStat objStat = getObjStatForAbsolutePath ( irodsAbsolutePath ) ; log . info ( "seeing if share already present.." ) ; IRODSSharedFileOrCollection currentSharedFile = findSharedGivenObjStat ( irodsAbsolutePath , objStat ) ; code_block = IfStatement ; MetadataDomain metadataDomain ; code_block = IfStatement ; IRODSSharedFileOrCollection irodsSharedFileOrCollection = new IRODSSharedFileOrCollection ( metadataDomain , irodsAbsolutePath , shareName , irodsAccount . getUserName ( ) , irodsAccount . getZone ( ) , new ArrayList < ShareUser > ( ) ) ; log . info ( "adding share tag" ) ; AvuData avuData = buildAVUBasedOnShare ( irodsSharedFileOrCollection ) ; log . info ( "setting inheritance and ACL" ) ; code_block = IfStatement ; log . info ( "share created" ) ; }
@ Override public void createShare ( final String irodsAbsolutePath , final String shareName ) throws ShareAlreadyExistsException , FileNotFoundException , JargonException { log . info ( "createShare()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "irodsAbsolutePath:{}" , irodsAbsolutePath ) ; log . info ( "deciding whether a file or collection..." ) ; ObjStat objStat = getObjStatForAbsolutePath ( irodsAbsolutePath ) ; log . info ( "seeing if share already present.." ) ; IRODSSharedFileOrCollection currentSharedFile = findSharedGivenObjStat ( irodsAbsolutePath , objStat ) ; code_block = IfStatement ; MetadataDomain metadataDomain ; code_block = IfStatement ; IRODSSharedFileOrCollection irodsSharedFileOrCollection = new IRODSSharedFileOrCollection ( metadataDomain , irodsAbsolutePath , shareName , irodsAccount . getUserName ( ) , irodsAccount . getZone ( ) , new ArrayList < ShareUser > ( ) ) ; log . info ( "adding share tag" ) ; AvuData avuData = buildAVUBasedOnShare ( irodsSharedFileOrCollection ) ; log . info ( "setting inheritance and ACL" ) ; code_block = IfStatement ; log . info ( "share created" ) ; }
@ Override public void createShare ( final String irodsAbsolutePath , final String shareName ) throws ShareAlreadyExistsException , FileNotFoundException , JargonException { log . info ( "createShare()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "irodsAbsolutePath:{}" , irodsAbsolutePath ) ; log . info ( "deciding whether a file or collection..." ) ; ObjStat objStat = getObjStatForAbsolutePath ( irodsAbsolutePath ) ; log . info ( "seeing if share already present.." ) ; IRODSSharedFileOrCollection currentSharedFile = findSharedGivenObjStat ( irodsAbsolutePath , objStat ) ; code_block = IfStatement ; MetadataDomain metadataDomain ; code_block = IfStatement ; IRODSSharedFileOrCollection irodsSharedFileOrCollection = new IRODSSharedFileOrCollection ( metadataDomain , irodsAbsolutePath , shareName , irodsAccount . getUserName ( ) , irodsAccount . getZone ( ) , new ArrayList < ShareUser > ( ) ) ; log . info ( "adding share tag" ) ; AvuData avuData = buildAVUBasedOnShare ( irodsSharedFileOrCollection ) ; log . info ( "setting inheritance and ACL" ) ; code_block = IfStatement ; log . info ( "share created" ) ; }
@ Override public void createShare ( final String irodsAbsolutePath , final String shareName ) throws ShareAlreadyExistsException , FileNotFoundException , JargonException { log . info ( "createShare()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "irodsAbsolutePath:{}" , irodsAbsolutePath ) ; log . info ( "deciding whether a file or collection..." ) ; ObjStat objStat = getObjStatForAbsolutePath ( irodsAbsolutePath ) ; log . info ( "seeing if share already present.." ) ; IRODSSharedFileOrCollection currentSharedFile = findSharedGivenObjStat ( irodsAbsolutePath , objStat ) ; code_block = IfStatement ; MetadataDomain metadataDomain ; code_block = IfStatement ; IRODSSharedFileOrCollection irodsSharedFileOrCollection = new IRODSSharedFileOrCollection ( metadataDomain , irodsAbsolutePath , shareName , irodsAccount . getUserName ( ) , irodsAccount . getZone ( ) , new ArrayList < ShareUser > ( ) ) ; log . info ( "adding share tag" ) ; AvuData avuData = buildAVUBasedOnShare ( irodsSharedFileOrCollection ) ; log . info ( "setting inheritance and ACL" ) ; code_block = IfStatement ; log . info ( "share created" ) ; }
@ Override public void createShare ( final String irodsAbsolutePath , final String shareName ) throws ShareAlreadyExistsException , FileNotFoundException , JargonException { log . info ( "createShare()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "irodsAbsolutePath:{}" , irodsAbsolutePath ) ; log . info ( "deciding whether a file or collection..." ) ; ObjStat objStat = getObjStatForAbsolutePath ( irodsAbsolutePath ) ; log . info ( "seeing if share already present.." ) ; IRODSSharedFileOrCollection currentSharedFile = findSharedGivenObjStat ( irodsAbsolutePath , objStat ) ; code_block = IfStatement ; MetadataDomain metadataDomain ; code_block = IfStatement ; IRODSSharedFileOrCollection irodsSharedFileOrCollection = new IRODSSharedFileOrCollection ( metadataDomain , irodsAbsolutePath , shareName , irodsAccount . getUserName ( ) , irodsAccount . getZone ( ) , new ArrayList < ShareUser > ( ) ) ; log . info ( "adding share tag" ) ; AvuData avuData = buildAVUBasedOnShare ( irodsSharedFileOrCollection ) ; log . info ( "setting inheritance and ACL" ) ; code_block = IfStatement ; log . info ( "share created" ) ; }
@ Override public void createShare ( final String irodsAbsolutePath , final String shareName ) throws ShareAlreadyExistsException , FileNotFoundException , JargonException { log . info ( "createShare()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "irodsAbsolutePath:{}" , irodsAbsolutePath ) ; log . info ( "deciding whether a file or collection..." ) ; ObjStat objStat = getObjStatForAbsolutePath ( irodsAbsolutePath ) ; log . info ( "seeing if share already present.." ) ; IRODSSharedFileOrCollection currentSharedFile = findSharedGivenObjStat ( irodsAbsolutePath , objStat ) ; code_block = IfStatement ; MetadataDomain metadataDomain ; code_block = IfStatement ; IRODSSharedFileOrCollection irodsSharedFileOrCollection = new IRODSSharedFileOrCollection ( metadataDomain , irodsAbsolutePath , shareName , irodsAccount . getUserName ( ) , irodsAccount . getZone ( ) , new ArrayList < ShareUser > ( ) ) ; log . info ( "adding share tag" ) ; AvuData avuData = buildAVUBasedOnShare ( irodsSharedFileOrCollection ) ; log . info ( "setting inheritance and ACL" ) ; code_block = IfStatement ; log . info ( "share created" ) ; }
private synchronized void stopTask ( ) { LOG . info ( "NoopTask.stopTask() invoked." ) ; this . isRunning = false ; this . notify ( ) ; }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { MCRCategory linkedCategory = link . getCategory ( ) ; StringBuilder debugMessage = new StringBuilder ( "Adding Link from " ) . append ( linkedCategory . getId ( ) ) ; code_block = IfStatement ; debugMessage . append ( "to " ) . append ( objectReference ) ; LOGGER . debug ( debugMessage . toString ( ) ) ; } }
@ Override public QueryResult execute ( String query , String language ) throws RepositoryException { logger . trace ( "Executing query: {0}" , query ) ; final Query jcrQuery = getLocalSession ( ) . getSession ( ) . getWorkspace ( ) . getQueryManager ( ) . createQuery ( query , language ) ; return jcrQuery . execute ( ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Spring Security setup complete." ) ; } }
public void test() { { logger . debug ( "-----installProject--- , project name: {}, branch name: {}" , projectName , branchName ) ; PortablePreconditions . checkNotNull ( "spaceName" , spaceName ) ; PortablePreconditions . checkNotNull ( "projectName" , projectName ) ; final String id = newId ( ) ; final InstallProjectRequest jobRequest = new InstallProjectRequest ( ) ; jobRequest . setStatus ( JobStatus . ACCEPTED ) ; jobRequest . setJobId ( id ) ; jobRequest . setSpaceName ( spaceName ) ; jobRequest . setProjectName ( projectName ) ; jobRequest . setBranchName ( branchName ) ; addAcceptedJobResult ( id ) ; jobRequestObserver . installProjectRequest ( jobRequest ) ; return createAcceptedStatusResponse ( jobRequest ) ; } }
private void report ( ) { final long deviceConnectionDuration = noOfDeviceConnections . get ( ) * Duration . between ( startInstant . getAndSet ( Instant . now ( ) ) , Instant . now ( ) ) . toMillis ( ) ; recorder . accept ( deviceConnectionDuration ) ; log . trace ( "Reported device connection duration [tenant : {}, noOfDeviceConnections: {}, connectionDurationInMs: {}]." , tenantId , noOfDeviceConnections . get ( ) , deviceConnectionDuration ) ; }
public void test() { if ( provenanceEventJmsWriter == null ) { log . error ( "!!!!!!!ProvenanceEventJmsWriter is NULL !!!!!!" ) ; } }
public void test() { for ( Long executionId : execIds ) { ExecutionActionResult result = cancelExecutionService . requestCancelExecution ( executionId ) ; logger . warn ( "Requested cancel of execution id: " + executionId + ", result: " + result ) ; } }
public void test() { if ( _log . isWarnEnabled ( ) ) { _log . warn ( "No workflow definitions found to " + workflowDefinitionName ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Populated SecurityContextHolder with pre-auth token: '" + context . getAuthentication ( ) + "'" ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "SecurityContextHolder not populated with pre-auth token" ) ; } }
public void test() { try { this . camelContext . stop ( ) ; } catch ( Exception e ) { LOGGER . error ( "Failed at stopping KIE Server extension {}" , EXTENSION_NAME ) ; } }
public void start ( ) { code_block = IfStatement ; log . info ( "Starting up the default async job executor [{}]." , getClass ( ) . getName ( ) ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; startTimerAcquisitionThread ( ) ; startResetExpiredJobsThread ( ) ; isActive = true ; executeTemporaryJobs ( ) ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( sb . toString ( ) ) ; } }
public void test() { try { return IOUtils . toString ( InputStream . class . cast ( getService ( RoleService . class ) . getAnyLayout ( roleKey ) . getEntity ( ) ) , StandardCharsets . UTF_8 ) ; } catch ( Exception e ) { LOG . error ( "Error retrieving console layout info for role {}" , roleKey , e ) ; return StringUtils . EMPTY ; } }
public void test() { if ( variantSourceObj instanceof VariantFileMetadata ) { sortedSampleNames = ( ( VariantFileMetadata ) variantSourceObj ) . getSampleIds ( ) ; } else-if ( variantSourceObj instanceof Map ) { sortedSampleNames = new ObjectMap ( ( Map ) variantSourceObj ) . getAsStringList ( "sampleIds" ) ; } else { logger . warn ( "Unexpected object type of variantSource ({}) in file attributes. Expected {} or {}" , variantSourceObj . getClass ( ) , VariantFileMetadata . class , Map . class ) ; } }
public void test() { if ( alignmentHeaderObj instanceof Map ) { sortedSampleNames = getSampleFromAlignmentHeader ( ( Map ) alignmentHeaderObj ) ; } else { logger . warn ( "Unexpected object type of AlignmentHeader ({}) in file attributes. Expected {}" , alignmentHeaderObj . getClass ( ) , Map . class ) ; } }
public void testStateTransfer ( ) throws Exception { CyclicBarrier barrier = new CyclicBarrier ( 2 ) ; blockDataContainerIteration ( cache ( 0 ) , barrier ) ; Set < Object > keys = new HashSet < > ( ) ; code_block = ForStatement ; log . trace ( "State transfer happens here" ) ; addClusterEnabledCacheManager ( TestDataSCI . INSTANCE , dccc ) ; waitForClusterToForm ( ) ; barrier . await ( 10 , TimeUnit . SECONDS ) ; log . trace ( "Checking the values from caches..." ) ; code_block = ForStatement ; barrier . await ( 10 , TimeUnit . SECONDS ) ; cache ( 0 ) . getAdvancedCache ( ) . getAsyncInterceptorChain ( ) . removeInterceptor ( BlockingInterceptor . class ) ; code_block = ForStatement ; }
public void testStateTransfer ( ) throws Exception { CyclicBarrier barrier = new CyclicBarrier ( 2 ) ; blockDataContainerIteration ( cache ( 0 ) , barrier ) ; Set < Object > keys = new HashSet < > ( ) ; code_block = ForStatement ; log . trace ( "State transfer happens here" ) ; addClusterEnabledCacheManager ( TestDataSCI . INSTANCE , dccc ) ; waitForClusterToForm ( ) ; barrier . await ( 10 , TimeUnit . SECONDS ) ; log . trace ( "Checking the values from caches..." ) ; code_block = ForStatement ; barrier . await ( 10 , TimeUnit . SECONDS ) ; cache ( 0 ) . getAdvancedCache ( ) . getAsyncInterceptorChain ( ) . removeInterceptor ( BlockingInterceptor . class ) ; code_block = ForStatement ; }
public void test() { try { studio . getModel ( ) . clearModuleInfo ( ) ; unregisterArtifactInProjectDescriptor ( childArtefact ) ; childArtefact . delete ( ) ; repositoryTreeState . refreshSelectedNode ( ) ; resetStudioModel ( ) ; WebStudioUtils . addInfoMessage ( "Element was deleted successfully." ) ; } catch ( Exception e ) { log . error ( "Error deleting element." , e ) ; WebStudioUtils . addErrorMessage ( "Error deleting." , e . getMessage ( ) ) ; } }
public void test() { try { signature = generateSignature ( selectedSignatureHashAlgo ) ; } catch ( CryptoException E ) { LOGGER . warn ( "Could not generate Signature! Using empty one instead!" , E ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Retriving classifications for entity={}" , guid ) ; } }
public void test() { try { TaskExitState exitState = new TaskExitState ( ) ; exitState . setInput ( inputMeter . count ( ) ) ; exitState . setTotalEmitted ( outputMeter . count ( ) ) ; exitState . setMeanRate ( outputMeter . meanRate ( ) ) ; Files . write ( CodecJSON . INSTANCE . encode ( exitState ) , new File ( "job.exit" ) ) ; } catch ( Exception ex ) { log . error ( "" , ex ) ; } }
public void test() { try { InputStreamReader inputStreamReader = new InputStreamReader ( response . getEntity ( ) . getContent ( ) ) ; BufferedReader reader = new BufferedReader ( inputStreamReader ) ; ObjectMapper objectMapper = new ObjectMapper ( ) ; Map < String , Map < String , Object > > responseMap = null ; code_block = TryStatement ;  code_block = IfStatement ; } catch ( JsonParseException e ) { LOGGER . error ( "SBIMOPS reconciliation, error while parsing the response content" , e ) ; } catch ( IOException e ) { LOGGER . error ( "SBIMOPS reconciliation, error while reading the response content" , e ) ; } }
public void test() { try { InputStreamReader inputStreamReader = new InputStreamReader ( response . getEntity ( ) . getContent ( ) ) ; BufferedReader reader = new BufferedReader ( inputStreamReader ) ; ObjectMapper objectMapper = new ObjectMapper ( ) ; Map < String , Map < String , Object > > responseMap = null ; code_block = TryStatement ;  code_block = IfStatement ; } catch ( JsonParseException e ) { LOGGER . error ( "SBIMOPS reconciliation, error while parsing the response content" , e ) ; } catch ( IOException e ) { LOGGER . error ( "SBIMOPS reconciliation, error while reading the response content" , e ) ; } }
public void test() { if ( verboseLogs ) { log . info ( "Waiting until connectors of address space: '{}' messages {} will be in ready state" , name , getConnectorStatuses ( clientAddressSpace ) ) ; } }
public void test() { if ( cachedRowSet . size ( ) == 0 ) { return Optional . empty ( ) ; } else-if ( cachedRowSet . size ( ) > 1 ) { LOGGER . info ( MessageFormat . format ( "Found multiple implementations for ScriptVersionTrace {0}. Returning first implementation" , scriptVersionTraceKey . toString ( ) ) ) ; } }
public void test() { try { return Optional . of ( Identities . builder ( ) . identities ( identities ) . build ( ) ) ; } catch ( IllegalArgumentException | NoSuchElementException e ) { log . warn ( "Got exception: " , e ) ; return Optional . empty ( ) ; } }
public void test() { try { WorkflowRun wr = ll . findWorkflowRun ( "/" + workflowRunAccession ) ; Workflow w = ll . findWorkflowByWorkflowRun ( workflowRunAccession ) ; wr . setWorkflow ( w ) ; return ( wr ) ; } catch ( IOException | JAXBException ex ) { Log . error ( "" , ex ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( portalException , portalException ) ; } }
@ Test public void test1 ( ) throws Exception { log . debug ( "=========== test1() =================" ) ; myTestExecution ( DatesPage1 . class , "DatesPage1_ExpectedResult.html" ) ; }
protected long sweep ( GarbageCollectorFileState fs , long markStart , boolean forceBlobRetrieve ) throws Exception { long earliestRefAvailTime ; earliestRefAvailTime = GarbageCollectionType . get ( blobStore ) . mergeAllMarkedReferences ( blobStore , fs , clock , maxLastModifiedInterval , sweepIfRefsPastRetention ) ; LOG . debug ( "Earliest reference available for timestamp [{}]" , earliestRefAvailTime ) ; earliestRefAvailTime = ( earliestRefAvailTime < markStart ? earliestRefAvailTime : markStart ) ; ( new BlobIdRetriever ( fs , forceBlobRetrieve ) ) . call ( ) ; difference ( fs ) ; long count = 0 ; long deleted = 0 ; long maxModifiedTime = getMaxModifiedTime ( earliestRefAvailTime ) ; LOG . debug ( "Starting sweep phase of the garbage collector" ) ; LOG . debug ( "Sweeping blobs with modified time > than the configured max deleted time ({}). " , timestampToString ( maxModifiedTime ) ) ; BufferedWriter removesWriter = null ; LineIterator iterator = null ; long deletedSize = 0 ; int numDeletedSizeAvailable = 0 ; code_block = TryStatement ;  code_block = IfStatement ; BlobCollectionType . get ( blobStore ) . handleRemoves ( blobStore , fs . getGarbage ( ) , fs . getMarkedRefs ( ) ) ; code_block = IfStatement ; code_block = IfStatement ; statsCollector . updateNumCandidates ( count ) ; statsCollector . updateNumDeleted ( deleted ) ; statsCollector . updateTotalSizeDeleted ( deletedSize ) ; GarbageCollectionType . get ( blobStore ) . removeAllMarkedReferences ( blobStore ) ; LOG . debug ( "Ending sweep phase of the garbage collector" ) ; return deleted ; }
protected long sweep ( GarbageCollectorFileState fs , long markStart , boolean forceBlobRetrieve ) throws Exception { long earliestRefAvailTime ; earliestRefAvailTime = GarbageCollectionType . get ( blobStore ) . mergeAllMarkedReferences ( blobStore , fs , clock , maxLastModifiedInterval , sweepIfRefsPastRetention ) ; LOG . debug ( "Earliest reference available for timestamp [{}]" , earliestRefAvailTime ) ; earliestRefAvailTime = ( earliestRefAvailTime < markStart ? earliestRefAvailTime : markStart ) ; ( new BlobIdRetriever ( fs , forceBlobRetrieve ) ) . call ( ) ; difference ( fs ) ; long count = 0 ; long deleted = 0 ; long maxModifiedTime = getMaxModifiedTime ( earliestRefAvailTime ) ; LOG . debug ( "Starting sweep phase of the garbage collector" ) ; LOG . debug ( "Sweeping blobs with modified time > than the configured max deleted time ({}). " , timestampToString ( maxModifiedTime ) ) ; BufferedWriter removesWriter = null ; LineIterator iterator = null ; long deletedSize = 0 ; int numDeletedSizeAvailable = 0 ; code_block = TryStatement ;  code_block = IfStatement ; BlobCollectionType . get ( blobStore ) . handleRemoves ( blobStore , fs . getGarbage ( ) , fs . getMarkedRefs ( ) ) ; code_block = IfStatement ; code_block = IfStatement ; statsCollector . updateNumCandidates ( count ) ; statsCollector . updateNumDeleted ( deleted ) ; statsCollector . updateTotalSizeDeleted ( deletedSize ) ; GarbageCollectionType . get ( blobStore ) . removeAllMarkedReferences ( blobStore ) ; LOG . debug ( "Ending sweep phase of the garbage collector" ) ; return deleted ; }
protected long sweep ( GarbageCollectorFileState fs , long markStart , boolean forceBlobRetrieve ) throws Exception { long earliestRefAvailTime ; earliestRefAvailTime = GarbageCollectionType . get ( blobStore ) . mergeAllMarkedReferences ( blobStore , fs , clock , maxLastModifiedInterval , sweepIfRefsPastRetention ) ; LOG . debug ( "Earliest reference available for timestamp [{}]" , earliestRefAvailTime ) ; earliestRefAvailTime = ( earliestRefAvailTime < markStart ? earliestRefAvailTime : markStart ) ; ( new BlobIdRetriever ( fs , forceBlobRetrieve ) ) . call ( ) ; difference ( fs ) ; long count = 0 ; long deleted = 0 ; long maxModifiedTime = getMaxModifiedTime ( earliestRefAvailTime ) ; LOG . debug ( "Starting sweep phase of the garbage collector" ) ; LOG . debug ( "Sweeping blobs with modified time > than the configured max deleted time ({}). " , timestampToString ( maxModifiedTime ) ) ; BufferedWriter removesWriter = null ; LineIterator iterator = null ; long deletedSize = 0 ; int numDeletedSizeAvailable = 0 ; code_block = TryStatement ;  code_block = IfStatement ; BlobCollectionType . get ( blobStore ) . handleRemoves ( blobStore , fs . getGarbage ( ) , fs . getMarkedRefs ( ) ) ; code_block = IfStatement ; code_block = IfStatement ; statsCollector . updateNumCandidates ( count ) ; statsCollector . updateNumDeleted ( deleted ) ; statsCollector . updateTotalSizeDeleted ( deletedSize ) ; GarbageCollectionType . get ( blobStore ) . removeAllMarkedReferences ( blobStore ) ; LOG . debug ( "Ending sweep phase of the garbage collector" ) ; return deleted ; }
public void test() { if ( checkConsistencyAfterGc ) { BlobCollectionType . get ( blobStore ) . checkConsistencyAfterGC ( blobStore , fs , consistencyStatsCollector , new File ( root ) ) ; } }
public void test() { if ( checkConsistencyAfterGc ) { BlobCollectionType . get ( blobStore ) . checkConsistencyAfterGC ( blobStore , fs , consistencyStatsCollector , new File ( root ) ) ; } }
protected long sweep ( GarbageCollectorFileState fs , long markStart , boolean forceBlobRetrieve ) throws Exception { long earliestRefAvailTime ; earliestRefAvailTime = GarbageCollectionType . get ( blobStore ) . mergeAllMarkedReferences ( blobStore , fs , clock , maxLastModifiedInterval , sweepIfRefsPastRetention ) ; LOG . debug ( "Earliest reference available for timestamp [{}]" , earliestRefAvailTime ) ; earliestRefAvailTime = ( earliestRefAvailTime < markStart ? earliestRefAvailTime : markStart ) ; ( new BlobIdRetriever ( fs , forceBlobRetrieve ) ) . call ( ) ; difference ( fs ) ; long count = 0 ; long deleted = 0 ; long maxModifiedTime = getMaxModifiedTime ( earliestRefAvailTime ) ; LOG . debug ( "Starting sweep phase of the garbage collector" ) ; LOG . debug ( "Sweeping blobs with modified time > than the configured max deleted time ({}). " , timestampToString ( maxModifiedTime ) ) ; BufferedWriter removesWriter = null ; LineIterator iterator = null ; long deletedSize = 0 ; int numDeletedSizeAvailable = 0 ; code_block = TryStatement ;  code_block = IfStatement ; BlobCollectionType . get ( blobStore ) . handleRemoves ( blobStore , fs . getGarbage ( ) , fs . getMarkedRefs ( ) ) ; code_block = IfStatement ; code_block = IfStatement ; statsCollector . updateNumCandidates ( count ) ; statsCollector . updateNumDeleted ( deleted ) ; statsCollector . updateTotalSizeDeleted ( deletedSize ) ; GarbageCollectionType . get ( blobStore ) . removeAllMarkedReferences ( blobStore ) ; LOG . debug ( "Ending sweep phase of the garbage collector" ) ; return deleted ; }
public void test() { try { String langCode = this . getCurrentLang ( ) . getCode ( ) ; String nodeRootCode = this . getIdeaManager ( ) . getCategoryRoot ( ) ; categories = this . getCategoryLeaf ( nodeRootCode , langCode , completeTitle ) ; } catch ( Throwable t ) { _logger . error ( "Errore loading categories" , t ) ; throw new RuntimeException ( "Error loading categories" ) ; } }
@ Test public void testTypes ( ) throws Exception { logger . info ( "" + Schema . getDefaultSchema ( ) . getEntityClass ( "sample_entity" ) ) ; logger . info ( "" + Schema . getDefaultSchema ( ) . getEntityType ( SampleEntity . class ) ) ; SampleEntity entity = new SampleEntity ( ) ; logger . info ( entity . getType ( ) ) ; }
@ Test public void testTypes ( ) throws Exception { logger . info ( "" + Schema . getDefaultSchema ( ) . getEntityClass ( "sample_entity" ) ) ; logger . info ( "" + Schema . getDefaultSchema ( ) . getEntityType ( SampleEntity . class ) ) ; SampleEntity entity = new SampleEntity ( ) ; logger . info ( entity . getType ( ) ) ; }
@ Test public void testTypes ( ) throws Exception { logger . info ( "" + Schema . getDefaultSchema ( ) . getEntityClass ( "sample_entity" ) ) ; logger . info ( "" + Schema . getDefaultSchema ( ) . getEntityType ( SampleEntity . class ) ) ; SampleEntity entity = new SampleEntity ( ) ; logger . info ( entity . getType ( ) ) ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( message , exception ) ; } else { _log . error ( message ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( message , exception ) ; } else { _log . error ( message ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Failed verifying signature of token {}" , token , e ) ; } }
public void test() { if ( registration == null ) { localLogger . error ( "SequencerRunDAOHibernate insert SequencerRun registration is null" ) ; } else-if ( registration . isLIMSAdmin ( ) || sequencerRun . givesPermission ( registration ) ) { localLogger . info ( "insert sequencer run object" ) ; insert ( sequencerRun ) ; return ( sequencerRun . getSwAccession ( ) ) ; } else { localLogger . error ( "sequencerRunDAOHibernate insert not authorized" ) ; } }
public void test() { if ( registration == null ) { localLogger . error ( "SequencerRunDAOHibernate insert SequencerRun registration is null" ) ; } else-if ( registration . isLIMSAdmin ( ) || sequencerRun . givesPermission ( registration ) ) { localLogger . info ( "insert sequencer run object" ) ; insert ( sequencerRun ) ; return ( sequencerRun . getSwAccession ( ) ) ; } else { localLogger . error ( "sequencerRunDAOHibernate insert not authorized" ) ; } }
public void test() { if ( registration == null ) { localLogger . error ( "SequencerRunDAOHibernate insert SequencerRun registration is null" ) ; } else-if ( registration . isLIMSAdmin ( ) || sequencerRun . givesPermission ( registration ) ) { localLogger . info ( "insert sequencer run object" ) ; insert ( sequencerRun ) ; return ( sequencerRun . getSwAccession ( ) ) ; } else { localLogger . error ( "sequencerRunDAOHibernate insert not authorized" ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( _NO_SUCH_ENTITY_WITH_PRIMARY_KEY + primaryKey ) ; } }
public void test() { if ( type == null ) { LOGGER . warn ( "The type value of a reference link is false, the link was not added to the link table" ) ; return ; } }
public void test() { try { linkTableInstance . create ( from , to , type , attr ) ; } catch ( Exception e ) { LOGGER . warn ( "An error occured while adding a dataset from the reference link table, adding not succesful." , e ) ; } }
public void test() { try { fs = hdfsPath . getFileSystem ( conf ) ; } catch ( IOException e ) { logger . error ( "Fail to get HDFS! " , e ) ; } }
@ Test public void testDateFolderDistributor ( ) throws Exception { log . info ( "-----  testDateFolderDistributor  -----" ) ; File f = setUpFlagDir ( ) ; createTestFiles ( 5 , 5 ) ; createBogusTestFiles ( 5 , 5 ) ; createCopyingTestFiles ( 5 , 5 ) ; fmc . setDistributorType ( "folderdate" ) ; FlagMaker instance = new TestWrappedFlagMaker ( fmc ) ; instance . processFlags ( ) ; File [ ] flags = f . listFiles ( pathname -> pathname . toString ( ) . endsWith ( "flag" ) ) ; assertEquals ( "Incorrect number of flags: " + Arrays . toString ( flags ) , 5 , flags . length ) ; HashSet < Long > buckets = new HashSet < > ( ) ; DateUtils du = new DateUtils ( ) ; code_block = ForStatement ; }
public void test() { try { response = new ResponseData ( vulnerabilityService . getVulnerabilityByAppAndEnv ( assetGroup , "tags.Application.keyword" , "" ) ) ; } catch ( Exception e ) { LOGGER . error ( "Exception in vulnerabilitybyapplications " , e ) ; return ResponseUtils . buildFailureResponse ( e ) ; } }
public void test() { if ( null == user ) { _logger . error ( "Null user" ) ; return false ; } }
@ Override public void createPages ( ) { Stopwatch stopwatch = Stopwatch . createStarted ( ) ; createModel ( ) ; Display display = getSite ( ) . getShell ( ) . getDisplay ( ) ; code_block = IfStatement ; getContainer ( ) . addControlListener ( new ControlAdapter ( ) code_block = "" ; ) ; display . asyncExec ( this :: updateProblemIndication ) ; Log . debug ( "NeoEMF Editor opened in {0}" , stopwatch . stop ( ) . elapsed ( ) ) ; }
public void test() { if ( isConnected ( ) ) { logger . debug ( "Open: connection is already open" ) ; } else { sslContextFactory . setTrustAll ( true ) ; sslContextFactory . setEndpointIdentificationAlgorithm ( null ) ; WebSocketClient client = this . client ; code_block = IfStatement ; TibberWebSocketListener socket = this . socket ; code_block = IfStatement ; ClientUpgradeRequest newRequest = new ClientUpgradeRequest ( ) ; newRequest . setHeader ( "Authorization" , "Bearer " + tibberConfig . getToken ( ) ) ; newRequest . setSubProtocols ( "graphql-subscriptions" ) ; code_block = TryStatement ;  code_block = TryStatement ;  } }
public void test() { try { logger . debug ( "Starting Websocket connection" ) ; client . start ( ) ; } catch ( Exception e ) { logger . warn ( "Websocket Start Exception: {}" , e . getMessage ( ) ) ; } }
public void test() { try { logger . debug ( "Connecting Websocket connection" ) ; sessionFuture = client . connect ( socket , new URI ( SUBSCRIPTION_URL ) , newRequest ) ; } catch ( IOException e ) { logger . warn ( "Websocket Connect Exception: {}" , e . getMessage ( ) ) ; } catch ( URISyntaxException e ) { logger . warn ( "Websocket URI Exception: {}" , e . getMessage ( ) ) ; } }
public void test() { try { logger . debug ( "Connecting Websocket connection" ) ; sessionFuture = client . connect ( socket , new URI ( SUBSCRIPTION_URL ) , newRequest ) ; } catch ( IOException e ) { logger . warn ( "Websocket Connect Exception: {}" , e . getMessage ( ) ) ; } catch ( URISyntaxException e ) { logger . warn ( "Websocket URI Exception: {}" , e . getMessage ( ) ) ; } }
@ Override public void executeUnit ( Person unit ) { LOGGER . warn ( "Executing: " + unit ) ; executed . add ( unit . getId ( ) ) ; }
public void test() { if ( d == null ) { logger . debug ( "drug was null returning false" ) ; return false ; } }
public void test() { if ( d . getProviderNo ( ) == null || d . getProviderNo ( ) . equals ( "" ) ) { logger . debug ( "provider was null or blank returning false" ) ; return false ; } }
public void test() { if ( d . getDemographicId ( ) == null || d . getDemographicId ( ) < 0 ) { logger . debug ( "demographic was null returning false" ) ; return false ; } }
public void test() { if ( d . getRxDate ( ) == null ) { logger . debug ( "rx date was null returning false" ) ; return false ; } }
public void test() { if ( d . getEndDate ( ) == null || d . getRxDate ( ) . after ( d . getEndDate ( ) ) ) { logger . debug ( "drug endDate was null" ) ; return false ; } }
@ Test public void testMarkerFiltering ( ) throws Exception { Slf4jLogger log = new Slf4jLogger ( LoggerFactory . getLogger ( Slf4jLoggerMarkerTest . class ) ) ; log . error ( "IGNORE_ME" , "Ignored error" , null ) ; log . warning ( "IGNORE_ME" , "Ignored warning" , null ) ; log . info ( "IGNORE_ME" , "Ignored info" ) ; log . debug ( "IGNORE_ME" , "Ignored debug" ) ; log . trace ( "IGNORE_ME" , "Ignored trace" ) ; log . error ( "ACCEPT_ME" , "Accepted error" , null ) ; log . warning ( "ACCEPT_ME" , "Accepted warning" , null ) ; log . info ( "ACCEPT_ME" , "Accepted info" ) ; log . debug ( "ACCEPT_ME" , "Accepted debug" ) ; log . trace ( "ACCEPT_ME" , "Accepted trace" ) ; File allFile = U . resolveIgnitePath ( LOG_ALL ) ; assertNotNull ( allFile ) ; String all = U . readFileToString ( allFile . getPath ( ) , "UTF-8" ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored error" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored warning" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored info" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored debug" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored trace" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted error" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted warning" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted info" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted debug" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted trace" ) ) ; File filteredFile = U . resolveIgnitePath ( LOG_FILTERED ) ; assertNotNull ( filteredFile ) ; String filtered = U . readFileToString ( filteredFile . getPath ( ) , "UTF-8" ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored error" ) ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored warning" ) ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored info" ) ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored debug" ) ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored trace" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted error" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted warning" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted info" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted debug" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted trace" ) ) ; }
@ Test public void testMarkerFiltering ( ) throws Exception { Slf4jLogger log = new Slf4jLogger ( LoggerFactory . getLogger ( Slf4jLoggerMarkerTest . class ) ) ; log . error ( "IGNORE_ME" , "Ignored error" , null ) ; log . warning ( "IGNORE_ME" , "Ignored warning" , null ) ; log . info ( "IGNORE_ME" , "Ignored info" ) ; log . debug ( "IGNORE_ME" , "Ignored debug" ) ; log . trace ( "IGNORE_ME" , "Ignored trace" ) ; log . error ( "ACCEPT_ME" , "Accepted error" , null ) ; log . warning ( "ACCEPT_ME" , "Accepted warning" , null ) ; log . info ( "ACCEPT_ME" , "Accepted info" ) ; log . debug ( "ACCEPT_ME" , "Accepted debug" ) ; log . trace ( "ACCEPT_ME" , "Accepted trace" ) ; File allFile = U . resolveIgnitePath ( LOG_ALL ) ; assertNotNull ( allFile ) ; String all = U . readFileToString ( allFile . getPath ( ) , "UTF-8" ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored error" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored warning" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored info" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored debug" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored trace" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted error" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted warning" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted info" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted debug" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted trace" ) ) ; File filteredFile = U . resolveIgnitePath ( LOG_FILTERED ) ; assertNotNull ( filteredFile ) ; String filtered = U . readFileToString ( filteredFile . getPath ( ) , "UTF-8" ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored error" ) ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored warning" ) ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored info" ) ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored debug" ) ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored trace" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted error" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted warning" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted info" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted debug" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted trace" ) ) ; }
@ Test public void testMarkerFiltering ( ) throws Exception { Slf4jLogger log = new Slf4jLogger ( LoggerFactory . getLogger ( Slf4jLoggerMarkerTest . class ) ) ; log . error ( "IGNORE_ME" , "Ignored error" , null ) ; log . warning ( "IGNORE_ME" , "Ignored warning" , null ) ; log . info ( "IGNORE_ME" , "Ignored info" ) ; log . debug ( "IGNORE_ME" , "Ignored debug" ) ; log . trace ( "IGNORE_ME" , "Ignored trace" ) ; log . error ( "ACCEPT_ME" , "Accepted error" , null ) ; log . warning ( "ACCEPT_ME" , "Accepted warning" , null ) ; log . info ( "ACCEPT_ME" , "Accepted info" ) ; log . debug ( "ACCEPT_ME" , "Accepted debug" ) ; log . trace ( "ACCEPT_ME" , "Accepted trace" ) ; File allFile = U . resolveIgnitePath ( LOG_ALL ) ; assertNotNull ( allFile ) ; String all = U . readFileToString ( allFile . getPath ( ) , "UTF-8" ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored error" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored warning" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored info" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored debug" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored trace" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted error" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted warning" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted info" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted debug" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted trace" ) ) ; File filteredFile = U . resolveIgnitePath ( LOG_FILTERED ) ; assertNotNull ( filteredFile ) ; String filtered = U . readFileToString ( filteredFile . getPath ( ) , "UTF-8" ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored error" ) ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored warning" ) ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored info" ) ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored debug" ) ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored trace" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted error" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted warning" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted info" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted debug" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted trace" ) ) ; }
@ Test public void testMarkerFiltering ( ) throws Exception { Slf4jLogger log = new Slf4jLogger ( LoggerFactory . getLogger ( Slf4jLoggerMarkerTest . class ) ) ; log . error ( "IGNORE_ME" , "Ignored error" , null ) ; log . warning ( "IGNORE_ME" , "Ignored warning" , null ) ; log . info ( "IGNORE_ME" , "Ignored info" ) ; log . debug ( "IGNORE_ME" , "Ignored debug" ) ; log . trace ( "IGNORE_ME" , "Ignored trace" ) ; log . error ( "ACCEPT_ME" , "Accepted error" , null ) ; log . warning ( "ACCEPT_ME" , "Accepted warning" , null ) ; log . info ( "ACCEPT_ME" , "Accepted info" ) ; log . debug ( "ACCEPT_ME" , "Accepted debug" ) ; log . trace ( "ACCEPT_ME" , "Accepted trace" ) ; File allFile = U . resolveIgnitePath ( LOG_ALL ) ; assertNotNull ( allFile ) ; String all = U . readFileToString ( allFile . getPath ( ) , "UTF-8" ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored error" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored warning" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored info" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored debug" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored trace" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted error" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted warning" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted info" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted debug" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted trace" ) ) ; File filteredFile = U . resolveIgnitePath ( LOG_FILTERED ) ; assertNotNull ( filteredFile ) ; String filtered = U . readFileToString ( filteredFile . getPath ( ) , "UTF-8" ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored error" ) ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored warning" ) ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored info" ) ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored debug" ) ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored trace" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted error" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted warning" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted info" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted debug" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted trace" ) ) ; }
@ Test public void testMarkerFiltering ( ) throws Exception { Slf4jLogger log = new Slf4jLogger ( LoggerFactory . getLogger ( Slf4jLoggerMarkerTest . class ) ) ; log . error ( "IGNORE_ME" , "Ignored error" , null ) ; log . warning ( "IGNORE_ME" , "Ignored warning" , null ) ; log . info ( "IGNORE_ME" , "Ignored info" ) ; log . debug ( "IGNORE_ME" , "Ignored debug" ) ; log . trace ( "IGNORE_ME" , "Ignored trace" ) ; log . error ( "ACCEPT_ME" , "Accepted error" , null ) ; log . warning ( "ACCEPT_ME" , "Accepted warning" , null ) ; log . info ( "ACCEPT_ME" , "Accepted info" ) ; log . debug ( "ACCEPT_ME" , "Accepted debug" ) ; log . trace ( "ACCEPT_ME" , "Accepted trace" ) ; File allFile = U . resolveIgnitePath ( LOG_ALL ) ; assertNotNull ( allFile ) ; String all = U . readFileToString ( allFile . getPath ( ) , "UTF-8" ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored error" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored warning" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored info" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored debug" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored trace" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted error" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted warning" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted info" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted debug" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted trace" ) ) ; File filteredFile = U . resolveIgnitePath ( LOG_FILTERED ) ; assertNotNull ( filteredFile ) ; String filtered = U . readFileToString ( filteredFile . getPath ( ) , "UTF-8" ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored error" ) ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored warning" ) ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored info" ) ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored debug" ) ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored trace" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted error" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted warning" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted info" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted debug" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted trace" ) ) ; }
@ Test public void testMarkerFiltering ( ) throws Exception { Slf4jLogger log = new Slf4jLogger ( LoggerFactory . getLogger ( Slf4jLoggerMarkerTest . class ) ) ; log . error ( "IGNORE_ME" , "Ignored error" , null ) ; log . warning ( "IGNORE_ME" , "Ignored warning" , null ) ; log . info ( "IGNORE_ME" , "Ignored info" ) ; log . debug ( "IGNORE_ME" , "Ignored debug" ) ; log . trace ( "IGNORE_ME" , "Ignored trace" ) ; log . error ( "ACCEPT_ME" , "Accepted error" , null ) ; log . warning ( "ACCEPT_ME" , "Accepted warning" , null ) ; log . info ( "ACCEPT_ME" , "Accepted info" ) ; log . debug ( "ACCEPT_ME" , "Accepted debug" ) ; log . trace ( "ACCEPT_ME" , "Accepted trace" ) ; File allFile = U . resolveIgnitePath ( LOG_ALL ) ; assertNotNull ( allFile ) ; String all = U . readFileToString ( allFile . getPath ( ) , "UTF-8" ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored error" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored warning" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored info" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored debug" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored trace" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted error" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted warning" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted info" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted debug" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted trace" ) ) ; File filteredFile = U . resolveIgnitePath ( LOG_FILTERED ) ; assertNotNull ( filteredFile ) ; String filtered = U . readFileToString ( filteredFile . getPath ( ) , "UTF-8" ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored error" ) ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored warning" ) ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored info" ) ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored debug" ) ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored trace" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted error" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted warning" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted info" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted debug" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted trace" ) ) ; }
@ Test public void testMarkerFiltering ( ) throws Exception { Slf4jLogger log = new Slf4jLogger ( LoggerFactory . getLogger ( Slf4jLoggerMarkerTest . class ) ) ; log . error ( "IGNORE_ME" , "Ignored error" , null ) ; log . warning ( "IGNORE_ME" , "Ignored warning" , null ) ; log . info ( "IGNORE_ME" , "Ignored info" ) ; log . debug ( "IGNORE_ME" , "Ignored debug" ) ; log . trace ( "IGNORE_ME" , "Ignored trace" ) ; log . error ( "ACCEPT_ME" , "Accepted error" , null ) ; log . warning ( "ACCEPT_ME" , "Accepted warning" , null ) ; log . info ( "ACCEPT_ME" , "Accepted info" ) ; log . debug ( "ACCEPT_ME" , "Accepted debug" ) ; log . trace ( "ACCEPT_ME" , "Accepted trace" ) ; File allFile = U . resolveIgnitePath ( LOG_ALL ) ; assertNotNull ( allFile ) ; String all = U . readFileToString ( allFile . getPath ( ) , "UTF-8" ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored error" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored warning" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored info" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored debug" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored trace" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted error" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted warning" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted info" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted debug" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted trace" ) ) ; File filteredFile = U . resolveIgnitePath ( LOG_FILTERED ) ; assertNotNull ( filteredFile ) ; String filtered = U . readFileToString ( filteredFile . getPath ( ) , "UTF-8" ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored error" ) ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored warning" ) ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored info" ) ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored debug" ) ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored trace" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted error" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted warning" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted info" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted debug" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted trace" ) ) ; }
@ Test public void testMarkerFiltering ( ) throws Exception { Slf4jLogger log = new Slf4jLogger ( LoggerFactory . getLogger ( Slf4jLoggerMarkerTest . class ) ) ; log . error ( "IGNORE_ME" , "Ignored error" , null ) ; log . warning ( "IGNORE_ME" , "Ignored warning" , null ) ; log . info ( "IGNORE_ME" , "Ignored info" ) ; log . debug ( "IGNORE_ME" , "Ignored debug" ) ; log . trace ( "IGNORE_ME" , "Ignored trace" ) ; log . error ( "ACCEPT_ME" , "Accepted error" , null ) ; log . warning ( "ACCEPT_ME" , "Accepted warning" , null ) ; log . info ( "ACCEPT_ME" , "Accepted info" ) ; log . debug ( "ACCEPT_ME" , "Accepted debug" ) ; log . trace ( "ACCEPT_ME" , "Accepted trace" ) ; File allFile = U . resolveIgnitePath ( LOG_ALL ) ; assertNotNull ( allFile ) ; String all = U . readFileToString ( allFile . getPath ( ) , "UTF-8" ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored error" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored warning" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored info" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored debug" ) ) ; assertTrue ( all . contains ( "[IGNORE_ME] Ignored trace" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted error" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted warning" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted info" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted debug" ) ) ; assertTrue ( all . contains ( "[ACCEPT_ME] Accepted trace" ) ) ; File filteredFile = U . resolveIgnitePath ( LOG_FILTERED ) ; assertNotNull ( filteredFile ) ; String filtered = U . readFileToString ( filteredFile . getPath ( ) , "UTF-8" ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored error" ) ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored warning" ) ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored info" ) ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored debug" ) ) ; assertFalse ( filtered . contains ( "[IGNORE_ME] Ignored trace" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted error" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted warning" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted info" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted debug" ) ) ; assertTrue ( filtered . contains ( "[ACCEPT_ME] Accepted trace" ) ) ; }
public void test() { try { int returnValue = CommerceDiscountRelServiceUtil . getCategoriesByCommerceDiscountIdCount ( commerceDiscountId , name ) ; return returnValue ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try { return file . getCanonicalPath ( ) ; } catch ( IOException e ) { logger . warn ( file . getPath ( ) + " getCanonicalPath() error. Error:" + e . getMessage ( ) , e ) ; return file . getAbsolutePath ( ) ; } }
@ Override public void execute ( ) throws Exception { logger . debug ( "Executing samples command line" ) ; String subCommandString = getParsedSubCommand ( samplesCommandOptions . jCommander ) ; RestResponse queryResponse = null ; code_block = SwitchStatement ; createOutput ( queryResponse ) ; }
public void test() { switch ( subCommandString ) { case "create" : queryResponse = create ( ) ; break ; case "load" : queryResponse = load ( ) ; break ; case "info" : queryResponse = info ( ) ; break ; case "search" : queryResponse = search ( ) ; break ; case "update" : queryResponse = update ( ) ; break ; case "delete" : queryResponse = delete ( ) ; break ; case "stats" : queryResponse = stats ( ) ; break ; case "acl" : queryResponse = acl ( ) ; break ; case "acl-update" : queryResponse = updateAcl ( ) ; break ; case "annotation-sets-update" : queryResponse = updateAnnotations ( ) ; break ; default : logger . error ( "Subcommand not valid" ) ; break ; } }
public void test() { try { code_block = IfStatement ; } catch ( InterruptedException e ) { LOG . warn ( "Interrupted while sleeping" , e ) ; } }
public void test() { if ( jobRequest != null && jobRequest . getRequestor ( ) != null ) { logger . debug ( "Getting old job request" ) ; return jobRequest ; } else { logger . debug ( "Creating new job request" ) ; jobRequest = new JobRequest ( getJobUser ( request ) ) ; request . getSession ( ) . setAttribute ( JOB_REQUEST_ATTR , jobRequest ) ; return jobRequest ; } }
public void test() { if ( jobRequest != null && jobRequest . getRequestor ( ) != null ) { logger . debug ( "Getting old job request" ) ; return jobRequest ; } else { logger . debug ( "Creating new job request" ) ; jobRequest = new JobRequest ( getJobUser ( request ) ) ; request . getSession ( ) . setAttribute ( JOB_REQUEST_ATTR , jobRequest ) ; return jobRequest ; } }
@ Test public void testReadEntity ( ) throws Exception { final TestOlingo4ResponseHandler < ClientEntity > responseHandler = new TestOlingo4ResponseHandler < > ( ) ; olingoApp . read ( edm , TEST_AIRLINE , null , null , responseHandler ) ; ClientEntity entity = responseHandler . await ( ) ; assertEquals ( "Shanghai Airline" , entity . getProperty ( "Name" ) . getValue ( ) . toString ( ) ) ; LOG . info ( "Single Entity:  {}" , prettyPrint ( entity ) ) ; responseHandler . reset ( ) ; olingoApp . read ( edm , TEST_PEOPLE , null , null , responseHandler ) ; entity = responseHandler . await ( ) ; assertEquals ( "Russell" , entity . getProperty ( "FirstName" ) . getValue ( ) . toString ( ) ) ; LOG . info ( "Single Entry:  {}" , prettyPrint ( entity ) ) ; responseHandler . reset ( ) ; final Map < String , String > queryParams = new HashMap < > ( ) ; queryParams . put ( SystemQueryOptionKind . EXPAND . toString ( ) , TRIPS ) ; olingoApp . read ( edm , TEST_PEOPLE , queryParams , null , responseHandler ) ; ClientEntity entityExpanded = responseHandler . await ( ) ; LOG . info ( "Single People Entiry with expanded Trips relation:  {}" , prettyPrint ( entityExpanded ) ) ; }
@ Test public void testReadEntity ( ) throws Exception { final TestOlingo4ResponseHandler < ClientEntity > responseHandler = new TestOlingo4ResponseHandler < > ( ) ; olingoApp . read ( edm , TEST_AIRLINE , null , null , responseHandler ) ; ClientEntity entity = responseHandler . await ( ) ; assertEquals ( "Shanghai Airline" , entity . getProperty ( "Name" ) . getValue ( ) . toString ( ) ) ; LOG . info ( "Single Entity:  {}" , prettyPrint ( entity ) ) ; responseHandler . reset ( ) ; olingoApp . read ( edm , TEST_PEOPLE , null , null , responseHandler ) ; entity = responseHandler . await ( ) ; assertEquals ( "Russell" , entity . getProperty ( "FirstName" ) . getValue ( ) . toString ( ) ) ; LOG . info ( "Single Entry:  {}" , prettyPrint ( entity ) ) ; responseHandler . reset ( ) ; final Map < String , String > queryParams = new HashMap < > ( ) ; queryParams . put ( SystemQueryOptionKind . EXPAND . toString ( ) , TRIPS ) ; olingoApp . read ( edm , TEST_PEOPLE , queryParams , null , responseHandler ) ; ClientEntity entityExpanded = responseHandler . await ( ) ; LOG . info ( "Single People Entiry with expanded Trips relation:  {}" , prettyPrint ( entityExpanded ) ) ; }
@ Test public void testReadEntity ( ) throws Exception { final TestOlingo4ResponseHandler < ClientEntity > responseHandler = new TestOlingo4ResponseHandler < > ( ) ; olingoApp . read ( edm , TEST_AIRLINE , null , null , responseHandler ) ; ClientEntity entity = responseHandler . await ( ) ; assertEquals ( "Shanghai Airline" , entity . getProperty ( "Name" ) . getValue ( ) . toString ( ) ) ; LOG . info ( "Single Entity:  {}" , prettyPrint ( entity ) ) ; responseHandler . reset ( ) ; olingoApp . read ( edm , TEST_PEOPLE , null , null , responseHandler ) ; entity = responseHandler . await ( ) ; assertEquals ( "Russell" , entity . getProperty ( "FirstName" ) . getValue ( ) . toString ( ) ) ; LOG . info ( "Single Entry:  {}" , prettyPrint ( entity ) ) ; responseHandler . reset ( ) ; final Map < String , String > queryParams = new HashMap < > ( ) ; queryParams . put ( SystemQueryOptionKind . EXPAND . toString ( ) , TRIPS ) ; olingoApp . read ( edm , TEST_PEOPLE , queryParams , null , responseHandler ) ; ClientEntity entityExpanded = responseHandler . await ( ) ; LOG . info ( "Single People Entiry with expanded Trips relation:  {}" , prettyPrint ( entityExpanded ) ) ; }
@ Override public void deleteBuildGroup ( ) throws RepositoryManagerException { logger . info ( "BEGIN: Removing build aggregation group: {}" , buildContentId ) ; userLog . info ( "Removing build aggregation group" ) ; StopWatch stopWatch = StopWatch . createStarted ( ) ; code_block = TryStatement ;  logger . info ( "END: Removing build aggregation group: {}, took: {} seconds" , buildContentId , stopWatch . getTime ( TimeUnit . SECONDS ) ) ; stopWatch . reset ( ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Ids message handling failed. [exception=({})]" , exception . getMessage ( ) , exception ) ; } }
public void test() { try { Result result = buffer . pollLast ( waitTime . get ( ) , TimeUnit . MILLISECONDS ) ; code_block = IfStatement ; } catch ( InterruptedException e ) { LOG . error ( "ResultLog thread interrupted." , e ) ; } catch ( JsonProcessingException e ) { LOG . error ( "Failed to generate the JSON for a result." , e ) ; } catch ( IOException e ) { LOG . error ( "Failed to write JSON to output stream for a result" , e ) ; } }
public void test() { try { Result result = buffer . pollLast ( waitTime . get ( ) , TimeUnit . MILLISECONDS ) ; code_block = IfStatement ; } catch ( InterruptedException e ) { LOG . error ( "ResultLog thread interrupted." , e ) ; } catch ( JsonProcessingException e ) { LOG . error ( "Failed to generate the JSON for a result." , e ) ; } catch ( IOException e ) { LOG . error ( "Failed to write JSON to output stream for a result" , e ) ; } }
public void test() { try { Result result = buffer . pollLast ( waitTime . get ( ) , TimeUnit . MILLISECONDS ) ; code_block = IfStatement ; } catch ( InterruptedException e ) { LOG . error ( "ResultLog thread interrupted." , e ) ; } catch ( JsonProcessingException e ) { LOG . error ( "Failed to generate the JSON for a result." , e ) ; } catch ( IOException e ) { LOG . error ( "Failed to write JSON to output stream for a result" , e ) ; } }
public void joinTournament ( String engineEndpoint , int numOfPlayers ) { this . getStatus ( ) . setState ( PlayerStatus . State . Requesting ) ; Client client = ClientBuilder . newClient ( new ClientConfig ( ) . register ( LoggingFeature . class ) ) ; WebTarget webTarget = client . target ( engineEndpoint ) . path ( "api/v0.1/tournament/join" ) ; Invocation . Builder invocationBuilder = webTarget . request ( MediaType . APPLICATION_JSON ) ; GameTicket ticket = new GameTicket ( this , numOfPlayers ) ; Response response = invocationBuilder . put ( Entity . entity ( ticket , MediaType . APPLICATION_JSON ) ) ; GameStatusResponse status = response . readEntity ( GameStatusResponse . class ) ; code_block = IfStatement ; logger . debug ( "Request to join tournament - response (status " + response . getStatus ( ) + "): " + status ) ; }
public void test() { try { x . close ( ) ; } catch ( Exception e ) { LOG . debug ( "close result set error" , e ) ; } }
public void test() { try { ThemeDisplay themeDisplay = ( ThemeDisplay ) httpServletRequest . getAttribute ( WebKeys . THEME_DISPLAY ) ; return StringBundler . concat ( "<iframe data-video-liferay height=\"315\" frameborder=\"0\" " , "src=\"" , _dlURLHelper . getPreviewURL ( fileVersion . getFileEntry ( ) , fileVersion , themeDisplay , "&videoEmbed=true" , true , false ) , "\" width=\"560\"></iframe>" ) ; } catch ( PortalException portalException ) { _log . error ( portalException , portalException ) ; return null ; } }
public void test() { if ( ( se . getErrorCode ( ) == 50000 ) && ( se . getSQLState ( ) . equals ( "XJ015" ) ) ) { LOG . info ( "Derby shutdown complete normally." ) ; } else { LOG . info ( "Derby shutdown complete abnormally. - message:" + se . getMessage ( ) ) ; } }
public void test() { if ( ( se . getErrorCode ( ) == 50000 ) && ( se . getSQLState ( ) . equals ( "XJ015" ) ) ) { LOG . info ( "Derby shutdown complete normally." ) ; } else { LOG . info ( "Derby shutdown complete abnormally. - message:" + se . getMessage ( ) ) ; } }
public void test() { try { int returnValue = SiteNavigationMenuServiceUtil . getSiteNavigationMenusCount ( groupIds ) ; return returnValue ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try { s_logger . debug ( "Adding PhysicalNetworkServiceProvider VirtualRouter" ) ; String insertPNSP = "INSERT INTO `cloud`.`physical_network_service_providers` (`uuid`, `physical_network_id` , `provider_name`, `state` ," + "`destination_physical_network_id`, `vpn_service_provided`, `dhcp_service_provided`, `dns_service_provided`, `gateway_service_provided`," + "`firewall_service_provided`, `source_nat_service_provided`, `load_balance_service_provided`, `static_nat_service_provided`," + "`port_forwarding_service_provided`, `user_data_service_provided`, `security_group_service_provided`) VALUES (?,?,?,?,0,1,1,1,1,1,1,1,1,1,1,0)" ; String routerUUID = UUID . randomUUID ( ) . toString ( ) ; pstmtUpdate = conn . prepareStatement ( insertPNSP ) ; pstmtUpdate . setString ( 1 , routerUUID ) ; pstmtUpdate . setLong ( 2 , physicalNetworkId ) ; pstmtUpdate . setString ( 3 , "VirtualRouter" ) ; pstmtUpdate . setString ( 4 , "Enabled" ) ; pstmtUpdate . executeUpdate ( ) ; pstmtUpdate . close ( ) ; String fetchNSPid = "SELECT id from `cloud`.`physical_network_service_providers` where physical_network_id=" + physicalNetworkId + " AND provider_name = 'VirtualRouter' AND uuid = ?" ; pstmt2 = conn . prepareStatement ( fetchNSPid ) ; pstmt2 . setString ( 1 , routerUUID ) ; ResultSet rsNSPid = pstmt2 . executeQuery ( ) ; rsNSPid . next ( ) ; long nspId = rsNSPid . getLong ( 1 ) ; pstmt2 . close ( ) ; String insertRouter = "INSERT INTO `cloud`.`virtual_router_providers` (`nsp_id`, `uuid` , `type` , `enabled`) " + "VALUES (?,?,?,?)" ; pstmtUpdate = conn . prepareStatement ( insertRouter ) ; pstmtUpdate . setLong ( 1 , nspId ) ; pstmtUpdate . setString ( 2 , UUID . randomUUID ( ) . toString ( ) ) ; pstmtUpdate . setString ( 3 , "VirtualRouter" ) ; pstmtUpdate . setInt ( 4 , 1 ) ; pstmtUpdate . executeUpdate ( ) ; pstmtUpdate . close ( ) ; } catch ( SQLException e ) { throw new CloudRuntimeException ( "Exception while adding PhysicalNetworks" , e ) ; } finally { closeAutoCloseable ( pstmt2 ) ; closeAutoCloseable ( pstmtUpdate ) ; } }
public void test() { switch ( child . getName ( ) ) { case "NUMBERS" : formatOption = child . toString ( ) ; break ; case "ABDRUCK_NAME" : nameOption = child . toString ( ) ; break ; case "ALL_VERSIONS_HIGHLIGHT_COLOR" : highlightColors . put ( PrintBlockSignature . ALL_VERSIONS , checkHighlightColor ( child . toString ( ) ) ) ; break ; case "DRAFT_ONLY_HIGHLIGHT_COLOR" : highlightColors . put ( PrintBlockSignature . DRAFT_ONLY , checkHighlightColor ( child . toString ( ) ) ) ; break ; case "NOT_IN_ORIGINAL_HIGHLIGHT_COLOR" : highlightColors . put ( PrintBlockSignature . NOT_IN_ORIGINAL , checkHighlightColor ( child . toString ( ) ) ) ; break ; case "ORIGINAL_ONLY_HIGHLIGHT_COLOR" : highlightColors . put ( PrintBlockSignature . ORIGINAL_ONLY , checkHighlightColor ( child . toString ( ) ) ) ; break ; case "COPY_ONLY_HIGHLIGHT_COLOR" : highlightColors . put ( PrintBlockSignature . COPY_ONLY , checkHighlightColor ( child . toString ( ) ) ) ; break ; default : LOGGER . warn ( "Unbekannte Einstellung {}" , child . getName ( ) ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( NodeNotFoundException e ) { LOGGER . debug ( "" , e ) ; } finally { format = NumberFormat . valueOf ( formatOption . toUpperCase ( ) ) ; LOGGER . debug ( "\"Verwende Zahlenformat '{}' aus Attribut NUMBERS.\"" , format ) ; copyName = nameOption ; LOGGER . debug ( "Verwende ABDRUCK_NAME '{}'" , copyName ) ; } }
public void test() { try { List < ConfidenceData > objList = ( List < ConfidenceData > ) criteria . list ( ) ; return objList ; } catch ( HibernateException e ) { logger . error ( "exception" , e ) ; e . printStackTrace ( ) ; } }
@ Override protected Map < String , String > modifyIniFile ( String commaSeparatedFilePaths , String commaSeparatedParentAccessions ) { Log . debug ( "INI FILE:" + commaSeparatedFilePaths ) ; Map < String , String > iniFileMap = new TreeMap < > ( ) ; iniFileMap . put ( ReservedIniKeys . INPUT_FILE . getKey ( ) , commaSeparatedFilePaths ) ; return iniFileMap ; }
public void test() { try { handleCommandInternal ( channelUID , command ) ; updateModuleStatus ( ) ; } catch ( SmartherIllegalPropertyValueException e ) { logger . warn ( "Module[{}] Received command {} with illegal value {} on channel {}" , thing . getUID ( ) , command , e . getMessage ( ) , channelUID . getId ( ) ) ; } catch ( SmartherGatewayException e ) { updateStatus ( ThingStatus . OFFLINE , ThingStatusDetail . COMMUNICATION_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( exception , exception ) ; } }
public void test() { try { executors . execute ( new Runnable ( ) code_block = "" ; ) ; } catch ( Exception e ) { logger . error ( "[closeAllConnections] {}" , e ) ; } }
public void test() { try { final List < Principal > users = createTestNodes ( testUserType , 3 ) ; final List < TestThree > testThrees = new LinkedList < > ( ) ; final Random random = new Random ( ) ; String uuid = null ; int count = 0 ; code_block = TryStatement ;  code_block = TryStatement ;  RestAssured . given ( ) . contentType ( "application/json; charset=UTF-8" ) . filter ( ResponseLoggingFilter . logResponseIfStatusCodeIs ( 500 ) ) . expect ( ) . statusCode ( 200 ) . when ( ) . get ( concat ( "/test_threes?sort=createdDate&owner=" + uuid + "&enumProperty=" + TestEnum . Status1 ) ) ; } catch ( FrameworkException ex ) { ex . printStackTrace ( ) ; logger . warn ( "" , ex ) ; fail ( "Unexpected exception" ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "processing include ..." ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( resourceError . message , resourceError ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "to be created, uoc common" ) ; logger . debug ( objectAsXmlString ( uocCommon , UocCommon . class ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "to be created, uoc common" ) ; logger . debug ( objectAsXmlString ( uocCommon , UocCommon . class ) ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { LOGGER . warn ( "Could not cancel sync repl request" , e ) ; } }
public void runGenomeInfo ( ) throws IOException , InterruptedException { logger . info ( "Downloading genome info ..." ) ; String outputFolder = downloadFolder . getParent ( ) . toAbsolutePath ( ) . toString ( ) + "/generated_json/" ; code_block = IfStatement ; }
@ Test public void resolveOfEmailWithTagsReturnsEntity ( ) throws Exception { idsMan . addEntity ( new IdentityParam ( EmailIdentity . ID , "a+zzz@ex.com" ) , "cr-pass" , EntityState . valid ) ; HttpClient client = getClient ( ) ; HttpHost host = new HttpHost ( "localhost" , 53456 , "https" ) ; HttpContext localcontext = getClientContext ( host ) ; HttpGet resolve = new HttpGet ( "/restadm/v1/resolve/email/a+foo@ex.com" ) ; HttpResponse response = client . execute ( host , resolve , localcontext ) ; String contents = EntityUtils . toString ( response . getEntity ( ) ) ; assertEquals ( contents , Status . OK . getStatusCode ( ) , response . getStatusLine ( ) . getStatusCode ( ) ) ; log . info ( "User's info:\n" + formatJson ( contents ) ) ; }
public IRODSMidLevelProtocol currentConnectionCheckRenewalOfSocket ( final IRODSAccount irodsAccount ) throws AuthenticationException , JargonException { log . debug ( "renewConnection()" ) ; code_block = IfStatement ; IRODSMidLevelProtocol irodsMidLevelProtocol = currentConnection ( irodsAccount ) ; log . debug ( "evaluate conn for renewal:{}" , irodsAccount ) ; boolean shutdown = evaluateConnectionForRenewal ( irodsMidLevelProtocol ) ; code_block = IfStatement ; }
public IRODSMidLevelProtocol currentConnectionCheckRenewalOfSocket ( final IRODSAccount irodsAccount ) throws AuthenticationException , JargonException { log . debug ( "renewConnection()" ) ; code_block = IfStatement ; IRODSMidLevelProtocol irodsMidLevelProtocol = currentConnection ( irodsAccount ) ; log . debug ( "evaluate conn for renewal:{}" , irodsAccount ) ; boolean shutdown = evaluateConnectionForRenewal ( irodsMidLevelProtocol ) ; code_block = IfStatement ; }
public void test() { try { action . accept ( aTarget ) ; } catch ( Exception e ) { Page page = ( Page ) PageRequestHandlerTracker . getLastHandler ( RequestCycle . get ( ) ) . getPage ( ) ; LoggerFactory . getLogger ( page . getClass ( ) ) . error ( "Error: " + e . getMessage ( ) , e ) ; page . error ( "Error: " + e . getMessage ( ) ) ; aTarget . addChildren ( page , IFeedback . class ) ; } }
public void test() { try { client . close ( ) ; } catch ( IOException e ) { logger . error ( "{}" , e . getLocalizedMessage ( ) , e ) ; } }
public void test() { if ( Boolean . getBoolean ( "zookeeper.jmx.log4j.disable" ) ) { LOG . info ( "Log4j 1.2 jmx support is disabled by property." ) ; } else { code_block = TryStatement ;  } }
public void test() { try { Class . forName ( "org.apache.log4j.jmx.HierarchyDynamicMBean" ) ; enabled = true ; LOG . info ( "Log4j 1.2 jmx support found and enabled." ) ; } catch ( ClassNotFoundException e ) { LOG . info ( "Log4j 1.2 jmx support not found; jmx disabled." ) ; } }
public void test() { try { Class . forName ( "org.apache.log4j.jmx.HierarchyDynamicMBean" ) ; enabled = true ; LOG . info ( "Log4j 1.2 jmx support found and enabled." ) ; } catch ( ClassNotFoundException e ) { LOG . info ( "Log4j 1.2 jmx support not found; jmx disabled." ) ; } }
public void test() { try { pool . invokeAll ( tasks , thinkTime , TimeUnit . SECONDS ) ; pool . awaitTermination ( 1 , TimeUnit . SECONDS ) ; pool . shutdownNow ( ) ; } catch ( InterruptedException | RejectedExecutionException ex ) { logger . warn ( "applyMCTS interrupted" ) ; } }
public void test() { if ( USE_MULTIPLE_THREADS ) { ExecutorService pool = Executors . newFixedThreadPool ( poolSize ) ; List < MCTSExecutor > tasks = new ArrayList < > ( ) ; code_block = ForStatement ; code_block = TryStatement ;  int simCount = 0 ; code_block = ForStatement ; tasks . clear ( ) ; totalThinkTime += thinkTime ; totalSimulations += simCount ; logger . info ( "Player: " + name + " Simulated " + simCount + " games in " + thinkTime + " seconds - nodes in tree: " + root . size ( ) ) ; logger . info ( "Total: Simulated " + totalSimulations + " games in " + totalThinkTime + " seconds - Average: " + totalSimulations / totalThinkTime ) ; MCTSNode . logHitMiss ( ) ; } else { long startTime = System . nanoTime ( ) ; long endTime = startTime + ( thinkTime * 1000000000l ) ; MCTSNode current ; int simCount = 0 ; code_block = WhileStatement ; logger . info ( "Simulated " + simCount + " games - nodes in tree: " + root . size ( ) ) ; } }
public void test() { if ( USE_MULTIPLE_THREADS ) { ExecutorService pool = Executors . newFixedThreadPool ( poolSize ) ; List < MCTSExecutor > tasks = new ArrayList < > ( ) ; code_block = ForStatement ; code_block = TryStatement ;  int simCount = 0 ; code_block = ForStatement ; tasks . clear ( ) ; totalThinkTime += thinkTime ; totalSimulations += simCount ; logger . info ( "Player: " + name + " Simulated " + simCount + " games in " + thinkTime + " seconds - nodes in tree: " + root . size ( ) ) ; logger . info ( "Total: Simulated " + totalSimulations + " games in " + totalThinkTime + " seconds - Average: " + totalSimulations / totalThinkTime ) ; MCTSNode . logHitMiss ( ) ; } else { long startTime = System . nanoTime ( ) ; long endTime = startTime + ( thinkTime * 1000000000l ) ; MCTSNode current ; int simCount = 0 ; code_block = WhileStatement ; logger . info ( "Simulated " + simCount + " games - nodes in tree: " + root . size ( ) ) ; } }
public void test() { if ( USE_MULTIPLE_THREADS ) { ExecutorService pool = Executors . newFixedThreadPool ( poolSize ) ; List < MCTSExecutor > tasks = new ArrayList < > ( ) ; code_block = ForStatement ; code_block = TryStatement ;  int simCount = 0 ; code_block = ForStatement ; tasks . clear ( ) ; totalThinkTime += thinkTime ; totalSimulations += simCount ; logger . info ( "Player: " + name + " Simulated " + simCount + " games in " + thinkTime + " seconds - nodes in tree: " + root . size ( ) ) ; logger . info ( "Total: Simulated " + totalSimulations + " games in " + totalThinkTime + " seconds - Average: " + totalSimulations / totalThinkTime ) ; MCTSNode . logHitMiss ( ) ; } else { long startTime = System . nanoTime ( ) ; long endTime = startTime + ( thinkTime * 1000000000l ) ; MCTSNode current ; int simCount = 0 ; code_block = WhileStatement ; logger . info ( "Simulated " + simCount + " games - nodes in tree: " + root . size ( ) ) ; } }
void enrichTextUnitsWithUsages ( List < GitBlameWithUsage > gitBlameWithUsages ) { logger . debug ( "Enrich text units with usages" ) ; Map < Long , GitBlameWithUsage > assetTextUnitIdToGitBlameWithUsage = new HashMap < > ( ) ; code_block = ForStatement ; logger . debug ( "Fetch the asset text unit information" ) ; List < AssetTextUnit > assetTextUnits = assetTextUnitRepository . findByIdIn ( new ArrayList < Long > ( assetTextUnitIdToGitBlameWithUsage . keySet ( ) ) ) ; code_block = ForStatement ; logger . debug ( "End Enrich text units with usages" ) ; }
void enrichTextUnitsWithUsages ( List < GitBlameWithUsage > gitBlameWithUsages ) { logger . debug ( "Enrich text units with usages" ) ; Map < Long , GitBlameWithUsage > assetTextUnitIdToGitBlameWithUsage = new HashMap < > ( ) ; code_block = ForStatement ; logger . debug ( "Fetch the asset text unit information" ) ; List < AssetTextUnit > assetTextUnits = assetTextUnitRepository . findByIdIn ( new ArrayList < Long > ( assetTextUnitIdToGitBlameWithUsage . keySet ( ) ) ) ; code_block = ForStatement ; logger . debug ( "End Enrich text units with usages" ) ; }
public void test() { try { CalendarBookingServiceUtil . deleteCalendarBookingInstance ( calendarBookingId , startTime , allFollowing ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( portalException , portalException ) ; } }
public CheckPolicyRequestType transformPatientDiscoveryEntityToCheckPolicy ( RespondingGatewayPRPAIN201305UV02RequestType event ) { LOG . debug ( "Begin -- PatientDiscoveryPolicyTransformHelper.transformPatientDiscoveryEntityToCheckPolicy()" ) ; code_block = IfStatement ; CheckPolicyRequestType checkPolicyRequest = transformPRPAIN201305UV02ToCheckPolicy ( event . getPRPAIN201305UV02 ( ) , event . getAssertion ( ) ) ; LOG . debug ( "End -- PatientDiscoveryPolicyTransformHelper.transformPatientDiscoveryEntityToCheckPolicy()" ) ; return checkPolicyRequest ; }
public CheckPolicyRequestType transformPatientDiscoveryEntityToCheckPolicy ( RespondingGatewayPRPAIN201305UV02RequestType event ) { LOG . debug ( "Begin -- PatientDiscoveryPolicyTransformHelper.transformPatientDiscoveryEntityToCheckPolicy()" ) ; code_block = IfStatement ; CheckPolicyRequestType checkPolicyRequest = transformPRPAIN201305UV02ToCheckPolicy ( event . getPRPAIN201305UV02 ( ) , event . getAssertion ( ) ) ; LOG . debug ( "End -- PatientDiscoveryPolicyTransformHelper.transformPatientDiscoveryEntityToCheckPolicy()" ) ; return checkPolicyRequest ; }
@ Test public void testRedefineTerms ( ) throws Exception { logger . debug ( "testRedefineTerms" ) ; EntityManager em = app . getEntityManager ( ) ; assertNotNull ( em ) ; Map < String , Object > properties = new LinkedHashMap < String , Object > ( ) ; properties . put ( "username" , "edanuff" ) ; properties . put ( "email" , "ed@anuff.com" ) ; em . create ( "user" , properties ) ; app . waitForQueueDrainAndRefreshIndex ( ) ; String s = "select {name: username, email: email} where username = 'edanuff'" ; Query query = Query . fromQL ( s ) ; Results r = em . searchCollection ( em . getApplicationRef ( ) , "users" , query ) ; assertTrue ( r . size ( ) == 1 ) ; }
public void test() { if ( ( isSatisfiedBy != null ) && isSatisfiedBy . apply ( key ) ) { LOG . debug ( "removing " + timeField + " satisfied by " + key ) ; unsatisfied . remove ( timeField ) ; } }
public org . talend . mdm . webservice . WSBoolean isItemModifiedByOther ( org . talend . mdm . webservice . WSIsItemModifiedByOther arg0 ) { LOG . info ( "Executing operation isItemModifiedByOther" ) ; System . out . println ( arg0 ) ; code_block = TryStatement ;  }
@ Override public int getNumberOfResources ( ) { LOGGER . info ( "Received request for number of active resources" ) ; return ResourceManager . getTotalNumberOfWorkers ( ) ; }
public void test() { try { Thread . sleep ( ClusterUtils . START_UP_CHECK_TIME_INTERVAL_MS ) ; } catch ( InterruptedException e ) { Thread . currentThread ( ) . interrupt ( ) ; logger . error ( "Unexpected interruption when waiting for next start up check" , e ) ; } }
public void test() { try { report . lastReportAt = generateReport ( ) ; code_block = IfStatement ; } catch ( Exception e ) { log . debug ( "Error generating export report {} for {}." , report , key . user . email , e ) ; ctx . writeAndFlush ( illegalCommand ( message . id ) , ctx . voidPromise ( ) ) ; } }
public void test() { if ( deviceValue == null ) { logger . warn ( "Failed to convert Number {} because Smartthings returned a null value." , deviceType ) ; return UnDefType . UNDEF ; } }
public void test() { if ( deviceValue instanceof String ) { double d = Double . parseDouble ( ( String ) deviceValue ) ; d *= 3.6 ; return new DecimalType ( d ) ; } else-if ( deviceValue instanceof Long ) { double d = ( ( Long ) deviceValue ) . longValue ( ) ; d *= 3.6 ; return new DecimalType ( d ) ; } else-if ( deviceValue instanceof BigDecimal ) { double d = ( ( BigDecimal ) deviceValue ) . doubleValue ( ) ; d *= 3.6 ; return new DecimalType ( d ) ; } else-if ( deviceValue instanceof Number ) { double d = ( ( Number ) deviceValue ) . doubleValue ( ) ; d *= 3.6 ; return new DecimalType ( d ) ; } else { logger . warn ( "Failed to convert Number {} with a value of {} from class {} to an appropriate type." , deviceType , deviceValue , deviceValue . getClass ( ) . getName ( ) ) ; return UnDefType . UNDEF ; } }
public void test() { try { ctx = getContext ( getSecurityManager ( ) . getCurrentSubject ( ) ) ; SearchResult searchResult = findGroupByGroupName ( ctx , removeDomainPostfix ( name ) ) ; code_block = IfStatement ; final LDAPSearchContext search = ensureContextFactory ( ) . getSearch ( ) ; final String dnGroup = ( String ) searchResult . getAttributes ( ) . get ( search . getSearchGroup ( ) . getSearchAttribute ( LDAPSearchAttributeKey . DN ) ) . get ( ) ; final SearchAttribute sa = new SearchAttribute ( search . getSearchAccount ( ) . getSearchAttribute ( LDAPSearchAttributeKey . MEMBER_OF ) , escapeSearchAttribute ( dnGroup ) ) ; final String searchFilter = buildSearchFilter ( search . getSearchAccount ( ) . getSearchFilterPrefix ( ) , sa ) ; final SearchControls searchControls = new SearchControls ( ) ; searchControls . setSearchScope ( SearchControls . SUBTREE_SCOPE ) ; searchControls . setReturningAttributes ( new String [ ] code_block = "" ; ) ; final NamingEnumeration < SearchResult > results = ctx . search ( search . getBase ( ) , searchFilter , searchControls ) ; code_block = WhileStatement ; } catch ( final NamingException ne ) { LOG . error ( new AuthenticationException ( AuthenticationException . UNNOWN_EXCEPTION , ne . getMessage ( ) ) ) ; } finally { code_block = IfStatement ; } }
public void test() { try { defaultMQAdminExt . createAndUpdateTopicConfig ( addr , topicConfig ) ; } catch ( Exception e ) { logger . error ( "Create topic:{} addr:{} failed" , addr , topic ) ; } }
public void test() { if ( pharmacophoreMolecule . getAtomCount ( ) < pharmacophoreQuery . getAtomCount ( ) ) { logger . debug ( "Target [" + title + "] did not match the query SMARTS. Skipping constraints" ) ; return false ; } }
public void test() { try { bundle . getString ( key ) ; return getStringPropertyArray ( bundle , key ) ; } catch ( MissingResourceException e ) { logger . info ( "Using default value of " + Arrays . toString ( defaultValue ) + " for property " + key ) ; printOutArrayProperty ( key , defaultValue , true ) ; return defaultValue ; } }
public HTTPRequest filterLogAndConvertRe ( HttpRequest request ) { code_block = ForStatement ; checkRequestHasContentLengthOrChunkedEncoding ( request , "After filtering, the request has neither chunked encoding nor content length: " + request ) ; logger . debug ( "Sending request %s: %s" , request . hashCode ( ) , request . getRequestLine ( ) ) ; wirePayloadIfEnabled ( wire , request ) ; HTTPRequest nativeRequest = convertToGaeRequest . apply ( request ) ; utils . logRequest ( headerLog , request , ">>" ) ; return nativeRequest ; }
public void test() { if ( bk . disableEnsembleChangeFeature . isAvailable ( ) ) { blockAddCompletions . decrementAndGet ( ) ; LOG . debug ( "Ensemble change is disabled. Retry sending to failed bookies {} for ledger {}." , failedBookies , ledgerId ) ; unsetSuccessAndSendWriteRequest ( failedBookies . keySet ( ) ) ; return ; } }
public void test() { try { stream = asStream ( ) ; code_block = IfStatement ; br = new BufferedReader ( new InputStreamReader ( stream , "UTF-8" ) ) ; StringBuilder buf = new StringBuilder ( ) ; String line ; code_block = WhileStatement ; this . responseAsString = buf . toString ( ) ; logger . debug ( responseAsString ) ; stream . close ( ) ; streamConsumed = true ; } catch ( IOException ioe ) { throw new FacebookException ( ioe . getMessage ( ) , ioe ) ; } finally { code_block = IfStatement ; code_block = IfStatement ; disconnectForcibly ( ) ; } }
public void test() { try { exists = BackendConnector . getInstance ( ) . isAppExisting ( CoreConfiguration . buildGoalContextFromGlobalConfiguration ( ) , _app ) ; } catch ( BackendConnectionException e ) { log . error ( "Error while checking whether " + _app + " exists in backend: " + e . getMessage ( ) ) ; } }
public void test() { if ( lr . getStateProvince ( ) != null && stateProvinceCentrePoints . coordinatesMatchCentre ( lr . getStateProvince ( ) , lr . getDecimalLatitude ( ) , lr . getDecimalLongitude ( ) ) ) { addIssue ( lr , ALAOccurrenceIssue . COORDINATES_CENTRE_OF_STATEPROVINCE . name ( ) ) ; } else-if ( log . isTraceEnabled ( ) ) { log . trace ( "{},{} is not at the centre of {}!" , lr . getDecimalLatitude ( ) , lr . getDecimalLongitude ( ) , lr . getStateProvince ( ) ) ; } }
@ Override public BasisFunctionFactory getFactory ( final DataTableSpec spec ) { LOGGER . debug ( "fuzzy_norm   : " + Norm . NORMS [ m_norm ] ) ; LOGGER . debug ( "shrink       : " + Shrink . SHRINKS [ m_shrink ] ) ; return new FuzzyBasisFunctionFactory ( m_norm , m_shrink , spec , getTargetColumns ( ) , getDistance ( ) ) ; }
@ Override public BasisFunctionFactory getFactory ( final DataTableSpec spec ) { LOGGER . debug ( "fuzzy_norm   : " + Norm . NORMS [ m_norm ] ) ; LOGGER . debug ( "shrink       : " + Shrink . SHRINKS [ m_shrink ] ) ; return new FuzzyBasisFunctionFactory ( m_norm , m_shrink , spec , getTargetColumns ( ) , getDistance ( ) ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Subscribed to: " + pattern ) ; } }
public void test() { try { LOG . info ( "===========================testTaskMatrixClock===============================" ) ; AngelApplicationMaster angelAppMaster = LocalClusterContext . get ( ) . getMaster ( ) . getAppMaster ( ) ; assertTrue ( angelAppMaster != null ) ; AMTaskManager taskManager = angelAppMaster . getAppContext ( ) . getTaskManager ( ) ; Worker worker = LocalClusterContext . get ( ) . getWorker ( worker0Attempt0Id ) . getWorker ( ) ; PSAgentMatrixMetaManager matrixMetaManager = worker . getPSAgent ( ) . getMatrixMetaManager ( ) ; int w1Id = matrixMetaManager . getMatrixId ( "w1" ) ; int w2Id = matrixMetaManager . getMatrixId ( "w2" ) ; MasterClient masterClient = worker . getPSAgent ( ) . getMasterClient ( ) ; AMTask task0 = taskManager . getTask ( task0Id ) ; AMTask task1 = taskManager . getTask ( task1Id ) ; masterClient . updateClock ( task0Id . getIndex ( ) , w1Id , 1 ) ; masterClient . updateClock ( task0Id . getIndex ( ) , w2Id , 1 ) ; Int2IntOpenHashMap matrixClocks = task0 . getMatrixClocks ( ) ; assertEquals ( matrixClocks . size ( ) , 2 ) ; assertEquals ( matrixClocks . get ( w1Id ) , 1 ) ; assertEquals ( matrixClocks . get ( w2Id ) , 1 ) ; masterClient . updateClock ( task0Id . getIndex ( ) , w1Id , 2 ) ; assertEquals ( task0 . getMatrixClock ( w1Id ) , 2 ) ; assertEquals ( task0 . getMatrixClock ( w2Id ) , 1 ) ; masterClient . updateClock ( task1Id . getIndex ( ) , w1Id , 1 ) ; masterClient . updateClock ( task1Id . getIndex ( ) , w2Id , 1 ) ; matrixClocks = task1 . getMatrixClocks ( ) ; assertEquals ( matrixClocks . size ( ) , 2 ) ; assertEquals ( matrixClocks . get ( w1Id ) , 1 ) ; assertEquals ( matrixClocks . get ( w2Id ) , 1 ) ; masterClient . updateClock ( task1Id . getIndex ( ) , w1Id , 2 ) ; assertEquals ( task1 . getMatrixClock ( w1Id ) , 2 ) ; assertEquals ( task1 . getMatrixClock ( w2Id ) , 1 ) ; } catch ( Exception x ) { LOG . error ( "run testTaskMatrixClock failed " , x ) ; throw x ; } }
public void test() { try { LOG . info ( "===========================testTaskMatrixClock===============================" ) ; AngelApplicationMaster angelAppMaster = LocalClusterContext . get ( ) . getMaster ( ) . getAppMaster ( ) ; assertTrue ( angelAppMaster != null ) ; AMTaskManager taskManager = angelAppMaster . getAppContext ( ) . getTaskManager ( ) ; Worker worker = LocalClusterContext . get ( ) . getWorker ( worker0Attempt0Id ) . getWorker ( ) ; PSAgentMatrixMetaManager matrixMetaManager = worker . getPSAgent ( ) . getMatrixMetaManager ( ) ; int w1Id = matrixMetaManager . getMatrixId ( "w1" ) ; int w2Id = matrixMetaManager . getMatrixId ( "w2" ) ; MasterClient masterClient = worker . getPSAgent ( ) . getMasterClient ( ) ; AMTask task0 = taskManager . getTask ( task0Id ) ; AMTask task1 = taskManager . getTask ( task1Id ) ; masterClient . updateClock ( task0Id . getIndex ( ) , w1Id , 1 ) ; masterClient . updateClock ( task0Id . getIndex ( ) , w2Id , 1 ) ; Int2IntOpenHashMap matrixClocks = task0 . getMatrixClocks ( ) ; assertEquals ( matrixClocks . size ( ) , 2 ) ; assertEquals ( matrixClocks . get ( w1Id ) , 1 ) ; assertEquals ( matrixClocks . get ( w2Id ) , 1 ) ; masterClient . updateClock ( task0Id . getIndex ( ) , w1Id , 2 ) ; assertEquals ( task0 . getMatrixClock ( w1Id ) , 2 ) ; assertEquals ( task0 . getMatrixClock ( w2Id ) , 1 ) ; masterClient . updateClock ( task1Id . getIndex ( ) , w1Id , 1 ) ; masterClient . updateClock ( task1Id . getIndex ( ) , w2Id , 1 ) ; matrixClocks = task1 . getMatrixClocks ( ) ; assertEquals ( matrixClocks . size ( ) , 2 ) ; assertEquals ( matrixClocks . get ( w1Id ) , 1 ) ; assertEquals ( matrixClocks . get ( w2Id ) , 1 ) ; masterClient . updateClock ( task1Id . getIndex ( ) , w1Id , 2 ) ; assertEquals ( task1 . getMatrixClock ( w1Id ) , 2 ) ; assertEquals ( task1 . getMatrixClock ( w2Id ) , 1 ) ; } catch ( Exception x ) { LOG . error ( "run testTaskMatrixClock failed " , x ) ; throw x ; } }
private void testLoginFromConfig ( ) { log . debug ( "Starting testLoginFromConfig." ) ; if ( ccm . loginFromConfig ( ) == null ) throw new AssertionError ( "loginFromConfig returned null." ) ; if ( ! ccm . logout ( ) ) throw new AssertionError ( "logout returned false." ) ; log . debug ( "testLoginFromConfig success." ) ; }
private void testLoginFromConfig ( ) { log . debug ( "Starting testLoginFromConfig." ) ; if ( ccm . loginFromConfig ( ) == null ) throw new AssertionError ( "loginFromConfig returned null." ) ; if ( ! ccm . logout ( ) ) throw new AssertionError ( "logout returned false." ) ; log . debug ( "testLoginFromConfig success." ) ; }
public void test() { try { MochawesomeSpecRunReport specRunReport = objectMapper . readValue ( path . toFile ( ) , MochawesomeSpecRunReport . class ) ; specRunReport . fillInTestResults ( results ) ; } catch ( JsonMappingException e ) { LOG . warn ( "No test results were found in the report file:" + " " + path ) ; } }
public void test() { if ( baudRate == 9600 ) { serial . write ( new byte [ ] code_block = "" ; ) ; } else-if ( baudRate == 19200 ) { serial . write ( new byte [ ] code_block = "" ; ) ; } else-if ( baudRate == 38400 ) { serial . write ( new byte [ ] code_block = "" ; ) ; } else { log . error ( "You've specified an unsupported baud rate" ) ; } }
public void test() { try { commandOptional = service . applianceCommand ( appliance ) ; } catch ( IOException e ) { logger . debug ( "Could not get appliance command" , e ) ; return null ; } }
@ Override public void updateUserInfo ( final String userName , final String userInfo ) throws DataNotFoundException , JargonException { log . info ( "updateUserInfo()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "userName:{}" , userName ) ; log . info ( "userInfo:{}" , userInfo ) ; User user = findByName ( userName ) ; log . info ( "looked up user:{}" , user ) ; user . setInfo ( userInfo ) ; this . updateUserInfo ( user ) ; log . info ( "updated info" ) ; }
@ Override public void updateUserInfo ( final String userName , final String userInfo ) throws DataNotFoundException , JargonException { log . info ( "updateUserInfo()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "userName:{}" , userName ) ; log . info ( "userInfo:{}" , userInfo ) ; User user = findByName ( userName ) ; log . info ( "looked up user:{}" , user ) ; user . setInfo ( userInfo ) ; this . updateUserInfo ( user ) ; log . info ( "updated info" ) ; }
@ Override public void updateUserInfo ( final String userName , final String userInfo ) throws DataNotFoundException , JargonException { log . info ( "updateUserInfo()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "userName:{}" , userName ) ; log . info ( "userInfo:{}" , userInfo ) ; User user = findByName ( userName ) ; log . info ( "looked up user:{}" , user ) ; user . setInfo ( userInfo ) ; this . updateUserInfo ( user ) ; log . info ( "updated info" ) ; }
@ Override public void updateUserInfo ( final String userName , final String userInfo ) throws DataNotFoundException , JargonException { log . info ( "updateUserInfo()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "userName:{}" , userName ) ; log . info ( "userInfo:{}" , userInfo ) ; User user = findByName ( userName ) ; log . info ( "looked up user:{}" , user ) ; user . setInfo ( userInfo ) ; this . updateUserInfo ( user ) ; log . info ( "updated info" ) ; }
@ Override public void updateUserInfo ( final String userName , final String userInfo ) throws DataNotFoundException , JargonException { log . info ( "updateUserInfo()" ) ; code_block = IfStatement ; code_block = IfStatement ; log . info ( "userName:{}" , userName ) ; log . info ( "userInfo:{}" , userInfo ) ; User user = findByName ( userName ) ; log . info ( "looked up user:{}" , user ) ; user . setInfo ( userInfo ) ; this . updateUserInfo ( user ) ; log . info ( "updated info" ) ; }
public void test() { if ( ! moduleName . matches ( "^[a-zA-Z_]*$" ) ) { logger . error ( "Invalid module name. Should contain only alphabets and underscore" ) ; return false ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( RedirectEntryServiceUtil . class , "deleteRedirectEntry" , _deleteRedirectEntryParameterTypes2 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , redirectEntryId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . redirect . model . RedirectEntry ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { lh = bkc . createLedger ( digestType , ledgerPassword ) ; ledgerId = lh . getId ( ) ; LOG . info ( "Ledger ID: " + lh . getId ( ) ) ; code_block = ForStatement ; synchronized ( sync ) code_block = "" ; long length = numEntriesToWrite * 4 ; assertTrue ( "Ledger length before closing: " + lh . getLength ( ) , lh . getLength ( ) == length ) ; LOG . debug ( "*** WRITE COMPLETE ***" ) ; lh . close ( ) ; lh = bkc . openLedger ( ledgerId , digestType , ledgerPassword ) ; assertTrue ( "Ledger length after opening: " + lh . getLength ( ) , lh . getLength ( ) == length ) ; lh . close ( ) ; } catch ( BKException e ) { LOG . error ( "Test failed" , e ) ; fail ( "Test failed due to BookKeeper exception" ) ; } catch ( InterruptedException e ) { LOG . error ( "Test failed" , e ) ; fail ( "Test failed due to interruption" ) ; } }
public void test() { try { lh = bkc . createLedger ( digestType , ledgerPassword ) ; ledgerId = lh . getId ( ) ; LOG . info ( "Ledger ID: " + lh . getId ( ) ) ; code_block = ForStatement ; synchronized ( sync ) code_block = "" ; long length = numEntriesToWrite * 4 ; assertTrue ( "Ledger length before closing: " + lh . getLength ( ) , lh . getLength ( ) == length ) ; LOG . debug ( "*** WRITE COMPLETE ***" ) ; lh . close ( ) ; lh = bkc . openLedger ( ledgerId , digestType , ledgerPassword ) ; assertTrue ( "Ledger length after opening: " + lh . getLength ( ) , lh . getLength ( ) == length ) ; lh . close ( ) ; } catch ( BKException e ) { LOG . error ( "Test failed" , e ) ; fail ( "Test failed due to BookKeeper exception" ) ; } catch ( InterruptedException e ) { LOG . error ( "Test failed" , e ) ; fail ( "Test failed due to interruption" ) ; } }
public void test() { if ( ! scheduler . isTerminated ( ) ) { LOG . error ( "Failed to terminate periodic progress reporting in 1 minute. " + "Waiting for it to terminate indefinitely..." ) ; scheduler . awaitTermination ( Long . MAX_VALUE , TimeUnit . SECONDS ) ; LOG . info ( "Periodic progress reporting terminated." ) ; } }
public void test() { if ( progressUpdatePeriodMs == UPDATE_ON_EACH_ITERATION ) code_block = "" ; }
@ Override public void onDisconnect ( InternalDistributedSystem sys ) { logger . debug ( "Calling AdminDistributedSystemImplonDisconnect" ) ; disconnect ( ) ; logger . debug ( "Completed AdminDistributedSystemImplonDisconnect" ) ; }
@ Override public void onDisconnect ( InternalDistributedSystem sys ) { logger . debug ( "Calling AdminDistributedSystemImplonDisconnect" ) ; disconnect ( ) ; logger . debug ( "Completed AdminDistributedSystemImplonDisconnect" ) ; }
public void test() { try ( LoggingContext loggingContext = LoggingContext . forConnector ( connName ) ) { WorkerConnector workerConnector = connectors . get ( connName ) ; log . info ( "Stopping connector {}" , connName ) ; code_block = IfStatement ; ClassLoader savedLoader = plugins . currentThreadLoader ( ) ; code_block = TryStatement ;  } }
public void test() { if ( workerConnector == null ) { log . warn ( "Ignoring stop request for unowned connector {}" , connName ) ; return ; } }
public void test() { try { URL url = new URL ( current_uri . toString ( ) ) ; HttpsURLConnection urlConn = ( HttpsURLConnection ) url . openConnection ( ) ; urlConn . setRequestMethod ( "GET" ) ; urlConn . setDoOutput ( true ) ; urlConn . setRequestProperty ( REQUEST_PROPERTY_ACCEPT , REQUEST_VALUE_TEXT_JSON ) ; return fillMarketoRecordResultFromReader ( getReaderFromHttpResponse ( urlConn ) , schema ) ; } catch ( IOException e ) { LOG . error ( "Request failed: {}." , e . getMessage ( ) ) ; throw new MarketoException ( REST , e . getMessage ( ) ) ; } }
private void execInContainer ( String containerId , String execCommand , boolean logout ) throws DockerException , InterruptedException { LOGGER . info ( "exec container commmand: " + execCommand ) ; final String [ ] command = code_block = "" ; ; final ExecCreation execCreation = docker . execCreate ( containerId , command , DockerClient . ExecCreateParam . attachStdout ( ) , DockerClient . ExecCreateParam . attachStderr ( ) ) ; LogStream logStream = docker . execStart ( execCreation . id ( ) ) ; code_block = WhileStatement ; }
@ Test ( timeOut = 10_000 ) public void testCleanupWithDeleteRow ( ITestContext context ) throws Exception { TransactionManager tm = newTransactionManager ( context ) ; TTable tt = new TTable ( hbaseConf , TEST_TABLE ) ; Transaction t1 = tm . begin ( ) ; LOG . info ( "Transaction created " + t1 ) ; int rowcount = 10 ; int count = 0 ; byte [ ] fam = Bytes . toBytes ( TEST_FAMILY ) ; byte [ ] col = Bytes . toBytes ( "testdata" ) ; byte [ ] data1 = Bytes . toBytes ( "testWrite-1" ) ; byte [ ] data2 = Bytes . toBytes ( "testWrite-2" ) ; byte [ ] modrow = Bytes . toBytes ( "test-del" + 3 ) ; code_block = ForStatement ; tm . commit ( t1 ) ; Transaction t2 = tm . begin ( ) ; LOG . info ( "Transaction created " + t2 ) ; Delete d = new Delete ( modrow ) ; tt . delete ( t2 , d ) ; ResultScanner rs = tt . getScanner ( t2 , new Scan ( ) ) ; Result r = rs . next ( ) ; count = 0 ; code_block = WhileStatement ; assertEquals ( count , rowcount - 1 , "Wrong count" ) ; Transaction t3 = tm . begin ( ) ; LOG . info ( "Transaction created " + t3 ) ; Put p = new Put ( modrow ) ; p . add ( fam , col , data2 ) ; tt . put ( t3 , p ) ; tm . commit ( t3 ) ; boolean aborted = false ; code_block = TryStatement ;  assertTrue ( aborted , "Didn't raise exception" ) ; Transaction tscan = tm . begin ( ) ; rs = tt . getScanner ( tscan , new Scan ( ) ) ; r = rs . next ( ) ; count = 0 ; code_block = WhileStatement ; assertEquals ( count , rowcount , "Wrong count" ) ; }
@ Test ( timeOut = 10_000 ) public void testCleanupWithDeleteRow ( ITestContext context ) throws Exception { TransactionManager tm = newTransactionManager ( context ) ; TTable tt = new TTable ( hbaseConf , TEST_TABLE ) ; Transaction t1 = tm . begin ( ) ; LOG . info ( "Transaction created " + t1 ) ; int rowcount = 10 ; int count = 0 ; byte [ ] fam = Bytes . toBytes ( TEST_FAMILY ) ; byte [ ] col = Bytes . toBytes ( "testdata" ) ; byte [ ] data1 = Bytes . toBytes ( "testWrite-1" ) ; byte [ ] data2 = Bytes . toBytes ( "testWrite-2" ) ; byte [ ] modrow = Bytes . toBytes ( "test-del" + 3 ) ; code_block = ForStatement ; tm . commit ( t1 ) ; Transaction t2 = tm . begin ( ) ; LOG . info ( "Transaction created " + t2 ) ; Delete d = new Delete ( modrow ) ; tt . delete ( t2 , d ) ; ResultScanner rs = tt . getScanner ( t2 , new Scan ( ) ) ; Result r = rs . next ( ) ; count = 0 ; code_block = WhileStatement ; assertEquals ( count , rowcount - 1 , "Wrong count" ) ; Transaction t3 = tm . begin ( ) ; LOG . info ( "Transaction created " + t3 ) ; Put p = new Put ( modrow ) ; p . add ( fam , col , data2 ) ; tt . put ( t3 , p ) ; tm . commit ( t3 ) ; boolean aborted = false ; code_block = TryStatement ;  assertTrue ( aborted , "Didn't raise exception" ) ; Transaction tscan = tm . begin ( ) ; rs = tt . getScanner ( tscan , new Scan ( ) ) ; r = rs . next ( ) ; count = 0 ; code_block = WhileStatement ; assertEquals ( count , rowcount , "Wrong count" ) ; }
@ Test ( timeOut = 10_000 ) public void testCleanupWithDeleteRow ( ITestContext context ) throws Exception { TransactionManager tm = newTransactionManager ( context ) ; TTable tt = new TTable ( hbaseConf , TEST_TABLE ) ; Transaction t1 = tm . begin ( ) ; LOG . info ( "Transaction created " + t1 ) ; int rowcount = 10 ; int count = 0 ; byte [ ] fam = Bytes . toBytes ( TEST_FAMILY ) ; byte [ ] col = Bytes . toBytes ( "testdata" ) ; byte [ ] data1 = Bytes . toBytes ( "testWrite-1" ) ; byte [ ] data2 = Bytes . toBytes ( "testWrite-2" ) ; byte [ ] modrow = Bytes . toBytes ( "test-del" + 3 ) ; code_block = ForStatement ; tm . commit ( t1 ) ; Transaction t2 = tm . begin ( ) ; LOG . info ( "Transaction created " + t2 ) ; Delete d = new Delete ( modrow ) ; tt . delete ( t2 , d ) ; ResultScanner rs = tt . getScanner ( t2 , new Scan ( ) ) ; Result r = rs . next ( ) ; count = 0 ; code_block = WhileStatement ; assertEquals ( count , rowcount - 1 , "Wrong count" ) ; Transaction t3 = tm . begin ( ) ; LOG . info ( "Transaction created " + t3 ) ; Put p = new Put ( modrow ) ; p . add ( fam , col , data2 ) ; tt . put ( t3 , p ) ; tm . commit ( t3 ) ; boolean aborted = false ; code_block = TryStatement ;  assertTrue ( aborted , "Didn't raise exception" ) ; Transaction tscan = tm . begin ( ) ; rs = tt . getScanner ( tscan , new Scan ( ) ) ; r = rs . next ( ) ; count = 0 ; code_block = WhileStatement ; assertEquals ( count , rowcount , "Wrong count" ) ; }
@ Override public void run ( ) throws Exception { log . info ( "Server: {}" , getOpts ( ) . getUrl ( ) ) ; log . info ( "Username: {}" , getOpts ( ) . getUsername ( ) ) ; log . info ( "Source language: {}" , DEFAULT_SOURCE_LANG ) ; log . info ( "Translation language: {}" , getOpts ( ) . getTransLang ( ) ) ; code_block = IfStatement ; log . info ( "Glossary file: {}" , getOpts ( ) . getFile ( ) ) ; log . info ( "Batch size: {}" , getOpts ( ) . getBatchSize ( ) ) ; File glossaryFile = getOpts ( ) . getFile ( ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; String fileExtension = validateFileExtensionWithTransLang ( ) ; String project = getOpts ( ) . getProject ( ) ; String qualifiedName ; code_block = TryStatement ;  AbstractGlossaryPushReader reader = getReader ( fileExtension ) ; log . info ( "Pushing glossary document [{}] to server" , glossaryFile . getName ( ) ) ; Reader inputStreamReader = new InputStreamReader ( new FileInputStream ( glossaryFile ) , "UTF-8" ) ; BufferedReader br = new BufferedReader ( inputStreamReader ) ; Map < LocaleId , List < GlossaryEntry > > glossaries = reader . extractGlossary ( br , qualifiedName ) ; int totalEntries = 0 ; code_block = ForStatement ; int totalDone = 0 ; code_block = ForStatement ; }
@ Override public void run ( ) throws Exception { log . info ( "Server: {}" , getOpts ( ) . getUrl ( ) ) ; log . info ( "Username: {}" , getOpts ( ) . getUsername ( ) ) ; log . info ( "Source language: {}" , DEFAULT_SOURCE_LANG ) ; log . info ( "Translation language: {}" , getOpts ( ) . getTransLang ( ) ) ; code_block = IfStatement ; log . info ( "Glossary file: {}" , getOpts ( ) . getFile ( ) ) ; log . info ( "Batch size: {}" , getOpts ( ) . getBatchSize ( ) ) ; File glossaryFile = getOpts ( ) . getFile ( ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; String fileExtension = validateFileExtensionWithTransLang ( ) ; String project = getOpts ( ) . getProject ( ) ; String qualifiedName ; code_block = TryStatement ;  AbstractGlossaryPushReader reader = getReader ( fileExtension ) ; log . info ( "Pushing glossary document [{}] to server" , glossaryFile . getName ( ) ) ; Reader inputStreamReader = new InputStreamReader ( new FileInputStream ( glossaryFile ) , "UTF-8" ) ; BufferedReader br = new BufferedReader ( inputStreamReader ) ; Map < LocaleId , List < GlossaryEntry > > glossaries = reader . extractGlossary ( br , qualifiedName ) ; int totalEntries = 0 ; code_block = ForStatement ; int totalDone = 0 ; code_block = ForStatement ; }
@ Override public void run ( ) throws Exception { log . info ( "Server: {}" , getOpts ( ) . getUrl ( ) ) ; log . info ( "Username: {}" , getOpts ( ) . getUsername ( ) ) ; log . info ( "Source language: {}" , DEFAULT_SOURCE_LANG ) ; log . info ( "Translation language: {}" , getOpts ( ) . getTransLang ( ) ) ; code_block = IfStatement ; log . info ( "Glossary file: {}" , getOpts ( ) . getFile ( ) ) ; log . info ( "Batch size: {}" , getOpts ( ) . getBatchSize ( ) ) ; File glossaryFile = getOpts ( ) . getFile ( ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; String fileExtension = validateFileExtensionWithTransLang ( ) ; String project = getOpts ( ) . getProject ( ) ; String qualifiedName ; code_block = TryStatement ;  AbstractGlossaryPushReader reader = getReader ( fileExtension ) ; log . info ( "Pushing glossary document [{}] to server" , glossaryFile . getName ( ) ) ; Reader inputStreamReader = new InputStreamReader ( new FileInputStream ( glossaryFile ) , "UTF-8" ) ; BufferedReader br = new BufferedReader ( inputStreamReader ) ; Map < LocaleId , List < GlossaryEntry > > glossaries = reader . extractGlossary ( br , qualifiedName ) ; int totalEntries = 0 ; code_block = ForStatement ; int totalDone = 0 ; code_block = ForStatement ; }
@ Override public void run ( ) throws Exception { log . info ( "Server: {}" , getOpts ( ) . getUrl ( ) ) ; log . info ( "Username: {}" , getOpts ( ) . getUsername ( ) ) ; log . info ( "Source language: {}" , DEFAULT_SOURCE_LANG ) ; log . info ( "Translation language: {}" , getOpts ( ) . getTransLang ( ) ) ; code_block = IfStatement ; log . info ( "Glossary file: {}" , getOpts ( ) . getFile ( ) ) ; log . info ( "Batch size: {}" , getOpts ( ) . getBatchSize ( ) ) ; File glossaryFile = getOpts ( ) . getFile ( ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; String fileExtension = validateFileExtensionWithTransLang ( ) ; String project = getOpts ( ) . getProject ( ) ; String qualifiedName ; code_block = TryStatement ;  AbstractGlossaryPushReader reader = getReader ( fileExtension ) ; log . info ( "Pushing glossary document [{}] to server" , glossaryFile . getName ( ) ) ; Reader inputStreamReader = new InputStreamReader ( new FileInputStream ( glossaryFile ) , "UTF-8" ) ; BufferedReader br = new BufferedReader ( inputStreamReader ) ; Map < LocaleId , List < GlossaryEntry > > glossaries = reader . extractGlossary ( br , qualifiedName ) ; int totalEntries = 0 ; code_block = ForStatement ; int totalDone = 0 ; code_block = ForStatement ; }
public void test() { if ( StringUtils . isNotBlank ( getOpts ( ) . getProject ( ) ) ) { log . info ( "Project: {}" , getOpts ( ) . getProject ( ) ) ; } }
@ Override public void run ( ) throws Exception { log . info ( "Server: {}" , getOpts ( ) . getUrl ( ) ) ; log . info ( "Username: {}" , getOpts ( ) . getUsername ( ) ) ; log . info ( "Source language: {}" , DEFAULT_SOURCE_LANG ) ; log . info ( "Translation language: {}" , getOpts ( ) . getTransLang ( ) ) ; code_block = IfStatement ; log . info ( "Glossary file: {}" , getOpts ( ) . getFile ( ) ) ; log . info ( "Batch size: {}" , getOpts ( ) . getBatchSize ( ) ) ; File glossaryFile = getOpts ( ) . getFile ( ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; String fileExtension = validateFileExtensionWithTransLang ( ) ; String project = getOpts ( ) . getProject ( ) ; String qualifiedName ; code_block = TryStatement ;  AbstractGlossaryPushReader reader = getReader ( fileExtension ) ; log . info ( "Pushing glossary document [{}] to server" , glossaryFile . getName ( ) ) ; Reader inputStreamReader = new InputStreamReader ( new FileInputStream ( glossaryFile ) , "UTF-8" ) ; BufferedReader br = new BufferedReader ( inputStreamReader ) ; Map < LocaleId , List < GlossaryEntry > > glossaries = reader . extractGlossary ( br , qualifiedName ) ; int totalEntries = 0 ; code_block = ForStatement ; int totalDone = 0 ; code_block = ForStatement ; }
@ Override public void run ( ) throws Exception { log . info ( "Server: {}" , getOpts ( ) . getUrl ( ) ) ; log . info ( "Username: {}" , getOpts ( ) . getUsername ( ) ) ; log . info ( "Source language: {}" , DEFAULT_SOURCE_LANG ) ; log . info ( "Translation language: {}" , getOpts ( ) . getTransLang ( ) ) ; code_block = IfStatement ; log . info ( "Glossary file: {}" , getOpts ( ) . getFile ( ) ) ; log . info ( "Batch size: {}" , getOpts ( ) . getBatchSize ( ) ) ; File glossaryFile = getOpts ( ) . getFile ( ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; String fileExtension = validateFileExtensionWithTransLang ( ) ; String project = getOpts ( ) . getProject ( ) ; String qualifiedName ; code_block = TryStatement ;  AbstractGlossaryPushReader reader = getReader ( fileExtension ) ; log . info ( "Pushing glossary document [{}] to server" , glossaryFile . getName ( ) ) ; Reader inputStreamReader = new InputStreamReader ( new FileInputStream ( glossaryFile ) , "UTF-8" ) ; BufferedReader br = new BufferedReader ( inputStreamReader ) ; Map < LocaleId , List < GlossaryEntry > > glossaries = reader . extractGlossary ( br , qualifiedName ) ; int totalEntries = 0 ; code_block = ForStatement ; int totalDone = 0 ; code_block = ForStatement ; }
public void test() { if ( rpe . getResponse ( ) . getStatus ( ) == 404 ) { log . error ( "Project {} not found" , project ) ; return ; } else { throw rpe ; } }
@ Override public void run ( ) throws Exception { log . info ( "Server: {}" , getOpts ( ) . getUrl ( ) ) ; log . info ( "Username: {}" , getOpts ( ) . getUsername ( ) ) ; log . info ( "Source language: {}" , DEFAULT_SOURCE_LANG ) ; log . info ( "Translation language: {}" , getOpts ( ) . getTransLang ( ) ) ; code_block = IfStatement ; log . info ( "Glossary file: {}" , getOpts ( ) . getFile ( ) ) ; log . info ( "Batch size: {}" , getOpts ( ) . getBatchSize ( ) ) ; File glossaryFile = getOpts ( ) . getFile ( ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; String fileExtension = validateFileExtensionWithTransLang ( ) ; String project = getOpts ( ) . getProject ( ) ; String qualifiedName ; code_block = TryStatement ;  AbstractGlossaryPushReader reader = getReader ( fileExtension ) ; log . info ( "Pushing glossary document [{}] to server" , glossaryFile . getName ( ) ) ; Reader inputStreamReader = new InputStreamReader ( new FileInputStream ( glossaryFile ) , "UTF-8" ) ; BufferedReader br = new BufferedReader ( inputStreamReader ) ; Map < LocaleId , List < GlossaryEntry > > glossaries = reader . extractGlossary ( br , qualifiedName ) ; int totalEntries = 0 ; code_block = ForStatement ; int totalDone = 0 ; code_block = ForStatement ; }
public void testReceiveTwoThenRollback ( ) throws Exception { Message [ ] outbound = new Message [ ] code_block = "" ; ; beginTx ( ) ; code_block = WhileStatement ; commitTx ( ) ; beginTx ( ) ; producer . send ( outbound [ 0 ] ) ; producer . send ( outbound [ 1 ] ) ; commitTx ( ) ; LOG . info ( "Sent 0: " + outbound [ 0 ] ) ; LOG . info ( "Sent 1: " + outbound [ 1 ] ) ; ArrayList < Message > messages = new ArrayList < > ( ) ; beginTx ( ) ; Message message = consumer . receive ( 1000 ) ; assertEquals ( outbound [ 0 ] , message ) ; message = consumer . receive ( 1000 ) ; assertNotNull ( message ) ; assertEquals ( outbound [ 1 ] , message ) ; rollbackTx ( ) ; beginTx ( ) ; message = consumer . receive ( 5000 ) ; assertNotNull ( "Should have re-received the first message again!" , message ) ; messages . add ( message ) ; assertEquals ( outbound [ 0 ] , message ) ; message = consumer . receive ( 5000 ) ; assertNotNull ( "Should have re-received the second message again!" , message ) ; messages . add ( message ) ; assertEquals ( outbound [ 1 ] , message ) ; assertNull ( consumer . receiveNoWait ( ) ) ; commitTx ( ) ; Message inbound [ ] = new Message [ messages . size ( ) ] ; messages . toArray ( inbound ) ; assertTextMessagesEqual ( "Rollback did not work" , outbound , inbound ) ; }
public void testReceiveTwoThenRollback ( ) throws Exception { Message [ ] outbound = new Message [ ] code_block = "" ; ; beginTx ( ) ; code_block = WhileStatement ; commitTx ( ) ; beginTx ( ) ; producer . send ( outbound [ 0 ] ) ; producer . send ( outbound [ 1 ] ) ; commitTx ( ) ; LOG . info ( "Sent 0: " + outbound [ 0 ] ) ; LOG . info ( "Sent 1: " + outbound [ 1 ] ) ; ArrayList < Message > messages = new ArrayList < > ( ) ; beginTx ( ) ; Message message = consumer . receive ( 1000 ) ; assertEquals ( outbound [ 0 ] , message ) ; message = consumer . receive ( 1000 ) ; assertNotNull ( message ) ; assertEquals ( outbound [ 1 ] , message ) ; rollbackTx ( ) ; beginTx ( ) ; message = consumer . receive ( 5000 ) ; assertNotNull ( "Should have re-received the first message again!" , message ) ; messages . add ( message ) ; assertEquals ( outbound [ 0 ] , message ) ; message = consumer . receive ( 5000 ) ; assertNotNull ( "Should have re-received the second message again!" , message ) ; messages . add ( message ) ; assertEquals ( outbound [ 1 ] , message ) ; assertNull ( consumer . receiveNoWait ( ) ) ; commitTx ( ) ; Message inbound [ ] = new Message [ messages . size ( ) ] ; messages . toArray ( inbound ) ; assertTextMessagesEqual ( "Rollback did not work" , outbound , inbound ) ; }
public void test() { try { MAX_MEM_USAGE = Integer . parseInt ( limit ) ; } catch ( NumberFormatException nfe ) { log . error ( "Exception reading property " + CHUNK_QUEUE_LIMIT + ". Defaulting internal queue size to " + QUEUE_SIZE ) ; } }
public void test() { try { List < Integer > ruleIdList = validAlertRuleList . stream ( ) . map ( AlertRule :: getRuleId ) . collect ( Collectors . toList ( ) ) ; alertRuleService . updateAlertRuleLastCheckTime ( ruleIdList ) ; } catch ( Exception e ) { logger . error ( "Update alert rule last check time, " + validAlertRuleList , e ) ; } }
public void test() { if ( result . getHarEntry ( ) != null && result . getHarEntry ( ) . getResponse ( ) != null ) { HarResponse response = result . getHarEntry ( ) . getResponse ( ) ; LOG . info ( "TestStep [" + result . getTestStepName ( ) + "] response: " + response . getStatus ( ) + " - " + response . getContent ( ) . getText ( ) ) ; } else { LOG . debug ( "Missing HAR response for TestStep [" + result . getTestStepName ( ) + "]" ) ; } }
public void test() { if ( result . getHarEntry ( ) != null && result . getHarEntry ( ) . getResponse ( ) != null ) { HarResponse response = result . getHarEntry ( ) . getResponse ( ) ; LOG . info ( "TestStep [" + result . getTestStepName ( ) + "] response: " + response . getStatus ( ) + " - " + response . getContent ( ) . getText ( ) ) ; } else { LOG . debug ( "Missing HAR response for TestStep [" + result . getTestStepName ( ) + "]" ) ; } }
@ Override public void fireDisconnected ( ) { logger . trace ( "Firing Disconnected!" ) ; channelHandlerContext . pipeline ( ) . fireUserEventTriggered ( new DisconnectedEvent ( ) ) ; }
public void test() { if ( StringUtils . isEmpty ( siteName ) && logger . isDebugEnabled ( ) ) { logger . debug ( "No '" + paramOrCookieName + "' request param or cookie found" ) ; } }
public void test() { try { verifyProperties ( properties ) ; } catch ( IOException e ) { logger . error ( "Failed to initialize properties file. Verification failed." ) ; StartupListener . properties = null ; } }
public void test() { if ( properties != null ) { code_block = TryStatement ;  StartupListener . properties = properties ; } else { logger . error ( "Properties file path is missing." ) ; } }
public void test() { if ( LOGGER . isTraceEnabled ( ) ) { LOGGER . trace ( format ( "Create typed query for SELECT : %s" , regularStatement . getQueryString ( ) ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( pi ) ; } }
public void test() { if ( packageParser != null ) { PackageInfo pi = new PackageInfo ( ) ; pi . prefix = packageParser . getPackageName ( ) ; pi . namespace = namespace ; pi . version = extractPackageVersion ( namespace ) ; prefixMap . put ( pi . prefix , pi ) ; namespaceMap . put ( namespace , pi ) ; code_block = IfStatement ; } else { logger . warn ( "Package namespace unknow: '" + namespace + "'" ) ; } }
public void test() { try { Tracker t = new Tracker ( portValue ) ; File parent = new File ( directory ) ; code_block = ForStatement ; logger . info ( "Starting tracker with {} announced torrents..." , t . getTrackedTorrents ( ) . size ( ) ) ; t . start ( true ) ; } catch ( Exception e ) { logger . error ( "{}" , e . getMessage ( ) , e ) ; System . exit ( 2 ) ; } }
public void test() { try { PoxPayloadOut result = ConceptAuthorityClientUtils . createConceptInstance ( commonPartXML , commonPartName ) ; return result ; } catch ( DocumentException de ) { logger . error ( "Problem creating item from XML: " + de . getLocalizedMessage ( ) ) ; logger . debug ( "commonPartXML: " + commonPartXML ) ; } }
public void test() { try { PoxPayloadOut result = ConceptAuthorityClientUtils . createConceptInstance ( commonPartXML , commonPartName ) ; return result ; } catch ( DocumentException de ) { logger . error ( "Problem creating item from XML: " + de . getLocalizedMessage ( ) ) ; logger . debug ( "commonPartXML: " + commonPartXML ) ; } }
public void test() { if ( LOGGER . isLoggable ( Level . INFO ) ) { LOGGER . info ( "Specified tiling parameters are not valid. tileWidth = " + tileWidth + " tileHeight = " + tileHeight ) ; } }
public void test() { if ( LOGGER . isLoggable ( Level . INFO ) ) { LOGGER . info ( "Specified quality is not valid (it should be in the range [0,1])." + " compressionQuality = " + compressionQuality ) ; } }
public void test() { try ( SqlSession sqlSession = MyBatisUtil . getSqlSession ( ) ) { SysUserMapper userMapper = sqlSession . getMapper ( SysUserMapper . class ) ; userMapper . changePassword ( user ) ; sqlSession . commit ( ) ; } catch ( Exception e ) { LOG . error ( e . getMessage ( ) , e ) ; throw new Exception ( e ) ; } }
public void test() { try { task . go ( ) ; transaction . setStatus ( Transaction . SUCCESS ) ; } catch ( Throwable th ) { transaction . setStatus ( th ) ; logger . error ( "[logTransaction]" + type + "," + name + "," + task , th ) ; } finally { transaction . complete ( ) ; } }
public void test() { if ( e . getErrorType ( ) == READ_ERROR ) { log . error ( "Checksum read error during validation on {}" , checksumFile ) ; triggerConsumerError ( CHECKSUM_IO_ERROR , "Checksum I/O error during validation on " + checksumFile ) ; } else-if ( e . getErrorType ( ) == INVALID_FORMAT || e . getErrorType ( ) == DIGEST_ERROR ) { log . error ( "Digester failure during checksum validation on {}" , checksumFile ) ; triggerConsumerError ( CHECKSUM_DIGESTER_FAILURE , "Digester failure during checksum validation on " + checksumFile ) ; } else-if ( e . getErrorType ( ) == FILE_NOT_FOUND ) { log . error ( "File not found during checksum validation: " , e ) ; triggerConsumerError ( CHECKSUM_NOT_FOUND , "File not found during checksum validation: " + e . getMessage ( ) ) ; } }
public void test() { if ( e . getErrorType ( ) == READ_ERROR ) { log . error ( "Checksum read error during validation on {}" , checksumFile ) ; triggerConsumerError ( CHECKSUM_IO_ERROR , "Checksum I/O error during validation on " + checksumFile ) ; } else-if ( e . getErrorType ( ) == INVALID_FORMAT || e . getErrorType ( ) == DIGEST_ERROR ) { log . error ( "Digester failure during checksum validation on {}" , checksumFile ) ; triggerConsumerError ( CHECKSUM_DIGESTER_FAILURE , "Digester failure during checksum validation on " + checksumFile ) ; } else-if ( e . getErrorType ( ) == FILE_NOT_FOUND ) { log . error ( "File not found during checksum validation: " , e ) ; triggerConsumerError ( CHECKSUM_NOT_FOUND , "File not found during checksum validation: " + e . getMessage ( ) ) ; } }
public void test() { if ( e . getErrorType ( ) == READ_ERROR ) { log . error ( "Checksum read error during validation on {}" , checksumFile ) ; triggerConsumerError ( CHECKSUM_IO_ERROR , "Checksum I/O error during validation on " + checksumFile ) ; } else-if ( e . getErrorType ( ) == INVALID_FORMAT || e . getErrorType ( ) == DIGEST_ERROR ) { log . error ( "Digester failure during checksum validation on {}" , checksumFile ) ; triggerConsumerError ( CHECKSUM_DIGESTER_FAILURE , "Digester failure during checksum validation on " + checksumFile ) ; } else-if ( e . getErrorType ( ) == FILE_NOT_FOUND ) { log . error ( "File not found during checksum validation: " , e ) ; triggerConsumerError ( CHECKSUM_NOT_FOUND , "File not found during checksum validation: " + e . getMessage ( ) ) ; } }
public void test() { try { getSubscriptionManager ( ) . unsubscribe ( mailboxSession , mailboxName ) ; unsolicitedResponses ( session , responder , false ) ; okComplete ( request , responder ) ; } catch ( SubscriptionException e ) { LOGGER . info ( "Unsubscribe failed for mailbox {}" , mailboxName , e ) ; unsolicitedResponses ( session , responder , false ) ; no ( request , responder , HumanReadableText . GENERIC_SUBSCRIPTION_FAILURE ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( ResourceException e ) { logger . warn ( "Couldn't delete " + resource . getLocation ( ) + ". " + e . getMessage ( ) ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( ResourceException e ) { logger . warn ( "Couldn't delete " + iFile . getLocation ( ) + ". " + e . getMessage ( ) ) ; } }
private void insertRemoteToDb ( String siteId , String remoteName , String remoteUrl , String authenticationType , String remoteUsername , String remotePassword , String remoteToken , String remotePrivateKey ) throws CryptoException { logger . debug ( "Inserting remote " + remoteName + " for site " + siteId + " into database." ) ; Map < String , String > params = new HashMap < String , String > ( ) ; params . put ( "siteId" , siteId ) ; params . put ( "remoteName" , remoteName ) ; params . put ( "remoteUrl" , remoteUrl ) ; params . put ( "authenticationType" , authenticationType ) ; params . put ( "remoteUsername" , remoteUsername ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; logger . debug ( "Insert site remote record into database" ) ; remoteRepositoryDAO . insertRemoteRepository ( params ) ; params = new HashMap < String , String > ( ) ; params . put ( "siteId" , siteId ) ; params . put ( "remoteName" , remoteName ) ; RemoteRepository remoteRepository = remoteRepositoryDAO . getRemoteRepository ( params ) ; code_block = IfStatement ; }
public boolean flowUpdatePreStatusNone ( String networkId , BasicFlow flow ) { log . debug ( "" ) ; List < String > fedFlowIds = conversionTable . getFlow ( networkId , flow . getFlowId ( ) ) ; code_block = IfStatement ; String [ ] fedFlowId = fedFlowIds . get ( 0 ) . split ( "::" ) ; List < String > fedOrgIds = conversionTable . getFlow ( fedFlowId [ 0 ] , fedFlowId [ 1 ] ) ; code_block = ForStatement ; log . debug ( "next federate stauts:: none" ) ; return true ; }
public void test() { if ( fedFlowIds == null || fedFlowIds . size ( ) == 0 ) { log . warn ( "no Federator Flows in conversionTable: {}::{}" , networkId , flow . getFlowId ( ) ) ; return false ; } }
public void test() { if ( orgFlow == null ) { log . warn ( "not found Original Flow: {}" , flowId ) ; continue ; } }
public void test() { if ( ! FlowObject . FlowStatus . NONE . toString ( ) . equalsIgnoreCase ( orgFlow . getStatus ( ) ) ) { log . debug ( "not flow's status none." ) ; return false ; } }
public boolean flowUpdatePreStatusNone ( String networkId , BasicFlow flow ) { log . debug ( "" ) ; List < String > fedFlowIds = conversionTable . getFlow ( networkId , flow . getFlowId ( ) ) ; code_block = IfStatement ; String [ ] fedFlowId = fedFlowIds . get ( 0 ) . split ( "::" ) ; List < String > fedOrgIds = conversionTable . getFlow ( fedFlowId [ 0 ] , fedFlowId [ 1 ] ) ; code_block = ForStatement ; log . debug ( "next federate stauts:: none" ) ; return true ; }
public void waitForScanFinishing ( ) { logger . debug ( "Waiting for finishing Homematic device discovery scan" ) ; code_block = TryStatement ;  String gatewayId = bridgeHandler != null && bridgeHandler . getGateway ( ) != null ? bridgeHandler . getGateway ( ) . getId ( ) : "UNKNOWN" ; logger . debug ( "Finished Homematic device discovery scan on gateway '{}'" , gatewayId ) ; }
public void test() { try { waitForInstallModeFinished ( DISCOVER_TIMEOUT_SECONDS * 1000 ) ; waitForLoadDevicesFinished ( ) ; } catch ( ExecutionException | InterruptedException ex ) { } catch ( Exception ex ) { logger . error ( "Error waiting for device discovery scan: {}" , ex . getMessage ( ) , ex ) ; } }
public void waitForScanFinishing ( ) { logger . debug ( "Waiting for finishing Homematic device discovery scan" ) ; code_block = TryStatement ;  String gatewayId = bridgeHandler != null && bridgeHandler . getGateway ( ) != null ? bridgeHandler . getGateway ( ) . getId ( ) : "UNKNOWN" ; logger . debug ( "Finished Homematic device discovery scan on gateway '{}'" , gatewayId ) ; }
public static DataSource initDataSource ( Config config ) throws Exception { DruidDataSource dataSource = new DruidDataSource ( ) ; dataSource . setDriverClassName ( "com.mysql.cj.jdbc.Driver" ) ; dataSource . setUrl ( "jdbc:mysql://" + config . getDbUrl ( ) + ":" + config . getDbPort ( ) + "?useSSL=true&verifyServerCertificate=false&serverTimezone=GMT%2B8&characterEncoding=utf8" ) ; dataSource . setUsername ( config . getDbUsername ( ) ) ; dataSource . setPassword ( config . getDbPassword ( ) ) ; dataSource . setInitialSize ( 1 ) ; dataSource . setMaxActive ( 2 ) ; dataSource . setMaxWait ( 60000 ) ; dataSource . setTimeBetweenEvictionRunsMillis ( 60000 ) ; dataSource . setConnectionErrorRetryAttempts ( 2 ) ; dataSource . setBreakAfterAcquireFailure ( true ) ; dataSource . setMinEvictableIdleTimeMillis ( 300000 ) ; dataSource . setValidationQuery ( "SELECT 1 FROM DUAL" ) ; dataSource . setTestWhileIdle ( true ) ; log . info ( "init data source success" ) ; return dataSource ; }
@ GetMapping ( path = "/testvoidInRPC" ) public void testvoidInRPC ( ) { LOGGER . info ( "testvoidInRPC() is called!" ) ; testvoidInRPCSuccess = true ; }
public void test() { try { final String [ ] wNames = StringUtils . split ( widgetNames , "," ) ; dashboardsService . setWidgetsOrder ( guestId , dashboardId , wNames ) ; return getDashboards ( ) ; } catch ( Exception e ) { StringBuilder sb = new StringBuilder ( "module=API component=dashboardStore action=setWidgetOrder" ) . append ( " guestId=" ) . append ( guestId ) . append ( " stackTrace=<![CDATA[" ) . append ( Utils . stackTrace ( e ) ) . append ( "]]>" ) ; logger . warn ( sb . toString ( ) ) ; return Response . serverError ( ) . entity ( "Failed to set widget order: " + e . getMessage ( ) ) . build ( ) ; } }
private RestResponse < User > create ( ) throws ClientException { logger . debug ( "Creating user..." ) ; UserCommandOptions . CreateCommandOptions c = usersCommandOptions . createCommandOptions ; UserCreateParams createParams = new UserCreateParams ( ) . setId ( c . user ) . setName ( c . name ) . setEmail ( c . email ) . setOrganization ( c . organization ) . setPassword ( c . password ) ; return openCGAClient . getUserClient ( ) . create ( createParams ) ; }
@ Post ( subscribeToken = "csrf" ) public Boundary save ( ) { LOG . trace ( "access to save" ) ; String expires = getParam ( "expires" ) ; code_block = IfStatement ; Date date ; code_block = TryStatement ;  TokensEntity entity = TokensDao . get ( ) . selectOnUserId ( getLoginUserId ( ) ) ; code_block = IfStatement ; entity . setExpires ( new Timestamp ( date . getTime ( ) ) ) ; TokensDao . get ( ) . save ( entity ) ; addMsgSuccess ( "message.success.save" ) ; return index ( ) ; }
public void test() { try { ManagementFactory . getPlatformMBeanServer ( ) ; } catch ( Throwable t ) { logger . error ( "could not create platform mbean server: {}" , t . getMessage ( ) , t ) ; } }
public void test() { try ( DatagramSocket socket = IdentProtocol . sendRequest ( broadcastAddress ) ) { DatagramPacket incomingPacket ; code_block = WhileStatement ; } catch ( IOException e ) { logger . warn ( "Error sending broadcast: {}" , e . toString ( ) ) ; } }
public int executeUpdate ( String s ) throws SQLException { logger . debug ( "MetadataDB executeUpdate:" + s ) ; return getSql ( ) . executeUpdate ( s ) ; }
public void test() { if ( Logger . isDebugEnabled ( this ) ) { Logger . debug ( this , "attempting to lock" ) ; } }
public void test() { if ( ! this . configuration . isEventStoreEnabled ( ) || StringUtils . isEmpty ( this . configuration . getEventStore ( ) ) ) { this . logger . warn ( "New event store system is disabled" ) ; return ; } }
public void test() { try { this . eventStore = this . componentManager . getInstance ( EventStore . class , this . configuration . getEventStore ( ) ) ; } catch ( ComponentLookupException e ) { this . logger . error ( "Failed to get the configured event store [{}]" , this . configuration . getEventStore ( ) , e ) ; return ; } }
public void test() { if ( getRequest ( ) . isVerbose ( ) ) { this . logger . info ( "Synchronizing legacy events from index {} to {}" , offset , offset + events . size ( ) ) ; } }
public void test() { if ( modelDefinition != null && modelDefinition . getExtension ( CompConstants . shortLabel ) != null ) { initSubModels ( ( CompModelPlugin ) modelDefinition . getExtension ( CompConstants . shortLabel ) ) ; } else { LOGGER . info ( "No model definition found in " + submodel . getId ( ) + "." ) ; } }
@ ApiOperation ( value = "Patch entity" ) @ PatchMapping ( CommonConstants . PATH_ID ) @ ResponseStatus ( HttpStatus . OK ) public SecurityProfileDto patch ( final @ PathVariable ( "id" ) String id , @ RequestBody final Map < String , Object > partialDto ) { LOGGER . debug ( "Patch User {} with {}" , id , partialDto ) ; ParameterChecker . checkParameter ( "The Identifier, the partialEntity are mandatory parameters: " , id , partialDto ) ; Assert . isTrue ( StringUtils . equals ( id , ( String ) partialDto . get ( "id" ) ) , "Unable to patch securityProfile : the DTO id must match the path id." ) ; return service . patch ( buildUiHttpContext ( ) , partialDto , id ) ; }
@ Test public void testOneGroupMessage ( ) throws Exception { String message = "8=FIX 4.19=2034=135=049=INVMGR56=BRKR" + "1=BE.CHM.00111=CHM0001-0158=this is a camel - bindy test" + "22=448=BE000124567854=1" + "10=220" + "777=22-06-2013 12:21:11" ; List < String > data = Arrays . asList ( message . split ( "\\u0001" ) ) ; CamelContext camelContext = new DefaultCamelContext ( ) ; factory . bind ( camelContext , data , model , counter ) ; LOG . info ( ">>> Model : " + model . toString ( ) ) ; assertNotNull ( model ) ; }
public void test() { try { endpointUrl = new URL ( tmp . getProtocol ( ) , hostname , tmp . getPort ( ) , tmp . getFile ( ) ) ; } catch ( MalformedURLException e ) { LOG . error ( "Unable to process " + hostname + " as a valid hostname" , e ) ; return tmp ; } }
public void test() { try { this . resendNotifications ( ( short ) notificationsResent , createdBefore ) ; } catch ( final CircuitBreakerOpenException exc ) { LOGGER . warn ( "Processing notifications for this run will be stopped, because the circuit breaker is open." , exc ) ; break ; } }
List < T > errorAndReturnEmpty ( ) { log . error ( "poll invoked but consumer stopped for topic" + topic , new RuntimeException ( "stacktrace" ) ) ; return emptyList ( ) ; }
public void test() { try { return remoting . doWork ( ) ; } catch ( InterruptedException | CancellationException t ) { } catch ( MalformedURLException ex ) { logger . fatal ( "Connect: wrong server address" , ex ) ; showMessageToUser ( ex . getMessage ( ) ) ; } catch ( UndeclaredThrowableException ex ) { String addMessage = "" ; Throwable cause = ex . getCause ( ) ; code_block = IfStatement ; code_block = IfStatement ; showMessageToUser ( addMessage + ( ex . getMessage ( ) != null ? ex . getMessage ( ) : "" ) ) ; } catch ( IOException ex ) { logger . fatal ( "Connect: unknown IO error" , ex ) ; String addMessage = "" ; code_block = IfStatement ; showMessageToUser ( addMessage + ( ex . getMessage ( ) != null ? ex . getMessage ( ) : "" ) ) ; } catch ( MageVersionException ex ) { logger . warn ( "Connect: wrong versions" ) ; disconnect ( false ) ; code_block = IfStatement ; } catch ( CannotConnectException ex ) { code_block = IfStatement ; } catch ( Throwable t ) { logger . fatal ( "Connect: FAIL" , t ) ; disconnect ( false ) ; code_block = IfStatement ; } finally { lastRemotingTask = null ; } }
public void test() { if ( exep . getCause ( ) instanceof IOException ) { code_block = IfStatement ; } else { logger . error ( "Connect: unknown server error" , exep . getCause ( ) ) ; } }
public void test() { if ( addMessage . isEmpty ( ) ) { logger . fatal ( "Connect: unknown error" , ex ) ; } }
public void test() { try { return remoting . doWork ( ) ; } catch ( InterruptedException | CancellationException t ) { } catch ( MalformedURLException ex ) { logger . fatal ( "Connect: wrong server address" , ex ) ; showMessageToUser ( ex . getMessage ( ) ) ; } catch ( UndeclaredThrowableException ex ) { String addMessage = "" ; Throwable cause = ex . getCause ( ) ; code_block = IfStatement ; code_block = IfStatement ; showMessageToUser ( addMessage + ( ex . getMessage ( ) != null ? ex . getMessage ( ) : "" ) ) ; } catch ( IOException ex ) { logger . fatal ( "Connect: unknown IO error" , ex ) ; String addMessage = "" ; code_block = IfStatement ; showMessageToUser ( addMessage + ( ex . getMessage ( ) != null ? ex . getMessage ( ) : "" ) ) ; } catch ( MageVersionException ex ) { logger . warn ( "Connect: wrong versions" ) ; disconnect ( false ) ; code_block = IfStatement ; } catch ( CannotConnectException ex ) { code_block = IfStatement ; } catch ( Throwable t ) { logger . fatal ( "Connect: FAIL" , t ) ; disconnect ( false ) ; code_block = IfStatement ; } finally { lastRemotingTask = null ; } }
public void test() { try { return remoting . doWork ( ) ; } catch ( InterruptedException | CancellationException t ) { } catch ( MalformedURLException ex ) { logger . fatal ( "Connect: wrong server address" , ex ) ; showMessageToUser ( ex . getMessage ( ) ) ; } catch ( UndeclaredThrowableException ex ) { String addMessage = "" ; Throwable cause = ex . getCause ( ) ; code_block = IfStatement ; code_block = IfStatement ; showMessageToUser ( addMessage + ( ex . getMessage ( ) != null ? ex . getMessage ( ) : "" ) ) ; } catch ( IOException ex ) { logger . fatal ( "Connect: unknown IO error" , ex ) ; String addMessage = "" ; code_block = IfStatement ; showMessageToUser ( addMessage + ( ex . getMessage ( ) != null ? ex . getMessage ( ) : "" ) ) ; } catch ( MageVersionException ex ) { logger . warn ( "Connect: wrong versions" ) ; disconnect ( false ) ; code_block = IfStatement ; } catch ( CannotConnectException ex ) { code_block = IfStatement ; } catch ( Throwable t ) { logger . fatal ( "Connect: FAIL" , t ) ; disconnect ( false ) ; code_block = IfStatement ; } finally { lastRemotingTask = null ; } }
public void test() { try { return remoting . doWork ( ) ; } catch ( InterruptedException | CancellationException t ) { } catch ( MalformedURLException ex ) { logger . fatal ( "Connect: wrong server address" , ex ) ; showMessageToUser ( ex . getMessage ( ) ) ; } catch ( UndeclaredThrowableException ex ) { String addMessage = "" ; Throwable cause = ex . getCause ( ) ; code_block = IfStatement ; code_block = IfStatement ; showMessageToUser ( addMessage + ( ex . getMessage ( ) != null ? ex . getMessage ( ) : "" ) ) ; } catch ( IOException ex ) { logger . fatal ( "Connect: unknown IO error" , ex ) ; String addMessage = "" ; code_block = IfStatement ; showMessageToUser ( addMessage + ( ex . getMessage ( ) != null ? ex . getMessage ( ) : "" ) ) ; } catch ( MageVersionException ex ) { logger . warn ( "Connect: wrong versions" ) ; disconnect ( false ) ; code_block = IfStatement ; } catch ( CannotConnectException ex ) { code_block = IfStatement ; } catch ( Throwable t ) { logger . fatal ( "Connect: FAIL" , t ) ; disconnect ( false ) ; code_block = IfStatement ; } finally { lastRemotingTask = null ; } }
public void test() { try { Matcher m = PARAM_PATTERN . matcher ( paramNodeValue ) ; code_block = IfStatement ; } catch ( Throwable e ) { logger . error ( "Failed to parse the value to get the parameter id." , e ) ; } }
public void test() { { Path pathInZipFile = zipfs . getPath ( file . toString ( ) . substring ( dest . toString ( ) . length ( ) ) ) ; LOG . debug ( "compress: '{}'" , pathInZipFile ) ; Files . copy ( file , pathInZipFile , StandardCopyOption . REPLACE_EXISTING ) ; return FileVisitResult . CONTINUE ; } }
public void test() { try { JSONArray paths = ( ( JSONArray ) params . get ( "items" ) ) ; String paramDest = ( String ) params . get ( "destination" ) ; final Path dest = Paths . get ( REPOSITORY_BASE_PATH , paramDest ) ; Path zip = dest . resolve ( ( String ) params . get ( "compressedFilename" ) ) ; code_block = IfStatement ; Map < String , String > env = new HashMap < > ( ) ; env . put ( "create" , "true" ) ; boolean zipped = false ; code_block = TryStatement ;  return success ( params ) ; } catch ( IOException e ) { LOG . error ( "compress:" + e . getMessage ( ) , e ) ; return error ( e . getClass ( ) . getSimpleName ( ) + ":" + e . getMessage ( ) ) ; } }
@ Override public TqlElement visitFieldIsNull ( TqlParser . FieldIsNullContext ctx ) { LOG . debug ( "Visit is field null expression: " + ctx . getText ( ) ) ; TqlElement fieldName = ctx . getChild ( 0 ) . accept ( this ) ; FieldIsNullExpression isNullExpression = new FieldIsNullExpression ( fieldName ) ; LOG . debug ( "End visit is field null expression: " + ctx . getText ( ) ) ; return isNullExpression ; }
@ Override public TqlElement visitFieldIsNull ( TqlParser . FieldIsNullContext ctx ) { LOG . debug ( "Visit is field null expression: " + ctx . getText ( ) ) ; TqlElement fieldName = ctx . getChild ( 0 ) . accept ( this ) ; FieldIsNullExpression isNullExpression = new FieldIsNullExpression ( fieldName ) ; LOG . debug ( "End visit is field null expression: " + ctx . getText ( ) ) ; return isNullExpression ; }
@ Initialize public void init ( ) { final ClassLoader parent = getClass ( ) . getClassLoader ( ) ; _groovyClassLoader = new GroovyClassLoader ( parent ) ; logger . debug ( "Compiling Groovy code:\n{}" , code ) ; final Class < ? > groovyClass = _groovyClassLoader . parseClass ( code ) ; _groovyObject = ( GroovyObject ) ReflectionUtils . newInstance ( groovyClass ) ; }
@ Override public void onRegistrationSuccess ( ServerIdentity server , RegisterRequest request , String registrationID ) { log . info ( "ClientObserver -> onRegistrationSuccess...  EndpointName [{}] [{}]" , request . getEndpointName ( ) , registrationID ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Node {" + nodeName + "} joined the cluster." ) ; } }
public void test() { try { return regionPolicy . selectFromNetworkLocation ( bookieNodeToReplace . getNetworkLocation ( ) , excludeBookies , TruePredicate . INSTANCE , EnsembleForReplacementWithNoConstraints . INSTANCE , true ) ; } catch ( BKException . BKNotEnoughBookiesException e ) { LOG . warn ( "Failed to choose a bookie from {} : " + "excluded {}, fallback to choose bookie randomly from the cluster." , bookieNodeToReplace . getNetworkLocation ( ) , excludeBookies ) ; } }
public void update ( UpnpControlBindingConfiguration newConfig ) { String newPath = newConfig . path ; code_block = IfStatement ; logger . debug ( "Storage path updated to {}" , path ) ; UpnpControlUtil . bindingConfigurationChanged ( path ) ; }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "GetVertexStatus via AM for app: " + appId + " dag: " + dagId + " vertex: " + vertexName ) ; } }
public void test() { if ( LOGGER . isTraceEnabled ( ) ) { LOGGER . trace ( "->COLLECTED POINT X VALUE(LON): " + myCurrentLon ) ; } }
public void warn ( Throwable t , String message , Object ... formatArgs ) { log . warn ( StringUtils . safeFormat ( message , formatArgs ) , t ) ; }
protected void doTestStore ( IPageStore pageStore ) { this . pageStore = new AsynchronousPageStore ( pageStore , 100 ) ; generateSessionsAndPages ( ) ; log . info ( "Starting..." ) ; long start = System . currentTimeMillis ( ) ; code_block = ForStatement ; code_block = ForStatement ; code_block = ForStatement ; code_block = WhileStatement ; code_block = IfStatement ; long duration = System . currentTimeMillis ( ) - start ; log . info ( "Took: " + duration + " ms" ) ; log . info ( "Save: " + saveCount . intValue ( ) + " files, " + bytesWritten . get ( ) + " bytes" ) ; log . info ( "Read: " + ( read1Count . get ( ) + read2Count . get ( ) ) + " files, " + bytesRead . get ( ) + " bytes" ) ; log . info ( "Average save time (ns): " + ( double ) saveTime . get ( ) / ( double ) saveCount . get ( ) ) ; assertEquals ( 0 , failures . get ( ) ) ; code_block = ForStatement ; }
public void test() { try { Thread . sleep ( 50 ) ; } catch ( InterruptedException e ) { log . error ( e . getMessage ( ) , e ) ; } }
protected void doTestStore ( IPageStore pageStore ) { this . pageStore = new AsynchronousPageStore ( pageStore , 100 ) ; generateSessionsAndPages ( ) ; log . info ( "Starting..." ) ; long start = System . currentTimeMillis ( ) ; code_block = ForStatement ; code_block = ForStatement ; code_block = ForStatement ; code_block = WhileStatement ; code_block = IfStatement ; long duration = System . currentTimeMillis ( ) - start ; log . info ( "Took: " + duration + " ms" ) ; log . info ( "Save: " + saveCount . intValue ( ) + " files, " + bytesWritten . get ( ) + " bytes" ) ; log . info ( "Read: " + ( read1Count . get ( ) + read2Count . get ( ) ) + " files, " + bytesRead . get ( ) + " bytes" ) ; log . info ( "Average save time (ns): " + ( double ) saveTime . get ( ) / ( double ) saveCount . get ( ) ) ; assertEquals ( 0 , failures . get ( ) ) ; code_block = ForStatement ; }
protected void doTestStore ( IPageStore pageStore ) { this . pageStore = new AsynchronousPageStore ( pageStore , 100 ) ; generateSessionsAndPages ( ) ; log . info ( "Starting..." ) ; long start = System . currentTimeMillis ( ) ; code_block = ForStatement ; code_block = ForStatement ; code_block = ForStatement ; code_block = WhileStatement ; code_block = IfStatement ; long duration = System . currentTimeMillis ( ) - start ; log . info ( "Took: " + duration + " ms" ) ; log . info ( "Save: " + saveCount . intValue ( ) + " files, " + bytesWritten . get ( ) + " bytes" ) ; log . info ( "Read: " + ( read1Count . get ( ) + read2Count . get ( ) ) + " files, " + bytesRead . get ( ) + " bytes" ) ; log . info ( "Average save time (ns): " + ( double ) saveTime . get ( ) / ( double ) saveCount . get ( ) ) ; assertEquals ( 0 , failures . get ( ) ) ; code_block = ForStatement ; }
protected void doTestStore ( IPageStore pageStore ) { this . pageStore = new AsynchronousPageStore ( pageStore , 100 ) ; generateSessionsAndPages ( ) ; log . info ( "Starting..." ) ; long start = System . currentTimeMillis ( ) ; code_block = ForStatement ; code_block = ForStatement ; code_block = ForStatement ; code_block = WhileStatement ; code_block = IfStatement ; long duration = System . currentTimeMillis ( ) - start ; log . info ( "Took: " + duration + " ms" ) ; log . info ( "Save: " + saveCount . intValue ( ) + " files, " + bytesWritten . get ( ) + " bytes" ) ; log . info ( "Read: " + ( read1Count . get ( ) + read2Count . get ( ) ) + " files, " + bytesRead . get ( ) + " bytes" ) ; log . info ( "Average save time (ns): " + ( double ) saveTime . get ( ) / ( double ) saveCount . get ( ) ) ; assertEquals ( 0 , failures . get ( ) ) ; code_block = ForStatement ; }
protected void doTestStore ( IPageStore pageStore ) { this . pageStore = new AsynchronousPageStore ( pageStore , 100 ) ; generateSessionsAndPages ( ) ; log . info ( "Starting..." ) ; long start = System . currentTimeMillis ( ) ; code_block = ForStatement ; code_block = ForStatement ; code_block = ForStatement ; code_block = WhileStatement ; code_block = IfStatement ; long duration = System . currentTimeMillis ( ) - start ; log . info ( "Took: " + duration + " ms" ) ; log . info ( "Save: " + saveCount . intValue ( ) + " files, " + bytesWritten . get ( ) + " bytes" ) ; log . info ( "Read: " + ( read1Count . get ( ) + read2Count . get ( ) ) + " files, " + bytesRead . get ( ) + " bytes" ) ; log . info ( "Average save time (ns): " + ( double ) saveTime . get ( ) / ( double ) saveCount . get ( ) ) ; assertEquals ( 0 , failures . get ( ) ) ; code_block = ForStatement ; }
public void stopService ( ) { log . info ( "stopService, stopping..." ) ; running . set ( false ) ; code_block = TryStatement ;  clientHandler . interrupt ( ) ; log . info ( "stopService, complete." ) ; }
public void test() { try { serverSocket . close ( ) ; } catch ( IOException e ) { log . warn ( "ServerSocket.close" , e ) ; } }
@ Test public void testCleanupOnDelete ( ) throws Exception { logger . info ( "Started testStaleIndexCleanup()" ) ; System . setProperty ( EVENTS_DISABLED , "true" ) ; final EntityManager em = app . getEntityManager ( ) ; final int numEntities = 5 ; final int numUpdates = 5 ; final List < Entity > things = new ArrayList < Entity > ( numEntities ) ; code_block = ForStatement ; app . waitForQueueDrainAndRefreshIndex ( ) ; CandidateResults crs = queryCollectionCp ( "things" , "thing" , "select *" ) ; Assert . assertEquals ( "Expect no stale candidates yet" , numEntities , crs . size ( ) ) ; int count = 0 ; List < Entity > maxVersions = new ArrayList < > ( numEntities ) ; code_block = ForStatement ; em . refreshIndex ( ) ; code_block = ForStatement ; System . setProperty ( EVENTS_DISABLED , "false" ) ; Thread . sleep ( 250 ) ; app . waitForQueueDrainAndRefreshIndex ( ) ; Thread . sleep ( 250 ) ; Results results = null ; count = 0 ; do code_block = "" ; while ( crs . size ( ) > 0 && count ++ < 2000 ) ; Assert . assertEquals ( "Expect no candidates" , 0 , crs . size ( ) ) ; }
public void test() { if ( count % 100 == 0 ) { logger . info ( "Updated {} of {} times" , count , numEntities * numUpdates ) ; } }
private StringBuilder buildTargetTableArg ( StringBuilder builder , CatalogTable catalog ) throws FalconException { LOG . info ( "Catalog URI {}" , catalog . getUri ( ) ) ; builder . append ( "--skip-dist-cache" ) . append ( ImportExportCommon . ARG_SEPARATOR ) ; Iterator < String > itr = Splitter . on ( "" ) . split ( catalog . getUri ( ) ) . iterator ( ) ; String dbTable = itr . next ( ) ; String partitions = itr . next ( ) ; Iterator < String > itrDbTable = Splitter . on ( ":" ) . split ( dbTable ) . iterator ( ) ; itrDbTable . next ( ) ; String db = itrDbTable . next ( ) ; String table = itrDbTable . next ( ) ; LOG . debug ( "Target database {}, table {}" , db , table ) ; builder . append ( "--hcatalog-database" ) . append ( ImportExportCommon . ARG_SEPARATOR ) . append ( String . format ( "${coord:databaseOut('%s')}" , FeedImportCoordinatorBuilder . IMPORT_DATAOUT_NAME ) ) . append ( ImportExportCommon . ARG_SEPARATOR ) ; builder . append ( "--hcatalog-table" ) . append ( ImportExportCommon . ARG_SEPARATOR ) . append ( String . format ( "${coord:tableOut('%s')}" , FeedImportCoordinatorBuilder . IMPORT_DATAOUT_NAME ) ) . append ( ImportExportCommon . ARG_SEPARATOR ) ; Map < String , String > partitionsMap = ImportExportCommon . getPartitionKeyValues ( partitions ) ; code_block = IfStatement ; return builder ; }
private StringBuilder buildTargetTableArg ( StringBuilder builder , CatalogTable catalog ) throws FalconException { LOG . info ( "Catalog URI {}" , catalog . getUri ( ) ) ; builder . append ( "--skip-dist-cache" ) . append ( ImportExportCommon . ARG_SEPARATOR ) ; Iterator < String > itr = Splitter . on ( "" ) . split ( catalog . getUri ( ) ) . iterator ( ) ; String dbTable = itr . next ( ) ; String partitions = itr . next ( ) ; Iterator < String > itrDbTable = Splitter . on ( ":" ) . split ( dbTable ) . iterator ( ) ; itrDbTable . next ( ) ; String db = itrDbTable . next ( ) ; String table = itrDbTable . next ( ) ; LOG . debug ( "Target database {}, table {}" , db , table ) ; builder . append ( "--hcatalog-database" ) . append ( ImportExportCommon . ARG_SEPARATOR ) . append ( String . format ( "${coord:databaseOut('%s')}" , FeedImportCoordinatorBuilder . IMPORT_DATAOUT_NAME ) ) . append ( ImportExportCommon . ARG_SEPARATOR ) ; builder . append ( "--hcatalog-table" ) . append ( ImportExportCommon . ARG_SEPARATOR ) . append ( String . format ( "${coord:tableOut('%s')}" , FeedImportCoordinatorBuilder . IMPORT_DATAOUT_NAME ) ) . append ( ImportExportCommon . ARG_SEPARATOR ) ; Map < String , String > partitionsMap = ImportExportCommon . getPartitionKeyValues ( partitions ) ; code_block = IfStatement ; return builder ; }
public void test() { try { return saxFactory . newTemplates ( stylesheet ) ; } catch ( TransformerConfigurationException tcx ) { LOGGER . fatal ( tcx . getMessageAndLocation ( ) ) ; return null ; } }
@ Test public void test ( ) throws Exception { String publicWebappURL = OneRecordingServer . getPublicWebappUrl ( ) ; log . debug ( "Start uploading content" ) ; File fileToUpload = new File ( "test-files/logo.png" ) ; uploadFileWithCURL ( publicWebappURL + "repository_servlet/video-upload" , fileToUpload ) ; log . debug ( "Waiting 10 seconds to auto-termination..." ) ; Thread . sleep ( 10 * 1000 ) ; File downloadedFile = new File ( "test-files/sampleDownload.txt" ) ; log . debug ( "Start downloading file" ) ; downloadFromURL ( publicWebappURL + "repository_servlet/video-download" , downloadedFile ) ; boolean equalFiles = TestUtils . equalFiles ( fileToUpload , downloadedFile ) ; code_block = IfStatement ; assertTrue ( "The uploadad and downloaded files are different" , equalFiles ) ; }
@ Test public void test ( ) throws Exception { String publicWebappURL = OneRecordingServer . getPublicWebappUrl ( ) ; log . debug ( "Start uploading content" ) ; File fileToUpload = new File ( "test-files/logo.png" ) ; uploadFileWithCURL ( publicWebappURL + "repository_servlet/video-upload" , fileToUpload ) ; log . debug ( "Waiting 10 seconds to auto-termination..." ) ; Thread . sleep ( 10 * 1000 ) ; File downloadedFile = new File ( "test-files/sampleDownload.txt" ) ; log . debug ( "Start downloading file" ) ; downloadFromURL ( publicWebappURL + "repository_servlet/video-download" , downloadedFile ) ; boolean equalFiles = TestUtils . equalFiles ( fileToUpload , downloadedFile ) ; code_block = IfStatement ; assertTrue ( "The uploadad and downloaded files are different" , equalFiles ) ; }
@ Test public void test ( ) throws Exception { String publicWebappURL = OneRecordingServer . getPublicWebappUrl ( ) ; log . debug ( "Start uploading content" ) ; File fileToUpload = new File ( "test-files/logo.png" ) ; uploadFileWithCURL ( publicWebappURL + "repository_servlet/video-upload" , fileToUpload ) ; log . debug ( "Waiting 10 seconds to auto-termination..." ) ; Thread . sleep ( 10 * 1000 ) ; File downloadedFile = new File ( "test-files/sampleDownload.txt" ) ; log . debug ( "Start downloading file" ) ; downloadFromURL ( publicWebappURL + "repository_servlet/video-download" , downloadedFile ) ; boolean equalFiles = TestUtils . equalFiles ( fileToUpload , downloadedFile ) ; code_block = IfStatement ; assertTrue ( "The uploadad and downloaded files are different" , equalFiles ) ; }
public void test() { if ( equalFiles ) { log . debug ( "The uploadad and downloaded files are equal" ) ; } else { log . debug ( "The uploadad and downloaded files are different" ) ; } }
public void test() { if ( equalFiles ) { log . debug ( "The uploadad and downloaded files are equal" ) ; } else { log . debug ( "The uploadad and downloaded files are different" ) ; } }
public void test() { try { String marshalInfo = SolrConfigDOM . marshalConfig ( solrConfig ) ; this . getConfigManager ( ) . updateConfigItem ( SolrConnectorSystemConstants . JPSOLRCLIENT_SYSTEM_CONFIG_NAME , marshalInfo ) ; } catch ( ApsSystemException e ) { _logger . error ( "Error updating configuration" , e ) ; throw e ; } }
public void test() { if ( accountAsset . getQuantityATU ( ) > 0 || accountAsset . getUnconfirmedQuantityATU ( ) > 0 ) { accountAssetTable . insert ( accountAsset ) ; log . trace ( "<< update() INSERT accountAsset = {}" , accountAsset ) ; } else { int height = blockChainInfoService . getHeight ( ) ; accountAssetTable . deleteAtHeight ( accountAsset , height ) ; log . trace ( "<< update() DELETE, height={}, accountAsset = {}" , height , accountAsset ) ; } }
public void test() { if ( accountAsset . getQuantityATU ( ) > 0 || accountAsset . getUnconfirmedQuantityATU ( ) > 0 ) { accountAssetTable . insert ( accountAsset ) ; log . trace ( "<< update() INSERT accountAsset = {}" , accountAsset ) ; } else { int height = blockChainInfoService . getHeight ( ) ; accountAssetTable . deleteAtHeight ( accountAsset , height ) ; log . trace ( "<< update() DELETE, height={}, accountAsset = {}" , height , accountAsset ) ; } }
static ByteBuffer concatBuffers ( List < DataBuffer > buffers ) { log . info ( "[I198] creating BytBuffer from {} chunks" , buffers . size ( ) ) ; int partSize = 0 ; code_block = ForStatement ; ByteBuffer partData = ByteBuffer . allocate ( partSize ) ; buffers . forEach ( ( buffer ) code_block = LoopStatement ; ) ; partData . rewind ( ) ; log . info ( "[I208] partData: size={}" , partData . capacity ( ) ) ; return partData ; }
public void test() { try { conn . setAutoCommit ( false ) ; Statement statement = conn . createStatement ( ) ; int counter = 0 ; code_block = ForStatement ; statement . executeBatch ( ) ; conn . commit ( ) ; LOG . info ( new StringBuffer ( "Encrypted " ) . append ( " attributes of table " ) . append ( tableName ) ) ; } catch ( Exception e ) { LOG . error ( new StringBuffer ( "Caught exception, while encrypting " ) . append ( " attributes of table " ) . append ( tableName ) , e ) ; conn . rollback ( ) ; return false ; } }
public void test() { try { this . controller . update ( update ) ; } catch ( OwsExceptionReport ex ) { LOGGER . error ( "Error processing Event" , ex ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceInventoryWarehouseServiceUtil . class , "getCommerceInventoryWarehouses" , _getCommerceInventoryWarehousesParameterTypes10 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , companyId , groupId , active ) ; Object returnObj = null ; code_block = TryStatement ;  return ( java . util . List < com . liferay . commerce . inventory . model . CommerceInventoryWarehouse > ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( MembershipRequestServiceUtil . class , "updateStatus" , _updateStatusParameterTypes3 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , membershipRequestId , reviewComments , statusId , serviceContext ) ; code_block = TryStatement ;  } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( hostname == null || hostname . length ( ) == 0 ) { LOGGER . warn ( "Hostname not found, this leads to inproper client-configuration." ) ; } }
public void test() { if ( baseDN == null || baseDN . length ( ) == 0 ) { LOGGER . warn ( "BaseDN not found, this leads to inproper client-configuration." ) ; } }
public void test() { if ( externalIp . isPresent ( ) ) { hostname = externalIp . get ( ) . getHostAddress ( ) ; LOGGER . info ( "Replaced ${myip}-value 'localhost' with '{}'" , hostname ) ; } else { LOGGER . error ( "Cannot obtain the host ip-address, using default: " + hostname ) ; } }
public void test() { try { entry . setValue ( entry . getValue ( ) . toString ( ) . replaceAll ( "\\$\\{urlencoded\\:basedn\\}" , URLEncoder . encode ( baseDN , "UTF-8" ) ) ) ; } catch ( UnsupportedEncodingException e ) { LOGGER . error ( "UTF-8 encoding not supported" , e ) ; } }
public void test() { try { SHUTDOWN_LOGGER . debug ( "Disposing component [{}]..." , instance . getClass ( ) . getName ( ) ) ; ( ( Disposable ) instance ) . dispose ( ) ; SHUTDOWN_LOGGER . debug ( "Component [{}] has been disposed" , instance . getClass ( ) . getName ( ) ) ; } catch ( ComponentLifecycleException e ) { this . logger . error ( "Failed to dispose component with role type [{}] and role hint [{}]" , componentEntry . descriptor . getRoleType ( ) , componentEntry . descriptor . getRoleHint ( ) , e ) ; } }
public void test() { for ( int i = 0 ; i < 8 ; i ++ ) { SameDiff sd = SameDiff . create ( ) ; SDVariable in3 = sd . var ( "in3" , Nd4j . rand ( new int [ ] code_block = "" ; ) ) ; SDVariable in2 = sd . var ( "in2" , in2Shape ) ; SDVariable bcOp ; String name ; code_block = SwitchStatement ; SDVariable outVar = sd . sum ( bcOp ) ; String msg = "(test " + i + ": " + name + ", dimension=" + dim_sz1 + ")" ; log . info ( "*** Starting test: " + msg ) ; INDArray in3Arr = Nd4j . randn ( new int [ ] code_block = "" ; ) . muli ( 100 ) ; INDArray in2Arr = Nd4j . randn ( in2Shape ) . muli ( 100 ) ; sd . associateArrayWithVariable ( in3Arr , in3 ) ; sd . associateArrayWithVariable ( in2Arr , in2 ) ; TestCase tc = new TestCase ( sd ) ; String error = OpValidation . validate ( tc ) ; code_block = IfStatement ; } }
public void test() { try { response = HttpUtil . executeUrl ( "GET" , urlStr , 10000 ) ; logger . debug ( "OvenData = {}" , response ) ; result = gson . fromJson ( response , HaasSohnpelletstoveJsonDataDTO . class ) ; resultOk = true ; } catch ( IOException e ) { logger . debug ( "Error processiong Get request {}" , urlStr ) ; statusDescr = "Timeout error with" + config . hostIP + ". Cannot find service on give IP. Please verify the IP-Address!" ; errorDetail = e . getMessage ( ) ; resultOk = false ; } catch ( Exception e ) { logger . debug ( "Unknwon Error: {}" , e . getMessage ( ) ) ; errorDetail = e . getMessage ( ) ; resultOk = false ; } }
public void test() { try { response = HttpUtil . executeUrl ( "GET" , urlStr , 10000 ) ; logger . debug ( "OvenData = {}" , response ) ; result = gson . fromJson ( response , HaasSohnpelletstoveJsonDataDTO . class ) ; resultOk = true ; } catch ( IOException e ) { logger . debug ( "Error processiong Get request {}" , urlStr ) ; statusDescr = "Timeout error with" + config . hostIP + ". Cannot find service on give IP. Please verify the IP-Address!" ; errorDetail = e . getMessage ( ) ; resultOk = false ; } catch ( Exception e ) { logger . debug ( "Unknwon Error: {}" , e . getMessage ( ) ) ; errorDetail = e . getMessage ( ) ; resultOk = false ; } }
public void test() { try { response = HttpUtil . executeUrl ( "GET" , urlStr , 10000 ) ; logger . debug ( "OvenData = {}" , response ) ; result = gson . fromJson ( response , HaasSohnpelletstoveJsonDataDTO . class ) ; resultOk = true ; } catch ( IOException e ) { logger . debug ( "Error processiong Get request {}" , urlStr ) ; statusDescr = "Timeout error with" + config . hostIP + ". Cannot find service on give IP. Please verify the IP-Address!" ; errorDetail = e . getMessage ( ) ; resultOk = false ; } catch ( Exception e ) { logger . debug ( "Unknwon Error: {}" , e . getMessage ( ) ) ; errorDetail = e . getMessage ( ) ; resultOk = false ; } }
private static CuratorFramework provideInitializedZookeeperClient ( String zkConnection ) throws Exception { LOG . info ( "Creating Zookeeper Client connecting to {}" , zkConnection ) ; RetryPolicy retryPolicy = new ExponentialBackoffRetry ( 1000 , 3 ) ; CuratorFramework zkClient = CuratorFrameworkFactory . builder ( ) . namespace ( NAMESPACE ) . connectString ( zkConnection ) . retryPolicy ( retryPolicy ) . build ( ) ; LOG . info ( "Connecting to ZK cluster {}" , zkClient . getState ( ) ) ; zkClient . start ( ) ; zkClient . blockUntilConnected ( ) ; LOG . info ( "Connection to ZK cluster {}" , zkClient . getState ( ) ) ; return zkClient ; }
private static CuratorFramework provideInitializedZookeeperClient ( String zkConnection ) throws Exception { LOG . info ( "Creating Zookeeper Client connecting to {}" , zkConnection ) ; RetryPolicy retryPolicy = new ExponentialBackoffRetry ( 1000 , 3 ) ; CuratorFramework zkClient = CuratorFrameworkFactory . builder ( ) . namespace ( NAMESPACE ) . connectString ( zkConnection ) . retryPolicy ( retryPolicy ) . build ( ) ; LOG . info ( "Connecting to ZK cluster {}" , zkClient . getState ( ) ) ; zkClient . start ( ) ; zkClient . blockUntilConnected ( ) ; LOG . info ( "Connection to ZK cluster {}" , zkClient . getState ( ) ) ; return zkClient ; }
private static CuratorFramework provideInitializedZookeeperClient ( String zkConnection ) throws Exception { LOG . info ( "Creating Zookeeper Client connecting to {}" , zkConnection ) ; RetryPolicy retryPolicy = new ExponentialBackoffRetry ( 1000 , 3 ) ; CuratorFramework zkClient = CuratorFrameworkFactory . builder ( ) . namespace ( NAMESPACE ) . connectString ( zkConnection ) . retryPolicy ( retryPolicy ) . build ( ) ; LOG . info ( "Connecting to ZK cluster {}" , zkClient . getState ( ) ) ; zkClient . start ( ) ; zkClient . blockUntilConnected ( ) ; LOG . info ( "Connection to ZK cluster {}" , zkClient . getState ( ) ) ; return zkClient ; }
public void test() { if ( regulating && regulatingForcedToOff ) { LOGGER . warn ( "Transformer {}. Regulating control forced to off. Only one control is supported" , id ) ; regulating = false ; } }
public void startService ( String serviceName ) throws ServiceAdminException , RemoteException { serviceAdminStub . startService ( serviceName ) ; log . info ( "Service Started" ) ; }
public void test() { if ( ! shutdown ( 10 , TimeUnit . SECONDS ) ) { log . error ( "Some processors are still active" ) ; } }
public void failJob ( String message , String details ) { log . debug ( "failed deposit: {}" , message ) ; throw new JobFailedException ( message , details ) ; }
public void test() { if ( remoteCommit . getServerId ( ) == DomainClassInfo . getServerId ( ) ) { logger . debug ( "Ignoring self commit message." ) ; } else { logger . debug ( "Received remote commit message. serverId={}, tx={}" , remoteCommit . getServerId ( ) , remoteCommit . getTxNumber ( ) ) ; REMOTE_COMMITS . offer ( remoteCommit ) ; } }
public void test() { if ( column . getCellEditor ( ) != null && column . getBeanPropertyAccessors ( ) == null ) { log . warn ( Messages . getString ( "TableViewerCreator.columnNoIBeanProperty" , column . getId ( ) , column . getTitle ( ) ) ) ; } }
@ Override public int getProductCount ( ) { logger . debug ( "Getting product count: " , super . getProductCount ( ) ) ; return super . getProductCount ( ) ; }
public void test() { try { select ( ) ; } catch ( RuntimeException e ) { LOG . warn ( "Ignoring unexpected runtime exception" , e ) ; } catch ( Exception e ) { LOG . warn ( "Ignoring unexpected exception" , e ) ; } }
public void test() { try { select ( ) ; } catch ( RuntimeException e ) { LOG . warn ( "Ignoring unexpected runtime exception" , e ) ; } catch ( Exception e ) { LOG . warn ( "Ignoring unexpected exception" , e ) ; } }
public void test() { if ( adapter instanceof GeotoolsFeatureDataAdapter ) { gtAdapter = ( GeotoolsFeatureDataAdapter ) adapter ; } else-if ( ( adapter instanceof InternalDataAdapter ) && ( ( ( InternalDataAdapter ) adapter ) . getAdapter ( ) instanceof GeotoolsFeatureDataAdapter ) ) { gtAdapter = ( GeotoolsFeatureDataAdapter ) ( ( InternalDataAdapter ) adapter ) . getAdapter ( ) ; } else { LOGGER . error ( "Unable to perform aggregation on non-geotools feature adapter '" + adapter . getTypeName ( ) + "'" ) ; return null ; } }
public void test() { try { DriverManager . deregisterDriver ( driver ) ; } catch ( SQLException e ) { log . warn ( "SQL driver deregistration failed" , e ) ; } }
public void test() { try { code_block = IfStatement ; field . setAccessible ( true ) ; code_block = IfStatement ; } catch ( Exception t ) { log . debug ( "Could not set field {} to null in class {}" , field . getName ( ) , clazz . getName ( ) , t ) ; } }
public void test() { try { Field [ ] fields = clazz . getDeclaredFields ( ) ; code_block = ForStatement ; } catch ( Exception t ) { log . debug ( "Could not clean fields for class {}" , clazz . getName ( ) , t ) ; } }
public void test() { { String atPort = getAtPort ( ) ; logger . debug ( "sendCommand getSignalStrength :: {}" , SimTechSim7000AtCommands . getSignalStrength . getCommand ( ) ) ; byte [ ] reply ; CommConnection commAtConnection = openSerialPort ( atPort ) ; code_block = IfStatement ; code_block = TryStatement ;  closeSerialPort ( commAtConnection ) ; code_block = IfStatement ; } }
public void delete ( StgMUmsetzStat persistentInstance ) { log . debug ( "deleting StgMUmsetzStat instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . delete ( persistentInstance ) ; log . debug ( "delete successful" ) ; } catch ( RuntimeException re ) { log . error ( "delete failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . delete ( persistentInstance ) ; log . debug ( "delete successful" ) ; } catch ( RuntimeException re ) { log . error ( "delete failed" , re ) ; throw re ; } }
@ Override public void deleteByQuery ( TypeDescriptor typeDescriptor , Object query ) throws ClientException { JsonNode queryJsonNode = getModelConverter ( ) . convertQuery ( query ) ; LOG . debug ( QUERY_CONVERTED_QUERY , queryJsonNode ) ; String json = writeRequestFromJsonNode ( queryJsonNode ) ; Response deleteResponse = restCallTimeoutHandler ( ( ) -> getClient ( ) . performRequest ( ElasticsearchKeywords . ACTION_POST , ElasticsearchResourcePaths . deleteByQuery ( typeDescriptor ) , Collections . emptyMap ( ) , ApplicationJsonEntityBuilder . buildFrom ( json ) , new ContentTypeApplicationJsonHeader ( ) ) , typeDescriptor . getIndex ( ) , "DELETE BY QUERY" ) ; code_block = IfStatement ; }
private static X509Certificate generateCert ( ZonedDateTime now , X509CertificateHolder certHolder ) throws Exception { final X509Certificate certificate = new JcaX509CertificateConverter ( ) . getCertificate ( certHolder ) ; certificate . checkValidity ( Date . from ( now . toInstant ( ) ) ) ; certificate . verify ( certificate . getPublicKey ( ) ) ; final String fingerprint = BaseEncoding . base16 ( ) . withSeparator ( ":" , 2 ) . encode ( MessageDigest . getInstance ( "SHA-256" ) . digest ( certificate . getEncoded ( ) ) ) ; LOGGER . info ( "Certificate created (SHA-256 fingerprint: {})" , fingerprint ) ; return certificate ; }
public void test() { if ( ! ( abstractParameter instanceof NeptuneValidateParameters ) ) { log . error ( "invalid parameters for validator " + abstractParameter . getClass ( ) . getName ( ) ) ; return false ; } }
public void test() { try { job = this . jobManager . queueJob ( config ) ; } catch ( JobException e ) { String errmsg = this . i18n . tr ( "An unexpected exception occurred " + "while scheduling job \"{0}\"" , config . getJobKey ( ) ) ; log . error ( errmsg , e ) ; throw new IseException ( errmsg , e ) ; } }
public void test() { if ( pair . length != 2 ) { LOGGER . warn ( "Additional Header has wrong format {}" , nameValuePair ) ; continue ; } }
public void test() { if ( StringUtils . isNotBlank ( localPropsEnv ) ) { LOGGER . debug ( "Loading local.properties from env var [$LATKE_LOCAL_PROPS=" + localPropsEnv + "]" ) ; resourceAsStream = new FileInputStream ( localPropsEnv ) ; } else { LOGGER . debug ( "Loading local.properties from classpath [/local.properties]" ) ; resourceAsStream = Latkes . class . getResourceAsStream ( "/local.properties" ) ; } }
public void test() { if ( StringUtils . isNotBlank ( localPropsEnv ) ) { LOGGER . debug ( "Loading local.properties from env var [$LATKE_LOCAL_PROPS=" + localPropsEnv + "]" ) ; resourceAsStream = new FileInputStream ( localPropsEnv ) ; } else { LOGGER . debug ( "Loading local.properties from classpath [/local.properties]" ) ; resourceAsStream = Latkes . class . getResourceAsStream ( "/local.properties" ) ; } }
public void test() { try { SLDClassifier c = new SLDClassifier ( credentials , new ClassifierCommand ( getBodyFromRequest ( request ) ) , factory ) ; SLDDocService service = new SLDDocService ( this . docTempDir , this . connectionPool ) ; String fileName = service . saveData ( c . getSLD ( ) , SecurityHeaders . decode ( request . getHeader ( SEC_USERNAME ) ) ) ; PrintWriter out = response . getWriter ( ) ; out . println ( "{\"success\":true,\"" + FILEPATH_VARNAME + "\":\"" + SLD_URL + fileName + "\"}" ) ; } catch ( DocServiceException e ) { sendErrorToClient ( response , e . getErrorCode ( ) , e . getMessage ( ) ) ; LOG . error ( "Error occured while doing a classification" , e ) ; } catch ( IOException e ) { LOG . error ( "I/O exception encountered while doing a classification" , e ) ; } }
public void test() { try { SLDClassifier c = new SLDClassifier ( credentials , new ClassifierCommand ( getBodyFromRequest ( request ) ) , factory ) ; SLDDocService service = new SLDDocService ( this . docTempDir , this . connectionPool ) ; String fileName = service . saveData ( c . getSLD ( ) , SecurityHeaders . decode ( request . getHeader ( SEC_USERNAME ) ) ) ; PrintWriter out = response . getWriter ( ) ; out . println ( "{\"success\":true,\"" + FILEPATH_VARNAME + "\":\"" + SLD_URL + fileName + "\"}" ) ; } catch ( DocServiceException e ) { sendErrorToClient ( response , e . getErrorCode ( ) , e . getMessage ( ) ) ; LOG . error ( "Error occured while doing a classification" , e ) ; } catch ( IOException e ) { LOG . error ( "I/O exception encountered while doing a classification" , e ) ; } }
public void test() { if ( CONVERTER_EP . equals ( extensionPoint ) ) { ConverterDescriptor desc = ( ConverterDescriptor ) contribution ; registerConverter ( desc ) ; } else-if ( CONFIG_EP . equals ( extensionPoint ) ) { GlobalConfigDescriptor desc = ( GlobalConfigDescriptor ) contribution ; config . update ( desc ) ; } else { log . error ( "Unable to handle unknown extensionPoint " + extensionPoint ) ; } }
public void test() { try { String response = messageConverter . getResponseTextMessage ( ActiveThreadCountHandler . API_ACTIVE_THREAD_COUNT , resultMap ) ; TextMessage responseTextMessage = new TextMessage ( response ) ; return responseTextMessage ; } catch ( JsonProcessingException e ) { logger . warn ( "failed while to convert message. applicationName:{}, original:{}, message:{}." , applicationName , resultMap , e . getMessage ( ) , e ) ; } }
public void test() { if ( meterRemovalPlan . isEmpty ( ) ) { LOG . trace ( "no meters on device for node: {} -> SKIPPING" , nodeId . getValue ( ) ) ; return RpcResultBuilder . < Void > success ( ) . buildFuture ( ) ; } }
public void test() { for ( Meter meter : meterRemovalPlan . getItemsToPush ( ) ) { LOG . trace ( "removing meter {} - absent in config {}" , meter . getMeterId ( ) , nodeId ) ; final KeyedInstanceIdentifier < Meter , MeterKey > meterIdent = nodeIdent . child ( Meter . class , meter . key ( ) ) ; allResults . add ( meterForwarder . remove ( meterIdent , meter , nodeIdent ) ) ; meterCrudCounts . incRemoved ( ) ; } }
public void test() { if ( tableHelper != null ) { tableHelper . configure ( tops ) ; } else { log . info ( "No configuration supplied for table " + table ) ; } }
public void test() { if ( ActiveMQRALogger . LOGGER . isTraceEnabled ( ) ) { ActiveMQRALogger . LOGGER . trace ( "setRetryInterval(" + retryInterval + ")" ) ; } }
public void test() { if ( defaultZoneHostnames . size ( ) == 1 && defaultZoneHostnames . contains ( "localhost" ) ) { logger . debug ( "No root domains configured, UAA is catch-all domain for host:" + hostname ) ; return "" ; } }
public void test() { if ( lowerHostName . endsWith ( "." + internalHostname ) ) { return lowerHostName . substring ( 0 , lowerHostName . length ( ) - internalHostname . length ( ) - 1 ) ; } }
public void test() { try { List < Person > personenUmsetzungDurch = getPersonsbyProperty ( massnahme ) ; Set < Property > rolesToSearch = findRole ( massnahme ) ; code_block = IfStatement ; } catch ( CommandException ce ) { log . error ( "Error while creating relation" , ce ) ; } }
public void test() { try { Authentication authentication = registery . getByToken ( authToken . get ( ) ) . authenticate ( ( HttpServletRequest ) request , authToken . get ( ) ) ; SecurityContextHolder . getContext ( ) . setAuthentication ( authentication ) ; chain . doFilter ( request , response ) ; SecurityContextHolder . getContext ( ) . setAuthentication ( null ) ; return ; } catch ( OAuthAuthenticationException e ) { LOGGER . warn ( "Could not authenticate bearer / jwt token" , e ) ; ( ( HttpServletResponse ) response ) . sendError ( HttpServletResponse . SC_UNAUTHORIZED , e . getMessage ( ) ) ; return ; } catch ( Exception e ) { LOGGER . warn ( "Server Error." , e ) ; ( ( HttpServletResponse ) response ) . sendError ( HttpServletResponse . SC_INTERNAL_SERVER_ERROR , e . getMessage ( ) ) ; return ; } }
public void test() { try { Authentication authentication = registery . getByToken ( authToken . get ( ) ) . authenticate ( ( HttpServletRequest ) request , authToken . get ( ) ) ; SecurityContextHolder . getContext ( ) . setAuthentication ( authentication ) ; chain . doFilter ( request , response ) ; SecurityContextHolder . getContext ( ) . setAuthentication ( null ) ; return ; } catch ( OAuthAuthenticationException e ) { LOGGER . warn ( "Could not authenticate bearer / jwt token" , e ) ; ( ( HttpServletResponse ) response ) . sendError ( HttpServletResponse . SC_UNAUTHORIZED , e . getMessage ( ) ) ; return ; } catch ( Exception e ) { LOGGER . warn ( "Server Error." , e ) ; ( ( HttpServletResponse ) response ) . sendError ( HttpServletResponse . SC_INTERNAL_SERVER_ERROR , e . getMessage ( ) ) ; return ; } }
public void test() { try ( InputStream in = Files . newInputStream ( propertiesFilePath ) ) { Properties props = new Properties ( ) ; props . load ( in ) ; logger . info ( "properties loaded from {}" , propertiesFilePath ) ; return props ; } catch ( Exception e ) { logger . error ( "Loading properties {} failed" , propertiesFilePath , e ) ; } }
public void test() { try ( InputStream in = Files . newInputStream ( propertiesFilePath ) ) { Properties props = new Properties ( ) ; props . load ( in ) ; logger . info ( "properties loaded from {}" , propertiesFilePath ) ; return props ; } catch ( Exception e ) { logger . error ( "Loading properties {} failed" , propertiesFilePath , e ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( _NO_SUCH_ENTITY_WITH_PRIMARY_KEY + primaryKey ) ; } }
private MCRDerivate createDerivate ( MCRObjectID objectID , List < MCRMetaClassification > classifications ) throws MCRPersistenceException , MCRAccessException { MCRObjectID derivateID = getNewCreateDerivateID ( objectID ) ; MCRDerivate derivate = new MCRDerivate ( ) ; derivate . setId ( derivateID ) ; derivate . getDerivate ( ) . getClassifications ( ) . addAll ( classifications ) ; String schema = MCRConfiguration2 . getString ( "MCR.Metadata.Config.derivate" ) . orElse ( "datamodel-derivate.xml" ) . replaceAll ( ".xml" , ".xsd" ) ; derivate . setSchema ( schema ) ; MCRMetaLinkID linkId = new MCRMetaLinkID ( ) ; linkId . setSubTag ( "linkmeta" ) ; linkId . setReference ( objectID , null , null ) ; derivate . getDerivate ( ) . setLinkMeta ( linkId ) ; MCRMetaIFS ifs = new MCRMetaIFS ( ) ; ifs . setSubTag ( "internal" ) ; ifs . setSourcePath ( null ) ; derivate . getDerivate ( ) . setInternals ( ifs ) ; LOGGER . debug ( "Creating new derivate with ID {}" , derivateID ) ; MCRMetadataManager . create ( derivate ) ; setDefaultPermissions ( derivateID ) ; return derivate ; }
@ Override @ Transactional public boolean pruneData ( OrganisationUnit organisationUnit ) { User user = currentUserService . getCurrentUser ( ) ; code_block = IfStatement ; dataApprovalService . deleteDataApprovals ( organisationUnit ) ; dataApprovalAuditService . deleteDataApprovalAudits ( organisationUnit ) ; completeRegistrationService . deleteCompleteDataSetRegistrations ( organisationUnit ) ; dataValueAuditService . deleteDataValueAudits ( organisationUnit ) ; dataValueService . deleteDataValues ( organisationUnit ) ; log . info ( "Pruned data for organisation unit: " + organisationUnit ) ; return true ; }
public void test() { try { code_block = ForStatement ; } catch ( EOFException e ) { LOGGER . info ( "End of file reached" ) ; } }
public void test() { try ( InputStream inputStream = provider . getServer ( url ) . sendGet ( url , response ) ) { code_block = IfStatement ; int in = 0 ; int total = 0 ; DataInputStream dataStream = new DataInputStream ( inputStream ) ; code_block = TryStatement ;  double tolerance = ( 10 - .5 ) / 10 ; return in / ( double ) total > tolerance ; } catch ( IOException | URISyntaxException e ) { LOGGER . error ( "Could not read shape file." + e , e ) ; } }
public void test() { if ( ! inThread ) { log . warn ( "waitWarrantBlockChange invoked from invalid context" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "waitWarrantBlockChange {}" , warrant . getDisplayName ( ) ) ; } }
public void test() { if ( warrant . getRunMode ( ) != Warrant . MODE_RUN ) { log . debug ( "waitWarrantBlockChange returns immediately" ) ; return null ; } }
public void test() { try { logger . debug ( "Upserting a Permission. userLink [{}], permission id [{}]" , userLink , permission . getId ( ) ) ; RxDocumentServiceRequest request = getPermissionRequest ( userLink , permission , options , OperationType . Upsert ) ; code_block = IfStatement ; return this . upsert ( request ) . map ( response -> toResourceResponse ( response , Permission . class ) ) ; } catch ( Exception e ) { logger . debug ( "Failure in upserting a Permission due to [{}]" , e . getMessage ( ) , e ) ; return Observable . error ( e ) ; } }
public void test() { try { logger . debug ( "Upserting a Permission. userLink [{}], permission id [{}]" , userLink , permission . getId ( ) ) ; RxDocumentServiceRequest request = getPermissionRequest ( userLink , permission , options , OperationType . Upsert ) ; code_block = IfStatement ; return this . upsert ( request ) . map ( response -> toResourceResponse ( response , Permission . class ) ) ; } catch ( Exception e ) { logger . debug ( "Failure in upserting a Permission due to [{}]" , e . getMessage ( ) , e ) ; return Observable . error ( e ) ; } }
public void test() { try { code_block = IfStatement ; long msb = 0 ; long lsb = 0 ; for ( int i = 0 ; i < 8 ; i ++ ) msb = ( msb << 8 ) | ( bytes [ i ] & 0xff ) ; for ( int i = 8 ; i < 16 ; i ++ ) lsb = ( lsb << 8 ) | ( bytes [ i ] & 0xff ) ; return new UUID ( msb , lsb ) ; } catch ( Exception e ) { log . error ( "Error occured, Caused by {}." , e ) ; throw new PropertyAccessException ( e ) ; } }
public void test() { try { row = URLEncoder . encode ( Bytes . toStringBinary ( put . getRow ( ) ) , UTF8 ) ; cf = URLEncoder . encode ( Bytes . toString ( CF ) , UTF8 ) ; } catch ( UnsupportedEncodingException e ) { LOGGER . error ( "UTF-8 not supported?" , e ) ; } }
public void test() { try { exist = ( readPubsubNumsubReply ( channel ) > 0 ) ? true : false ; } catch ( ProtocolException e ) { log . error ( "Redis protocol error" , e ) ; } }
public void test() { try { return IOUtils . toString ( is ) ; } catch ( IOException e ) { LOGGER . error ( "File not readable %s" , file , e ) ; } }
public void test() { try { metadata = fileDataStore . add ( byteSource , metadata ) ; } catch ( IOException e1 ) { logger . error ( e1 . getMessage ( ) , e1 ) ; throw new BusinessException ( "Can not create a copy of existing document." ) ; } }
public void test() { try { sanRecords = cert . getSubjectAlternativeNames ( ) ; } catch ( CertificateParsingException ex ) { log . error ( "Error parsing certificate of host '{}': {}" , hostName , ex . getMessage ( ) ) ; log . debug ( "Exception" , ex ) ; sanRecords = null ; } }
private void log ( final LogMessage . Stream stream , final String containerId , final JobId jobId , final StringBuilder stringBuilder ) { log . info ( "[{}] [{}] {} {}" , jobId . getName ( ) , containerId . substring ( 0 , Math . min ( 7 , containerId . length ( ) ) ) , stream . id ( ) , stringBuilder . toString ( ) ) ; stringBuilder . setLength ( 0 ) ; }
public void test() { if ( size > rangeFraction . getMaxSize ( ) . getBytes ( ) ) { LOG . info ( "The derived from fraction {} ({}) is greater than its max value {}, max value will be used instead" , memoryDescription , relative . toHumanReadableString ( ) , rangeFraction . getMaxSize ( ) . toHumanReadableString ( ) ) ; size = rangeFraction . getMaxSize ( ) . getBytes ( ) ; } else-if ( size < rangeFraction . getMinSize ( ) . getBytes ( ) ) { LOG . info ( "The derived from fraction {} ({}) is less than its min value {}, min value will be used instead" , memoryDescription , relative . toHumanReadableString ( ) , rangeFraction . getMinSize ( ) . toHumanReadableString ( ) ) ; size = rangeFraction . getMinSize ( ) . getBytes ( ) ; } }
public void test() { if ( size > rangeFraction . getMaxSize ( ) . getBytes ( ) ) { LOG . info ( "The derived from fraction {} ({}) is greater than its max value {}, max value will be used instead" , memoryDescription , relative . toHumanReadableString ( ) , rangeFraction . getMaxSize ( ) . toHumanReadableString ( ) ) ; size = rangeFraction . getMaxSize ( ) . getBytes ( ) ; } else-if ( size < rangeFraction . getMinSize ( ) . getBytes ( ) ) { LOG . info ( "The derived from fraction {} ({}) is less than its min value {}, min value will be used instead" , memoryDescription , relative . toHumanReadableString ( ) , rangeFraction . getMinSize ( ) . toHumanReadableString ( ) ) ; size = rangeFraction . getMinSize ( ) . getBytes ( ) ; } }
public void test() { try { code_block = IfStatement ; return _commerceBOMFolderModelResourcePermission . contains ( permissionChecker , commerceBOMFolder , ActionKeys . UPDATE ) ; } catch ( PortalException portalException ) { _log . error ( portalException , portalException ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "{} moveTo {} {} {}" , getName ( ) , topStom , midStom , lowStom ) ; } }
public void test() { try { listener . tagStyleChanged ( event ) ; } catch ( Exception ex ) { logger . error ( "unhandled exception in plugin on tag style changed" , ex ) ; } }
public void test() { try { PluginTagStyleEvent event = createTagStyleEvent ( e ) ; code_block = ForStatement ; } catch ( Exception ex ) { logger . error ( "Unknown error in plug-in interface when tag style was added" , ex ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Request success. request={}, result={}" , logString ( message ) , logString ( response ) ) ; } }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "Request fail. request={}, result={}" , logString ( message ) , logString ( response ) ) ; } }
public void test() { if ( numRetriesInARow >= request . getNumRetriesOnFailure ( ) . get ( ) ) { LOG . debug ( "Request {} had {} retries in a row, not retrying again (num retries on failure: {})" , request . getId ( ) , numRetriesInARow , request . getNumRetriesOnFailure ( ) ) ; return false ; } }
private void warnFragmented ( ByteBuffer buffer , int iterations , AtomicInteger counter , String operation ) { int total = counter . incrementAndGet ( ) ; _logger . warn ( "{} for buffer with length {} was fragmented {} times (total fragmented operations: {})" , operation , buffer . limit ( ) , iterations , total ) ; }
public PreparedCommand buildCommand ( MetaCommand mc , List < ArgumentAssignment > argAssignmentList , String origin , int seq , User user ) throws ErrorInCommand , YamcsException { log . debug ( "Building command {} with arguments {}" , mc . getName ( ) , argAssignmentList ) ; CommandBuildResult cbr = metaCommandProcessor . buildCommand ( mc , argAssignmentList ) ; CommandId cmdId = CommandId . newBuilder ( ) . setCommandName ( mc . getQualifiedName ( ) ) . setOrigin ( origin ) . setSequenceNumber ( seq ) . setGenerationTime ( processor . getCurrentTime ( ) ) . build ( ) ; PreparedCommand pc = new PreparedCommand ( cmdId ) ; pc . setMetaCommand ( mc ) ; pc . setBinary ( cbr . getCmdPacket ( ) ) ; pc . setUsername ( user . getName ( ) ) ; Set < String > userAssignedArgumentNames = argAssignmentList . stream ( ) . map ( a -> a . getArgumentName ( ) ) . collect ( Collectors . toSet ( ) ) ; pc . setArgAssignment ( cbr . getArgs ( ) , userAssignedArgumentNames ) ; return pc ; }
public void init ( ) { String methodName = "init" ; LOGGER . trace ( ENTERING , methodName ) ; LOGGER . trace ( EXITING , methodName ) ; }
public void init ( ) { String methodName = "init" ; LOGGER . trace ( ENTERING , methodName ) ; LOGGER . trace ( EXITING , methodName ) ; }
public void test() { try { connector . sendCommand ( NuvoCommand . CFGTIME . getValue ( ) + DATE_FORMAT . format ( new Date ( ) ) ) ; } catch ( NuvoException e ) { logger . debug ( "Error syncing clock: {}" , e . getMessage ( ) ) ; } }
public void test() { try { fetchWeatherInfo ( latitude , longitude , closestCity . geo_name , date ) ; } catch ( Exception e ) { logger . warn ( "action=fetchWeather error date=" + date + ", lat=" + latitude + ", lon=" + longitude + ", city=" + closestCity . geo_name ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( String . format ( "A cluster monitor is not found in autoscaler context " + "[cluster] %s" , clusterId ) ) ; } }
public void test() { if ( debugEnabled ) { logger . debug ( "{} channelActive()" , logPrefix ( ) ) ; } }
public void test() { if ( debugEnabled ) { logger . debug ( "{} channelActive() done" , logPrefix ( ) ) ; } }
protected Response getPort ( String nodeId , String portId ) { log . debug ( "" ) ; Port port = topology . getPort ( nodeId , portId ) ; code_block = IfStatement ; return new Response ( Response . OK , port ) ; }
public void test() { if ( element == null ) { log . error ( "cannot update null element" ) ; return RequestStatus . FAILURE ; } }
public void test() { try { code_block = IfStatement ; return RequestStatus . SUCCESS ; } catch ( Exception e ) { log . error ( "error on updating " + element . getClass ( ) . getSimpleName ( ) + ": " + e . getMessage ( ) ) ; return RequestStatus . FAILURE ; } }
public void test() { if ( contextChain . addAuxiliaryConnection ( connectionContext ) ) { LOG . info ( "An auxiliary connection was added to device: {}" , deviceInfo ) ; return ConnectionStatus . MAY_CONTINUE ; } else { LOG . warn ( "Not able to add auxiliary connection to the device {}" , deviceInfo ) ; return ConnectionStatus . REFUSING_AUXILIARY_CONNECTION ; } }
public void test() { if ( contextChain . addAuxiliaryConnection ( connectionContext ) ) { LOG . info ( "An auxiliary connection was added to device: {}" , deviceInfo ) ; return ConnectionStatus . MAY_CONTINUE ; } else { LOG . warn ( "Not able to add auxiliary connection to the device {}" , deviceInfo ) ; return ConnectionStatus . REFUSING_AUXILIARY_CONNECTION ; } }
public void test() { try ( RDF4JProtocolSession protocolSession = getSharedHttpClientSessionManager ( ) . createRDF4JProtocolSession ( serverURL ) ) { protocolSession . setUsernameAndPassword ( username , password ) ; protocolSession . deleteRepository ( repositoryID ) ; } catch ( IOException e ) { logger . warn ( "error while deleting remote repository" , e ) ; throw new RepositoryConfigException ( e ) ; } }
public void test() { try { return _assetEntryService . getEntries ( assetEntryQuery ) ; } catch ( Exception exception ) { _log . error ( "Unable to get asset entries" , exception ) ; } }
@ Override public Optional < Page > getGroupPage ( String groupId , String pageUrl ) throws IOException { LOG . debug ( "retrieving page " + pageUrl + " for group " + groupId ) ; String encodedUrl = URLEncoder . encode ( pageUrl , CanvasConstants . URLENCODING_TYPE ) ; String url = buildCanvasUrl ( "groups/" + groupId + "/pages/" + encodedUrl , Collections . emptyMap ( ) ) ; Response response = canvasMessenger . getSingleResponseFromCanvas ( oauthToken , url ) ; return responseParser . parseToObject ( Page . class , response ) ; }
public void test() { try { Message msg = currentConnector . getWithoutAck ( batchSize , timeout , unit ) ; return msg ; } catch ( Throwable t ) { logger . warn ( String . format ( "something goes wrong when getWithoutAck data from server:%s" , currentConnector != null ? currentConnector . getAddress ( ) : "null" ) , t ) ; times ++ ; restart ( ) ; logger . info ( "restart the connector for next round retry." ) ; } }
public void test() { try { Message msg = currentConnector . getWithoutAck ( batchSize , timeout , unit ) ; return msg ; } catch ( Throwable t ) { logger . warn ( String . format ( "something goes wrong when getWithoutAck data from server:%s" , currentConnector != null ? currentConnector . getAddress ( ) : "null" ) , t ) ; times ++ ; restart ( ) ; logger . info ( "restart the connector for next round retry." ) ; } }
public void test() { if ( retryInterval > 0 ) { LOG . info ( "Failed to send audit message:" , e ) ; spoolMessage ( msg ) ; scheduleRetry ( ) ; } else { throw e ; } }
@ Override public synchronized void init ( HiveConf hiveConf ) { ml = new LensMLImpl ( hiveConf ) ; ml . init ( hiveConf ) ; super . init ( hiveConf ) ; serviceProviderFactory = getServiceProviderFactory ( hiveConf ) ; log . info ( "Inited ML service" ) ; }
@ Override public void onMode ( String string , IRCUser ircUser , IRCModeParser ircModeParser ) { super . onMode ( string , ircUser , ircModeParser ) ; LOG . info ( "onMode.string = " + string ) ; LOG . info ( "onMode.ircUser = " + ircUser ) ; LOG . info ( "onMode.ircModeParser = " + ircModeParser ) ; }
@ Override public void onMode ( String string , IRCUser ircUser , IRCModeParser ircModeParser ) { super . onMode ( string , ircUser , ircModeParser ) ; LOG . info ( "onMode.string = " + string ) ; LOG . info ( "onMode.ircUser = " + ircUser ) ; LOG . info ( "onMode.ircModeParser = " + ircModeParser ) ; }
@ Override public void onMode ( String string , IRCUser ircUser , IRCModeParser ircModeParser ) { super . onMode ( string , ircUser , ircModeParser ) ; LOG . info ( "onMode.string = " + string ) ; LOG . info ( "onMode.ircUser = " + ircUser ) ; LOG . info ( "onMode.ircModeParser = " + ircModeParser ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Unable to get Cqs. Error: {}" , cqe . getMessage ( ) , cqe ) ; } }
public void test() { if ( session != null ) { session . setAttribute ( "user" , "999998" ) ; } else { log . error ( "session was null" ) ; } }
public void test() { if ( orderType == null ) { orderType = myMixin . deserializeToken ( orderTypeId ) ; } }
public void test() { try { spawnMQ . sendControlMessage ( new HostState ( HostMessage . ALL_HOSTS ) ) ; } catch ( Exception e ) { log . warn ( "unable to request host state update: " , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "abortTest: Disconnecting from distributed system and sending null chunk to abort" ) ; } }
public void test() { if ( error != null ) { LOG . warn ( "Failed to locate region in '" + tableName + "', row='" + Bytes . toStringBinary ( req . row ) + "', locateType=" + req . locateType , error ) ; } }
public void test() { try { int returnValue = CommerceNotificationQueueEntryServiceUtil . getCommerceNotificationQueueEntriesCount ( groupId ) ; return returnValue ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( listeners . isEmpty ( ) ) { logger . warn ( "Received AF_INCOMING_MSG but no listeners. " + "Message was from {} and cluster {} to end point {}. Data: {}" , msg . getSrcAddr ( ) , msg . getClusterId ( ) , msg . getDstEndpoint ( ) , msg ) ; } else { logger . trace ( "Received AF_INCOMING_MSG from {} and cluster {} to end point {}. Data: {}" , msg . getSrcAddr ( ) , msg . getClusterId ( ) , msg . getDstEndpoint ( ) , msg ) ; } }
public void test() { if ( listeners . isEmpty ( ) ) { logger . warn ( "Received AF_INCOMING_MSG but no listeners. " + "Message was from {} and cluster {} to end point {}. Data: {}" , msg . getSrcAddr ( ) , msg . getClusterId ( ) , msg . getDstEndpoint ( ) , msg ) ; } else { logger . trace ( "Received AF_INCOMING_MSG from {} and cluster {} to end point {}. Data: {}" , msg . getSrcAddr ( ) , msg . getClusterId ( ) , msg . getDstEndpoint ( ) , msg ) ; } }
public void test() { try { listener . notify ( msg ) ; } catch ( final Exception e ) { logger . error ( "Error AF message listener notify." , e ) ; } }
public void test() { try { List < RadiusClient > radiusclients = null ; code_block = IfStatement ; radiusclients . sort ( Comparator . comparing ( RadiusClient :: getName ) ) ; this . results . clear ( ) ; code_block = ForStatement ; this . oldSearchPattern = this . searchPattern ; this . searchPattern = "" ; } catch ( Exception e ) { log . debug ( "Failed to find radius clients" , e ) ; facesMessages . add ( FacesMessage . SEVERITY_ERROR , "{msgs['radius.clients.search.error']}" ) ; conversationService . endConversation ( ) ; return OxTrustConstants . RESULT_FAILURE ; } }
public void test() { try { listener . gotOAuthAccessToken ( token ) ; } catch ( Exception e ) { logger . warn ( "Exception at getOAuthAccessTokenAsync" , e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { LOG . error ( "Error for model: {} and context: {}" , model , dataObject , e ) ; } }
public void test() { try { wifiInterfaceAddressConfig = ( WifiInterfaceAddressConfig ) ( ( AbstractNetInterface < ? > ) wifiInterfaceConfig ) . getNetInterfaceAddressConfig ( ) ; wifiMode = wifiInterfaceAddressConfig . getMode ( ) ; } catch ( KuraException e ) { logger . error ( "Failed to obtain WifiInterfaceMode" , e ) ; } }
public void test() { try { edlController . setEdlGlobalConfig ( edlGlobalConfig ) ; edlController . setDefaultDOM ( getDefaultDOM ( edlAssociation ) ) ; loadConfigProcessors ( edlController , edlGlobalConfig ) ; loadPreProcessors ( edlController , edlGlobalConfig ) ; loadPostProcessor ( edlController , edlGlobalConfig ) ; loadStateComponents ( edlController , edlGlobalConfig ) ; loadStyle ( edlController ) ; } catch ( Exception e ) { String edl = null ; code_block = IfStatement ; String message = "Error creating controller for EDL" + ( edl == null ? "" : ": " + edl ) ; LOG . error ( message , e ) ; throw new WorkflowRuntimeException ( "Problems creating controller for EDL: " + edl , e ) ; } }
public void setPassivator ( final ConnectionPassivator p ) { logger . trace ( "setting passivator: {}" , p ) ; passivator = p ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Metadata update was skipped [typeId=" + typeId + ", typeName=" + newMeta . typeName ( ) + ']' ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { IgniteInternalTx tx = ctx . cache ( ) . context ( ) . tm ( ) . tx ( ) ; log . debug ( "Completed metadata update [typeId=" + typeId + ", typeName=" + newMeta . typeName ( ) + ", waitTime=" + MILLISECONDS . convert ( System . nanoTime ( ) - t0 , NANOSECONDS ) + "ms" + ", fut=" + fut + ", tx=" + CU . txString ( tx ) + ']' ) ; } }
public void test() { try { log . debug ( "Trying AK 2.2 SslFactory methods." ) ; return ( SSLContext ) SslFactory . class . getDeclaredMethod ( "sslContext" ) . invoke ( sslFactory ) ; } catch ( Exception e ) { log . debug ( "Could not find AK 2.2 SslFactory methods. Trying AK 2.3+ methods for SslFactory." ) ; Object sslEngine ; code_block = TryStatement ;  code_block = TryStatement ;  } }
public void test() { try { sslEngine = SslFactory . class . getDeclaredMethod ( "sslEngineBuilder" ) . invoke ( sslFactory ) ; log . debug ( "Using AK 2.2-2.5 SslFactory methods." ) ; } catch ( Exception ex ) { log . debug ( "Could not find AK 2.3-2.5 SslFactory methods. Trying AK 2.6+ methods for SslFactory." ) ; code_block = TryStatement ;  } }
public void test() { if ( null != pendingSlotRequest ) { pendingSlotRequest . setRequestFuture ( null ) ; code_block = TryStatement ;  } else { LOG . debug ( "There was not pending slot request with allocation id {}. Probably the request has been fulfilled or cancelled." , allocationId ) ; } }
public void test() { try { synchronized ( runState ) code_block = "" ; log . debug ( "Adapter [" + name + "] is stopping receivers" ) ; code_block = ForStatement ; code_block = ForStatement ; int currentNumOfMessagesInProcess = getNumOfMessagesInProcess ( ) ; code_block = IfStatement ; waitForNoMessagesInProcess ( ) ; log . debug ( "Adapter [" + name + "] is stopping pipeline" ) ; pipeline . stop ( ) ; statsUpSince = 0 ; runState . setRunState ( RunStateEnum . STOPPED ) ; getMessageKeeper ( ) . add ( "Adapter [" + name + "] stopped" ) ; } catch ( Throwable t ) { addErrorMessageToMessageKeeper ( "got error stopping Adapter" , t ) ; runState . setRunState ( RunStateEnum . ERROR ) ; } finally { configuration . removeStopAdapterThread ( this ) ; } }
public void test() { try { Thread . sleep ( 1000 ) ; } catch ( InterruptedException e ) { log . warn ( "Interrupted waiting for threads of receiver [" + receiver . getName ( ) + "] to end" , e ) ; } }
public void test() { try { synchronized ( runState ) code_block = "" ; log . debug ( "Adapter [" + name + "] is stopping receivers" ) ; code_block = ForStatement ; code_block = ForStatement ; int currentNumOfMessagesInProcess = getNumOfMessagesInProcess ( ) ; code_block = IfStatement ; waitForNoMessagesInProcess ( ) ; log . debug ( "Adapter [" + name + "] is stopping pipeline" ) ; pipeline . stop ( ) ; statsUpSince = 0 ; runState . setRunState ( RunStateEnum . STOPPED ) ; getMessageKeeper ( ) . add ( "Adapter [" + name + "] stopped" ) ; } catch ( Throwable t ) { addErrorMessageToMessageKeeper ( "got error stopping Adapter" , t ) ; runState . setRunState ( RunStateEnum . ERROR ) ; } finally { configuration . removeStopAdapterThread ( this ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "<== HBaseResourceMgr.connectionTest() ServiceName: " + serviceName + "Configs" + configs ) ; } }
public void test() { try { ret = HBaseClient . connectionTest ( serviceName , configs ) ; } catch ( HadoopException e ) { LOG . error ( "<== HBaseResourceMgr.connectionTest() Error: " + e ) ; throw e ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "<== HBaseResourceMgr.connectionTest() Result: " + ret ) ; } }
public void test() { if ( reader . peek ( ) == JsonToken . NULL ) { reader . skipValue ( ) ; } else-if ( name . equals ( VALUE ) ) { scope . setValue ( reader . nextString ( ) ) ; } else-if ( name . equals ( DESCRIPTION ) ) { scope . setDescription ( reader . nextString ( ) ) ; } else-if ( name . equals ( RESTRICTED ) ) { scope . setRestricted ( reader . nextBoolean ( ) ) ; } else-if ( name . equals ( DEFAULT_SCOPE ) ) { scope . setDefaultScope ( reader . nextBoolean ( ) ) ; } else-if ( name . equals ( ICON ) ) { scope . setIcon ( reader . nextString ( ) ) ; } else-if ( name . equals ( STRUCTURED ) ) { logger . warn ( "Found a structured scope, ignoring structure" ) ; } else-if ( name . equals ( STRUCTURED_PARAMETER ) ) { logger . warn ( "Found a structured scope, ignoring structure" ) ; } else { logger . debug ( "found unexpected entry" ) ; reader . skipValue ( ) ; } }
public void test() { switch ( reader . peek ( ) ) { case END_OBJECT : continue ; case NAME : String name = reader . nextName ( ) ; code_block = IfStatement ; break ; default : logger . debug ( "Found unexpected entry" ) ; reader . skipValue ( ) ; continue ; } }
private void writeCommandToDevice ( RequestMessage requestMessage ) throws IOException { logger . trace ( "Processor for thing {} writing command to device" , thingID ( ) ) ; code_block = IfStatement ; byte [ ] deviceCommand = ( requestMessage . getDeviceCommand ( ) + '\r' ) . getBytes ( ) ; connectionManager . getCommandOut ( ) . write ( deviceCommand ) ; connectionManager . getCommandOut ( ) . flush ( ) ; }
private void generateCoordinates ( Vector2d firstBondVector , boolean isConnected , boolean isSubLayout ) throws CDKException { if ( firstBondVector == DEFAULT_BOND_VECTOR ) firstBondVector = new Vector2d ( firstBondVector ) ; final int numAtoms = molecule . getAtomCount ( ) ; final int numBonds = molecule . getBondCount ( ) ; this . firstBondVector = firstBondVector ; logger . debug ( "Entry point of generateCoordinates()" ) ; logger . debug ( "We have a molecules with " + numAtoms + " atoms." ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; seedLayout ( ) ; int iter = 0 ; code_block = ForStatement ; if ( iter == numAtoms && ! AtomPlacer . allPlaced ( molecule ) ) throw new CDKException ( "Could not generate layout? If a set of 'fixed' atoms were provided" + " try removing these and regenerating the layout." ) ; code_block = IfStatement ; refinePlacement ( molecule ) ; finalizeLayout ( molecule ) ; if ( ! isSubLayout ) assignStereochem ( molecule ) ; }
private void generateCoordinates ( Vector2d firstBondVector , boolean isConnected , boolean isSubLayout ) throws CDKException { if ( firstBondVector == DEFAULT_BOND_VECTOR ) firstBondVector = new Vector2d ( firstBondVector ) ; final int numAtoms = molecule . getAtomCount ( ) ; final int numBonds = molecule . getBondCount ( ) ; this . firstBondVector = firstBondVector ; logger . debug ( "Entry point of generateCoordinates()" ) ; logger . debug ( "We have a molecules with " + numAtoms + " atoms." ) ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; seedLayout ( ) ; int iter = 0 ; code_block = ForStatement ; if ( iter == numAtoms && ! AtomPlacer . allPlaced ( molecule ) ) throw new CDKException ( "Could not generate layout? If a set of 'fixed' atoms were provided" + " try removing these and regenerating the layout." ) ; code_block = IfStatement ; refinePlacement ( molecule ) ; finalizeLayout ( molecule ) ; if ( ! isSubLayout ) assignStereochem ( molecule ) ; }
public void test() { if ( logger . isTraceEnabled ( LogMarker . SERIALIZER_VERBOSE ) ) { logger . trace ( LogMarker . SERIALIZER_VERBOSE , "Writing Boolean {}" , value ) ; } }
public void attachDirty ( MbStatus instance ) { log . debug ( "attaching dirty MbStatus instance" ) ; code_block = TryStatement ;  }
public void test() { try { getSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { getSession ( ) . saveOrUpdate ( instance ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { listTB = getDirectConfigProxy ( ) . getTrustBundles ( fetchAnchors ) ; } catch ( Exception ex ) { LOG . error ( "Unable to get List of Trust Bundles: " , ex ) ; } }
public void test() { try { return CryptoUtils . decrypt ( message , CLOUD_KEY ) ; } catch ( GeneralSecurityException e ) { logger . error ( LoggingAnchor . THREE , MessageEnum . RA_GENERAL_EXCEPTION . toString ( ) , ErrorCode . BusinessProcessError . getValue ( ) , "Exception in encryptPassword " , e ) ; return null ; } }
@ NamespacePermission ( fields = "emrClusterDefinitionKey?.namespace" , permissions = NamespacePermissionEnum . WRITE ) @ Override public EmrClusterDefinitionInformation deleteEmrClusterDefinition ( EmrClusterDefinitionKey emrClusterDefinitionKey ) throws Exception { emrClusterDefinitionHelper . validateEmrClusterDefinitionKey ( emrClusterDefinitionKey ) ; EmrClusterDefinitionEntity emrClusterDefinitionEntity = emrClusterDefinitionDaoHelper . getEmrClusterDefinitionEntity ( emrClusterDefinitionKey ) ; LOGGER . info ( "Logging EMR cluster definition being deleted. emrClusterDefinition={}" , xmlHelper . objectToXml ( createEmrClusterDefinitionFromEntity ( emrClusterDefinitionEntity ) , true ) ) ; emrClusterDefinitionDao . delete ( emrClusterDefinitionEntity ) ; return createEmrClusterDefinitionFromEntity ( emrClusterDefinitionEntity ) ; }
public void test() { if ( logMessage != null && ! logMessage . isEmpty ( ) ) { log . info ( "{} - {} socket {}" , reset ? "Resetting" : "Closing" , logMessage , socket ) ; } else { log . debug ( "{} socket {}" , reset ? "Resetting" : "Closing" , socket ) ; } }
public void test() { if ( logMessage != null && ! logMessage . isEmpty ( ) ) { log . info ( "{} - {} socket {}" , reset ? "Resetting" : "Closing" , logMessage , socket ) ; } else { log . debug ( "{} socket {}" , reset ? "Resetting" : "Closing" , socket ) ; } }
@ Test public void testCreateOrder ( ) throws Exception { Order order = new Order ( ) ; order . setAmount ( 1 ) ; order . setPartName ( "motor" ) ; order . setCustomerName ( "honda" ) ; String xml = context . getTypeConverter ( ) . convertTo ( String . class , order ) ; log . info ( "Sending order using xml payload: {}" , xml ) ; String id = template . requestBody ( "http://localhost:8080/orders" , xml , String . class ) ; assertNotNull ( id ) ; log . info ( "Created new order with id " + id ) ; assertEquals ( "3" , id ) ; }
@ Test public void testCreateOrder ( ) throws Exception { Order order = new Order ( ) ; order . setAmount ( 1 ) ; order . setPartName ( "motor" ) ; order . setCustomerName ( "honda" ) ; String xml = context . getTypeConverter ( ) . convertTo ( String . class , order ) ; log . info ( "Sending order using xml payload: {}" , xml ) ; String id = template . requestBody ( "http://localhost:8080/orders" , xml , String . class ) ; assertNotNull ( id ) ; log . info ( "Created new order with id " + id ) ; assertEquals ( "3" , id ) ; }
public void test() { try { Class . forName ( driverClassName ) ; } catch ( ClassNotFoundException e ) { logger . error ( "Failed to initialize JDBC driver class '" + driverClassName + "'!" , e ) ; } }
public void test() { try { String message ; code_block = IfStatement ; code_block = IfStatement ; Window window = SwingUtilities . getWindowAncestor ( this ) ; code_block = IfStatement ; } catch ( PrivateKeyProviderException e ) { LOGGER . warn ( "Could not get certificate from provider: " + e , e ) ; } }
public void test() { try { constructor = pageClass . getDeclaredConstructor ( new Class [ ] code_block = "" ; ) ; Constructor < C > tmpConstructor = ( Constructor < C > ) constructorForClass . putIfAbsent ( pageClass , constructor ) ; code_block = IfStatement ; log . debug ( "Found constructor for Page of type '{}' and argument of type '{}'." , pageClass , argumentType ) ; } catch ( NoSuchMethodException e ) { log . debug ( "Page of type '{}' has not visible constructor with an argument of type '{}'." , pageClass , argumentType ) ; return null ; } }
public void handleGetLightSensorStatusResponse ( final LightSensorStatusDto lightSensorStatusDto , final CorrelationIds ids , final String messageType , final int messagePriority , final ResponseMessageResultType deviceResult , final OsgpException exception ) { LOGGER . info ( "handleResponse for MessageType: {}" , messageType ) ; final GetLightSensorStatusResponse response = new GetLightSensorStatusResponse ( ) ; response . setOsgpException ( exception ) ; response . setResult ( deviceResult ) ; code_block = IfStatement ; code_block = IfStatement ; final ResponseMessage responseMessage = ResponseMessage . newResponseMessageBuilder ( ) . withIds ( ids ) . withResult ( response . getResult ( ) ) . withOsgpException ( response . getOsgpException ( ) ) . withDataObject ( response . getLightSensorStatus ( ) ) . withMessagePriority ( messagePriority ) . build ( ) ; code_block = IfStatement ; }
public void test() { if ( deviceResult == ResponseMessageResultType . NOT_OK || exception != null ) { LOGGER . error ( "Device Response not ok." , exception ) ; } }
public void test() { if ( ! OsgpSystemCorrelationUid . CORRELATION_UID . equals ( ids . getCorrelationUid ( ) ) ) { this . webServiceResponseMessageSender . send ( responseMessage ) ; } else { LOGGER . info ( "We used sensor status to keep 104 LMDs connected, ignore response: {}" , responseMessage ) ; } }
public void test() { try { wireGraphService . delete ( ) ; } catch ( KuraException e ) { logger . error ( "Test error" , e ) ; throw e ; } }
public MbB2mDel merge ( MbB2mDel detachedInstance ) { log . debug ( "merging MbB2mDel instance" ) ; code_block = TryStatement ;  }
public void test() { try { MbB2mDel result = ( MbB2mDel ) sessionFactory . getCurrentSession ( ) . merge ( detachedInstance ) ; log . debug ( "merge successful" ) ; return result ; } catch ( RuntimeException re ) { log . error ( "merge failed" , re ) ; throw re ; } }
public void test() { try { MbB2mDel result = ( MbB2mDel ) sessionFactory . getCurrentSession ( ) . merge ( detachedInstance ) ; log . debug ( "merge successful" ) ; return result ; } catch ( RuntimeException re ) { log . error ( "merge failed" , re ) ; throw re ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( LayoutPageTemplateEntryServiceUtil . class , "addLayoutPageTemplateEntry" , _addLayoutPageTemplateEntryParameterTypes0 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , layoutPageTemplateCollectionId , classNameId , classTypeId , name , status , serviceContext ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . layout . page . template . model . LayoutPageTemplateEntry ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( calendar == null ) { log . info ( "calendar empty, returning empty set" ) ; return Collections . emptySet ( ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "processing event " + event . getSummary ( ) . getValue ( ) ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "added event " + newevent ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "The policy of region is {}" , ( this . enablePersistence ? DataPolicy . PERSISTENT_REPLICATE : DataPolicy . REPLICATE ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "{}: Attempting to create queue region: {}" , this , this . regionName ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "{}: Created queue region: {}" , this , this . region ) ; } }
public void test() { try ( ClassicHttpResponse httpResponse = call . execute ( ) ) { code_block = IfStatement ; } catch ( final IOException ex ) { jobFailed ( cacheKey ) ; LOG . debug ( "Asynchronous revalidation failed due to I/O error" , ex ) ; } catch ( final HttpException ex ) { jobFailed ( cacheKey ) ; LOG . error ( "HTTP protocol exception during asynchronous revalidation" , ex ) ; } catch ( final RuntimeException ex ) { jobFailed ( cacheKey ) ; LOG . error ( "Unexpected runtime exception thrown during asynchronous revalidation" , ex ) ; } }
public void test() { try ( ClassicHttpResponse httpResponse = call . execute ( ) ) { code_block = IfStatement ; } catch ( final IOException ex ) { jobFailed ( cacheKey ) ; LOG . debug ( "Asynchronous revalidation failed due to I/O error" , ex ) ; } catch ( final HttpException ex ) { jobFailed ( cacheKey ) ; LOG . error ( "HTTP protocol exception during asynchronous revalidation" , ex ) ; } catch ( final RuntimeException ex ) { jobFailed ( cacheKey ) ; LOG . error ( "Unexpected runtime exception thrown during asynchronous revalidation" , ex ) ; } }
public void test() { try ( ClassicHttpResponse httpResponse = call . execute ( ) ) { code_block = IfStatement ; } catch ( final IOException ex ) { jobFailed ( cacheKey ) ; LOG . debug ( "Asynchronous revalidation failed due to I/O error" , ex ) ; } catch ( final HttpException ex ) { jobFailed ( cacheKey ) ; LOG . error ( "HTTP protocol exception during asynchronous revalidation" , ex ) ; } catch ( final RuntimeException ex ) { jobFailed ( cacheKey ) ; LOG . error ( "Unexpected runtime exception thrown during asynchronous revalidation" , ex ) ; } }
public void test() { try { FutureTask < Void > future = new FutureTask < > ( run , null ) ; Platform . runLater ( future ) ; future . get ( ) ; } catch ( ExecutionException | InterruptedException e ) { logger . error ( "invokeInFxThreadAndWait() failed" , e ) ; } }
public void test() { { String targetPackageId = "target1" ; addTargetPackage ( targetPackageId ) ; ResourceIdentifier id = ResourceIdentifier . create ( ResourceType . PACKAGE , PACKAGE_A ) ; TestProgress progress = new TestProgress ( ) ; copyService . copy ( singletonList ( id ) , targetPackageId , progress ) ; await ( ) . atMost ( 5 , TimeUnit . SECONDS ) . until ( copyJobFinished ( progress ) ) ; LOG . info ( "Copy job progress: {}/{}" , progress . getProgress ( ) , progress . getProgressMax ( ) ) ; waitForWorkToBeFinished ( indexService , LOG ) ; Package targetPackage = metadataService . getPackage ( targetPackageId ) . get ( ) ; List < Package > packages = newArrayList ( targetPackage . getChildren ( ) ) ; assertEquals ( 1 , packages . size ( ) ) ; Package packageACopy = packages . get ( 0 ) ; assertEquals ( "Package A" , packageACopy . getLabel ( ) ) ; List < EntityType > entityTypesInACopy = newArrayList ( packageACopy . getEntityTypes ( ) ) ; List < Package > packagesInACopy = newArrayList ( packageACopy . getChildren ( ) ) ; assertEquals ( 1 , entityTypesInACopy . size ( ) ) ; assertEquals ( 1 , packagesInACopy . size ( ) ) ; Package packageBCopy = packagesInACopy . get ( 0 ) ; assertEquals ( "Package B (child of A)" , packageBCopy . getLabel ( ) ) ; List < EntityType > entityTypesInBCopy = newArrayList ( packageBCopy . getEntityTypes ( ) ) ; List < Package > packagesInBCopy = newArrayList ( packageBCopy . getChildren ( ) ) ; assertEquals ( 1 , entityTypesInBCopy . size ( ) ) ; assertEquals ( 0 , packagesInBCopy . size ( ) ) ; EntityType entityTypeACopy = entityTypesInACopy . get ( 0 ) ; EntityType entityTypeBCopy = entityTypesInBCopy . get ( 0 ) ; assertEquals ( "EntityType A" , entityTypeACopy . getLabel ( ) ) ; assertEquals ( "EntityType B (referenced by A)" , entityTypeBCopy . getLabel ( ) ) ; assertEquals ( ENTITY_TYPE_B , entityTypeA . getAttribute ( "xref_attr" ) . getRefEntity ( ) . getId ( ) ) ; assertEquals ( entityTypeBCopy . getId ( ) , entityTypeACopy . getAttribute ( "xref_attr" ) . getRefEntity ( ) . getId ( ) ) ; assertEquals ( 4 , progress . getProgress ( ) ) ; List < Object > entitiesOfA = dataService . findAll ( entityTypeACopy . getId ( ) ) . map ( Entity :: getIdValue ) . collect ( toList ( ) ) ; assertEquals ( asList ( "0" , "1" , "2" ) , entitiesOfA ) ; List < Object > entitiesOfB = dataService . findAll ( entityTypeBCopy . getId ( ) ) . map ( Entity :: getIdValue ) . collect ( toList ( ) ) ; assertEquals ( asList ( "0" , "1" , "2" ) , entitiesOfB ) ; cleanupTargetPackage ( targetPackageId ) ; } }
public void test() { try { return RefreshScopeConfigurationScaleTests . this . service . getMessage ( ) ; } finally { latch . countDown ( ) ; logger . debug ( "Background done." ) ; } }
@ Override public void info ( String string , Object o ) { logger . info ( string , o ) ; }
public void test() { if ( pluginName == null ) { logger . warn ( String . format ( "Failed to dispatch event %s : Input must have a pluginName or a valid pluginKey specified json=%s" , commandType , metadata . getEventJson ( ) ) ) ; return ; } }
public void setProcessVariable ( String containerId , Number processInstanceId , String varName , String variablePayload , String marshallingType ) { containerId = context . getContainerId ( containerId , new ByProcessInstanceIdContainerLocator ( processInstanceId . longValue ( ) ) ) ; logger . debug ( "About to unmarshal variable from payload: '{}'" , variablePayload ) ; Object variable = marshallerHelper . unmarshal ( containerId , variablePayload , marshallingType , Object . class ) ; logger . debug ( "Setting variable '{}' on process instance with id {} with value {}" , varName , processInstanceId , variable ) ; processService . setProcessVariable ( containerId , processInstanceId . longValue ( ) , varName , variable ) ; }
public void setProcessVariable ( String containerId , Number processInstanceId , String varName , String variablePayload , String marshallingType ) { containerId = context . getContainerId ( containerId , new ByProcessInstanceIdContainerLocator ( processInstanceId . longValue ( ) ) ) ; logger . debug ( "About to unmarshal variable from payload: '{}'" , variablePayload ) ; Object variable = marshallerHelper . unmarshal ( containerId , variablePayload , marshallingType , Object . class ) ; logger . debug ( "Setting variable '{}' on process instance with id {} with value {}" , varName , processInstanceId , variable ) ; processService . setProcessVariable ( containerId , processInstanceId . longValue ( ) , varName , variable ) ; }
@ Test public void testDispatchingJobsHigherMaxLoad ( ) throws Exception { logger . debug ( "KHD start of testDispatchingJobsHigherMaxLoad" ) ; Job testJob = serviceRegistryJpaImpl . createJob ( TEST_HOST , TEST_SERVICE_FAIRNESS , TEST_OPERATION , null , null , true , null , 10.0f ) ; JobBarrier barrier = new JobBarrier ( null , serviceRegistryJpaImpl , testJob ) ; launchDispatcherOnce ( false ) ; assertThrows ( IllegalStateException . class , ( ) -> barrier . waitForJobs ( JOB_BARRIER_TIMEOUT ) ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( String . format ( "Cancel action %s" , action ) ) ; } }
public void test() { { log . info ( "JMXFetch is closing" ) ; LogManager . shutdown ( ) ; } }
public void test() { try { commands . add ( CommandFactory . create ( initialContext , GtfsDisposeImportCommand . class . getName ( ) ) ) ; } catch ( Exception e ) { log . error ( e , e ) ; throw new RuntimeException ( "unable to call factories" ) ; } }
protected String createWorkDirectory ( String location ) { code_block = IfStatement ; log . info ( "creating work directory {}" , location ) ; File f = new File ( location ) ; f . mkdirs ( ) ; return location ; }
@ EventListener public void onApplicationEvent ( ApplicationReadyEvent aEvt ) { log . info ( "Console: " + ( ( System . console ( ) != null ) ? "available" : "not available" ) ) ; log . info ( "Headless: " + ( GraphicsEnvironment . isHeadless ( ) ? "yes" : "no" ) ) ; code_block = IfStatement ; }
public void test() { try { return ResponseUtils . buildSucessResponse ( userRolesService . updateUserRole ( roleDetailsRequest , user . getName ( ) ) ) ; } catch ( Exception exception ) { log . error ( UNEXPECTED_ERROR_OCCURRED , exception ) ; return ResponseUtils . buildFailureResponse ( new Exception ( UNEXPECTED_ERROR_OCCURRED ) , exception . getMessage ( ) ) ; } }
public void test() { if ( ! device . awaitServiceDiscovery ( 10 , TimeUnit . SECONDS ) ) { logger . debug ( "Service discovery for device {} timed out" , device . getAddress ( ) ) ; return null ; } }
public void test() { if ( ! device . awaitConnection ( 1 , TimeUnit . SECONDS ) ) { logger . debug ( "Connection to device {} timed out" , device . getAddress ( ) ) ; return null ; } }
public void test() { if ( ! device . awaitConnection ( 1 , TimeUnit . SECONDS ) ) { logger . debug ( "Connection to device {} timed out" , device . getAddress ( ) ) ; return null ; } }
public void test() { try { DiscoveryResult result = participant . createResult ( device ) ; code_block = IfStatement ; } catch ( RuntimeException e ) { logger . warn ( "Participant '{}' threw an exception" , participant . getClass ( ) . getName ( ) , e ) ; } }
public void test() { try { final File dataDir = new File ( nodeInternDir , CFG_DATA_DIR_NAME ) ; loadPieInternals ( dataDir , exec ) ; } catch ( final CanceledExecutionException e ) { throw e ; } catch ( final Exception e ) { LOGGER . debug ( "Error while loading internals: " + e . getMessage ( ) ) ; } }
private List < DownloadFile > downloadEnsemblData ( Path geneFolder ) throws IOException , InterruptedException { logger . info ( "Downloading gene Ensembl data (gtf, pep, cdna, motifs) ..." ) ; List < String > downloadedUrls = new ArrayList < > ( 4 ) ; List < DownloadFile > downloadFiles = new ArrayList < > ( ) ; String ensemblHost = ensemblHostUrl + "/" + ensemblRelease ; code_block = IfStatement ; String ensemblCollection = "" ; code_block = IfStatement ; String version = ensemblRelease . split ( "-" ) [ 1 ] ; String url = ensemblHost + "/gtf/" + ensemblCollection + speciesShortName + "/*" + version + ".gtf.gz" ; String fileName = geneFolder . resolve ( speciesShortName + ".gtf.gz" ) . toString ( ) ; downloadFiles . add ( downloadFile ( url , fileName ) ) ; downloadedUrls . add ( url ) ; url = ensemblHost + "/fasta/" + ensemblCollection + speciesShortName + "/pep/*.pep.all.fa.gz" ; fileName = geneFolder . resolve ( speciesShortName + ".pep.all.fa.gz" ) . toString ( ) ; downloadFiles . add ( downloadFile ( url , fileName ) ) ; downloadedUrls . add ( url ) ; url = ensemblHost + "/fasta/" + ensemblCollection + speciesShortName + "/cdna/*.cdna.all.fa.gz" ; fileName = geneFolder . resolve ( speciesShortName + ".cdna.all.fa.gz" ) . toString ( ) ; downloadFiles . add ( downloadFile ( url , fileName ) ) ; downloadedUrls . add ( url ) ; saveVersionData ( EtlCommons . GENE_DATA , ENSEMBL_NAME , ensemblVersion , getTimeStamp ( ) , downloadedUrls , geneFolder . resolve ( "ensemblCoreVersion.json" ) ) ; return downloadFiles ; }
public void test() { if ( null == scannerFactory ) { log . debug ( "ScannerFactory was never initialized because, therefore there are no connections to close: " + System . identityHashCode ( this ) ) ; } else { log . debug ( "Closing ShardQueryLogic scannerFactory: " + System . identityHashCode ( this ) ) ; code_block = TryStatement ;  } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cleaned up " + nClosed + " batch scanners associated with this query logic." ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Cleaned up " + nClosed + " scanner sessions." ) ; } }
public void test() { try { int nClosed = 0 ; scannerFactory . lockdown ( ) ; code_block = ForStatement ; code_block = IfStatement ; nClosed = 0 ; code_block = ForStatement ; code_block = IfStatement ; } catch ( Exception e ) { log . error ( "Caught exception trying to close scannerFactory" , e ) ; } }
public void test() { try { log . debug ( "Closing ShardQueryLogic planner: " + System . identityHashCode ( this ) + '(' + ( this . getSettings ( ) == null ? "empty" : this . getSettings ( ) . getId ( ) ) + ')' ) ; this . planner . close ( getConfig ( ) , this . getSettings ( ) ) ; } catch ( Exception e ) { log . error ( "Caught exception trying to close QueryPlanner" , e ) ; } }
public void test() { try { log . debug ( "Closing ShardQueryLogic queries: " + System . identityHashCode ( this ) ) ; this . queries . close ( ) ; } catch ( IOException e ) { log . error ( "Caught exception trying to close CloseableIterable of queries" , e ) ; } }
public void test() { try { log . debug ( "Closing ShardQueryLogic scheduler: " + System . identityHashCode ( this ) ) ; this . scheduler . close ( ) ; ScanSessionStats stats = this . scheduler . getSchedulerStats ( ) ; code_block = IfStatement ; } catch ( IOException e ) { log . error ( "Caught exception trying to close Scheduler" , e ) ; } }
public org . talend . mdm . webservice . WSTransformerPK putTransformer ( org . talend . mdm . webservice . WSPutTransformer arg0 ) { LOG . info ( "Executing operation putTransformer" ) ; System . out . println ( arg0 ) ; code_block = TryStatement ;  }
public void test() { if ( this . getLogger ( ) . isDebugEnabled ( ) ) { getLogger ( ) . debug ( "Shutting down HSQLDB" ) ; } }
public void test() { if ( this . getLogger ( ) . isDebugEnabled ( ) ) { getLogger ( ) . debug ( "Shutting down HSQLDB: Done" ) ; } }
public void test() { try { code_block = IfStatement ; code_block = ForStatement ; authorizeWithHiveBindings ( context , stmtAuthObject , stmtOperation ) ; } catch ( AuthorizationException e ) { executeOnFailureHooks ( context , stmtOperation , e ) ; StringBuilder permsBuilder = new StringBuilder ( ) ; code_block = ForStatement ; String permsRequired = permsBuilder . toString ( ) ; SessionState . get ( ) . getConf ( ) . set ( HiveAuthzConf . HIVE_SENTRY_AUTH_ERRORS , permsRequired ) ; String msgForLog = HiveAuthzConf . HIVE_SENTRY_PRIVILEGE_ERROR_MESSAGE + "\n Required privileges for this query: " + permsRequired ; String msgForConsole = HiveAuthzConf . HIVE_SENTRY_PRIVILEGE_ERROR_MESSAGE + "\n " + e . getMessage ( ) + "\n The required privileges: " + permsRequired ; LOG . info ( msgForLog ) ; throw new SemanticException ( msgForConsole , e ) ; } finally { hiveAuthzBinding . close ( ) ; } }
public void test() { try { task . run ( ) ; } catch ( Exception e ) { log . error ( "Failed to execute shutdownhook" , e ) ; } }
public void test() { try { code_block = IfStatement ; rocketMQConsumer . subscribe ( this . topic , this . filter ) ; rocketMQConsumer . registerMessageListener ( ( MessageListenerOrderly ) ( messageExts , context ) code_block = LoopStatement ; ) ; rocketMQConsumer . start ( ) ; } catch ( MQClientException ex ) { logger . error ( "Start RocketMQ consumer error" , ex ) ; } }
@ ApiOperation ( value = "Get access contract by ID" ) @ GetMapping ( path = RestApi . PATH_REFERENTIAL_ID ) public AccessContractDto getById ( final @ PathVariable ( "identifier" ) String identifier ) throws UnsupportedEncodingException { LOGGER . debug ( "getById {} / {}" , identifier , URLEncoder . encode ( identifier , StandardCharsets . UTF_8 . toString ( ) ) ) ; return service . getOne ( buildUiHttpContext ( ) , URLEncoder . encode ( identifier , StandardCharsets . UTF_8 . toString ( ) ) ) ; }
public void test() { try { String clazzName = ( String ) ( args . get ( 0 ) ) ; String methodName = ( String ) ( args . get ( 1 ) ) ; LOGGER . debug ( "XEditor extension function calling {} {}" , clazzName , methodName ) ; Class [ ] argTypes = new Class [ args . size ( ) - 2 ] ; Object [ ] params = new Object [ args . size ( ) - 2 ] ; code_block = ForStatement ; Class clazz = ClassUtils . getClass ( clazzName ) ; Method method = MethodUtils . getMatchingAccessibleMethod ( clazz , methodName , argTypes ) ; return method . invoke ( null , params ) ; } catch ( Exception ex ) { LOGGER . warn ( "Exception in call to external java method" , ex ) ; return ex . getMessage ( ) ; } }
public void test() { try { String clazzName = ( String ) ( args . get ( 0 ) ) ; String methodName = ( String ) ( args . get ( 1 ) ) ; LOGGER . debug ( "XEditor extension function calling {} {}" , clazzName , methodName ) ; Class [ ] argTypes = new Class [ args . size ( ) - 2 ] ; Object [ ] params = new Object [ args . size ( ) - 2 ] ; code_block = ForStatement ; Class clazz = ClassUtils . getClass ( clazzName ) ; Method method = MethodUtils . getMatchingAccessibleMethod ( clazz , methodName , argTypes ) ; return method . invoke ( null , params ) ; } catch ( Exception ex ) { LOGGER . warn ( "Exception in call to external java method" , ex ) ; return ex . getMessage ( ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Configure channel {} with method {} on bean {}" , channel , method , bean ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Configure again channel {} with method {} on bean {}" , channelName , method , bean ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Channel {} already initialized. Not called method {} on bean {}" , channelName , method , bean ) ; } }
public void test() { try { String result = pipeParser . encode ( message ) ; result = result . replace ( "\r" , "\r\n" ) ; result = result . replace ( "\r\r" , "\r" ) ; logger . error ( result ) ; } catch ( HL7Exception e ) { logger . error ( "Unexpected error." , e ) ; } }
public void test() { try { String result = pipeParser . encode ( message ) ; result = result . replace ( "\r" , "\r\n" ) ; result = result . replace ( "\r\r" , "\r" ) ; logger . error ( result ) ; } catch ( HL7Exception e ) { logger . error ( "Unexpected error." , e ) ; } }
public void removeTableFromGroup ( TableName groupName , String schemaName , String tableName ) { LOG . trace ( "removeTableFromGroup: " + groupName + ": " + schemaName + "." + tableName ) ; Group group = ais . getGroup ( groupName ) ; checkFound ( group , "removing join from group" , "group" , groupName ) ; Table table = ais . getTable ( schemaName , tableName ) ; checkFound ( table , "removing join from group" , "table table" , concat ( schemaName , tableName ) ) ; checkInGroup ( group , table , "removing join from group" , "table table" ) ; code_block = IfStatement ; setTablesGroup ( table , null ) ; }
public void test() { if ( ! Boolean . TRUE . equals ( operation ( ) . inNamespace ( namespace ) . withName ( resourceName . toString ( ) ) . withPropagationPolicy ( DeletionPropagation . FOREGROUND ) . delete ( ) ) ) { LOGGER . warn ( "KafkaTopic {} could not be deleted, since it doesn't seem to exist" , resourceName . toString ( ) ) ; future . complete ( ) ; } else { Util . waitFor ( vertx , "sync resource deletion " + resourceName , "deleted" , 1000 , Long . MAX_VALUE , ( ) code_block = LoopStatement ; ) . onComplete ( future ) ; } }
public void test() { -> { KafkaTopic kafkaTopic = operation ( ) . inNamespace ( namespace ) . withName ( resourceName . toString ( ) ) . get ( ) ; boolean notExists = kafkaTopic == null ; LOGGER . debug ( "KafkaTopic {} deleted {}" , resourceName . toString ( ) , notExists ) ; return notExists ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( AssetListEntryServiceUtil . class , "getAssetListEntriesCount" , _getAssetListEntriesCountParameterTypes20 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupIds ) ; Object returnObj = null ; code_block = TryStatement ;  return ( ( Integer ) returnObj ) . intValue ( ) ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( request . getType ( ) . equals ( Type . get ) ) { LOGGER . trace ( "Using search processor: SearchGet" ) ; this . searchGet . process ( request ) ; return ; } else-if ( request . getType ( ) . equals ( Type . set ) ) { LOGGER . trace ( "Using search processor: SearchSet" ) ; this . searchSet . process ( request ) ; return ; } }
public void test() { if ( request . getType ( ) . equals ( Type . get ) ) { LOGGER . trace ( "Using search processor: SearchGet" ) ; this . searchGet . process ( request ) ; return ; } else-if ( request . getType ( ) . equals ( Type . set ) ) { LOGGER . trace ( "Using search processor: SearchSet" ) ; this . searchSet . process ( request ) ; return ; } }
public void test() { try { ret = pagePlugin . render ( this , ret ) ; } catch ( Exception e ) { mLogger . error ( "ERROR from plugin: " + pagePlugin . getName ( ) , e ) ; } }
@ Override public void requestInitialized ( ServletRequestEvent servletRequestEvent ) { Request request = ( ( Request ) servletRequestEvent . getServletRequest ( ) ) ; LOG . info ( "Styx REST Interface incoming request uri={}, method={}" , request . getRequestURI ( ) , request . getMethod ( ) ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "String already represents JSON. Skipping conversion in favor of 'getBytes(StandardCharsets.UTF_8'." ) ; } }
public void test() { try { fetchedList = criteria . list ( ) ; return fetchedList ; } catch ( Exception e ) { logger . error ( "getByCriteriaWithAliasByOrder failed, criteria = " + criterion . toString ( ) , e ) ; throw new HibernateException ( "getByCriteriaWithAliasByOrder failed, criteria = " + criterion . toString ( ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Entered StatAlertsManager.updateAlertDefinition *****" ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Removed StatAlertDefinition: {}" , defns [ i ] . getName ( ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Exiting StatAlertsManager.updateAlertDefinition *****" ) ; } }
public List < CosemObject > filterMeterValues ( List < CosemObject > cosemObjects ) { logger . trace ( "supported identifiers: {}, searching for objects {}" , supportedIdentifiers , cosemObjects ) ; List < CosemObject > filteredValues = cosemObjects . stream ( ) . filter ( cosemObject -> supportedIdentifiers . contains ( cosemObject . getObisIdentifier ( ) . getReducedOBISIdentifier ( ) ) ) . collect ( Collectors . toList ( ) ) ; return filteredValues ; }
public void test() { try { final XsLocalNetwork nw = citrixResourceBase . getNetworkByName ( conn , label ) ; code_block = IfStatement ; s_logger . debug ( "Network object:" + nw . getNetwork ( ) . getUuid ( conn ) ) ; final PIF pif = nw . getPif ( conn ) ; final PIF . Record pifRec = pif . getRecord ( conn ) ; s_logger . debug ( "PIF object:" + pifRec . uuid + "(" + pifRec . device + ")" ) ; return new OvsFetchInterfaceAnswer ( command , true , "Interface " + pifRec . device + " retrieved successfully" , pifRec . IP , pifRec . netmask , pifRec . MAC ) ; } catch ( final BadServerResponse e ) { s_logger . error ( "An error occurred while fetching the interface to " + label + " on host " + citrixResourceBase . getHost ( ) . getIp ( ) , e ) ; return new OvsFetchInterfaceAnswer ( command , false , "EXCEPTION:" + e . getMessage ( ) ) ; } catch ( final XenAPIException e ) { s_logger . error ( "An error occurred while fetching the interface to " + label + " on host " + citrixResourceBase . getHost ( ) . getIp ( ) , e ) ; return new OvsFetchInterfaceAnswer ( command , false , "EXCEPTION:" + e . getMessage ( ) ) ; } catch ( final XmlRpcException e ) { s_logger . error ( "An error occurred while fetching the interface to " + label + " on host " + citrixResourceBase . getHost ( ) . getIp ( ) , e ) ; return new OvsFetchInterfaceAnswer ( command , false , "EXCEPTION:" + e . getMessage ( ) ) ; } }
public void test() { try { final XsLocalNetwork nw = citrixResourceBase . getNetworkByName ( conn , label ) ; code_block = IfStatement ; s_logger . debug ( "Network object:" + nw . getNetwork ( ) . getUuid ( conn ) ) ; final PIF pif = nw . getPif ( conn ) ; final PIF . Record pifRec = pif . getRecord ( conn ) ; s_logger . debug ( "PIF object:" + pifRec . uuid + "(" + pifRec . device + ")" ) ; return new OvsFetchInterfaceAnswer ( command , true , "Interface " + pifRec . device + " retrieved successfully" , pifRec . IP , pifRec . netmask , pifRec . MAC ) ; } catch ( final BadServerResponse e ) { s_logger . error ( "An error occurred while fetching the interface to " + label + " on host " + citrixResourceBase . getHost ( ) . getIp ( ) , e ) ; return new OvsFetchInterfaceAnswer ( command , false , "EXCEPTION:" + e . getMessage ( ) ) ; } catch ( final XenAPIException e ) { s_logger . error ( "An error occurred while fetching the interface to " + label + " on host " + citrixResourceBase . getHost ( ) . getIp ( ) , e ) ; return new OvsFetchInterfaceAnswer ( command , false , "EXCEPTION:" + e . getMessage ( ) ) ; } catch ( final XmlRpcException e ) { s_logger . error ( "An error occurred while fetching the interface to " + label + " on host " + citrixResourceBase . getHost ( ) . getIp ( ) , e ) ; return new OvsFetchInterfaceAnswer ( command , false , "EXCEPTION:" + e . getMessage ( ) ) ; } }
@ BeforeClass public static void setup ( ) { logger . info ( "========================================================" ) ; logger . info ( "      Iniciando Teste de RegressÃ£o de Layout" ) ; logger . info ( "========================================================" ) ; }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { try { fcall . sendResponse ( fb , result , org . apache . thrift . protocol . TMessageType . REPLY , seqid ) ; } catch ( org . apache . thrift . transport . TTransportException e ) { _LOGGER . error ( "TTransportException writing to internal frame buffer" , e ) ; fb . close ( ) ; } catch ( java . lang . Exception e ) { _LOGGER . error ( "Exception writing to internal frame buffer" , e ) ; onError ( e ) ; } }
public void test() { if ( e instanceof org . apache . accumulo . core . clientImpl . thrift . ThriftSecurityException ) { result . sec = ( org . apache . accumulo . core . clientImpl . thrift . ThriftSecurityException ) e ; result . setSecIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof org . apache . accumulo . core . clientImpl . thrift . ThriftSecurityException ) { result . sec = ( org . apache . accumulo . core . clientImpl . thrift . ThriftSecurityException ) e ; result . setSecIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { if ( e instanceof org . apache . accumulo . core . clientImpl . thrift . ThriftSecurityException ) { result . sec = ( org . apache . accumulo . core . clientImpl . thrift . ThriftSecurityException ) e ; result . setSecIsSet ( true ) ; msg = result ; } else-if ( e instanceof org . apache . thrift . transport . TTransportException ) { _LOGGER . error ( "TTransportException inside handler" , e ) ; fb . close ( ) ; return ; } else-if ( e instanceof org . apache . thrift . TApplicationException ) { _LOGGER . error ( "TApplicationException inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = ( org . apache . thrift . TApplicationException ) e ; } else { _LOGGER . error ( "Exception inside handler" , e ) ; msgType = org . apache . thrift . protocol . TMessageType . EXCEPTION ; msg = new org . apache . thrift . TApplicationException ( org . apache . thrift . TApplicationException . INTERNAL_ERROR , e . getMessage ( ) ) ; } }
public void test() { try { fcall . sendResponse ( fb , msg , msgType , seqid ) ; } catch ( java . lang . Exception ex ) { _LOGGER . error ( "Exception writing to internal frame buffer" , ex ) ; fb . close ( ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Fill " + filler . fillerId + ": Creating " + query . getLanguage ( ) + " query executer" ) ; } }
@ Override public void doPostRollback ( IndexWriter writer ) throws IOException { LOG . info ( "Finished rollback on [{0}/{1}]" , _shard , _table ) ; Path path = directory . getPath ( ) ; String name = path . getName ( ) ; fileSystem . rename ( path , new Path ( path . getParent ( ) , rename ( name , BADROWIDS ) ) ) ; }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "JGroupsMessenger received {} headers: {}" , jgmsg , jgmsg . getHeaders ( ) ) ; } }
public void test() { try { pingPonger . sendPongMessage ( myChannel , jgAddress , jgmsg . getSrc ( ) ) ; } catch ( Exception e ) { logger . info ( "Failed sending Pong response to " + jgmsg . getSrc ( ) ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "JGroupsMessenger dispatching {} from {}" , msg , msg . getSender ( ) ) ; } }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "stop() started" ) ; } }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "stop() completed" ) ; } }
public void test() { if ( info . getStatus ( ) == ProtocolStatus . Accept ) { code_block = IfStatement ; } }
public void test() { if ( rsfRequest . isMessage ( ) ) { Class < ? > returnType = rsfRequest . getMethod ( ) . getReturnType ( ) ; RsfResultDO returnObject = null ; code_block = IfStatement ; code_block = IfStatement ; String errorInfo = "errorCode = " + returnObject . getErrorCode ( ) + ", errorMessage=" + returnObject . getErrorMessage ( ) ; invLogger . error ( "response({}) -> invokeFailed, {}" , requestID , errorInfo ) ; return rsfFuture . failed ( new RsfException ( local . getStatus ( ) , errorInfo ) ) ; } }
public void test() { try { result = connector . schema ( ) . getObjectClassInfo ( ) ; } catch ( Exception e ) { LOG . debug ( "While reading schema on connector {}" , connector , e ) ; } }
public void test() { try { process = pb . start ( ) ; pid = Platform . getPID ( process ) ; addToGlobalGC ( process , pid ) ; return process ; } catch ( Exception e ) { throw new ExecutorException ( e . getMessage ( ) , e ) ; } finally { alreadyPerformed = true ; LOGGER . debug ( "Process started. PID = " + pid ) ; } }
private void parseClientNonce ( ClientEsniInner clientEsniInne ) { byte [ ] clientNonce = parseByteArrayField ( ExtensionByteLength . NONCE ) ; clientEsniInne . setClientNonce ( clientNonce ) ; LOGGER . debug ( "clientNonce: " + ArrayConverter . bytesToHexString ( clientEsniInne . getClientNonce ( ) . getValue ( ) ) ) ; }
public void test() { try { return SAML2ComponentBuilder . createSubjectConfirmationData ( confirDataBean , keyInforBean ) ; } catch ( SecurityException | WSSecurityException e ) { LOG . error ( e . getLocalizedMessage ( ) ) ; throw new SAMLComponentBuilderException ( e . getLocalizedMessage ( ) , e ) ; } }
public void test() { if ( curSubscription . getLastPersistedSeqId ( ) < minConsumedMessage ) { minConsumedMessage = curSubscription . getLastPersistedSeqId ( ) ; } }
public void test() { try { boolean success = taggerService . deleteAttribute ( id ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Error while deleting attributeId: " + id , e ) ; return getUIWrapper ( false , e . getMessage ( ) ) ; } }
public void test() { if ( ! cluster . isConnected ( ) || ! cluster . isHealthy ( ) ) { LOG . info ( "Skipping index range cleanup because the Elasticsearch cluster is unreachable or unhealthy" ) ; return ; } }
protected RestContainer assertParseRestAsJaxb ( String uri ) throws JAXBException { Object value = parseUri ( uri ) ; RestContainer context = assertIsInstanceOf ( RestContainer . class , value ) ; log . info ( "Found: " + context ) ; return context ; }
public void test() { if ( ! StringUtils . isNullOrEmpty ( path ) ) { logger . info ( "Resolved env. variable DATACLEANER_HOME: {}" , path ) ; } else { path = System . getProperty ( "DATACLEANER_HOME" ) ; code_block = IfStatement ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { throw new IOException ( "Failed to get number of rows and total size from HiveTable" , e ) ; } finally { logger . debug ( "Took {} Âµs to get stats from {}.{}" , timeGetStats . elapsed ( TimeUnit . NANOSECONDS ) / 1000 , table . getDbName ( ) , table . getTableName ( ) ) ; } }
public void test() { try { process . isAliveOrThrow ( ) ; instructionHandler = clientSource . take ( workerId , Duration . ofSeconds ( 5 ) ) ; } catch ( TimeoutException timeoutEx ) { LOG . info ( "Still waiting for startup of environment '{}' for worker id {}" , processPayload . getCommand ( ) , workerId ) ; } catch ( InterruptedException interruptEx ) { Thread . currentThread ( ) . interrupt ( ) ; throw new RuntimeException ( interruptEx ) ; } }
public void test() { try { Field configListenersField = this . getClass ( ) . getSuperclass ( ) . getDeclaredField ( "configListeners" ) ; configListenersField . setAccessible ( true ) ; List < IConfigurationListener > listeners = ( List < IConfigurationListener > ) configListenersField . get ( this ) ; invokeListeners ( tr , tm , listeners ) ; } catch ( NoSuchFieldException | IllegalAccessException e ) { LOGGER . error ( "Could not invoke configuration listeners." , e ) ; } }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; } catch ( Exception e ) { log . error ( "Shutdown failed: {}" , e . getMessage ( ) , e ) ; } }
public void test() { try { ifaceManager . deleteBridgeDomainFromInterface ( rEp ) . get ( ) ; LOG . debug ( "bridge-domain was deleted from interface for endpoint {}" , rEp ) ; } catch ( InterruptedException | ExecutionException e ) { LOG . warn ( "bridge-domain was not deleted from interface for endpoint {}" , rEp , e ) ; } }
public void test() { if ( ! Strings . isNullOrEmpty ( rEpLoc . getExternalNode ( ) ) ) { code_block = TryStatement ;  } else { LOG . debug ( "Forwarding is not removed - Location of renderer endpoint does not contain " + "external-node therefore VPP renderer assumes that interface for endpoint is not " + "assigned to bridge-domain representing external-node. {}" , rEp ) ; } }
public void test() { if ( scriptFactory == null ) { throw new IllegalStateException ( "No script factory associated to current site context '" + siteContext . getSiteName ( ) + "'" ) ; } }
public void test() { if ( storeService . exists ( siteContext . getContext ( ) , scriptUrl ) ) { code_block = IfStatement ; return scriptFactory . getScript ( scriptUrl ) ; } else-if ( logger . isDebugEnabled ( ) ) { logger . debug ( "No controller script for page " + pageUrl + " at " + scriptUrl ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( CrafterException e ) { logger . error ( "Error while trying to retrieve controller script at " + scriptUrl , e ) ; } }
public void test() { if ( isValidHttpHeaderName ( headerName ) ) { response . addHeader ( ACCESS_CONTROL_ALLOW_HEADERS , headerName ) ; } else { log . warn ( "Invalid HTTP header specified in " + ACCESS_CONTROL_REQUEST_HEADERS + " '" + headerName + "'. " + "It will be ignored and not attached to the " + ACCESS_CONTROL_ALLOW_HEADERS + " response header" ) ; } }
public void test() { try { clazz = findClass ( version ) ; } catch ( ClassNotFoundException e ) { logger . trace ( "WorkplaceSearchClient class not found for version {} in the classpath. Skipping..." , version ) ; } }
public PinData [ ] publishPinArray ( int [ ] data ) { log . debug ( "publishPinArray {}" , data ) ; int pinDataCnt = data . length / 3 ; PinData [ ] pinArray = new PinData [ pinDataCnt ] ; code_block = ForStatement ; HashMap < String , PinData > pinDataMap = new HashMap < String , PinData > ( ) ; code_block = ForStatement ; code_block = ForStatement ; return pinArray ; }
public void test() { if ( pinDef == null ) { log . error ( "not a valid pin address {}" , address ) ; continue ; } }
public void disconnect ( ) { LOG . info ( "Disconnecting from broker {} on user {}" , config . url ( ) , config . username ( ) ) ; JmsUtils . closeQuietly ( connection ) ; }
public void test() { if ( studyId != null ) { code_block = IfStatement ; code_block = ForStatement ; } else { logger . error ( "Study ID is null" ) ; } }
public void test() { try { heliumVisualizationFactory . bundle ( helium . getVisualizationPackagesToBundle ( ) ) ; } catch ( Exception e ) { LOG . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( subProcess == null ) { LOGGER . info ( "Ignore, already stop or reusing exist instance, displayName={}." , deployDefinition . getDisplayName ( ) ) ; return ; } }
public void stop ( ) { code_block = IfStatement ; subProcess . destroy ( ) ; subProcess = null ; afterStop ( ) ; LOGGER . info ( "stop complete, displayName={}." , deployDefinition . getDisplayName ( ) ) ; }
public void test() { try { final String url = COMMITTE_PROPOSAL . replace ( ID_KEY , UrlHelper . urlEncode ( id , StandardCharsets . UTF_8 . toString ( ) ) ) ; return ( ( JAXBElement < CommitteeProposalComponentData > ) xmlAgent . unmarshallXml ( riksdagenCommitteeProposalMarshaller , url , HTTP_UTSKOTTSFORSLAG_RIKSDAGEN_EXTERNAL_MODEL_CIA_HACK23_COM_IMPL , null , null ) ) . getValue ( ) ; } catch ( final XmlAgentException e ) { LOGGER . warn ( PROBLEM_GETTING_COMMITTEE_PROPOSAL_FOR_ID_S_FROM_DATA_RIKSDAGEN_SE , id ) ; throw new DataFailureException ( e ) ; } }
public void test() { try { int returnValue = CommerceOrderItemServiceUtil . getCommerceOrderItemsCount ( groupId , commerceAccountId , orderStatuses ) ; return returnValue ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
private static void logHadoopVersionInfo ( ) { getLogger ( ) . info ( "SYSINFO Hadoop version: " + VersionInfo . getVersion ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop revision: " + VersionInfo . getRevision ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop date: " + VersionInfo . getDate ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop user: " + VersionInfo . getUser ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop url: " + VersionInfo . getUrl ( ) ) ; }
private static void logHadoopVersionInfo ( ) { getLogger ( ) . info ( "SYSINFO Hadoop version: " + VersionInfo . getVersion ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop revision: " + VersionInfo . getRevision ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop date: " + VersionInfo . getDate ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop user: " + VersionInfo . getUser ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop url: " + VersionInfo . getUrl ( ) ) ; }
private static void logHadoopVersionInfo ( ) { getLogger ( ) . info ( "SYSINFO Hadoop version: " + VersionInfo . getVersion ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop revision: " + VersionInfo . getRevision ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop date: " + VersionInfo . getDate ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop user: " + VersionInfo . getUser ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop url: " + VersionInfo . getUrl ( ) ) ; }
private static void logHadoopVersionInfo ( ) { getLogger ( ) . info ( "SYSINFO Hadoop version: " + VersionInfo . getVersion ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop revision: " + VersionInfo . getRevision ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop date: " + VersionInfo . getDate ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop user: " + VersionInfo . getUser ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop url: " + VersionInfo . getUrl ( ) ) ; }
private static void logHadoopVersionInfo ( ) { getLogger ( ) . info ( "SYSINFO Hadoop version: " + VersionInfo . getVersion ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop revision: " + VersionInfo . getRevision ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop date: " + VersionInfo . getDate ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop user: " + VersionInfo . getUser ( ) ) ; getLogger ( ) . info ( "SYSINFO Hadoop url: " + VersionInfo . getUrl ( ) ) ; }
public void addAttributeListener ( ZclAttributeListener listener ) { logger . trace ( "{}: ZclCluster.addAttributeListener adding {}" , zigbeeEndpoint . getEndpointAddress ( ) , listener ) ; attributeListeners . add ( listener ) ; }
public void test() { try { tryInstallingFailedArtifacts ( ) ; } catch ( Exception e ) { LOGGER . debug ( "error when trying to instal artifacts" , e ) ; } }
public void test() { try { log . info ( "Logging out participant via SOAP: " + participant ) ; LogoutRequest logoutRequest = createSignedLogoutRequest ( participant , soapLogoutEndpoint ) ; IClientConfiguration soapClientConfig = createSoapClientConfig ( participant ) ; SAMLLogoutClient client = new SAMLLogoutClient ( soapLogoutEndpoint . getUrl ( ) , soapClientConfig ) ; LogoutResponseDocument resp = client . logout ( logoutRequest . getXMLBeanDoc ( ) ) ; updateContextAfterParicipantLogout ( ctx , participant , resp ) ; } catch ( Exception e ) { log . warn ( "Logging out the participant " + participant + " via SOAP failed" , e ) ; ctx . getFailed ( ) . add ( participant ) ; } }
public void test() { try { log . info ( "Logging out participant via SOAP: " + participant ) ; LogoutRequest logoutRequest = createSignedLogoutRequest ( participant , soapLogoutEndpoint ) ; IClientConfiguration soapClientConfig = createSoapClientConfig ( participant ) ; SAMLLogoutClient client = new SAMLLogoutClient ( soapLogoutEndpoint . getUrl ( ) , soapClientConfig ) ; LogoutResponseDocument resp = client . logout ( logoutRequest . getXMLBeanDoc ( ) ) ; updateContextAfterParicipantLogout ( ctx , participant , resp ) ; } catch ( Exception e ) { log . warn ( "Logging out the participant " + participant + " via SOAP failed" , e ) ; ctx . getFailed ( ) . add ( participant ) ; } }
@ Override public void bridgeStatusChanged ( ThingStatusInfo bridgeStatusInfo ) { logger . debug ( "bridgeStatusChanged {} for thing {}" , bridgeStatusInfo , getThing ( ) . getUID ( ) ) ; initializeBridge ( ( getBridge ( ) == null ) ? null : getBridge ( ) . getHandler ( ) , bridgeStatusInfo . getStatus ( ) ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "proceedElement '" + obj . getClass ( ) + "'...." ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . info ( "proceedElement: field '" + field . getDeclaringClass ( ) . getName ( ) + "." + field . getName ( ) + "'." ) ; } }
public void test() { if ( foundField != null ) { final Class < ? > type = foundField . getType ( ) ; final Object value = read ( type , el , key , attrValue ) ; setField ( foundField , obj , value , el , key , attrValue ) ; code_block = IfStatement ; } else { log . warn ( "Field '" + key + "' not found." ) ; } }
public void test() { if ( ! col . isPresent ( ) ) { logger . error ( "Could not find secondary index target column {} in the columns of table {}.{} " + "({}). This should not happen and should be reported for investigation. That index " + "will not be visible in Stargate schema view." , targetColumn , keyspaceName , tableName , baseTableColumns ) ; return null ; } }
public void test() { for ( DataFile inFile : inFiles ) { count ++ ; final DataFile outFile = entries . get ( inFile ) ; final Path f = new Path ( tmpInputDir , "distcp-" + nf . format ( count ) + ".cp" ) ; getLogger ( ) . info ( "Task copy " + inFile + " in " + f . toString ( ) ) ; BufferedWriter bw = new BufferedWriter ( new OutputStreamWriter ( fs . create ( f ) , CHARSET ) ) ; bw . write ( inFile . getSource ( ) + "\t" + outFile . getSource ( ) + "\n" ) ; bw . close ( ) ; } }
@ Test ( groups = "Integration" ) public void testChangeModeFailureStopsTasksButHappyUponResumption ( ) throws Exception { DynamicCluster origServerPool = origApp . createAndManageChild ( EntitySpec . create ( DynamicCluster . class ) . configure ( DynamicCluster . MEMBER_SPEC , EntitySpec . create ( TomcatServer . class ) . configure ( "war" , getTestWar ( ) ) ) . configure ( "initialSize" , 1 ) ) ; NginxController origNginx = origApp . createAndManageChild ( EntitySpec . create ( NginxController . class ) . configure ( "serverPool" , origServerPool ) . configure ( "domain" , "localhost" ) ) ; origApp . start ( ImmutableList . of ( loc ) ) ; Assert . assertTrue ( RecordingSshjTool . connectionCount . get ( ) > 0 ) ; Collection < Feed > origFeeds = ( ( EntityInternal ) origNginx ) . feeds ( ) . getFeeds ( ) ; LOG . info ( "feeds before rebind are: " + origFeeds ) ; Assert . assertTrue ( origFeeds . size ( ) >= 1 ) ; origManagementContext . getRebindManager ( ) . forcePersistNow ( ) ; List < Task < ? > > tasksBefore = ( ( BasicExecutionManager ) origManagementContext . getExecutionManager ( ) ) . getAllTasks ( ) ; LOG . info ( "tasks before disabling HA, " + tasksBefore . size ( ) + ": " + tasksBefore ) ; Assert . assertFalse ( tasksBefore . isEmpty ( ) ) ; origManagementContext . getHighAvailabilityManager ( ) . changeMode ( HighAvailabilityMode . DISABLED ) ; origApp = null ; Repeater . create ( ) . every ( Duration . millis ( 20 ) ) . backoffTo ( Duration . ONE_SECOND ) . limitTimeTo ( Duration . THIRTY_SECONDS ) . until ( new Callable < Boolean > ( ) code_block = "" ; ) . runRequiringTrue ( ) ; RecordingSshjTool . forbidden . set ( true ) ; newManagementContext = createNewManagementContext ( ) ; newApp = ( TestApplication ) RebindTestUtils . rebind ( ( LocalManagementContext ) newManagementContext , classLoader ) ; NginxController newNginx = Iterables . getOnlyElement ( Entities . descendants ( newApp , NginxController . class ) ) ; Collection < Feed > newFeeds = ( ( EntityInternal ) newNginx ) . feeds ( ) . getFeeds ( ) ; LOG . info ( "feeds after rebind are: " + newFeeds ) ; Assert . assertTrue ( newFeeds . size ( ) >= 1 ) ; EntityTestUtils . assertAttributeEqualsEventually ( newNginx , Attributes . SERVICE_STATE_ACTUAL , Lifecycle . ON_FIRE ) ; RecordingSshjTool . forbidden . set ( false ) ; EntityTestUtils . assertAttributeEqualsEventually ( newNginx , Attributes . SERVICE_STATE_ACTUAL , Lifecycle . RUNNING ) ; }
@ Test ( groups = "Integration" ) public void testChangeModeFailureStopsTasksButHappyUponResumption ( ) throws Exception { DynamicCluster origServerPool = origApp . createAndManageChild ( EntitySpec . create ( DynamicCluster . class ) . configure ( DynamicCluster . MEMBER_SPEC , EntitySpec . create ( TomcatServer . class ) . configure ( "war" , getTestWar ( ) ) ) . configure ( "initialSize" , 1 ) ) ; NginxController origNginx = origApp . createAndManageChild ( EntitySpec . create ( NginxController . class ) . configure ( "serverPool" , origServerPool ) . configure ( "domain" , "localhost" ) ) ; origApp . start ( ImmutableList . of ( loc ) ) ; Assert . assertTrue ( RecordingSshjTool . connectionCount . get ( ) > 0 ) ; Collection < Feed > origFeeds = ( ( EntityInternal ) origNginx ) . feeds ( ) . getFeeds ( ) ; LOG . info ( "feeds before rebind are: " + origFeeds ) ; Assert . assertTrue ( origFeeds . size ( ) >= 1 ) ; origManagementContext . getRebindManager ( ) . forcePersistNow ( ) ; List < Task < ? > > tasksBefore = ( ( BasicExecutionManager ) origManagementContext . getExecutionManager ( ) ) . getAllTasks ( ) ; LOG . info ( "tasks before disabling HA, " + tasksBefore . size ( ) + ": " + tasksBefore ) ; Assert . assertFalse ( tasksBefore . isEmpty ( ) ) ; origManagementContext . getHighAvailabilityManager ( ) . changeMode ( HighAvailabilityMode . DISABLED ) ; origApp = null ; Repeater . create ( ) . every ( Duration . millis ( 20 ) ) . backoffTo ( Duration . ONE_SECOND ) . limitTimeTo ( Duration . THIRTY_SECONDS ) . until ( new Callable < Boolean > ( ) code_block = "" ; ) . runRequiringTrue ( ) ; RecordingSshjTool . forbidden . set ( true ) ; newManagementContext = createNewManagementContext ( ) ; newApp = ( TestApplication ) RebindTestUtils . rebind ( ( LocalManagementContext ) newManagementContext , classLoader ) ; NginxController newNginx = Iterables . getOnlyElement ( Entities . descendants ( newApp , NginxController . class ) ) ; Collection < Feed > newFeeds = ( ( EntityInternal ) newNginx ) . feeds ( ) . getFeeds ( ) ; LOG . info ( "feeds after rebind are: " + newFeeds ) ; Assert . assertTrue ( newFeeds . size ( ) >= 1 ) ; EntityTestUtils . assertAttributeEqualsEventually ( newNginx , Attributes . SERVICE_STATE_ACTUAL , Lifecycle . ON_FIRE ) ; RecordingSshjTool . forbidden . set ( false ) ; EntityTestUtils . assertAttributeEqualsEventually ( newNginx , Attributes . SERVICE_STATE_ACTUAL , Lifecycle . RUNNING ) ; }
public void test() { { origManagementContext . getGarbageCollector ( ) . gcIteration ( ) ; List < Task < ? > > tasksAfter = ( ( BasicExecutionManager ) origManagementContext . getExecutionManager ( ) ) . getAllTasks ( ) ; LOG . info ( "tasks after disabling HA, " + tasksAfter . size ( ) + ": " + tasksAfter ) ; return tasksAfter . isEmpty ( ) ; } }
@ Test ( groups = "Integration" ) public void testChangeModeFailureStopsTasksButHappyUponResumption ( ) throws Exception { DynamicCluster origServerPool = origApp . createAndManageChild ( EntitySpec . create ( DynamicCluster . class ) . configure ( DynamicCluster . MEMBER_SPEC , EntitySpec . create ( TomcatServer . class ) . configure ( "war" , getTestWar ( ) ) ) . configure ( "initialSize" , 1 ) ) ; NginxController origNginx = origApp . createAndManageChild ( EntitySpec . create ( NginxController . class ) . configure ( "serverPool" , origServerPool ) . configure ( "domain" , "localhost" ) ) ; origApp . start ( ImmutableList . of ( loc ) ) ; Assert . assertTrue ( RecordingSshjTool . connectionCount . get ( ) > 0 ) ; Collection < Feed > origFeeds = ( ( EntityInternal ) origNginx ) . feeds ( ) . getFeeds ( ) ; LOG . info ( "feeds before rebind are: " + origFeeds ) ; Assert . assertTrue ( origFeeds . size ( ) >= 1 ) ; origManagementContext . getRebindManager ( ) . forcePersistNow ( ) ; List < Task < ? > > tasksBefore = ( ( BasicExecutionManager ) origManagementContext . getExecutionManager ( ) ) . getAllTasks ( ) ; LOG . info ( "tasks before disabling HA, " + tasksBefore . size ( ) + ": " + tasksBefore ) ; Assert . assertFalse ( tasksBefore . isEmpty ( ) ) ; origManagementContext . getHighAvailabilityManager ( ) . changeMode ( HighAvailabilityMode . DISABLED ) ; origApp = null ; Repeater . create ( ) . every ( Duration . millis ( 20 ) ) . backoffTo ( Duration . ONE_SECOND ) . limitTimeTo ( Duration . THIRTY_SECONDS ) . until ( new Callable < Boolean > ( ) code_block = "" ; ) . runRequiringTrue ( ) ; RecordingSshjTool . forbidden . set ( true ) ; newManagementContext = createNewManagementContext ( ) ; newApp = ( TestApplication ) RebindTestUtils . rebind ( ( LocalManagementContext ) newManagementContext , classLoader ) ; NginxController newNginx = Iterables . getOnlyElement ( Entities . descendants ( newApp , NginxController . class ) ) ; Collection < Feed > newFeeds = ( ( EntityInternal ) newNginx ) . feeds ( ) . getFeeds ( ) ; LOG . info ( "feeds after rebind are: " + newFeeds ) ; Assert . assertTrue ( newFeeds . size ( ) >= 1 ) ; EntityTestUtils . assertAttributeEqualsEventually ( newNginx , Attributes . SERVICE_STATE_ACTUAL , Lifecycle . ON_FIRE ) ; RecordingSshjTool . forbidden . set ( false ) ; EntityTestUtils . assertAttributeEqualsEventually ( newNginx , Attributes . SERVICE_STATE_ACTUAL , Lifecycle . RUNNING ) ; }
public List < OWLOntologyChange > getChanges ( ) { logger . info ( LogBanner . start ( "Deprecating entity" ) ) ; logger . info ( "[Deprecate Entity] Deprecating " + info . getEntityToDeprecate ( ) ) ; List < OWLOntologyChange > changes = new ArrayList < > ( ) ; switchUsageOfDeprecatedEntityWithReplacement ( changes ) ; updateDeprecatedEntityLogicalDefinition ( changes ) ; updateDeprecatedEntityAnnotations ( changes ) ; addDeprecatedAnnotationAssertion ( changes ) ; addDeprecationReason ( changes ) ; addDeprecationCode ( changes ) ; relabelDeprecatedEntity ( changes ) ; prefixDeprecatedAnnotationValues ( changes ) ; addReplacedByAnnotation ( changes ) ; addAlternateEntityAnnotations ( changes ) ; reparentDeprecatedEntity ( changes ) ; logger . info ( LogBanner . end ( ) ) ; return changes ; }
public List < OWLOntologyChange > getChanges ( ) { logger . info ( LogBanner . start ( "Deprecating entity" ) ) ; logger . info ( "[Deprecate Entity] Deprecating " + info . getEntityToDeprecate ( ) ) ; List < OWLOntologyChange > changes = new ArrayList < > ( ) ; switchUsageOfDeprecatedEntityWithReplacement ( changes ) ; updateDeprecatedEntityLogicalDefinition ( changes ) ; updateDeprecatedEntityAnnotations ( changes ) ; addDeprecatedAnnotationAssertion ( changes ) ; addDeprecationReason ( changes ) ; addDeprecationCode ( changes ) ; relabelDeprecatedEntity ( changes ) ; prefixDeprecatedAnnotationValues ( changes ) ; addReplacedByAnnotation ( changes ) ; addAlternateEntityAnnotations ( changes ) ; reparentDeprecatedEntity ( changes ) ; logger . info ( LogBanner . end ( ) ) ; return changes ; }
public List < OWLOntologyChange > getChanges ( ) { logger . info ( LogBanner . start ( "Deprecating entity" ) ) ; logger . info ( "[Deprecate Entity] Deprecating " + info . getEntityToDeprecate ( ) ) ; List < OWLOntologyChange > changes = new ArrayList < > ( ) ; switchUsageOfDeprecatedEntityWithReplacement ( changes ) ; updateDeprecatedEntityLogicalDefinition ( changes ) ; updateDeprecatedEntityAnnotations ( changes ) ; addDeprecatedAnnotationAssertion ( changes ) ; addDeprecationReason ( changes ) ; addDeprecationCode ( changes ) ; relabelDeprecatedEntity ( changes ) ; prefixDeprecatedAnnotationValues ( changes ) ; addReplacedByAnnotation ( changes ) ; addAlternateEntityAnnotations ( changes ) ; reparentDeprecatedEntity ( changes ) ; logger . info ( LogBanner . end ( ) ) ; return changes ; }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceAccountUserRelServiceUtil . class , "getCommerceAccountUserRels" , _getCommerceAccountUserRelsParameterTypes7 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , commerceAccountId , start , end ) ; Object returnObj = null ; code_block = TryStatement ;  return ( java . util . List < com . liferay . commerce . account . model . CommerceAccountUserRel > ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( lastIndex < index ) { log . info ( "Delete existing log (lastIndex '{}') and replace with received snapshot (index '{}')" , lastIndex , index ) ; raftLog . reset ( index + 1 ) ; } }
@ Override public void run ( ) { logger . info ( "[testTokenCleanup] vm1 tests recursion" ) ; connectDistributedSystem ( ) ; DLockService dls = ( DLockService ) DistributedLockService . getServiceNamed ( dlsName ) ; assertThat ( dls . lock ( key1 , - 1 , - 1 ) ) . isTrue ( ) ; assertThat ( dls . getToken ( key1 ) . getUsageCount ( ) ) . isEqualTo ( 1 ) ; assertThat ( dls . lock ( key1 , - 1 , - 1 ) ) . isTrue ( ) ; assertThat ( dls . getToken ( key1 ) . getUsageCount ( ) ) . isEqualTo ( 2 ) ; assertThat ( dls . lock ( key1 , - 1 , - 1 ) ) . isTrue ( ) ; assertThat ( dls . getToken ( key1 ) . getUsageCount ( ) ) . isEqualTo ( 3 ) ; DLockToken token0 = dls . getToken ( key1 ) ; assertThat ( token0 ) . isNotNull ( ) ; Collection tokens = dls . getTokens ( ) ; assertThat ( tokens . contains ( token0 ) ) . isTrue ( ) ; assertThat ( tokens . size ( ) ) . isEqualTo ( 1 ) ; dls . unlock ( key1 ) ; assertThat ( dls . getToken ( key1 ) . getUsageCount ( ) ) . isEqualTo ( 2 ) ; dls . freeResources ( key1 ) ; DLockToken token1 = dls . getToken ( key1 ) ; assertThat ( token1 ) . isNotNull ( ) ; assertThat ( token1 ) . isEqualTo ( token0 ) ; tokens = dls . getTokens ( ) ; assertThat ( tokens . contains ( token1 ) ) . isTrue ( ) ; assertThat ( tokens . size ( ) ) . isEqualTo ( 1 ) ; dls . unlock ( key1 ) ; assertThat ( dls . getToken ( key1 ) . getUsageCount ( ) ) . isEqualTo ( 1 ) ; dls . freeResources ( key1 ) ; assertThat ( dls . getToken ( key1 ) ) . isNotNull ( ) ; DLockToken token2 = dls . getToken ( key1 ) ; assertThat ( token2 ) . isNotNull ( ) ; assertThat ( token2 ) . isEqualTo ( token0 ) ; tokens = dls . getTokens ( ) ; assertThat ( tokens . contains ( token2 ) ) . isTrue ( ) ; assertThat ( tokens . size ( ) ) . isEqualTo ( 1 ) ; dls . unlock ( key1 ) ; assertThat ( dls . getToken ( key1 ) . getUsageCount ( ) ) . isEqualTo ( 0 ) ; dls . freeResources ( key1 ) ; DLockToken token3 = dls . getToken ( key1 ) ; assertThat ( token3 ) . withFailMessage ( "Failed with bug 38180: " + token3 ) . isNull ( ) ; tokens = dls . getTokens ( ) ; assertThat ( tokens . size ( ) ) . isEqualTo ( 0 ) . withFailMessage ( "Failed with bug 38180: tokens=" + tokens ) ; }
public void test() { try { messageCount = getQueueNotVisibleMessageCount ( queueUrl , false ) ; } catch ( Exception ex ) { logger . error ( "event=failed_to_get_number_of_not_visible_messages queue_url=" + queueUrl ) ; } }
public void test() { try { ret . append ( paramName ) ; ret . append ( "=" ) ; ret . append ( urlCodec . encode ( paramValue ) ) ; } catch ( EncoderException ex ) { LOG . error ( "Unable to encode parameter name or value: " + paramName + "=" + paramValue , ex ) ; throw new RuntimeException ( "Unable to encode parameter name or value: " + paramName + "=" + paramValue , ex ) ; } }
public void test() { try { return new BigtableAsyncAdmin ( BigtableAsyncConnection . this ) ; } catch ( IOException e ) { LOG . error ( "failed to build BigtableAsyncAdmin" , e ) ; throw new UncheckedIOException ( "failed to build BigtableAsyncAdmin" , e ) ; } }
public static void prepareRuntimeContext ( ) { RuntimeContext . set ( RuntimeContext . DEFAULT . apply ( System . getenv ( ) ) ) ; RuntimeContext . get ( ) . verifyApplication ( WindGate . class . getClassLoader ( ) ) ; LOG . debug ( "Runtime context is prepared: {}" , RuntimeContext . get ( ) ) ; }
public void test() { try { VersioningIterator it = new VersioningIterator ( ) ; IteratorSetting is = new IteratorSetting ( 1 , VersioningIterator . class ) ; VersioningIterator . setMaxVersions ( is , 3 ) ; it . init ( new SortedMapIterator ( tm ) , is . getOptions ( ) , null ) ; it . seek ( new Range ( ) , EMPTY_COL_FAMS , false ) ; TreeMap < Key , Value > tmOut = iteratorOverTestData ( it ) ; code_block = ForStatement ; assertEquals ( "size after keeping 3 versions was " + tmOut . size ( ) , 6 , tmOut . size ( ) ) ; } catch ( IOException e ) { fail ( ) ; } catch ( Exception e ) { log . error ( "{}" , e . getMessage ( ) , e ) ; fail ( ) ; } }
@ Deactivate public void stop ( ) { LOGGER . debug ( "Stopping lock service" ) ; }
ChecksumValue computeLocalFileChecksum ( final File localFile , final ChecksumEncodingEnum overrideChecksumEncoding ) throws JargonException { log . info ( "computeLocalFileChecksum()" ) ; code_block = IfStatement ; log . info ( "localFile:{}" , localFile ) ; code_block = IfStatement ; code_block = IfStatement ; ChecksumEncodingEnum checksumEncoding ; code_block = IfStatement ; log . info ( "using checksum algorithm:{}" , checksumEncoding ) ; AbstractChecksumComputeStrategy strategy = irodsAccessObjectFactory . getIrodsSession ( ) . getLocalChecksumComputerFactory ( ) . instance ( checksumEncoding ) ; code_block = TryStatement ;  }
ChecksumValue computeLocalFileChecksum ( final File localFile , final ChecksumEncodingEnum overrideChecksumEncoding ) throws JargonException { log . info ( "computeLocalFileChecksum()" ) ; code_block = IfStatement ; log . info ( "localFile:{}" , localFile ) ; code_block = IfStatement ; code_block = IfStatement ; ChecksumEncodingEnum checksumEncoding ; code_block = IfStatement ; log . info ( "using checksum algorithm:{}" , checksumEncoding ) ; AbstractChecksumComputeStrategy strategy = irodsAccessObjectFactory . getIrodsSession ( ) . getLocalChecksumComputerFactory ( ) . instance ( checksumEncoding ) ; code_block = TryStatement ;  }
ChecksumValue computeLocalFileChecksum ( final File localFile , final ChecksumEncodingEnum overrideChecksumEncoding ) throws JargonException { log . info ( "computeLocalFileChecksum()" ) ; code_block = IfStatement ; log . info ( "localFile:{}" , localFile ) ; code_block = IfStatement ; code_block = IfStatement ; ChecksumEncodingEnum checksumEncoding ; code_block = IfStatement ; log . info ( "using checksum algorithm:{}" , checksumEncoding ) ; AbstractChecksumComputeStrategy strategy = irodsAccessObjectFactory . getIrodsSession ( ) . getLocalChecksumComputerFactory ( ) . instance ( checksumEncoding ) ; code_block = TryStatement ;  }
public void test() { if ( overrideChecksumEncoding == null ) { checksumEncoding = checksumManager . determineChecksumEncodingForTargetServer ( ) ; } else { checksumEncoding = overrideChecksumEncoding ; } }
public void test() { try { Map . Entry indexEntry = ( Map . Entry ) it . next ( ) ; PartitionedIndex index = ( PartitionedIndex ) indexEntry . getValue ( ) ; IndexCreationData icd = new IndexCreationData ( index . getName ( ) ) ; new QCompiler ( ) ; String imports = index . getImports ( ) ; icd . setIndexData ( index . getType ( ) , index . getCanonicalizedFromClause ( ) , index . getCanonicalizedIndexedExpression ( ) , index . getImports ( ) ) ; icd . setPartitionedIndex ( index ) ; indexes . add ( icd ) ; } catch ( Exception ignor ) { logger . info ( String . format ( "Excpetion  in bucket index creation : %s" , ignor . getLocalizedMessage ( ) ) , ignor ) ; } }
public void test() { try { Thread . sleep ( 30000 ) ; getEndpoint ( ) . createQueue ( getClient ( ) ) ; } catch ( Exception e ) { LOG . warn ( "failed to retry queue connection." , e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( QueueDeletedRecentlyException qdr ) { LOG . debug ( "Queue recently deleted, will retry in 30 seconds." ) ; code_block = TryStatement ;  } catch ( Exception e ) { LOG . warn ( "Could not connect to queue in amazon." , e ) ; } }
public void test() { if ( errored . compareAndSet ( false , true ) ) { log . error ( "unrecoverable error in task feeder or one of its mapper threads. immediately halting jvm" , t ) ; Runtime . getRuntime ( ) . halt ( 1 ) ; } }
public edu . indiana . extreme . wsdl . benchmark1 . ReceiveMeshInterfaceObjectsResponse receiveMeshInterfaceObjects ( edu . indiana . extreme . wsdl . benchmark1 . ReceiveMeshInterfaceObjectsRequest input ) { LOG . info ( "Executing operation receiveMeshInterfaceObjects" ) ; ReceiveMeshInterfaceObjectsResponse ret = new ReceiveMeshInterfaceObjectsResponse ( ) ; ret . setReceiveMeshInterfaceObjectsReturn ( input . getInput ( ) . getItem ( ) . size ( ) ) ; return ret ; }
@ Override public void deleteList ( String uuid ) throws BusinessException { ContactList listToDelete = findByUuid ( uuid ) ; logger . debug ( "List to delete: " + uuid ) ; listRepository . delete ( listToDelete ) ; }
public void test() { try { listener . gotGeoDetails ( place ) ; } catch ( Exception e ) { logger . warn ( "Exception at getGeoDetails" , e ) ; } }
public void test() { for ( Entry < String , Object > e : l ) { logger . info ( e . getKey ( ) + " = " + e . getValue ( ) ) ; } }
public void test() { try { ColorScale cs = getColorScaleFromName ( name ) ; return createColorScaleSwatch ( cs , format , width , height ) ; } catch ( Exception e ) { log . error ( "Exception thrown" , e ) ; throw new MrsPyramidServiceException ( "Error creating color scale " + name , e ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "AtlasPAMAuthenticationProvider{groupsFromUGI= " + groupsFromUGI + '\'' + ", options=" + options + '}' ) ; } }
public void test() { try { this . groupsFromUGI = ApplicationProperties . get ( ) . getBoolean ( "atlas.authentication.method.pam.ugi-groups" , true ) ; Properties properties = ConfigurationConverter . getProperties ( ApplicationProperties . get ( ) . subset ( "atlas.authentication.method.pam" ) ) ; code_block = ForStatement ; code_block = IfStatement ; code_block = IfStatement ; } catch ( Exception e ) { LOG . error ( "Exception while setLdapProperties" , e ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "{} cookie has been found and is being processed" , cookieName ) ; } }
public void test() { try { return PropertyAccessor . getInstance ( ) . getProperty ( NhincConstants . MESSAGES_PROPERTY_FILE , key ) ; } catch ( PropertyAccessException ex ) { LOG . error ( "Unable to read {} from file {}: {}" , key , NhincConstants . MESSAGES_PROPERTY_FILE , ex . getLocalizedMessage ( ) , ex ) ; } }
public void test() { try { byte [ ] d = HexUtils . hexToBytes ( data . toString ( ) ) ; logger . debug ( "File len: {}" , d . length ) ; return d ; } catch ( IllegalArgumentException e ) { logger . debug ( "Exception occured during data conversion: {}" , e . getMessage ( ) ) ; } }
public void test() { try { byte [ ] d = HexUtils . hexToBytes ( data . toString ( ) ) ; logger . debug ( "File len: {}" , d . length ) ; return d ; } catch ( IllegalArgumentException e ) { logger . debug ( "Exception occured during data conversion: {}" , e . getMessage ( ) ) ; } }
public void test() { if ( storeFile . exists ( ) ) { LOG . debug ( "Trying to load store from file" ) ; fin = new FileInputStream ( storeFile ) ; } else { LOG . debug ( "Trying to load store from classpath" ) ; fin = getClass ( ) . getClassLoader ( ) . getResourceAsStream ( storeFile . getPath ( ) ) ; code_block = IfStatement ; } }
public void test() { if ( storeFile . exists ( ) ) { LOG . debug ( "Trying to load store from file" ) ; fin = new FileInputStream ( storeFile ) ; } else { LOG . debug ( "Trying to load store from classpath" ) ; fin = getClass ( ) . getClassLoader ( ) . getResourceAsStream ( storeFile . getPath ( ) ) ; code_block = IfStatement ; } }
@ Override protected void parseHandshakeMessageContent ( HelloRetryRequestMessage msg ) { LOGGER . debug ( "Parsing HelloRetryRequestMessage" ) ; parseProtocolVersion ( msg ) ; parseSelectedCiphersuite ( msg ) ; code_block = IfStatement ; }
public void test() { if ( commandLineArguments . getLogLevel ( ) == Level . DEBUG ) { LOG . error ( "Unexpected error" , e ) ; } else { LOG . error ( getMessagesFromException ( e ) ) ; } }
public void test() { if ( commandLineArguments . getLogLevel ( ) == Level . DEBUG ) { LOG . error ( "Unexpected error" , e ) ; } else { LOG . error ( getMessagesFromException ( e ) ) ; } }
public void test() { try { cloudStackContext = new CloudStackSpringContext ( ) ; cloudStackContext . registerShutdownHook ( ) ; event . getServletContext ( ) . setAttribute ( CloudStackSpringContext . CLOUDSTACK_CONTEXT_SERVLET_KEY , cloudStackContext ) ; } catch ( IOException e ) { log . error ( "Failed to start CloudStack" , e ) ; throw new RuntimeException ( "Failed to initialize CloudStack Spring modules" , e ) ; } }
protected void truncate ( final long length ) throws IOException { log . info ( "Truncating file [{0}] to {1}" , filepath , length ) ; fc . truncate ( length ) ; }
public void test() { if ( this . logInvalidTraces ) { this . logger . error ( "Invalid trace: {}" , invalidTrace ) ; } }
public void test() { try { updaterFuture . get ( ) ; } catch ( ExecutionException ee ) { log . error ( ee . getCause ( ) , "Error in %s" , this ) ; } catch ( CancellationException ce ) { log . error ( ce , "Future for %s has already been cancelled" , this ) ; } catch ( InterruptedException ie ) { Thread . currentThread ( ) . interrupt ( ) ; throw new RuntimeException ( ie ) ; } }
public void test() { try { updaterFuture . get ( ) ; } catch ( ExecutionException ee ) { log . error ( ee . getCause ( ) , "Error in %s" , this ) ; } catch ( CancellationException ce ) { log . error ( ce , "Future for %s has already been cancelled" , this ) ; } catch ( InterruptedException ie ) { Thread . currentThread ( ) . interrupt ( ) ; throw new RuntimeException ( ie ) ; } }
@ Override public void init ( final FilterConfig filterConfig ) throws ServletException { CorsAwareNegotiateSecurityFilter . LOGGER . info ( "[waffle.servlet.CorsAwareNegotiateSecurityFilter] Starting" ) ; super . init ( filterConfig ) ; CorsAwareNegotiateSecurityFilter . LOGGER . info ( "[waffle.servlet.CorsAwareNegotiateSecurityFilter] Started" ) ; }
@ Override public void init ( final FilterConfig filterConfig ) throws ServletException { CorsAwareNegotiateSecurityFilter . LOGGER . info ( "[waffle.servlet.CorsAwareNegotiateSecurityFilter] Starting" ) ; super . init ( filterConfig ) ; CorsAwareNegotiateSecurityFilter . LOGGER . info ( "[waffle.servlet.CorsAwareNegotiateSecurityFilter] Started" ) ; }
public void test() { if ( configuredLimit > recommendedLimit ) { LOG . warn ( "Configured connection limit {} is too high: Recommended is maximum {} (based on {})" , configuredLimit , recommendedLimit , strategy . getResourcesDescription ( ) ) ; } else { LOG . debug ( "Configured connection limit: {}" , configuredLimit ) ; } }
public void test() { if ( configuredLimit > recommendedLimit ) { LOG . warn ( "Configured connection limit {} is too high: Recommended is maximum {} (based on {})" , configuredLimit , recommendedLimit , strategy . getResourcesDescription ( ) ) ; } else { LOG . debug ( "Configured connection limit: {}" , configuredLimit ) ; } }
@ Override public void run ( ) { logger . info ( "Shutdown of management agent will commence in: " + MANAGEMENT_AGENT_SHUTDOWN_INTERNAL_SECONDS + " seconds" ) ; code_block = TryStatement ;  logger . info ( "Initiating shutdown of management agents" ) ; code_block = ForStatement ; }
@ Override public void run ( ) { logger . info ( "Shutdown of management agent will commence in: " + MANAGEMENT_AGENT_SHUTDOWN_INTERNAL_SECONDS + " seconds" ) ; code_block = TryStatement ;  logger . info ( "Initiating shutdown of management agents" ) ; code_block = ForStatement ; }
public void test() { try { Message msg = currentConnector . get ( batchSize , timeout , unit ) ; return msg ; } catch ( Throwable t ) { logger . warn ( String . format ( "something goes wrong when getting data from server:%s" , currentConnector != null ? currentConnector . getAddress ( ) : "null" ) , t ) ; times ++ ; restart ( ) ; logger . info ( "restart the connector for next round retry." ) ; } }
public void test() { try { Message msg = currentConnector . get ( batchSize , timeout , unit ) ; return msg ; } catch ( Throwable t ) { logger . warn ( String . format ( "something goes wrong when getting data from server:%s" , currentConnector != null ? currentConnector . getAddress ( ) : "null" ) , t ) ; times ++ ; restart ( ) ; logger . info ( "restart the connector for next round retry." ) ; } }
public void test() { if ( registration == null ) { localLogger . error ( "FileDAOHibernate updateDetached registration is null" ) ; } else-if ( registration . isLIMSAdmin ( ) || dbObject . givesPermission ( registration ) ) { localLogger . info ( "updateDetached file object" ) ; return updateDetached ( file ) ; } else { localLogger . error ( "FileDAOHibernate updateDetached not authorized" ) ; } }
public void test() { if ( registration == null ) { localLogger . error ( "FileDAOHibernate updateDetached registration is null" ) ; } else-if ( registration . isLIMSAdmin ( ) || dbObject . givesPermission ( registration ) ) { localLogger . info ( "updateDetached file object" ) ; return updateDetached ( file ) ; } else { localLogger . error ( "FileDAOHibernate updateDetached not authorized" ) ; } }
public void test() { if ( registration == null ) { localLogger . error ( "FileDAOHibernate updateDetached registration is null" ) ; } else-if ( registration . isLIMSAdmin ( ) || dbObject . givesPermission ( registration ) ) { localLogger . info ( "updateDetached file object" ) ; return updateDetached ( file ) ; } else { localLogger . error ( "FileDAOHibernate updateDetached not authorized" ) ; } }
public void test() { if ( hasEnvelopeForOffering ( offering ) ) { final ReferencedEnvelope offeringEnvelope = this . envelopeForOfferings . get ( offering ) ; LOG . trace ( "Expanding envelope {} for offering {} to include {}" , offeringEnvelope , offering , envelope ) ; offeringEnvelope . expandToInclude ( envelope ) ; } else { setEnvelopeForOffering ( offering , new ReferencedEnvelope ( envelope , getDefaultEPSGCode ( ) ) ) ; } }
public void test() { if ( ! future . isSuccess ( ) ) { logger . debug ( "Failed to send a 413 Request Entity Too Large." , future . cause ( ) ) ; ctx . close ( ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( PortalException portalException ) { _log . error ( portalException , portalException ) ; return false ; } }
public void test() { try { ResourceHost resourceHost = peerManager . getLocalPeer ( ) . getManagementHost ( ) ; resourceHost . joinP2PSwarmDHCP ( proxyDto . getP2pIfaceName ( ) , proxyDto . getP2pHash ( ) , proxyDto . getP2SecretKey ( ) , proxyDto . getP2pSecretTTL ( ) ) ; code_block = ForStatement ; proxyDto . setState ( ProxyDto . State . READY ) ; } catch ( Exception e ) { proxyDto . setState ( ProxyDto . State . FAILED ) ; proxyDto . setLogs ( e . getMessage ( ) ) ; log . error ( e . getMessage ( ) ) ; } }
public void test() { if ( message == null ) { LOGGER . info ( "Received empty message, count = " + count ) ; } else { LOGGER . info ( "Received message, id = " + message . getJMSMessageID ( ) ) ; receivedMessages . add ( ( MapMessage ) message ) ; } }
public void test() { if ( count >= MAX_MESSAGE_COUNT ) { LOGGER . warn ( "Not reading more messages, already read " + count + " messages." ) ; } }
public void test() { try { LOGGER . info ( "Starting to receive messages." ) ; int count = 0 ; code_block = ForStatement ; code_block = IfStatement ; } finally { LOGGER . info ( "Stopping to receive messages." ) ; connection . close ( ) ; } }
public void test() { try { Connection connection = new ActiveMQConnectionFactory ( brokerURL ) . createConnection ( ) ; connection . start ( ) ; Session session = connection . createSession ( false , Session . AUTO_ACKNOWLEDGE ) ; Destination destination = session . createTopic ( topicName ) ; MessageConsumer consumer = session . createConsumer ( destination ) ; code_block = TryStatement ;  } catch ( Exception e ) { LOGGER . info ( "caught exception: " + ExceptionUtils . getStackTrace ( e ) ) ; } }
public void migrateNavigation ( FileSystem sourceFS , FileSystem targetFS ) { LOGGER . info ( "attempt to migrate navigation" ) ; migrate ( sourceFS , targetFS , path -> path . getFileName ( ) . toString ( ) . equals ( "navtree.json" ) ) ; }
public void test() { try { Files . walk ( file . toPath ( ) ) . sorted ( Comparator . reverseOrder ( ) ) . map ( Path :: toFile ) . forEach ( File :: delete ) ; } catch ( IOException e ) { log . error ( "Failed to delete file: {}" , file , e ) ; } }
public synchronized void activate ( ComponentContext context , Map < String , Object > properties ) { logger . info ( "Bundle {} is starting!" , this . getClass ( ) . getSimpleName ( ) ) ; this . componentContext = context ; this . keystoreServiceOptions = new KeystoreServiceOptions ( properties , this . cryptoService ) ; this . selfUpdaterExecutor = Executors . newSingleThreadScheduledExecutor ( ) ; code_block = IfStatement ; logger . info ( "Bundle {} has started!" , this . getClass ( ) . getSimpleName ( ) ) ; }
public synchronized void activate ( ComponentContext context , Map < String , Object > properties ) { logger . info ( "Bundle {} is starting!" , this . getClass ( ) . getSimpleName ( ) ) ; this . componentContext = context ; this . keystoreServiceOptions = new KeystoreServiceOptions ( properties , this . cryptoService ) ; this . selfUpdaterExecutor = Executors . newSingleThreadScheduledExecutor ( ) ; code_block = IfStatement ; logger . info ( "Bundle {} has started!" , this . getClass ( ) . getSimpleName ( ) ) ; }
public void test() { try { LoadTestDataSimpleResponseMessageType response = ( LoadTestDataSimpleResponseMessageType ) invokeClientPort ( AdminWSConstants . ADMIN_LTD_SAVEPATIENT , request ) ; logDebug ( AdminWSConstants . ADMIN_LTD_SAVEPATIENT , response . isStatus ( ) , response . getMessage ( ) ) ; code_block = IfStatement ; return response . isStatus ( ) ; } catch ( Exception e ) { LOG . error ( "error during save patient: {}" , e . getLocalizedMessage ( ) , e ) ; } }
public void test() { if ( iStatus . getStatus ( ) == null ) { logger . info ( "Missing status on Status Object. " ) ; return ; } }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; WatchdogDistributionStatus watchdogDistributionStatus = watchdogDistributionStatusRepository . findById ( iStatus . getDistributionID ( ) ) . orElseGet ( ( ) -> null ) ; code_block = IfStatement ; logger . debug ( event ) ; toscaInstaller . installTheComponentStatus ( iStatus ) ; } catch ( ArtifactInstallerException e ) { logger . error ( "Error in ASDCStatusCallback {}" , e . getMessage ( ) , e ) ; logger . debug ( "Error in ASDCStatusCallback {}" , e . getMessage ( ) ) ; } }
@ Override protected boolean bridgeDirectCommunicate ( BridgeCommunicationProtocol communication , boolean useAuthentication ) { logger . trace ( "bridgeDirectCommunicate(BCP: {},{}authenticated) called." , communication . name ( ) , useAuthentication ? "" : "un" ) ; return bridgeDirectCommunicate ( ( JsonBridgeCommunicationProtocol ) communication , useAuthentication ) ; }
@ Override public void generate ( Model model , MolgenisOptions options ) throws Exception { Template template = createTemplate ( "/" + this . getClass ( ) . getSimpleName ( ) + ".psql.ftl" ) ; Map < String , Object > templateArgs = createTemplateArguments ( options ) ; List < Entity > entityList = model . getEntities ( ) ; entityList = MolgenisModel . sortEntitiesByDependency ( entityList , model ) ; File target = new File ( this . getSqlPath ( options ) + "/create_tables.sql" ) ; boolean created = target . getParentFile ( ) . mkdirs ( ) ; code_block = IfStatement ; templateArgs . put ( "model" , model ) ; templateArgs . put ( "entities" , entityList ) ; OutputStream targetOut = new FileOutputStream ( target ) ; template . process ( templateArgs , new OutputStreamWriter ( targetOut , Charset . forName ( "UTF-8" ) ) ) ; targetOut . close ( ) ; logger . info ( "generated " + target ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Loading properties files from '{}' with loader '{}'" , resourceStream , loader ) ; } }
public void test() { try { in = new BufferedInputStream ( resourceStream . getInputStream ( ) ) ; ValueMap data = loader . loadWicketProperties ( in ) ; code_block = IfStatement ; return data ; } catch ( ResourceStreamNotFoundException | IOException e ) { log . warn ( "Unable to find resource " + resourceStream , e ) ; } finally { IOUtils . closeQuietly ( in ) ; IOUtils . closeQuietly ( resourceStream ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "clients size: {}" , clients . size ( ) ) ; } }
@ Override public void log ( JobLogPo jobLogPo ) { LOGGER . info ( JSON . toJSONString ( jobLogPo ) ) ; }
public void test() { try { DeployedUnit deployedUnit = deploymentService . getDeployedUnit ( id ) ; code_block = IfStatement ; } catch ( Exception e ) { messages . add ( new Message ( Severity . WARN , "Unable to create image reference for container " + id + " by extension " + this + " due to " + e . getMessage ( ) ) ) ; logger . warn ( "Unable to create image reference for container {} due to {}" , id , e . getMessage ( ) , e ) ; } }
@ Override public BuildSetTask build ( BuildConfigurationSet buildConfigurationSet , User user , BuildOptions buildOptions ) { logger . warn ( "Invoking unimplemented method build" ) ; return Mockito . mock ( BuildSetTask . class ) ; }
public void test() { try { pt = ( PreparedTickler ) cl . loadClass ( className ) . newInstance ( ) ; } catch ( Exception e ) { log . warn ( "Warning" , e ) ; } }
public void stop ( ) { running = false ; logger . debug ( "{} stopped" , this ) ; close ( ) ; }
void unblockTo ( Address address ) { LOGGER . info ( "Unblocked messages to " + address ) ; LockPair lockPair = getLockPair ( address ) ; lockPair . unblockOutgoing ( ) ; }
public void test() { if ( _log . isWarnEnabled ( ) ) { _log . warn ( "Unable to parse date " + value , exception ) ; } }
public void test() { try { return env . getServerHome ( ) . getCanonicalPath ( ) ; } catch ( IOException e ) { log . error ( "Cannot get home path" ) ; return null ; } }
@ Override public void error ( Marker marker , String message , Throwable t ) { getLogger ( ) . error ( marker , message , t ) ; }
@ Test public void shortExample5 ( ) { String sequence = "QIKDLLVSSSTDLDTTLVLVNAIYFKGMWKTAFNAEDTRECMPFHVTKQESKPVQMMCMNNSFNVATLPAE" ; logger . debug ( "Absorbance (Cys Reduced): {}" , PeptideProperties . getAbsorbance ( sequence , true ) ) ; logger . debug ( "Absorbance (Cys Not Reduced): {}" , PeptideProperties . getAbsorbance ( sequence , false ) ) ; logger . debug ( "Extinction Coefficient (Cys Reduced): {}" , PeptideProperties . getExtinctionCoefficient ( sequence , true ) ) ; logger . debug ( "Extinction Coefficient (Cys Not Reduced): {}" , PeptideProperties . getExtinctionCoefficient ( sequence , false ) ) ; logger . debug ( "Instability Index: {}" , PeptideProperties . getInstabilityIndex ( sequence ) ) ; logger . debug ( "Apliphatic Index: {}" , PeptideProperties . getApliphaticIndex ( sequence ) ) ; logger . debug ( "Average Hydropathy Value: {}" , PeptideProperties . getAvgHydropathy ( sequence ) ) ; logger . debug ( "Isoelectric Point: {}" , PeptideProperties . getIsoelectricPoint ( sequence ) ) ; logger . debug ( "Net Charge at pH 7: {}" , PeptideProperties . getNetCharge ( sequence ) ) ; }
@ Test public void shortExample5 ( ) { String sequence = "QIKDLLVSSSTDLDTTLVLVNAIYFKGMWKTAFNAEDTRECMPFHVTKQESKPVQMMCMNNSFNVATLPAE" ; logger . debug ( "Absorbance (Cys Reduced): {}" , PeptideProperties . getAbsorbance ( sequence , true ) ) ; logger . debug ( "Absorbance (Cys Not Reduced): {}" , PeptideProperties . getAbsorbance ( sequence , false ) ) ; logger . debug ( "Extinction Coefficient (Cys Reduced): {}" , PeptideProperties . getExtinctionCoefficient ( sequence , true ) ) ; logger . debug ( "Extinction Coefficient (Cys Not Reduced): {}" , PeptideProperties . getExtinctionCoefficient ( sequence , false ) ) ; logger . debug ( "Instability Index: {}" , PeptideProperties . getInstabilityIndex ( sequence ) ) ; logger . debug ( "Apliphatic Index: {}" , PeptideProperties . getApliphaticIndex ( sequence ) ) ; logger . debug ( "Average Hydropathy Value: {}" , PeptideProperties . getAvgHydropathy ( sequence ) ) ; logger . debug ( "Isoelectric Point: {}" , PeptideProperties . getIsoelectricPoint ( sequence ) ) ; logger . debug ( "Net Charge at pH 7: {}" , PeptideProperties . getNetCharge ( sequence ) ) ; }
@ Test public void shortExample5 ( ) { String sequence = "QIKDLLVSSSTDLDTTLVLVNAIYFKGMWKTAFNAEDTRECMPFHVTKQESKPVQMMCMNNSFNVATLPAE" ; logger . debug ( "Absorbance (Cys Reduced): {}" , PeptideProperties . getAbsorbance ( sequence , true ) ) ; logger . debug ( "Absorbance (Cys Not Reduced): {}" , PeptideProperties . getAbsorbance ( sequence , false ) ) ; logger . debug ( "Extinction Coefficient (Cys Reduced): {}" , PeptideProperties . getExtinctionCoefficient ( sequence , true ) ) ; logger . debug ( "Extinction Coefficient (Cys Not Reduced): {}" , PeptideProperties . getExtinctionCoefficient ( sequence , false ) ) ; logger . debug ( "Instability Index: {}" , PeptideProperties . getInstabilityIndex ( sequence ) ) ; logger . debug ( "Apliphatic Index: {}" , PeptideProperties . getApliphaticIndex ( sequence ) ) ; logger . debug ( "Average Hydropathy Value: {}" , PeptideProperties . getAvgHydropathy ( sequence ) ) ; logger . debug ( "Isoelectric Point: {}" , PeptideProperties . getIsoelectricPoint ( sequence ) ) ; logger . debug ( "Net Charge at pH 7: {}" , PeptideProperties . getNetCharge ( sequence ) ) ; }
@ Test public void shortExample5 ( ) { String sequence = "QIKDLLVSSSTDLDTTLVLVNAIYFKGMWKTAFNAEDTRECMPFHVTKQESKPVQMMCMNNSFNVATLPAE" ; logger . debug ( "Absorbance (Cys Reduced): {}" , PeptideProperties . getAbsorbance ( sequence , true ) ) ; logger . debug ( "Absorbance (Cys Not Reduced): {}" , PeptideProperties . getAbsorbance ( sequence , false ) ) ; logger . debug ( "Extinction Coefficient (Cys Reduced): {}" , PeptideProperties . getExtinctionCoefficient ( sequence , true ) ) ; logger . debug ( "Extinction Coefficient (Cys Not Reduced): {}" , PeptideProperties . getExtinctionCoefficient ( sequence , false ) ) ; logger . debug ( "Instability Index: {}" , PeptideProperties . getInstabilityIndex ( sequence ) ) ; logger . debug ( "Apliphatic Index: {}" , PeptideProperties . getApliphaticIndex ( sequence ) ) ; logger . debug ( "Average Hydropathy Value: {}" , PeptideProperties . getAvgHydropathy ( sequence ) ) ; logger . debug ( "Isoelectric Point: {}" , PeptideProperties . getIsoelectricPoint ( sequence ) ) ; logger . debug ( "Net Charge at pH 7: {}" , PeptideProperties . getNetCharge ( sequence ) ) ; }
@ Test public void shortExample5 ( ) { String sequence = "QIKDLLVSSSTDLDTTLVLVNAIYFKGMWKTAFNAEDTRECMPFHVTKQESKPVQMMCMNNSFNVATLPAE" ; logger . debug ( "Absorbance (Cys Reduced): {}" , PeptideProperties . getAbsorbance ( sequence , true ) ) ; logger . debug ( "Absorbance (Cys Not Reduced): {}" , PeptideProperties . getAbsorbance ( sequence , false ) ) ; logger . debug ( "Extinction Coefficient (Cys Reduced): {}" , PeptideProperties . getExtinctionCoefficient ( sequence , true ) ) ; logger . debug ( "Extinction Coefficient (Cys Not Reduced): {}" , PeptideProperties . getExtinctionCoefficient ( sequence , false ) ) ; logger . debug ( "Instability Index: {}" , PeptideProperties . getInstabilityIndex ( sequence ) ) ; logger . debug ( "Apliphatic Index: {}" , PeptideProperties . getApliphaticIndex ( sequence ) ) ; logger . debug ( "Average Hydropathy Value: {}" , PeptideProperties . getAvgHydropathy ( sequence ) ) ; logger . debug ( "Isoelectric Point: {}" , PeptideProperties . getIsoelectricPoint ( sequence ) ) ; logger . debug ( "Net Charge at pH 7: {}" , PeptideProperties . getNetCharge ( sequence ) ) ; }
@ Test public void shortExample5 ( ) { String sequence = "QIKDLLVSSSTDLDTTLVLVNAIYFKGMWKTAFNAEDTRECMPFHVTKQESKPVQMMCMNNSFNVATLPAE" ; logger . debug ( "Absorbance (Cys Reduced): {}" , PeptideProperties . getAbsorbance ( sequence , true ) ) ; logger . debug ( "Absorbance (Cys Not Reduced): {}" , PeptideProperties . getAbsorbance ( sequence , false ) ) ; logger . debug ( "Extinction Coefficient (Cys Reduced): {}" , PeptideProperties . getExtinctionCoefficient ( sequence , true ) ) ; logger . debug ( "Extinction Coefficient (Cys Not Reduced): {}" , PeptideProperties . getExtinctionCoefficient ( sequence , false ) ) ; logger . debug ( "Instability Index: {}" , PeptideProperties . getInstabilityIndex ( sequence ) ) ; logger . debug ( "Apliphatic Index: {}" , PeptideProperties . getApliphaticIndex ( sequence ) ) ; logger . debug ( "Average Hydropathy Value: {}" , PeptideProperties . getAvgHydropathy ( sequence ) ) ; logger . debug ( "Isoelectric Point: {}" , PeptideProperties . getIsoelectricPoint ( sequence ) ) ; logger . debug ( "Net Charge at pH 7: {}" , PeptideProperties . getNetCharge ( sequence ) ) ; }
@ Test public void shortExample5 ( ) { String sequence = "QIKDLLVSSSTDLDTTLVLVNAIYFKGMWKTAFNAEDTRECMPFHVTKQESKPVQMMCMNNSFNVATLPAE" ; logger . debug ( "Absorbance (Cys Reduced): {}" , PeptideProperties . getAbsorbance ( sequence , true ) ) ; logger . debug ( "Absorbance (Cys Not Reduced): {}" , PeptideProperties . getAbsorbance ( sequence , false ) ) ; logger . debug ( "Extinction Coefficient (Cys Reduced): {}" , PeptideProperties . getExtinctionCoefficient ( sequence , true ) ) ; logger . debug ( "Extinction Coefficient (Cys Not Reduced): {}" , PeptideProperties . getExtinctionCoefficient ( sequence , false ) ) ; logger . debug ( "Instability Index: {}" , PeptideProperties . getInstabilityIndex ( sequence ) ) ; logger . debug ( "Apliphatic Index: {}" , PeptideProperties . getApliphaticIndex ( sequence ) ) ; logger . debug ( "Average Hydropathy Value: {}" , PeptideProperties . getAvgHydropathy ( sequence ) ) ; logger . debug ( "Isoelectric Point: {}" , PeptideProperties . getIsoelectricPoint ( sequence ) ) ; logger . debug ( "Net Charge at pH 7: {}" , PeptideProperties . getNetCharge ( sequence ) ) ; }
@ Test public void shortExample5 ( ) { String sequence = "QIKDLLVSSSTDLDTTLVLVNAIYFKGMWKTAFNAEDTRECMPFHVTKQESKPVQMMCMNNSFNVATLPAE" ; logger . debug ( "Absorbance (Cys Reduced): {}" , PeptideProperties . getAbsorbance ( sequence , true ) ) ; logger . debug ( "Absorbance (Cys Not Reduced): {}" , PeptideProperties . getAbsorbance ( sequence , false ) ) ; logger . debug ( "Extinction Coefficient (Cys Reduced): {}" , PeptideProperties . getExtinctionCoefficient ( sequence , true ) ) ; logger . debug ( "Extinction Coefficient (Cys Not Reduced): {}" , PeptideProperties . getExtinctionCoefficient ( sequence , false ) ) ; logger . debug ( "Instability Index: {}" , PeptideProperties . getInstabilityIndex ( sequence ) ) ; logger . debug ( "Apliphatic Index: {}" , PeptideProperties . getApliphaticIndex ( sequence ) ) ; logger . debug ( "Average Hydropathy Value: {}" , PeptideProperties . getAvgHydropathy ( sequence ) ) ; logger . debug ( "Isoelectric Point: {}" , PeptideProperties . getIsoelectricPoint ( sequence ) ) ; logger . debug ( "Net Charge at pH 7: {}" , PeptideProperties . getNetCharge ( sequence ) ) ; }
@ Test public void shortExample5 ( ) { String sequence = "QIKDLLVSSSTDLDTTLVLVNAIYFKGMWKTAFNAEDTRECMPFHVTKQESKPVQMMCMNNSFNVATLPAE" ; logger . debug ( "Absorbance (Cys Reduced): {}" , PeptideProperties . getAbsorbance ( sequence , true ) ) ; logger . debug ( "Absorbance (Cys Not Reduced): {}" , PeptideProperties . getAbsorbance ( sequence , false ) ) ; logger . debug ( "Extinction Coefficient (Cys Reduced): {}" , PeptideProperties . getExtinctionCoefficient ( sequence , true ) ) ; logger . debug ( "Extinction Coefficient (Cys Not Reduced): {}" , PeptideProperties . getExtinctionCoefficient ( sequence , false ) ) ; logger . debug ( "Instability Index: {}" , PeptideProperties . getInstabilityIndex ( sequence ) ) ; logger . debug ( "Apliphatic Index: {}" , PeptideProperties . getApliphaticIndex ( sequence ) ) ; logger . debug ( "Average Hydropathy Value: {}" , PeptideProperties . getAvgHydropathy ( sequence ) ) ; logger . debug ( "Isoelectric Point: {}" , PeptideProperties . getIsoelectricPoint ( sequence ) ) ; logger . debug ( "Net Charge at pH 7: {}" , PeptideProperties . getNetCharge ( sequence ) ) ; }
public void test() { try { InputStream in = BalancerRunner . class . getResourceAsStream ( "release.properties" ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . warn ( "Unable to extract the version of Restcomm Load Balancer currently running" , e ) ; } }
public void test() { try { checkForToken ( console ) ; setResourceLocation ( tokenFile . getCanonicalPath ( ) ) ; salt = "Ge0W@v3-Ro0t-K3y" . getBytes ( "UTF-8" ) ; generateRootKeyFromToken ( ) ; } catch ( final Throwable t ) { LOGGER . error ( t . getLocalizedMessage ( ) , t ) ; } }
public void test() { if ( CommonUtils . isBlank ( value ) ) { logger . trace ( "No value found for property {}, returning default {}" , configurationKey . getName ( ) , configurationKey . getDefaultValue ( ) ) ; return configurationKey . getDefaultValue ( ) ; } else { logger . trace ( "Loaded property {} with value {}" , configurationKey . getName ( ) , configurationKey . getDefaultValue ( ) ) ; } }
public void test() { if ( CommonUtils . isBlank ( value ) ) { logger . trace ( "No value found for property {}, returning default {}" , configurationKey . getName ( ) , configurationKey . getDefaultValue ( ) ) ; return configurationKey . getDefaultValue ( ) ; } else { logger . trace ( "Loaded property {} with value {}" , configurationKey . getName ( ) , configurationKey . getDefaultValue ( ) ) ; } }
public static void handle ( MessagePublisher publisher , PipelinesBalancerMessage message ) throws IOException { log . info ( "Process PipelinesIndexedMessage - {}" , message ) ; ObjectMapper mapper = new ObjectMapper ( ) ; PipelinesIndexedMessage m = mapper . readValue ( message . getPayload ( ) , PipelinesIndexedMessage . class ) ; PipelinesIndexedMessage outputMessage = new PipelinesIndexedMessage ( m . getDatasetUuid ( ) , m . getAttempt ( ) , m . getPipelineSteps ( ) , m . getRunner ( ) , m . getExecutionId ( ) ) ; publisher . send ( outputMessage ) ; log . info ( "The message has been sent - {}" , outputMessage ) ; }
public static void handle ( MessagePublisher publisher , PipelinesBalancerMessage message ) throws IOException { log . info ( "Process PipelinesIndexedMessage - {}" , message ) ; ObjectMapper mapper = new ObjectMapper ( ) ; PipelinesIndexedMessage m = mapper . readValue ( message . getPayload ( ) , PipelinesIndexedMessage . class ) ; PipelinesIndexedMessage outputMessage = new PipelinesIndexedMessage ( m . getDatasetUuid ( ) , m . getAttempt ( ) , m . getPipelineSteps ( ) , m . getRunner ( ) , m . getExecutionId ( ) ) ; publisher . send ( outputMessage ) ; log . info ( "The message has been sent - {}" , outputMessage ) ; }
public void test() { if ( workflow == null ) { LOG . debug ( "Workflow {} does not exist possibly due to stale workflow cache" , state . workflowInstance ( ) . workflowId ( ) . toKey ( ) ) ; return false ; } }
public void test() { if ( pkg . contains ( OPERATION_REVISON_FILE ) ) { byte [ ] revisionByteArray = pkg . getBytes ( OPERATION_REVISON_FILE ) ; code_block = IfStatement ; } else { LOG . debug ( "Created a new revision number: 1" ) ; } }
public void test() { try { OdfPackage pkg = mTextDocument . getPackage ( ) ; int revisionNo = 0 ; code_block = IfStatement ; revisionNo ++ ; pkg . insert ( operations . toString ( ) . getBytes ( ) , OPERATION_TEXT_FILE_PREFIX + revisionNo + ".txt" , "text/plain" ) ; pkg . insert ( Integer . toString ( revisionNo ) . getBytes ( ) , OPERATION_REVISON_FILE , "text/plain" ) ; } catch ( Exception ex ) { LOG . error ( null , ex ) ; } }
public void test() { try { resultSet . close ( ) ; } catch ( final SQLException e ) { LOGGER . warn ( "Caught exception while closing result set '" + resultSet + "', ignoring." , e ) ; } }
@ Test public void get ( ) { LOG . info ( "\n===RunDaoTest.get===\n" ) ; Run run = ESSuiteTest . runDao . get ( ESSuiteTest . RUN_ID_3 ) ; assertEquals ( ESSuiteTest . COMMIT_ID_3 , run . getCommitId ( ) ) ; assertEquals ( ESSuiteTest . RUNNER_HOSTNAME_1 , run . getRunner ( ) ) ; assertEquals ( ESSuiteTest . RUN_DURATION , ( Long ) run . getActualTime ( ) ) ; }
public void test() { try { int [ ] data = ( int [ ] ) notification . getUserData ( ) ; int repairNo = data [ 0 ] ; ActiveRepairService . Status status = ActiveRepairService . Status . values ( ) [ data [ 1 ] ] ; String message = notification . getMessage ( ) ; code_block = IfStatement ; } catch ( RuntimeException e ) { LOG . error ( "Error while processing JMX notification" , e ) ; } }
public AbstractFeatureEntity getFeature ( String identifier , Session session ) { Criteria criteria = getDefaultCriteria ( session ) . add ( Restrictions . eq ( AbstractFeatureEntity . IDENTIFIER , identifier ) ) ; LOGGER . trace ( "QUERY getFeature(identifier): {}" , HibernateHelper . getSqlString ( criteria ) ) ; return ( AbstractFeatureEntity ) criteria . uniqueResult ( ) ; }
public void test() { try { AddressBookId addressBookId = folder . getTypedBackendId ( ) ; return getBookClient ( ) . getContactFromId ( getAccessToken ( ) , addressBookId . getId ( ) , serverId . getItemId ( ) ) != null ; } catch ( ServerFault | ContactNotFoundException e ) { logger . info ( "This contact has not been found by obm-sync" , e ) ; } }
public void test() { if ( logger . isTraceEnabled ( LogMarker . SERIALIZER_VERBOSE ) ) { logger . trace ( LogMarker . SERIALIZER_VERBOSE , "Writing Long {}" , value ) ; } }
@ Override protected void startJsonRpcServer ( RomServerJsonRpcHandler jsonRpcHandler ) { handler = jsonRpcHandler ; Properties properties = new Properties ( ) ; String port = getPort ( ) ; properties . put ( "server.port" , port ) ; SpringApplication application = new SpringApplication ( BootTestApplication . class ) ; application . setDefaultProperties ( properties ) ; log . debug ( "Creating server in port: " + port ) ; context = application . run ( ) ; }
public void test() { try { logger . debug ( "Adding AWS {} mapping to database: {} points to {}, object version {}" , getStoreType ( ) , storagePath , objectName , objectVersion ) ; database . storeMapping ( storagePath , objectName , objectVersion ) ; } catch ( AwsAssetDatabaseException e ) { throw new AssetStoreException ( e ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( sb . toString ( ) ) ; } }
public void test() { if ( ! initializer . isDatabaseStructureCreated ( checkSql ) ) { log . info ( "Initializing Certificate management repository database schema" ) ; initializer . createRegistryDatabase ( ) ; } else { log . info ( "Certificate management repository database already exists. Not creating a new database." ) ; } }
public void test() { if ( ! initializer . isDatabaseStructureCreated ( checkSql ) ) { log . info ( "Initializing Certificate management repository database schema" ) ; initializer . createRegistryDatabase ( ) ; } else { log . info ( "Certificate management repository database already exists. Not creating a new database." ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Certificate management metadata repository schema has been successfully initialized" ) ; } }
@ Override protected void doInTransactionWithoutResult ( TransactionStatus status ) { emailDao . populateEmailHash ( email , correctedEmailHash ) ; LOG . info ( "Fixed: " + orcid + " " + correctedEmailHash ) ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( noSuchFileShortcutException , noSuchFileShortcutException ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Missing requested artifact. [artifact=({}), contract=({}), " + "issuer=({}), messageId=({})]" , requestedArtifact , transferContract , issuerConnector , messageId ) ; } }
public void test() { try { log . trace ( "Failed message " + msg . getJMSMessageID ( ) ) ; } catch ( JMSException e ) { log . warn ( "Could not identify failed message " , e ) ; } }
public void test() { try { log . trace ( "Failed message " + msg . getJMSMessageID ( ) ) ; } catch ( JMSException e ) { log . warn ( "Could not identify failed message " , e ) ; } }
public void test() { if ( messages != null ) { code_block = ForStatement ; } else { log . warn ( "Failed batch has no messages with transaction id " + transactionId ) ; } }
public void test() { try { listener . onUpdate ( identifiable , attribute , variantId , oldValue , newValue ) ; } catch ( Exception t ) { LOGGER . error ( t . toString ( ) , t ) ; } }
public void test() { try { URLConnection connection = IOUtils . getUrlConnection ( documentURI , CONNECTION_ACCEPT_HTTP_COMPRESSION , CONNECTION_TIMEOUT ) ; code_block = IfStatement ; } catch ( MalformedURLException e ) { logger . info ( "Imported ontology document IRI {} is malformed." , ontologyIRI ) ; } catch ( FileNotFoundException e ) { logger . info ( "Imported ontology document {} does not exist on the Web (File Not Found)." , ontologyIRI ) ; } catch ( UnknownHostException e ) { String host = e . getMessage ( ) ; logger . info ( "Imported ontology document {} could not be retrieved. Cannot connect to {} (Unknown Host)." , ontologyIRI , host ) ; } catch ( IOException e ) { logger . info ( "Imported ontology document {} could not be retrieved: {}" , ontologyIRI , e . getMessage ( ) ) ; } }
public void test() { try { URLConnection connection = IOUtils . getUrlConnection ( documentURI , CONNECTION_ACCEPT_HTTP_COMPRESSION , CONNECTION_TIMEOUT ) ; code_block = IfStatement ; } catch ( MalformedURLException e ) { logger . info ( "Imported ontology document IRI {} is malformed." , ontologyIRI ) ; } catch ( FileNotFoundException e ) { logger . info ( "Imported ontology document {} does not exist on the Web (File Not Found)." , ontologyIRI ) ; } catch ( UnknownHostException e ) { String host = e . getMessage ( ) ; logger . info ( "Imported ontology document {} could not be retrieved. Cannot connect to {} (Unknown Host)." , ontologyIRI , host ) ; } catch ( IOException e ) { logger . info ( "Imported ontology document {} could not be retrieved: {}" , ontologyIRI , e . getMessage ( ) ) ; } }
public void test() { try { URLConnection connection = IOUtils . getUrlConnection ( documentURI , CONNECTION_ACCEPT_HTTP_COMPRESSION , CONNECTION_TIMEOUT ) ; code_block = IfStatement ; } catch ( MalformedURLException e ) { logger . info ( "Imported ontology document IRI {} is malformed." , ontologyIRI ) ; } catch ( FileNotFoundException e ) { logger . info ( "Imported ontology document {} does not exist on the Web (File Not Found)." , ontologyIRI ) ; } catch ( UnknownHostException e ) { String host = e . getMessage ( ) ; logger . info ( "Imported ontology document {} could not be retrieved. Cannot connect to {} (Unknown Host)." , ontologyIRI , host ) ; } catch ( IOException e ) { logger . info ( "Imported ontology document {} could not be retrieved: {}" , ontologyIRI , e . getMessage ( ) ) ; } }
public void test() { try { URLConnection connection = IOUtils . getUrlConnection ( documentURI , CONNECTION_ACCEPT_HTTP_COMPRESSION , CONNECTION_TIMEOUT ) ; code_block = IfStatement ; } catch ( MalformedURLException e ) { logger . info ( "Imported ontology document IRI {} is malformed." , ontologyIRI ) ; } catch ( FileNotFoundException e ) { logger . info ( "Imported ontology document {} does not exist on the Web (File Not Found)." , ontologyIRI ) ; } catch ( UnknownHostException e ) { String host = e . getMessage ( ) ; logger . info ( "Imported ontology document {} could not be retrieved. Cannot connect to {} (Unknown Host)." , ontologyIRI , host ) ; } catch ( IOException e ) { logger . info ( "Imported ontology document {} could not be retrieved: {}" , ontologyIRI , e . getMessage ( ) ) ; } }
public void test() { try ( AsyncHttpClient client = getAsyncHttpClient ( cg ) ) { String redirectTarget = "bar/test1" ; String destinationUrl = new URI ( getTargetUrl ( ) ) . resolve ( redirectTarget ) . toString ( ) ; Response response = client . prepareGet ( getTargetUrl ( ) ) . setHeader ( "X-redirect" , redirectTarget ) . execute ( ) . get ( ) ; assertNotNull ( response ) ; assertEquals ( response . getStatusCode ( ) , 200 ) ; assertEquals ( response . getUri ( ) . toString ( ) , destinationUrl ) ; log . debug ( "{} was redirected to {}" , redirectTarget , destinationUrl ) ; } }
public void test() { try { TajoMaster master = new TajoMaster ( ) ; TajoConf conf = new TajoConf ( ) ; master . init ( conf ) ; master . start ( ) ; } catch ( Throwable t ) { LOG . fatal ( "Error starting TajoMaster" , t ) ; System . exit ( - 1 ) ; } }
public void test() { try { com . liferay . portal . kernel . repository . model . FileEntry returnValue = DLTrashServiceUtil . moveFileEntryToTrash ( fileEntryId ) ; return com . liferay . portal . kernel . repository . model . FileEntrySoap . toSoapModel ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( ! productToMetacardIdMap . loadAllKeys ( ) . contains ( ref ) ) { LOGGER . debug ( "Received a [{}] operation, but no mapped metacardIds were available for product [{}]." , catalogOperation , referenceKey ) ; return null ; } }
public void test() { if ( validPass ) { LOGGER . info ( "Using Fedora Client with valid user, valid pass" ) ; code_block = IfStatement ; return CLIENT_VALID_USER_VALID_PASS ; } else { LOGGER . info ( "Using Fedora Client with valid user, bogus pass" ) ; code_block = IfStatement ; return CLIENT_VALID_USER_BOGUS_PASS ; } }
public void test() { if ( validPass ) { LOGGER . info ( "Using Fedora Client with valid user, valid pass" ) ; code_block = IfStatement ; return CLIENT_VALID_USER_VALID_PASS ; } else { LOGGER . info ( "Using Fedora Client with valid user, bogus pass" ) ; code_block = IfStatement ; return CLIENT_VALID_USER_BOGUS_PASS ; } }
public void test() { if ( args . length != ARGS_LENGTH ) { logger . error ( "Usage: <node>" ) ; return ; } }
@ Test public void skimPomForExistingRepoAndAddItInGroup ( ) throws Exception { RemoteRepository repo = new RemoteRepository ( REPO , server . formatUrl ( REPO ) ) ; repo . setAllowReleases ( Boolean . TRUE ) ; repo . setAllowSnapshots ( Boolean . FALSE ) ; repo = client . stores ( ) . create ( repo , "Pre stored remote repo" , RemoteRepository . class ) ; final StoreKey remoteRepoKey = repo . getKey ( ) ; final PomRef ref = loadPom ( "one-repo" , Collections . singletonMap ( "one-repo.url" , server . formatUrl ( REPO ) ) ) ; server . expect ( "HEAD" , server . formatUrl ( REPO , "/" ) , 200 , ( String ) null ) ; server . expect ( server . formatUrl ( TEST_REPO , ref . path ) , 200 , ref . pom ) ; final StoreKey pubGroupKey = new StoreKey ( MavenPackageTypeDescriptor . MAVEN_PKG_KEY , StoreType . group , PUBLIC ) ; Group g = client . stores ( ) . load ( pubGroupKey , Group . class ) ; assertThat ( "Group membership should not contain implied before getting pom." , g . getConstituents ( ) . contains ( remoteRepoKey ) , equalTo ( false ) ) ; logger . debug ( "Start fetching pom!" ) ; final InputStream stream = client . content ( ) . get ( pubGroupKey , ref . path ) ; final String downloaded = IOUtils . toString ( stream ) ; IOUtils . closeQuietly ( stream ) ; System . out . println ( "Waiting 5s for events to run." ) ; Thread . sleep ( 5000 ) ; g = client . stores ( ) . load ( pubGroupKey , Group . class ) ; assertThat ( "Group membership does not contain implied repository" , g . getConstituents ( ) . contains ( remoteRepoKey ) , equalTo ( true ) ) ; repo = client . stores ( ) . load ( new StoreKey ( MavenPackageTypeDescriptor . MAVEN_PKG_KEY , StoreType . remote , TEST_REPO ) , RemoteRepository . class ) ; String metadata = repo . getMetadata ( ImpliedRepoMetadataManager . IMPLIED_STORES ) ; assertThat ( "Reference to repositories implied by POMs in this repo is missing from metadata." , metadata . contains ( "remote:" + REPO ) , equalTo ( true ) ) ; repo = client . stores ( ) . load ( remoteRepoKey , RemoteRepository . class ) ; metadata = repo . getMetadata ( ImpliedRepoMetadataManager . IMPLIED_BY_STORES ) ; assertThat ( "Backref to repo with pom that implies this repo is missing from metadata." , metadata . contains ( "remote:" + TEST_REPO ) , equalTo ( true ) ) ; }
public void test() { if ( Math . abs ( millisNow - millisStored ) > TimeUnit . MILLISECONDS . convert ( 1 , TimeUnit . HOURS ) ) { user . setLastAuthenticationTimestamp ( new java . sql . Timestamp ( millisNow ) ) ; context . commitChanges ( ) ; LOGGER . debug ( "did store last authenticated timestamp for user [{}]" , user . getNickname ( ) ) ; } }
public void test() { try { code_block = IfStatement ; Lang lang = this . extractLang ( pageUrl , reqCtx ) ; IPage destPage = this . extractDestPage ( pageUrl , reqCtx ) ; String friendlyCode = ( ( PageURL ) pageUrl ) . getFriendlyCode ( ) ; code_block = IfStatement ; HttpServletRequest request = ( null != reqCtx ) ? reqCtx . getRequest ( ) : null ; String url = null ; code_block = IfStatement ; code_block = IfStatement ; } catch ( Exception e ) { _logger . error ( "Error creating url" , e ) ; throw new RuntimeException ( "Error creating url" , e ) ; } }
public void test() { if ( ! graphiteConfiguration . isEnabled ( ) ) { LOG . info ( "Not reporting data points to graphite." ) ; return ; } }
@ Override public void start ( ) throws Exception { code_block = IfStatement ; LOG . info ( "Reporting data points to graphite server {}:{} every {} seconds with prefix '{}' and predicates '{}'." , graphiteConfiguration . getHostname ( ) , graphiteConfiguration . getPort ( ) , graphiteConfiguration . getPeriodSeconds ( ) , graphiteConfiguration . getPrefix ( ) , JavaUtils . COMMA_JOINER . join ( graphiteConfiguration . getPredicates ( ) ) ) ; final Graphite graphite = new Graphite ( new InetSocketAddress ( graphiteConfiguration . getHostname ( ) , graphiteConfiguration . getPort ( ) ) ) ; final GraphiteReporter . Builder reporterBuilder = GraphiteReporter . forRegistry ( registry ) ; code_block = IfStatement ; code_block = IfStatement ; reporter = Optional . of ( reporterBuilder . build ( graphite ) ) ; reporter . get ( ) . start ( graphiteConfiguration . getPeriodSeconds ( ) , TimeUnit . SECONDS ) ; }
@ Override public List < Page > listPagesInCourse ( String courseId ) throws IOException { LOG . debug ( "fetching all pages for course " + courseId ) ; String url = buildCanvasUrl ( "courses/" + courseId + "/pages" , Collections . emptyMap ( ) ) ; return getListFromCanvas ( url ) ; }
@ Override public Set < ? extends NodeMetadata > listNodesByIds ( Iterable < String > ids ) { checkNotNull ( ids , "ids" ) ; logger . trace ( ">> listing node with ids(%s)" , ids ) ; Set < NodeMetadata > set = ImmutableSet . copyOf ( listNodesStrategy . listNodesByIds ( ids ) ) ; logger . trace ( "<< list(%d)" , set . size ( ) ) ; return set ; }
@ Override public Set < ? extends NodeMetadata > listNodesByIds ( Iterable < String > ids ) { checkNotNull ( ids , "ids" ) ; logger . trace ( ">> listing node with ids(%s)" , ids ) ; Set < NodeMetadata > set = ImmutableSet . copyOf ( listNodesStrategy . listNodesByIds ( ids ) ) ; logger . trace ( "<< list(%d)" , set . size ( ) ) ; return set ; }
public void test() { try { Response response = searchLocation ( request . getRequest ( ) , request . getRequestContext ( ) ) ; sender ( ) . tell ( response , self ( ) ) ; SearchDTO searchDto = Util . createSearchDto ( request . getRequest ( ) ) ; String [ ] types = code_block = "" ; ; generateSearchTelemetryEvent ( searchDto , types , response . getResult ( ) , request . getContext ( ) ) ; } catch ( Exception ex ) { logger . error ( request . getRequestContext ( ) , ex . getMessage ( ) , ex ) ; sender ( ) . tell ( ex , self ( ) ) ; } }
public void test() { -> { if ( throwable == null ) return CompletableFuture . completedFuture ( rebalancingStatus != RebalancingStatus . SUSPENDED ) ; code_block = IfStatement ; log . debug ( "Timed out waiting for rebalancing status from coordinator, trying again" ) ; return fetchRebalancingStatusFromCoordinator ( attempts - 1 ) ; } }
public void test() { try ( Tx tx = StructrApp . getInstance ( securityContext ) . tx ( ) ) { org . structr . web . entity . User structrUser = getStructrUser ( userName ) ; tx . success ( ) ; code_block = IfStatement ; } catch ( FrameworkException fex ) { logger . error ( "Unable to get user by its name" , fex ) ; } }
public void test() { try { authenticateProxy ( this . connectMethod ) ; } catch ( AuthenticationException e ) { LOG . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "The name of the queue region is {} and the size is {}. keyset size is {}" , prQ . getName ( ) , prQ . size ( ) , prQ . keys ( ) . size ( ) ) ; } }
public void test() { if ( _log . isWarnEnabled ( ) ) { _log . warn ( exception , exception ) ; } }
@ Override public Response toResponse ( SelfManagedOnlyException exception ) { LOG . error ( "Only self managed allowed!" , exception ) ; return Response . status ( Status . FORBIDDEN ) . entity ( new SelfManagedOnlyExceptionInfo ( Status . FORBIDDEN , exception ) ) . build ( ) ; }
public void test() { if ( portId == null ) { logger . debug ( "Unable to resolve port [{}] to an identifier" , portName ) ; } else { logger . debug ( "Resolved port [{}] to identifier [{}]" , portName , portId ) ; this . portIdentifier = portId ; } }
@ Override public void rdbBgsave ( Jedis jedis , String host , String port ) { Long currentTime = System . currentTimeMillis ( ) ; Long oldLastsave = jedis . lastsave ( ) ; LOG . info ( "RCT start to generate redis rdb file.{}:{}" , host , port ) ; jedis . bgsave ( ) ; boolean isCheck = true ; code_block = WhileStatement ; LOG . info ( "RCT generate redis rdb file success. cost time:{} ms" , ( System . currentTimeMillis ( ) - currentTime ) ) ; }
@ Override public void rdbBgsave ( Jedis jedis , String host , String port ) { Long currentTime = System . currentTimeMillis ( ) ; Long oldLastsave = jedis . lastsave ( ) ; LOG . info ( "RCT start to generate redis rdb file.{}:{}" , host , port ) ; jedis . bgsave ( ) ; boolean isCheck = true ; code_block = WhileStatement ; LOG . info ( "RCT generate redis rdb file success. cost time:{} ms" , ( System . currentTimeMillis ( ) - currentTime ) ) ; }
@ Ignore @ Test public void shouldBeFasterWhenRunningProcessingInParallel ( ) throws Exception { testMojoWithConfigurableWroManagerFactoryWithValidConfigFileSet ( ) ; final long begin = System . currentTimeMillis ( ) ; victim . setParallelProcessing ( false ) ; testMojoWithConfigurableWroManagerFactoryWithValidConfigFileSet ( ) ; final long endSerial = System . currentTimeMillis ( ) ; victim . setParallelProcessing ( true ) ; testMojoWithConfigurableWroManagerFactoryWithValidConfigFileSet ( ) ; final long endParallel = System . currentTimeMillis ( ) ; final long serial = endSerial - begin ; final long parallel = endParallel - endSerial ; LOG . info ( "serial took: {}ms" , serial ) ; LOG . info ( "parallel took: {}ms" , parallel ) ; assertTrue ( String . format ( "Serial (%s) > Parallel (%s)" , serial , parallel ) , serial > parallel ) ; }
@ Ignore @ Test public void shouldBeFasterWhenRunningProcessingInParallel ( ) throws Exception { testMojoWithConfigurableWroManagerFactoryWithValidConfigFileSet ( ) ; final long begin = System . currentTimeMillis ( ) ; victim . setParallelProcessing ( false ) ; testMojoWithConfigurableWroManagerFactoryWithValidConfigFileSet ( ) ; final long endSerial = System . currentTimeMillis ( ) ; victim . setParallelProcessing ( true ) ; testMojoWithConfigurableWroManagerFactoryWithValidConfigFileSet ( ) ; final long endParallel = System . currentTimeMillis ( ) ; final long serial = endSerial - begin ; final long parallel = endParallel - endSerial ; LOG . info ( "serial took: {}ms" , serial ) ; LOG . info ( "parallel took: {}ms" , parallel ) ; assertTrue ( String . format ( "Serial (%s) > Parallel (%s)" , serial , parallel ) , serial > parallel ) ; }
public void test() { try { jsonText = mapper . writerWithDefaultPrettyPrinter ( ) . writeValueAsString ( jsonObj ) ; } catch ( IOException e ) { LOGGER . error ( "Failed to transform JSON: " + e , e ) ; } }
public ClusterStatus checkStatus ( ) { List < Result > pollResults = poll ( ) ; Set < String > partitions = new HashSet < String > ( ) ; Set < String > incompleteJoinNodes = new HashSet < String > ( ) ; Set < String > incompleteStateTransferNodes = new HashSet < String > ( ) ; int numAvailableNodes = 0 ; int numMembers = - 1 ; boolean numMembersEqual = true ; log . trace ( "Number of results of polling: " + pollResults . size ( ) ) ; code_block = ForStatement ; return new ClusterStatus ( numAvailableNodes , ( numMembersEqual ? numMembers : - 1 ) , partitions , incompleteJoinNodes , incompleteStateTransferNodes ) ; }
public void test() { if ( r . connectError != null ) { log . trace ( "Connection error" , r . connectError ) ; } }
public void test() { if ( r . pollError != null ) { log . trace ( "Polling error" , r . pollError ) ; } }
@ Test public void findExperimentRunsNegativeTest ( ) { LOGGER . info ( "FindExperimentRuns Negative test start................................" ) ; FindExperimentRuns findExperimentRuns ; code_block = TryStatement ;  code_block = TryStatement ;  LOGGER . info ( "FindExperimentRuns Negative test stop................................" ) ; }
public void test() { try { List < String > experimentRunIds = new ArrayList < > ( ) ; experimentRunIds . add ( "abc" ) ; experimentRunIds . add ( "xyz" ) ; findExperimentRuns = FindExperimentRuns . newBuilder ( ) . addAllExperimentRunIds ( experimentRunIds ) . build ( ) ; experimentRunServiceStub . findExperimentRuns ( findExperimentRuns ) ; fail ( ) ; } catch ( StatusRuntimeException exc ) { Status status = Status . fromThrowable ( exc ) ; LOGGER . warn ( "Error Code : " + status . getCode ( ) + " Description : " + status . getDescription ( ) ) ; assertEquals ( Status . PERMISSION_DENIED . getCode ( ) , status . getCode ( ) ) ; } }
@ Test public void findExperimentRunsNegativeTest ( ) { LOGGER . info ( "FindExperimentRuns Negative test start................................" ) ; FindExperimentRuns findExperimentRuns ; code_block = TryStatement ;  code_block = TryStatement ;  LOGGER . info ( "FindExperimentRuns Negative test stop................................" ) ; }
public void test() { if ( ! appFolderPath . exists ( ) ) { log . info ( "Old apps folder does not exist, stopping discovery" ) ; return appMap ; } }
public void test() { if ( ! appFolderPath . isDirectory ( ) ) { log . error ( "Failed to discover installed apps: Path is not a directory '" + path + "'" ) ; } else { File [ ] listFiles = appFolderPath . listFiles ( ) ; code_block = IfStatement ; } }
public void test() { if ( listFiles == null ) { log . error ( "Failed to discover installed apps: Could not list contents of directory '" + path + "'" ) ; } else { code_block = ForStatement ; } }
public void test() { if ( ! folder . isDirectory ( ) ) { log . warn ( "Failed to discover app '" + folder . getName ( ) + "': Path is not a directory '" + folder . getPath ( ) + "'" ) ; } else { File appManifest = new File ( folder , "manifest.webapp" ) ; code_block = IfStatement ; } }
public void test() { if ( ! appManifest . exists ( ) ) { log . warn ( "Failed to discover app '" + folder . getName ( ) + "': Missing 'manifest.webapp' in app directory" ) ; } else { code_block = TryStatement ;  } }
public void test() { try { App app = mapper . readValue ( appManifest , App . class ) ; app . setFolderName ( folder . getName ( ) ) ; app . setAppStorageSource ( AppStorageSource . LOCAL ) ; appList . add ( app ) ; } catch ( IOException ex ) { log . error ( ex . getLocalizedMessage ( ) , ex ) ; } }
public void test() { if ( appList . isEmpty ( ) ) { log . info ( "No apps found during local discovery." ) ; } }
private OAuthUser getAuthParams ( AuthInfo authInfo , String code , OAuthServer server ) throws IOException , InterruptedException { String requestInfoUrl = prepareUrl ( server . getRequestInfoUrl ( ) , getParams ( server , code , authInfo ) ) ; HttpRequest . Builder builder = setNoCache ( HttpRequest . newBuilder ( ) . uri ( URI . create ( requestInfoUrl ) ) ) ; code_block = IfStatement ; String json = doRequest ( builder . build ( ) ) ; log . debug ( "User info={}" , json ) ; return new OAuthUser ( json , server ) ; }
public void test() { try { logger . debug ( "Begin write to HTML" ) ; resultWriter . write ( analysisResult , _configuration , writer ) ; logger . debug ( "End write to HTML" ) ; } finally { FileHelper . safeClose ( writer ) ; } }
public void test() { try { logger . debug ( "Begin write to HTML" ) ; resultWriter . write ( analysisResult , _configuration , writer ) ; logger . debug ( "End write to HTML" ) ; } finally { FileHelper . safeClose ( writer ) ; } }
public void test() { try { get ( ) ; } catch ( final ExecutionException e ) { logger . error ( "ExecutionException occurred while getting the result of the HTML rendering" , e ) ; final Throwable cause = e . getCause ( ) ; WidgetUtils . showErrorMessage ( "Error writing result to HTML page" , cause ) ; } catch ( final InterruptedException e ) { logger . warn ( "Unexpected interrupt in done() method!" ) ; } }
public void test() { try { get ( ) ; } catch ( final ExecutionException e ) { logger . error ( "ExecutionException occurred while getting the result of the HTML rendering" , e ) ; final Throwable cause = e . getCause ( ) ; WidgetUtils . showErrorMessage ( "Error writing result to HTML page" , cause ) ; } catch ( final InterruptedException e ) { logger . warn ( "Unexpected interrupt in done() method!" ) ; } }
public void test() { try { AuditLogEventRequest auditRequest = AuditEventMapper . fromHttpServletRequest ( request ) ; code_block = IfStatement ; code_block = IfStatement ; notificationList = notificationService . getNotificationList ( 0 , "" ) ; code_block = ForStatement ; map . addAttribute ( "notificationList" , notificationList ) ; auditLogEventHelper . logEvent ( APP_LEVEL_NOTIFICATION_LIST_VIEWED , auditRequest ) ; mav = new ModelAndView ( "notificationListPage" , map ) ; } catch ( Exception e ) { logger . error ( "NotificationController - viewNotificationList() - ERROR " , e ) ; } }
public void test() { if ( size > 0 ) { code_block = ForStatement ; } else { StaticLog . error ( exception , exception . getMessage ( ) ) ; } }
@ Override public InstancesInfo getApplicationInstances ( CloudApplication app ) { logger . debug ( Messages . GETTING_INSTANCES_OF_APPLICATION_0 , app . getName ( ) ) ; return delegate . getApplicationInstances ( app ) ; }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Registering module state with id [" + id + "]" ) ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( "Channel open: {}" , ctx . channel ( ) ) ; } }
public void test() { try { key . setPrivateKey ( privateKey , newCipherFactory ) ; } catch ( final GeneralSecurityException e ) { LOGGER . error ( "Could not encrypt private key: " + e , e ) ; iter . remove ( ) ; continue ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Could not decrypt private key: " + e , e ) ; } }
public void test() { if ( HttpStatus . getCode ( contentResponse . getStatus ( ) ) . isSuccess ( ) ) { String content = contentResponse . getContentAsString ( ) ; logger . debug ( "Online check completed with success: {} - status code: {}" , content , contentResponse . getStatus ( ) ) ; return true ; } else { logger . debug ( "Online check failed with status code: {}" , contentResponse . getStatus ( ) ) ; return false ; } }
public void test() { if ( HttpStatus . getCode ( contentResponse . getStatus ( ) ) . isSuccess ( ) ) { String content = contentResponse . getContentAsString ( ) ; logger . debug ( "Online check completed with success: {} - status code: {}" , content , contentResponse . getStatus ( ) ) ; return true ; } else { logger . debug ( "Online check failed with status code: {}" , contentResponse . getStatus ( ) ) ; return false ; } }
public void test() { try { String url = this . getPublicInformationUrl ( ) ; Request request = this . createRequest ( url , GET ) ; ContentResponse contentResponse = request . send ( ) ; code_block = IfStatement ; } catch ( TimeoutException | ExecutionException | NullPointerException e ) { logger . debug ( "Online check failed because of {}!" , e . getMessage ( ) ) ; return false ; } }
public void test() { if ( workspacesBackupLocks . putIfAbsent ( workspaceId , lock ) != null ) { String err = "Restore of workspace " + workspaceId + " failed. Another restore process of the same workspace is in progress" ; LOG . error ( err ) ; throw new ServerException ( err ) ; } }
public void test() { try { code_block = IfStatement ; Files . createDirectories ( Paths . get ( srcPath ) ) ; CommandLine commandLine = new CommandLine ( restoreScript , srcPath , destinationPath , destAddress , Integer . toString ( destPort ) , destUserId , destGroupId , destUserName ) ; executeCommand ( commandLine . asArray ( ) , restoreDuration , destAddress , workspaceId , RESTORE_SUCCESS_RETURN_CODES ) ; restored = true ; } catch ( TimeoutException e ) { throw new ServerException ( "Restoring of workspace " + workspaceId + " filesystem terminated due to timeout on " + destAddress + " node." ) ; } catch ( InterruptedException e ) { Thread . currentThread ( ) . interrupt ( ) ; throw new ServerException ( "Restoring of workspace " + workspaceId + " filesystem interrupted on " + destAddress + " node." ) ; } catch ( IOException e ) { String error = "Restoring of workspace " + workspaceId + " filesystem terminated on " + destAddress + " node. " + e . getLocalizedMessage ( ) ; LOG . error ( error , e ) ; throw new ServerException ( error ) ; } finally { lock . unlock ( ) ; code_block = IfStatement ; } }
public static String getTaskManagerShellCommand ( Configuration flinkConfig , ContaineredTaskManagerParameters tmParams , String configDirectory , String logDirectory , boolean hasLogback , boolean hasLog4j , boolean hasKrb5 , Class < ? > mainClass , String mainArgs ) { final Map < String , String > startCommandValues = new HashMap < > ( ) ; startCommandValues . put ( "java" , "$JAVA_HOME/bin/java" ) ; final TaskExecutorProcessSpec taskExecutorProcessSpec = tmParams . getTaskExecutorProcessSpec ( ) ; startCommandValues . put ( "jvmmem" , ProcessMemoryUtils . generateJvmParametersStr ( taskExecutorProcessSpec ) ) ; String javaOpts = flinkConfig . getString ( CoreOptions . FLINK_JVM_OPTIONS ) ; code_block = IfStatement ; code_block = IfStatement ; startCommandValues . put ( "jvmopts" , javaOpts ) ; String logging = "" ; code_block = IfStatement ; startCommandValues . put ( "logging" , logging ) ; startCommandValues . put ( "class" , mainClass . getName ( ) ) ; startCommandValues . put ( "redirects" , "1> " + logDirectory + "/taskmanager.out " + "2> " + logDirectory + "/taskmanager.err" ) ; String argsStr = TaskExecutorProcessUtils . generateDynamicConfigsStr ( taskExecutorProcessSpec ) + " --configDir " + configDirectory ; code_block = IfStatement ; startCommandValues . put ( "args" , argsStr ) ; final String commandTemplate = flinkConfig . getString ( ConfigConstants . YARN_CONTAINER_START_COMMAND_TEMPLATE , ConfigConstants . DEFAULT_YARN_CONTAINER_START_COMMAND_TEMPLATE ) ; String startCommand = getStartCommand ( commandTemplate , startCommandValues ) ; LOG . debug ( "TaskManager start command: " + startCommand ) ; return startCommand ; }
@ Test public void test_12_BRCA_Splice_15_Hgvs ( ) { Log . debug ( "Test" ) ; int spliceSize = 15 ; String genome = "test_BRCA" ; String vcf = path ( "test_BRCA_splice_15.vcf" ) ; String args [ ] = code_block = "" ; ; SnpEffCmdEff snpeff = new SnpEffCmdEff ( ) ; snpeff . parseArgs ( args ) ; snpeff . setDebug ( debug ) ; snpeff . setVerbose ( verbose ) ; snpeff . setSupressOutput ( ! verbose ) ; snpeff . setFormatVersion ( EffFormatVersion . FORMAT_EFF_4 ) ; snpeff . setSpliceSiteSize ( spliceSize ) ; snpeff . setUpDownStreamLength ( 0 ) ; snpeff . setShiftHgvs ( false ) ; List < VcfEntry > results = snpeff . run ( true ) ; VcfEntry ve = results . get ( 0 ) ; boolean ok = false ; code_block = ForStatement ; Assert . assertTrue ( ok ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . info ( format ( "Job %s is already running or it has been stop" , job . getName ( ) ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( format ( "Starting execution of job %s" , job . getName ( ) ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( format ( "Job %s execution failed" , job . getName ( ) ) , failure ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( format ( "Job close %s failure" , job . getName ( ) ) , exception ) ; } }
public void persist ( StgMsCmState transientInstance ) { log . debug ( "persisting StgMsCmState instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . persist ( transientInstance ) ; log . debug ( "persist successful" ) ; } catch ( RuntimeException re ) { log . error ( "persist failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . persist ( transientInstance ) ; log . debug ( "persist successful" ) ; } catch ( RuntimeException re ) { log . error ( "persist failed" , re ) ; throw re ; } }
public void addLogln ( String message ) { logger . debug ( message ) ; progressStatus . append ( message ) ; progressStatus . append ( NEWLINE ) ; }
public void test() { try { sourceType = PTNetworkSourceTypeEnum . valueOf ( firstLetterUpcase ( netexSourceType ) ) ; } catch ( Exception e ) { logger . error ( "unable to translate " + netexSourceType + " as PTNetworkSourceType" ) ; } }
public void test() { if ( Log . isDebugEnabled ( ) ) { Log . debug ( "Adds a monitored point '" + point . getLabel ( ) + "' to container with id " + containerId + "." ) ; } }
public void test() { try { responseJson = PacHttpUtils . doHttpPost ( urlToQuery . toString ( ) , requestBody . toString ( ) ) ; } catch ( Exception e ) { LOGGER . error ( "Error in getAppsBySeverity from ES" , e ) ; throw e ; } }
private ModulesConfigurationProperties getModulesConfigurationPropertiesFallback ( String serviceName , Throwable e ) { log . error ( "getModulesConfigurationPropertiesFallback: " + serviceName ) ; e . printStackTrace ( ) ; return null ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Setting autocommit to " + desiredAutoCommit + " on JDBC Connection [" + connection + "]" ) ; } }
public void test() { if ( ( ! closing ) && ( ! closed ) ) { LOG . error ( "create connection Thread Interrupted, url: " + jdbcUrl , e ) ; } }
public void test() { try { connection = createPhysicalConnection ( ) ; } catch ( SQLException e ) { LOG . error ( "create connection SQLException, url: " + jdbcUrl + ", errorCode " + e . getErrorCode ( ) + ", state " + e . getSQLState ( ) , e ) ; errorCount ++ ; code_block = IfStatement ; } catch ( RuntimeException e ) { LOG . error ( "create connection RuntimeException" , e ) ; setFailContinuous ( true ) ; continue ; } catch ( Error e ) { LOG . error ( "create connection Error" , e ) ; setFailContinuous ( true ) ; break ; } }
public void test() { try { connection = createPhysicalConnection ( ) ; } catch ( SQLException e ) { LOG . error ( "create connection SQLException, url: " + jdbcUrl + ", errorCode " + e . getErrorCode ( ) + ", state " + e . getSQLState ( ) , e ) ; errorCount ++ ; code_block = IfStatement ; } catch ( RuntimeException e ) { LOG . error ( "create connection RuntimeException" , e ) ; setFailContinuous ( true ) ; continue ; } catch ( Error e ) { LOG . error ( "create connection Error" , e ) ; setFailContinuous ( true ) ; break ; } }
public void test() { try { connection = createPhysicalConnection ( ) ; } catch ( SQLException e ) { LOG . error ( "create connection SQLException, url: " + jdbcUrl + ", errorCode " + e . getErrorCode ( ) + ", state " + e . getSQLState ( ) , e ) ; errorCount ++ ; code_block = IfStatement ; } catch ( RuntimeException e ) { LOG . error ( "create connection RuntimeException" , e ) ; setFailContinuous ( true ) ; continue ; } catch ( Error e ) { LOG . error ( "create connection Error" , e ) ; setFailContinuous ( true ) ; break ; } }
public void test() { try { IEntityManager entityManager = this . getEntityManager ( ) ; Map < String , AttributeInterface > attributeTypes = entityManager . getEntityAttributePrototypes ( ) ; Iterator < AttributeInterface > attributeIter = attributeTypes . values ( ) . iterator ( ) ; code_block = WhileStatement ; Collections . sort ( attributes , new BeanComparator ( "type" ) ) ; } catch ( Throwable t ) { _logger . error ( "Error while extracting Allowed Nested Types" , t ) ; throw new RuntimeException ( "Error while extracting Allowed Nested Types" , t ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( has ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "\n" + sb . toString ( ) ) ; } }
public void test() { if ( SortOrder . ASCENDING . equals ( sortOrder ) ) { sortPropertyType . setSortOrder ( SortOrderType . ASC ) ; } else-if ( SortOrder . DESCENDING . equals ( sortOrder ) ) { sortPropertyType . setSortOrder ( SortOrderType . DESC ) ; } else { LOGGER . debug ( "Unable to build query. Unknown sort order of [{}]." , sortOrder == null ? null : sortOrder . identifier ( ) ) ; return null ; } }
public void test() { try ( CancellableInputStream cancellableStream = new CancellableInputStream ( inStream , emptyHandler ) ) { Iterator < VideoDecoder > decodersIterator = ServiceLoader . load ( VideoDecoder . class ) . iterator ( ) ; code_block = IfStatement ; VideoDecoder videoDecoder = decodersIterator . next ( ) ; videoDecoder . setInputStream ( cancellableStream , TimeInstant . get ( startTime ) ) ; videoDecoder . registerVideoContentHandler ( new OSHVideoContentHandler ( dataType , tracker ) , null ) ; videoDecoder . decode ( ) ; } catch ( IOException | VideoDecoderException e ) { LOGGER . error ( e , e ) ; } }
public void test() { if ( signInDetails != null ) { UserDetails user = userDetailsService . loadUserByUsername ( signInDetails . getConnectionData ( ) . getProviderId ( ) + Constants . USER_NAME_SPLITTER + signInDetails . getUserId ( ) ) ; code_block = IfStatement ; updateUserKeys ( signInDetails . getConnectionData ( ) , signInDetails . getUserId ( ) ) ; return authenticationFactory . createAuthenticationFromUserDetails ( user ) ; } else-if ( allowRepeatedAuthenticationAttempts && alreadyAuthenticatedUserId != null ) { return SecurityContextHolder . getContext ( ) . getAuthentication ( ) ; } else { logger . info ( "SpringSocialSecurity sign in details not found in session" ) ; throw new InsufficientAuthenticationException ( "SpringSocialSecurity sign in details not found in session" ) ; } }
@ Override protected void before ( ) throws Throwable { log . debug ( "Starting up test server" ) ; server = H2ServerBootstrap . bootstrap ( ) . setLookupRegistry ( new UriPatternMatcher < > ( ) ) . setVersionPolicy ( HttpVersionPolicy . NEGOTIATE ) . setIOReactorConfig ( IOReactorConfig . custom ( ) . setSoTimeout ( TIMEOUT ) . build ( ) ) . setTlsStrategy ( scheme == URIScheme . HTTPS ? new H2ServerTlsStrategy ( SSLTestContexts . createServerSSLContext ( ) ) : null ) . setStreamListener ( LoggingHttp1StreamListener . INSTANCE_SERVER ) . setStreamListener ( LoggingH2StreamListener . INSTANCE ) . setIOSessionDecorator ( LoggingIOSessionDecorator . INSTANCE ) . setExceptionCallback ( LoggingExceptionCallback . INSTANCE ) . setIOSessionListener ( LoggingIOSessionListener . INSTANCE ) . register ( "*" , ( ) -> new EchoHandler ( 2048 ) ) . create ( ) ; }
public void test() { else-if ( candidates . size ( ) > 1 ) { StringBuffer buf = new StringBuffer ( ) ; code_block = ForStatement ; logger . debug ( "More than one element builder plugin claims responsibilty to " + flavor + ": " + buf . toString ( ) ) ; } }
@ Test ( description = "Deploy autoscaling policy" , timeOut = APPLICATION_TEST_TIMEOUT ) public void testAutoscalingPolicy ( ) throws Exception { log . info ( "-------------------------Started autoscaling policy test case-------------------------" ) ; String policyId = "autoscaling-policy-autoscaling-policy-test" ; boolean added = restClient . addEntity ( RESOURCES_PATH + RestConstants . AUTOSCALING_POLICIES_PATH + "/" + policyId + ".json" , RestConstants . AUTOSCALING_POLICIES , RestConstants . AUTOSCALING_POLICIES_NAME ) ; assertTrue ( String . format ( "Autoscaling policy did not added: [autoscaling-policy-id] %s" , policyId ) , added ) ; AutoscalePolicyBean bean = ( AutoscalePolicyBean ) restClient . getEntity ( RestConstants . AUTOSCALING_POLICIES , policyId , AutoscalePolicyBean . class , RestConstants . AUTOSCALING_POLICIES_NAME ) ; assertEquals ( String . format ( "[autoscaling-policy-id] %s is not correct" , bean . getId ( ) ) , bean . getId ( ) , policyId ) ; assertEquals ( String . format ( "[autoscaling-policy-id] %s RIF is not correct" , policyId ) , bean . getLoadThresholds ( ) . getRequestsInFlight ( ) . getThreshold ( ) , 35.0 , 0.0 ) ; assertEquals ( String . format ( "[autoscaling-policy-id] %s Memory is not correct" , policyId ) , bean . getLoadThresholds ( ) . getMemoryConsumption ( ) . getThreshold ( ) , 45.0 , 0.0 ) ; assertEquals ( String . format ( "[autoscaling-policy-id] %s Load is not correct" , policyId ) , bean . getLoadThresholds ( ) . getLoadAverage ( ) . getThreshold ( ) , 25.0 , 0.0 ) ; boolean updated = restClient . updateEntity ( RESOURCES_PATH + RestConstants . AUTOSCALING_POLICIES_PATH + "/" + policyId + "-v1.json" , RestConstants . AUTOSCALING_POLICIES , RestConstants . AUTOSCALING_POLICIES_NAME ) ; assertTrue ( String . format ( "[autoscaling-policy-id] %s update failed" , policyId ) , updated ) ; AutoscalePolicyBean updatedBean = ( AutoscalePolicyBean ) restClient . getEntity ( RestConstants . AUTOSCALING_POLICIES , policyId , AutoscalePolicyBean . class , RestConstants . AUTOSCALING_POLICIES_NAME ) ; assertEquals ( String . format ( "[autoscaling-policy-id] %s RIF is not correct" , policyId ) , updatedBean . getLoadThresholds ( ) . getRequestsInFlight ( ) . getThreshold ( ) , 30.0 , 0.0 ) ; assertEquals ( String . format ( "[autoscaling-policy-id] %s Load is not correct" , policyId ) , updatedBean . getLoadThresholds ( ) . getMemoryConsumption ( ) . getThreshold ( ) , 40.0 , 0.0 ) ; assertEquals ( String . format ( "[autoscaling-policy-id] %s Memory is not correct" , policyId ) , updatedBean . getLoadThresholds ( ) . getLoadAverage ( ) . getThreshold ( ) , 20.0 , 0.0 ) ; boolean removed = restClient . removeEntity ( RestConstants . AUTOSCALING_POLICIES , policyId , RestConstants . AUTOSCALING_POLICIES_NAME ) ; assertTrue ( String . format ( "[autoscaling-policy-id] %s couldn't be removed" , policyId ) , removed ) ; AutoscalePolicyBean beanRemoved = ( AutoscalePolicyBean ) restClient . getEntity ( RestConstants . AUTOSCALING_POLICIES , policyId , AutoscalePolicyBean . class , RestConstants . AUTOSCALING_POLICIES_NAME ) ; assertNull ( String . format ( "[autoscaling-policy-id] %s didn't get removed successfully" , policyId ) , beanRemoved ) ; log . info ( "-------------------------Ended autoscaling policy test case---------------------------" ) ; }
@ Test ( description = "Deploy autoscaling policy" , timeOut = APPLICATION_TEST_TIMEOUT ) public void testAutoscalingPolicy ( ) throws Exception { log . info ( "-------------------------Started autoscaling policy test case-------------------------" ) ; String policyId = "autoscaling-policy-autoscaling-policy-test" ; boolean added = restClient . addEntity ( RESOURCES_PATH + RestConstants . AUTOSCALING_POLICIES_PATH + "/" + policyId + ".json" , RestConstants . AUTOSCALING_POLICIES , RestConstants . AUTOSCALING_POLICIES_NAME ) ; assertTrue ( String . format ( "Autoscaling policy did not added: [autoscaling-policy-id] %s" , policyId ) , added ) ; AutoscalePolicyBean bean = ( AutoscalePolicyBean ) restClient . getEntity ( RestConstants . AUTOSCALING_POLICIES , policyId , AutoscalePolicyBean . class , RestConstants . AUTOSCALING_POLICIES_NAME ) ; assertEquals ( String . format ( "[autoscaling-policy-id] %s is not correct" , bean . getId ( ) ) , bean . getId ( ) , policyId ) ; assertEquals ( String . format ( "[autoscaling-policy-id] %s RIF is not correct" , policyId ) , bean . getLoadThresholds ( ) . getRequestsInFlight ( ) . getThreshold ( ) , 35.0 , 0.0 ) ; assertEquals ( String . format ( "[autoscaling-policy-id] %s Memory is not correct" , policyId ) , bean . getLoadThresholds ( ) . getMemoryConsumption ( ) . getThreshold ( ) , 45.0 , 0.0 ) ; assertEquals ( String . format ( "[autoscaling-policy-id] %s Load is not correct" , policyId ) , bean . getLoadThresholds ( ) . getLoadAverage ( ) . getThreshold ( ) , 25.0 , 0.0 ) ; boolean updated = restClient . updateEntity ( RESOURCES_PATH + RestConstants . AUTOSCALING_POLICIES_PATH + "/" + policyId + "-v1.json" , RestConstants . AUTOSCALING_POLICIES , RestConstants . AUTOSCALING_POLICIES_NAME ) ; assertTrue ( String . format ( "[autoscaling-policy-id] %s update failed" , policyId ) , updated ) ; AutoscalePolicyBean updatedBean = ( AutoscalePolicyBean ) restClient . getEntity ( RestConstants . AUTOSCALING_POLICIES , policyId , AutoscalePolicyBean . class , RestConstants . AUTOSCALING_POLICIES_NAME ) ; assertEquals ( String . format ( "[autoscaling-policy-id] %s RIF is not correct" , policyId ) , updatedBean . getLoadThresholds ( ) . getRequestsInFlight ( ) . getThreshold ( ) , 30.0 , 0.0 ) ; assertEquals ( String . format ( "[autoscaling-policy-id] %s Load is not correct" , policyId ) , updatedBean . getLoadThresholds ( ) . getMemoryConsumption ( ) . getThreshold ( ) , 40.0 , 0.0 ) ; assertEquals ( String . format ( "[autoscaling-policy-id] %s Memory is not correct" , policyId ) , updatedBean . getLoadThresholds ( ) . getLoadAverage ( ) . getThreshold ( ) , 20.0 , 0.0 ) ; boolean removed = restClient . removeEntity ( RestConstants . AUTOSCALING_POLICIES , policyId , RestConstants . AUTOSCALING_POLICIES_NAME ) ; assertTrue ( String . format ( "[autoscaling-policy-id] %s couldn't be removed" , policyId ) , removed ) ; AutoscalePolicyBean beanRemoved = ( AutoscalePolicyBean ) restClient . getEntity ( RestConstants . AUTOSCALING_POLICIES , policyId , AutoscalePolicyBean . class , RestConstants . AUTOSCALING_POLICIES_NAME ) ; assertNull ( String . format ( "[autoscaling-policy-id] %s didn't get removed successfully" , policyId ) , beanRemoved ) ; log . info ( "-------------------------Ended autoscaling policy test case---------------------------" ) ; }
public void test() { if ( state == PlayerState . PREPARED || state == PlayerState . PAUSED || state == PlayerState . STOPPED || state == PlayerState . LAGGING || state == PlayerState . PLAYBACK_COMPLETE ) { code_block = IfStatement ; exoPlayer . setPlayWhenReady ( true ) ; code_block = IfStatement ; state = PlayerState . PLAYING ; logger . debug ( "Playback started" ) ; showController ( ) ; requestAccessibilityFocusPausePlay ( ) ; } else { logger . warn ( "Cannot start" ) ; } }
synchronized void start ( ) { if ( ! isEnabled ( ) || started ) return ; pageSize = ctx . igniteConfiguration ( ) . getDataStorageConfiguration ( ) . getPageSize ( ) ; pageMemoryMock = mockPageMemory ( ) ; GridCacheSharedContext sharedCtx = gridCtx . cache ( ) . context ( ) ; long maxMemorySize = 0 ; code_block = ForStatement ; long [ ] chunks = new long [ ] code_block = "" ; ; memoryProvider = new UnsafeMemoryProvider ( log ) ; memoryProvider . initialize ( chunks ) ; memoryRegion = memoryProvider . nextRegion ( ) ; GridUnsafe . setMemory ( memoryRegion . address ( ) , memoryRegion . size ( ) , ( byte ) 0 ) ; maxPages = ( int ) ( maxMemorySize / pageSize ) ; pageSlots = new DirectMemoryPageSlot [ maxPages ] ; freeSlotsCnt = maxPages ; tmpBuf1 = ByteBuffer . allocateDirect ( pageSize ) ; tmpBuf2 = ByteBuffer . allocateDirect ( pageSize ) ; code_block = IfStatement ; lastPageIdx = 0 ; started = true ; log . info ( "PageMemory tracker started, " + U . readableSize ( maxMemorySize , false ) + " offheap memory allocated." ) ; }
@ Incoming ( "my-server" ) public CompletionStage < Void > source ( MqttMessage message ) { LOGGER . info ( "MQTT Message received {}" , new String ( message . getPayload ( ) ) ) ; return message . ack ( ) ; }
public void test() { if ( ! emitted . contains ( msgId ) ) { LOG . debug ( "Received ack for message [{}], associated with tuple emitted for a ConsumerRecord that " + "came from a topic-partition that this consumer group instance is no longer tracking " + "due to rebalance/partition reassignment. No action taken." , msgId ) ; } else { Validate . isTrue ( ! retryService . isScheduled ( msgId ) , "The message id " + msgId + " is queued for retry while being acked." + " This should never occur barring errors in the RetryService implementation or the spout code." ) ; offsetManagers . get ( msgId . getTopicPartition ( ) ) . addToAckMsgs ( msgId ) ; emitted . remove ( msgId ) ; } }
public void test() { try ( FileOutputStream output = new FileOutputStream ( pipe , true ) ; ) { output . write ( result . getBytes ( ) ) ; output . flush ( ) ; output . close ( ) ; } catch ( Exception e ) { RUNTIME_LOGGER . debug ( LOG_PREFIX + "Error on adaptation result pipe write. " , e ) ; } }
public void test() { try { return new URI ( string ) ; } catch ( final URISyntaxException e ) { LOG . error ( "Failed to transform String to URI: {} " , string ) ; } }
protected void onLinkAddedPost ( final String networkId , final Link link , final HashMap < String , Response > respList ) { log . debug ( "" ) ; }
public void test() { try { FindHuiUrls command = new FindHuiUrls ( allIDs ) ; command = ServiceFactory . lookupCommandService ( ) . executeCommand ( command ) ; result = command . getList ( ) ; } catch ( Exception e ) { LOG . error ( STD_ERR_MSG , e ) ; } }
public CancellableCompletableFuture < Void > monitor ( Supplier < Boolean > condition , int checkInterval , int timeout , TimeUnit timeUnit ) { log . debug ( "Monitoring condition with specified checkInterval of {}, timeout of {}, timeUnit {}" , checkInterval , timeout , timeUnit ) ; return scheduledExecutor . scheduleWithFixedDelayAndTimeout ( condition , 0L , checkInterval , timeout , timeUnit ) ; }
public void test() { try { Utils . moveToDirectory ( destinationDir , file ) ; } catch ( Exception ex ) { logger . error ( "Unable to move file '" + file . getName ( ) + "' to directory: " + destinationDir , ex ) ; } }
private void parseHeartbeatMode ( HeartbeatExtensionMessage msg ) { msg . setHeartbeatMode ( parseByteArrayField ( ExtensionByteLength . HEARTBEAT_MODE ) ) ; LOGGER . debug ( "HeartbeatMode: " + ArrayConverter . bytesToHexString ( msg . getHeartbeatMode ( ) . getValue ( ) ) ) ; }
@ GetMapping ( CommonConstants . PATH_LEVELS ) public List < String > getLevels ( final Optional < String > criteria ) { LOGGER . debug ( "Get levels with criteria={}" , criteria ) ; RestUtils . checkCriteria ( criteria ) ; return internalProfileService . getLevels ( criteria ) ; }
public void test() { try { RoleRestClient . setAnyLayout ( wrapper . getKey ( ) , wrapper . getContent ( ) ) ; SyncopeConsoleSession . get ( ) . success ( getString ( Constants . OPERATION_SUCCEEDED ) ) ; modal . show ( false ) ; modal . close ( target ) ; } catch ( Exception e ) { LOG . error ( "While updating console layout for role {}" , wrapper . getKey ( ) , e ) ; SyncopeConsoleSession . get ( ) . onException ( e ) ; } }
public void test() { try { fileDescriptor = irodsFileSystemAO . createFile ( getAbsolutePath ( ) , DataObjInp . OpenFlags . READ_WRITE , 0600 ) ; log . debug ( "file descriptor from new file create: {}" , fileDescriptor ) ; } catch ( JargonFileOrCollAlreadyExistsException e ) { return false ; } catch ( ResourceHierarchyException rhe ) { return false ; } catch ( JargonException e ) { String msg = "JargonException caught and rethrown as IOException:" + e . getMessage ( ) ; log . error ( msg , e ) ; throw new IOException ( e ) ; } }
public void test() { try { fileDescriptor = irodsFileSystemAO . createFile ( getAbsolutePath ( ) , DataObjInp . OpenFlags . READ_WRITE , 0600 ) ; log . debug ( "file descriptor from new file create: {}" , fileDescriptor ) ; } catch ( JargonFileOrCollAlreadyExistsException e ) { return false ; } catch ( ResourceHierarchyException rhe ) { return false ; } catch ( JargonException e ) { String msg = "JargonException caught and rethrown as IOException:" + e . getMessage ( ) ; log . error ( msg , e ) ; throw new IOException ( e ) ; } }
public void test() { try { TimeSpan span = TimeSpan . fromISO8601String ( time . getCurrent ( ) ) ; resetPlanIfNecessary ( span ) ; myTimeManager . setPrimaryActiveTimeSpan ( span ) ; } catch ( ParseException e ) { LOGGER . error ( "Failed to parse state interval: " + e , e ) ; } }
public void test() { if ( ! actionReturnValue . getSucceeded ( ) ) { log . error ( "Error cloning Managed block disk '{}': {}" , managedBlockDisk . getDiskAlias ( ) ) ; getReturnValue ( ) . setFault ( actionReturnValue . getFault ( ) ) ; return ; } }
public void test() { try { List < WikiObjectComponentBuilder > componentBuilders = this . contextComponent . getInstanceList ( WikiObjectComponentBuilder . class ) ; code_block = ForStatement ; } catch ( ComponentLookupException e ) { logger . warn ( "Unable to collect a list of wiki objects components: %s" , e ) ; } }
public void test() { try { return mapper . readValue ( jsonstr , type ) ; } catch ( IOException e ) { logger . error ( "fail to unMarshal json {}" , e . getMessage ( ) ) ; } }
public void test() { -> { LOGGER . info ( "Removing Cruise Control to the classic Kafka." ) ; kafka . getSpec ( ) . setCruiseControl ( null ) ; } }
@ ParallelNamespaceTest void testDeployAndUnDeployCruiseControl ( ExtensionContext extensionContext ) throws IOException { final String namespaceName = extensionContext . getStore ( ExtensionContext . Namespace . GLOBAL ) . get ( Constants . NAMESPACE_KEY ) . toString ( ) ; final String clusterName = mapWithClusterNames . get ( extensionContext . getDisplayName ( ) ) ; resourceManager . createResource ( extensionContext , KafkaTemplates . kafkaWithCruiseControl ( clusterName , 3 , 3 ) . build ( ) ) ; Map < String , String > kafkaPods = StatefulSetUtils . ssSnapshot ( namespaceName , KafkaResources . kafkaStatefulSetName ( clusterName ) ) ; KafkaResource . replaceKafkaResourceInSpecificNamespace ( clusterName , kafka code_block = LoopStatement ; , namespaceName ) ; StatefulSetUtils . waitTillSsHasRolled ( namespaceName , kafkaStatefulSetName ( clusterName ) , 3 , kafkaPods ) ; LOGGER . info ( "Verifying that in {} is not present in the Kafka cluster" , Constants . CRUISE_CONTROL_NAME ) ; assertThat ( KafkaResource . kafkaClient ( ) . inNamespace ( namespaceName ) . withName ( clusterName ) . get ( ) . getSpec ( ) . getCruiseControl ( ) , nullValue ( ) ) ; LOGGER . info ( "Verifying that {} pod is not present" , clusterName + "-cruise-control-" ) ; PodUtils . waitUntilPodStabilityReplicasCount ( namespaceName , clusterName + "-cruise-control-" , 0 ) ; LOGGER . info ( "Verifying that in Kafka config map there is no configuration to cruise control metric reporter" ) ; assertThrows ( WaitException . class , ( ) -> CruiseControlUtils . verifyCruiseControlMetricReporterConfigurationInKafkaConfigMapIsPresent ( CruiseControlUtils . getKafkaCruiseControlMetricsReporterConfiguration ( namespaceName , clusterName ) ) ) ; LOGGER . info ( "Cruise Control topics will not be deleted and will stay in the Kafka cluster" ) ; CruiseControlUtils . verifyThatCruiseControlTopicsArePresent ( namespaceName ) ; kafkaPods = StatefulSetUtils . ssSnapshot ( namespaceName , KafkaResources . kafkaStatefulSetName ( clusterName ) ) ; KafkaResource . replaceKafkaResourceInSpecificNamespace ( clusterName , kafka code_block = LoopStatement ; , namespaceName ) ; StatefulSetUtils . waitTillSsHasRolled ( namespaceName , kafkaStatefulSetName ( clusterName ) , 3 , kafkaPods ) ; LOGGER . info ( "Verifying that in Kafka config map there is configuration to cruise control metric reporter" ) ; CruiseControlUtils . verifyCruiseControlMetricReporterConfigurationInKafkaConfigMapIsPresent ( CruiseControlUtils . getKafkaCruiseControlMetricsReporterConfiguration ( namespaceName , clusterName ) ) ; LOGGER . info ( "Verifying that {} topics are created after CC is instantiated." , Constants . CRUISE_CONTROL_NAME ) ; CruiseControlUtils . verifyThatCruiseControlTopicsArePresent ( namespaceName ) ; }
@ ParallelNamespaceTest void testDeployAndUnDeployCruiseControl ( ExtensionContext extensionContext ) throws IOException { final String namespaceName = extensionContext . getStore ( ExtensionContext . Namespace . GLOBAL ) . get ( Constants . NAMESPACE_KEY ) . toString ( ) ; final String clusterName = mapWithClusterNames . get ( extensionContext . getDisplayName ( ) ) ; resourceManager . createResource ( extensionContext , KafkaTemplates . kafkaWithCruiseControl ( clusterName , 3 , 3 ) . build ( ) ) ; Map < String , String > kafkaPods = StatefulSetUtils . ssSnapshot ( namespaceName , KafkaResources . kafkaStatefulSetName ( clusterName ) ) ; KafkaResource . replaceKafkaResourceInSpecificNamespace ( clusterName , kafka code_block = LoopStatement ; , namespaceName ) ; StatefulSetUtils . waitTillSsHasRolled ( namespaceName , kafkaStatefulSetName ( clusterName ) , 3 , kafkaPods ) ; LOGGER . info ( "Verifying that in {} is not present in the Kafka cluster" , Constants . CRUISE_CONTROL_NAME ) ; assertThat ( KafkaResource . kafkaClient ( ) . inNamespace ( namespaceName ) . withName ( clusterName ) . get ( ) . getSpec ( ) . getCruiseControl ( ) , nullValue ( ) ) ; LOGGER . info ( "Verifying that {} pod is not present" , clusterName + "-cruise-control-" ) ; PodUtils . waitUntilPodStabilityReplicasCount ( namespaceName , clusterName + "-cruise-control-" , 0 ) ; LOGGER . info ( "Verifying that in Kafka config map there is no configuration to cruise control metric reporter" ) ; assertThrows ( WaitException . class , ( ) -> CruiseControlUtils . verifyCruiseControlMetricReporterConfigurationInKafkaConfigMapIsPresent ( CruiseControlUtils . getKafkaCruiseControlMetricsReporterConfiguration ( namespaceName , clusterName ) ) ) ; LOGGER . info ( "Cruise Control topics will not be deleted and will stay in the Kafka cluster" ) ; CruiseControlUtils . verifyThatCruiseControlTopicsArePresent ( namespaceName ) ; kafkaPods = StatefulSetUtils . ssSnapshot ( namespaceName , KafkaResources . kafkaStatefulSetName ( clusterName ) ) ; KafkaResource . replaceKafkaResourceInSpecificNamespace ( clusterName , kafka code_block = LoopStatement ; , namespaceName ) ; StatefulSetUtils . waitTillSsHasRolled ( namespaceName , kafkaStatefulSetName ( clusterName ) , 3 , kafkaPods ) ; LOGGER . info ( "Verifying that in Kafka config map there is configuration to cruise control metric reporter" ) ; CruiseControlUtils . verifyCruiseControlMetricReporterConfigurationInKafkaConfigMapIsPresent ( CruiseControlUtils . getKafkaCruiseControlMetricsReporterConfiguration ( namespaceName , clusterName ) ) ; LOGGER . info ( "Verifying that {} topics are created after CC is instantiated." , Constants . CRUISE_CONTROL_NAME ) ; CruiseControlUtils . verifyThatCruiseControlTopicsArePresent ( namespaceName ) ; }
@ ParallelNamespaceTest void testDeployAndUnDeployCruiseControl ( ExtensionContext extensionContext ) throws IOException { final String namespaceName = extensionContext . getStore ( ExtensionContext . Namespace . GLOBAL ) . get ( Constants . NAMESPACE_KEY ) . toString ( ) ; final String clusterName = mapWithClusterNames . get ( extensionContext . getDisplayName ( ) ) ; resourceManager . createResource ( extensionContext , KafkaTemplates . kafkaWithCruiseControl ( clusterName , 3 , 3 ) . build ( ) ) ; Map < String , String > kafkaPods = StatefulSetUtils . ssSnapshot ( namespaceName , KafkaResources . kafkaStatefulSetName ( clusterName ) ) ; KafkaResource . replaceKafkaResourceInSpecificNamespace ( clusterName , kafka code_block = LoopStatement ; , namespaceName ) ; StatefulSetUtils . waitTillSsHasRolled ( namespaceName , kafkaStatefulSetName ( clusterName ) , 3 , kafkaPods ) ; LOGGER . info ( "Verifying that in {} is not present in the Kafka cluster" , Constants . CRUISE_CONTROL_NAME ) ; assertThat ( KafkaResource . kafkaClient ( ) . inNamespace ( namespaceName ) . withName ( clusterName ) . get ( ) . getSpec ( ) . getCruiseControl ( ) , nullValue ( ) ) ; LOGGER . info ( "Verifying that {} pod is not present" , clusterName + "-cruise-control-" ) ; PodUtils . waitUntilPodStabilityReplicasCount ( namespaceName , clusterName + "-cruise-control-" , 0 ) ; LOGGER . info ( "Verifying that in Kafka config map there is no configuration to cruise control metric reporter" ) ; assertThrows ( WaitException . class , ( ) -> CruiseControlUtils . verifyCruiseControlMetricReporterConfigurationInKafkaConfigMapIsPresent ( CruiseControlUtils . getKafkaCruiseControlMetricsReporterConfiguration ( namespaceName , clusterName ) ) ) ; LOGGER . info ( "Cruise Control topics will not be deleted and will stay in the Kafka cluster" ) ; CruiseControlUtils . verifyThatCruiseControlTopicsArePresent ( namespaceName ) ; kafkaPods = StatefulSetUtils . ssSnapshot ( namespaceName , KafkaResources . kafkaStatefulSetName ( clusterName ) ) ; KafkaResource . replaceKafkaResourceInSpecificNamespace ( clusterName , kafka code_block = LoopStatement ; , namespaceName ) ; StatefulSetUtils . waitTillSsHasRolled ( namespaceName , kafkaStatefulSetName ( clusterName ) , 3 , kafkaPods ) ; LOGGER . info ( "Verifying that in Kafka config map there is configuration to cruise control metric reporter" ) ; CruiseControlUtils . verifyCruiseControlMetricReporterConfigurationInKafkaConfigMapIsPresent ( CruiseControlUtils . getKafkaCruiseControlMetricsReporterConfiguration ( namespaceName , clusterName ) ) ; LOGGER . info ( "Verifying that {} topics are created after CC is instantiated." , Constants . CRUISE_CONTROL_NAME ) ; CruiseControlUtils . verifyThatCruiseControlTopicsArePresent ( namespaceName ) ; }
@ ParallelNamespaceTest void testDeployAndUnDeployCruiseControl ( ExtensionContext extensionContext ) throws IOException { final String namespaceName = extensionContext . getStore ( ExtensionContext . Namespace . GLOBAL ) . get ( Constants . NAMESPACE_KEY ) . toString ( ) ; final String clusterName = mapWithClusterNames . get ( extensionContext . getDisplayName ( ) ) ; resourceManager . createResource ( extensionContext , KafkaTemplates . kafkaWithCruiseControl ( clusterName , 3 , 3 ) . build ( ) ) ; Map < String , String > kafkaPods = StatefulSetUtils . ssSnapshot ( namespaceName , KafkaResources . kafkaStatefulSetName ( clusterName ) ) ; KafkaResource . replaceKafkaResourceInSpecificNamespace ( clusterName , kafka code_block = LoopStatement ; , namespaceName ) ; StatefulSetUtils . waitTillSsHasRolled ( namespaceName , kafkaStatefulSetName ( clusterName ) , 3 , kafkaPods ) ; LOGGER . info ( "Verifying that in {} is not present in the Kafka cluster" , Constants . CRUISE_CONTROL_NAME ) ; assertThat ( KafkaResource . kafkaClient ( ) . inNamespace ( namespaceName ) . withName ( clusterName ) . get ( ) . getSpec ( ) . getCruiseControl ( ) , nullValue ( ) ) ; LOGGER . info ( "Verifying that {} pod is not present" , clusterName + "-cruise-control-" ) ; PodUtils . waitUntilPodStabilityReplicasCount ( namespaceName , clusterName + "-cruise-control-" , 0 ) ; LOGGER . info ( "Verifying that in Kafka config map there is no configuration to cruise control metric reporter" ) ; assertThrows ( WaitException . class , ( ) -> CruiseControlUtils . verifyCruiseControlMetricReporterConfigurationInKafkaConfigMapIsPresent ( CruiseControlUtils . getKafkaCruiseControlMetricsReporterConfiguration ( namespaceName , clusterName ) ) ) ; LOGGER . info ( "Cruise Control topics will not be deleted and will stay in the Kafka cluster" ) ; CruiseControlUtils . verifyThatCruiseControlTopicsArePresent ( namespaceName ) ; kafkaPods = StatefulSetUtils . ssSnapshot ( namespaceName , KafkaResources . kafkaStatefulSetName ( clusterName ) ) ; KafkaResource . replaceKafkaResourceInSpecificNamespace ( clusterName , kafka code_block = LoopStatement ; , namespaceName ) ; StatefulSetUtils . waitTillSsHasRolled ( namespaceName , kafkaStatefulSetName ( clusterName ) , 3 , kafkaPods ) ; LOGGER . info ( "Verifying that in Kafka config map there is configuration to cruise control metric reporter" ) ; CruiseControlUtils . verifyCruiseControlMetricReporterConfigurationInKafkaConfigMapIsPresent ( CruiseControlUtils . getKafkaCruiseControlMetricsReporterConfiguration ( namespaceName , clusterName ) ) ; LOGGER . info ( "Verifying that {} topics are created after CC is instantiated." , Constants . CRUISE_CONTROL_NAME ) ; CruiseControlUtils . verifyThatCruiseControlTopicsArePresent ( namespaceName ) ; }
public void test() { -> { LOGGER . info ( "Adding Cruise Control to the classic Kafka." ) ; kafka . getSpec ( ) . setCruiseControl ( new CruiseControlSpec ( ) ) ; } }
@ ParallelNamespaceTest void testDeployAndUnDeployCruiseControl ( ExtensionContext extensionContext ) throws IOException { final String namespaceName = extensionContext . getStore ( ExtensionContext . Namespace . GLOBAL ) . get ( Constants . NAMESPACE_KEY ) . toString ( ) ; final String clusterName = mapWithClusterNames . get ( extensionContext . getDisplayName ( ) ) ; resourceManager . createResource ( extensionContext , KafkaTemplates . kafkaWithCruiseControl ( clusterName , 3 , 3 ) . build ( ) ) ; Map < String , String > kafkaPods = StatefulSetUtils . ssSnapshot ( namespaceName , KafkaResources . kafkaStatefulSetName ( clusterName ) ) ; KafkaResource . replaceKafkaResourceInSpecificNamespace ( clusterName , kafka code_block = LoopStatement ; , namespaceName ) ; StatefulSetUtils . waitTillSsHasRolled ( namespaceName , kafkaStatefulSetName ( clusterName ) , 3 , kafkaPods ) ; LOGGER . info ( "Verifying that in {} is not present in the Kafka cluster" , Constants . CRUISE_CONTROL_NAME ) ; assertThat ( KafkaResource . kafkaClient ( ) . inNamespace ( namespaceName ) . withName ( clusterName ) . get ( ) . getSpec ( ) . getCruiseControl ( ) , nullValue ( ) ) ; LOGGER . info ( "Verifying that {} pod is not present" , clusterName + "-cruise-control-" ) ; PodUtils . waitUntilPodStabilityReplicasCount ( namespaceName , clusterName + "-cruise-control-" , 0 ) ; LOGGER . info ( "Verifying that in Kafka config map there is no configuration to cruise control metric reporter" ) ; assertThrows ( WaitException . class , ( ) -> CruiseControlUtils . verifyCruiseControlMetricReporterConfigurationInKafkaConfigMapIsPresent ( CruiseControlUtils . getKafkaCruiseControlMetricsReporterConfiguration ( namespaceName , clusterName ) ) ) ; LOGGER . info ( "Cruise Control topics will not be deleted and will stay in the Kafka cluster" ) ; CruiseControlUtils . verifyThatCruiseControlTopicsArePresent ( namespaceName ) ; kafkaPods = StatefulSetUtils . ssSnapshot ( namespaceName , KafkaResources . kafkaStatefulSetName ( clusterName ) ) ; KafkaResource . replaceKafkaResourceInSpecificNamespace ( clusterName , kafka code_block = LoopStatement ; , namespaceName ) ; StatefulSetUtils . waitTillSsHasRolled ( namespaceName , kafkaStatefulSetName ( clusterName ) , 3 , kafkaPods ) ; LOGGER . info ( "Verifying that in Kafka config map there is configuration to cruise control metric reporter" ) ; CruiseControlUtils . verifyCruiseControlMetricReporterConfigurationInKafkaConfigMapIsPresent ( CruiseControlUtils . getKafkaCruiseControlMetricsReporterConfiguration ( namespaceName , clusterName ) ) ; LOGGER . info ( "Verifying that {} topics are created after CC is instantiated." , Constants . CRUISE_CONTROL_NAME ) ; CruiseControlUtils . verifyThatCruiseControlTopicsArePresent ( namespaceName ) ; }
@ ParallelNamespaceTest void testDeployAndUnDeployCruiseControl ( ExtensionContext extensionContext ) throws IOException { final String namespaceName = extensionContext . getStore ( ExtensionContext . Namespace . GLOBAL ) . get ( Constants . NAMESPACE_KEY ) . toString ( ) ; final String clusterName = mapWithClusterNames . get ( extensionContext . getDisplayName ( ) ) ; resourceManager . createResource ( extensionContext , KafkaTemplates . kafkaWithCruiseControl ( clusterName , 3 , 3 ) . build ( ) ) ; Map < String , String > kafkaPods = StatefulSetUtils . ssSnapshot ( namespaceName , KafkaResources . kafkaStatefulSetName ( clusterName ) ) ; KafkaResource . replaceKafkaResourceInSpecificNamespace ( clusterName , kafka code_block = LoopStatement ; , namespaceName ) ; StatefulSetUtils . waitTillSsHasRolled ( namespaceName , kafkaStatefulSetName ( clusterName ) , 3 , kafkaPods ) ; LOGGER . info ( "Verifying that in {} is not present in the Kafka cluster" , Constants . CRUISE_CONTROL_NAME ) ; assertThat ( KafkaResource . kafkaClient ( ) . inNamespace ( namespaceName ) . withName ( clusterName ) . get ( ) . getSpec ( ) . getCruiseControl ( ) , nullValue ( ) ) ; LOGGER . info ( "Verifying that {} pod is not present" , clusterName + "-cruise-control-" ) ; PodUtils . waitUntilPodStabilityReplicasCount ( namespaceName , clusterName + "-cruise-control-" , 0 ) ; LOGGER . info ( "Verifying that in Kafka config map there is no configuration to cruise control metric reporter" ) ; assertThrows ( WaitException . class , ( ) -> CruiseControlUtils . verifyCruiseControlMetricReporterConfigurationInKafkaConfigMapIsPresent ( CruiseControlUtils . getKafkaCruiseControlMetricsReporterConfiguration ( namespaceName , clusterName ) ) ) ; LOGGER . info ( "Cruise Control topics will not be deleted and will stay in the Kafka cluster" ) ; CruiseControlUtils . verifyThatCruiseControlTopicsArePresent ( namespaceName ) ; kafkaPods = StatefulSetUtils . ssSnapshot ( namespaceName , KafkaResources . kafkaStatefulSetName ( clusterName ) ) ; KafkaResource . replaceKafkaResourceInSpecificNamespace ( clusterName , kafka code_block = LoopStatement ; , namespaceName ) ; StatefulSetUtils . waitTillSsHasRolled ( namespaceName , kafkaStatefulSetName ( clusterName ) , 3 , kafkaPods ) ; LOGGER . info ( "Verifying that in Kafka config map there is configuration to cruise control metric reporter" ) ; CruiseControlUtils . verifyCruiseControlMetricReporterConfigurationInKafkaConfigMapIsPresent ( CruiseControlUtils . getKafkaCruiseControlMetricsReporterConfiguration ( namespaceName , clusterName ) ) ; LOGGER . info ( "Verifying that {} topics are created after CC is instantiated." , Constants . CRUISE_CONTROL_NAME ) ; CruiseControlUtils . verifyThatCruiseControlTopicsArePresent ( namespaceName ) ; }
public String getCaseFileDataByName ( String containerId , String caseId , String name , String marshallingType ) { verifyContainerId ( containerId , caseId ) ; CaseFileInstance caseFileInstance = caseService . getCaseFileInstance ( caseId ) ; Object caseFileData = caseFileInstance . getData ( name ) ; logger . debug ( "About to marshal case file data (name = {}) for case with id '{}' {}" , name , caseId , caseFileData ) ; return marshallerHelper . marshal ( containerId , marshallingType , caseFileData , new ByCaseIdContainerLocator ( caseId ) ) ; }
public void test() { try { return resource . delete ( jobId ) . execute ( ) ; } catch ( IOException e ) { LOG . warn ( "Unable to fetch system jobs from node {}:" , entry . getKey ( ) , e ) ; return null ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "deleteIfExists({}): {}" , path , r ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Executing query {}." , cqlQuery ) ; } }
public void test() { try { code_block = IfStatement ; cqlResult = ( CqlResult ) executeCQLQuery ( cqlQuery , true ) ; code_block = IfStatement ; } catch ( Exception e ) { log . error ( "Error while executing native CQL query Caused by {}." , e ) ; throw new PersistenceException ( e ) ; } }
public void test() { try { return systemMailboxesProvider . findMailbox ( Role . INBOX , addedEvent . getUsername ( ) ) . getId ( ) . equals ( addedEvent . getMailboxId ( ) ) ; } catch ( MailboxException e ) { LOGGER . warn ( "Could not resolve Inbox mailbox" , e ) ; return false ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "No pod for: " + podName + " port: " + podPort + " path: " + podPath ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Invoking: " + url + " from pod: " + podName + " port: " + podPort + " path: " + podPath ) ; } }
public void test() { if ( changedCollections . hasChanges ( ) ) { logger . info ( "changes detected : {}" , changedCollections . toString ( ) ) ; } }
@ Override public RetrieveDocumentSetResponseType retrieveDocument ( RetrieveDocumentSetRequestType msg , AssertionType assertion ) { LOG . debug ( "Begin retrieveDocument" ) ; RetrieveDocumentSetResponseType response = null ; code_block = TryStatement ;  LOG . debug ( "End retrieveDocument" ) ; return response ; }
@ Override public RetrieveDocumentSetResponseType retrieveDocument ( RetrieveDocumentSetRequestType msg , AssertionType assertion ) { LOG . debug ( "Begin retrieveDocument" ) ; RetrieveDocumentSetResponseType response = null ; code_block = TryStatement ;  LOG . debug ( "End retrieveDocument" ) ; return response ; }
@ Override public void onFailure ( IMqttToken asyncActionToken , Throwable exception ) { LOGGER . error ( "Exception:" , exception ) ; Assert . fail ( "MQTT connect failed: " + exception . getMessage ( ) ) ; }
@ Override public void commit ( Xid xid , boolean b ) throws XAException { this . commitStarted = true ; logger . debug ( "Committing XA TX. {}, One Face: {}" , xid , b ) ; }
public void test() { if ( dirView != null ) { return dirView ; } }
public void test() { if ( options . isEvictionAllowed ( ) ) { LOG . debug ( "Free space for block expansion: freeing {} bytes on {}. " , options . getSize ( ) , options . getLocation ( ) ) ; freeSpace ( sessionId , options . getSize ( ) , options . getSize ( ) , options . getLocation ( ) ) ; dirView = mAllocator . allocateBlockWithView ( sessionId , options . getSize ( ) , options . getLocation ( ) , allocatorView . refreshView ( ) , true ) ; code_block = IfStatement ; code_block = IfStatement ; } else { LOG . warn ( "Target tier: {} has no available space to store {} bytes for session: {}" , options . getLocation ( ) , options . getSize ( ) , sessionId ) ; break ; } }
public void test() { if ( dirView != null ) { return dirView ; } }
public void run ( RegressionEnvironment env ) { String methodName = ".testPerfPropertyAccess" ; String joinStatement = "@name('s0') select * from " + "SupportBeanCombinedPropslength(1)" + " where indexed[0].mapped('a').value = 'dummy'" ; env . compileDeploy ( joinStatement ) . addListener ( "s0" ) ; SupportBeanCombinedProps theEvent = SupportBeanCombinedProps . makeDefaultBean ( ) ; log . info ( methodName + " Sending events" ) ; long startTime = System . currentTimeMillis ( ) ; code_block = ForStatement ; log . info ( methodName + " Done sending events" ) ; long endTime = System . currentTimeMillis ( ) ; log . info ( methodName + " delta=" + ( endTime - startTime ) ) ; assertTrue ( ( endTime - startTime ) < 1000 ) ; env . undeployAll ( ) ; }
public void run ( RegressionEnvironment env ) { String methodName = ".testPerfPropertyAccess" ; String joinStatement = "@name('s0') select * from " + "SupportBeanCombinedPropslength(1)" + " where indexed[0].mapped('a').value = 'dummy'" ; env . compileDeploy ( joinStatement ) . addListener ( "s0" ) ; SupportBeanCombinedProps theEvent = SupportBeanCombinedProps . makeDefaultBean ( ) ; log . info ( methodName + " Sending events" ) ; long startTime = System . currentTimeMillis ( ) ; code_block = ForStatement ; log . info ( methodName + " Done sending events" ) ; long endTime = System . currentTimeMillis ( ) ; log . info ( methodName + " delta=" + ( endTime - startTime ) ) ; assertTrue ( ( endTime - startTime ) < 1000 ) ; env . undeployAll ( ) ; }
public void run ( RegressionEnvironment env ) { String methodName = ".testPerfPropertyAccess" ; String joinStatement = "@name('s0') select * from " + "SupportBeanCombinedPropslength(1)" + " where indexed[0].mapped('a').value = 'dummy'" ; env . compileDeploy ( joinStatement ) . addListener ( "s0" ) ; SupportBeanCombinedProps theEvent = SupportBeanCombinedProps . makeDefaultBean ( ) ; log . info ( methodName + " Sending events" ) ; long startTime = System . currentTimeMillis ( ) ; code_block = ForStatement ; log . info ( methodName + " Done sending events" ) ; long endTime = System . currentTimeMillis ( ) ; log . info ( methodName + " delta=" + ( endTime - startTime ) ) ; assertTrue ( ( endTime - startTime ) < 1000 ) ; env . undeployAll ( ) ; }
public void accept ( RoutingContext ctx ) { logger . info ( "test-auth: Auth accept OK" ) ; HttpResponse . responseText ( ctx , 202 ) ; echo ( ctx ) ; }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( String . format ( "Minimal bounds on layer '%s' computed in %s: %s" , tileLayerName , sw , formattedWKT ( minimalBounds ) ) ) ; } }
public void test() { try { MinimalDiffBounds geomBuildCommand = context . command ( MinimalDiffBounds . class ) . setOldVersion ( oldCommit . toString ( ) ) . setNewVersion ( newCommit . toString ( ) ) ; geomBuildCommand . setTreeNameFilter ( layerTreeName ) ; minimalBounds = geomBuildCommand . call ( ) ; sw . stop ( ) ; code_block = IfStatement ; code_block = IfStatement ; } catch ( Exception e ) { sw . stop ( ) ; LOGGER . error ( String . format ( "Error computing minimal bounds for %s...%s on layer '%s' after %s" , oldCommit , newCommit , tileLayerName , sw ) ) ; throw Throwables . propagate ( e ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "geometry mask reprojected to gridset {}: {}" , gridsetId , formattedWKT ( geomInGridsetCrs ) ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( InvalidTransportHandlerStateException ex ) { LOGGER . warn ( ex ) ; return SocketState . DATA_AVAILABLE ; } }
@ Test public void testQueryList ( ) throws Exception { MvcResult mvcResult = mockMvc . perform ( get ( "/queue/list" ) . header ( SESSION_ID , sessionId ) ) . andExpect ( status ( ) . isOk ( ) ) . andExpect ( content ( ) . contentType ( MediaType . APPLICATION_JSON_UTF8 ) ) . andReturn ( ) ; Result result = JSONUtils . parseObject ( mvcResult . getResponse ( ) . getContentAsString ( ) , Result . class ) ; Assert . assertEquals ( Status . SUCCESS . getCode ( ) , result . getCode ( ) . intValue ( ) ) ; logger . info ( "query list queue return result:{}" , mvcResult . getResponse ( ) . getContentAsString ( ) ) ; }
public void test() { if ( Objects . isNull ( branch ) ) { LOGGER . debug ( "Nothing to commit. Transaction branch is null" ) ; return ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( format ( "Injecting read/write/serial consistency levels %s/%s/%s into entity meta of %s" , readConsistencyLevel . name ( ) , writeConsistencyLevel . name ( ) , serialConsistencyLevel . name ( ) , entityClass . getCanonicalName ( ) ) ) ; } }
public void test() { if ( ! serviceConfiguration . isPresent ( ) && this . evaluatorConfigurationProviders . isEmpty ( ) ) { LOG . info ( "No service configuration given and no ConfigurationProviders set." ) ; return Optional . empty ( ) ; } else { final ConfigurationBuilder configurationBuilder = getConfigurationBuilder ( serviceConfiguration ) ; code_block = ForStatement ; return Optional . of ( configurationBuilder . build ( ) ) ; } }
@ Override public void onNext ( T t ) { log . info ( t + "" ) ; }
public void test() { if ( getLogger ( ) . isDebugEnabled ( ) ) { getLogger ( ) . debug ( "Key(%s) not found : %s" , k , cstatus ) ; } }
public void test() { if ( getLogger ( ) . isDebugEnabled ( ) ) { getLogger ( ) . debug ( "Element(%s) not found : %s" , k , cstatus ) ; } }
public void test() { if ( getLogger ( ) . isDebugEnabled ( ) ) { getLogger ( ) . debug ( "Collection(%s) is not readable : %s" , k , cstatus ) ; } }
public void test() { if ( getLogger ( ) . isDebugEnabled ( ) ) { getLogger ( ) . debug ( "Collection(%s) has wrong bkey : %s(%s)" , k , cstatus , get . getBkeyObject ( ) . getType ( ) ) ; } }
public void test() { if ( getLogger ( ) . isDebugEnabled ( ) ) { getLogger ( ) . debug ( "Collection(%s) is not a B+Tree : %s" , k , cstatus ) ; } }
public void test() { try { return templateManagerAdminServiceStub . deleteConfiguration ( domainName , configurationName ) ; } catch ( RemoteException e ) { log . error ( "RemoteException" , e ) ; throw new RemoteException ( e . getMessage ( ) , e ) ; } }
public void test() { try { setStatus ( PluginStatus . STOPPING ) ; code_block = TryStatement ;  sensorThread = null ; setStatus ( PluginStatus . STOPPED ) ; PluginHasChanged event = new PluginHasChanged ( this , getName ( ) , PluginHasChanged . PluginActions . STOP ) ; event . getPayload ( ) . addStatement ( "plugin.status" , getStatus ( ) ) ; getBusService ( ) . send ( event ) ; } catch ( Exception e ) { setStatus ( PluginStatus . FAILED ) ; setDescription ( "Plugin stopping FAILED. see logs for details." ) ; LOG . error ( "Error stopping plugin \"" + getName ( ) + "\": " + e . getLocalizedMessage ( ) , e ) ; PluginHasChanged event = new PluginHasChanged ( this , getName ( ) , PluginHasChanged . PluginActions . START ) ; event . getPayload ( ) . addStatement ( "plugin.status" , getStatus ( ) ) ; getBusService ( ) . send ( event ) ; } }
public static void main ( String [ ] args ) { BasePipelineOptions options = PipelinesOptionsFactory . create ( BasePipelineOptions . class , args ) ; String inputPath = options . getInputPath ( ) ; String tmpDir = PathBuilder . getTempDir ( options ) ; String outPath = options . getTargetPath ( ) ; Pipeline p = Pipeline . create ( options ) ; p . apply ( "Read DwCA zip archive" , DwcaIO . Read . fromCompressed ( inputPath , tmpDir ) ) . apply ( "Interpret TemporalRecord" , TemporalTransform . builder ( ) . create ( ) . interpret ( ) ) . apply ( "Interpret ExampleRecord" , ExampleTransform . exampleOne ( ) ) . apply ( "Write as Avro files" , AvroIO . write ( ExampleRecord . class ) . to ( outPath ) . withSuffix ( AVRO_EXTENSION ) ) ; LOG . info ( "Running the pipeline" ) ; p . run ( ) . waitUntilFinish ( ) ; LOG . info ( "Pipeline has been finished!" ) ; }
public static void main ( String [ ] args ) { BasePipelineOptions options = PipelinesOptionsFactory . create ( BasePipelineOptions . class , args ) ; String inputPath = options . getInputPath ( ) ; String tmpDir = PathBuilder . getTempDir ( options ) ; String outPath = options . getTargetPath ( ) ; Pipeline p = Pipeline . create ( options ) ; p . apply ( "Read DwCA zip archive" , DwcaIO . Read . fromCompressed ( inputPath , tmpDir ) ) . apply ( "Interpret TemporalRecord" , TemporalTransform . builder ( ) . create ( ) . interpret ( ) ) . apply ( "Interpret ExampleRecord" , ExampleTransform . exampleOne ( ) ) . apply ( "Write as Avro files" , AvroIO . write ( ExampleRecord . class ) . to ( outPath ) . withSuffix ( AVRO_EXTENSION ) ) ; LOG . info ( "Running the pipeline" ) ; p . run ( ) . waitUntilFinish ( ) ; LOG . info ( "Pipeline has been finished!" ) ; }
public void test() { try { log . debug ( "kurentoTest." + jsFunction + "('" + peerConnectionId + "');" ) ; browser . executeScript ( "kurentoTest." + jsFunction + "('" + peerConnectionId + "');" ) ; } catch ( WebDriverException we ) { we . printStackTrace ( ) ; log . warn ( "Client does not support RTC statistics (function kurentoTest.{}() not defined)" ) ; } }
public void test() { if ( fs . isFile ( jarPath ) && jarPath . getName ( ) . endsWith ( ".jar" ) ) { LOG . info ( "Copying jarFile = " + jarPath ) ; fs . copyToLocalFile ( jarPath , localPath ) ; } }
public List < Product > getProducts ( ) { logger . debug ( "Product service returning list of products" ) ; return Arrays . asList ( new Product ( "Laptop" , 31000.00 ) , new Product ( "Mobile" , 16000.00 ) , new Product ( "Tablet" , 15000.00 ) , new Product ( "Camera" , 23000.00 ) ) ; }
public void test() { try { code_block = IfStatement ; } catch ( Exception ex ) { log . error ( "Failed to persist session, id: " + session . getId ( ) , ex ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( EntryPersistenceException e ) { code_block = TryStatement ;  } catch ( Exception e ) { log . error ( "Failed to persist session, id: " + session . getId ( ) , e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceWishListItemServiceUtil . class , "getCommerceWishListItemByContainsCProductCount" , _getCommerceWishListItemByContainsCProductCountParameterTypes5 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , commerceWishListId , cProductId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( ( Integer ) returnObj ) . intValue ( ) ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { cipher = createDecryptCipherInternal ( ) ; } catch ( Exception e ) { Log . error ( e . getMessage ( ) ) ; cipher = null ; } }
public void test() { if ( failedNodes . size ( ) > 0 ) { Set < String > lostIds = Sets . newLinkedHashSet ( ) ; code_block = ForStatement ; int numberOfNodesToDelete = lostIds . size ( ) ; logger . info ( String . format ( "Destroying failed nodes with ids: %s" , lostIds . toString ( ) ) ) ; Set < ? extends NodeMetadata > destroyedNodes = novaContext . getComputeService ( ) . destroyNodesMatching ( Predicates . in ( failedNodes . keySet ( ) ) ) ; lostIds . clear ( ) ; code_block = ForStatement ; logger . info ( "Failed nodes destroyed ;)" ) ; int numberOfNodesSuccesfullyDeleted = lostIds . size ( ) ; success = numberOfNodesSuccesfullyDeleted == numberOfNodesToDelete ; } else { success = true ; } }
public void test() { if ( failedNodes . size ( ) > 0 ) { Set < String > lostIds = Sets . newLinkedHashSet ( ) ; code_block = ForStatement ; int numberOfNodesToDelete = lostIds . size ( ) ; logger . info ( String . format ( "Destroying failed nodes with ids: %s" , lostIds . toString ( ) ) ) ; Set < ? extends NodeMetadata > destroyedNodes = novaContext . getComputeService ( ) . destroyNodesMatching ( Predicates . in ( failedNodes . keySet ( ) ) ) ; lostIds . clear ( ) ; code_block = ForStatement ; logger . info ( "Failed nodes destroyed ;)" ) ; int numberOfNodesSuccesfullyDeleted = lostIds . size ( ) ; success = numberOfNodesSuccesfullyDeleted == numberOfNodesToDelete ; } else { success = true ; } }
public void test() { if ( LOGGER . isInfoEnabled ( ) ) { LOGGER . info ( "ServiceLoader loaded IntegrationStepHandlers: {}" , handlers . size ( ) - integrationStepHandlers . size ( ) ) ; } }
public void test() { try { process . stop ( ) ; } catch ( Throwable t ) { LOG . error ( "Failed to stop process" , t ) ; } }
public void test() { try { cli . init ( ) ; } catch ( Exception e ) { logger . error ( e ) ; System . exit ( 0 ) ; } }
public void test() { try { _log . info ( sql ) ; return facade . selectStringList ( sql , columnList ) ; } catch ( RuntimeException continued ) { _log . info ( "*Failed to select sequence meta: " + continued . getMessage ( ) ) ; return DfCollectionUtil . newArrayList ( ) ; } }
public void test() { if ( tcpServer == null ) { logger . info ( "begin to create h2 database tcp server..." ) ; tcpServer = Server . createTcpServer ( new String [ ] code_block = "" ; ) . start ( ) ; logger . info ( String . format ( "h2 database tcp server is started on port %s" , TCP_PORT ) ) ; } else-if ( ! tcpServer . isRunning ( true ) ) { logger . info ( "begin to start h2 database tcp server..." ) ; tcpServer . start ( ) ; logger . info ( String . format ( "h2 database tcp server is started on port %s" , TCP_PORT ) ) ; } else { logger . info ( "h2 database tcp server is already running" ) ; } }
public void test() { if ( tcpServer == null ) { logger . info ( "begin to create h2 database tcp server..." ) ; tcpServer = Server . createTcpServer ( new String [ ] code_block = "" ; ) . start ( ) ; logger . info ( String . format ( "h2 database tcp server is started on port %s" , TCP_PORT ) ) ; } else-if ( ! tcpServer . isRunning ( true ) ) { logger . info ( "begin to start h2 database tcp server..." ) ; tcpServer . start ( ) ; logger . info ( String . format ( "h2 database tcp server is started on port %s" , TCP_PORT ) ) ; } else { logger . info ( "h2 database tcp server is already running" ) ; } }
public void test() { if ( tcpServer == null ) { logger . info ( "begin to create h2 database tcp server..." ) ; tcpServer = Server . createTcpServer ( new String [ ] code_block = "" ; ) . start ( ) ; logger . info ( String . format ( "h2 database tcp server is started on port %s" , TCP_PORT ) ) ; } else-if ( ! tcpServer . isRunning ( true ) ) { logger . info ( "begin to start h2 database tcp server..." ) ; tcpServer . start ( ) ; logger . info ( String . format ( "h2 database tcp server is started on port %s" , TCP_PORT ) ) ; } else { logger . info ( "h2 database tcp server is already running" ) ; } }
public void test() { if ( webServer == null ) { logger . info ( "begin to create h2 database web server..." ) ; webServer = Server . createWebServer ( new String [ ] code_block = "" ; ) . start ( ) ; logger . info ( String . format ( "h2 database web server is started on port %s" , WEB_PORT ) ) ; } else-if ( ! webServer . isRunning ( true ) ) { logger . info ( "begin to start h2 database web server..." ) ; webServer . start ( ) ; logger . info ( String . format ( "h2 database web server is started on port %s" , WEB_PORT ) ) ; } else { logger . info ( "h2 database web server is already running." ) ; } }
public void test() { if ( webServer == null ) { logger . info ( "begin to create h2 database web server..." ) ; webServer = Server . createWebServer ( new String [ ] code_block = "" ; ) . start ( ) ; logger . info ( String . format ( "h2 database web server is started on port %s" , WEB_PORT ) ) ; } else-if ( ! webServer . isRunning ( true ) ) { logger . info ( "begin to start h2 database web server..." ) ; webServer . start ( ) ; logger . info ( String . format ( "h2 database web server is started on port %s" , WEB_PORT ) ) ; } else { logger . info ( "h2 database web server is already running." ) ; } }
public void test() { if ( webServer == null ) { logger . info ( "begin to create h2 database web server..." ) ; webServer = Server . createWebServer ( new String [ ] code_block = "" ; ) . start ( ) ; logger . info ( String . format ( "h2 database web server is started on port %s" , WEB_PORT ) ) ; } else-if ( ! webServer . isRunning ( true ) ) { logger . info ( "begin to start h2 database web server..." ) ; webServer . start ( ) ; logger . info ( String . format ( "h2 database web server is started on port %s" , WEB_PORT ) ) ; } else { logger . info ( "h2 database web server is already running." ) ; } }
public void test() { try { String baseDir = getBaseDir ( url ) ; code_block = IfStatement ; code_block = IfStatement ; } catch ( SQLException e ) { logger . error ( "start h2 dabase server error" , e ) ; } }
public void test() { if ( s3ObjectSummary . getSize ( ) == 0 ) { LOGGER . info ( "Ignoring unregistered zero byte S3 file. s3Key=\"{}\" storageName=\"{}\" businessObjectDataKey={}" , s3ObjectSummary . getKey ( ) , storageName , businessObjectDataKeyAsJson ) ; } else { throw new IllegalStateException ( String . format ( "Found unregistered non-empty S3 file \"%s\" in \"%s\" storage. Business object data {%s}" , s3ObjectSummary . getKey ( ) , storageName , businessObjectDataHelper . businessObjectDataKeyToString ( businessObjectDataKey ) ) ) ; } }
public void test() { try { userSettings = userSettingsService . findUserSettings ( limit , offset ) ; } catch ( UserSettingsServiceException e ) { logger . error ( "Unable to get user settings:" , e ) ; return ( Response . serverError ( ) . build ( ) ) ; } }
public void test() { -> { Registration registration = new Registration ( WORKSPACE_FOLDERS_CAPABILITY_ID , WORKSPACE_FOLDERS_CAPABILITY_NAME , null ) ; RegistrationParams registrationParams = new RegistrationParams ( Collections . singletonList ( registration ) ) ; getClient ( ) . registerCapability ( registrationParams ) ; this . initialized . complete ( null ) ; log . info ( "Initialization completed after {} ms" , ManagementFactory . getRuntimeMXBean ( ) . getUptime ( ) ) ; } }
public void test() { if ( loggedMBeanGauges . add ( mbeanObjectName ) ) { logger . warn ( "error accessing mbean: {}" , mbeanObjectName , e ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "==> HiveMetastoreHook.onCreateTable()" ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "<== HiveMetastoreHook.onCreateTable()" ) ; } }
@ Test void testUpgradeAcrossVersionsWithNoKafkaVersion ( ExtensionContext extensionContext ) throws IOException { JsonObject acrossUpgradeData = buildDataForUpgradeAcrossVersions ( ) ; JsonObject conversionTool = getConversionToolDataFromUpgradeJSON ( ) ; String continuousTopicName = "continuous-topic" ; String producerName = "hello-world-producer" ; String consumerName = "hello-world-consumer" ; String continuousConsumerGroup = "continuous-consumer-group" ; setupEnvAndUpgradeClusterOperator ( extensionContext , acrossUpgradeData , producerName , consumerName , continuousTopicName , continuousConsumerGroup , null , NAMESPACE ) ; convertCRDs ( conversionTool , NAMESPACE ) ; changeClusterOperator ( acrossUpgradeData , NAMESPACE ) ; zkPods = StatefulSetUtils . waitTillSsHasRolled ( KafkaResources . zookeeperStatefulSetName ( clusterName ) , 3 , zkPods ) ; kafkaPods = StatefulSetUtils . waitTillSsHasRolled ( KafkaResources . kafkaStatefulSetName ( clusterName ) , 3 , kafkaPods ) ; eoPods = DeploymentUtils . waitTillDepHasRolled ( KafkaResources . entityOperatorDeploymentName ( clusterName ) , 1 , eoPods ) ; LOGGER . info ( "Rolling to new images has finished!" ) ; logPodImages ( clusterName ) ; changeKafkaAndLogFormatVersion ( acrossUpgradeData . getJsonObject ( "proceduresAfterOperatorUpgrade" ) , acrossUpgradeData , clusterName , extensionContext ) ; logPodImages ( clusterName ) ; checkAllImages ( acrossUpgradeData . getJsonObject ( "imagesAfterKafkaUpgrade" ) ) ; PodUtils . verifyThatRunningPodsAreStable ( clusterName ) ; verifyProcedure ( acrossUpgradeData , producerName , consumerName , NAMESPACE ) ; assertNoCoErrorsLogged ( 0 ) ; }
@ Override public void init ( ) throws Exception { String xmlConfig = this . getConfigManager ( ) . getConfigItem ( SystemConstants . CONFIG_ITEM_LANGS ) ; this . getCacheWrapper ( ) . initCache ( xmlConfig ) ; logger . debug ( "{} ready: initialized" , this . getClass ( ) . getName ( ) ) ; }
public void test() { switch ( channelUID . getId ( ) ) { case PLAYURL : playMedia ( channelUID , command ) ; break ; case STOP : stopMedia ( channelUID , command ) ; break ; default : logger . debug ( "Thing {}: unexpected command {} from channel {}" , getThing ( ) . getUID ( ) , command , channelUID . getId ( ) ) ; break ; } }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "Destroy registry:" + getUrl ( ) ) ; } }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "Destroy unregister url " + url ) ; } }
public void test() { try { unregister ( url ) ; code_block = IfStatement ; } catch ( Throwable t ) { logger . warn ( "Failed to unregister url " + url + " to registry " + getUrl ( ) + " on destroy, cause: " + t . getMessage ( ) , t ) ; } }
public void test() { if ( logger . isInfoEnabled ( ) ) { logger . info ( "Destroy unsubscribe url " + url ) ; } }
public void test() { try { unsubscribe ( url , listener ) ; code_block = IfStatement ; } catch ( Throwable t ) { logger . warn ( "Failed to unsubscribe url " + url + " to registry " + getUrl ( ) + " on destroy, cause: " + t . getMessage ( ) , t ) ; } }
public void test() { try { addMemberToRoleOrGroup ( session , groupDN , memberDN , UNIQUE_MEMBER ) ; } catch ( final LdapNoSuchObjectException e ) { LOGGER . debug ( "Group {} doesn't exist" , groupDN ) ; } }
public void test() { try { String xml = new AvatarConfigDOM ( ) . createConfigXml ( config ) ; this . getConfigManager ( ) . updateConfigItem ( JpAvatarSystemConstants . CONFIG_ITEM , xml ) ; this . setConfig ( config ) ; } catch ( Throwable t ) { _logger . error ( "Error updating jpavatar config" , t ) ; throw new ApsSystemException ( "Error updating jpavatar config" , t ) ; } }
@ Test public void testMarkStartTime ( ) { final DefaultTraceId traceId = new DefaultTraceId ( "agentId" , 0 , 0 ) ; TraceRoot traceRoot = new DefaultTraceRoot ( traceId , "agentId" , System . currentTimeMillis ( ) , 0 ) ; Span span = new Span ( traceRoot ) ; span . markBeforeTime ( ) ; span . setElapsedTime ( ( int ) ( span . getStartTime ( ) + 10 ) ) ; logger . debug ( "span:{}" , span ) ; final SpanEvent spanEvent = new SpanEvent ( ) ; long currentTime = System . currentTimeMillis ( ) ; spanEvent . setStartTime ( currentTime ) ; spanEvent . setElapsedTime ( 10 ) ; logger . debug ( "spanEvent:{}" , spanEvent ) ; span . setSpanEventList ( Arrays . asList ( spanEvent ) ) ; TSpan tSpan = new TSpan ( ) ; TSpanEvent tSpanEvent = new TSpanEvent ( ) ; tSpan . addToSpanEventList ( tSpanEvent ) ; compressorV1 . preProcess ( span , tSpan ) ; compressorV1 . postProcess ( span , tSpan ) ; Assert . assertEquals ( "startTime" , span . getStartTime ( ) + tSpanEvent . getStartElapsed ( ) , spanEvent . getStartTime ( ) ) ; Assert . assertEquals ( "endTime" , span . getStartTime ( ) + tSpanEvent . getStartElapsed ( ) + spanEvent . getElapsedTime ( ) , spanEvent . getAfterTime ( ) ) ; }
@ Test public void testMarkStartTime ( ) { final DefaultTraceId traceId = new DefaultTraceId ( "agentId" , 0 , 0 ) ; TraceRoot traceRoot = new DefaultTraceRoot ( traceId , "agentId" , System . currentTimeMillis ( ) , 0 ) ; Span span = new Span ( traceRoot ) ; span . markBeforeTime ( ) ; span . setElapsedTime ( ( int ) ( span . getStartTime ( ) + 10 ) ) ; logger . debug ( "span:{}" , span ) ; final SpanEvent spanEvent = new SpanEvent ( ) ; long currentTime = System . currentTimeMillis ( ) ; spanEvent . setStartTime ( currentTime ) ; spanEvent . setElapsedTime ( 10 ) ; logger . debug ( "spanEvent:{}" , spanEvent ) ; span . setSpanEventList ( Arrays . asList ( spanEvent ) ) ; TSpan tSpan = new TSpan ( ) ; TSpanEvent tSpanEvent = new TSpanEvent ( ) ; tSpan . addToSpanEventList ( tSpanEvent ) ; compressorV1 . preProcess ( span , tSpan ) ; compressorV1 . postProcess ( span , tSpan ) ; Assert . assertEquals ( "startTime" , span . getStartTime ( ) + tSpanEvent . getStartElapsed ( ) , spanEvent . getStartTime ( ) ) ; Assert . assertEquals ( "endTime" , span . getStartTime ( ) + tSpanEvent . getStartElapsed ( ) + spanEvent . getElapsedTime ( ) , spanEvent . getAfterTime ( ) ) ; }
@ Override public void setValency ( Integer valency ) { logger . debug ( "Setting valency: " , valency ) ; super . setValency ( valency ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Receiving message from camel endpoint: '" + endpointUri + "'" ) ; } }
@ Override public Message receive ( TestContext context , long timeout ) { String endpointUri = context . replaceDynamicContentInString ( endpointConfiguration . getEndpointUri ( ) ) ; code_block = IfStatement ; Exchange exchange = getConsumerTemplate ( ) . receive ( endpointUri , timeout ) ; code_block = IfStatement ; log . info ( "Received message from camel endpoint: '" + endpointUri + "'" ) ; Message message = endpointConfiguration . getMessageConverter ( ) . convertInbound ( exchange , endpointConfiguration , context ) ; context . onInboundMessage ( message ) ; String correlationKeyName = endpointConfiguration . getCorrelator ( ) . getCorrelationKeyName ( getName ( ) ) ; String correlationKey = endpointConfiguration . getCorrelator ( ) . getCorrelationKey ( message ) ; correlationManager . saveCorrelationKey ( correlationKeyName , correlationKey , context ) ; correlationManager . store ( correlationKey , exchange ) ; return message ; }
public void test() { try { MethodKey methodKey = new MethodKey ( CalendarBookingServiceUtil . class , "getCalendarBookings" , _getCalendarBookingsParameterTypes12 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , calendarId , startTime , endTime ) ; Object returnObj = null ; code_block = TryStatement ;  return ( java . util . List < com . liferay . calendar . model . CalendarBooking > ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( MBBanServiceUtil . class , "addBan" , _addBanParameterTypes0 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , banUserId , serviceContext ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . message . boards . model . MBBan ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void registerUuid ( String key ) throws DaikinCommunicationException { Map < String , String > params = new HashMap < > ( ) ; params . put ( "key" , key ) ; String response = invoke ( registerUuidUri , params ) ; logger . debug ( "registerUuid result: {}" , response ) ; }
public boolean isFinancialYearActiveForPosting ( Date fromDate , Date toDate ) { logger . info ( "Obtained session" ) ; String result = "" ; Query query = getCurrentSession ( ) . createQuery ( "" + " from CFinancialYear cfinancialyear where   cfinancialyear.isActiveForPosting=false and cfinancialyear.startingDate <=:sDate and cfinancialyear.endingDate >=:eDate  " ) ; query . setDate ( "sDate" , fromDate ) ; query . setDate ( "eDate" , toDate ) ; ArrayList list = ( ArrayList ) query . list ( ) ; if ( list . size ( ) > 0 ) return false ; else return true ; }
public void test() { try { String data = ( String ) s . get ( JsonKey . FEED_DATA ) ; code_block = IfStatement ; feedList . add ( mapper . convertValue ( s , Feed . class ) ) ; } catch ( Exception ex ) { logger . error ( context , "FeedServiceImpl:getRecordsByUserId :Exception occurred while mapping feed data." , ex ) ; } }
public void test() { try { jsonObject . put ( "testsList" , array ) ; response . setContentType ( "application/json" ) ; response . getWriter ( ) . print ( jsonObject . toString ( ) ) ; } catch ( JSONException exception ) { LOG . warn ( exception . toString ( ) ) ; } }
public void test() { if ( innerRank != size ) { LOG . warn ( "Failed to build sorted map from state, this may result in wrong result. " + "The sort key is {}, partition key is {}, " + "treeMap is {}. The expected inner rank is {}, but current size is {}." , sortKey , partitionKey , treeMap , innerRank , size ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( exception , exception ) ; } }
public void test() { if ( LOGGER . isWarnEnabled ( ) ) { LOGGER . warn ( "cannot find event handler by class: " + context . getClass ( ) ) ; } }
private List < BundleItem > getBundleListFromTDM ( ) throws Exception { ArrayList < BundleItem > output = new ArrayList < BundleItem > ( ) ; _log . info ( "Getting current bundle list from TDM..." ) ; List < JsonObject > bundles = _apiLibrary . getItemsForRequest ( "bundle" , "list" ) ; code_block = ForStatement ; _log . info ( "Found " + output . size ( ) + " bundle(s) available from the TDM." ) ; return output ; }
private List < BundleItem > getBundleListFromTDM ( ) throws Exception { ArrayList < BundleItem > output = new ArrayList < BundleItem > ( ) ; _log . info ( "Getting current bundle list from TDM..." ) ; List < JsonObject > bundles = _apiLibrary . getItemsForRequest ( "bundle" , "list" ) ; code_block = ForStatement ; _log . info ( "Found " + output . size ( ) + " bundle(s) available from the TDM." ) ; return output ; }
public void test() { if ( level . intValue ( ) >= Level . SEVERE . intValue ( ) ) { slf4jLogger . error ( msg ) ; } else-if ( level . intValue ( ) >= Level . WARNING . intValue ( ) ) { slf4jLogger . warn ( msg ) ; } else-if ( level . intValue ( ) >= Level . CONFIG . intValue ( ) ) { slf4jLogger . info ( msg ) ; } else-if ( level . intValue ( ) >= Level . FINE . intValue ( ) ) { slf4jLogger . debug ( msg ) ; } else { slf4jLogger . trace ( msg ) ; } }
public void test() { if ( level . intValue ( ) >= Level . SEVERE . intValue ( ) ) { slf4jLogger . error ( msg ) ; } else-if ( level . intValue ( ) >= Level . WARNING . intValue ( ) ) { slf4jLogger . warn ( msg ) ; } else-if ( level . intValue ( ) >= Level . CONFIG . intValue ( ) ) { slf4jLogger . info ( msg ) ; } else-if ( level . intValue ( ) >= Level . FINE . intValue ( ) ) { slf4jLogger . debug ( msg ) ; } else { slf4jLogger . trace ( msg ) ; } }
public void test() { if ( level . intValue ( ) >= Level . SEVERE . intValue ( ) ) { slf4jLogger . error ( msg ) ; } else-if ( level . intValue ( ) >= Level . WARNING . intValue ( ) ) { slf4jLogger . warn ( msg ) ; } else-if ( level . intValue ( ) >= Level . CONFIG . intValue ( ) ) { slf4jLogger . info ( msg ) ; } else-if ( level . intValue ( ) >= Level . FINE . intValue ( ) ) { slf4jLogger . debug ( msg ) ; } else { slf4jLogger . trace ( msg ) ; } }
public void test() { if ( level . intValue ( ) >= Level . SEVERE . intValue ( ) ) { slf4jLogger . error ( msg ) ; } else-if ( level . intValue ( ) >= Level . WARNING . intValue ( ) ) { slf4jLogger . warn ( msg ) ; } else-if ( level . intValue ( ) >= Level . CONFIG . intValue ( ) ) { slf4jLogger . info ( msg ) ; } else-if ( level . intValue ( ) >= Level . FINE . intValue ( ) ) { slf4jLogger . debug ( msg ) ; } else { slf4jLogger . trace ( msg ) ; } }
public void test() { if ( level . intValue ( ) >= Level . SEVERE . intValue ( ) ) { slf4jLogger . error ( msg ) ; } else-if ( level . intValue ( ) >= Level . WARNING . intValue ( ) ) { slf4jLogger . warn ( msg ) ; } else-if ( level . intValue ( ) >= Level . CONFIG . intValue ( ) ) { slf4jLogger . info ( msg ) ; } else-if ( level . intValue ( ) >= Level . FINE . intValue ( ) ) { slf4jLogger . debug ( msg ) ; } else { slf4jLogger . trace ( msg ) ; } }
public void test() { try { cancelJob ( datasetName , bqDataset ) ; } catch ( BigQueryException e ) { LOG . error ( "Exception when cancelling BigQuery job '{}' for stage '{}': {}" , bqDataset . getJobId ( ) , datasetName , e . getMessage ( ) ) ; ex = new SQLEngineException ( String . format ( "Exception when executing cleanup for stage '%s'" , datasetName ) , e ) ; } }
public void test() { try { deleteTempFolder ( bqDataset ) ; } catch ( IOException e ) { LOG . error ( "Failed to delete temporary directory '{}' for stage '{}': {}" , bqDataset . getGCSPath ( ) , datasetName , e . getMessage ( ) ) ; code_block = IfStatement ; } }
public void test() { try { pool . setDriverClass ( driver ) ; } catch ( PropertyVetoException ex ) { logger . error ( "Error when setting the database driver " + driver + "{}" , ex . getMessage ( ) ) ; } }
public void test() { try { return Double . parseDouble ( provider . getAsString ( new OID ( ".1.3.6.1.4.1.2021.11.11.0" ) ) ) / 100.0 ; } catch ( Exception e ) { logger . warn ( "Exception during getCPUStateIdle" , e ) ; } }
private void killBookie ( ArrayList < BookieSocketAddress > firstEnsemble , BookieSocketAddress ensemble ) throws InterruptedException { LOG . info ( "Killing " + ensemble + " from ensemble=" + firstEnsemble ) ; killBookie ( ensemble ) ; }
public void test() { try { FileUtils . saveCommandHistoryString ( history , new File ( Constants . CMD_HISTORY_FILE ) ) ; } catch ( Throwable e ) { logger . error ( "save command history failed" , e ) ; } }
@ Test public void testGetRootFolder ( ) throws Exception { final com . box . sdk . BoxFolder result = requestBody ( "direct://GETROOTFOLDER" , null ) ; assertNotNull ( result , "getRootFolder result" ) ; LOG . debug ( "getRootFolder: " + result ) ; }
private synchronized void stopping ( Bundle bundle ) { ModuleState . State moduleState = getModuleState ( bundle ) . getState ( ) ; code_block = IfStatement ; logger . info ( "--- Stopping DX OSGi bundle {} --" , getDisplayName ( bundle ) ) ; code_block = IfStatement ; long startTime = System . currentTimeMillis ( ) ; JahiaTemplatesPackage jahiaTemplatesPackage = templatePackageRegistry . lookupByBundle ( bundle ) ; code_block = IfStatement ; code_block = IfStatement ; long totalTime = System . currentTimeMillis ( ) - startTime ; logger . info ( "--- Finished stopping DX OSGi bundle {} in {}ms --" , getDisplayName ( bundle ) , totalTime ) ; }
private synchronized void stopping ( Bundle bundle ) { ModuleState . State moduleState = getModuleState ( bundle ) . getState ( ) ; code_block = IfStatement ; logger . info ( "--- Stopping DX OSGi bundle {} --" , getDisplayName ( bundle ) ) ; code_block = IfStatement ; long startTime = System . currentTimeMillis ( ) ; JahiaTemplatesPackage jahiaTemplatesPackage = templatePackageRegistry . lookupByBundle ( bundle ) ; code_block = IfStatement ; code_block = IfStatement ; long totalTime = System . currentTimeMillis ( ) - startTime ; logger . info ( "--- Finished stopping DX OSGi bundle {} in {}ms --" , getDisplayName ( bundle ) , totalTime ) ; }
public void test() { try { infoListBasicListTag . doTag ( infoListRendererContext . getHttpServletRequest ( ) , infoListRendererContext . getHttpServletResponse ( ) ) ; } catch ( Exception exception ) { _log . error ( "Unable to render journal articles list" , exception ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "==> AbstractServiceStore.updateTagServiceDefForDeletingRowFilterDef(" + serviceDefName + ")" ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "<== AbstractServiceStore.updateTagServiceDefForDeletingRowFilterDef(" + serviceDefName + ")" ) ; } }
public void test() { if ( isSpoolingEnabled ( conf ) && StringUtils . isNotEmpty ( spoolDir ) ) { LOG . info ( "Notification spooling is enabled: spool directory={}" , spoolDir ) ; conf . setProperty ( CONF_ATLAS_HOOK_SPOOL_DIR , spoolDir ) ; notificationProvider = new AtlasFileSpool ( conf , kafka ) ; } else { LOG . info ( "Notification spooling is not enabled" ) ; notificationProvider = kafka ; } }
public void test() { if ( isSpoolingEnabled ( conf ) && StringUtils . isNotEmpty ( spoolDir ) ) { LOG . info ( "Notification spooling is enabled: spool directory={}" , spoolDir ) ; conf . setProperty ( CONF_ATLAS_HOOK_SPOOL_DIR , spoolDir ) ; notificationProvider = new AtlasFileSpool ( conf , kafka ) ; } else { LOG . info ( "Notification spooling is not enabled" ) ; notificationProvider = kafka ; } }
public void test() { try { return findChannel ( channelName ) == null ; } catch ( IOException e ) { getLogger ( ) . error ( "Error while searching for channel " + channelName , e ) ; return false ; } }
public void test() { try { Object obj = compile ( expression , context ) ; return MVEL . executeExpression ( obj , vars ) ; } catch ( Throwable e ) { log . error ( "eval expression {} error" , expression , e ) ; return null ; } }
public void test() { if ( name != null ) { log . warn ( "Cannot close: " + name + ". Reason: " + e . getMessage ( ) , e ) ; } else { log . warn ( "Cannot close. Reason: {}" , e . getMessage ( ) , e ) ; } }
public String registerBlockListener ( BlockingQueue < QueuedBlockEvent > blockEventQueue , long timeout , TimeUnit timeUnit ) throws InvalidArgumentException { code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; code_block = IfStatement ; String handle = new BL ( blockEventQueue , timeout , timeUnit ) . getHandle ( ) ; logger . trace ( format ( "Register QueuedBlockEvent listener %s" , handle ) ) ; return handle ; }
public void test() { try { logger . info ( "Incremental build request being processed: " + module . getRootPath ( ) + " (updated)." ) ; final BuildResults results = buildService . build ( module ) ; buildResultsEvent . fire ( results ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { logger . info ( "Incremental build request being processed: " + module . getRootPath ( ) + " (updated)." ) ; final BuildResults results = buildService . build ( module ) ; buildResultsEvent . fire ( results ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { try { service . createVcVms ( spec , vNodes , null , false , null ) ; Assert . assertTrue ( false , "should throw exception but not." ) ; } catch ( Exception e ) { logger . info ( e . getMessage ( ) , e ) ; Assert . assertTrue ( true , "got expected exception." ) ; } }
@ Override public CoachShuttleGatheringSolution readSolution ( ) throws IOException { solution = new CoachShuttleGatheringSolution ( ) ; solution . setId ( 0L ) ; readLocationList ( ) ; busOrStopOrHubId = 0L ; readBusList ( ) ; readBusStopList ( ) ; int busListSize = solution . getCoachList ( ) . size ( ) + solution . getShuttleList ( ) . size ( ) ; int base = solution . getStopList ( ) . size ( ) + solution . getShuttleList ( ) . size ( ) ; BigInteger a = factorial ( base + busListSize - 1 ) ; BigInteger b = factorial ( busListSize - 1 ) ; BigInteger possibleSolutionSize = ( a == null || b == null ) ? null : a . divide ( b ) ; logger . info ( "CoachShuttleGathering {} has {} road locations, {} coaches, {} shuttles and {} bus stops" + " with a search space of {}." , getInputId ( ) , solution . getLocationList ( ) . size ( ) , solution . getCoachList ( ) . size ( ) , solution . getShuttleList ( ) . size ( ) , solution . getStopList ( ) . size ( ) , getFlooredPossibleSolutionSize ( possibleSolutionSize ) ) ; return solution ; }
public void test() { try { return binder . getApplicationTO ( applicationDAO . find ( key ) ) ; } catch ( Throwable ignore ) { LOG . debug ( "Unresolved reference" , ignore ) ; throw new UnresolvedReferenceException ( ignore ) ; } }
@ Test public void testGrantResource ( ) throws Exception { MultiValueMap < String , String > paramsMap = new LinkedMultiValueMap < > ( ) ; paramsMap . add ( "userId" , "32" ) ; paramsMap . add ( "resourceIds" , "5" ) ; MvcResult mvcResult = mockMvc . perform ( post ( "/users/grant-file" ) . header ( SESSION_ID , sessionId ) . params ( paramsMap ) ) . andExpect ( status ( ) . isOk ( ) ) . andExpect ( content ( ) . contentType ( MediaType . APPLICATION_JSON_UTF8 ) ) . andReturn ( ) ; Result result = JSONUtils . parseObject ( mvcResult . getResponse ( ) . getContentAsString ( ) , Result . class ) ; Assert . assertEquals ( Status . SUCCESS . getCode ( ) , result . getCode ( ) . intValue ( ) ) ; logger . info ( mvcResult . getResponse ( ) . getContentAsString ( ) ) ; }
public Module find ( final Long id ) { log . debug ( "find() - id: {}" , id ) ; return moduleRepository . findOne ( id ) ; }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { log . info ( "Failed to copy from blob store. Downloading from BLOB server instead." , e ) ; } }
public void test() { if ( localFile . exists ( ) ) { return localFile ; } }
public void test() { try { Thread . sleep ( initialSleep ) ; } catch ( InterruptedException e ) { LOG . warn ( e . getMessage ( ) , e ) ; } }
public void test() { if ( debug ) { Logger . debug ( PentahoSystem . class , "System Listener Start: " + systemListener . getClass ( ) . getName ( ) ) ; } }
public void test() { try { return context . getJCRPath ( path ) ; } catch ( NamespaceException e ) { log . error ( "failed to convert {} to a JCR path" , path ) ; return path . toString ( ) ; } }
static void addRegistrationDateColumn ( Statement st , String tableName , Columns col ) throws SQLException { st . executeUpdate ( "ALTER TABLE " + tableName + " ADD COLUMN " + col . REGISTRATION_DATE + " BIGINT NOT NULL DEFAULT 0;" ) ; long currentTimestamp = System . currentTimeMillis ( ) ; int updatedRows = st . executeUpdate ( String . format ( "UPDATE %s SET %s = %d;" , tableName , col . REGISTRATION_DATE , currentTimestamp ) ) ; logger . info ( "Created column '" + col . REGISTRATION_DATE + "' and set the current timestamp, " + currentTimestamp + ", to all " + updatedRows + " rows" ) ; }
public void test() { try { resp = httpclient . execute ( httpget ) ; code_block = IfStatement ; } catch ( Exception e ) { LOGGER . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( ! directory . exists ( ) ) { logger . info ( "HL7 A04 directory does not exist!  Location: " + this . dirName ) ; return ; } }
public void test() { if ( ! moveSuccess ) { logger . info ( "HL7 A04 not successfully moved to 'sent' directory." ) ; } }
public void test() { try { File directory = new File ( this . dirName ) ; code_block = IfStatement ; File [ ] listOfFiles = directory . listFiles ( new HL7A04FileFilter ( ) ) ; code_block = IfStatement ; String sendAddr = oscarProperties . getEmeraldHL7A04TransportAddr ( ) ; int sendPort = oscarProperties . getEmeraldHL7A04TransportPort ( ) ; Socket client = new Socket ( sendAddr , sendPort ) ; PrintStream out = new PrintStream ( client . getOutputStream ( ) ) ; BufferedReader in = new BufferedReader ( new InputStreamReader ( client . getInputStream ( ) ) ) ; logger . info ( "Sending file(s) to '" + sendAddr + ":" + sendPort + "'" ) ; int numErrors = 0 ; code_block = ForStatement ; in . close ( ) ; out . close ( ) ; if ( ! client . isClosed ( ) ) client . shutdownOutput ( ) ; if ( ! client . isClosed ( ) ) client . close ( ) ; if ( numErrors == 0 ) logger . info ( "Successfully sent all HL7 A04 file(s)!" ) ; code_block = "" ; } catch ( Exception e ) { logger . error ( "ERROR while sending HL7 A04 file: " + e . toString ( ) , e ) ; } }
public void test() { if ( null == group ) { log . warn ( "Missing com.apple.security.application-groups in sandbox entitlements" ) ; } else { final String application = PreferencesFactory . get ( ) . getProperty ( "application.datafolder.name" ) ; final Local folder = new FinderLocal ( String . format ( "%s/Library/Application Support" , group . path ( ) ) , application ) ; final Local previous = new ApplicationSupportDirectoryFinder ( ) . find ( ) ; code_block = IfStatement ; return folder ; } }
public void test() { try { final Trash trash = LocalTrashFactory . get ( ) ; trash . trash ( previous ) ; final Symlink symlink = LocalSymlinkFactory . get ( ) ; symlink . symlink ( previous , folder . getAbsolute ( ) ) ; } catch ( AccessDeniedException e ) { log . warn ( String . format ( "Failure cleaning up previous application support directory. %s" , e . getMessage ( ) ) ) ; } }
public void test() { try { FileUtils . copyDirectory ( new File ( previous . getAbsolute ( ) ) , new File ( folder . getAbsolute ( ) ) ) ; log . warn ( String . format ( "Move application support folder %s to Trash" , previous ) ) ; code_block = TryStatement ;  } catch ( IOException e ) { log . warn ( String . format ( "Failure migrating %s to security application group directory %s. %s" , previous , folder , e . getMessage ( ) ) ) ; } }
public void test() { if ( previous . exists ( ) && ! previous . isSymbolicLink ( ) ) { log . warn ( String . format ( "Migrate application support folder from %s to %s" , previous , folder ) ) ; code_block = TryStatement ;  } else { log . debug ( String . format ( "No previous application support folder found in %s" , previous ) ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceShipmentServiceUtil . class , "getCommerceShipment" , _getCommerceShipmentParameterTypes4 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , commerceShipmentId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . commerce . model . CommerceShipment ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
@ Test public void testGetPaginatedGroup ( ) { LOGGER . debug ( "testGetPaginatedGroup" ) ; super . testGetPaginatedEntities ( ) ; }
public void test() { try { Map < String , Object > dynamicMap = new HashMap < String , Object > ( ) ; TrasformazioniUtils . fillDynamicMapRispostaTracciatoCSV ( log , dynamicMap , ContextThreadLocal . get ( ) , headerRisposta , jsonEsito , codDominio , codTipoVersamento , dominio , applicazione , versamento , documento , esitoOperazione , descrizioneEsitoOperazione , tipoOperazione ) ; TrasformazioniUtils . convertFreeMarkerTemplate ( name , template , dynamicMap , bw ) ; log . debug ( "Trasformazione esito caricamento pendenza JSON -> CSV tramite template freemarker completata con successo." ) ; } catch ( TrasformazioneException e ) { log . error ( "Trasformazione esito caricamento pendenza JSON -> CSV tramite template freemarker completata con errore: " + e . getMessage ( ) , e ) ; throw new GovPayException ( e . getMessage ( ) , EsitoOperazione . TRASFORMAZIONE , e , e . getMessage ( ) ) ; } catch ( UnprocessableEntityException e ) { log . error ( "Trasformazione esito caricamento pendenza JSON -> CSV tramite template freemarker completata con errore: " + e . getMessage ( ) , e ) ; } }
public void test() { if ( reader . next ( key , value ) ) { LOG . info ( "Output File: " + status . getPath ( ) ) ; LOG . info ( "key: '" + key + "' value: '" + value + "' expected: '" + expectedResult + "'" ) ; assertEquals ( "Expected value: '" + expectedResult + "' != '" + value + "'" , expectedResult , value . get ( ) , delta ) ; } }
public void test() { if ( id == null ) { LOG . error ( "Container complete event for unknown container id " + cont . getContainerId ( ) ) ; } else { assignedContainerToIDMap . remove ( cont . getContainerId ( ) ) ; idToContainerMap . remove ( id ) ; String diagnostics = StringInterner . weakIntern ( cont . getDiagnostics ( ) ) ; code_block = IfStatement ; } }
public void test() { try { sendHeartbeat ( false ) ; } catch ( Exception e ) { log . error ( "Error performing heartbeat: " + e . getMessage ( ) ) ; } }
public void test() { try { secretsService . delete ( secret . getAccountId ( ) , secret . getValue ( ) ) ; } catch ( IOException e ) { log . error ( "Failed to delete secret from storage [{}]" , secret . getId ( ) , e ) ; throw new IllegalStateException ( e ) ; } }
private void processCurrentTagAgainstDesiredTags ( final IRODSTagValue currentTag , final String [ ] userTags , final UserAnnotatedCatalogItem irodsTagGrouping ) throws JargonException { log . info ( "looking to see if iRODS tag still desired:{}" , currentTag ) ; boolean isDesired = false ; code_block = ForStatement ; code_block = IfStatement ; }
public void test() { try { listener . updatedProfileImage ( user ) ; } catch ( Exception e ) { logger . warn ( "Exception at updateProfileImage" , e ) ; } }
public String getRemoteBssWsUrl ( ) { String servletAddress = "https://" + host + ":" + port + "/oscm/" ; code_block = IfStatement ; servletAddress += serviceName + "/" + servicePort . name ( ) ; String url = servletAddress + "?wsdl" ; code_block = IfStatement ; logger . debug ( url ) ; verifyAttributes ( url ) ; return url ; }
@ Override public void removeStoreCommand ( final org . locationtech . geowave . service . grpc . protobuf . RemoveStoreCommandParametersProtos request , final StreamObserver < org . locationtech . geowave . service . grpc . protobuf . GeoWaveReturnTypesProtos . StringResponseProtos > responseObserver ) { final RemoveStoreCommand cmd = new RemoveStoreCommand ( ) ; final Map < FieldDescriptor , Object > m = request . getAllFields ( ) ; GeoWaveGrpcServiceCommandUtil . setGrpcToCommandFields ( m , cmd ) ; final File configFile = GeoWaveGrpcServiceOptions . geowaveConfigFile ; final OperationParams params = new ManualOperationParams ( ) ; params . getContext ( ) . put ( ConfigOptions . PROPERTIES_FILE_CONTEXT , configFile ) ; cmd . prepare ( params ) ; LOGGER . info ( "Executing RemoveStoreCommand..." ) ; code_block = TryStatement ;  }
public void test() { try { final String result = cmd . computeResults ( params ) ; final StringResponseProtos resp = StringResponseProtos . newBuilder ( ) . setResponseValue ( result ) . build ( ) ; responseObserver . onNext ( resp ) ; responseObserver . onCompleted ( ) ; } catch ( final Exception e ) { LOGGER . error ( "Exception encountered executing command" , e ) ; responseObserver . onError ( e ) ; } }
public void test() { if ( result . size ( ) == 0 ) { LOGGER . warn ( "Project {1} does not exist." , project . getName ( ) ) ; } else { commit . addDelete ( project ) ; } }
public MGsiegel findById ( java . lang . Short id ) { log . debug ( "getting MGsiegel instance with id: " + id ) ; code_block = TryStatement ;  }
public void test() { try { MGsiegel instance = ( MGsiegel ) getSession ( ) . get ( "sernet.gs.reveng.MGsiegel" , id ) ; return instance ; } catch ( RuntimeException re ) { log . error ( "get failed" , re ) ; throw re ; } }
private void changeInputValueForm2 ( int index , String keyword ) throws Exception { WebElement input = getSettingWebElement ( index ) ; input . clear ( ) ; input . sendKeys ( keyword ) ; logger . info ( String . format ( "Wrote value: %s to element with id %s" , keyword , index ) ) ; driver . findElement ( By . xpath ( "//form[@id='" + AppHtmlElements . APP_CONFIG_FORM2 + "']" + "//input[@class='" + AppHtmlElements . APP_CONFIG_FORM_BUTTON_CLASS + "']" ) ) . click ( ) ; logger . info ( "Clicked save configuration button in APP settings" ) ; if ( ! getExecutionResult ( ) ) throw new Exception ( ) ; }
private void changeInputValueForm2 ( int index , String keyword ) throws Exception { WebElement input = getSettingWebElement ( index ) ; input . clear ( ) ; input . sendKeys ( keyword ) ; logger . info ( String . format ( "Wrote value: %s to element with id %s" , keyword , index ) ) ; driver . findElement ( By . xpath ( "//form[@id='" + AppHtmlElements . APP_CONFIG_FORM2 + "']" + "//input[@class='" + AppHtmlElements . APP_CONFIG_FORM_BUTTON_CLASS + "']" ) ) . click ( ) ; logger . info ( "Clicked save configuration button in APP settings" ) ; if ( ! getExecutionResult ( ) ) throw new Exception ( ) ; }
public void test() { try { setupCurrentEntry ( ) ; hll . addRaw ( id . hashCode ( ) ) ; } catch ( Exception e ) { log . error ( "Failed to report active user." , e ) ; } }
public void test() { try { return Integer . parseInt ( matcher . group ( 1 ) ) ; } catch ( NumberFormatException ex ) { LOGGER . debug ( "Can't parse the matched number." , ex ) ; } }
@ Test public void testLanguageJsonSchema ( ) throws Exception { CamelContext context = new DefaultCamelContext ( ) ; String json = context . getLanguageParameterJsonSchema ( "groovy" ) ; assertNotNull ( "Should have found some auto-generated JSON" , json ) ; log . info ( json ) ; assertTrue ( json . contains ( "\"name\": \"groovy\"" ) ) ; assertTrue ( json . contains ( "\"modelName\": \"groovy\"" ) ) ; }
public void test() { try { Map < String , Object > changes = new HashMap < > ( ) ; changes . put ( "encodingType" , "SQUARE" ) ; changes . put ( "feature" , new JSONObject ( "{ \"type\": \"Point\", \"coordinates\": [-114.05, 51.05] }" ) ) ; changes . put ( "name" , "POIUYTREW" ) ; changes . put ( "description" , "POIUYTREW" ) ; return changes ; } catch ( JSONException ex ) { LOGGER . error ( "Exception:" , ex ) ; Assert . fail ( "Generating FeatureOfInterest changes failed: " + ex . getMessage ( ) ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( JournalArticleServiceUtil . class , "getGroupArticlesCount" , _getGroupArticlesCountParameterTypes43 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , userId , rootFolderId , status ) ; Object returnObj = null ; code_block = TryStatement ;  return ( ( Integer ) returnObj ) . intValue ( ) ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { LOG . debug ( String . format ( "Preparing to start process %s" , commandLine . toString ( ) ) ) ; printResult = executeProgram ( commandLine , workingDirectory , printJobTimeout , executeInBackground , successExitValue , streamHandler ) ; LOG . debug ( String . format ( "Successfully start process %s" , commandLine . toString ( ) ) ) ; } catch ( Exception ex ) { LOG . trace ( String . format ( "Problem during starting process %s" , commandLine . toString ( ) ) , ex ) ; ex . printStackTrace ( ) ; return false ; } }
public static boolean executeProgram ( CommandLine commandLine , String workingDirectory , boolean executeInBackground , int successExitValue , OutputStream outputStream ) { long printJobTimeout = PRINT_JOB_TIMEOUT ; ExecuteStreamHandler streamHandler = null ; code_block = IfStatement ; PrintResultHandler printResult = null ; code_block = TryStatement ;  LOG . debug ( String . format ( "Waiting for the proces %s finish" , commandLine . toString ( ) ) ) ; code_block = TryStatement ;  LOG . debug ( String . format ( "Process %s has finished" , commandLine . toString ( ) ) ) ; return true ; }
public void test() { try { code_block = IfStatement ; printResult . waitFor ( ) ; } catch ( InterruptedException ex ) { LOG . error ( String . format ( "Problem during process execution %s" , commandLine . toString ( ) ) , ex ) ; } }
public void test() { try { producer = initTransactionalProducer ( transaction . transactionalId , false ) ; producer . resumeTransaction ( transaction . producerId , transaction . epoch ) ; producer . commitTransaction ( ) ; } catch ( InvalidTxnStateException | ProducerFencedException ex ) { LOG . warn ( "Encountered error {} while recovering transaction {}. " + "Presumably this transaction has been already committed before" , ex , transaction ) ; } finally { code_block = IfStatement ; } }
public void test() { switch ( role ) { case VARIABLE_NAME : return DISTINGUISH_VARIABLES ; case FIELD_NAME : return DISTINGUISH_VARIABLES ; case FUNCTION_NAME : return DISTINGUISH_FUNCTIONS ; default : LOG . error ( "Unknown role " + role ) ; return true ; } }
public void test() { if ( ! dir . mkdirs ( ) ) { log . warn ( "Directory {} for 2nd level cache could not be created." , location ) ; } }
@ AdapterDelegationEvent ( beforeBuilder = PRPAIN201305UV02EventDescriptionBuilder . class , afterReturningBuilder = PRPAIN201306UV02EventDescriptionBuilder . class , serviceType = "Patient Discovery" , version = "LEVEL_a0" ) @ Override public PRPAIN201306UV02 respondingGatewayPRPAIN201305UV02 ( PRPAIN201305UV02 body , AssertionType assertion ) { LOG . debug ( "Entering AdapterPatientDiscoveryProxyNoOpImpl.respondingGatewayPRPAIN201305UV02" ) ; return new PRPAIN201306UV02 ( ) ; }
public void test() { if ( LOG . isInfoEnabled ( ) ) { LOG . info ( "exchangeVertexPartitions: Nothing to exchange, " + "exiting early" ) ; } }
public void test() { if ( workerIdSet . isEmpty ( ) ) { break ; } }
public void test() { if ( LOG . isInfoEnabled ( ) ) { LOG . info ( "exchangeVertexPartitions: Done with exchange." ) ; } }
@ Test public void testTimePositionSubset ( ) { String xml = "<wfs:GetFeature " + "service=\"WFS\" " + "version=\"1.1.0\" " + "outputFormat=\"gml32\" " + "xmlns:cdf=\"http://www.opengis.net/cite/data\" " + "xmlns:ogc=\"http://www.opengis.net/ogc\" " + "xmlns:wfs=\"http://www.opengis.net/wfs\" " + "xmlns:gml=\"http://www.opengis.net/gml/3.2\" " + "xmlns:csml=\"" + TimeSeriesMockData . CSML_URI + "\" " + "xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" " + "xsi:schemaLocation=\"" + "http://www.opengis.net/wfs http://schemas.opengis.net/wfs/1.1.0/wfs.xsd" + "\"" + ">" + "<wfs:Query typeName=\"csml:PointSeriesFeature\">" + "    <ogc:Filter>" + "        <ogc:PropertyIsBetween>" + "             <ogc:PropertyName>csml:PointSeriesFeature/csml:value/csml:PointSeriesCoverage/csml:pointSeriesDomain/csml:TimeSeries/csml:timePositionList</ogc:PropertyName>" + "             <ogc:LowerBoundary><ogc:Literal>1949-05-01</ogc:Literal></ogc:LowerBoundary>" + "             <ogc:UpperBoundary><ogc:Literal>1949-09-01</ogc:Literal></ogc:UpperBoundary>" + "        </ogc:PropertyIsBetween>" + "    </ogc:Filter>" + "</wfs:Query> " + "</wfs:GetFeature>" ; validate ( xml ) ; Document doc = postAsDOM ( "wfs" , xml ) ; LOGGER . info ( "WFS filter GetFeature response:\n" + prettyString ( doc ) ) ; assertEquals ( "wfs:FeatureCollection" , doc . getDocumentElement ( ) . getNodeName ( ) ) ; assertXpathEvaluatesTo ( "1" , "/wfs:FeatureCollection/@numberReturned" , doc ) ; assertXpathCount ( 1 , "//csml:PointSeriesFeature" , doc ) ; checkPointFeatureTwo ( doc ) ; assertXpathEvaluatesTo ( "1949-05-01 1949-06-01 1949-07-01 1949-08-01 1949-09-01" , "//csml:PointSeriesFeature[@gml:id='" + "ID2" + "']/csml:value/csml:PointSeriesCoverage/csml:pointSeriesDomain/csml:TimeSeries/csml:timePositionList" , doc ) ; assertXpathEvaluatesTo ( "16.2 17.1 22.0 25.1 23.9" , "//csml:PointSeriesFeature[@gml:id='" + "ID2" + "']/csml:value/csml:PointSeriesCoverage/gml:rangeSet/gml:ValueArray/gml:valueComponent/gml:QuantityList" , doc ) ; }
public void test() { for ( ActiveQuerySnapshot q : sublist ) { log . debug ( q . toString ( ) ) ; } }
public boolean cancel ( final AccountDTO accountDTO ) { LOGGER . info ( "============æ§è¡cancel ä»æ¬¾æ¥å£===============" ) ; return accountMapper . cancel ( accountDTO ) > 0 ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Ignoring put(). New entry is not from a newer reader. " + "existing: " + e . creationTick + ", new: " + reader . getCreationTick ( ) ) ; } }
public void test() { if ( Validator . isNotNull ( release . getSchemaVersion ( ) ) ) { Version version = new Version ( release . getSchemaVersion ( ) ) ; properties . put ( "release.schema.version" , version ) ; } }
public String getProcessInstanceVariable ( String containerId , Number processInstanceId , String varName , String marshallingType ) { containerId = context . getContainerId ( containerId , new ByProcessInstanceIdContainerLocator ( processInstanceId . longValue ( ) ) ) ; Object variable = processService . getProcessInstanceVariable ( containerId , processInstanceId . longValue ( ) , varName ) ; code_block = IfStatement ; logger . debug ( "About to marshal process variable with name '{}' {}" , varName , variable ) ; String response = marshallerHelper . marshal ( containerId , marshallingType , variable ) ; return response ; }
@ PostMapping public ContextDto create ( @ Valid @ RequestBody ContextDto contextDto ) { LOGGER . debug ( "create context={}" , contextDto ) ; final VitamContext vitamContext = securityService . buildVitamContext ( securityService . getTenantIdentifier ( ) ) ; return contextInternalService . create ( vitamContext , contextDto ) ; }
public void test() { if ( ! sensorState . equals ( DeviceTypeConstants . STATE_ON ) && ! sensorState . equals ( DeviceTypeConstants . STATE_OFF ) ) { log . error ( "The requested state change should be either - 'ON' or 'OFF'" ) ; return Response . status ( Response . Status . BAD_REQUEST . getStatusCode ( ) ) . build ( ) ; } }
public void test() { try { code_block = IfStatement ; String sensorState = state . toUpperCase ( ) ; code_block = IfStatement ; Map < String , String > dynamicProperties = new HashMap < > ( ) ; String publishTopic = APIUtil . getAuthenticatedUserTenantDomain ( ) + "/" + DeviceTypeConstants . DEVICE_TYPE + "/" + deviceId + "/command" ; dynamicProperties . put ( DeviceTypeConstants . ADAPTER_TOPIC_PROPERTY , publishTopic ) ; APIUtil . getOutputEventAdapterService ( ) . publish ( DeviceTypeConstants . MQTT_ADAPTER_NAME , dynamicProperties , state ) ; return Response . ok ( ) . build ( ) ; } catch ( DeviceAccessAuthorizationException e ) { log . error ( e . getErrorMessage ( ) , e ) ; return Response . status ( Response . Status . INTERNAL_SERVER_ERROR ) . build ( ) ; } }
public void test() { try { String rootKeyHex = CryptoUtils . extractKeyFromBootstrapFile ( ) ; return new SecretKeySpec ( Hex . decodeHex ( rootKeyHex . toCharArray ( ) ) , "AES" ) ; } catch ( IOException | DecoderException e ) { logger . error ( "Encountered an error: " , e ) ; throw new KeyManagementException ( e ) ; } }
public void test() { if ( ! running . compareAndSet ( false , true ) ) { logger . info ( "the java mongo replica already start" ) ; return ; } }
public void test() { try { this . mongoClient = replicaSetsContext . createMongoClient ( replicaSetConfig ) ; this . checkReplicaMongo ( ) ; executorService . submit ( new ReplicatorTask ( this , mongoClient , replicaSetConfig , replicaSetsContext ) ) ; } catch ( Exception e ) { logger . error ( "start replicator:{} error" , replicaSetConfig , e ) ; shutdown ( ) ; } }
public void test() { if ( ! autoStartScheduler ) { LOG . info ( "Not starting scheduler because autoStartScheduler is set to false." ) ; } else { code_block = IfStatement ; } }
public void test() { if ( scheduler . isStarted ( ) ) { LOG . warn ( "The scheduler has already started. Cannot apply the 'startDelayedSeconds' configuration!" ) ; } else { LOG . info ( "Starting scheduler with startDelayedSeconds={}" , startDelayedSeconds ) ; scheduler . startDelayed ( startDelayedSeconds ) ; } }
public void test() { if ( scheduler . isStarted ( ) ) { LOG . info ( "The scheduler has already been started." ) ; } else { LOG . info ( "Starting scheduler." ) ; scheduler . start ( ) ; } }
public void test() { if ( scheduler . isStarted ( ) ) { LOG . info ( "The scheduler has already been started." ) ; } else { LOG . info ( "Starting scheduler." ) ; scheduler . start ( ) ; } }
public void run ( ) { log . info ( "Started event send for read" ) ; code_block = TryStatement ;  log . info ( "Completed event send for read" ) ; }
public void test() { try { code_block = WhileStatement ; } catch ( RuntimeException ex ) { log . error ( "Exception encountered: " + ex . getMessage ( ) , ex ) ; exception = ex ; } }
public void run ( ) { log . info ( "Started event send for read" ) ; code_block = TryStatement ;  log . info ( "Completed event send for read" ) ; }
@ Override public Response getLatestBundle ( ) { _log . info ( "Starting getLatestBundle." ) ; Bundle latestBundle = getLatestBundleAsBundle ( ) ; code_block = IfStatement ; return Response . ok ( "Error: No bundles deployed." ) . build ( ) ; }
public void test() { try { JSONObject response = new JSONObject ( ) ; response . put ( "id" , latestBundle . getId ( ) ) ; response . put ( "dataset" , latestBundle . getDataset ( ) ) ; response . put ( "name" , latestBundle . getName ( ) ) ; return Response . ok ( response . toString ( ) ) . build ( ) ; } catch ( Exception e ) { _log . error ( "Error reading latest bundle: " + e ) ; } }
public void test() { try { dataIn . reset ( ) ; } catch ( IOException e ) { logger . debug ( "Caught IOException at dataIn.reset(): {}" , e . getMessage ( ) ) ; } }
public void test() { try { SerialPortIdentifier portIdentifier = serialPortManager . getIdentifier ( serialPortName ) ; code_block = IfStatement ; SerialPort commPort = portIdentifier . open ( this . getClass ( ) . getName ( ) , 2000 ) ; commPort . setSerialPortParams ( 19200 , SerialPort . DATABITS_8 , SerialPort . STOPBITS_1 , SerialPort . PARITY_NONE ) ; commPort . enableReceiveThreshold ( 1 ) ; commPort . enableReceiveTimeout ( 100 ) ; commPort . setFlowControlMode ( SerialPort . FLOWCONTROL_NONE ) ; InputStream dataIn = commPort . getInputStream ( ) ; OutputStream dataOut = commPort . getOutputStream ( ) ; code_block = IfStatement ; code_block = IfStatement ; Thread thread = new KaleidescapeReaderThread ( this , this . uid , this . serialPortName ) ; setReaderThread ( thread ) ; thread . start ( ) ; this . serialPort = commPort ; this . dataIn = dataIn ; this . dataOut = dataOut ; setConnected ( true ) ; logger . debug ( "Serial connection opened" ) ; } catch ( PortInUseException e ) { setConnected ( false ) ; throw new KaleidescapeException ( "Opening serial connection failed: Port in Use Exception" , e ) ; } catch ( UnsupportedCommOperationException e ) { setConnected ( false ) ; throw new KaleidescapeException ( "Opening serial connection failed: Unsupported Comm Operation Exception" , e ) ; } catch ( UnsupportedEncodingException e ) { setConnected ( false ) ; throw new KaleidescapeException ( "Opening serial connection failed: Unsupported Encoding Exception" , e ) ; } catch ( IOException e ) { setConnected ( false ) ; throw new KaleidescapeException ( "Opening serial connection failed: IO Exception" , e ) ; } }
@ VisibleForTesting void logFailure ( AttachmentJobStatusResponse response ) throws InternalServerErrorException { CloudStackErrorResponse errorResponse = response . getErrorResponse ( ) ; String errorText = String . format ( FAILED_ATTACH_ERROR_MESSAGE , errorResponse . getErrorCode ( ) , errorResponse . getErrorText ( ) ) ; LOGGER . error ( String . format ( Messages . Log . ERROR_WHILE_ATTACHING_VOLUME_GENERAL_S , errorText ) ) ; }
public void test() { if ( _startTime == null ) { LOG . error ( "All schedules must have a start time!" ) ; return false ; } }
public void test() { if ( ( _recurUnit == null && _recurInterval != null ) || ( _recurUnit != null && _recurInterval == null ) ) { LOG . error ( "Recurrence interval and unit must either both be present or both be absent" ) ; return false ; } }
public void test() { if ( _recurInterval != null && _recurInterval <= 0 ) { LOG . error ( "Recurrence interval must be positive" ) ; return false ; } }
public void test() { if ( converted < MIN_RECURRENCE_MILLIS ) { LOG . error ( "Recurrence must be at least {} ms" , MIN_RECURRENCE_MILLIS ) ; return false ; } }
public void test() { try { Document doc = createDocument ( docketData , true ) ; XMLOutputter outp = new XMLOutputter ( ) ; outp . setFormat ( Format . getPrettyFormat ( ) ) ; outp . output ( doc , os ) ; os . close ( ) ; } catch ( RuntimeException e ) { logger . error ( "Document creation failed." ) ; throw new IOException ( e ) ; } }
public void test() { try ( MetaDataClient metaDataClient = metaDataClientFactory . getClient ( ) ) { SanityChecker . checkJsonAll ( jsonQuery ) ; LOGGER . debug ( "{}" , jsonNode ) ; final RequestParserMultiple parser = RequestParserHelper . getParser ( jsonQuery . deepCopy ( ) ) ; parser . getRequest ( ) . reset ( ) ; code_block = IfStatement ; jsonNode = metaDataClient . selectObjectGroups ( jsonQuery ) ; LOGGER . debug ( "DEBUG {}" , jsonNode ) ; } catch ( final InvalidParseOperationException e ) { LOGGER . error ( PARSING_ERROR , e ) ; throw e ; } catch ( final IllegalArgumentException e ) { LOGGER . error ( ILLEGAL_ARGUMENT , e ) ; throw e ; } catch ( final Exception e ) { LOGGER . error ( "exeption thrown" , e ) ; throw new AccessInternalExecutionException ( e ) ; } }
public void test() { try ( MetaDataClient metaDataClient = metaDataClientFactory . getClient ( ) ) { SanityChecker . checkJsonAll ( jsonQuery ) ; LOGGER . debug ( "{}" , jsonNode ) ; final RequestParserMultiple parser = RequestParserHelper . getParser ( jsonQuery . deepCopy ( ) ) ; parser . getRequest ( ) . reset ( ) ; code_block = IfStatement ; jsonNode = metaDataClient . selectObjectGroups ( jsonQuery ) ; LOGGER . debug ( "DEBUG {}" , jsonNode ) ; } catch ( final InvalidParseOperationException e ) { LOGGER . error ( PARSING_ERROR , e ) ; throw e ; } catch ( final IllegalArgumentException e ) { LOGGER . error ( ILLEGAL_ARGUMENT , e ) ; throw e ; } catch ( final Exception e ) { LOGGER . error ( "exeption thrown" , e ) ; throw new AccessInternalExecutionException ( e ) ; } }
public void test() { try ( MetaDataClient metaDataClient = metaDataClientFactory . getClient ( ) ) { SanityChecker . checkJsonAll ( jsonQuery ) ; LOGGER . debug ( "{}" , jsonNode ) ; final RequestParserMultiple parser = RequestParserHelper . getParser ( jsonQuery . deepCopy ( ) ) ; parser . getRequest ( ) . reset ( ) ; code_block = IfStatement ; jsonNode = metaDataClient . selectObjectGroups ( jsonQuery ) ; LOGGER . debug ( "DEBUG {}" , jsonNode ) ; } catch ( final InvalidParseOperationException e ) { LOGGER . error ( PARSING_ERROR , e ) ; throw e ; } catch ( final IllegalArgumentException e ) { LOGGER . error ( ILLEGAL_ARGUMENT , e ) ; throw e ; } catch ( final Exception e ) { LOGGER . error ( "exeption thrown" , e ) ; throw new AccessInternalExecutionException ( e ) ; } }
public void test() { try ( MetaDataClient metaDataClient = metaDataClientFactory . getClient ( ) ) { SanityChecker . checkJsonAll ( jsonQuery ) ; LOGGER . debug ( "{}" , jsonNode ) ; final RequestParserMultiple parser = RequestParserHelper . getParser ( jsonQuery . deepCopy ( ) ) ; parser . getRequest ( ) . reset ( ) ; code_block = IfStatement ; jsonNode = metaDataClient . selectObjectGroups ( jsonQuery ) ; LOGGER . debug ( "DEBUG {}" , jsonNode ) ; } catch ( final InvalidParseOperationException e ) { LOGGER . error ( PARSING_ERROR , e ) ; throw e ; } catch ( final IllegalArgumentException e ) { LOGGER . error ( ILLEGAL_ARGUMENT , e ) ; throw e ; } catch ( final Exception e ) { LOGGER . error ( "exeption thrown" , e ) ; throw new AccessInternalExecutionException ( e ) ; } }
public void test() { try ( MetaDataClient metaDataClient = metaDataClientFactory . getClient ( ) ) { SanityChecker . checkJsonAll ( jsonQuery ) ; LOGGER . debug ( "{}" , jsonNode ) ; final RequestParserMultiple parser = RequestParserHelper . getParser ( jsonQuery . deepCopy ( ) ) ; parser . getRequest ( ) . reset ( ) ; code_block = IfStatement ; jsonNode = metaDataClient . selectObjectGroups ( jsonQuery ) ; LOGGER . debug ( "DEBUG {}" , jsonNode ) ; } catch ( final InvalidParseOperationException e ) { LOGGER . error ( PARSING_ERROR , e ) ; throw e ; } catch ( final IllegalArgumentException e ) { LOGGER . error ( ILLEGAL_ARGUMENT , e ) ; throw e ; } catch ( final Exception e ) { LOGGER . error ( "exeption thrown" , e ) ; throw new AccessInternalExecutionException ( e ) ; } }
public int read ( final String context ) throws IOException { int b = read ( ) ; log . trace ( "Read {}  byte, val is {}" , context , ByteUtils . formatByte ( b ) ) ; return b ; }
public void test() { try { dnsResponse = dnsClient . reverseLookup ( key . toString ( ) ) ; } catch ( Exception e ) { LOG . error ( "Could not perform reverse DNS lookup for [{}]. Cause [{}]" , key , ExceptionUtils . getRootCauseOrMessage ( e ) ) ; errorCounter . inc ( ) ; return getErrorResult ( ) ; } }
public void test() { try { return Integer . parseInt ( sPoolSize ) ; } catch ( NumberFormatException exc ) { log . error ( "Invalid setting to " + prop + " value=" + sPoolSize + " using defaults." ) ; } }
@ Override public void rip ( ) throws IOException { logger . info ( "Retrieving " + this . url ) ; Document doc = Http . url ( url ) . get ( ) ; List < String > mp4s = Utils . between ( doc . html ( ) , "file:\"" , "\"" ) ; code_block = IfStatement ; String vidUrl = mp4s . get ( 0 ) ; addURLToDownload ( new URL ( vidUrl ) , HOST + "_" + getGID ( this . url ) ) ; waitForThreads ( ) ; }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "Creating a new custom container builder" ) ; } }
@ Override public void init ( ) throws Exception { this . getCacheWrapper ( ) . initCache ( this . getWidgetTypeDAO ( ) ) ; logger . debug ( "{} ready. Initialized" , this . getClass ( ) . getName ( ) ) ; }
@ UserAggregationUpdate public void logAround ( ) { log . debug ( "1" ) ; log . info ( "2" ) ; log . warn ( "3" ) ; log . error ( "4" ) ; }
@ UserAggregationUpdate public void logAround ( ) { log . debug ( "1" ) ; log . info ( "2" ) ; log . warn ( "3" ) ; log . error ( "4" ) ; }
@ UserAggregationUpdate public void logAround ( ) { log . debug ( "1" ) ; log . info ( "2" ) ; log . warn ( "3" ) ; log . error ( "4" ) ; }
@ UserAggregationUpdate public void logAround ( ) { log . debug ( "1" ) ; log . info ( "2" ) ; log . warn ( "3" ) ; log . error ( "4" ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Cancelling query {}" , QueryIdHelper . getQueryId ( id ) ) ; } }
protected void initParams ( final Request request ) { log . trace ( "Entered init" ) ; uuid = ( String ) request . getAttributes ( ) . get ( "uuid" ) ; log . trace ( "Exit init" ) ; }
protected void initParams ( final Request request ) { log . trace ( "Entered init" ) ; uuid = ( String ) request . getAttributes ( ) . get ( "uuid" ) ; log . trace ( "Exit init" ) ; }
public void test() { if ( deref == null ) { LOGGER . warn ( "Node instance with url " + ref . getUrl ( ) + " not found" ) ; } else { transformNode ( deref , matrix , results ) ; } }
public ResponseEntity < ResultsDto > findObjectById ( String id , ExternalHttpContext context ) { LOGGER . info ( "Get the Object Group with Identifier {}" , id ) ; return archiveSearchExternalRestClient . findObjectById ( id , context ) ; }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "==> HiveHook.initialize()" ) ; } }
public void test() { try { atlasPluginClassLoader = AtlasPluginClassLoader . getInstance ( ATLAS_PLUGIN_TYPE , this . getClass ( ) ) ; @ SuppressWarnings ( "unchecked" ) Class < ExecuteWithHookContext > cls = ( Class < ExecuteWithHookContext > ) Class . forName ( ATLAS_HIVE_HOOK_IMPL_CLASSNAME , true , atlasPluginClassLoader ) ; activatePluginClassLoader ( ) ; hiveHookImpl = cls . newInstance ( ) ; } catch ( Exception excp ) { LOG . error ( "Error instantiating Atlas hook implementation" , excp ) ; } finally { deactivatePluginClassLoader ( ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "<== HiveHook.initialize()" ) ; } }
public void test() { if ( ( value == null && oldValue == null ) || ( value != null && value . equals ( oldValue ) ) ) { logger . trace ( "Value '{}' for {} hasn't changed, ignoring update" , value , variable ) ; return ; } }
public void test() { try { Response < JsonElement > response = execJavaMethod ( transaction . getSession ( ) , object , m , transaction , request ) ; code_block = IfStatement ; } catch ( InvocationTargetException e ) { code_block = IfStatement ; } catch ( Exception e ) { log . error ( "Exception processing request " + request , e ) ; transaction . sendError ( e ) ; } }
public void test() { if ( published ) { LOG . info ( "Published [%d] segments" , newSegments . size ( ) ) ; } else { throw new ISE ( "Failed to publish segments" ) ; } }
public void test() { if ( getCreated ( ) != null && getCreated ( ) . after ( invocationDate ) ) { LOG . info ( "Validation of Timestamp: The message was created in the future!" ) ; return false ; } else { return true ; } }
public void test() { if ( alertConfigSpec == null ) { LOG . info ( "Found scheduled, but not in database {}" , configId ) ; stopJob ( scheduledJobKey ) ; } }
@ Override public void start ( Collection < ? extends Location > locs ) { LOG . trace ( "Starting {}" , this ) ; callHistory . add ( "start" ) ; ServiceStateLogic . setExpectedState ( this , Lifecycle . STARTING ) ; counter . incrementAndGet ( ) ; addLocations ( locs ) ; sensors ( ) . set ( SERVICE_UP , true ) ; ServiceStateLogic . setExpectedState ( this , Lifecycle . RUNNING ) ; }
public void test() { try { transport . disconnect ( true ) ; } catch ( IOException e ) { log . warn ( "Failed to disconnect transport" , e ) ; se . addSuppressed ( e ) ; } }
public void test() { try { getPage ( i ) . subscribeEvents ( "playing" ) ; getPage ( i ) . initWebRtc ( webRtcReceiver [ i - 1 ] , WebRtcChannel . AUDIO_AND_VIDEO , WebRtcMode . RCV_ONLY ) ; Assert . assertTrue ( "Not received media in receiver " + i , getPage ( i ) . waitForEvent ( "playing" ) ) ; recorder [ i - 1 ] . record ( ) ; Thread . sleep ( PLAYTIME_MS ) ; recorder [ i - 1 ] . stopAndWait ( ) ; Thread . sleep ( 4000 ) ; } catch ( InterruptedException e ) { log . error ( "InterruptedException in receiver " + i , e ) ; } }
public void test() { if ( ! service . getClusterVersion ( ) . equals ( request . getClusterVersion ( ) ) ) { logger . info ( "Join check from " + getCallerAddress ( ) + " failed validation due to incompatible version," + "remote cluster version is " + request . getClusterVersion ( ) + ", this cluster is " + service . getClusterVersion ( ) ) ; return false ; } }
public void test() { if ( validFields . size ( ) != 1 ) { log . error ( "[Sentinel Starter] DataSource " + dataSourceName + " multi datasource active and won't loaded: " + dataSourceProperties . getValidField ( ) ) ; return ; } }
public void test() { try { List < String > validFields = dataSourceProperties . getValidField ( ) ; code_block = IfStatement ; AbstractDataSourceProperties abstractDataSourceProperties = dataSourceProperties . getValidDataSourceProperties ( ) ; abstractDataSourceProperties . setEnv ( env ) ; abstractDataSourceProperties . preCheck ( dataSourceName ) ; registerBean ( abstractDataSourceProperties , dataSourceName + "-sentinel-" + validFields . get ( 0 ) + "-datasource" ) ; } catch ( Exception e ) { log . error ( "[Sentinel Starter] DataSource " + dataSourceName + " build error: " + e . getMessage ( ) , e ) ; } }
public void test() { while ( true ) { Query query = new QueryImpl ( filter , 1 , BATCH_SIZE , SortBy . NATURAL_ORDER , false , TimeUnit . SECONDS . toMillis ( 90 ) ) ; QueryRequest queryRequest = new QueryRequestImpl ( query ) ; LOGGER . trace ( "Removing existing geonames data with filter: {}" , filter ) ; QueryResponse response = catalogFramework . query ( queryRequest ) ; List < Serializable > metacardsToDelete = response . getResults ( ) . stream ( ) . map ( Result :: getMetacard ) . map ( Metacard :: getId ) . collect ( Collectors . toList ( ) ) ; code_block = IfStatement ; LOGGER . trace ( "Deleting {} GeoNames metacards" , metacardsToDelete . size ( ) ) ; removeMetacards ( catalogProvider , extractionCallback , metacardsToDelete ) ; } }
public void test() { if ( hasDiff ) { LOG . error ( myDiff . toString ( ) ) ; } }
@ Test public void testDeleted ( ) { SentinelHello normalHello = new SentinelHello ( sentinels . iterator ( ) . next ( ) , masterAddr , sentinelMonitorName ) ; HostPort remoteDcMAster = new HostPort ( "127.0.0.1" , 7379 ) ; Mockito . when ( metaCache . getDc ( remoteDcMAster ) ) . thenReturn ( "remote-dc" ) ; Set < SentinelHello > hellos = new HashSet < > ( ) ; hellos . add ( normalHello ) ; hellos . add ( new SentinelHello ( new HostPort ( "11.0.0.1" , 5000 ) , masterAddr , sentinelMonitorName ) ) ; hellos . add ( new SentinelHello ( sentinels . iterator ( ) . next ( ) , remoteDcMAster , sentinelMonitorName ) ) ; hellos . add ( new SentinelHello ( sentinels . iterator ( ) . next ( ) , masterAddr , "error-monitor-name" ) ) ; Set < SentinelHello > needDeletedHello = collector . checkAndDelete ( sentinelMonitorName , sentinels , hellos , quorumConfig , masterAddr ) ; logger . info ( "[testDeleted] {}" , needDeletedHello ) ; Assert . assertEquals ( 3 , needDeletedHello . size ( ) ) ; Assert . assertFalse ( needDeletedHello . contains ( normalHello ) ) ; Assert . assertEquals ( 1 , hellos . size ( ) ) ; Assert . assertTrue ( hellos . contains ( normalHello ) ) ; }
public void test() { for ( CacheConfiguration < Object , Object > ccfg : cacheCfgs ) { ccfg . setCacheStoreFactory ( new TestStoreFactory ( ) ) ; ccfg . setReadThrough ( false ) ; boolean near = ( ccfg . getNearConfiguration ( ) != null ) ; log . info ( "Test cache [mode=" + ccfg . getCacheMode ( ) + ", atomicity=" + ccfg . getAtomicityMode ( ) + ", backups=" + ccfg . getBackups ( ) + ", near=" + near + "]" ) ; ignite ( 0 ) . createCache ( ccfg ) ; awaitPartitionMapExchange ( ) ; code_block = TryStatement ;  } }
@ Before public void setup ( ) throws Exception { LOGGER . info ( "TestJDBCInterface setup" ) ; policyFile = super . setupPolicy ( ) ; super . setup ( ) ; }
public void test() { if ( imageURL == null ) { logger . error ( "Resource not found: " + path ) ; return null ; } else { return ( new ImageIcon ( imageURL , description ) ) . getImage ( ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception exc ) { LOG . info ( "Failed to resolve the network location" , exc ) ; } }
private Application create ( AssemblyTemplate template , CampPlatform platform ) { ManagementContext mgmt = getManagementContext ( platform ) ; BrooklynClassLoadingContext loader = JavaBrooklynClassLoadingContext . create ( mgmt ) ; EntitySpec < ? extends Application > spec = createApplicationSpec ( template , platform , loader , MutableSet . < String > of ( ) ) ; Application instance = mgmt . getEntityManager ( ) . createEntity ( spec ) ; log . info ( "CAMP created '{}'" , instance ) ; return instance ; }
public void test() { if ( assignmentErrorCode . get ( ) == AssignorError . SHUTDOWN_REQUESTED . code ( ) ) { log . warn ( "Detected that shutdown was requested. " + "All clients in this app will now begin to shutdown" ) ; mainConsumer . enforceRebalance ( ) ; } }
public void test() { if ( reason != null ) { reason . addSuppressed ( e ) ; } else { LOG . error ( SshdText . get ( ) . sessionCloseFailed , e ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( "Unable to get file entry " + fileEntryId , portalException ) ; } }
public void test() { if ( getParent ( ) == null ) { LOGGER . trace ( "Parent null, skipping virtual items" ) ; return ; } }
public void test() { try { ItemPath virtualItemPath = getVirtualItemPath ( virtualItem ) ; ItemWrapper itemWrapper = objectWrapper . findItem ( virtualItemPath , ItemWrapper . class ) ; code_block = IfStatement ; code_block = IfStatement ; nonContainers . add ( itemWrapper ) ; } catch ( SchemaException e ) { LOGGER . error ( "Cannot find wrapper with path {}, error occurred {}" , virtualItem , e . getMessage ( ) , e ) ; } }
@ Override public void initializeUI ( UIBuilder builder ) throws Exception { StopWatch watch = new StopWatch ( ) ; final UIContext context = builder . getUIContext ( ) ; letsChat = ( LetsChatClient ) builder . getUIContext ( ) . getAttributeMap ( ) . get ( "letsChatClient" ) ; taigaClient = ( TaigaClient ) builder . getUIContext ( ) . getAttributeMap ( ) . get ( "taigaClient" ) ; ProjectConfig config = ( ProjectConfig ) context . getAttributeMap ( ) . get ( "projectConfig" ) ; code_block = IfStatement ; builder . add ( chatRoom ) ; builder . add ( issueProjectName ) ; builder . add ( codeReview ) ; LOG . info ( "initializeUI took " + watch . taken ( ) ) ; }
public List findByExample ( FilterResZob instance ) { log . debug ( "finding FilterResZob instance by example" ) ; code_block = TryStatement ;  }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.FilterResZob" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
public void test() { try { List results = sessionFactory . getCurrentSession ( ) . createCriteria ( "sernet.gs.reveng.FilterResZob" ) . add ( Example . create ( instance ) ) . list ( ) ; log . debug ( "find by example successful, result size: " + results . size ( ) ) ; return results ; } catch ( RuntimeException re ) { log . error ( "find by example failed" , re ) ; throw re ; } }
public void test() { catch ( IOException e ) { log . warn ( "Cannot find host ip" , e ) ; return DOCKER_FOR_LINUX_STATIC_IP ; } }
@ Test public void test_05 ( ) { Log . debug ( "Test" ) ; double pvals [ ] = code_block = "" ; ; ScoreList gpl = new ScoreList ( ) ; for ( double pval : pvals ) gpl . add ( pval ) ; double pvalue = gpl . score ( ScoreSummary . FISHER_CHI_SQUARE ) ; Assert . assertEquals ( 0.021561751324834642 , pvalue , EPSILON ) ; }
public void test() { if ( this . bluetoothAdapter != null ) { logger . info ( "Bluetooth adapter interface => {}" , this . name ) ; logger . info ( "Bluetooth adapter address => {}" , this . bluetoothAdapter . getAddress ( ) ) ; logger . info ( "Bluetooth adapter le enabled => {}" , this . bluetoothAdapter . isLeReady ( ) ) ; code_block = IfStatement ; configureBeacon ( ) ; } else { logger . warn ( "No Bluetooth adapter found ..." ) ; } }
public void test() { if ( this . bluetoothAdapter != null ) { logger . info ( "Bluetooth adapter interface => {}" , this . name ) ; logger . info ( "Bluetooth adapter address => {}" , this . bluetoothAdapter . getAddress ( ) ) ; logger . info ( "Bluetooth adapter le enabled => {}" , this . bluetoothAdapter . isLeReady ( ) ) ; code_block = IfStatement ; configureBeacon ( ) ; } else { logger . warn ( "No Bluetooth adapter found ..." ) ; } }
public void test() { if ( ! this . bluetoothAdapter . isEnabled ( ) ) { logger . info ( "Enabling bluetooth adapter..." ) ; this . bluetoothAdapter . enable ( ) ; logger . info ( "Bluetooth adapter address => {}" , this . bluetoothAdapter . getAddress ( ) ) ; } }
public void test() { if ( this . bluetoothAdapter != null ) { logger . info ( "Bluetooth adapter interface => {}" , this . name ) ; logger . info ( "Bluetooth adapter address => {}" , this . bluetoothAdapter . getAddress ( ) ) ; logger . info ( "Bluetooth adapter le enabled => {}" , this . bluetoothAdapter . isLeReady ( ) ) ; code_block = IfStatement ; configureBeacon ( ) ; } else { logger . warn ( "No Bluetooth adapter found ..." ) ; } }
protected void updated ( Map < String , Object > properties ) { doUpdate ( properties ) ; this . bluetoothAdapter . stopBeaconAdvertising ( ) ; this . bluetoothAdapter = null ; this . bluetoothAdapter = this . bluetoothService . getBluetoothAdapter ( this . name , this ) ; code_block = IfStatement ; logger . debug ( "Updating Beacon Example... Done." ) ; }
public void test() { try { Integer version = processDefinitionLogMapper . queryMaxVersionForDefinition ( processDefinition . getCode ( ) ) ; ProcessDefinitionLog processDefinitionLog = new ProcessDefinitionLog ( processDefinition ) ; processDefinitionLog . setVersion ( version == null || version == 0 ? 1 : version + 1 ) ; processDefinitionLog . setProjectCode ( targetProjectCode ) ; processDefinitionLog . setOperator ( loginUser . getId ( ) ) ; Date now = new Date ( ) ; processDefinitionLog . setOperateTime ( now ) ; processDefinitionLog . setUpdateTime ( now ) ; processDefinitionLog . setCreateTime ( now ) ; int update = processDefinitionMapper . updateById ( processDefinitionLog ) ; int insertLog = processDefinitionLogMapper . insert ( processDefinitionLog ) ; code_block = IfStatement ; return processDefinitionLog ; } catch ( Exception e ) { putMsg ( result , Status . UPDATE_PROCESS_DEFINITION_ERROR ) ; failedProcessList . add ( processDefinition . getId ( ) + "[" + processDefinition . getName ( ) + "]" ) ; logger . error ( "move processDefinition error: {}" , e . getMessage ( ) , e ) ; } }
private int runCommand ( boolean json ) throws InterruptedException , ExecutionException , IOException { final int ret = command . run ( options , client , out , json , null ) ; log . debug ( "Output from command: [{}]" , baos . toString ( ) . replaceAll ( "\n" , "\\\\n" ) ) ; return ret ; }
private BrooklynNode setUpBrooklynNodeWithApp ( ) throws InterruptedException , ExecutionException { BrooklynNode brooklynNode = app . createAndManageChild ( EntitySpec . create ( BrooklynNode . class ) . configure ( BrooklynNode . NO_WEB_CONSOLE_AUTHENTICATION , Boolean . TRUE ) ) ; app . start ( locs ) ; log . info ( "started " + app + " containing " + brooklynNode + " to " + JavaClassNames . niceClassAndMethod ( ) ) ; EntityAsserts . assertAttributeEqualsEventually ( brooklynNode , BrooklynNode . SERVICE_UP , true ) ; String baseUrl = brooklynNode . getAttribute ( BrooklynNode . WEB_CONSOLE_URI ) . toString ( ) ; waitForApps ( baseUrl ) ; final String id = brooklynNode . invoke ( BrooklynNode . DEPLOY_BLUEPRINT , ConfigBag . newInstance ( ) . configure ( DeployBlueprintEffector . BLUEPRINT_TYPE , BasicApplication . class . getName ( ) ) . getAllConfig ( ) ) . get ( ) ; String entityUrl = Urls . mergePaths ( baseUrl , "v1/applications/" , id , "entities" , id ) ; Entity mirror = brooklynNode . addChild ( EntitySpec . create ( BrooklynEntityMirror . class ) . configure ( BrooklynEntityMirror . MIRRORED_ENTITY_URL , entityUrl ) . configure ( BrooklynEntityMirror . MIRRORED_ENTITY_ID , id ) ) ; assertEquals ( brooklynNode . getChildren ( ) . size ( ) , 1 ) ; return brooklynNode ; }
public void test() { try { MethodKey methodKey = new MethodKey ( SocialActivityServiceUtil . class , "getUserGroupsAndOrganizationsActivitiesCount" , _getUserGroupsAndOrganizationsActivitiesCountParameterTypes28 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , userId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( ( Integer ) returnObj ) . intValue ( ) ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( exported ) { logger . warn ( StringMessageUtils . getBoilerPlate ( newArrayList ( "WARNING:" , " " , "Class: '" + name + "' is NOT exposed by the plugin but it will be visible " + "due to it was manually forced to be exported for testing purposes." , " " , "Check if this is really necessary, this class won't be visible in standalone mode." ) , '*' , DEFAULT_MESSAGE_WIDTH ) ) ; } }
public void test() { try { int [ ] domains = conn . listDomains ( ) ; s_logger . debug ( String . format ( "found %d domains" , domains . length ) ) ; code_block = ForStatement ; } catch ( LibvirtException e ) { s_logger . warn ( "[ignored] Error trying to cleanup " , e ) ; } }
public static void dumpToLog ( final AtlasGraph < ? , ? > graph ) { LOG . debug ( "*******************Graph Dump****************************" ) ; LOG . debug ( "Vertices of {}" , graph ) ; code_block = ForStatement ; LOG . debug ( "Edges of {}" , graph ) ; code_block = ForStatement ; LOG . debug ( "*******************Graph Dump****************************" ) ; }
public void test() { for ( AtlasVertex vertex : graph . getVertices ( ) ) { LOG . debug ( vertexString ( vertex ) ) ; } }
public static void dumpToLog ( final AtlasGraph < ? , ? > graph ) { LOG . debug ( "*******************Graph Dump****************************" ) ; LOG . debug ( "Vertices of {}" , graph ) ; code_block = ForStatement ; LOG . debug ( "Edges of {}" , graph ) ; code_block = ForStatement ; LOG . debug ( "*******************Graph Dump****************************" ) ; }
public void test() { for ( AtlasEdge edge : graph . getEdges ( ) ) { LOG . debug ( edgeString ( edge ) ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { Object in = object ; code_block = IfStatement ; LOG . debug ( "Received body: {}" , in ) ; } }
public void test() { if ( response != null ) { LOG . debug ( "Writing body: {}" , response ) ; MinaHelper . writeBody ( session , response , exchange , configuration . getWriteTimeout ( ) ) ; } else { LOG . debug ( "Writing no response" ) ; disconnect = Boolean . TRUE ; } }
public void test() { if ( response != null ) { LOG . debug ( "Writing body: {}" , response ) ; MinaHelper . writeBody ( session , response , exchange , configuration . getWriteTimeout ( ) ) ; } else { LOG . debug ( "Writing no response" ) ; disconnect = Boolean . TRUE ; } }
public void test() { try { resetSearchers ( ) ; code_block = ForStatement ; _indexManager . enqueue ( mutations ) ; } catch ( Exception e ) { LOG . error ( "Unknown error during processing of [mutations={0}]" , e , mutations ) ; code_block = IfStatement ; throw new BException ( e . getMessage ( ) , e ) ; } }
public void test() { try { return connectedUser . equals ( fromUser ) || emailIsAnAliasOfTheConnectedUser ( connectedUser , fromUser ) ; } catch ( RecipientRewriteTableException | RecipientRewriteTable . ErrorMappingException e ) { LOGGER . warn ( "Error upon {} mapping resolution for {}. You might want to audit mapping content for this mapping entry. " , fromUser . asString ( ) , connectedUser . asString ( ) ) ; return false ; } }
@ NwhinInvocationEvent ( beforeBuilder = DeferredResponseDescriptionBuilder . class , afterReturningBuilder = DeferredResponseDescriptionBuilder . class , serviceType = "Document Submission Deferred Response" , version = "1.1" ) @ Override public XDRAcknowledgementType provideAndRegisterDocumentSetBDeferredResponse11 ( RegistryResponseType request , AssertionType assertion , NhinTargetSystemType target ) { LOG . debug ( "Begin provideAndRegisterDocumentSetBDeferredResponse" ) ; XDRAcknowledgementType response = null ; code_block = TryStatement ;  LOG . debug ( "End provideAndRegisterDocumentSetBDeferredResponse" ) ; return response ; }
@ NwhinInvocationEvent ( beforeBuilder = DeferredResponseDescriptionBuilder . class , afterReturningBuilder = DeferredResponseDescriptionBuilder . class , serviceType = "Document Submission Deferred Response" , version = "1.1" ) @ Override public XDRAcknowledgementType provideAndRegisterDocumentSetBDeferredResponse11 ( RegistryResponseType request , AssertionType assertion , NhinTargetSystemType target ) { LOG . debug ( "Begin provideAndRegisterDocumentSetBDeferredResponse" ) ; XDRAcknowledgementType response = null ; code_block = TryStatement ;  LOG . debug ( "End provideAndRegisterDocumentSetBDeferredResponse" ) ; return response ; }
public void test() { try { s = c . prepareStatement ( "SELECT template_id, orderxml, isActive FROM ordertemplates WHERE name = ?" ) ; s . setString ( 1 , orderXmlName ) ; ResultSet res = s . executeQuery ( ) ; code_block = IfStatement ; Reader orderTemplateReader = null ; long template_id = res . getLong ( 1 ) ; code_block = IfStatement ; HeritrixTemplate heritrixTemplate = HeritrixTemplate . read ( template_id , orderTemplateReader ) ; heritrixTemplate . setIsActive ( res . getBoolean ( 3 ) ) ; return heritrixTemplate ; } catch ( SQLException e ) { final String message = "SQL error finding order.xml to " + orderXmlName + "\n" + ExceptionUtils . getSQLExceptionCause ( e ) ; log . warn ( message , e ) ; throw new IOFailure ( message , e ) ; } finally { DBUtils . closeStatementIfOpen ( s ) ; HarvestDBConnection . release ( c ) ; } }
public void test() { if ( checkName instanceof Text ) { return checkName . toString ( ) ; } else { LOGGER . warn ( MessageFormat . format ( this . getActionExecution ( ) . getAction ( ) . getType ( ) + " does not accept {0} as type for check name" , checkName . getClass ( ) ) ) ; return checkName . toString ( ) ; } }
public static void main ( final String [ ] args ) { final long start = System . currentTimeMillis ( ) ; LOG . info ( Arrays . toString ( args ) ) ; final String inputFilePath = args [ 0 ] ; final Integer numFeatures = Integer . parseInt ( args [ 1 ] ) ; final Integer numClasses = Integer . parseInt ( args [ 2 ] ) ; final Integer numItr = Integer . parseInt ( args [ 3 ] ) ; final List < Integer > initialModelKeys = new ArrayList < > ( numClasses ) ; code_block = ForStatement ; final PipelineOptions options = NemoPipelineOptionsFactory . create ( ) ; options . setJobName ( "MLR" ) ; options . setStableUniqueNames ( PipelineOptions . CheckEnabled . OFF ) ; final Pipeline p = Pipeline . create ( options ) ; PCollection < KV < Integer , List < Double > > > model = p . apply ( Create . of ( initialModelKeys ) ) . apply ( ParDo . of ( new DoFn < Integer , KV < Integer , List < Double > > > ( ) code_block = "" ; ) ) ; final PCollection < String > readInput = GenericSourceSink . read ( p , inputFilePath ) ; code_block = ForStatement ; p . run ( ) . waitUntilFinish ( ) ; LOG . info ( "JCT " + ( System . currentTimeMillis ( ) - start ) ) ; }
public static void main ( final String [ ] args ) { final long start = System . currentTimeMillis ( ) ; LOG . info ( Arrays . toString ( args ) ) ; final String inputFilePath = args [ 0 ] ; final Integer numFeatures = Integer . parseInt ( args [ 1 ] ) ; final Integer numClasses = Integer . parseInt ( args [ 2 ] ) ; final Integer numItr = Integer . parseInt ( args [ 3 ] ) ; final List < Integer > initialModelKeys = new ArrayList < > ( numClasses ) ; code_block = ForStatement ; final PipelineOptions options = NemoPipelineOptionsFactory . create ( ) ; options . setJobName ( "MLR" ) ; options . setStableUniqueNames ( PipelineOptions . CheckEnabled . OFF ) ; final Pipeline p = Pipeline . create ( options ) ; PCollection < KV < Integer , List < Double > > > model = p . apply ( Create . of ( initialModelKeys ) ) . apply ( ParDo . of ( new DoFn < Integer , KV < Integer , List < Double > > > ( ) code_block = "" ; ) ) ; final PCollection < String > readInput = GenericSourceSink . read ( p , inputFilePath ) ; code_block = ForStatement ; p . run ( ) . waitUntilFinish ( ) ; LOG . info ( "JCT " + ( System . currentTimeMillis ( ) - start ) ) ; }
public void test() { try { pulsar ( ) . getBrokerService ( ) . deleteTopic ( topicName . toString ( ) , false , deleteSchema ) . get ( ) ; log . info ( "[{}] Successfully removed topic {}" , clientAppId ( ) , topicName ) ; } catch ( Exception e ) { Throwable t = e . getCause ( ) ; log . error ( "[{}] Failed to delete topic {}" , clientAppId ( ) , topicName , t ) ; code_block = IfStatement ; } }
public void test() { if ( shouldLogAsWarning ( msg . getSeverity ( ) ) ) { log . warn ( "Global FacesMessage to user (wid: {}): {})" , windowContext . getCurrentWindowId ( ) , msg . getSummary ( ) ) ; } }
public void test() { for ( String s : result ) { logger . info ( s ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Error occurred while executing 'describe disk-store': {}!" , e . getMessage ( ) , e ) ; context . getResultSender ( ) . sendException ( e ) ; } }
@ Override public Object call ( ) { Thread . currentThread ( ) . setName ( THREAD_NAME ) ; log . trace ( "Amqp reactor thread {} has started" , THREAD_NAME ) ; code_block = TryStatement ;  log . trace ( "Amqp reactor thread {} has finished" , THREAD_NAME ) ; return null ; }
public void test() { try { amqpReactor . run ( ) ; } catch ( HandlerException e ) { log . error ( "Encountered an exception while running the AMQP reactor" , e ) ; throw e ; } }
public void test() { if ( notification . getDeviceToken ( ) != null && notification . getDeviceToken ( ) . length ( ) > 0 && appPropertiesDetails != null ) { authKey = appPropertiesDetails . getAndroidServerKey ( ) ; URL url = new URL ( ( String ) applicationPropertyConfiguration . getApiUrlFcm ( ) ) ; HttpURLConnection conn = ( HttpURLConnection ) url . openConnection ( ) ; conn . setUseCaches ( false ) ; conn . setDoInput ( true ) ; conn . setDoOutput ( true ) ; conn . setRequestMethod ( "POST" ) ; conn . setRequestProperty ( "Authorization" , "key=" + authKey ) ; conn . setRequestProperty ( "Content-Type" , "application/json" ) ; JSONObject json = new JSONObject ( ) ; json . put ( "registration_ids" , notification . getDeviceToken ( ) ) ; json . put ( "priority" , "high" ) ; JSONObject dataInfo = new JSONObject ( ) ; dataInfo . put ( "subtype" , notification . getNotificationSubType ( ) ) ; dataInfo . put ( "type" , notification . getNotificationType ( ) ) ; dataInfo . put ( "title" , notification . getNotificationTitle ( ) ) ; dataInfo . put ( "message" , notification . getNotificationText ( ) ) ; code_block = IfStatement ; json . put ( "data" , dataInfo ) ; OutputStreamWriter wr = new OutputStreamWriter ( conn . getOutputStream ( ) ) ; wr . write ( json . toString ( ) ) ; wr . flush ( ) ; String response = IOUtils . toString ( conn . getInputStream ( ) , StandardCharsets . UTF_8 ) ; JsonNode responseJson = new ObjectMapper ( ) . readTree ( response ) ; FcmPushNotificationResponse fcmNotificationResponse = new FcmPushNotificationResponse ( responseJson , conn . getResponseCode ( ) , conn . getResponseMessage ( ) ) ; logger . trace ( String . format ( "FCM Notification Response status=%d, response=%s" , conn . getResponseCode ( ) , response ) ) ; return fcmNotificationResponse ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "SEARCHING FOR NOTES WITH CRITERIA: " + criteria ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( DLAppServiceUtil . class , "unsubscribeFolder" , _unsubscribeFolderParameterTypes97 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId , folderId ) ; code_block = TryStatement ;  } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
@ Test public void testCreateAndRemoveElement ( ) throws Exception { uuidList = new LinkedList < String > ( ) ; Organization organization = createOrganization ( ) ; uuidList . add ( organization . getUuid ( ) ) ; checkOrganization ( organization ) ; uuidList . addAll ( createElementsInGroups ( organization , NUMBER_PER_GROUP ) ) ; LOG . info ( "Total number of created elements: " + uuidList . size ( ) ) ; RemoveElement < CnATreeElement > removeCommand = new RemoveElement < CnATreeElement > ( organization ) ; commandService . executeCommand ( removeCommand ) ; code_block = ForStatement ; }
public void test() { try ( BufferedReader r = new BufferedReader ( new InputStreamReader ( response . getEntity ( ) . getContent ( ) ) ) ) { content = r . readLine ( ) ; } catch ( Exception e ) { logger . error ( "Failed to read response from remote service: " , e ) ; } finally { closeConnection ( response ) ; } }
public void test() { try { getModelRepository ( ModelId . fromPrettyFormat ( modelId ) ) . removeModel ( ModelId . fromPrettyFormat ( modelId ) ) ; return new ResponseEntity < > ( false , HttpStatus . OK ) ; } catch ( FatalModelRepositoryException | NullPointerException e ) { LOGGER . error ( e ) ; return new ResponseEntity < > ( false , HttpStatus . NOT_FOUND ) ; } }
public void test() { try { Method method = dsClass . getMethod ( methodName ) ; method . invoke ( ds ) ; } catch ( NoSuchMethodException e ) { LOG . warn ( "ExternalDataset '{}' does not have method '{}'. " + "Can't register {} lineage for this dataset" , referenceName , methodName , accessType ) ; } catch ( Exception e ) { LOG . warn ( "Unable to register {} access for dataset {}" , accessType , referenceName ) ; } }
public void test() { try { Method method = dsClass . getMethod ( methodName ) ; method . invoke ( ds ) ; } catch ( NoSuchMethodException e ) { LOG . warn ( "ExternalDataset '{}' does not have method '{}'. " + "Can't register {} lineage for this dataset" , referenceName , methodName , accessType ) ; } catch ( Exception e ) { LOG . warn ( "Unable to register {} access for dataset {}" , accessType , referenceName ) ; } }
public void test() { try { NodeId nodeId = activity . getId ( ) ; code_block = IfStatement ; NodeStateEx act = getNodeStateEx ( nodeId ) ; NodeId parentId = act . getParentId ( ) ; Name name = act . getName ( ) ; code_block = WhileStatement ; operation . save ( ) ; } catch ( ItemStateException e ) { log . error ( "Error while storing: " + e . toString ( ) ) ; } finally { operation . close ( ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Cannot match record: merged record has a too low confidence value (" + normalizedConfidence + " < " + minConfidenceValue + ")" ) ; } }
public void test() { try { setterName = getSetterName ( propertyName ) ; setterMethod = step . getClass ( ) . getMethod ( setterName , expectedType ) ; } catch ( Exception e1 ) { LOG . warn ( e1 . getMessage ( ) , e1 ) ; } }
public void test() { try { sendStatusMessage ( new StatusTaskEnd ( uuid , kick . getJobUuid ( ) , kick . getNodeID ( ) , 0 , 0 , 0 ) ) ; } catch ( Exception ex ) { log . warn ( "---> send fail {} {} {}" , uuid , key , kick , ex ) ; } }
public void test() { try { Class . forName ( "org.apache.derby.jdbc.EmbeddedDriver" ) ; DriverManager . getConnection ( "jdbc:derby:memory:argus;create=true" ) . close ( ) ; } catch ( Exception ex ) { LoggerFactory . getLogger ( AlertServiceTest . class ) . error ( "Exception in setUp:{}" , ex . getMessage ( ) ) ; fail ( "Exception during database startup." ) ; } }
public void test() { try { String tokenStr = token . encodeToUrlString ( ) ; HiveConf hcatConf = createHiveConf ( metaStoreUri , hiveMetaStorePrincipal ) ; LOG . debug ( "renewing delegation tokens for principal={}" , hiveMetaStorePrincipal ) ; hcatClient = HCatClient . create ( hcatConf ) ; Long expiryTime = hcatClient . renewDelegationToken ( tokenStr ) ; LOG . info ( "Renewed delegation token. new expiryTime={}" , expiryTime ) ; return expiryTime ; } catch ( Exception ex ) { throw new RuntimeException ( "Failed to renew delegation tokens." , ex ) ; } finally { code_block = IfStatement ; } }
public void test() { try { String tokenStr = token . encodeToUrlString ( ) ; HiveConf hcatConf = createHiveConf ( metaStoreUri , hiveMetaStorePrincipal ) ; LOG . debug ( "renewing delegation tokens for principal={}" , hiveMetaStorePrincipal ) ; hcatClient = HCatClient . create ( hcatConf ) ; Long expiryTime = hcatClient . renewDelegationToken ( tokenStr ) ; LOG . info ( "Renewed delegation token. new expiryTime={}" , expiryTime ) ; return expiryTime ; } catch ( Exception ex ) { throw new RuntimeException ( "Failed to renew delegation tokens." , ex ) ; } finally { code_block = IfStatement ; } }
public void test() { try { hcatClient . close ( ) ; } catch ( HCatException e ) { LOG . error ( " Exception" , e ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( count + " " + uids . size ( ) + " " + uidsToRemove . size ( ) + " " + uidsCopy . size ( ) + " removing " + ( count == 0 && uidsCopy . isEmpty ( ) ) ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( JMSException e ) { log . error ( e . getMessage ( ) , e ) ; failed ( "Got a JMSException " + e . toString ( ) ) ; latch . countDown ( ) ; } }
public void test() { try { result = persistenceEntryManager . contains ( GluuCustomPerson . class ) ; } catch ( Exception e ) { log . debug ( e . getMessage ( ) , e ) ; } }
@ Ignore @ Test public void findAllUsers ( ) { UserSearchForm searchForm = new UserSearchForm ( ) ; searchForm . setSelectedLanguage ( UserLanguage . ENGLISH ) ; Pageable pageable = new PageRequest ( 0 , 10 , Sort . Direction . ASC , "firstName" ) ; Page < User > usersPage = userService . findAll ( pageable , searchForm ) ; log . debug ( "usersPage.getTotalElements(): {}" , usersPage . getTotalElements ( ) ) ; assertEquals ( 5 , usersPage . getTotalElements ( ) ) ; }
@ Test public void test_binSeq_03 ( ) { Log . debug ( "Test" ) ; checkOverlap ( "acgtacgtacgtacgtacgtacgtacgtacgtacgtacgtacgtacgtacgtacgtacgtacgtacgtacgt" , "acgt" , 0 , 0 ) ; }
@ Override public void onError ( Throwable t , Invoker < ? > invoker , Invocation invocation ) { logger . error ( "Filter get error" , t ) ; }
public void test() { if ( cycleYear == null || cycleYear . isEmpty ( ) ) { logger . error ( new CycleYearEmptyException ( ) ) ; } else { p = cb . and ( p , cb . equal ( root . get ( AttendanceDetail_ . cycleYear ) , cycleYear ) ) ; } }
public void test() { if ( cycleMonth == null || cycleMonth . isEmpty ( ) ) { logger . error ( new CycleMonthEmptyException ( ) ) ; } else { p = cb . and ( p , cb . equal ( root . get ( AttendanceDetail_ . cycleMonth ) , cycleMonth ) ) ; } }
public void test() { try { serviceRegistration . unregister ( ) ; } catch ( final IllegalStateException ex ) { logger . trace ( "Service already unregistered." ) ; } }
@ Override public List < Project > getProjects ( String key , String value , UserInfo userInfo ) throws InvalidProtocolBufferException , ExecutionException , InterruptedException { FindProjects findProjects = FindProjects . newBuilder ( ) . addPredicates ( KeyValueQuery . newBuilder ( ) . setKey ( key ) . setValue ( Value . newBuilder ( ) . setStringValue ( value ) . build ( ) ) . setOperator ( OperatorEnum . Operator . EQ ) . setValueType ( ValueTypeEnum . ValueType . STRING ) . build ( ) ) . build ( ) ; ProjectPaginationDTO projectPaginationDTO = findProjects ( findProjects , null , userInfo , ResourceVisibility . PRIVATE ) ; LOGGER . debug ( "Projects size is {}" , projectPaginationDTO . getProjects ( ) . size ( ) ) ; return projectPaginationDTO . getProjects ( ) ; }
@ Override public QanaryMessage process ( QanaryMessage myQanaryMessage ) throws Exception { long startTime = System . currentTimeMillis ( ) ; logger . info ( "Qanary Message: {}" , myQanaryMessage ) ; QanaryUtils myQanaryUtils = this . getUtils ( myQanaryMessage ) ; QanaryQuestion < String > myQanaryQuestion = this . getQanaryQuestion ( myQanaryMessage ) ; String myQuestion = myQanaryQuestion . getTextualRepresentation ( ) ; logger . info ( "Question: {}" , myQuestion ) ; Detector detector = DetectorFactory . create ( ) ; detector . append ( myQuestion ) ; String lang = detector . detect ( ) ; logger . info ( "Language: {}" , lang ) ; logger . info ( "store data in graph {}" , myQanaryMessage . getEndpoint ( ) ) ; myQanaryQuestion . setLanguageText ( lang ) ; return myQanaryMessage ; }
@ Override public QanaryMessage process ( QanaryMessage myQanaryMessage ) throws Exception { long startTime = System . currentTimeMillis ( ) ; logger . info ( "Qanary Message: {}" , myQanaryMessage ) ; QanaryUtils myQanaryUtils = this . getUtils ( myQanaryMessage ) ; QanaryQuestion < String > myQanaryQuestion = this . getQanaryQuestion ( myQanaryMessage ) ; String myQuestion = myQanaryQuestion . getTextualRepresentation ( ) ; logger . info ( "Question: {}" , myQuestion ) ; Detector detector = DetectorFactory . create ( ) ; detector . append ( myQuestion ) ; String lang = detector . detect ( ) ; logger . info ( "Language: {}" , lang ) ; logger . info ( "store data in graph {}" , myQanaryMessage . getEndpoint ( ) ) ; myQanaryQuestion . setLanguageText ( lang ) ; return myQanaryMessage ; }
@ Override public QanaryMessage process ( QanaryMessage myQanaryMessage ) throws Exception { long startTime = System . currentTimeMillis ( ) ; logger . info ( "Qanary Message: {}" , myQanaryMessage ) ; QanaryUtils myQanaryUtils = this . getUtils ( myQanaryMessage ) ; QanaryQuestion < String > myQanaryQuestion = this . getQanaryQuestion ( myQanaryMessage ) ; String myQuestion = myQanaryQuestion . getTextualRepresentation ( ) ; logger . info ( "Question: {}" , myQuestion ) ; Detector detector = DetectorFactory . create ( ) ; detector . append ( myQuestion ) ; String lang = detector . detect ( ) ; logger . info ( "Language: {}" , lang ) ; logger . info ( "store data in graph {}" , myQanaryMessage . getEndpoint ( ) ) ; myQanaryQuestion . setLanguageText ( lang ) ; return myQanaryMessage ; }
@ Override public QanaryMessage process ( QanaryMessage myQanaryMessage ) throws Exception { long startTime = System . currentTimeMillis ( ) ; logger . info ( "Qanary Message: {}" , myQanaryMessage ) ; QanaryUtils myQanaryUtils = this . getUtils ( myQanaryMessage ) ; QanaryQuestion < String > myQanaryQuestion = this . getQanaryQuestion ( myQanaryMessage ) ; String myQuestion = myQanaryQuestion . getTextualRepresentation ( ) ; logger . info ( "Question: {}" , myQuestion ) ; Detector detector = DetectorFactory . create ( ) ; detector . append ( myQuestion ) ; String lang = detector . detect ( ) ; logger . info ( "Language: {}" , lang ) ; logger . info ( "store data in graph {}" , myQanaryMessage . getEndpoint ( ) ) ; myQanaryQuestion . setLanguageText ( lang ) ; return myQanaryMessage ; }
public void test() { try { URI uri = new URI ( url ) ; desktop . browse ( uri ) ; } catch ( IOException | URISyntaxException ex ) { LOG . error ( ex . getLocalizedMessage ( ) ) ; } }
public void test() { try { handles = getAllPreparedQueries ( sessionid , user , queryName , fromDate , toDate ) ; code_block = ForStatement ; } catch ( Exception e ) { log . error ( "Error destroying prepared queries" , e ) ; failed = true ; } }
public void test() { try ( Tx tx = StructrApp . getInstance ( securityContext ) . tx ( ) ) { final Date date = file . getLastModifiedDate ( ) ; code_block = IfStatement ; tx . success ( ) ; } catch ( FrameworkException fex ) { logger . error ( "" , fex ) ; } }
public void test() { try { final SuggestionEntry entry = new SuggestionEntry ( this . server , factory , application , search , result , start , end , result . getQueryTime ( ) , result . getElapsedTime ( ) , session ) ; final HashMap < String , Object > mergedMetadata = new HashMap < > ( ) ; mergedMetadata . putAll ( this . monitoringMetadata ) ; mergedMetadata . putAll ( metadata ) ; entry . setMetadata ( mergedMetadata ) ; log . debug ( "Monitoring is adding a Suggestion entry" ) ; logger . log ( entry ) ; } catch ( Exception e ) { log . error ( "Suggestion monitoring error: {}" , e . getMessage ( ) , e ) ; code_block = IfStatement ; } }
public void test() { if ( logger . isTraceEnabled ( ) && valid && ( b < - 128 || b > 127 ) ) { logger . trace ( "The value {} ({}) is considered a byte because only the 8 least significant bits are set" + ", but its value is outside signed byte that is between -128 and 127" , b , Integer . toHexString ( b ) ) ; } }
public void test() { if ( ! checkCompatibility ( pluginInfo ) ) { final String msg = "Plugin '" + pluginInfo . getName ( ) + "' not compatible with this version of JSPWiki" ; log . info ( msg ) ; } else { plugin = pluginInfo . newPluginInstance ( m_searchPath , m_externalJars ) ; } }
void process ( Text key , Reducer < Text , BytesWritable , Text , Text > . Context context , Iterable < BytesWritable > documents , int maxNumberOfDocuments ) throws IOException , InterruptedException { Iterator < BytesWritable > docsIterator = documents . iterator ( ) ; log . info ( "-- start process, key: {}" , key . toString ( ) ) ; int partNb = 1 ; code_block = WhileStatement ; log . info ( "-- end process, key: {}" , key ) ; }
void process ( Text key , Reducer < Text , BytesWritable , Text , Text > . Context context , Iterable < BytesWritable > documents , int maxNumberOfDocuments ) throws IOException , InterruptedException { Iterator < BytesWritable > docsIterator = documents . iterator ( ) ; log . info ( "-- start process, key: {}" , key . toString ( ) ) ; int partNb = 1 ; code_block = WhileStatement ; log . info ( "-- end process, key: {}" , key ) ; }
public void test() { try { logger . debug ( "PublishAuthUpdateTask.run started..." ) ; code_block = IfStatement ; authorizationDriver . publishAuthUpdate ( systemId ) ; } catch ( final Throwable ex ) { logger . debug ( "Exception:" , ex . getMessage ( ) ) ; } }
public void test() { if ( Thread . currentThread ( ) . isInterrupted ( ) ) { logger . trace ( "Thread {} is interrupted..." , Thread . currentThread ( ) . getName ( ) ) ; return ; } }
public void test() { try { logger . debug ( "PublishAuthUpdateTask.run started..." ) ; code_block = IfStatement ; authorizationDriver . publishAuthUpdate ( systemId ) ; } catch ( final Throwable ex ) { logger . debug ( "Exception:" , ex . getMessage ( ) ) ; } }
public StgSysExportZos merge ( StgSysExportZos detachedInstance ) { log . debug ( "merging StgSysExportZos instance" ) ; code_block = TryStatement ;  }
public void test() { try { StgSysExportZos result = ( StgSysExportZos ) sessionFactory . getCurrentSession ( ) . merge ( detachedInstance ) ; log . debug ( "merge successful" ) ; return result ; } catch ( RuntimeException re ) { log . error ( "merge failed" , re ) ; throw re ; } }
public void test() { try { StgSysExportZos result = ( StgSysExportZos ) sessionFactory . getCurrentSession ( ) . merge ( detachedInstance ) ; log . debug ( "merge successful" ) ; return result ; } catch ( RuntimeException re ) { log . error ( "merge failed" , re ) ; throw re ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Open file: {}" , fileChooser . getSelectedFile ( ) . getPath ( ) ) ; } }
@ PatchMapping ( CommonConstants . PATH_ID ) @ Secured ( ServicesData . ROLE_UPDATE_SECURITY_PROFILES ) public SecurityProfileDto patch ( final @ PathVariable ( "id" ) String id , @ RequestBody final Map < String , Object > partialDto ) { LOGGER . debug ( "Patch {} with {}" , id , partialDto ) ; ParameterChecker . checkParameter ( "The Identifier is a mandatory parameter: " , id ) ; Assert . isTrue ( StringUtils . equals ( id , ( String ) partialDto . get ( "id" ) ) , "The DTO identifier must match the path identifier for update." ) ; return securityProfileExternalService . patch ( partialDto ) ; }
@ Override public void onDeviceAdded ( Bridge bridge , Device device ) { logger . trace ( "Adding new MAX! {} with id '{}' to inbox" , device . getType ( ) , device . getSerialNumber ( ) ) ; ThingUID thingUID = null ; code_block = SwitchStatement ; code_block = IfStatement ; }
public void test() { if ( thingUID != null ) { String name = device . getName ( ) ; code_block = IfStatement ; DiscoveryResult discoveryResult = DiscoveryResultBuilder . create ( thingUID ) . withProperty ( Thing . PROPERTY_SERIAL_NUMBER , device . getSerialNumber ( ) ) . withBridge ( bridge . getUID ( ) ) . withLabel ( device . getType ( ) + ": " + name ) . withRepresentationProperty ( Thing . PROPERTY_SERIAL_NUMBER ) . build ( ) ; thingDiscovered ( discoveryResult ) ; } else { logger . debug ( "Discovered MAX! device is unsupported: type '{}' with id '{}'" , device . getType ( ) , device . getSerialNumber ( ) ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "ignored duplicate packet: apid={} time={} seq={}" , apid , TimeEncoding . toOrdinalDateTime ( instant ) , seq ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "ignored duplicate packet: apid={} time={} seq={}" , apid , TimeEncoding . toOrdinalDateTime ( instant ) , seq ) ; } }
public void test() { try { Analyzer analyzer = createAnalyzer ( ) ; QueryParser queryParser1 = new QueryParser ( "text" , analyzer ) ; Query termQuery1 = queryParser1 . parse ( keyword ) ; BooleanClause booleanClause1 = new BooleanClause ( termQuery1 , BooleanClause . Occur . SHOULD ) ; QueryParser queryParser2 = new QueryParser ( "title" , analyzer ) ; Query termQuery2 = queryParser2 . parse ( keyword ) ; BooleanClause booleanClause2 = new BooleanClause ( termQuery2 , BooleanClause . Occur . SHOULD ) ; BooleanQuery . Builder builder = new BooleanQuery . Builder ( ) ; builder . add ( booleanClause1 ) . add ( booleanClause2 ) ; return builder . build ( ) ; } catch ( ParseException e ) { LOG . error ( e . toString ( ) , e ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( exception , exception ) ; } }
public void test() { if ( ! dependencySet . isUseTransitiveDependencies ( ) && dependencySet . isUseTransitiveFiltering ( ) ) { logger . warn ( "DependencySet has nonsensical configuration: useTransitiveDependencies == false " + "AND useTransitiveFiltering == true. Transitive filtering flag will be ignored." ) ; } }
public void test() { try { ProjectBuildingResult build = projectBuilder1 . build ( depArtifact , pbr ) ; depProject = build . getProject ( ) ; } catch ( final ProjectBuildingException e ) { logger . debug ( "Error retrieving POM of module-dependency: " + depArtifact . getId ( ) + "; Reason: " + e . getMessage ( ) + "\n\nBuilding stub project instance." ) ; depProject = buildProjectStub ( depArtifact ) ; } }
public void test() { try { return doSearch ( dataRequest , cube , tupleConverter , recordsSerializer , receiver , tupleInfo ) ; } catch ( IOException e ) { exception = e ; failedReceivers . put ( receiver , System . currentTimeMillis ( ) ) ; logger . error ( "exception throws for receiver:" + receiver + " retry another receiver" ) ; } }
public void test() { try { return doSearch ( dataRequest , cube , tupleConverter , recordsSerializer , receiver , tupleInfo ) ; } catch ( IOException e ) { exception = e ; failedReceivers . put ( receiver , System . currentTimeMillis ( ) ) ; logger . error ( "exception throws for receiver:" + receiver + " retry another receiver" ) ; } }
@ Override public void connectionLost ( SocketAddress sa ) { String msg = "lost memcached connection [" + sa + "] reconnecting..." ; log . error ( msg ) ; }
public void test() { if ( to . getScheduleSnapshots ( ) != from . getScheduleSnapshots ( ) || to . getReplicationSupported ( ) != from . getFileReplicationSupported ( ) || to . getAllowFilePolicyAtProjectLevel ( ) != from . getAllowFilePolicyAtProjectLevel ( ) || to . getAllowFilePolicyAtFSLevel ( ) != from . getAllowFilePolicyAtFSLevel ( ) ) { _log . info ( "Protection parameters cannot be modified to a vpool with provisioned filessystems " , from . getId ( ) ) ; return true ; } }
public void test() { if ( to . getMinRpoType ( ) != from . getFrRpoType ( ) || to . getMinRpoValue ( ) != from . getFrRpoValue ( ) ) { _log . info ( "RPO parameters cannot be modified to a vpool with provisioned filessystems " , from . getId ( ) ) ; return true ; } }
public void test() { try { Content publishingContent = this . getContentManager ( ) . loadContent ( this . getContentId ( ) , true ) ; code_block = IfStatement ; } catch ( Throwable t ) { _logger . error ( "Error validating content {}" , this . getContentId ( ) , t ) ; throw new RuntimeException ( "Errore in validazione contenuto con id " + this . getContentId ( ) , t ) ; } }
public void test() { try { this . createValuedShowlet ( ) ; } catch ( Throwable t ) { _logger . error ( "error creating new widget" , t ) ; throw new RuntimeException ( "Errore in creazione widget valorizzato" , t ) ; } }
private synchronized void discover ( ) { logger . debug ( "Discovery job is running" ) ; MulticastListener epsonMulticastListener ; String local = "127.0.0.1" ; code_block = TryStatement ;  code_block = WhileStatement ; epsonMulticastListener . shutdown ( ) ; logger . debug ( "Discovery job is exiting" ) ; }
public void test() { try { String ip = networkAddressService . getPrimaryIpv4HostAddress ( ) ; epsonMulticastListener = new MulticastListener ( ( ip != null ? ip : local ) ) ; } catch ( SocketException se ) { logger . debug ( "Discovery job got Socket exception creating multicast socket: {}" , se . getMessage ( ) ) ; return ; } catch ( IOException ioe ) { logger . debug ( "Discovery job got IO exception creating multicast socket: {}" , ioe . getMessage ( ) ) ; return ; } }
public void test() { try { String ip = networkAddressService . getPrimaryIpv4HostAddress ( ) ; epsonMulticastListener = new MulticastListener ( ( ip != null ? ip : local ) ) ; } catch ( SocketException se ) { logger . debug ( "Discovery job got Socket exception creating multicast socket: {}" , se . getMessage ( ) ) ; return ; } catch ( IOException ioe ) { logger . debug ( "Discovery job got IO exception creating multicast socket: {}" , ioe . getMessage ( ) ) ; return ; } }
public void test() { try { beaconReceived = epsonMulticastListener . waitForBeacon ( ) ; } catch ( IOException ioe ) { logger . debug ( "Discovery job got exception waiting for beacon: {}" , ioe . getMessage ( ) ) ; beaconReceived = false ; } }
private synchronized void discover ( ) { logger . debug ( "Discovery job is running" ) ; MulticastListener epsonMulticastListener ; String local = "127.0.0.1" ; code_block = TryStatement ;  code_block = WhileStatement ; epsonMulticastListener . shutdown ( ) ; logger . debug ( "Discovery job is exiting" ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "unwrapping package {} completed in {}ms" , getId ( ) , System . currentTimeMillis ( ) - now ) ; } }
public void test() { try { publishAppCertificate ( ) ; } catch ( KuraException e ) { logger . warn ( "Unable to publish updated App Certificate" ) ; } }
public void test() { if ( taggerAllCrisesResponse . getCrisises ( ) != null ) { logger . info ( "Tagger returned " + taggerAllCrisesResponse . getCrisises ( ) . size ( ) + " crisis for user" ) ; } }
public void test() { try { WebTarget webResource = client . target ( taggerMainUrl + "/crisis?userID=" + userId ) ; ObjectMapper objectMapper = JacksonWrapper . getObjectMapper ( ) ; objectMapper . configure ( DeserializationConfig . Feature . FAIL_ON_UNKNOWN_PROPERTIES , false ) ; Response clientResponse = webResource . request ( MediaType . APPLICATION_JSON ) . get ( ) ; String jsonResponse = clientResponse . readEntity ( String . class ) ; TaggerAllCrisesResponse taggerAllCrisesResponse = objectMapper . readValue ( jsonResponse , TaggerAllCrisesResponse . class ) ; code_block = IfStatement ; return taggerAllCrisesResponse . getCrisises ( ) ; } catch ( Exception e ) { logger . error ( "Exception while fetching crisis by userId: " + userId , e ) ; throw new AidrException ( "No collection is enabled for Tagger. Please enable tagger for one of your collections." , e ) ; } }
public void test() { try { docs = serializer . toDocuments ( index , value ) ; } catch ( Exception e ) { exceptionHappened = true ; stats . incFailedEntries ( ) ; logger . info ( "Failed to add index to " + value + " due to " + e . getMessage ( ) ) ; } }
public void test() { if ( ! rack . isPresent ( ) ) { LOG . warn ( "Rack {} not present, discarding {}" , expiringObject . getMachineId ( ) , expiringObject ) ; } else { code_block = TryStatement ;  } }
public void test() { try { handleExpiringObject ( expiringObject , rack . get ( ) , getMessage ( expiringObject ) ) ; } catch ( Exception e ) { LOG . error ( "Could not return rack {} to state {}" , rack . get ( ) . getName ( ) , expiringObject . getRevertToState ( ) ) ; } }
@ AfterClass public static void reportTest ( ) { LOGGER . warn ( "-----------------------------------------" ) ; LOGGER . warn ( "*                                       *" ) ; LOGGER . warn ( "*      FINISHED CustomCRSLandsatIT               *" ) ; LOGGER . warn ( "*         " + ( ( System . currentTimeMillis ( ) - startMillis ) / 1000 ) + "s elapsed.                 *" ) ; LOGGER . warn ( "*                                       *" ) ; LOGGER . warn ( "-----------------------------------------" ) ; }
public void test() { try { service . executeTransaction ( interfaceName , txId , arguments , callerServiceId , context ) ; } catch ( Exception e ) { logger . info ( "Transaction execution failed (service={}, txId={}, txMessageHash={})" , service . getName ( ) , txId , context . getTransactionMessageHash ( ) , e ) ; throw e ; } }
public void test() { if ( ! differences . isEmpty ( ) ) { LOGGER . warn ( "Objects in source {} not found for ids: {}" , masterSource , differences ) ; } }
public void test() { try { sourceContext . setCurrent ( masterSource ) ; differences = loadObjects ( proxy , loadedObjects ) ; code_block = IfStatement ; } catch ( IllegalSourceException e ) { LOGGER . debug ( "Source not configured: {}" , masterSource , e ) ; } finally { sourceContext . setCurrent ( originalSource ) ; } }
public void test() { try { code_block = IfStatement ; upgradeLogPath . createNewFile ( ) ; upgradeLogWriter = new BufferedWriter ( new FileWriter ( getUpgradeLogPath ( ) , true ) ) ; return true ; } catch ( IOException e ) { logger . error ( "meet error when create upgrade log, file path:{}" , upgradeLogPath , e ) ; return false ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "Adding pageTransaction " + pageTransaction . getTransactionID ( ) ) ; } }
public org . talend . mdm . webservice . WSString getTransformerPluginV2Configuration ( org . talend . mdm . webservice . WSTransformerPluginV2GetConfiguration arg0 ) { LOG . info ( "Executing operation getTransformerPluginV2Configuration" ) ; System . out . println ( arg0 ) ; code_block = TryStatement ;  }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "==> RangerDefaultAuditHandler.getAuthzEvents(" + results + ")" ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "<== RangerDefaultAuditHandler.getAuthzEvents(" + results + "): " + ret ) ; } }
public void test() { try { newToken . setOpenidToken ( createIdToken ( now , newToken , Arrays . asList ( new Audience ( clientUserName ) ) , userInfoClaimSet ) ) ; } catch ( Exception e ) { log . error ( "Cannot create new id token" , e ) ; throw new OAuthErrorException ( makeError ( OAuth2Error . SERVER_ERROR , e . getMessage ( ) ) ) ; } }
public void test() { try { targetType = arg1 . getRequiredType ( ) ; Converter conv = defaultConv . lookupConverterForType ( targetType ) ; result = conv . unmarshal ( arg0 , arg1 ) ; } catch ( final Exception ex ) { log . warn ( "Ignore unknown class or property " + targetType + " " + ex . getMessage ( ) ) ; return null ; } }
public void test() { try { code_block = IfStatement ; } catch ( final HibernateException | NullPointerException ex ) { log . error ( "Failed to write " + result + " ex=" + ex , ex ) ; } }
public void test() { -> { log . warn ( "{} was deprecated and will be removed in a future release; assuming user meant" + " its replacement {} and will remove that instead" , property , replacement ) ; } }
public void test() { for ( Logger logger : this . loggers ) { logger . warn ( marker , format , arguments ) ; } }
@ Override public Optional < MigrationIssue > getCourseMigrationIssue ( String courseId , Integer migrationId , Integer issueId ) throws IOException { LOG . debug ( "listing a migration issue for a content migration for the course" ) ; String url = buildCanvasUrl ( "courses/" + courseId + "/content_migrations/" + migrationId . toString ( ) + "/migration_issues/" + issueId . toString ( ) , Collections . emptyMap ( ) ) ; Response response = canvasMessenger . getSingleResponseFromCanvas ( oauthToken , url ) ; return responseParser . parseToObject ( MigrationIssue . class , response ) ; }
public void test() { if ( givenWorkflowIndex . isEmpty ( ) ) { LOGGER . warn ( "No workflow index given to create model, use workflow with index: " + workflowIndex ) ; } }
public void test() { if ( machine . isEmpty ( ) || workflowIndex < 0 || machine . get ( ) . getWorkflows ( ) . size ( ) <= workflowIndex ) { LOGGER . error ( "No workflow with index " + workflowIndex + " in " + machine + ", return empty model." ) ; return createEmptyRoot ( ) ; } }
public void test() { try { sendErrorStatus ( ) ; } catch ( IOException ioe ) { log . error ( "Error writing to network port" ) ; } }
public void test() { if ( Notification . isWrongBody ( body ) ) { log . debug ( "Wrong push body." ) ; return ; } }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; } catch ( Throwable e ) { LOG . warn ( "Exception occurred during done UnitOfWork for Exchange: {}. This exception will be ignored." , exchange , e ) ; } }
public void test() { switch ( result ) { case SUCCESS : LOG . debug ( "Deleted {} open keys out of {} submitted keys." , numDeletedOpenKeys , numSubmittedOpenKeys ) ; break ; case FAILURE : omMetrics . incNumOpenKeyDeleteRequestFails ( ) ; LOG . error ( "Failure occurred while trying to delete {} submitted open " + "keys." , numSubmittedOpenKeys ) ; break ; default : LOG . error ( "Unrecognized result for OMOpenKeysDeleteRequest: {}" , request ) ; } }
public void test() { switch ( result ) { case SUCCESS : LOG . debug ( "Deleted {} open keys out of {} submitted keys." , numDeletedOpenKeys , numSubmittedOpenKeys ) ; break ; case FAILURE : omMetrics . incNumOpenKeyDeleteRequestFails ( ) ; LOG . error ( "Failure occurred while trying to delete {} submitted open " + "keys." , numSubmittedOpenKeys ) ; break ; default : LOG . error ( "Unrecognized result for OMOpenKeysDeleteRequest: {}" , request ) ; } }
public void test() { switch ( result ) { case SUCCESS : LOG . debug ( "Deleted {} open keys out of {} submitted keys." , numDeletedOpenKeys , numSubmittedOpenKeys ) ; break ; case FAILURE : omMetrics . incNumOpenKeyDeleteRequestFails ( ) ; LOG . error ( "Failure occurred while trying to delete {} submitted open " + "keys." , numSubmittedOpenKeys ) ; break ; default : LOG . error ( "Unrecognized result for OMOpenKeysDeleteRequest: {}" , request ) ; } }
public void test() { if ( ! ObjectUtils . isNull ( category ) ) { final KualiDecimal oneCent = new KualiDecimal ( 0.01 ) ; int size = contractsGrantsInvoiceDocument . getAccountDetails ( ) . size ( ) ; KualiDecimal amount = new KualiDecimal ( invoiceDetail . getInvoiceAmount ( ) . bigDecimalValue ( ) . divide ( new BigDecimal ( size ) , 2 , RoundingMode . HALF_UP ) ) ; KualiDecimal remainder = invoiceDetail . getInvoiceAmount ( ) . subtract ( amount . multiply ( new KualiDecimal ( size ) ) ) ; code_block = ForStatement ; } else { LOG . error ( "Category Code cannot be found from the category list during recalculation of account object code for Contracts & Grants Invoice Document." ) ; } }
public void test() { if ( log . isTraceEnabled ( ) ) { log . trace ( "no store found to " + o . getUID ( ) + " for disposal" ) ; } }
private void interruptBackup ( ) { LOG . info ( "Interrupting ongoing backup." ) ; code_block = IfStatement ; boolean shouldResume = true ; code_block = IfStatement ; code_block = IfStatement ; LOG . warn ( "Backup interrupted successfully." ) ; }
public void test() { if ( mBackupFuture != null && ! mBackupFuture . isDone ( ) ) { LOG . info ( "Attempt to cancel backup task." ) ; mBackupFuture . cancel ( true ) ; } }
public void test() { try { LOG . info ( "Attempt to resume journal application." ) ; mJournalSystem . resume ( ) ; } catch ( Exception e ) { LOG . warn ( "Failed to resume journal application: {}" , e . toString ( ) ) ; } }
public void test() { try { logger . trace ( "Assertion from where roles will be sought = " + AssertionUtil . asString ( assertion ) ) ; } catch ( ProcessingException ignore ) { } }
public void test() { try { String clusterName = ( String ) getRequest ( ) . getAttributes ( ) . get ( "clusterName" ) ; JsonParameters jsonParameters = new JsonParameters ( entity ) ; String command = jsonParameters . getCommand ( ) ; ZkClient zkClient = ( ZkClient ) getContext ( ) . getAttributes ( ) . get ( RestAdminApplication . ZKCLIENT ) ; ClusterSetup setupTool = new ClusterSetup ( zkClient ) ; code_block = IfStatement ; getResponse ( ) . setEntity ( getInstancesRepresentation ( clusterName ) ) ; getResponse ( ) . setStatus ( Status . SUCCESS_OK ) ; } catch ( Exception e ) { getResponse ( ) . setEntity ( ClusterRepresentationUtil . getErrorAsJsonStringFromException ( e ) , MediaType . APPLICATION_JSON ) ; getResponse ( ) . setStatus ( Status . SUCCESS_OK ) ; LOG . error ( "" , e ) ; } }
public void test() { try { final GUIDImpl parsed1 = new GUIDImpl ( properties . getProperty ( FIELDS . BASE32 . name ( ) ) ) ; final GUIDImpl parsed2 = new GUIDImpl ( properties . getProperty ( FIELDS . BASE64 . name ( ) ) ) ; final GUIDImpl parsed0 = new GUIDImpl ( properties . getProperty ( FIELDS . BASE16 . name ( ) ) ) ; final GUIDImpl parsed8 = new GUIDImpl ( properties . getProperty ( FIELDS . BASEARK . name ( ) ) ) ; final byte [ ] bytes = StringUtils . getBytesFromArraysToString ( properties . getProperty ( FIELDS . BYTES . name ( ) ) ) ; final GUIDImpl parsed9 = new GUIDImpl ( bytes ) ; assertTrue ( parsed1 . equals ( parsed2 ) ) ; assertTrue ( parsed1 . equals ( parsed0 ) ) ; assertTrue ( parsed1 . equals ( parsed8 ) ) ; assertTrue ( parsed1 . equals ( parsed9 ) ) ; final GUIDImpl parsed3 = new GUIDImpl ( parsed9 . getBytes ( ) ) ; final GUIDImpl parsed4 = new GUIDImpl ( parsed9 . toBase32 ( ) ) ; final GUIDImpl parsed5 = new GUIDImpl ( parsed9 . toHex ( ) ) ; final GUIDImpl parsed6 = new GUIDImpl ( parsed9 . toString ( ) ) ; final GUIDImpl parsed7 = new GUIDImpl ( parsed9 . toBase64 ( ) ) ; assertTrue ( parsed9 . equals ( parsed3 ) ) ; assertTrue ( parsed9 . equals ( parsed4 ) ) ; assertTrue ( parsed9 . equals ( parsed5 ) ) ; assertTrue ( parsed9 . equals ( parsed6 ) ) ; assertTrue ( parsed9 . equals ( parsed7 ) ) ; final GUIDImpl generated = new GUIDImpl ( ) ; assertTrue ( generated . getVersion ( ) == 0 ) ; } catch ( final InvalidGuidOperationException e ) { LOGGER . debug ( e ) ; fail ( e . getMessage ( ) ) ; } }
public void test() { try { mp = MP_FACTORY . newMediaPackageBuilder ( ) . loadFromXml ( mediaPackageXml ) ; code_block = IfStatement ; } catch ( MediaPackageException e ) { logger . debug ( "Rejected ingest with invalid media package {}" , mp ) ; return Response . status ( Status . BAD_REQUEST ) . build ( ) ; } }
public void test() { if ( mediaPackageElements . length != 1 ) { logger . debug ( "There can be only one (and exactly one) episode dublin core catalog: https://youtu.be/_J3VeogFUOs" ) ; return Response . status ( Status . BAD_REQUEST ) . build ( ) ; } }
public void test() { if ( conditionHandlers . isEmpty ( ) ) { String message = "Condition encountered processing deployment id '" + deplomentId + "' statement '" + statementName + "'" ; code_block = IfStatement ; message += " :" + condition . toString ( ) ; log . info ( message ) ; return ; } }
public void test() { if ( systemVmIsoPath != null ) { LOGGER . debug ( "found systemvm patch iso " + systemVmIsoPath ) ; isoFile = new File ( systemVmIsoPath ) ; } }
public void test() { if ( ! isoFile . exists ( ) ) { LOGGER . error ( "Unable to locate " + iso + " in your setup at " + isoFile . toString ( ) ) ; } }
public SecurityEvent createSecurityEvent ( String loggingClass , URI requestUri , String slMessage , Entity explicitRealmEntity , String entityType , Set < Entity > entities ) { LOG . debug ( "Creating security event with targetEdOrgList determined from entities of type " + entityType ) ; Set < String > targetEdOrgs = getTargetEdOrgStateIds ( entityType , entities ) ; return createSecurityEvent ( loggingClass , requestUri , slMessage , null , null , explicitRealmEntity , targetEdOrgs , false ) ; }
public void test() { if ( bridge != null ) { ObjectStatus objStatus = bridge . requestObjectStatus ( Message . OBJ_TYPE_AREA , thingID , thingID , true ) ; return Optional . of ( ( ExtendedAreaStatus ) objStatus . getStatuses ( ) [ 0 ] ) ; } else { logger . debug ( "Received null bridge while updating Area status!" ) ; return Optional . empty ( ) ; } }
public void test() { try { final OmnilinkBridgeHandler bridge = getOmnilinkBridgeHandler ( ) ; code_block = IfStatement ; } catch ( OmniInvalidResponseException | OmniUnknownMessageTypeException | BridgeOfflineException e ) { logger . debug ( "Received exception while refreshing Area status: {}" , e . getMessage ( ) ) ; return Optional . empty ( ) ; } }
public void test() { try { primaryClient = establishConnectionToPrimary ( replicaSet ) ; code_block = IfStatement ; } catch ( Throwable t ) { LOGGER . error ( "Streaming for replica set {} failed" , replicaSet . replicaSetName ( ) , t ) ; errorHandler . setProducerThrowable ( t ) ; } finally { code_block = IfStatement ; } }
public void test() { if ( LOG . isInfoEnabled ( ) ) { LOG . info ( "Writing person: " + person ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( exception , exception ) ; } }
@ Override public SegmentMetadata load ( File segmentDir ) throws Exception { final SegmentMetadata segmentMetadata = new SegmentMetadataImpl ( segmentDir ) ; LOGGER . info ( "Loaded segment metadata for segment : " + segmentMetadata . getName ( ) ) ; return segmentMetadata ; }
public void test() { try { LOG . debug ( "Running beforeSuite ..." ) ; ServerLauncher . setSetUpSuite ( true ) ; SUPPORT = new DropwizardTestSupport < OxdServerConfiguration > ( OxdServerApplication . class , ResourceHelpers . resourceFilePath ( "oxd-server-jenkins.yml" ) , ConfigOverride . config ( "server.applicationConnectors[0].port" , "0" ) ) ; SUPPORT . before ( ) ; LOG . debug ( "HTTP server started." ) ; removeExistingRps ( ) ; LOG . debug ( "Existing RPs are removed." ) ; RegisterSiteResponse setupClient = SetupClientTest . setupClient ( Tester . newClient ( host ) , opHost , redirectUrls ) ; Tester . setSetupClient ( setupClient , host , opHost ) ; LOG . debug ( "SETUP_CLIENT is set in Tester." ) ; Preconditions . checkNotNull ( Tester . getAuthorization ( ) ) ; LOG . debug ( "Tester's authorization is set." ) ; setupSwaggerSuite ( Tester . getTargetHost ( host ) , opHost , redirectUrls ) ; LOG . debug ( "Finished beforeSuite!" ) ; } catch ( Exception e ) { LOG . error ( "Failed to start suite." , e ) ; throw new AssertionError ( "Failed to start suite." ) ; } }
public void test() { try { LOG . debug ( "Running beforeSuite ..." ) ; ServerLauncher . setSetUpSuite ( true ) ; SUPPORT = new DropwizardTestSupport < OxdServerConfiguration > ( OxdServerApplication . class , ResourceHelpers . resourceFilePath ( "oxd-server-jenkins.yml" ) , ConfigOverride . config ( "server.applicationConnectors[0].port" , "0" ) ) ; SUPPORT . before ( ) ; LOG . debug ( "HTTP server started." ) ; removeExistingRps ( ) ; LOG . debug ( "Existing RPs are removed." ) ; RegisterSiteResponse setupClient = SetupClientTest . setupClient ( Tester . newClient ( host ) , opHost , redirectUrls ) ; Tester . setSetupClient ( setupClient , host , opHost ) ; LOG . debug ( "SETUP_CLIENT is set in Tester." ) ; Preconditions . checkNotNull ( Tester . getAuthorization ( ) ) ; LOG . debug ( "Tester's authorization is set." ) ; setupSwaggerSuite ( Tester . getTargetHost ( host ) , opHost , redirectUrls ) ; LOG . debug ( "Finished beforeSuite!" ) ; } catch ( Exception e ) { LOG . error ( "Failed to start suite." , e ) ; throw new AssertionError ( "Failed to start suite." ) ; } }
public void test() { try { LOG . debug ( "Running beforeSuite ..." ) ; ServerLauncher . setSetUpSuite ( true ) ; SUPPORT = new DropwizardTestSupport < OxdServerConfiguration > ( OxdServerApplication . class , ResourceHelpers . resourceFilePath ( "oxd-server-jenkins.yml" ) , ConfigOverride . config ( "server.applicationConnectors[0].port" , "0" ) ) ; SUPPORT . before ( ) ; LOG . debug ( "HTTP server started." ) ; removeExistingRps ( ) ; LOG . debug ( "Existing RPs are removed." ) ; RegisterSiteResponse setupClient = SetupClientTest . setupClient ( Tester . newClient ( host ) , opHost , redirectUrls ) ; Tester . setSetupClient ( setupClient , host , opHost ) ; LOG . debug ( "SETUP_CLIENT is set in Tester." ) ; Preconditions . checkNotNull ( Tester . getAuthorization ( ) ) ; LOG . debug ( "Tester's authorization is set." ) ; setupSwaggerSuite ( Tester . getTargetHost ( host ) , opHost , redirectUrls ) ; LOG . debug ( "Finished beforeSuite!" ) ; } catch ( Exception e ) { LOG . error ( "Failed to start suite." , e ) ; throw new AssertionError ( "Failed to start suite." ) ; } }
public void test() { try { LOG . debug ( "Running beforeSuite ..." ) ; ServerLauncher . setSetUpSuite ( true ) ; SUPPORT = new DropwizardTestSupport < OxdServerConfiguration > ( OxdServerApplication . class , ResourceHelpers . resourceFilePath ( "oxd-server-jenkins.yml" ) , ConfigOverride . config ( "server.applicationConnectors[0].port" , "0" ) ) ; SUPPORT . before ( ) ; LOG . debug ( "HTTP server started." ) ; removeExistingRps ( ) ; LOG . debug ( "Existing RPs are removed." ) ; RegisterSiteResponse setupClient = SetupClientTest . setupClient ( Tester . newClient ( host ) , opHost , redirectUrls ) ; Tester . setSetupClient ( setupClient , host , opHost ) ; LOG . debug ( "SETUP_CLIENT is set in Tester." ) ; Preconditions . checkNotNull ( Tester . getAuthorization ( ) ) ; LOG . debug ( "Tester's authorization is set." ) ; setupSwaggerSuite ( Tester . getTargetHost ( host ) , opHost , redirectUrls ) ; LOG . debug ( "Finished beforeSuite!" ) ; } catch ( Exception e ) { LOG . error ( "Failed to start suite." , e ) ; throw new AssertionError ( "Failed to start suite." ) ; } }
public void test() { try { LOG . debug ( "Running beforeSuite ..." ) ; ServerLauncher . setSetUpSuite ( true ) ; SUPPORT = new DropwizardTestSupport < OxdServerConfiguration > ( OxdServerApplication . class , ResourceHelpers . resourceFilePath ( "oxd-server-jenkins.yml" ) , ConfigOverride . config ( "server.applicationConnectors[0].port" , "0" ) ) ; SUPPORT . before ( ) ; LOG . debug ( "HTTP server started." ) ; removeExistingRps ( ) ; LOG . debug ( "Existing RPs are removed." ) ; RegisterSiteResponse setupClient = SetupClientTest . setupClient ( Tester . newClient ( host ) , opHost , redirectUrls ) ; Tester . setSetupClient ( setupClient , host , opHost ) ; LOG . debug ( "SETUP_CLIENT is set in Tester." ) ; Preconditions . checkNotNull ( Tester . getAuthorization ( ) ) ; LOG . debug ( "Tester's authorization is set." ) ; setupSwaggerSuite ( Tester . getTargetHost ( host ) , opHost , redirectUrls ) ; LOG . debug ( "Finished beforeSuite!" ) ; } catch ( Exception e ) { LOG . error ( "Failed to start suite." , e ) ; throw new AssertionError ( "Failed to start suite." ) ; } }
public void test() { try { LOG . debug ( "Running beforeSuite ..." ) ; ServerLauncher . setSetUpSuite ( true ) ; SUPPORT = new DropwizardTestSupport < OxdServerConfiguration > ( OxdServerApplication . class , ResourceHelpers . resourceFilePath ( "oxd-server-jenkins.yml" ) , ConfigOverride . config ( "server.applicationConnectors[0].port" , "0" ) ) ; SUPPORT . before ( ) ; LOG . debug ( "HTTP server started." ) ; removeExistingRps ( ) ; LOG . debug ( "Existing RPs are removed." ) ; RegisterSiteResponse setupClient = SetupClientTest . setupClient ( Tester . newClient ( host ) , opHost , redirectUrls ) ; Tester . setSetupClient ( setupClient , host , opHost ) ; LOG . debug ( "SETUP_CLIENT is set in Tester." ) ; Preconditions . checkNotNull ( Tester . getAuthorization ( ) ) ; LOG . debug ( "Tester's authorization is set." ) ; setupSwaggerSuite ( Tester . getTargetHost ( host ) , opHost , redirectUrls ) ; LOG . debug ( "Finished beforeSuite!" ) ; } catch ( Exception e ) { LOG . error ( "Failed to start suite." , e ) ; throw new AssertionError ( "Failed to start suite." ) ; } }
public void test() { try { LOG . debug ( "Running beforeSuite ..." ) ; ServerLauncher . setSetUpSuite ( true ) ; SUPPORT = new DropwizardTestSupport < OxdServerConfiguration > ( OxdServerApplication . class , ResourceHelpers . resourceFilePath ( "oxd-server-jenkins.yml" ) , ConfigOverride . config ( "server.applicationConnectors[0].port" , "0" ) ) ; SUPPORT . before ( ) ; LOG . debug ( "HTTP server started." ) ; removeExistingRps ( ) ; LOG . debug ( "Existing RPs are removed." ) ; RegisterSiteResponse setupClient = SetupClientTest . setupClient ( Tester . newClient ( host ) , opHost , redirectUrls ) ; Tester . setSetupClient ( setupClient , host , opHost ) ; LOG . debug ( "SETUP_CLIENT is set in Tester." ) ; Preconditions . checkNotNull ( Tester . getAuthorization ( ) ) ; LOG . debug ( "Tester's authorization is set." ) ; setupSwaggerSuite ( Tester . getTargetHost ( host ) , opHost , redirectUrls ) ; LOG . debug ( "Finished beforeSuite!" ) ; } catch ( Exception e ) { LOG . error ( "Failed to start suite." , e ) ; throw new AssertionError ( "Failed to start suite." ) ; } }
public void test() { try { plugin = formatPlugin . createIngestFromHdfsPlugin ( formatOptions ) ; code_block = IfStatement ; } catch ( final UnsupportedOperationException e ) { LOGGER . warn ( "Plugin provider for ingest type '" + formatPlugin . getIngestFormatName ( ) + "' does not support hdfs ingest" , e ) ; continue ; } }
public void test() { if ( UserGroupInformation . isLoginKeytabBased ( ) ) { log . info ( "Performing keytab-based Kerberos re-login" ) ; loginUser . reloginFromKeytab ( ) ; } else { log . info ( "Performing ticket-cache-based Kerberos re-login" ) ; loginUser . reloginFromTicketCache ( ) ; } }
public void test() { if ( UserGroupInformation . isLoginKeytabBased ( ) ) { log . info ( "Performing keytab-based Kerberos re-login" ) ; loginUser . reloginFromKeytab ( ) ; } else { log . info ( "Performing ticket-cache-based Kerberos re-login" ) ; loginUser . reloginFromTicketCache ( ) ; } }
public void test() { if ( loginUser . equals ( currentUser ) || loginUser . equals ( realUser ) ) { code_block = IfStatement ; code_block = TryStatement ;  } else { log . debug ( "Not attempting Kerberos re-login: loginUser={}, currentUser={}, realUser={}" , loginUser , currentUser , realUser ) ; } }
public org . talend . mdm . webservice . WSTransformerV2PK putTransformerV2 ( org . talend . mdm . webservice . WSPutTransformerV2 arg0 ) { LOG . info ( "Executing operation putTransformerV2" ) ; System . out . println ( arg0 ) ; code_block = TryStatement ;  }
public void test() { if ( _logger . isDebugEnabled ( ) ) { String localAddress = "not connected" ; code_block = IfStatement ; _logger . debug ( "At " + methodName + " method, " + description + " [invoker address=" + localAddress + ", ServerEndPoint=" + getConnectionURL ( ) + "]" ) ; } }
public void test() { if ( _logger . isTraceEnabled ( ) ) { _logger . trace ( "At " + methodName + ", thread stack:" + StringUtils . NEW_LINE + StringUtils . getCurrentStackTrace ( ) ) ; } }
@ Override public void onError ( Throwable t ) { PingPongImpl . this . streamRequests . add ( streamRequests ) ; LOG . info ( "Error in pingAsyncAsync() {}" , t . getMessage ( ) ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "{} Stopping leadership election..." , logPrefix ( ) ) ; } }
public void test() { try { entry . getValue ( ) . onEvent ( target , ctx . getEvent ( ) , node , ctx ) ; } catch ( final RuntimeException ex ) { LOG . error ( "Failed while processing event" , ex ) ; } }
private void saveTranslationsInBatches ( List < HTextFlow > textFlows , MTDocument transDoc , HLocale targetLocale , ContentState saveState ) { int batchStart = 0 ; log . debug ( "[PERF] Saving {} translations in batches" , textFlows . size ( ) ) ; Stopwatch saveAllTimer = Stopwatch . createStarted ( ) ; code_block = WhileStatement ; log . debug ( "[PERF] Batches complete ({}ms)" , saveAllTimer ) ; }
public void test() { -> { List < TransUnitUpdateRequest > updateRequests = makeUpdateRequestsForBatch ( targetLocale , transDoc . getBackendId ( ) , saveState , sourceBatch , transContentBatch ) ; Stopwatch storeTranslations = Stopwatch . createStarted ( ) ; translationService . translate ( targetLocale . getLocaleId ( ) , updateRequests ) ; log . debug ( "[PERF] Commit translations to database ({}ms)" , storeTranslations ) ; } }
public void test() { try { transactionUtil . run ( ( ) code_block = LoopStatement ; ) ; } catch ( Exception e ) { log . error ( "Error filling target with machine translation" , e ) ; } }
public void test() { while ( batchStart < textFlows . size ( ) ) { int batchEnd = Math . min ( batchStart + SAVE_BATCH_SIZE , textFlows . size ( ) ) ; List < HTextFlow > sourceBatch = textFlows . subList ( batchStart , batchEnd ) ; List < TypeString > transContentBatch = transDoc . getContents ( ) . subList ( batchStart , batchEnd ) ; log . debug ( "[PERF] Batch save transaction {} - {}" , batchStart , batchEnd ) ; Stopwatch transactionTime = Stopwatch . createStarted ( ) ; code_block = TryStatement ;  entityManager . clear ( ) ; batchStart = batchEnd ; log . debug ( "[PERF] Transaction complete {} - {} ({}ms)" , batchStart , batchEnd , transactionTime ) ; } }
private void saveTranslationsInBatches ( List < HTextFlow > textFlows , MTDocument transDoc , HLocale targetLocale , ContentState saveState ) { int batchStart = 0 ; log . debug ( "[PERF] Saving {} translations in batches" , textFlows . size ( ) ) ; Stopwatch saveAllTimer = Stopwatch . createStarted ( ) ; code_block = WhileStatement ; log . debug ( "[PERF] Batches complete ({}ms)" , saveAllTimer ) ; }
public void test() { if ( this . currentAttemptCount < this . maxAttemptCount && ( retryDelay = checkIfRetryNeeded ( exception ) ) != null ) { this . currentAttemptCount ++ ; logger . warn ( "Operation will be retried after {} milliseconds. Current attempt {}, Cumulative delay {}" , retryDelay . toMillis ( ) , this . currentAttemptCount , this . cumulativeRetryDelay , exception ) ; return Single . just ( ShouldRetryResult . retryAfter ( retryDelay ) ) ; } else { logger . debug ( "Operation will NOT be retried. Current attempt {}" , this . currentAttemptCount , exception ) ; return Single . just ( ShouldRetryResult . noRetry ( ) ) ; } }
public void test() { if ( this . currentAttemptCount < this . maxAttemptCount && ( retryDelay = checkIfRetryNeeded ( exception ) ) != null ) { this . currentAttemptCount ++ ; logger . warn ( "Operation will be retried after {} milliseconds. Current attempt {}, Cumulative delay {}" , retryDelay . toMillis ( ) , this . currentAttemptCount , this . cumulativeRetryDelay , exception ) ; return Single . just ( ShouldRetryResult . retryAfter ( retryDelay ) ) ; } else { logger . debug ( "Operation will NOT be retried. Current attempt {}" , this . currentAttemptCount , exception ) ; return Single . just ( ShouldRetryResult . noRetry ( ) ) ; } }
public void test() { try { InetAddress inetAddress = InetAddress . getByName ( ipAddress . getHostAddress ( ) ) ; ret = inetAddress . isReachable ( tout ) ; logger . info ( "Access point reachable? {}" , ret ) ; } catch ( Exception e ) { throw new KuraException ( KuraErrorCode . INTERNAL_ERROR , e ) ; } }
public void test() { if ( wsSession . isOpen ( ) ) { wsSession . sendMessage ( new TextMessage ( jsonMessage ) ) ; } else { log . error ( "Trying to send a message to a closed session" ) ; } }
public void test() { try { String messageJson = message . getPayload ( ) ; ServerSessionFactory factory = new ServerSessionFactory ( ) code_block = "" ; ; protocolManager . processMessage ( messageJson , factory , new ResponseSender ( ) code_block = "" ; , wsSession . getId ( ) ) ; } catch ( Throwable t ) { log . error ( "{} Exception processing request {}." , label , message . getPayload ( ) , t ) ; } }
private void initializeFileSystem ( final IndexedDiskCacheAttributes cattr ) { this . rafDir = cattr . getDiskPath ( ) ; log . info ( "{0}: Cache file root directory: {1}" , logCacheName , rafDir ) ; }
public void test() { if ( hasSampling ) { String samplingPath = String . join ( "/" , samplingDir , "*.avro" ) ; log . info ( "Loading sampling from {}" , samplingPath ) ; return p . apply ( AvroIO . read ( SampleRecord . class ) . from ( samplingPath ) ) ; } else { return p . apply ( Create . empty ( AvroCoder . of ( SampleRecord . class ) ) ) ; } }
public void test() { try { final IpAddressTO [ ] ips = cmd . getIpAddresses ( ) ; code_block = ForStatement ; } catch ( final InternalErrorException e ) { s_logger . error ( "Ip Assoc failure on applying one ip due to exception:  " , e ) ; return new ExecutionResult ( false , e . getMessage ( ) ) ; } catch ( final Exception e ) { return new ExecutionResult ( false , e . getMessage ( ) ) ; } }
private void assertEqualPositions ( SourceFile [ ] sourceFiles , SourceFile sourceFile , String javaCodeSnippet , String tsCodeSnippet ) { logger . info ( "assert equal positions to '" + javaCodeSnippet + "' -> '" + tsCodeSnippet + "'" ) ; SourcePosition tsPosition = getPosition ( sourceFile . getTsFile ( ) , tsCodeSnippet ) ; SourcePosition javaPosition = SourceFile . findOriginPosition ( tsPosition , Arrays . asList ( sourceFiles ) ) ; logger . info ( "org: " + javaPosition + " --> " + tsPosition ) ; assertEquals ( getPosition ( sourceFile . getJavaFile ( ) , javaCodeSnippet ) . getStartLine ( ) , javaPosition . getStartLine ( ) ) ; }
private void assertEqualPositions ( SourceFile [ ] sourceFiles , SourceFile sourceFile , String javaCodeSnippet , String tsCodeSnippet ) { logger . info ( "assert equal positions to '" + javaCodeSnippet + "' -> '" + tsCodeSnippet + "'" ) ; SourcePosition tsPosition = getPosition ( sourceFile . getTsFile ( ) , tsCodeSnippet ) ; SourcePosition javaPosition = SourceFile . findOriginPosition ( tsPosition , Arrays . asList ( sourceFiles ) ) ; logger . info ( "org: " + javaPosition + " --> " + tsPosition ) ; assertEquals ( getPosition ( sourceFile . getJavaFile ( ) , javaCodeSnippet ) . getStartLine ( ) , javaPosition . getStartLine ( ) ) ; }
@ Override public void onCommunicationLost ( Association association ) { logger . warn ( String . format ( "Communication channel lost for AspFactroy=%s Association=%s" , this . name , association . getName ( ) ) ) ; this . handleCommDown ( ) ; }
public void test() { try { eventProcessorAdminServiceStub . setTracingEnabled ( executionPlanName , isEnabled ) ; } catch ( RemoteException e ) { log . error ( "RemoteException" , e ) ; throw e ; } }
public void test() { try { sleep ( flushInterval ) ; bookie . getSyncThread ( ) . checkpoint ( Checkpoint . MAX ) ; ledgerCache . flushLedger ( true ) ; } catch ( InterruptedException ie ) { Thread . currentThread ( ) . interrupt ( ) ; return ; } catch ( Exception e ) { LOG . error ( "Exception in flush thread" , e ) ; } }
public void test() { if ( this . properties . isVerbose ( ) ) { this . logger . warn ( LOG_DESCRIPTOR_FAILREAD , "Failed to read XAR descriptor from entry [{}]: {}" , entry . getName ( ) , ExceptionUtils . getRootCauseMessage ( e ) ) ; } }
public void test() { if ( this . properties . isVerbose ( ) ) { this . logger . info ( LOG_DOCUMENT_SKIPPED , "Skipped document [{}]" , skip . getEntityReference ( ) ) ; } }
public void test() { if ( this . properties . isVerbose ( ) ) { this . logger . warn ( LOG_DOCUMENT_FAILREAD , "Failed to read XAR XML document from entry [{}]: {}" , entry . getName ( ) , ExceptionUtils . getRootCauseMessage ( e ) , e ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Result in " + this . getClass ( ) . getCanonicalName ( ) + " injected from cache" ) ; } }
public void test() { try { insertTestCaseCountryProperties ( tccp ) ; } catch ( CerberusException ex ) { LOG . warn ( ex . toString ( ) ) ; return false ; } }
public void test() { if ( ip . getFailCount ( ) . incrementAndGet ( ) >= switchDomain . getCheckTimes ( ) ) { code_block = IfStatement ; } else { Loggers . EVT_LOG . info ( "serviceName: {} {OTHER} {IP-DISABLED} pre-invalid: {}:{}@{} in {}, msg: {}" , cluster . getService ( ) . getName ( ) , ip . getIp ( ) , ip . getPort ( ) , cluster . getName ( ) , ip . getFailCount ( ) , msg ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Throwable t ) { Loggers . SRV_LOG . error ( "[CHECK-FAIL] error when close check task." , t ) ; } }
public void test() { -> { List < String > current = TestUtils . listReadyPods ( Kubernetes . getInstance ( ) , SystemtestsKubernetesApps . SELENIUM_PROJECT ) . stream ( ) . map ( pod -> pod . getMetadata ( ) . getName ( ) ) . collect ( Collectors . toList ( ) ) ; current . removeAll ( beforeRestart ) ; log . info ( "Following pods are in ready state {}" , current ) ; return current . size ( ) == beforeRestart . size ( ) ; } }
public void test() { try { TestUtils . waitUntilCondition ( "Selenium pods ready" , ( phase ) code_block = LoopStatement ; , new TimeoutBudget ( 1 , TimeUnit . MINUTES ) ) ; break ; } catch ( Exception ex ) { log . warn ( "Selenium application was not redeployed correctly, try it again {}/{}" , i , attempts ) ; } }
public void test() { if ( LOGGER . isTraceEnabled ( ) ) { LOGGER . trace ( StringUtilities . formatTimingMessage ( "Added/Remove Geometries to registry[" + newVisibleGeomSet . size ( ) + "/" + newHiddenGeomSet . size ( ) + "] in " , System . nanoTime ( ) - start ) ) ; } }
public void test() { try { final String payload = String . format ( "grant_type=client_credentials&client_id=%s&client_secret=%s&resource=%s" , principal . getValue ( ) , URLEncoder . encode ( secret . getValue ( ) , "UTF-8" ) , URLEncoder . encode ( "https://graph.windows.net" , "UTF-8" ) ) ; final URL url = new URL ( String . format ( "https://login.microsoftonline.com/%s/oauth2/token" , tenant . getName ( ) ) ) ; final HttpsURLConnection connection = ( HttpsURLConnection ) url . openConnection ( ) ; connection . setRequestMethod ( "POST" ) ; connection . setRequestProperty ( "Host" , "login.microsoftonline.com" ) ; connection . setRequestProperty ( "Content-Type" , "application/x-www-form-urlencoded" ) ; connection . setRequestProperty ( "Accept" , "application/json" ) ; connection . setDoOutput ( true ) ; connection . getOutputStream ( ) . write ( payload . getBytes ( ) ) ; connection . getOutputStream ( ) . flush ( ) ; final StringBuilder result = new StringBuilder ( ) ; code_block = TryStatement ;  final ObjectMapper mapper = new ObjectMapper ( ) ; final JsonNode node = mapper . readValue ( result . toString ( ) . getBytes ( ) , JsonNode . class ) ; return node . get ( "access_token" ) . asText ( ) ; } catch ( IOException e ) { LOGGER . error ( "IO Exception" , e ) ; return null ; } catch ( RuntimeException e ) { LOGGER . error ( e . getMessage ( ) , e ) ; return null ; } }
public void test() { try { final String payload = String . format ( "grant_type=client_credentials&client_id=%s&client_secret=%s&resource=%s" , principal . getValue ( ) , URLEncoder . encode ( secret . getValue ( ) , "UTF-8" ) , URLEncoder . encode ( "https://graph.windows.net" , "UTF-8" ) ) ; final URL url = new URL ( String . format ( "https://login.microsoftonline.com/%s/oauth2/token" , tenant . getName ( ) ) ) ; final HttpsURLConnection connection = ( HttpsURLConnection ) url . openConnection ( ) ; connection . setRequestMethod ( "POST" ) ; connection . setRequestProperty ( "Host" , "login.microsoftonline.com" ) ; connection . setRequestProperty ( "Content-Type" , "application/x-www-form-urlencoded" ) ; connection . setRequestProperty ( "Accept" , "application/json" ) ; connection . setDoOutput ( true ) ; connection . getOutputStream ( ) . write ( payload . getBytes ( ) ) ; connection . getOutputStream ( ) . flush ( ) ; final StringBuilder result = new StringBuilder ( ) ; code_block = TryStatement ;  final ObjectMapper mapper = new ObjectMapper ( ) ; final JsonNode node = mapper . readValue ( result . toString ( ) . getBytes ( ) , JsonNode . class ) ; return node . get ( "access_token" ) . asText ( ) ; } catch ( IOException e ) { LOGGER . error ( "IO Exception" , e ) ; return null ; } catch ( RuntimeException e ) { LOGGER . error ( e . getMessage ( ) , e ) ; return null ; } }
public void subscribeEventChannel ( ) throws MagentaTVException { String sid = "" ; logger . debug ( "{}: Subscribe Event Channel (terminalID={}, {}:{}" , thingId , config . getTerminalID ( ) , config . getIpAddress ( ) , config . getPort ( ) ) ; String subscribe = MessageFormat . format ( PAIRING_SUBSCRIBE , config . getIpAddress ( ) , config . getPort ( ) , network . getLocalIP ( ) , network . getLocalPort ( ) , PAIRING_NOTIFY_URI , PAIRING_TIMEOUT_SEC ) ; String response = http . sendData ( config . getIpAddress ( ) , config . getPort ( ) , subscribe ) ; code_block = IfStatement ; code_block = IfStatement ; StringTokenizer tokenizer = new StringTokenizer ( response , "\r\n" ) ; code_block = WhileStatement ; }
public void test() { if ( ! notifiedRecords ) { logger . trace ( "Failed to notify success ({}) to a promise: {}" , result , promise ) ; } }
public void test() { if ( keyspaces . isEmpty ( ) ) { String message = format ( "No keyspace found in cluster %s" , cluster . getName ( ) ) ; LOG . debug ( message ) ; throw new IllegalArgumentException ( message ) ; } }
@ Override public void onResponse ( Result appResponse , Invoker < ? > invoker , Invocation invocation ) { logger . info ( "Filter get the value: " + appResponse . getValue ( ) ) ; }
public void test() { else { LOG . error ( "Unrecognized search sql: " + searchSql ) ; } }
public void test() { if ( requestedHost . getStatus ( ) == ResourceHostRegistrationStatus . REQUESTED && System . currentTimeMillis ( ) - ( requestedHost . getDateUpdated ( ) == null ? 0L : requestedHost . getDateUpdated ( ) ) > TimeUnit . MINUTES . toMillis ( 60 ) ) { LOG . warn ( "Deleting stale registration request {} : {}" , requestedHost . getHostname ( ) , requestedHost . getAddress ( ) ) ; requestDataService . remove ( requestedHost . getId ( ) ) ; } }
public void test() { try { AccountEntryUserRelServiceUtil . deleteAccountEntryUserRelByEmailAddress ( accountEntryId , emailAddress ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( logger . isTraceEnabled ( LogMarker . VERSIONED_OBJECT_LIST_VERBOSE ) ) { logger . trace ( LogMarker . VERSIONED_OBJECT_LIST_VERBOSE , "reading object {} of type {}" , index , objectTypeArray [ index ] ) ; } }
private void preLoadClass ( String agentId , long agentStartTimestamp , AgentStatMetricCollector < AgentStatMetricSnapshot > agentStatCollector ) { logger . debug ( "pre-load class start" ) ; CollectJob collectJob = new CollectJob ( EmptyDataSender . INSTANCE , agentId , agentStartTimestamp , agentStatCollector , 1 ) ; collectJob . run ( ) ; collectJob . run ( ) ; logger . debug ( "pre-load class end" ) ; }
public String computeDiffWithDiffCommand ( File directoryVersionOne , File directoryVersionTwo ) { LOGGER . info ( "Computing the diff with diff commnd line" ) ; LOGGER . info ( "The diff will be computed between:" ) ; LOGGER . info ( directoryVersionOne . getAbsolutePath ( ) + " and " ) ; LOGGER . info ( directoryVersionTwo . getAbsolutePath ( ) ) ; final String command = String . join ( " " , new String [ ] code_block = "" ; ) ; return this . executeCommand ( command , directoryVersionOne ) ; }
public String computeDiffWithDiffCommand ( File directoryVersionOne , File directoryVersionTwo ) { LOGGER . info ( "Computing the diff with diff commnd line" ) ; LOGGER . info ( "The diff will be computed between:" ) ; LOGGER . info ( directoryVersionOne . getAbsolutePath ( ) + " and " ) ; LOGGER . info ( directoryVersionTwo . getAbsolutePath ( ) ) ; final String command = String . join ( " " , new String [ ] code_block = "" ; ) ; return this . executeCommand ( command , directoryVersionOne ) ; }
public String computeDiffWithDiffCommand ( File directoryVersionOne , File directoryVersionTwo ) { LOGGER . info ( "Computing the diff with diff commnd line" ) ; LOGGER . info ( "The diff will be computed between:" ) ; LOGGER . info ( directoryVersionOne . getAbsolutePath ( ) + " and " ) ; LOGGER . info ( directoryVersionTwo . getAbsolutePath ( ) ) ; final String command = String . join ( " " , new String [ ] code_block = "" ; ) ; return this . executeCommand ( command , directoryVersionOne ) ; }
public String computeDiffWithDiffCommand ( File directoryVersionOne , File directoryVersionTwo ) { LOGGER . info ( "Computing the diff with diff commnd line" ) ; LOGGER . info ( "The diff will be computed between:" ) ; LOGGER . info ( directoryVersionOne . getAbsolutePath ( ) + " and " ) ; LOGGER . info ( directoryVersionTwo . getAbsolutePath ( ) ) ; final String command = String . join ( " " , new String [ ] code_block = "" ; ) ; return this . executeCommand ( command , directoryVersionOne ) ; }
@ Override public boolean create ( Personname personnameRecord ) { LOG . trace ( "PersonnameDAO.create() - Begin" ) ; return personnameRecord != null ? super . create ( personnameRecord ) : true ; }
public void test() { try ( EntityManagerContainer emc = EntityManagerContainerFactory . instance ( ) . create ( ) ) { _bbsConfigSetting = emc . find ( bbsConfigSetting . getId ( ) , BBSConfigSetting . class ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . warn ( "BBSConfigSetting update/ got a error!" ) ; throw e ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( configurationException , configurationException ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "user=" + userName + " password=" + passwd + " password length=" + passwd . getBytes ( ) . length ) ; } }
public void test() { if ( bom != null ) { log . info ( bom . toString ( ) ) ; } }
public void test() { try { wsSession . close ( new CloseStatus ( CloseStatus . NORMAL . getCode ( ) , reason ) ) ; } catch ( IOException e ) { log . warn ( "Exception closing webSocket session" , e ) ; } }
public void test() { try { connection2 . connect ( ) ; fail ( "Should not be able to connect with same container Id." ) ; } catch ( Exception ex ) { LOG . debug ( "Second connection with same container Id failed as expected." ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "QueueRemovalMessage: removing dispatched events on queue {} for {}" , regionName , id ) ; } }
public void test() { try { code_block = IfStatement ; hrq . removeDispatchedEvents ( id ) ; } catch ( RegionDestroyedException ignore ) { logger . info ( "Queue found destroyed while processing the last dispatched sequence ID for a HARegionQueue's DACE. The event ID is {} code_block = ForStatement ; ) ; } catch ( CancelException ignore ) { return false ; } catch ( CacheException e ) { logger . error ( String . format ( "QueueRemovalMessage::process:Exception in processing the last dispatched sequence ID for a HARegionQueue's DACE. The problem is with event ID, %s for HARegion with name=%s" , regionName , id ) , e ) ; } catch ( InterruptedException ignore ) { Thread . currentThread ( ) . interrupt ( ) ; return false ; } catch ( RejectedExecutionException ignore ) { interrupted = true ; } finally { code_block = IfStatement ; } }
public void test() { if ( sessionMd != null ) { sessionMd . sendPing ( currentTime ) ; } else { log . warn ( "[{}][{}] Failed to find session by internal id" , externalId , internalId ) ; } }
public void test() { if ( internalId != null ) { SessionMetaData sessionMd = internalSessionMap . get ( internalId ) ; code_block = IfStatement ; } else { log . warn ( "[{}] Failed to find session by external id" , externalId ) ; } }
public void test() { try { preDestroy . invoke ( obj ) ; } catch ( Exception e ) { log . error ( e , "Stopping %s.%s() failed:" , obj . getClass ( ) . getName ( ) , preDestroy . getName ( ) ) ; } }
public void test() { try { code_block = WhileStatement ; } catch ( final IOException e ) { LOGGER . error ( "Error on sending registry entry." , e ) ; } }
public void test() { try { setFailedStatus ( query , "Launching query failed" , e ) ; } catch ( LensException e1 ) { log . error ( "Error in setting failed status" , e1 ) ; } }
public void test() { try { release ( query . getLensSessionIdentifier ( ) ) ; } catch ( LensException e ) { log . error ( "Error releasing session" , e ) ; } }
public void test() { if ( showFull && logger . isDebugEnabled ( ) ) { logger . debug ( testName + ": returned payload:" ) ; logger . debug ( objectAsXmlString ( contact , ContactsCommon . class ) ) ; } }
public void test() { -> { LOGGER . info ( "[{}][doShutdown] Shutting down" , id ) ; return Mono . whenDelayError ( processBeforeDestroy ( ) , compositeDiscovery . shutdown ( ) , gatewayBootstrap . shutdown ( ) , transportBootstrap . shutdown ( ) ) . doOnSuccess ( s -> LOGGER . info ( "[{}][doShutdown] Shutdown" , id ) ) ; } }
public void test() { try { cf = readChemFile ( cf ) ; } catch ( IOException exception ) { String error = "Input/Output error while reading from input: " + exception . getMessage ( ) ; logger . error ( error ) ; logger . debug ( exception ) ; throw new CDKException ( error , exception ) ; } }
public void test() { if ( newest . size ( ) > 0 ) { ret = newest . get ( 0 ) . date ; logger . info ( "Moves: guestId=" + updateInfo . getGuestId ( ) + ", maxDateInDB=" + ret ) ; } else { logger . info ( "Moves: guestId=" + updateInfo . getGuestId ( ) + ", maxDateInDB=null" ) ; } }
public void test() { if ( newest . size ( ) > 0 ) { ret = newest . get ( 0 ) . date ; logger . info ( "Moves: guestId=" + updateInfo . getGuestId ( ) + ", maxDateInDB=" + ret ) ; } else { logger . info ( "Moves: guestId=" + updateInfo . getGuestId ( ) + ", maxDateInDB=null" ) ; } }
public void test() { switch ( fieldSchema . getType ( ) ) { case MAP : result = convertDocFieldToAvroMap ( docf , fieldSchema , obj , field , storeType ) ; break ; case ARRAY : result = convertDocFieldToAvroList ( docf , fieldSchema , obj , field , storeType ) ; break ; case RECORD : ODocument record = obj . field ( docf ) ; code_block = IfStatement ; result = convertAvroBeanToOrientDoc ( fieldSchema , record ) ; break ; case BOOLEAN : result = OType . convert ( obj . field ( docf ) , Boolean . class ) ; break ; case DOUBLE : result = OType . convert ( obj . field ( docf ) , Double . class ) ; break ; case FLOAT : result = OType . convert ( obj . field ( docf ) , Float . class ) ; break ; case INT : result = OType . convert ( obj . field ( docf ) , Integer . class ) ; break ; case LONG : result = OType . convert ( obj . field ( docf ) , Long . class ) ; break ; case STRING : result = convertDocFieldToAvroString ( storeType , docf , obj ) ; break ; case ENUM : result = AvroUtils . getEnumValue ( fieldSchema , obj . field ( docf ) ) ; break ; case BYTES : case FIXED : code_block = IfStatement ; result = ByteBuffer . wrap ( ( byte [ ] ) obj . field ( docf ) ) ; break ; case NULL : result = null ; break ; case UNION : result = convertDocFieldToAvroUnion ( fieldSchema , storeType , field , docf , obj ) ; break ; default : LOG . warn ( "Unable to read {}" , docf ) ; break ; } }
public void test() { try { code_block = ForStatement ; } catch ( Exception e ) { LOG . warn ( "Failed to take over file access count for all tables, " + "which may make the measurement for data temperature inaccurate!" , e . getMessage ( ) ) ; } }
public synchronized void deleteUser ( User user ) throws IOException { String username = user . getName ( ) ; log . info ( "Removing user {}" , user ) ; code_block = ForStatement ; users . remove ( username ) ; persistChanges ( ) ; }
public void test() { try { ctx . close ( ) ; } catch ( Exception ex1 ) { __log . error ( "Error closing JNDI connection." , ex1 ) ; } }
public void test() { try { log . info ( "Retrieving and copying persisted state to " + destinationDir + " with orphaned state deleted" ) ; PersistMode persistMode = PersistMode . REBIND ; HighAvailabilityMode highAvailabilityMode = HighAvailabilityMode . DISABLED ; launcher = BrooklynLauncher . newInstance ( ) . localBrooklynPropertiesFile ( localBrooklynProperties ) . brooklynProperties ( OsgiManager . USE_OSGI , false ) . persistMode ( persistMode ) . persistenceDir ( persistenceDir ) . persistenceLocation ( persistenceLocation ) . highAvailabilityMode ( highAvailabilityMode ) ; } catch ( FatalConfigurationRuntimeException e ) { throw e ; } catch ( Exception e ) { throw new FatalConfigurationRuntimeException ( "Fatal error configuring Brooklyn launch: " + e . getMessage ( ) , e ) ; } }
public void test() { try { launcher . cleanOrphanedState ( destinationDir , destinationLocation ) ; } catch ( FatalRuntimeException e ) { throw e ; } catch ( Exception e ) { Exceptions . propagateIfFatal ( e ) ; log . error ( "Error retrieving persisted state: " + Exceptions . collapseText ( e ) , e ) ; Exceptions . propagate ( e ) ; } finally { code_block = TryStatement ;  } }
public void test() { try { launcher . terminate ( ) ; } catch ( Exception e2 ) { log . warn ( "Subsequent error during termination: " + e2 ) ; log . debug ( "Details of subsequent error during termination: " + e2 , e2 ) ; } }
@ Override public void start ( ) { logger . info ( "Starting {}..." , this ) ; sinkCounter . start ( ) ; super . start ( ) ; pathController . setBaseDirectory ( directory ) ; code_block = IfStatement ; logger . info ( "RollingFileSink {} started." , getName ( ) ) ; }
public void test() { if ( rollInterval > 0 ) { rollService = Executors . newScheduledThreadPool ( 1 , new ThreadFactoryBuilder ( ) . setNameFormat ( "rollingFileSink-roller-" + Thread . currentThread ( ) . getId ( ) + "-%d" ) . build ( ) ) ; rollService . scheduleAtFixedRate ( new Runnable ( ) code_block = "" ; , rollInterval , rollInterval , TimeUnit . SECONDS ) ; } else { logger . info ( "RollInterval is not valid, file rolling will not happen." ) ; } }
@ Override public void start ( ) { logger . info ( "Starting {}..." , this ) ; sinkCounter . start ( ) ; super . start ( ) ; pathController . setBaseDirectory ( directory ) ; code_block = IfStatement ; logger . info ( "RollingFileSink {} started." , getName ( ) ) ; }
public void test() { try { return Optional . of ( user . asMailAddress ( ) ) ; } catch ( AddressException e ) { LOGGER . warn ( "Ignoring failing MappingSource to MailAddress conversion for user {}" , user , e ) ; return Optional . empty ( ) ; } }
public void test() { try { LOGGER . debug ( "Requesting certificate verification." ) ; Callback [ ] callbacks = new Callback [ 1 ] ; callbacks [ 0 ] = callback ; handler . handle ( callbacks ) ; } catch ( UnsupportedCallbackException e ) { LOGGER . debug ( "Certificate verification failed." ) ; throw new ClientException ( e ) ; } catch ( IOException e ) { throw new ClientException ( e ) ; } }
public void test() { if ( ! callback . isVerified ( ) ) { LOGGER . debug ( "Certificate verification failed." ) ; throw new ClientException ( "CA certificate fingerprint could not be verified." ) ; } else { LOGGER . debug ( "Certificate verification passed." ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( SharingEntryServiceUtil . class , "deleteSharingEntry" , _deleteSharingEntryParameterTypes2 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , sharingEntryId , serviceContext ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . sharing . model . SharingEntry ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
@ Secured ( ServicesData . ROLE_GET_INGEST ) @ GetMapping ( CommonConstants . PATH_ID ) public LogbookOperationDto getOne ( @ PathVariable ( "id" ) final String id ) { LOGGER . debug ( "get One Ingest id={}" , id ) ; ParameterChecker . checkParameter ( "The Identifier is a mandatory parameter: " , id ) ; return ingestExternalService . getOne ( id ) ; }
@ Test @ Order ( 1 ) void sampleBook_shouldHave_injectionPointsResolved ( ) { log . debug ( "TEST 1 ENTERING" ) ; val book = getSampleBook ( ) ; assertTrue ( book . hasInjectionPointsResolved ( ) ) ; log . debug ( "TEST 1 EXITING" ) ; }
@ Test @ Order ( 1 ) void sampleBook_shouldHave_injectionPointsResolved ( ) { log . debug ( "TEST 1 ENTERING" ) ; val book = getSampleBook ( ) ; assertTrue ( book . hasInjectionPointsResolved ( ) ) ; log . debug ( "TEST 1 EXITING" ) ; }
public void test() { try { code_block = WhileStatement ; } catch ( InterruptedException ex ) { LOG . warn ( "shutdown interrupted on " + execService , ex ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { log . warn ( "Compaction failed for {} on {}" , compactable . getExtent ( ) , getJob ( ) , e ) ; status . compareAndSet ( Status . RUNNING , Status . FAILED ) ; } finally { status . compareAndSet ( Status . RUNNING , Status . COMPLETE ) ; } }
protected InputStream getConfigurationInputStream ( String resource ) throws HibernateException { log . info ( "Configuration resource: " + resource ) ; return ConfigHelper . getResourceAsStream ( resource ) ; }
@ Test public void testBadPassword_PreHashedServer ( ) throws Exception { Map < String , Object > serverProps = new HashMap < String , Object > ( ) ; serverProps . put ( REALM_PROPERTY , "TestRealm" ) ; serverProps . put ( PRE_DIGESTED_PROPERTY , "true" ) ; SaslServer server = new SaslServerBuilder ( DigestServerFactory . class , DIGEST ) . setUserName ( "George" ) . setPassword ( DigestPassword . ALGORITHM_DIGEST_MD5 , getDigestKeySpec ( "George" , "gpwd" , "TestRealm" ) ) . setProperties ( serverProps ) . setProtocol ( "TestProtocol" ) . setServerName ( "TestServer" ) . build ( ) ; CallbackHandler clientCallback = createClearPwdClientCallbackHandler ( "George" , "bad" , null ) ; SaslClient client = Sasl . createSaslClient ( new String [ ] code_block = "" ; , "George" , "TestProtocol" , "TestServer" , Collections . < String , Object > emptyMap ( ) , clientCallback ) ; assertFalse ( client . hasInitialResponse ( ) ) ; byte [ ] message = server . evaluateResponse ( new byte [ 0 ] ) ; log . debug ( "Challenge:" + new String ( message , StandardCharsets . ISO_8859_1 ) ) ; message = client . evaluateChallenge ( message ) ; log . debug ( "Client response:" + new String ( message , StandardCharsets . ISO_8859_1 ) ) ; code_block = TryStatement ;  }
@ Test public void testBadPassword_PreHashedServer ( ) throws Exception { Map < String , Object > serverProps = new HashMap < String , Object > ( ) ; serverProps . put ( REALM_PROPERTY , "TestRealm" ) ; serverProps . put ( PRE_DIGESTED_PROPERTY , "true" ) ; SaslServer server = new SaslServerBuilder ( DigestServerFactory . class , DIGEST ) . setUserName ( "George" ) . setPassword ( DigestPassword . ALGORITHM_DIGEST_MD5 , getDigestKeySpec ( "George" , "gpwd" , "TestRealm" ) ) . setProperties ( serverProps ) . setProtocol ( "TestProtocol" ) . setServerName ( "TestServer" ) . build ( ) ; CallbackHandler clientCallback = createClearPwdClientCallbackHandler ( "George" , "bad" , null ) ; SaslClient client = Sasl . createSaslClient ( new String [ ] code_block = "" ; , "George" , "TestProtocol" , "TestServer" , Collections . < String , Object > emptyMap ( ) , clientCallback ) ; assertFalse ( client . hasInitialResponse ( ) ) ; byte [ ] message = server . evaluateResponse ( new byte [ 0 ] ) ; log . debug ( "Challenge:" + new String ( message , StandardCharsets . ISO_8859_1 ) ) ; message = client . evaluateChallenge ( message ) ; log . debug ( "Client response:" + new String ( message , StandardCharsets . ISO_8859_1 ) ) ; code_block = TryStatement ;  }
@ Override public int indexOf ( ISingleElectron electron ) { logger . debug ( "Getting index of single electron: " , electron ) ; return super . indexOf ( electron ) ; }
public void test() { try { StorageUtils . writeInOut ( fis , fos , true ) ; return tmp ; } catch ( Exception e ) { LOGGER . error ( "Error writing uploaded File in temp file" , e ) ; return null ; } finally { fos . close ( ) ; fis . close ( ) ; } }
public static LdapContext getWiredContext ( LdapServer ldapServer , String principalDn , String password ) throws NamingException { LOG . debug ( "Creating a wired context to local LDAP server on port {}" , ldapServer . getPort ( ) ) ; Hashtable < String , String > env = new Hashtable < > ( ) ; env . put ( Context . INITIAL_CONTEXT_FACTORY , CTX_FACTORY ) ; env . put ( Context . PROVIDER_URL , Network . ldapLoopbackUrl ( ldapServer . getPort ( ) ) ) ; env . put ( Context . SECURITY_PRINCIPAL , principalDn ) ; env . put ( Context . SECURITY_CREDENTIALS , password ) ; env . put ( Context . SECURITY_AUTHENTICATION , "simple" ) ; return new InitialLdapContext ( env , null ) ; }
public void test() { if ( i % 1000 == 0 ) { logger . debug ( "simpleReadUUID() Created {} entities" , i ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( AssetListEntryServiceUtil . class , "getAssetListEntriesCount" , _getAssetListEntriesCountParameterTypes18 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( ( Integer ) returnObj ) . intValue ( ) ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
@ GET @ Timed @ Compress @ Produces ( APPLICATION_JSON_WITH_CHARSET ) public String list ( @ Context GraphManager manager , @ PathParam ( "graph" ) String graph , @ QueryParam ( "ids" ) List < String > stringIds ) { LOG . debug ( "Graph [{}] get edges by ids: {}" , graph , stringIds ) ; E . checkArgument ( stringIds != null && ! stringIds . isEmpty ( ) , "The ids parameter can't be null or empty" ) ; Object [ ] ids = new Id [ stringIds . size ( ) ] ; code_block = ForStatement ; HugeGraph g = graph ( manager , graph ) ; Iterator < Edge > edges = g . edges ( ids ) ; return manager . serializer ( g ) . writeEdges ( edges , false ) ; }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( "Stopping periodic Server Refreshes." ) ; } }
public void test() { try { String json = _OBJECT_MAPPER . writeValueAsString ( regionModels ) ; return Response . ok ( json , MediaType . APPLICATION_JSON ) . build ( ) ; } catch ( JsonProcessingException jsonProcessingException ) { _log . error ( jsonProcessingException , jsonProcessingException ) ; } }
public void test() { try { FileStatus fileStatus = fileSystem . getFileStatus ( path ) ; code_block = IfStatement ; } catch ( FileNotFoundException e ) { Path parent = path . getParent ( ) ; code_block = IfStatement ; } catch ( InterruptedIOException ioe ) { throw ioe ; } catch ( Exception e ) { log . error ( "Failure to validate path " + path , e ) ; } }
public void test() { if ( StringUtils . isEmpty ( System . getenv ( "DART_POST_PROCESS_FILE" ) ) ) { LOGGER . info ( "Environment variable DART_POST_PROCESS_FILE not defined so the Dart code may not be properly formatted. To define it, try `export DART_POST_PROCESS_FILE=\"/usr/local/bin/dartfmt -w\"` (Linux/Mac)" ) ; LOGGER . info ( "NOTE: To enable file post-processing, 'enablePostProcessFile' must be set to `true` (--enable-post-process-file for CLI)." ) ; } }
public void test() { if ( ! additionalProperties . containsKey ( CLIENT_NAME ) ) { final String name = org . openapitools . codegen . utils . StringUtils . camelize ( pubName ) ; additionalProperties . put ( CLIENT_NAME , name ) ; LOGGER . debug ( "Client name not set, using default {}" , DATE_LIBRARY_DEFAULT ) ; } }
public void test() { try { synchronized ( solrServer ) code_block = "" ; } catch ( Exception e ) { logger . warn ( "Could not delete from index series {}: {}" , id , e . getMessage ( ) ) ; } }
public void test() { try { addEntryEventSubscription ( LINK_CHANGED , layerizedId ) ; addEntryEventSubscription ( FLOW_CHANGED , layerizedId ) ; addEntryEventSubscription ( OUT_PACKET_ADDED , layerizedId ) ; String attrBase = AttrElements . ATTRIBUTES + SEPARATOR + "%s" ; ArrayList < String > portAttributes = new ArrayList < String > ( Arrays . asList ( String . format ( attrBase , AttrElements . UNRESERVED_BANDWIDTH ) , String . format ( attrBase , AttrElements . IS_BOUNDARY ) ) ) ; updateEntryEventSubscription ( PORT_CHANGED , layerizedId , portAttributes ) ; ArrayList < String > linkAttributes = new ArrayList < String > ( Arrays . asList ( String . format ( attrBase , AttrElements . COST ) , String . format ( attrBase , AttrElements . REQ_LATENCY ) , String . format ( attrBase , AttrElements . UNRESERVED_BANDWIDTH ) , String . format ( attrBase , AttrElements . REQ_BANDWIDTH ) ) ) ; updateEntryEventSubscription ( LINK_CHANGED , layerizedId , linkAttributes ) ; ArrayList < String > flowAttributes = new ArrayList < String > ( Arrays . asList ( NetworkElements . OWNER , NetworkElements . ENABLED , NetworkElements . PRIORITY , String . format ( attrBase , AttrElements . BANDWIDTH ) , String . format ( attrBase , AttrElements . LATENCY ) ) ) ; updateEntryEventSubscription ( FLOW_CHANGED , layerizedId , flowAttributes ) ; applyEventSubscription ( ) ; } catch ( Exception ex ) { log . error ( ex . getMessage ( ) , ex ) ; } }
public void test() { if ( ! c . equals ( b ) ) { logger . error ( "map entry wrong " + e + " to " + c + " old " + b ) ; } }
public void test() { if ( this . locationCache . shouldRefreshEndpoints ( canRefreshInBackground ) ) { logger . debug ( "shouldRefreshEndpoints: true" ) ; code_block = IfStatement ; code_block = IfStatement ; this . isRefreshing . set ( false ) ; return Completable . complete ( ) ; } else { logger . debug ( "shouldRefreshEndpoints: false, nothing to do." ) ; this . isRefreshing . set ( false ) ; return Completable . complete ( ) ; } }
public void test() { if ( this . locationCache . shouldRefreshEndpoints ( canRefreshInBackground ) ) { logger . debug ( "shouldRefreshEndpoints: true" ) ; code_block = IfStatement ; code_block = IfStatement ; this . isRefreshing . set ( false ) ; return Completable . complete ( ) ; } else { logger . debug ( "shouldRefreshEndpoints: false, nothing to do." ) ; this . isRefreshing . set ( false ) ; return Completable . complete ( ) ; } }
public void test() { try { logger . debug ( "deleting temp file : " + tempFile . getName ( ) ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "can not delete temp file : " + e . getMessage ( ) ) ; } }
public void setScannerFactory ( ScannerFactory scannerFactory ) { log . debug ( "Setting scanner factory on ShardQueryLogic: " + System . identityHashCode ( this ) + ".setScannerFactory(" + System . identityHashCode ( scannerFactory ) + ')' ) ; this . scannerFactory = scannerFactory ; }
public void test() { if ( bridge == null ) { updateStatus ( ThingStatus . OFFLINE , ThingStatusDetail . CONFIGURATION_ERROR , "No bridge configured" ) ; logger . debug ( "Initialized device state for shade {} {}" , ThingStatus . OFFLINE , ThingStatusDetail . CONFIGURATION_ERROR ) ; } else-if ( bridge . getStatus ( ) == ThingStatus . ONLINE ) { updateStatus ( ThingStatus . ONLINE ) ; logger . debug ( "Initialized device state for shade {}" , ThingStatus . ONLINE ) ; } else { updateStatus ( ThingStatus . OFFLINE , ThingStatusDetail . BRIDGE_OFFLINE ) ; logger . debug ( "Initialized device state for shade {} {}" , ThingStatus . OFFLINE , ThingStatusDetail . BRIDGE_OFFLINE ) ; } }
public void test() { if ( bridge == null ) { updateStatus ( ThingStatus . OFFLINE , ThingStatusDetail . CONFIGURATION_ERROR , "No bridge configured" ) ; logger . debug ( "Initialized device state for shade {} {}" , ThingStatus . OFFLINE , ThingStatusDetail . CONFIGURATION_ERROR ) ; } else-if ( bridge . getStatus ( ) == ThingStatus . ONLINE ) { updateStatus ( ThingStatus . ONLINE ) ; logger . debug ( "Initialized device state for shade {}" , ThingStatus . ONLINE ) ; } else { updateStatus ( ThingStatus . OFFLINE , ThingStatusDetail . BRIDGE_OFFLINE ) ; logger . debug ( "Initialized device state for shade {} {}" , ThingStatus . OFFLINE , ThingStatusDetail . BRIDGE_OFFLINE ) ; } }
public void test() { if ( bridge == null ) { updateStatus ( ThingStatus . OFFLINE , ThingStatusDetail . CONFIGURATION_ERROR , "No bridge configured" ) ; logger . debug ( "Initialized device state for shade {} {}" , ThingStatus . OFFLINE , ThingStatusDetail . CONFIGURATION_ERROR ) ; } else-if ( bridge . getStatus ( ) == ThingStatus . ONLINE ) { updateStatus ( ThingStatus . ONLINE ) ; logger . debug ( "Initialized device state for shade {}" , ThingStatus . ONLINE ) ; } else { updateStatus ( ThingStatus . OFFLINE , ThingStatusDetail . BRIDGE_OFFLINE ) ; logger . debug ( "Initialized device state for shade {} {}" , ThingStatus . OFFLINE , ThingStatusDetail . BRIDGE_OFFLINE ) ; } }
public static void main ( String [ ] args ) { logger . info ( "==> Starting: LambdaApplication" ) ; code_block = IfStatement ; SpringApplication . run ( LambdaApplication . class , args ) ; }
public void test() { if ( ! ObjectUtils . isEmpty ( args ) ) { logger . info ( "==>  args: " + Arrays . asList ( args ) ) ; } }
public void test() { try { final JwtParser jwtParser = Jwts . parser ( ) . setSigningKey ( key ) ; final Jws < Claims > claims = jwtParser . parseClaimsJws ( token ) ; this . subject = claims . getBody ( ) . getSubject ( ) ; this . expiration = claims . getBody ( ) . getExpiration ( ) ; this . identityProvider = IdentityProvider . forName ( claims . getBody ( ) . get ( IDENTITY_PROVIDER_CLAIM , String . class ) ) ; return true ; } catch ( SignatureException e ) { LOGGER . info ( SecurityMarkers . SECURITY_FAILURE , "Received token that did not pass signature verification" ) ; } catch ( ExpiredJwtException e ) { LOGGER . debug ( SecurityMarkers . SECURITY_FAILURE , "Received expired token" ) ; } catch ( MalformedJwtException e ) { LOGGER . debug ( SecurityMarkers . SECURITY_FAILURE , "Received malformed token" ) ; LOGGER . debug ( SecurityMarkers . SECURITY_FAILURE , e . getMessage ( ) ) ; } catch ( UnsupportedJwtException | IllegalArgumentException e ) { LOGGER . error ( SecurityMarkers . SECURITY_FAILURE , e . getMessage ( ) ) ; } }
public void test() { try { final JwtParser jwtParser = Jwts . parser ( ) . setSigningKey ( key ) ; final Jws < Claims > claims = jwtParser . parseClaimsJws ( token ) ; this . subject = claims . getBody ( ) . getSubject ( ) ; this . expiration = claims . getBody ( ) . getExpiration ( ) ; this . identityProvider = IdentityProvider . forName ( claims . getBody ( ) . get ( IDENTITY_PROVIDER_CLAIM , String . class ) ) ; return true ; } catch ( SignatureException e ) { LOGGER . info ( SecurityMarkers . SECURITY_FAILURE , "Received token that did not pass signature verification" ) ; } catch ( ExpiredJwtException e ) { LOGGER . debug ( SecurityMarkers . SECURITY_FAILURE , "Received expired token" ) ; } catch ( MalformedJwtException e ) { LOGGER . debug ( SecurityMarkers . SECURITY_FAILURE , "Received malformed token" ) ; LOGGER . debug ( SecurityMarkers . SECURITY_FAILURE , e . getMessage ( ) ) ; } catch ( UnsupportedJwtException | IllegalArgumentException e ) { LOGGER . error ( SecurityMarkers . SECURITY_FAILURE , e . getMessage ( ) ) ; } }
public void test() { try { final JwtParser jwtParser = Jwts . parser ( ) . setSigningKey ( key ) ; final Jws < Claims > claims = jwtParser . parseClaimsJws ( token ) ; this . subject = claims . getBody ( ) . getSubject ( ) ; this . expiration = claims . getBody ( ) . getExpiration ( ) ; this . identityProvider = IdentityProvider . forName ( claims . getBody ( ) . get ( IDENTITY_PROVIDER_CLAIM , String . class ) ) ; return true ; } catch ( SignatureException e ) { LOGGER . info ( SecurityMarkers . SECURITY_FAILURE , "Received token that did not pass signature verification" ) ; } catch ( ExpiredJwtException e ) { LOGGER . debug ( SecurityMarkers . SECURITY_FAILURE , "Received expired token" ) ; } catch ( MalformedJwtException e ) { LOGGER . debug ( SecurityMarkers . SECURITY_FAILURE , "Received malformed token" ) ; LOGGER . debug ( SecurityMarkers . SECURITY_FAILURE , e . getMessage ( ) ) ; } catch ( UnsupportedJwtException | IllegalArgumentException e ) { LOGGER . error ( SecurityMarkers . SECURITY_FAILURE , e . getMessage ( ) ) ; } }
public void test() { try { final JwtParser jwtParser = Jwts . parser ( ) . setSigningKey ( key ) ; final Jws < Claims > claims = jwtParser . parseClaimsJws ( token ) ; this . subject = claims . getBody ( ) . getSubject ( ) ; this . expiration = claims . getBody ( ) . getExpiration ( ) ; this . identityProvider = IdentityProvider . forName ( claims . getBody ( ) . get ( IDENTITY_PROVIDER_CLAIM , String . class ) ) ; return true ; } catch ( SignatureException e ) { LOGGER . info ( SecurityMarkers . SECURITY_FAILURE , "Received token that did not pass signature verification" ) ; } catch ( ExpiredJwtException e ) { LOGGER . debug ( SecurityMarkers . SECURITY_FAILURE , "Received expired token" ) ; } catch ( MalformedJwtException e ) { LOGGER . debug ( SecurityMarkers . SECURITY_FAILURE , "Received malformed token" ) ; LOGGER . debug ( SecurityMarkers . SECURITY_FAILURE , e . getMessage ( ) ) ; } catch ( UnsupportedJwtException | IllegalArgumentException e ) { LOGGER . error ( SecurityMarkers . SECURITY_FAILURE , e . getMessage ( ) ) ; } }
public void test() { try { final JwtParser jwtParser = Jwts . parser ( ) . setSigningKey ( key ) ; final Jws < Claims > claims = jwtParser . parseClaimsJws ( token ) ; this . subject = claims . getBody ( ) . getSubject ( ) ; this . expiration = claims . getBody ( ) . getExpiration ( ) ; this . identityProvider = IdentityProvider . forName ( claims . getBody ( ) . get ( IDENTITY_PROVIDER_CLAIM , String . class ) ) ; return true ; } catch ( SignatureException e ) { LOGGER . info ( SecurityMarkers . SECURITY_FAILURE , "Received token that did not pass signature verification" ) ; } catch ( ExpiredJwtException e ) { LOGGER . debug ( SecurityMarkers . SECURITY_FAILURE , "Received expired token" ) ; } catch ( MalformedJwtException e ) { LOGGER . debug ( SecurityMarkers . SECURITY_FAILURE , "Received malformed token" ) ; LOGGER . debug ( SecurityMarkers . SECURITY_FAILURE , e . getMessage ( ) ) ; } catch ( UnsupportedJwtException | IllegalArgumentException e ) { LOGGER . error ( SecurityMarkers . SECURITY_FAILURE , e . getMessage ( ) ) ; } }
public void test() { for ( AugmentedRow ar : extractedAugmentedRows ) { ar . setCommitTimestamp ( commitTimestamp ) ; ar . setTransactionSequenceNumber ( transactionSequenceNumber ) ; Long microsOverride = commitTimestamp * 1000 + ar . getTransactionSequenceNumber ( ) ; LOG . debug ( String . format ( "table : %s, UUID: %s, commit-ts: %d, seq-no: %d, micro-ts: %d" , ar . getTableName ( ) , ar . getTransactionUUID ( ) , commitTimestamp , transactionSequenceNumber , microsOverride ) ) ; ar . setRowMicrosecondTimestamp ( microsOverride ) ; } }
public void test() { for ( int i = 0 ; i < this . runs ; i ++ ) { this . logger . info ( fmt , i , i * 7L , i / 16.0 ) ; } }
public void test() { for ( Pair < AnnotationKey > annotationKey : annotationKeys ) { logger . info ( annotationKeyPairToString ( annotationKey ) ) ; } }
public void delete ( StgMUmsetzStatTxt persistentInstance ) { log . debug ( "deleting StgMUmsetzStatTxt instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . delete ( persistentInstance ) ; log . debug ( "delete successful" ) ; } catch ( RuntimeException re ) { log . error ( "delete failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . delete ( persistentInstance ) ; log . debug ( "delete successful" ) ; } catch ( RuntimeException re ) { log . error ( "delete failed" , re ) ; throw re ; } }
public void stop ( ) { clientController . stopClient ( false ) ; if ( executor == null ) return ; serverChannel . unbind ( ) ; serverSecureChannel . unbind ( ) ; serverChannel . close ( ) ; serverSecureChannel . close ( ) ; serverChannel . getCloseFuture ( ) . awaitUninterruptibly ( ) ; serverSecureChannel . getCloseFuture ( ) . awaitUninterruptibly ( ) ; executor . shutdownNow ( ) ; executor = null ; serverBootstrap . shutdown ( ) ; serverSecureBootstrap . shutdown ( ) ; nioServerSocketChannelFactory . shutdown ( ) ; logger . info ( "HTTP server stoped : " + node ) ; }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( Thread . currentThread ( ) . getName ( ) + " points:" + uploadCounter . get ( ) + " uploaded before error, now  release the lock." ) ; } }
public void test() { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( Thread . currentThread ( ) . getName ( ) + " no more points, total points:" + uploadCounter . get ( ) + " uploaded, now release the lock." ) ; } }
public void onReleased ( String serviceName ) { log . info ( "released {}" , serviceName ) ; }
public void test() { if ( distanceTolerance < MIN_DISTANCE_TOLERANCE_IN_METERS ) { this . distanceTolerance = distanceTolerance ; LOGGER . debug ( "Admin supplied distance tolerance is too low. Defaulting to the minimum of {} meter" , MIN_DISTANCE_TOLERANCE_IN_METERS ) ; } else { this . distanceTolerance = distanceTolerance ; } }
public void test() { try { attMap . put ( "id.documentId" , documentNominalLabel . getIdDTO ( ) . getDocumentId ( ) ) ; attMap . put ( "id.nominalLabelId" , documentNominalLabel . getIdDTO ( ) . getNominalLabelId ( ) ) ; } catch ( PropertyNotSetException e ) { logger . warn ( "Warning! duplication nominal label" ) ; } }
public void test() { if ( JoalAudioFactory . checkALError ( ) ) { log . warn ( "Error updating JoalAudioSource ({})" , this . getSystemName ( ) ) ; } }
public void test() { try { termsPolicyResponse = appMetaDataOrchestration . termsPolicy ( ) ; code_block = IfStatement ; } catch ( Exception e ) { LOGGER . error ( "StudyMetaDataService - termsPolicy() :: ERROR" , e ) ; StudyMetaDataUtil . getFailureResponse ( ErrorCodes . STATUS_104 , ErrorCodes . UNKNOWN , StudyMetaDataConstants . FAILURE , response ) ; return Response . status ( Response . Status . INTERNAL_SERVER_ERROR ) . entity ( StudyMetaDataConstants . FAILURE ) . build ( ) ; } }
public void test() { try { Files . copy ( file , dest , StandardCopyOption . REPLACE_EXISTING ) ; } catch ( Exception ex ) { LOG . error ( ex . getMessage ( ) , ex ) ; } }
public void test() { try { future . get ( ) ; } catch ( InterruptedException | ExecutionException ex ) { Log . fatal ( ex ) ; } }
public void test() { try { sink . process ( ) ; failed = false ; } catch ( EventDeliveryException ex ) { logger . info ( "Correctly failed to send event" , ex ) ; failed = true ; } }
@ Before public void createClient ( ) throws IOException { final SshClient client = SshClient . setUpDefaultClient ( ) ; client . setForwardingFilter ( AcceptAllForwardingFilter . INSTANCE ) ; client . start ( ) ; LOG . debug ( "Connecting..." ) ; session = client . connect ( "user" , TEST_LOCALHOST , sshServerPort ) . verify ( CONNECT_TIMEOUT ) . getSession ( ) ; LOG . debug ( "Authenticating..." ) ; session . addPasswordIdentity ( "foo" ) ; session . auth ( ) . verify ( AUTH_TIMEOUT ) ; LOG . debug ( "Authenticated" ) ; }
@ Before public void createClient ( ) throws IOException { final SshClient client = SshClient . setUpDefaultClient ( ) ; client . setForwardingFilter ( AcceptAllForwardingFilter . INSTANCE ) ; client . start ( ) ; LOG . debug ( "Connecting..." ) ; session = client . connect ( "user" , TEST_LOCALHOST , sshServerPort ) . verify ( CONNECT_TIMEOUT ) . getSession ( ) ; LOG . debug ( "Authenticating..." ) ; session . addPasswordIdentity ( "foo" ) ; session . auth ( ) . verify ( AUTH_TIMEOUT ) ; LOG . debug ( "Authenticated" ) ; }
@ Before public void createClient ( ) throws IOException { final SshClient client = SshClient . setUpDefaultClient ( ) ; client . setForwardingFilter ( AcceptAllForwardingFilter . INSTANCE ) ; client . start ( ) ; LOG . debug ( "Connecting..." ) ; session = client . connect ( "user" , TEST_LOCALHOST , sshServerPort ) . verify ( CONNECT_TIMEOUT ) . getSession ( ) ; LOG . debug ( "Authenticating..." ) ; session . addPasswordIdentity ( "foo" ) ; session . auth ( ) . verify ( AUTH_TIMEOUT ) ; LOG . debug ( "Authenticated" ) ; }
public void test() { if ( ActiveMQRALogger . LOGGER . isTraceEnabled ( ) ) { ActiveMQRALogger . LOGGER . trace ( "createTextMessage" + session ) ; } }
@ Override public void onClose ( ) { log . info ( "Watch closed" ) ; doWakeup ( ) ; }
private Response notifyInPacketAdded ( Packet inpacket ) throws Exception { log . debug ( "" ) ; InPacketAdded msg = new InPacketAdded ( inpacket ) ; return postEvent ( InPacketAdded . TYPE , msg ) ; }
public void test() { try { VerbRef verbRef = _mohoRemote . getRayoClient ( ) . output ( text , this . getId ( ) ) ; output = new OutputImpl < T > ( verbRef , this , ( T ) this ) ; } catch ( XmppException e ) { LOG . error ( "" , e ) ; throw new MediaException ( e ) ; } finally { getComponentstLock ( ) . unlock ( ) ; } }
@ Test public void testProcessFlagsOrder ( ) throws Exception { log . info ( "-----  testProcessFlagsOrder  -----" ) ; setUpFlagDir ( ) ; createTestFiles ( 2 , 5 ) ; final List < Collection < InputFile > > flagFileLists = new ArrayList < > ( ) ; FlagMaker instance = new TestWrappedFlagMaker ( fmc ) code_block = "" ; ; instance . processFlags ( ) ; assertEquals ( 2 , flagFileLists . size ( ) ) ; long lastTime = 0 ; code_block = ForStatement ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Complete topology event received" ) ; } }
public void test() { if ( log . isErrorEnabled ( ) ) { log . error ( "Error processing complete topology event" , e ) ; } }
@ Override public void putCollectionReference ( String ref , CollectionReference col ) { LOGGER . debug ( "Inserting collection reference '" + ref + "' in cache" ) ; code_block = TryStatement ;  LOGGER . debug ( "Clearing field types of collection '" + ref + "' from cache" ) ; this . instance . getReplicatedMap ( ref ) . clear ( ) ; }
public void test() { if ( logger . isTraceEnabled ( LogMarker . SERIALIZER_VERBOSE ) ) { logger . trace ( LogMarker . SERIALIZER_VERBOSE , "Writing Class {}" , c ) ; } }
public void test() { try { FileWriter fw = new FileWriter ( urlFile , true ) ; fw . write ( url . toExternalForm ( ) ) ; fw . write ( "\n" ) ; fw . close ( ) ; RipStatusMessage msg = new RipStatusMessage ( STATUS . DOWNLOAD_COMPLETE , urlFile ) ; observer . update ( this , msg ) ; } catch ( IOException e ) { logger . error ( "Error while writing to " + urlFile , e ) ; return false ; } }
@ Override public List < String > getListInDirWithFilter ( final IRODSFile irodsFile , final FilenameFilter fileNameFilter ) throws JargonException , DataNotFoundException { log . info ( "getListInDirWithFilter(final IRODSFile irodsFile,final FilenameFilter fileNameFilter) " ) ; code_block = IfStatement ; code_block = IfStatement ; List < String > subdirs = new ArrayList < > ( ) ; String path = irodsFile . getAbsolutePath ( ) ; log . debug ( "path for query:{}" , path ) ; IRODSGenQueryBuilder builder = new IRODSGenQueryBuilder ( true , null ) ; IRODSQueryResultSet resultSet = null ; code_block = TryStatement ;  resultSet = null ; builder = new IRODSGenQueryBuilder ( true , null ) ; IRODSFileSystemAOHelper . buildQueryListAllFiles ( path , builder ) ; code_block = TryStatement ;  return subdirs ; }
@ Override public List < String > getListInDirWithFilter ( final IRODSFile irodsFile , final FilenameFilter fileNameFilter ) throws JargonException , DataNotFoundException { log . info ( "getListInDirWithFilter(final IRODSFile irodsFile,final FilenameFilter fileNameFilter) " ) ; code_block = IfStatement ; code_block = IfStatement ; List < String > subdirs = new ArrayList < > ( ) ; String path = irodsFile . getAbsolutePath ( ) ; log . debug ( "path for query:{}" , path ) ; IRODSGenQueryBuilder builder = new IRODSGenQueryBuilder ( true , null ) ; IRODSQueryResultSet resultSet = null ; code_block = TryStatement ;  resultSet = null ; builder = new IRODSGenQueryBuilder ( true , null ) ; IRODSFileSystemAOHelper . buildQueryListAllFiles ( path , builder ) ; code_block = TryStatement ;  return subdirs ; }
public void test() { try { email_intent . addFlags ( Intent . FLAG_ACTIVITY_NEW_TASK ) ; code_block = IfStatement ; } catch ( android . content . ActivityNotFoundException ex ) { Toast . makeText ( activityContext , activityContext . getString ( R . string . email_client_not_present ) , Toast . LENGTH_SHORT ) . show ( ) ; logger . error ( ex ) ; } }
public void test() { try { ListTypeServiceUtil . validate ( listTypeId , classNameId , type ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
protected void processProcedure ( Connection conn , DatabaseMetaData metaData ) throws SQLException { _log . info ( "...Extracting procedures" ) ; final Map < String , DfProcedureMeta > procedureMap = extractProcedureMap ( ) ; code_block = IfStatement ; _log . info ( "...Processing procedures: " + procedureMap . size ( ) ) ; final Element procedureGroupElement = _doc . createElement ( "procedureGroup" ) ; code_block = ForStatement ; _databaseNode . appendChild ( procedureGroupElement ) ; }
@ Test public void restartBothBundles ( ) throws BundleException { Logger log = LoggerFactory . getLogger ( this . getClass ( ) ) ; Bundle paxLoggingApi = Helpers . paxLoggingApi ( context ) ; Bundle paxLoggingService = Helpers . paxLoggingLog4j1 ( context ) ; log . info ( "Before restarting" ) ; osgiLog . log ( LogService . LOG_INFO , "Before restarting" ) ; paxLoggingApi . stop ( Bundle . STOP_TRANSIENT ) ; paxLoggingService . stop ( Bundle . STOP_TRANSIENT ) ; log . info ( "When bundles are stopped" ) ; osgiLog . log ( LogService . LOG_INFO , "When bundles are stopped (log service)" ) ; assertNull ( context . getServiceReference ( LogService . class ) ) ; Logger log1 = LoggerFactory . getLogger ( this . getClass ( ) ) ; log1 . info ( "When bundles are stopped (log1)" ) ; paxLoggingService . start ( Bundle . START_TRANSIENT ) ; paxLoggingApi . start ( Bundle . START_TRANSIENT ) ; log . info ( "After restarting bundles" ) ; log1 . info ( "After restarting bundles (log1)" ) ; osgiLog . log ( LogService . LOG_INFO , "After restarting bundles (log service old ref)" ) ; ServiceReference < LogService > ref = context . getServiceReference ( LogService . class ) ; assertNotNull ( ref ) ; context . getService ( ref ) . log ( LogService . LOG_INFO , "After restarting bundles (log service new ref)" ) ; Logger log3 = LoggerFactory . getLogger ( this . getClass ( ) ) ; log3 . info ( "After restarting bundles (log3)" ) ; List < String > lines = readLines ( ) ; assertTrue ( "TTCCLayout" , lines . contains ( "[main] INFO org.ops4j.pax.logging.it.Log4J1RestartBothPaxLoggingBundlesIntegrationTest - Before restarting" ) ) ; assertTrue ( "TTCCLayout" , lines . contains ( "[main] INFO PaxExam-Probe - Before restarting" ) ) ; assertTrue ( "TTCCLayout" , lines . contains ( "[main] INFO org.ops4j.pax.logging.internal.Activator - Disabling SLF4J API support." ) ) ; assertTrue ( "Default Logger" , lines . contains ( "PaxExam-Probe [org.ops4j.pax.logging.it.Log4J1RestartBothPaxLoggingBundlesIntegrationTest] INFO : When bundles are stopped" ) ) ; assertFalse ( "old LogService reference should not work" , lines . contains ( "When pax-logging-log4j1 is stopped (log service)" ) ) ; assertTrue ( "old LogService reference should not work" , lines . contains ( "org.ops4j.pax.logging.pax-logging-log4j1 [log4j] WARN : No appenders could be found for logger (PaxExam-Probe)." ) ) ; assertTrue ( "Default Logger" , lines . contains ( "PaxExam-Probe [org.ops4j.pax.logging.it.Log4J1RestartBothPaxLoggingBundlesIntegrationTest] INFO : When bundles are stopped (log1)" ) ) ; assertTrue ( "Reconfiguration" , lines . contains ( "org.ops4j.pax.logging.pax-logging-log4j1 [log4j] DEBUG : Finished configuring." ) ) ; assertTrue ( "TTCCLayout, startup of pax-logging-api" , lines . contains ( "[main] INFO org.ops4j.pax.logging.internal.Activator - Enabling Java Util Logging API support." ) ) ; assertTrue ( "Default Logger" , lines . contains ( "PaxExam-Probe [org.ops4j.pax.logging.it.Log4J1RestartBothPaxLoggingBundlesIntegrationTest] INFO : After restarting bundles" ) ) ; assertTrue ( "TTCCLayout" , lines . contains ( "[main] INFO org.ops4j.pax.logging.it.Log4J1RestartBothPaxLoggingBundlesIntegrationTest - After restarting bundles (log1)" ) ) ; assertTrue ( "TTCCLayout" , lines . contains ( "[main] INFO PaxExam-Probe - After restarting bundles (log service old ref)" ) ) ; assertTrue ( "TTCCLayout" , lines . contains ( "[main] INFO PaxExam-Probe - After restarting bundles (log service new ref)" ) ) ; assertTrue ( "TTCCLayout" , lines . contains ( "[main] INFO org.ops4j.pax.logging.it.Log4J1RestartBothPaxLoggingBundlesIntegrationTest - After restarting bundles (log3)" ) ) ; }
@ Test public void restartBothBundles ( ) throws BundleException { Logger log = LoggerFactory . getLogger ( this . getClass ( ) ) ; Bundle paxLoggingApi = Helpers . paxLoggingApi ( context ) ; Bundle paxLoggingService = Helpers . paxLoggingLog4j1 ( context ) ; log . info ( "Before restarting" ) ; osgiLog . log ( LogService . LOG_INFO , "Before restarting" ) ; paxLoggingApi . stop ( Bundle . STOP_TRANSIENT ) ; paxLoggingService . stop ( Bundle . STOP_TRANSIENT ) ; log . info ( "When bundles are stopped" ) ; osgiLog . log ( LogService . LOG_INFO , "When bundles are stopped (log service)" ) ; assertNull ( context . getServiceReference ( LogService . class ) ) ; Logger log1 = LoggerFactory . getLogger ( this . getClass ( ) ) ; log1 . info ( "When bundles are stopped (log1)" ) ; paxLoggingService . start ( Bundle . START_TRANSIENT ) ; paxLoggingApi . start ( Bundle . START_TRANSIENT ) ; log . info ( "After restarting bundles" ) ; log1 . info ( "After restarting bundles (log1)" ) ; osgiLog . log ( LogService . LOG_INFO , "After restarting bundles (log service old ref)" ) ; ServiceReference < LogService > ref = context . getServiceReference ( LogService . class ) ; assertNotNull ( ref ) ; context . getService ( ref ) . log ( LogService . LOG_INFO , "After restarting bundles (log service new ref)" ) ; Logger log3 = LoggerFactory . getLogger ( this . getClass ( ) ) ; log3 . info ( "After restarting bundles (log3)" ) ; List < String > lines = readLines ( ) ; assertTrue ( "TTCCLayout" , lines . contains ( "[main] INFO org.ops4j.pax.logging.it.Log4J1RestartBothPaxLoggingBundlesIntegrationTest - Before restarting" ) ) ; assertTrue ( "TTCCLayout" , lines . contains ( "[main] INFO PaxExam-Probe - Before restarting" ) ) ; assertTrue ( "TTCCLayout" , lines . contains ( "[main] INFO org.ops4j.pax.logging.internal.Activator - Disabling SLF4J API support." ) ) ; assertTrue ( "Default Logger" , lines . contains ( "PaxExam-Probe [org.ops4j.pax.logging.it.Log4J1RestartBothPaxLoggingBundlesIntegrationTest] INFO : When bundles are stopped" ) ) ; assertFalse ( "old LogService reference should not work" , lines . contains ( "When pax-logging-log4j1 is stopped (log service)" ) ) ; assertTrue ( "old LogService reference should not work" , lines . contains ( "org.ops4j.pax.logging.pax-logging-log4j1 [log4j] WARN : No appenders could be found for logger (PaxExam-Probe)." ) ) ; assertTrue ( "Default Logger" , lines . contains ( "PaxExam-Probe [org.ops4j.pax.logging.it.Log4J1RestartBothPaxLoggingBundlesIntegrationTest] INFO : When bundles are stopped (log1)" ) ) ; assertTrue ( "Reconfiguration" , lines . contains ( "org.ops4j.pax.logging.pax-logging-log4j1 [log4j] DEBUG : Finished configuring." ) ) ; assertTrue ( "TTCCLayout, startup of pax-logging-api" , lines . contains ( "[main] INFO org.ops4j.pax.logging.internal.Activator - Enabling Java Util Logging API support." ) ) ; assertTrue ( "Default Logger" , lines . contains ( "PaxExam-Probe [org.ops4j.pax.logging.it.Log4J1RestartBothPaxLoggingBundlesIntegrationTest] INFO : After restarting bundles" ) ) ; assertTrue ( "TTCCLayout" , lines . contains ( "[main] INFO org.ops4j.pax.logging.it.Log4J1RestartBothPaxLoggingBundlesIntegrationTest - After restarting bundles (log1)" ) ) ; assertTrue ( "TTCCLayout" , lines . contains ( "[main] INFO PaxExam-Probe - After restarting bundles (log service old ref)" ) ) ; assertTrue ( "TTCCLayout" , lines . contains ( "[main] INFO PaxExam-Probe - After restarting bundles (log service new ref)" ) ) ; assertTrue ( "TTCCLayout" , lines . contains ( "[main] INFO org.ops4j.pax.logging.it.Log4J1RestartBothPaxLoggingBundlesIntegrationTest - After restarting bundles (log3)" ) ) ; }
@ Test public void restartBothBundles ( ) throws BundleException { Logger log = LoggerFactory . getLogger ( this . getClass ( ) ) ; Bundle paxLoggingApi = Helpers . paxLoggingApi ( context ) ; Bundle paxLoggingService = Helpers . paxLoggingLog4j1 ( context ) ; log . info ( "Before restarting" ) ; osgiLog . log ( LogService . LOG_INFO , "Before restarting" ) ; paxLoggingApi . stop ( Bundle . STOP_TRANSIENT ) ; paxLoggingService . stop ( Bundle . STOP_TRANSIENT ) ; log . info ( "When bundles are stopped" ) ; osgiLog . log ( LogService . LOG_INFO , "When bundles are stopped (log service)" ) ; assertNull ( context . getServiceReference ( LogService . class ) ) ; Logger log1 = LoggerFactory . getLogger ( this . getClass ( ) ) ; log1 . info ( "When bundles are stopped (log1)" ) ; paxLoggingService . start ( Bundle . START_TRANSIENT ) ; paxLoggingApi . start ( Bundle . START_TRANSIENT ) ; log . info ( "After restarting bundles" ) ; log1 . info ( "After restarting bundles (log1)" ) ; osgiLog . log ( LogService . LOG_INFO , "After restarting bundles (log service old ref)" ) ; ServiceReference < LogService > ref = context . getServiceReference ( LogService . class ) ; assertNotNull ( ref ) ; context . getService ( ref ) . log ( LogService . LOG_INFO , "After restarting bundles (log service new ref)" ) ; Logger log3 = LoggerFactory . getLogger ( this . getClass ( ) ) ; log3 . info ( "After restarting bundles (log3)" ) ; List < String > lines = readLines ( ) ; assertTrue ( "TTCCLayout" , lines . contains ( "[main] INFO org.ops4j.pax.logging.it.Log4J1RestartBothPaxLoggingBundlesIntegrationTest - Before restarting" ) ) ; assertTrue ( "TTCCLayout" , lines . contains ( "[main] INFO PaxExam-Probe - Before restarting" ) ) ; assertTrue ( "TTCCLayout" , lines . contains ( "[main] INFO org.ops4j.pax.logging.internal.Activator - Disabling SLF4J API support." ) ) ; assertTrue ( "Default Logger" , lines . contains ( "PaxExam-Probe [org.ops4j.pax.logging.it.Log4J1RestartBothPaxLoggingBundlesIntegrationTest] INFO : When bundles are stopped" ) ) ; assertFalse ( "old LogService reference should not work" , lines . contains ( "When pax-logging-log4j1 is stopped (log service)" ) ) ; assertTrue ( "old LogService reference should not work" , lines . contains ( "org.ops4j.pax.logging.pax-logging-log4j1 [log4j] WARN : No appenders could be found for logger (PaxExam-Probe)." ) ) ; assertTrue ( "Default Logger" , lines . contains ( "PaxExam-Probe [org.ops4j.pax.logging.it.Log4J1RestartBothPaxLoggingBundlesIntegrationTest] INFO : When bundles are stopped (log1)" ) ) ; assertTrue ( "Reconfiguration" , lines . contains ( "org.ops4j.pax.logging.pax-logging-log4j1 [log4j] DEBUG : Finished configuring." ) ) ; assertTrue ( "TTCCLayout, startup of pax-logging-api" , lines . contains ( "[main] INFO org.ops4j.pax.logging.internal.Activator - Enabling Java Util Logging API support." ) ) ; assertTrue ( "Default Logger" , lines . contains ( "PaxExam-Probe [org.ops4j.pax.logging.it.Log4J1RestartBothPaxLoggingBundlesIntegrationTest] INFO : After restarting bundles" ) ) ; assertTrue ( "TTCCLayout" , lines . contains ( "[main] INFO org.ops4j.pax.logging.it.Log4J1RestartBothPaxLoggingBundlesIntegrationTest - After restarting bundles (log1)" ) ) ; assertTrue ( "TTCCLayout" , lines . contains ( "[main] INFO PaxExam-Probe - After restarting bundles (log service old ref)" ) ) ; assertTrue ( "TTCCLayout" , lines . contains ( "[main] INFO PaxExam-Probe - After restarting bundles (log service new ref)" ) ) ; assertTrue ( "TTCCLayout" , lines . contains ( "[main] INFO org.ops4j.pax.logging.it.Log4J1RestartBothPaxLoggingBundlesIntegrationTest - After restarting bundles (log3)" ) ) ; }
public void test() { if ( DependencyUtil . containsLibraryDependency ( deps , nested_lib ) ) { log . warn ( "Dependency for library " + nested_lib + " already exists, will not be duplicated for the nested library with" + " path [" + paa . getArchivePath ( ) + "]" ) ; } else { final Dependency dep = new Dependency ( ) ; dep . setLib ( nested_lib ) ; dep . setApp ( this . getApplication ( ) ) ; final Path archive_path = paa . getArchivePath ( ) ; code_block = IfStatement ; dep . setDeclared ( false ) ; dep . setScope ( Scope . RUNTIME ) ; dep . setTransitive ( true ) ; deps . add ( dep ) ; } }
public void test() { if ( nested_fa instanceof PythonArchiveAnalyzer ) { code_block = TryStatement ;  } else { log . warn ( "Nested analyzer of unexpected type [" + nested_fa . getClass ( ) . getSimpleName ( ) + "]" ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception ex ) { LOGGER . error ( "Error creating transfer thread" , ex ) ; } }
@ Override public void onClick ( AjaxRequestTarget target ) { String cacheInformation = getCacheInformation ( ) ; LOGGER . info ( "Dumping the content of the caches.\nCurrent counters:\n{}\n" , cacheInformation ) ; MidPointApplication . get ( ) . getCacheRegistry ( ) . dumpContent ( ) ; getSession ( ) . success ( getPageBase ( ) . getString ( "InternalsCachePanel.result.dumped" ) ) ; target . add ( getPageBase ( ) ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "entering 'save' method..." ) ; } }
public void foo ( @ Simple ( "${header.foo}" ) String bar ) { this . bar = bar ; LOG . info ( "foo() method called with: " + bar ) ; }
public void test() { try { getCoordinator ( ) . makeCubeImmutableForReceiver ( node , cubeName ) ; } catch ( IOException ioe ) { logger . error ( String . format ( Locale . ROOT , "Convert %s to immutable for node %s failed." , cubeName , node . toNormalizeString ( ) ) , ioe ) ; failedNodes . add ( node ) ; } }
public void test() { try { } catch ( Exception e ) { LOG . error ( e . getMessage ( ) ) ; } }
public void test() { try { eventExecutorGroup . shutdownGracefully ( ) . sync ( ) ; } catch ( InterruptedException e ) { logger . error ( "Error stopping socket " + socketDescription , e ) ; } }
public void test() { if ( ! StringUtil . isEmpty ( dbDir ) ) { logger . info ( "kylin.source.hive.warehouse-dir is {}" , dbDir ) ; } else { logger . warn ( "kylin.source.hive.warehouse-dir is null" ) ; } }
public void test() { if ( ! StringUtil . isEmpty ( dbDir ) ) { logger . info ( "kylin.source.hive.warehouse-dir is {}" , dbDir ) ; } else { logger . warn ( "kylin.source.hive.warehouse-dir is null" ) ; } }
public void test() { if ( LOGGER . isInfoEnabled ( ) ) { LOGGER . info ( task ) ; } }
public void test() { if ( caught instanceof AuthenticationError ) { Log . error ( "Authentication error." , caught ) ; Application . redirectToLogin ( ) ; } else-if ( caught instanceof InvalidTokenError ) { Log . error ( "Invalid Token error (" + sessionId + ")" , caught ) ; Application . redirectToLogin ( ) ; } else-if ( caught instanceof AuthorizationError ) { Log . info ( "RCP Authorization Error calling " + action . getClass ( ) + ": " + caught . getMessage ( ) ) ; callback . onFailure ( caught ) ; } else { callback . onFailure ( caught ) ; } }
public void test() { if ( caught instanceof AuthenticationError ) { Log . error ( "Authentication error." , caught ) ; Application . redirectToLogin ( ) ; } else-if ( caught instanceof InvalidTokenError ) { Log . error ( "Invalid Token error (" + sessionId + ")" , caught ) ; Application . redirectToLogin ( ) ; } else-if ( caught instanceof AuthorizationError ) { Log . info ( "RCP Authorization Error calling " + action . getClass ( ) + ": " + caught . getMessage ( ) ) ; callback . onFailure ( caught ) ; } else { callback . onFailure ( caught ) ; } }
public void test() { if ( caught instanceof AuthenticationError ) { Log . error ( "Authentication error." , caught ) ; Application . redirectToLogin ( ) ; } else-if ( caught instanceof InvalidTokenError ) { Log . error ( "Invalid Token error (" + sessionId + ")" , caught ) ; Application . redirectToLogin ( ) ; } else-if ( caught instanceof AuthorizationError ) { Log . info ( "RCP Authorization Error calling " + action . getClass ( ) + ": " + caught . getMessage ( ) ) ; callback . onFailure ( caught ) ; } else { callback . onFailure ( caught ) ; } }
@ Override public void deleteStaticRoute ( String netIdIpAdress , String nextHopIpAddress ) throws CapabilityException { log . info ( "Start of deleteStaticRoute call" ) ; String [ ] aParams = new String [ 2 ] ; aParams [ 0 ] = netIdIpAdress ; aParams [ 1 ] = nextHopIpAddress ; IAction action = createActionAndCheckParams ( StaticRouteActionSet . STATIC_ROUTE_DELETE , aParams ) ; queueAction ( action ) ; log . info ( "End of deleteStaticRoute call" ) ; }
@ Override public void deleteStaticRoute ( String netIdIpAdress , String nextHopIpAddress ) throws CapabilityException { log . info ( "Start of deleteStaticRoute call" ) ; String [ ] aParams = new String [ 2 ] ; aParams [ 0 ] = netIdIpAdress ; aParams [ 1 ] = nextHopIpAddress ; IAction action = createActionAndCheckParams ( StaticRouteActionSet . STATIC_ROUTE_DELETE , aParams ) ; queueAction ( action ) ; log . info ( "End of deleteStaticRoute call" ) ; }
@ Override public int read ( final String context ) throws IOException { int b = read ( ) ; logger . trace ( "Read {}  byte, val is {}" , context , ByteUtils . formatByte ( b ) ) ; return b ; }
public void test() { if ( logScore ) { log . info ( "Score at iteration {} is {}" , iteration , score ) ; } }
@ BeforeRun public void beforeRun ( ThreadState state ) { state . threadId = ( int ) threadIdGenerator . getAndIncrement ( ) ; state . totalThreadCount = ( int ) totalThreadCount . get ( ) ; logger . info ( "totalThreadCount: " + state . totalThreadCount ) ; logger . info ( "threadId: " + state . threadId ) ; }
@ BeforeRun public void beforeRun ( ThreadState state ) { state . threadId = ( int ) threadIdGenerator . getAndIncrement ( ) ; state . totalThreadCount = ( int ) totalThreadCount . get ( ) ; logger . info ( "totalThreadCount: " + state . totalThreadCount ) ; logger . info ( "threadId: " + state . threadId ) ; }
public void test() { case role assignments" , required = false , examples = @ Example ( value = { @ ExampleProperty ( mediaType = JSON , value = CASE_FILE_JSON ) , @ ExampleProperty ( mediaType = XML , value = CASE_FILE_XML ) } }
public void test() { try { URL serviceRoot = new URL ( serviceRootUrl ) ; List < String > genEndpoints = new ArrayList < > ( ) ; genEndpoints . add ( "mqtt://" + serviceRoot . getHost ( ) + ":" + getPort ( ) ) ; endpoints = Collections . unmodifiableList ( genEndpoints ) ; LOGGER . info ( "Generated MQTT endpoint list: {}" , endpoints ) ; LOGGER . info ( "Please set " + PREFIX_MQTT + TAG_EXPOSED_MQTT_ENDPOINTS + " to set the correct MQTT end points." ) ; } catch ( MalformedURLException ex ) { LOGGER . error ( "Failed to create MQTT urls." , ex ) ; } }
public void test() { try { URL serviceRoot = new URL ( serviceRootUrl ) ; List < String > genEndpoints = new ArrayList < > ( ) ; genEndpoints . add ( "mqtt://" + serviceRoot . getHost ( ) + ":" + getPort ( ) ) ; endpoints = Collections . unmodifiableList ( genEndpoints ) ; LOGGER . info ( "Generated MQTT endpoint list: {}" , endpoints ) ; LOGGER . info ( "Please set " + PREFIX_MQTT + TAG_EXPOSED_MQTT_ENDPOINTS + " to set the correct MQTT end points." ) ; } catch ( MalformedURLException ex ) { LOGGER . error ( "Failed to create MQTT urls." , ex ) ; } }
public void test() { try { URL serviceRoot = new URL ( serviceRootUrl ) ; List < String > genEndpoints = new ArrayList < > ( ) ; genEndpoints . add ( "mqtt://" + serviceRoot . getHost ( ) + ":" + getPort ( ) ) ; endpoints = Collections . unmodifiableList ( genEndpoints ) ; LOGGER . info ( "Generated MQTT endpoint list: {}" , endpoints ) ; LOGGER . info ( "Please set " + PREFIX_MQTT + TAG_EXPOSED_MQTT_ENDPOINTS + " to set the correct MQTT end points." ) ; } catch ( MalformedURLException ex ) { LOGGER . error ( "Failed to create MQTT urls." , ex ) ; } }
public void test() { try { java . util . List < com . liferay . document . library . kernel . model . DLFileEntry > returnValue = DLFileEntryServiceUtil . getFileEntries ( groupId , folderId , mimeTypes , status , start , end , orderByComparator ) ; return com . liferay . document . library . kernel . model . DLFileEntrySoap . toSoapModels ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( ! accepted ) { _log . error ( "Local trip update buffer full!  Clearing!  Dropping " + tripUpdate . getId ( ) + " record" ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "addObjectField fieldName: {}" , fieldName ) ; } }
public void test() { try { wsl . onWebsocketHandshakeSentAsClient ( this , this . handshakerequest ) ; } catch ( InvalidDataException e ) { throw new InvalidHandshakeException ( "Handshake data rejected by client." ) ; } catch ( RuntimeException e ) { log . error ( "Exception in startHandshake" , e ) ; wsl . onWebsocketError ( this , e ) ; throw new InvalidHandshakeException ( "rejected because of " + e ) ; } }
public void test() { if ( session != null ) { session . removeAttribute ( ActionContext . SESSION_ATTRIBUTE_PREFIX . concat ( sources [ 0 ] . toString ( ) ) ) ; } else { logger . warn ( "{}: No session available to remvoe session attribute! (this can happen in onStructrLogin/onStructrLogout)" , getReplacement ( ) ) ; } }
public void test() { try { accountServlet = new AccountServlet ( httpService , this . getThing ( ) . getUID ( ) . getId ( ) , this , gson ) ; } catch ( IllegalStateException e ) { logger . warn ( "Failed to create account servlet" , e ) ; } }
public void test() { if ( dto != null ) { logger . info ( "Add crisis successful: " + dto . getName ( ) + ":" + dto . getCrisisTypeId ( ) ) ; Integer ret = remoteCrisisTypeEJB . deleteCrisisType ( dto . getCrisisTypeId ( ) ) ; if ( ret != null && ret . intValue ( ) == 1 ) return Response . ok ( "CrisisType Add-Delete test successful" + dto ) . build ( ) ; } }
public void test() { try { CrisisTypeDTO crisisType = new CrisisTypeDTO ( "test_db-manager_crisisType" ) ; CrisisTypeDTO dto = remoteCrisisTypeEJB . addCrisisType ( crisisType ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Error in /crisisType/addde." , e ) ; return Response . ok ( "Exception: " + e ) . build ( ) ; } }
public void test() { if ( oslpDevice == null ) { LOGGER . error ( "Unable to find OSLP device: {}" , deviceIdentification ) ; return ; } }
public void addSession ( Session session ) { this . availableSessionsById . put ( session . getId ( ) , session ) ; final KieServerMessageHandler messageHandler = new KieServerMessageHandler ( session ) ; this . handlersPerSession . put ( session . getId ( ) , messageHandler ) ; session . addMessageHandler ( messageHandler ) ; logger . debug ( "Session '{}' added to Web Socket manager" , session . getId ( ) ) ; }
public void test() { try { commentDocModel = ( DocumentModel ) relationManager . getResourceRepresentation ( config . commentNamespace , subject , ctxMap ) ; } catch ( Exception e ) { log . error ( "failed to retrieve commentDocModel from relations" ) ; } }
public void test() { while ( startBatch < textFlowsToTranslate . size ( ) ) { int batchEnd = Math . min ( startBatch + REQUEST_BATCH_SIZE , textFlowsToTranslate . size ( ) ) ; log . debug ( "[PERF] Starting batch {} - {}" , startBatch , batchEnd ) ; List < HTextFlow > textFlowBatch = textFlowsToTranslate . subList ( startBatch , batchEnd ) ; MTDocument mtDocument = textFlowsToMTDoc . fromTextFlows ( projectSlug , versionSlug , doc . getDocId ( ) , doc . getSourceLocaleId ( ) , textFlowBatch , TextFlowsToMTDoc :: extractPluralIfPresent , backendId ) ; log . debug ( "[PERF] Sending batch {} - {}" , startBatch , batchEnd ) ; Stopwatch mtProviderStopwatch = Stopwatch . createStarted ( ) ; MTDocument result = getTranslationFromMT ( mtDocument , targetLocale . getLocaleId ( ) ) ; log . debug ( "[PERF] Received response [{} contents] ({}ms)" , result . getContents ( ) . size ( ) , mtProviderStopwatch ) ; saveTranslationsInBatches ( textFlowBatch , result , targetLocale , saveState ) ; backendIdConfirmation = result . getBackendId ( ) ; startBatch = batchEnd ; taskHandle . increaseProgress ( textFlowBatch . size ( ) ) ; } }
public void test() { while ( startBatch < textFlowsToTranslate . size ( ) ) { int batchEnd = Math . min ( startBatch + REQUEST_BATCH_SIZE , textFlowsToTranslate . size ( ) ) ; log . debug ( "[PERF] Starting batch {} - {}" , startBatch , batchEnd ) ; List < HTextFlow > textFlowBatch = textFlowsToTranslate . subList ( startBatch , batchEnd ) ; MTDocument mtDocument = textFlowsToMTDoc . fromTextFlows ( projectSlug , versionSlug , doc . getDocId ( ) , doc . getSourceLocaleId ( ) , textFlowBatch , TextFlowsToMTDoc :: extractPluralIfPresent , backendId ) ; log . debug ( "[PERF] Sending batch {} - {}" , startBatch , batchEnd ) ; Stopwatch mtProviderStopwatch = Stopwatch . createStarted ( ) ; MTDocument result = getTranslationFromMT ( mtDocument , targetLocale . getLocaleId ( ) ) ; log . debug ( "[PERF] Received response [{} contents] ({}ms)" , result . getContents ( ) . size ( ) , mtProviderStopwatch ) ; saveTranslationsInBatches ( textFlowBatch , result , targetLocale , saveState ) ; backendIdConfirmation = result . getBackendId ( ) ; startBatch = batchEnd ; taskHandle . increaseProgress ( textFlowBatch . size ( ) ) ; } }
public void test() { while ( startBatch < textFlowsToTranslate . size ( ) ) { int batchEnd = Math . min ( startBatch + REQUEST_BATCH_SIZE , textFlowsToTranslate . size ( ) ) ; log . debug ( "[PERF] Starting batch {} - {}" , startBatch , batchEnd ) ; List < HTextFlow > textFlowBatch = textFlowsToTranslate . subList ( startBatch , batchEnd ) ; MTDocument mtDocument = textFlowsToMTDoc . fromTextFlows ( projectSlug , versionSlug , doc . getDocId ( ) , doc . getSourceLocaleId ( ) , textFlowBatch , TextFlowsToMTDoc :: extractPluralIfPresent , backendId ) ; log . debug ( "[PERF] Sending batch {} - {}" , startBatch , batchEnd ) ; Stopwatch mtProviderStopwatch = Stopwatch . createStarted ( ) ; MTDocument result = getTranslationFromMT ( mtDocument , targetLocale . getLocaleId ( ) ) ; log . debug ( "[PERF] Received response [{} contents] ({}ms)" , result . getContents ( ) . size ( ) , mtProviderStopwatch ) ; saveTranslationsInBatches ( textFlowBatch , result , targetLocale , saveState ) ; backendIdConfirmation = result . getBackendId ( ) ; startBatch = batchEnd ; taskHandle . increaseProgress ( textFlowBatch . size ( ) ) ; } }
public void test() { try { int aEventId = eventManager . createAsync ( 60 ) ; LOGGER . info ( "Async Event [{}] has been created." , aEventId ) ; eventManager . start ( aEventId ) ; LOGGER . info ( "Async Event [{}] has been started." , aEventId ) ; int sEventId = eventManager . create ( 60 ) ; LOGGER . info ( "Sync Event [{}] has been created." , sEventId ) ; eventManager . start ( sEventId ) ; LOGGER . info ( "Sync Event [{}] has been started." , sEventId ) ; eventManager . status ( aEventId ) ; eventManager . status ( sEventId ) ; eventManager . cancel ( aEventId ) ; LOGGER . info ( "Async Event [{}] has been stopped." , aEventId ) ; eventManager . cancel ( sEventId ) ; LOGGER . info ( "Sync Event [{}] has been stopped." , sEventId ) ; } catch ( MaxNumOfEventsAllowedException | LongRunningEventException | EventDoesNotExistException | InvalidOperationException e ) { LOGGER . error ( e . getMessage ( ) ) ; } }
public void test() { try { int aEventId = eventManager . createAsync ( 60 ) ; LOGGER . info ( "Async Event [{}] has been created." , aEventId ) ; eventManager . start ( aEventId ) ; LOGGER . info ( "Async Event [{}] has been started." , aEventId ) ; int sEventId = eventManager . create ( 60 ) ; LOGGER . info ( "Sync Event [{}] has been created." , sEventId ) ; eventManager . start ( sEventId ) ; LOGGER . info ( "Sync Event [{}] has been started." , sEventId ) ; eventManager . status ( aEventId ) ; eventManager . status ( sEventId ) ; eventManager . cancel ( aEventId ) ; LOGGER . info ( "Async Event [{}] has been stopped." , aEventId ) ; eventManager . cancel ( sEventId ) ; LOGGER . info ( "Sync Event [{}] has been stopped." , sEventId ) ; } catch ( MaxNumOfEventsAllowedException | LongRunningEventException | EventDoesNotExistException | InvalidOperationException e ) { LOGGER . error ( e . getMessage ( ) ) ; } }
public void test() { try { int aEventId = eventManager . createAsync ( 60 ) ; LOGGER . info ( "Async Event [{}] has been created." , aEventId ) ; eventManager . start ( aEventId ) ; LOGGER . info ( "Async Event [{}] has been started." , aEventId ) ; int sEventId = eventManager . create ( 60 ) ; LOGGER . info ( "Sync Event [{}] has been created." , sEventId ) ; eventManager . start ( sEventId ) ; LOGGER . info ( "Sync Event [{}] has been started." , sEventId ) ; eventManager . status ( aEventId ) ; eventManager . status ( sEventId ) ; eventManager . cancel ( aEventId ) ; LOGGER . info ( "Async Event [{}] has been stopped." , aEventId ) ; eventManager . cancel ( sEventId ) ; LOGGER . info ( "Sync Event [{}] has been stopped." , sEventId ) ; } catch ( MaxNumOfEventsAllowedException | LongRunningEventException | EventDoesNotExistException | InvalidOperationException e ) { LOGGER . error ( e . getMessage ( ) ) ; } }
public void test() { try { int aEventId = eventManager . createAsync ( 60 ) ; LOGGER . info ( "Async Event [{}] has been created." , aEventId ) ; eventManager . start ( aEventId ) ; LOGGER . info ( "Async Event [{}] has been started." , aEventId ) ; int sEventId = eventManager . create ( 60 ) ; LOGGER . info ( "Sync Event [{}] has been created." , sEventId ) ; eventManager . start ( sEventId ) ; LOGGER . info ( "Sync Event [{}] has been started." , sEventId ) ; eventManager . status ( aEventId ) ; eventManager . status ( sEventId ) ; eventManager . cancel ( aEventId ) ; LOGGER . info ( "Async Event [{}] has been stopped." , aEventId ) ; eventManager . cancel ( sEventId ) ; LOGGER . info ( "Sync Event [{}] has been stopped." , sEventId ) ; } catch ( MaxNumOfEventsAllowedException | LongRunningEventException | EventDoesNotExistException | InvalidOperationException e ) { LOGGER . error ( e . getMessage ( ) ) ; } }
public void test() { try { int aEventId = eventManager . createAsync ( 60 ) ; LOGGER . info ( "Async Event [{}] has been created." , aEventId ) ; eventManager . start ( aEventId ) ; LOGGER . info ( "Async Event [{}] has been started." , aEventId ) ; int sEventId = eventManager . create ( 60 ) ; LOGGER . info ( "Sync Event [{}] has been created." , sEventId ) ; eventManager . start ( sEventId ) ; LOGGER . info ( "Sync Event [{}] has been started." , sEventId ) ; eventManager . status ( aEventId ) ; eventManager . status ( sEventId ) ; eventManager . cancel ( aEventId ) ; LOGGER . info ( "Async Event [{}] has been stopped." , aEventId ) ; eventManager . cancel ( sEventId ) ; LOGGER . info ( "Sync Event [{}] has been stopped." , sEventId ) ; } catch ( MaxNumOfEventsAllowedException | LongRunningEventException | EventDoesNotExistException | InvalidOperationException e ) { LOGGER . error ( e . getMessage ( ) ) ; } }
public void test() { try { int aEventId = eventManager . createAsync ( 60 ) ; LOGGER . info ( "Async Event [{}] has been created." , aEventId ) ; eventManager . start ( aEventId ) ; LOGGER . info ( "Async Event [{}] has been started." , aEventId ) ; int sEventId = eventManager . create ( 60 ) ; LOGGER . info ( "Sync Event [{}] has been created." , sEventId ) ; eventManager . start ( sEventId ) ; LOGGER . info ( "Sync Event [{}] has been started." , sEventId ) ; eventManager . status ( aEventId ) ; eventManager . status ( sEventId ) ; eventManager . cancel ( aEventId ) ; LOGGER . info ( "Async Event [{}] has been stopped." , aEventId ) ; eventManager . cancel ( sEventId ) ; LOGGER . info ( "Sync Event [{}] has been stopped." , sEventId ) ; } catch ( MaxNumOfEventsAllowedException | LongRunningEventException | EventDoesNotExistException | InvalidOperationException e ) { LOGGER . error ( e . getMessage ( ) ) ; } }
public void test() { try { int aEventId = eventManager . createAsync ( 60 ) ; LOGGER . info ( "Async Event [{}] has been created." , aEventId ) ; eventManager . start ( aEventId ) ; LOGGER . info ( "Async Event [{}] has been started." , aEventId ) ; int sEventId = eventManager . create ( 60 ) ; LOGGER . info ( "Sync Event [{}] has been created." , sEventId ) ; eventManager . start ( sEventId ) ; LOGGER . info ( "Sync Event [{}] has been started." , sEventId ) ; eventManager . status ( aEventId ) ; eventManager . status ( sEventId ) ; eventManager . cancel ( aEventId ) ; LOGGER . info ( "Async Event [{}] has been stopped." , aEventId ) ; eventManager . cancel ( sEventId ) ; LOGGER . info ( "Sync Event [{}] has been stopped." , sEventId ) ; } catch ( MaxNumOfEventsAllowedException | LongRunningEventException | EventDoesNotExistException | InvalidOperationException e ) { LOGGER . error ( e . getMessage ( ) ) ; } }
public void test() { try { return getRedisKeyValueState ( namespace , topoConf , context , getStateConfig ( topoConf ) ) ; } catch ( Exception ex ) { LOG . error ( "Error loading config from storm conf {}" , topoConf ) ; throw new RuntimeException ( ex ) ; } }
public void test() { try { String res = results . get ( Replica . getReplicaFromId ( replicaId ) ) ; KeyValuePair < String , String > kvp = ChecksumJob . parseLine ( res ) ; code_block = IfStatement ; log . warn ( "Found unexpected file '" + kvp . getKey ( ) + "' while asking replica '" + Replica . getReplicaFromId ( replicaId ) + "' for file '" + filename + "'" ) ; } catch ( ArgumentNotValid e ) { log . warn ( "Unexpected error '" + e + "' while asking " + "replica '" + Replica . getReplicaFromId ( replicaId ) + "' for file '" + filename + "'" ) ; } }
public void test() { try { Object o = PentahoSystem . get ( ISolutionEngine . class ) ; code_block = IfStatement ; } catch ( Throwable e ) { logger . error ( "Failed to set DEBUG log level due to ISolutionEngine not being available in MicroPlatform" ) ; } }
public void test() { if ( runtime > 2000 ) { log . warn ( this . getClass ( ) . getName ( ) + "." + methodName + "() test took " + runtime + "ms" ) ; } }
@ Override public void stop ( ) { getLogger ( ) . info ( "Try to kill " + this . jobId + " Hadoop job" ) ; final Configuration conf = createConfiguration ( ) ; code_block = TryStatement ;  getLogger ( ) . info ( "Hadoop job " + this . jobId + " killed" ) ; }
public void test() { try { log . debug ( "fetching calendar configurations to " + subscribeId ) ; @ SuppressWarnings ( "unchecked" ) List < UserDefinedCalendarConfiguration > configurations = ( List < UserDefinedCalendarConfiguration > ) getHibernateTemplate ( ) . find ( "from CalendarConfiguration config where " + "subscribeId = ? and displayed = true " + "order by calendarDefinition.name" , subscribeId ) ; return configurations ; } catch ( HibernateException ex ) { throw convertHibernateAccessException ( ex ) ; } }
public void test() { { @ Override public List < String > merge ( BlurExecutorCompletionService < List < String > > service ) throws BlurException code_block = "" ; } }
public CreateProjectPage enterDescription ( String projectDescription ) { log . info ( "Enter project description {}" , projectDescription ) ; enterText ( readyElement ( descriptionField ) , projectDescription ) ; return new CreateProjectPage ( getDriver ( ) ) ; }
public void test() { if ( parentSysmlId == null || childSysmlId == null || parentSysmlId . isEmpty ( ) || childSysmlId . isEmpty ( ) ) { logger . warn ( "Parent or child not found" ) ; logger . warn ( "parentSysmlId: " + parentSysmlId ) ; logger . warn ( "childSysmlId: " + childSysmlId ) ; return ; } }
public void test() { if ( parentSysmlId == null || childSysmlId == null || parentSysmlId . isEmpty ( ) || childSysmlId . isEmpty ( ) ) { logger . warn ( "Parent or child not found" ) ; logger . warn ( "parentSysmlId: " + parentSysmlId ) ; logger . warn ( "childSysmlId: " + childSysmlId ) ; return ; } }
public void test() { if ( parentSysmlId == null || childSysmlId == null || parentSysmlId . isEmpty ( ) || childSysmlId . isEmpty ( ) ) { logger . warn ( "Parent or child not found" ) ; logger . warn ( "parentSysmlId: " + parentSysmlId ) ; logger . warn ( "childSysmlId: " + childSysmlId ) ; return ; } }
public void test() { if ( e . getMessage ( ) . contains ( "duplicate key" ) ) { logger . info ( String . format ( "%s" , LogUtil . getStackTrace ( e ) ) ) ; } else { logger . warn ( String . format ( "%s" , LogUtil . getStackTrace ( e ) ) ) ; } }
public void test() { if ( e . getMessage ( ) . contains ( "duplicate key" ) ) { logger . info ( String . format ( "%s" , LogUtil . getStackTrace ( e ) ) ) ; } else { logger . warn ( String . format ( "%s" , LogUtil . getStackTrace ( e ) ) ) ; } }
public void test() { try { File dataExample = new File ( Info . PATHS . PATH_WORKDIR . getParentFile ( ) . getParentFile ( ) + "/data-example" ) ; LOG . info ( dataExample . getAbsolutePath ( ) ) ; copy ( dataExample . getAbsolutePath ( ) , Info . PATHS . PATH_DATA_FOLDER . getAbsolutePath ( ) ) ; canPerformTest = true ; } catch ( IOException e ) { canPerformTest = false ; } }
@ Test ( enabled = false ) public void testPresenceSolutionBlind ( ) { log . info ( "=== TEST for SOLUTION GENERATION of BLIND optimizer STARTED (syntax July 2015)===" ) ; optimizer = new Optimizer ( NUM_PLANS_TO_GENERATE , SearchMethodName . BLINDSEARCH ) ; String [ ] arrayDam = optimizer . optimize ( appModel , suitableCloudOffer ) ; code_block = ForStatement ; log . info ( "=== TEST for SOLUTION GENERATION of BLIND optimizer FINISEHD ===" ) ; }
public void test() { try { checkCorrectness ( arrayDam [ damnum ] ) ; } catch ( Exception e ) { log . error ( "There was an error in the check of correctness. Solution was: " + arrayDam [ damnum ] ) ; throw e ; } }
@ Test ( enabled = false ) public void testPresenceSolutionBlind ( ) { log . info ( "=== TEST for SOLUTION GENERATION of BLIND optimizer STARTED (syntax July 2015)===" ) ; optimizer = new Optimizer ( NUM_PLANS_TO_GENERATE , SearchMethodName . BLINDSEARCH ) ; String [ ] arrayDam = optimizer . optimize ( appModel , suitableCloudOffer ) ; code_block = ForStatement ; log . info ( "=== TEST for SOLUTION GENERATION of BLIND optimizer FINISEHD ===" ) ; }
public void test() { try { String value = this . extractValue ( ) ; code_block = IfStatement ; IParameterParentTag parentTag = ( IParameterParentTag ) findAncestorWithClass ( this , IParameterParentTag . class ) ; parentTag . addParameter ( this . getName ( ) , value ) ; } catch ( Throwable t ) { _logger . error ( "Error closing tag" , t ) ; throw new JspException ( "Error closing tag " , t ) ; } }
public void test() { try { validator . validate ( context , reader ) ; } catch ( ObjectValidityException e ) { logger . error ( "Object validation error " + reader . GetObjectPID ( ) + ": " + e . getMessage ( ) ) ; throw e ; } }
public void test() { try { fileSystem = dir . getFileSystem ( hadoopConf ) ; res = fileSystem . exists ( dir ) ; } catch ( IOException e ) { LOG . warn ( "Exception checking for exists on: " + key ) ; } }
public void test() { try { GridCompoundFuture < SchemaIndexCacheStat , SchemaIndexCacheStat > compoundFut = ( GridCompoundFuture < SchemaIndexCacheStat , SchemaIndexCacheStat > ) fut ; SchemaIndexCacheStat resStat = new SchemaIndexCacheStat ( ) ; compoundFut . futures ( ) . stream ( ) . map ( IgniteInternalFuture :: result ) . filter ( Objects :: nonNull ) . forEach ( resStat :: accumulate ) ; log . info ( indexStatStr ( resStat ) ) ; } catch ( Exception e ) { log . error ( "Error when trying to print index build/rebuild statistics [cacheName=" + cctx . cache ( ) . name ( ) + ", grpName=" + cctx . group ( ) . name ( ) + "]" , e ) ; } }
public void test() { try { doOperations ( ) ; periodicRemoveObsoletes ( ) ; code_block = TryStatement ;  } catch ( Exception e ) { RUNTIME_LOGGER . error ( ERROR_OPT_RES , e ) ; } }
public void debugFreeList ( ) { LOG . debug ( "{}: {}" , FileUtils . fileName ( getFile ( ) ) , freeList . toString ( ) ) ; }
public void test() { try { Event event = serializer . deserialize ( message ) ; return Optional . of ( event ) ; } catch ( Exception e ) { LOGGER . error ( "Unable to deserialize '{}'" , message , e ) ; return Optional . empty ( ) ; } }
public void test() { try { Class . forName ( getChosenSQLDriver ( ) ) ; } catch ( ClassNotFoundException e ) { LOGGER . error ( getChosenSQLDriver ( ) + " class not found." , e ) ; } }
@ Override public void startFolderEvent ( Session session , FileOperation op , Path file , Set < PosixFilePermission > perms ) throws IOException { log . info ( "startFolderEvent({})[{}] {}" , session , op , file ) ; }
public void test() { try { ProcessBuilder processBuilder = new ProcessBuilder ( command ) . directory ( appCDSDir . toFile ( ) ) ; code_block = IfStatement ; exitCode = processBuilder . start ( ) . waitFor ( ) ; } catch ( Exception e ) { log . debug ( "Failed to launch process used to create '" + CLASSES_LIST_FILE_NAME + "'." , e ) ; return null ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "unmapSession(id={}): {}" , sessionId , ioSession ) ; } }
private RestResponse < CustomGroup > groups ( ) throws CatalogException , ClientException { logger . debug ( "Groups" ) ; studiesCommandOptions . groupsCommandOptions . study = getSingleValidStudy ( studiesCommandOptions . groupsCommandOptions . study ) ; ObjectMap params = new ObjectMap ( ) ; params . putIfNotNull ( "id" , studiesCommandOptions . groupsCommandOptions . group ) ; return openCGAClient . getStudyClient ( ) . groups ( studiesCommandOptions . groupsCommandOptions . study , params ) ; }
protected void deactivate ( ComponentContext componentContext ) { logger . info ( "deactivate {}..." , componentContext . getProperties ( ) . get ( ConfigurationService . KURA_SERVICE_PID ) ) ; code_block = IfStatement ; this . dataService . removeDataServiceListener ( this ) ; this . cloudClients . clear ( ) ; this . dataService = null ; this . systemService = null ; this . systemAdminService = null ; this . networkService = null ; this . positionService = null ; this . eventAdmin = null ; this . certificatesService = null ; this . cloudServiceRegistration . unregister ( ) ; this . notificationPublisherRegistration . unregister ( ) ; }
public void test() { try { publishDisconnectCertificate ( ) ; } catch ( KuraException e ) { logger . warn ( "Cannot publish disconnect certificate" ) ; } }
public void test() { if ( include . isResource ( ) ) { LOG . info ( "Loading includes from resource: {}" , include . getFile ( ) ) ; includeTopologyDef = parseResource ( include . getFile ( ) , true , false , properties , envSub ) ; } else { LOG . info ( "Loading includes from file: {}" , include . getFile ( ) ) ; includeTopologyDef = parseFile ( include . getFile ( ) , true , false , properties , envSub ) ; } }
public void test() { if ( include . isResource ( ) ) { LOG . info ( "Loading includes from resource: {}" , include . getFile ( ) ) ; includeTopologyDef = parseResource ( include . getFile ( ) , true , false , properties , envSub ) ; } else { LOG . info ( "Loading includes from file: {}" , include . getFile ( ) ) ; includeTopologyDef = parseFile ( include . getFile ( ) , true , false , properties , envSub ) ; } }
public void test() { if ( config . containsKey ( key ) ) { LOG . warn ( "Ignoring attempt to set topology config property '{}' with override == false" , key ) ; } else { config . put ( key , includeConfig . get ( key ) ) ; } }
@ Override public void start ( final Map < String , String > props ) { log . info ( "Starting JDBC Sink task" ) ; config = new JdbcSinkConfig ( props ) ; initWriter ( ) ; remainingRetries = config . maxRetries ; code_block = TryStatement ;  }
public void test() { if ( LOG . isInfoEnabled ( ) ) { StringBuilder entityInfo = new StringBuilder ( "Entity[" ) ; entityInfo . append ( dataDefinition . getPluginIdentifier ( ) ) . append ( '.' ) . append ( dataDefinition . getName ( ) ) ; entityInfo . append ( "][id=" ) . append ( entityId ) . append ( "] " ) ; entityInfo . append ( message ) ; LOG . info ( entityInfo . toString ( ) ) ; } }
public void test() { if ( verboseLogs ) { log . info ( "Waiting until Address space configuration will be applied. Current: {}" , addressSpace . getAnnotation ( AnnotationKeys . APPLIED_CONFIGURATION ) ) ; } }
public boolean evaluateState ( VeluxBridge bridge ) { logger . trace ( "evaluateState() called." ) ; boolean success = false ; GetHouseStatus bcp = bridge . bridgeAPI ( ) . getHouseStatus ( ) ; code_block = IfStatement ; logger . debug ( "evaluateState() finished {}." , ( success ? "successfully" : "with failure" ) ) ; return success ; }
public void test() { try { final CertPath certChain = message . getCertificateChain ( ) ; code_block = IfStatement ; final List < ? extends Certificate > list = certChain . getCertificates ( ) ; final int pathSize = list . size ( ) ; code_block = IfStatement ; code_block = IfStatement ; adapter . runOnContext ( ( v ) -> validateCertificateAndLoadDevice ( cid , certChain , session ) ) ; return null ; } catch ( HandshakeException e ) { LOG . debug ( "certificate validation failed" , e ) ; return new CertificateVerificationResult ( cid , e , null ) ; } }
public void test() { if ( res . succeeded ( ) && ( res . result ( ) . statusCode ( ) == 200 || res . result ( ) . statusCode ( ) == 404 ) ) { LOGGER . debug ( "Canceling of the job {} done with status code {} " , id , res . result ( ) . statusCode ( ) ) ; } else { LOGGER . error ( "Canceling of job {} failed with response code {}" , id , res . result ( ) . statusCode ( ) , res . cause ( ) ) ; } }
public void test() { if ( res . succeeded ( ) && ( res . result ( ) . statusCode ( ) == 200 || res . result ( ) . statusCode ( ) == 404 ) ) { LOGGER . debug ( "Canceling of the job {} done with status code {} " , id , res . result ( ) . statusCode ( ) ) ; } else { LOGGER . error ( "Canceling of job {} failed with response code {}" , id , res . result ( ) . statusCode ( ) , res . cause ( ) ) ; } }
public void test() { for ( Entry < Path , String > outputDataEntry : outputDataByPath . entrySet ( ) ) { Path outputPath = outputDataEntry . getKey ( ) ; LOGGER . debug ( "Writing: {}" , outputPath ) ; outputPath . getParent ( ) . toFile ( ) . mkdirs ( ) ; MoreFiles . asCharSink ( outputPath , UTF_8 ) . write ( outputDataEntry . getValue ( ) ) ; } }
@ Override public void transportInterupted ( ) { LOG . info ( "Worker " + name + " was interrupted..." ) ; interruptedCount . incrementAndGet ( ) ; }
public void test() { try { ObjectMapper objectMapper = new ObjectMapper ( ) ; fieldValues = objectMapper . readValue ( partitionfieldValuesStr , Map . class ) ; code_block = ForStatement ; return fieldValues ; } catch ( Exception e ) { LOG . error ( "" , e ) ; throw new RuntimeException ( e ) ; } }
public void test() { if ( ! elementFile . delete ( ) ) { logger . warn ( "Could not properly delete element file: {}" , elementFile ) ; } }
public void test() { if ( elementFile . exists ( ) ) { code_block = IfStatement ; } else { logger . warn ( "Tried to delete non-existent element file. Perhaps was already deleted?: {}" , elementFile ) ; } }
public void test() { if ( ! elementDir . delete ( ) ) { logger . warn ( "Could not properly delete element directory: {}" , elementDir ) ; } }
public void test() { if ( elementDir . list ( ) . length == 0 ) { code_block = IfStatement ; } else { logger . warn ( "Element directory was not empty after deleting element. Skipping deletion: {}" , elementDir ) ; } }
public void test() { if ( elementDir != null && elementDir . exists ( ) && elementDir . list ( ) != null ) { code_block = IfStatement ; } else { logger . warn ( "Element directory did not exist when trying to delete it: {}" , elementDir ) ; } }
public void test() { if ( ! mediapackageDir . delete ( ) ) { logger . warn ( "Could not properly delete mediapackage directory: {}" , mediapackageDir ) ; } }
public void test() { if ( mediapackageDir . list ( ) . length == 0 ) { code_block = IfStatement ; } else { logger . debug ( "Mediapackage directory was not empty after deleting element. Skipping deletion: {}" , mediapackageDir ) ; } }
public void test() { if ( mediapackageDir != null && mediapackageDir . exists ( ) ) { code_block = IfStatement ; } else { logger . warn ( "Mediapackage directory did not exist when trying to delete it: {}" , mediapackageDir ) ; } }
public StepPhase stageApp ( CloudApplication app ) { CloudPackage cloudPackage = context . getVariable ( Variables . CLOUD_PACKAGE ) ; code_block = IfStatement ; logger . info ( Messages . STAGING_APP , app . getName ( ) ) ; return createBuild ( cloudPackage . getGuid ( ) ) ; }
public void test() { if ( encryptedData . isIntegrityProtected ( ) ) { code_block = IfStatement ; } else { logger . warn ( "No message integrity check" ) ; } }
public void test() { if ( ! isAttached ( device ) ) { log . info ( "device {} not attached" , device . getName ( ) ) ; return ; } }
public void test() { if ( message == null ) { logger . warn ( null , t ) ; return ; } }
public void test() { try { Files . delete ( file ) ; } catch ( Exception e ) { log . error ( "Cannot delete file [" + file + "]: " + e . getMessage ( ) ) ; } }
public void test() { try { jid = JavaId . getJavaId ( type , qname ) ; def_ctx = JavaId . getCompilationUnit ( jid ) ; log . debug ( "Determined compilation unit " + def_ctx + " for qname [" + qname + "]" ) ; file = ClassDownloader . getInstance ( ) . getClass ( mvnGroup , artifact , version , def_ctx . getQualifiedName ( ) , ClassDownloader . Format . JAVA ) ; code_block = IfStatement ; } catch ( IllegalArgumentException iae ) { log . error ( "Error: " + iae . getMessage ( ) , iae ) ; throw new RuntimeException ( iae . getMessage ( ) ) ; } catch ( FileNotFoundException e ) { log . error ( "Error: " + e . getMessage ( ) , e ) ; throw new RuntimeException ( "Cannot read file [" + file + "]" ) ; } catch ( IOException e ) { log . error ( "Error: " + e . getMessage ( ) , e ) ; throw new RuntimeException ( "IO exception when reading file [" + file + "]" ) ; } catch ( Exception e ) { log . error ( "Error: " + e . getMessage ( ) , e ) ; throw new RuntimeException ( e . getClass ( ) . getSimpleName ( ) + " when writing file to output stream: " + e . getMessage ( ) ) ; } }
public void test() { try ( Connection con = getDatasource ( ) . getConnection ( ) ; PreparedStatement stmt = con . prepareStatement ( query ) ; ) { ResultSet rs = stmt . executeQuery ( ) ; code_block = WhileStatement ; } catch ( SQLException e ) { logger . error ( "Failed to retrieve value from sequence " + seqName , e ) ; String message = Messages . get ( locale , "error_db_seq" , seqName ) ; throw new Exception ( message ) ; } }
public void test() { if ( srcContentSummary . getFileCount ( ) > conf . getLongVar ( HiveConf . ConfVars . HIVE_EXEC_COPYFILE_MAXNUMFILES ) && srcContentSummary . getLength ( ) > conf . getLongVar ( HiveConf . ConfVars . HIVE_EXEC_COPYFILE_MAXSIZE ) ) { LOG . info ( "Source is " + srcContentSummary . getLength ( ) + " bytes. (MAX: " + conf . getLongVar ( HiveConf . ConfVars . HIVE_EXEC_COPYFILE_MAXSIZE ) + ")" ) ; LOG . info ( "Source is " + srcContentSummary . getFileCount ( ) + " files. (MAX: " + conf . getLongVar ( HiveConf . ConfVars . HIVE_EXEC_COPYFILE_MAXNUMFILES ) + ")" ) ; LOG . info ( "Launch distributed copy (distcp) job." ) ; triedDistcp = true ; copied = distCp ( srcFS , Collections . singletonList ( src ) , dst , deleteSource , null , conf , shims ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Error on creating column family, ignoring" , e ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Successfully wrote request to fence ledger and read entry: " + entryId + " ledger-id: " + ledgerId + " bookie: " + channel . getRemoteAddress ( ) ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( SQLException ex ) { Logger . error ( this . getClass ( ) . getName ( ) , ex . getMessage ( ) , ex ) ; } }
public void test() { if ( trimmedPassword != null && ! trimmedPassword . equals ( trimmedPassword2 ) ) { addFieldError ( "password2" , getText ( "validation.password2.wrong" ) ) ; LOG . error ( "The passwords entered do not match" ) ; password2 = null ; } else-if ( trimmedPassword == null ) { addFieldError ( "user.password" , getText ( "validation.password.reentered" ) ) ; LOG . error ( "The primary password entered is empty" ) ; } else { user . setPassword ( trimmedPassword ) ; LOG . error ( "The password has been reset" ) ; } }
public void test() { if ( trimmedPassword != null && ! trimmedPassword . equals ( trimmedPassword2 ) ) { addFieldError ( "password2" , getText ( "validation.password2.wrong" ) ) ; LOG . error ( "The passwords entered do not match" ) ; password2 = null ; } else-if ( trimmedPassword == null ) { addFieldError ( "user.password" , getText ( "validation.password.reentered" ) ) ; LOG . error ( "The primary password entered is empty" ) ; } else { user . setPassword ( trimmedPassword ) ; LOG . error ( "The password has been reset" ) ; } }
public void test() { if ( WARN_ON_ERROR ) { numErrors ++ ; logger . warn ( "***** " + e . getMessage ( ) + " *****" ) ; } else { throw e ; } }
public void test() { try { this . saveLevelDat ( ) ; } catch ( IOException cause ) { this . logger . warn ( "level.dat for world '{}' could not be saved: " , this . levelName , cause ) ; } }
public void test() { if ( ( enclosingTrace != null ) && ( enclosingTrace . getTraceId ( ) != tp . traceId ) ) { LOGGER . error ( "Enclosing trace does not match split point. Found: {} expected: {}" , enclosingTrace . getTraceId ( ) , enclosingTrace . getTraceId ( ) ) ; } }
public final Response putConnection ( final ComponentConnection body ) { code_block = IfStatement ; String path = String . format ( CONNECTION_PATH , body . getObjectId ( ) ) ; log . debug ( "" ) ; return putObjectToSystemMng ( path , body ) ; }
public void test() { try { code_block = IfStatement ; } catch ( IllegalArgumentException e ) { LOGGER . debug ( "Unable to get X500 Name" , e ) ; } }
public void test() { try { instants = metaClient . scanHoodieInstantsFromFileSystem ( new Path ( metaClient . getMetaAuxiliaryPath ( ) ) , HoodieActiveTimeline . VALID_EXTENSIONS_IN_ACTIVE_TIMELINE , false ) ; } catch ( FileNotFoundException e ) { LOG . warn ( "Aux path not found. Skipping: " + metaClient . getMetaAuxiliaryPath ( ) ) ; return success ; } }
public void test() { try { result = read ( attribute ) . get ( ) ; } catch ( InterruptedException e ) { logger . debug ( "readSync interrupted" ) ; return null ; } catch ( ExecutionException e ) { logger . debug ( "readSync exception " , e ) ; return null ; } }
public void test() { try { result = read ( attribute ) . get ( ) ; } catch ( InterruptedException e ) { logger . debug ( "readSync interrupted" ) ; return null ; } catch ( ExecutionException e ) { logger . debug ( "readSync exception " , e ) ; return null ; } }
@ EventListener ( ApplicationReadyEvent . class ) @ Order ( value = 2 ) public void onApplicationEvent ( ApplicationReadyEvent event ) { log . info ( "Subscribing to notifications: {}" , nfConsumer . getTopic ( ) ) ; this . nfConsumer . subscribe ( ) ; launchNotificationsConsumer ( ) ; launchMainConsumers ( ) ; }
public void test() { if ( ! subscriptionStates . containsKey ( subscriptionId ) || ! broadcastSubscriptionListenerDirectory . containsKey ( subscriptionId ) ) { logger . error ( "Received publication for not existing subscription callback with subscriptionId {}" , subscriptionId ) ; } }
public void test() { try { dispatchMessageQueue . put ( new ServerEvent ( EventType . SERVER_FAILED , server . psLoc ) ) ; } catch ( Exception e ) { LOG . error ( "add SERVER_FAILED event for request failed, " , e ) ; } }
private boolean updateTextSections ( ) { XTextSectionsSupplier supp = UNO . XTextSectionsSupplier ( doc ) ; code_block = IfStatement ; long startTime = System . currentTimeMillis ( ) ; HashSet < String > knownTextSections = new HashSet < > ( ) ; HashSet < TextSection > invalidTextSections = new HashSet < > ( ) ; code_block = ForStatement ; HashSet < TextSection > newTextSections = new HashSet < > ( ) ; String [ ] textSectionNames = supp . getTextSections ( ) . getElementNames ( ) ; code_block = ForStatement ; removeInvalidTextSections ( invalidTextSections ) ; addNewTextSections ( newTextSections ) ; LOGGER . trace ( "updateTextSections fertig nach {} ms. Entfernte/Neue TextSections: {} / {}" , Integer . valueOf ( ( int ) ( System . currentTimeMillis ( ) - startTime ) ) , invalidTextSections . size ( ) , newTextSections . size ( ) ) ; return ! invalidTextSections . isEmpty ( ) || ! newTextSections . isEmpty ( ) ; }
public void test() { try { final List < Elem > choices = new ArrayList < Elem > ( ) ; getContainer ( ) . run ( true , true , new IRunnableWithProgress ( ) code_block = "" ; ) ; if ( ! choices . isEmpty ( ) ) return choices ; MessageDialog . openConfirm ( getShell ( ) , getLocalString ( "_UI_No_modules_found" ) , getLocalString ( "_UI_No_module_matching_keyword" , keyword ) ) ; } catch ( InvocationTargetException e ) { Throwable t = e . getTargetException ( ) ; StringBuilder builder = new StringBuilder ( ) ; builder . append ( t . getClass ( ) . getName ( ) ) ; builder . append ( "\n" ) ; builder . append ( t . getMessage ( ) ) ; builder . append ( "\n\n(See the log view for technical details)." ) ; MessageDialog . openError ( getShell ( ) , "Error while communicating with the ForgeAPI." , builder . toString ( ) ) ; log . error ( "Error while communicating with the ForgeAPI" , t ) ; } catch ( InterruptedException e ) { } }
public void test() { try { Map < String , String > params = new LinkedHashMap < String , String > ( ) ; code_block = IfStatement ; params . put ( OUTPUT_FORMAT_PARAM , DEFAULT_OUTPUT_FORMAT ) ; LOGGER . debug ( "HttpGet " + fullUrl . toString ( ) + " number of params: " + params . size ( ) ) ; HttpResponse response = httpGet ( fullUrl , params ) ; LOGGER . debug ( "Response code: " + response . getResponseCode ( ) + "\n\t" + response . getBody ( ) ) ; checkResponse ( response ) ; code_block = IfStatement ; } catch ( ArcgisException e ) { throw e ; } catch ( Exception e ) { throw new ArcgisException ( "getTableAttributesInfo, Unexpected Exception " + e . toString ( ) ) ; } }
public void test() { try { Map < String , String > params = new LinkedHashMap < String , String > ( ) ; code_block = IfStatement ; params . put ( OUTPUT_FORMAT_PARAM , DEFAULT_OUTPUT_FORMAT ) ; LOGGER . debug ( "HttpGet " + fullUrl . toString ( ) + " number of params: " + params . size ( ) ) ; HttpResponse response = httpGet ( fullUrl , params ) ; LOGGER . debug ( "Response code: " + response . getResponseCode ( ) + "\n\t" + response . getBody ( ) ) ; checkResponse ( response ) ; code_block = IfStatement ; } catch ( ArcgisException e ) { throw e ; } catch ( Exception e ) { throw new ArcgisException ( "getTableAttributesInfo, Unexpected Exception " + e . toString ( ) ) ; } }
public void test() { if ( response . isSuccessful ( ) ) { responseJSON = response . getBody ( ) ; LOGGER . debug ( "    tokenJSON: " + responseJSON ) ; getUniqueIdFieldFromJson ( responseJSON ) ; getAttributeInfoFromJson ( responseJSON ) ; getAttributeIndexFromJson ( responseJSON ) ; } else { String errorMsg = "getTableAttributesInfo: Unexpected server response, Error: " + response . getErrorCode ( ) + "\n" + response . getErrorMessage ( ) ; errorMsg += " \n\t token: " + token ; LOGGER . error ( errorMsg ) ; throw new ArcgisException ( errorMsg ) ; } }
@ PayloadRoot ( localPart = "SetDeviceVerificationKeyRequest" , namespace = DEVICE_MANAGEMENT_NAMESPACE ) @ ResponsePayload public SetDeviceVerificationKeyAsyncResponse setDeviceVerificationKey ( @ OrganisationIdentification final String organisationIdentification , @ RequestPayload final SetDeviceVerificationKeyRequest request , @ MessagePriority final String messagePriority ) throws OsgpException { LOGGER . info ( "Set Device Verification Key Request received from organisation: {} for device: {} with message priority: {}." , organisationIdentification , request . getDeviceIdentification ( ) , messagePriority ) ; final SetDeviceVerificationKeyAsyncResponse response = new SetDeviceVerificationKeyAsyncResponse ( ) ; code_block = TryStatement ;  return response ; }
protected Scroll buildScroll ( BulkCommand command ) { ScrollRequest request ; String query = command . getQuery ( ) ; log . debug ( "Build scroll with query: {}" , query ) ; code_block = IfStatement ; ScrollService service = Framework . getService ( ScrollService . class ) ; return service . scroll ( request ) ; }
private void executeImpl ( ) { log . info ( "Data purge started" ) ; boolean purgePoints = SystemSettingsDao . instance . getBooleanValue ( ENABLE_POINT_DATA_PURGE ) ; this . countPointValues = SystemSettingsDao . instance . getBooleanValue ( SystemSettingsDao . POINT_DATA_PURGE_COUNT ) ; code_block = IfStatement ; filedataPurge ( ) ; if ( deletedFiles > 0 ) log . info ( "Filedata purge ended, " + deletedFiles + " files deleted" ) ; eventPurge ( ) ; for ( PurgeDefinition def : ModuleRegistry . getDefinitions ( PurgeDefinition . class ) ) def . execute ( runtime ) ; }
public void test() { if ( countPointValues ) { log . info ( "Data purge ended, " + deletedSamples + " point values were deleted" ) ; } else-if ( anyDeletedSamples ) { log . info ( "Data purge ended, unknown number of point values were deleted" ) ; } else { log . info ( "Data purge ended, no point values were deleted" ) ; } }
public void test() { if ( countPointValues ) { log . info ( "Data purge ended, " + deletedSamples + " point values were deleted" ) ; } else-if ( anyDeletedSamples ) { log . info ( "Data purge ended, unknown number of point values were deleted" ) ; } else { log . info ( "Data purge ended, no point values were deleted" ) ; } }
public void test() { if ( countPointValues ) { log . info ( "Data purge ended, " + deletedSamples + " point values were deleted" ) ; } else-if ( anyDeletedSamples ) { log . info ( "Data purge ended, unknown number of point values were deleted" ) ; } else { log . info ( "Data purge ended, no point values were deleted" ) ; } }
public void test() { if ( purgePoints ) { List < PurgeFilter > purgeFilters = new ArrayList < PurgeFilter > ( ) ; for ( PurgeFilterDefinition pfd : ModuleRegistry . getDefinitions ( PurgeFilterDefinition . class ) ) purgeFilters . add ( pfd . getPurgeFilter ( ) ) ; List < DataPointVO > dataPoints = dataPointDao . getAll ( ) ; for ( DataPointVO dataPoint : dataPoints ) purgePoint ( dataPoint , countPointValues , purgeFilters ) ; code_block = IfStatement ; pointValueDao . deleteOrphanedPointValueAnnotations ( ) ; code_block = IfStatement ; } else { log . info ( "Purge for data points not enabled, skipping." ) ; } }
public void test() { { OuterJoinOperator oper = new OuterJoinOperator ( ) ; oper . setFullJoin ( true ) ; CollectorTestSink sink = new CollectorTestSink ( ) ; oper . outport . setSink ( sink ) ; Condition cond = new JoinColumnEqualCondition ( "a" , "a" ) ; oper . setJoinCondition ( cond ) ; oper . selectTable1Column ( new ColumnIndex ( "b" , null ) ) ; oper . selectTable2Column ( new ColumnIndex ( "c" , null ) ) ; oper . setup ( null ) ; oper . beginWindow ( 1 ) ; HashMap < String , Object > tuple = new HashMap < String , Object > ( ) ; tuple . put ( "a" , 0 ) ; tuple . put ( "b" , 1 ) ; tuple . put ( "c" , 2 ) ; oper . inport1 . process ( tuple ) ; tuple = new HashMap < String , Object > ( ) ; tuple . put ( "a" , 1 ) ; tuple . put ( "b" , 3 ) ; tuple . put ( "c" , 4 ) ; oper . inport1 . process ( tuple ) ; tuple = new HashMap < String , Object > ( ) ; tuple . put ( "a" , 2 ) ; tuple . put ( "b" , 11 ) ; tuple . put ( "c" , 12 ) ; oper . inport1 . process ( tuple ) ; tuple = new HashMap < String , Object > ( ) ; tuple . put ( "a" , 0 ) ; tuple . put ( "b" , 7 ) ; tuple . put ( "c" , 8 ) ; oper . inport2 . process ( tuple ) ; tuple = new HashMap < String , Object > ( ) ; tuple . put ( "a" , 1 ) ; tuple . put ( "b" , 5 ) ; tuple . put ( "c" , 6 ) ; oper . inport2 . process ( tuple ) ; oper . endWindow ( ) ; oper . teardown ( ) ; LOG . debug ( "{}" , sink . collectedTuples ) ; } }
private void severeCannotLoad ( AbstractResource menubarsLayoutXmlResource , Exception cause ) { log . error ( "{}: could not find readable resource {} for the Menubars-Layout." , WebAppContextPath . class . getName ( ) , menubarsLayoutXmlResource , cause ) ; }
public void test() { if ( u == null ) { logger . warn ( "Ignored child URL: {} in {}" , attrValue , url ) ; return ; } }
public void test() { if ( u == null ) { logger . warn ( "Ignored child URL: {} in {}" , attrValue , url ) ; return ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Add Child: {}" , u ) ; } }
public void test() { if ( StringUtil . isNotBlank ( u ) ) { code_block = IfStatement ; urlList . add ( u ) ; } else-if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Skip Child: {}" , u ) ; } }
public void test() { try { byte [ ] keySrcBytes = encodeUTF8 ( keyString ) ; byte [ ] newKey1 = Base64 . decodeBase64 ( keySrcBytes ) ; Key newKey2 = toKey ( newKey1 ) ; byte [ ] srcBytes = encodeUTF8 ( password ) ; byte [ ] desBytes = decrypt ( Base64 . decodeBase64 ( srcBytes ) , newKey2 ) ; String tempdecodeUTF8 = decodeUTF8 ( desBytes ) ; code_block = IfStatement ; return password ; } catch ( Exception e ) { logger . debug ( e . getMessage ( ) ) ; return password ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( portalException , portalException ) ; } }
public void test() { if ( command instanceof RefreshType ) { VelbusStatusRequestPacket packet = new VelbusStatusRequestPacket ( getModuleAddress ( ) . getChannelIdentifier ( channelUID ) ) ; byte [ ] packetBytes = packet . getBytes ( ) ; velbusBridgeHandler . sendPacket ( packetBytes ) ; } else-if ( command instanceof UpDownType ) { UpDownType s = ( UpDownType ) command ; code_block = IfStatement ; } else-if ( command instanceof StopMoveType ) { StopMoveType s = ( StopMoveType ) command ; code_block = IfStatement ; } else-if ( command instanceof PercentType ) { VelbusBlindPositionPacket packet = new VelbusBlindPositionPacket ( getModuleAddress ( ) . getChannelIdentifier ( channelUID ) , ( ( PercentType ) command ) . byteValue ( ) ) ; byte [ ] packetBytes = packet . getBytes ( ) ; velbusBridgeHandler . sendPacket ( packetBytes ) ; } else { logger . debug ( "The command '{}' is not supported by this handler." , command . getClass ( ) ) ; } }
public void test() { try { URI uri = new URI ( homepageUrl ) ; setHomepage ( uri ) ; } catch ( URISyntaxException e ) { LOG . warn ( "Dataset homepageURL was invalid: {}" , Strings . nullToEmpty ( homepageUrl ) ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( KBArticleServiceUtil . class , "getKBArticleRSS" , _getKBArticleRSSParameterTypes17 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , resourcePrimKey , status , rssDelta , rssDisplayStyle , rssFormat , themeDisplay ) ; Object returnObj = null ; code_block = TryStatement ;  return ( String ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { if ( isDebugEnabled_SERIALIZER ) { logger . trace ( LogMarker . SERIALIZER_VERBOSE , "basicWriteObject: {}" , o ) ; } }
public void test() { if ( isDebugEnabled_SERIALIZER ) { logger . trace ( LogMarker . SERIALIZER_VERBOSE , "Writing DataSerializable: {}" , o ) ; } }
public void test() { if ( logger . isTraceEnabled ( LogMarker . SERIALIZER_ANNOUNCE_TYPE_WRITTEN_VERBOSE ) ) { logger . trace ( LogMarker . SERIALIZER_ANNOUNCE_TYPE_WRITTEN_VERBOSE , "DataSerializer Serializing an instance of {}" , o . getClass ( ) . getName ( ) ) ; } }
public void test() { -> { LOGGER . info ( "Verifying that consumer name is created with 'kafka-bridge-consumer-' plus random hashcode" ) ; assertThat ( ar . succeeded ( ) , is ( true ) ) ; HttpResponse < JsonObject > response = ar . result ( ) ; LOGGER . info ( "Response code from the Bridge is " + response . statusCode ( ) ) ; assertThat ( response . statusCode ( ) , is ( HttpResponseStatus . OK . code ( ) ) ) ; JsonObject bridgeResponse = response . body ( ) ; consumerInstanceId = bridgeResponse . getString ( "instance_id" ) ; LOGGER . info ( "Consumer instance of the consumer is " + consumerInstanceId ) ; assertThat ( consumerInstanceId . startsWith ( "kafka-bridge-consumer-" ) , is ( true ) ) ; create . complete ( true ) ; } }
public void test() { -> { LOGGER . info ( "Verifying that consumer name is created with 'kafka-bridge-consumer-' plus random hashcode" ) ; assertThat ( ar . succeeded ( ) , is ( true ) ) ; HttpResponse < JsonObject > response = ar . result ( ) ; LOGGER . info ( "Response code from the Bridge is " + response . statusCode ( ) ) ; assertThat ( response . statusCode ( ) , is ( HttpResponseStatus . OK . code ( ) ) ) ; JsonObject bridgeResponse = response . body ( ) ; consumerInstanceId = bridgeResponse . getString ( "instance_id" ) ; LOGGER . info ( "Consumer instance of the consumer is " + consumerInstanceId ) ; assertThat ( consumerInstanceId . startsWith ( "kafka-bridge-consumer-" ) , is ( true ) ) ; create . complete ( true ) ; } }
public void test() { if ( ( condition . success == null || condition . success == succeeded ) && condition . predicate . test ( rCtx , rCommand ) ) { assert condition . action != null ; log . trace ( "Condition succeeded" ) ; toExecute . add ( condition . action ) ; code_block = IfStatement ; } else { log . trace ( "Condition test failed" ) ; } }
public void test() { try { output = mapper . readValue ( json , clazz ) ; } catch ( Exception e ) { log . error ( "Could no de-serialize the following json " + json , e ) ; } }
public void test() { if ( ! NotificationService . class . isAssignableFrom ( clazz ) ) { logger . error ( "Found configuration for Notification Service with ID '{}' and Class '{}' but class is not a Notification Service." , serviceId , className ) ; return null ; } }
public void test() { try { test = getNameFromIp ( host ) ; } catch ( UnknownHostException e1 ) { LOG . error ( "Unable to determine host name for IP: " + host + ", setting to default pool" , e1 ) ; return Collections . singletonList ( DEFAULT_POOL ) ; } }
public void test() { if ( cmdId > MAX_ID ) { id . set ( 0 ) ; } }
public void test() { try { JsonObject fullCommand = new JsonObject ( ) ; int cmdId = id . incrementAndGet ( ) ; code_block = IfStatement ; fullCommand . addProperty ( "id" , cmdId ) ; fullCommand . addProperty ( "method" , command ) ; fullCommand . add ( "params" , JsonParser . parseString ( params ) ) ; MiIoSendCommand sendCmd = new MiIoSendCommand ( cmdId , MiIoCommand . getCommand ( command ) , fullCommand , cloudServer ) ; concurrentLinkedQueue . add ( sendCmd ) ; code_block = IfStatement ; code_block = IfStatement ; return cmdId ; } catch ( JsonSyntaxException e ) { logger . warn ( "Send command '{}' with parameters {} -> {} (Device: {}) gave error {}" , command , params , ip , Utils . getHex ( deviceId ) , e . getMessage ( ) ) ; throw e ; } }
public void test() { try { FileDTO dto = new FileDTO ( ) ; dto . setFileName ( pckg . getName ( ) . replaceAll ( "\\s" , "_" ) + ".zip" ) ; tempZipFile = File . createTempFile ( "experimentDownload_" , ".zip" ) ; fileOutputStream = new FileOutputStream ( tempZipFile ) ; zipOutputStream = new ZipOutputStream ( fileOutputStream ) ; code_block = ForStatement ; dto . setFile ( tempZipFile ) ; FileUtils . deleteOnExitQuietly ( tempZipFile ) ; IOUtils . closeQuietly ( zipOutputStream ) ; IOUtils . closeQuietly ( fileOutputStream ) ; return dto ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; IOUtils . closeQuietly ( zipOutputStream ) ; IOUtils . closeQuietly ( fileOutputStream ) ; FileUtils . deleteOnExitQuietly ( tempZipFile ) ; FileUtils . deleteOnExitQuietly ( file ) ; FileUtils . deleteQuietly ( tempZipFile ) ; FileUtils . deleteQuietly ( file ) ; return null ; } }
public void test() { try { XMLConfiguration . validate ( XMLConfiguration . loadDocument ( new FileInputStream ( configFile ) ) ) ; } catch ( SAXParseException e ) { log . error ( e . getMessage ( ) ) ; fail ( e . getMessage ( ) ) ; } }
@ GET public Object getCountries ( @ QueryParam ( "fields" ) String fields ) { LOG . info ( "Getting all" ) ; List < Country > countries = CountryService . getInstance ( ) . getAll ( ) ; return parsedCountries ( countries , fields ) ; }
public void test() { { log . info ( Color . GREEN + "Stop_2 : empty column stop_id" + Color . NORMAL ) ; Context context = new Context ( ) ; CheckPointReport result = verifyValidation ( log , context , "stop_3_1" , GTFS_1_GTFS_Common_12 , SEVERITY . ERROR , RESULT . NOK , true ) ; Assert . assertEquals ( result . getCheckPointErrorCount ( ) , 1 , "detail count" ) ; code_block = ForStatement ; } }
public void test() { if ( input . length ( ) == 0 ) { logger . debug ( "Failed to cleanup the input " + origInput ) ; } }
public void test() { try { return _resourceTypeHandler . parsePath ( path ) ; } catch ( final Exception e ) { logger . debug ( "Failed to parse path '{}', returning null" , path , e ) ; return null ; } }
public void test() { if ( command instanceof PercentType ) { PercentType percent = ( PercentType ) command ; Transform transform = new Transform ( ) ; transform . setLuminanceGain ( percent . doubleValue ( ) / 100 ) ; TransformCommand transformCommand = new TransformCommand ( transform ) ; sendCommand ( transformCommand ) ; } else { logger . debug ( "Channel {} unable to process command {}" , CHANNEL_BRIGHTNESS , command ) ; } }
public void test() { try { sessions = new ArrayList < > ( getSocketAcceptor ( transport ) . getManagedSessions ( ) . values ( ) ) ; } catch ( IllegalArgumentException e ) { LOG . warn ( "Seems like the LDAP service ({}) has already been unbound." , getPort ( ) ) ; return ; } }
public void test() { try { code_block = ForStatement ; stopConsumers ( ) ; } catch ( Exception e ) { LOG . warn ( "Failed to sent NoD." , e ) ; } }
@ Override public void stop ( ) { code_block = TryStatement ;  started = false ; LOG . info ( "Ldap service stopped." ) ; }
public void test() { if ( message instanceof ObjectMessage ) { log . debug ( "message received is of ObjectMessage type." ) ; ObjectMessage objectMessage = ( ObjectMessage ) message ; UserActionRequest userActionRequest = ( UserActionRequest ) objectMessage . getObject ( ) ; log . debug ( "found action " + userActionRequest . getActionKey ( ) ) ; taskHandler . handleTask ( userActionRequest ) ; } else { log . debug ( "message received is not of ObjectMessage type." ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception e ) { log . error ( e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( objToLog ) ; } }
public void test() { { log . info ( Color . GREEN + "Stop_2_3 : missing column stop_lat" + Color . NORMAL ) ; Context context = new Context ( ) ; CheckPointReport result = verifyValidation ( log , context , "stop_2_3" , GTFS_1_GTFS_Common_9 , SEVERITY . ERROR , RESULT . NOK , true ) ; Assert . assertEquals ( result . getCheckPointErrorCount ( ) , 1 , "detail count" ) ; code_block = ForStatement ; } }
public void test() { if ( structure instanceof DestinationInfo ) { DestinationInfo destinationInfo = ( DestinationInfo ) structure ; LOG . info ( "Received: {}" , destinationInfo ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Received activation response [requestId=" + msg . getRequestId ( ) + ", nodeId=" + nodeId + "]" ) ; } }
public void test() { if ( log != null && log . isDebugEnabled ( ) ) { log . debug ( "IOException: " + e . getMessage ( ) ) ; } }
public void test() { if ( log != null && log . isErrorEnabled ( ) ) { log . error ( "Cannot find Maven application directory. Either specify 'maven.home' system property, or " + "M2_HOME environment variable." ) ; } }
public void test() { if ( ( errorCounter . getAndIncrement ( ) % 10 ) == 0 ) { log . error ( "Error updating aggregate host stats for SpawnBalancer: {}" , e . getMessage ( ) , e ) ; } else { log . error ( "Error updating aggregate host stats for SpawnBalancer: {}" , e . getMessage ( ) ) ; } }
public void test() { if ( ( errorCounter . getAndIncrement ( ) % 10 ) == 0 ) { log . error ( "Error updating aggregate host stats for SpawnBalancer: {}" , e . getMessage ( ) , e ) ; } else { log . error ( "Error updating aggregate host stats for SpawnBalancer: {}" , e . getMessage ( ) ) ; } }
@ Override public String getPassword ( ) { LOG . debug ( "Providing password for ssh authentication of user '{}'" , config . getUsername ( ) ) ; return config . getPassword ( ) ; }
public void test() { try { sourceHolder . close ( ) ; } catch ( Throwable e ) { logger . error ( "Error in closing data source " + sourceHolder , e ) ; } }
public void test() { if ( ActiveMQRALogger . LOGGER . isTraceEnabled ( ) ) { ActiveMQRALogger . LOGGER . trace ( "getMapNames()" ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Failed to close ledger {} : " , ledgerId , re ) ; } }
public void attachClean ( StgNZielobjekt instance ) { log . debug ( "attaching clean StgNZielobjekt instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . lock ( instance , LockMode . NONE ) ; log . debug ( "attach successful" ) ; } catch ( RuntimeException re ) { log . error ( "attach failed" , re ) ; throw re ; } }
public void test() { try { code_block = WhileStatement ; } catch ( SQLException e ) { log . error ( "query table error,{}" , e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( ReflectiveOperationException e ) { logger . warn ( "{} while creating {} instance: {}" , e . getClass ( ) . getSimpleName ( ) , state . getClass ( ) . getSimpleName ( ) , e . getMessage ( ) ) ; } }
public void test() { if ( isAccepted ) { item . setState ( newState ) ; } else { logger . debug ( "Received update of a not accepted type ({}) for item {}" , newState . getClass ( ) . getSimpleName ( ) , itemName ) ; } }
public void test() { try { GenericItem item = ( GenericItem ) itemRegistry . getItem ( itemName ) ; boolean isAccepted = false ; code_block = IfStatement ; code_block = IfStatement ; } catch ( ItemNotFoundException e ) { logger . debug ( "Received update for non-existing item: {}" , e . getMessage ( ) ) ; } }
public void test() { if ( channel == null ) { logger . debug ( "Cannot send command '{}' - socket channel was closed" , command ) ; } else { logger . debug ( "Sending Command: '{}'" , command ) ; channel . write ( toSend ) ; } }
public void test() { { log . info ( Color . GREEN + "Route_8 : route_long_name includes route_short_name" + Color . NORMAL ) ; Context context = new Context ( ) ; CheckPointReport result = verifyValidation ( log , context , "route_8" , GTFS_2_GTFS_Route_2 , SEVERITY . WARNING , RESULT . NOK , true ) ; Assert . assertEquals ( result . getCheckPointErrorCount ( ) , 1 , "detail count" ) ; code_block = ForStatement ; } }
public void test() { try { fs . delete ( basePath , true ) ; } catch ( IOException e ) { logger . warn ( e . getMessage ( ) ) ; } }
@ PostConstruct public void load ( ) { log . info ( "Reloading configuration" ) ; this . loadLoginModuleNames ( ) ; this . loadJaasConfig ( ) ; authenticatedSessionTimeoutMinutes = sysPropConfigStore . get ( "authenticatedSessionTimeoutMinutes" , 180 ) ; enforceMatchingUsernames = Boolean . parseBoolean ( sysPropConfigStore . get ( "zanata.enforce.matchingusernames" ) ) ; tokenExpiresInSeconds = sysPropConfigStore . getLong ( ACCESS_TOKEN_EXPIRES_IN_SECONDS , 3600 ) ; }
@ Transactional ( rollbackFor = ArrowheadException . class ) public void deleteSubscription ( final String eventType , final SystemRequestDTO subscriberSystem ) { logger . debug ( "deleteSubscriptionResponse started ..." ) ; final EventType validEventType = validateEventTypeIsInDB ( eventType ) ; final System validSubscriber = validateSystemRequestDTO ( subscriberSystem ) ; code_block = TryStatement ;  }
public void test() { try { final Optional < Subscription > subcriptionOptional = subscriptionRepository . findByEventTypeAndSubscriberSystem ( validEventType , validSubscriber ) ; code_block = IfStatement ; } catch ( final Exception ex ) { logger . debug ( ex . getMessage ( ) , ex ) ; throw new ArrowheadException ( CoreCommonConstants . DATABASE_OPERATION_EXCEPTION_MSG ) ; } }
public void test() { try { bis = new ByteArrayInputStream ( script . getBytes ( "UTF-8" ) ) ; externalType = pythonService . loadPythonScript ( bis , scriptName , customScriptType . getPythonClass ( ) , customScriptType . getCustomScriptType ( ) , new PyObject [ ] code_block = "" ; ) ; } catch ( UnsupportedEncodingException e ) { log . error ( String . format ( "%s. Script inum: %s" , e . getMessage ( ) , customScript . getInum ( ) ) , e ) ; } finally { IOUtils . closeQuietly ( bis ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception ex ) { log . error ( "Failed to initialize custom script: '{}'" , ex , customScript . getName ( ) ) ; } }
public void test() { try { bratRenderCommand ( getCasProvider ( ) . get ( ) ) . ifPresent ( cmd code_block = LoopStatement ; ) ; } catch ( IOException e ) { LOG . error ( "Unable to load data" , e ) ; error ( "Unable to load data: " + ExceptionUtils . getRootCauseMessage ( e ) ) ; aTarget . addChildren ( getPage ( ) , IFeedback . class ) ; } }
public void test() { try { return io . gomint . server . player . PlayerSkin . fromInputStream ( inputStream ) ; } catch ( IOException e ) { LOGGER . error ( "Could not read skin from input: " , e ) ; return null ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "writeBuffer({}) writing {} bytes" , this , buffer . available ( ) ) ; } }
public void test() { try { String q = "key=" + CouchDBUtils . appendQuotes ( pKeyColumnValue ) ; uri = new URI ( CouchDBConstants . PROTOCOL , null , httpHost . getHostName ( ) , httpHost . getPort ( ) , CouchDBConstants . URL_SEPARATOR + schemaName . toLowerCase ( ) + CouchDBConstants . URL_SEPARATOR + CouchDBConstants . DESIGN + tableName + CouchDBConstants . VIEW + pKeyColumnName , q , null ) ; HttpGet get = new HttpGet ( uri ) ; get . addHeader ( "Accept" , "application/json" ) ; response = httpClient . execute ( get ) ; InputStream content = response . getEntity ( ) . getContent ( ) ; Reader reader = new InputStreamReader ( content ) ; JsonObject json = gson . fromJson ( reader , JsonObject . class ) ; JsonElement jsonElement = json . get ( "rows" ) ; code_block = IfStatement ; JsonArray array = jsonElement . getAsJsonArray ( ) ; code_block = ForStatement ; } catch ( Exception e ) { log . error ( "Error while fetching column by id {}, Caused by {}." , pKeyColumnValue , e ) ; throw new KunderaException ( e ) ; } finally { closeContent ( response ) ; } }
@ Override public Void run ( ) { LOG . info ( "Starting Server using kerberos credential" ) ; startServer ( server ) ; return null ; }
public void test() { if ( testProcess == null ) { log . warn ( Messages . getString ( "ProcessorUtilities.nullProcess" ) ) ; continue ; } }
public void test() { try { CommerceInventoryWarehouseItemServiceUtil . moveQuantitiesBetweenWarehouses ( fromCommerceInventoryWarehouseId , toCommerceInventoryWarehouseId , sku , quantity ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { try { btree = new BTreeStore ( pool , STRUCTURAL_INDEX_ID , FILE_FORMAT_VERSION_ID , false , file , pool . getCacheManager ( ) ) ; } catch ( final DBException e ) { LOG . error ( "Failed to initialize structural index: {}" , e . getMessage ( ) , e ) ; throw new DatabaseConfigurationException ( e . getMessage ( ) , e ) ; } }
public void test() { try { String [ ] keys = key . split ( KEY_SEPARATOR ) ; int dpc = Integer . parseInt ( keys [ 0 ] ) ; code_block = ForStatement ; } catch ( Exception ex ) { logger . error ( String . format ( "Error while adding key=%s to As list=%s" , key , Arrays . toString ( asList ) ) ) ; } }
public void test() { try { method = Utils . findSetter ( field . getName ( ) , classEntity , field . getType ( ) ) ; currentBin = bins . get ( AnnotationUtils . deepFieldName ( field ) ) ; code_block = IfStatement ; } catch ( IllegalAccessException | InvocationTargetException | IllegalArgumentException e ) { LOG . error ( "impossible to create a java object from Bin:" + field . getName ( ) + " and type:" + field . getType ( ) + " and value:" + t + "; recordReceived:" + currentBin ) ; method . invoke ( t , Utils . castNumberType ( insert , classField ) ) ; } }
public void test() { if ( ! expired . isEmpty ( ) ) { deletedCount = mBlobRepository . deleteByIds ( expired ) ; logger . debug ( "Number of Mbob deleted: {}" , deletedCount ) ; } else { logger . debug ( "Nothing to delete" ) ; deletedCount = 0 ; } }
public void test() { if ( ! expired . isEmpty ( ) ) { deletedCount = mBlobRepository . deleteByIds ( expired ) ; logger . debug ( "Number of Mbob deleted: {}" , deletedCount ) ; } else { logger . debug ( "Nothing to delete" ) ; deletedCount = 0 ; } }
private void runJoinSample ( EPRuntime runtime ) { String epl = "select sw.* " + "from SampleJoinEventlastevent sje, MySampleWindow sw " + "where sw.key1 = sje.propOne and sw.key2 = sje.propTwo" ; EPStatement stmt = compileDeploy ( epl , runtime ) ; SampleUpdateListener sampleListener = new SampleUpdateListener ( ) ; stmt . addListener ( sampleListener ) ; runtime . getEventService ( ) . sendEventBean ( new SampleJoinEvent ( "sample1" , "sample2" ) , "SampleJoinEvent" ) ; log . info ( "Join query returned: " + sampleListener . getLastEvent ( ) . get ( "key1" ) + " and " + sampleListener . getLastEvent ( ) . get ( "key2" ) ) ; }
public void test() { if ( filter . matches ( map ) ) { LOGGER . trace ( "filter = {} matches {}" , importPAXWicketAPI ) ; return true ; } else { LOGGER . trace ( "filter = {} not matches {}" , importPAXWicketAPI ) ; } }
public void test() { if ( filter . matches ( map ) ) { LOGGER . trace ( "filter = {} matches {}" , importPAXWicketAPI ) ; return true ; } else { LOGGER . trace ( "filter = {} not matches {}" , importPAXWicketAPI ) ; } }
public void test() { try { Filter filter = paxBundleContext . createFilter ( filterString ) ; code_block = IfStatement ; } catch ( InvalidSyntaxException e ) { LOGGER . warn ( "can't parse filter expression: {}" , filterString ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( ClassNameServiceUtil . class , "fetchByClassNameId" , _fetchByClassNameIdParameterTypes0 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , classNameId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . portal . kernel . model . ClassName ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void setPrimaryFieldPos ( int p ) { LOG . debug ( "setPrimaryFieldPos()" ) ; primaryFieldPos = p ; }
public void test() { if ( IS_TIMER_COMPSS_ENABLED ) { timeAssignResourcesStart = System . nanoTime ( ) ; } }
public void test() { try { encodeSensorDataToNetcdf ( netcdfFile , sensorDataset , version ) ; return new BinaryAttachmentResponse ( Files . toByteArray ( netcdfFile ) , getContentType ( ) , String . format ( filename , makeDateSafe ( new DateTime ( DateTimeZone . UTC ) ) ) ) ; } catch ( IOException e ) { throw new EncodingException ( "Couldn't create netCDF file" , e ) ; } finally { LOGGER . debug ( "Temporary file deleted: {}" , tempDir . delete ( ) ) ; } }
public void test() { try { usersFileLoader . getProperties ( ) . remove ( username ) ; usersFileLoader . persistProperties ( ) ; getGroupsPropertiesManager ( ) . removeEntry ( username ) ; } catch ( IOException e ) { LOG . error ( "Error deleting user " + username , e ) ; throw new SecurityManagementException ( e ) ; } }
public void test() { if ( failIfLatencyProblem ) { thread . interrupt ( ) ; throw latencyException ; } else { log . warn ( latencyException . getMessage ( ) ) ; } }
public void test() { try { final Thread waitingThread = Thread . currentThread ( ) ; Thread thread ; code_block = IfStatement ; do code_block = "" ; code_block = WhileStatement ; while ( ! similarColor ( lastRemoteColor , Color . GREEN ) ) ; code_block = WhileStatement ; } catch ( InterruptedException e ) { log . debug ( "Finished LatencyController thread due to Interrupted Exception" ) ; } }
public static void schedule ( ) { service . scheduleWithFixedDelay ( new DataCacheHandler ( ) , 0 , TTL , TimeUnit . SECONDS ) ; logger . info ( "SchedulerManager:schedule: Started scheduler job for cache refresh with ttl in sec =" + TTL ) ; }
public void test() { if ( content != null ) { final Document change = DocumentReader . floatNumbersAsTextReader ( ) . read ( content ) ; LOGGER . trace ( "Change arrived for decoding {}" , change ) ; processor . process ( new Wal2JsonReplicationMessage ( txId , commitTime , change , containsMetadata , lastMessage , typeRegistry ) ) ; } else { LOGGER . trace ( "Empty change arrived" ) ; processor . process ( new NoopMessage ( txId , commitTime ) ) ; } }
public void test() { if ( content != null ) { final Document change = DocumentReader . floatNumbersAsTextReader ( ) . read ( content ) ; LOGGER . trace ( "Change arrived for decoding {}" , change ) ; processor . process ( new Wal2JsonReplicationMessage ( txId , commitTime , change , containsMetadata , lastMessage , typeRegistry ) ) ; } else { LOGGER . trace ( "Empty change arrived" ) ; processor . process ( new NoopMessage ( txId , commitTime ) ) ; } }
@ Test public void callerData ( ) { assertEquals ( 0 , listAppender . list . size ( ) ) ; PatternLayout pl = new PatternLayout ( ) ; pl . setPattern ( "%-5level [%class] %logger - %msg" ) ; pl . setContext ( lc ) ; pl . start ( ) ; listAppender . layout = pl ; Logger logger = Logger . getLogger ( "basic-test" ) ; logger . trace ( "none" ) ; assertEquals ( 0 , listAppender . list . size ( ) ) ; rootLogger . setLevel ( Level . TRACE ) ; logger . trace ( HELLO ) ; assertEquals ( 1 , listAppender . list . size ( ) ) ; ILoggingEvent event = ( ILoggingEvent ) listAppender . list . get ( 0 ) ; assertEquals ( HELLO , event . getMessage ( ) ) ; assertEquals ( 1 , listAppender . stringList . size ( ) ) ; assertEquals ( "TRACE [" + Log4jInvocation . class . getName ( ) + "] basic-test - Hello" , listAppender . stringList . get ( 0 ) ) ; }
@ Test public void callerData ( ) { assertEquals ( 0 , listAppender . list . size ( ) ) ; PatternLayout pl = new PatternLayout ( ) ; pl . setPattern ( "%-5level [%class] %logger - %msg" ) ; pl . setContext ( lc ) ; pl . start ( ) ; listAppender . layout = pl ; Logger logger = Logger . getLogger ( "basic-test" ) ; logger . trace ( "none" ) ; assertEquals ( 0 , listAppender . list . size ( ) ) ; rootLogger . setLevel ( Level . TRACE ) ; logger . trace ( HELLO ) ; assertEquals ( 1 , listAppender . list . size ( ) ) ; ILoggingEvent event = ( ILoggingEvent ) listAppender . list . get ( 0 ) ; assertEquals ( HELLO , event . getMessage ( ) ) ; assertEquals ( 1 , listAppender . stringList . size ( ) ) ; assertEquals ( "TRACE [" + Log4jInvocation . class . getName ( ) + "] basic-test - Hello" , listAppender . stringList . get ( 0 ) ) ; }
public void test() { if ( _log . isWarnEnabled ( ) ) { StringBundler sb = new StringBundler ( 4 ) ; sb . append ( "Unable to export small image " ) ; sb . append ( entry . getSmallImageId ( ) ) ; sb . append ( " to blogs entry " ) ; sb . append ( entry . getEntryId ( ) ) ; _log . warn ( sb . toString ( ) ) ; } }
@ Override protected void supplyActiveThreads ( ) { log . info ( "Start supplying threads" ) ; code_block = IfStatement ; startTime = System . currentTimeMillis ( ) ; boolean isDebugEnabled = log . isDebugEnabled ( ) ; code_block = WhileStatement ; log . info ( "Done supplying threads" ) ; }
public void test() { if ( isDebugEnabled ) { log . debug ( "Concurrency factual/expected: " + concurrTG . getConcurrency ( ) + "/" + getPlannedConcurrency ( isDebugEnabled ) ) ; } }
@ Override protected void supplyActiveThreads ( ) { log . info ( "Start supplying threads" ) ; code_block = IfStatement ; startTime = System . currentTimeMillis ( ) ; boolean isDebugEnabled = log . isDebugEnabled ( ) ; code_block = WhileStatement ; log . info ( "Done supplying threads" ) ; }
public void test() { try { return ( ContentRecordVO ) this . getContentDAO ( ) . loadEntityRecord ( id ) ; } catch ( Throwable t ) { logger . error ( "Error while loading content vo : id {}" , id , t ) ; throw new ApsSystemException ( "Error while loading content vo : id " + id , t ) ; } }
public void test() { try { URL dataSourcesUrl = thirdeyeConfig . getDataSourcesAsUrl ( ) ; DataSources dataSources = DataSourcesLoader . fromDataSourcesUrl ( dataSourcesUrl ) ; code_block = IfStatement ; Map < String , ThirdEyeDataSource > thirdEyeDataSourcesMap = DataSourcesLoader . getDataSourceMap ( dataSources ) ; QueryCache queryCache = new QueryCache ( thirdEyeDataSourcesMap , Executors . newCachedThreadPool ( ) ) ; ThirdEyeCacheRegistry . getInstance ( ) . registerQueryCache ( queryCache ) ; } catch ( Exception e ) { LOGGER . info ( "Caught exception while initializing caches" , e ) ; } }
public void test() { if ( heartbeatTime < oldTime ) { log . warn ( "[ClusterStatusHolder-{}] receive the expired heartbeat from {}, serverTime: {}, heartTime: {}" , appName , heartbeat . getWorkerAddress ( ) , System . currentTimeMillis ( ) , heartbeat . getHeartbeatTime ( ) ) ; return ; } }
public void test() { if ( ! resultOptional . isPresent ( ) ) { logger . warn ( "Dataflow timed out after waiting {} milliseconds. Will cancel the execution." , timeoutMillis ) ; trigger . cancel ( ) ; return null ; } }
public void test() { if ( ! triggerResult . isSuccessful ( ) ) { logger . error ( "Dataflow {} failed to execute properly" , dataflowName , triggerResult . getFailureCause ( ) . orElse ( null ) ) ; trigger . cancel ( ) ; failureYieldExpiration = System . currentTimeMillis ( ) + 1000L ; return null ; } }
@ Override public List < SourceRecord > poll ( ) throws InterruptedException { final long yieldExpiration = Math . max ( failureYieldExpiration , dataflow . getSourceYieldExpiration ( ) ) ; final long now = System . currentTimeMillis ( ) ; final long yieldMillis = yieldExpiration - now ; code_block = IfStatement ; code_block = IfStatement ; logger . debug ( "Triggering dataflow" ) ; final long start = System . nanoTime ( ) ; final DataflowTrigger trigger = dataflow . trigger ( ) ; final Optional < TriggerResult > resultOptional = trigger . getResult ( timeoutMillis , TimeUnit . MILLISECONDS ) ; code_block = IfStatement ; triggerResult = resultOptional . get ( ) ; code_block = IfStatement ; verifyFlowFilesTransferredToProperPort ( triggerResult , outputPortName , trigger ) ; final long nanos = System . nanoTime ( ) - start ; final List < FlowFile > outputFlowFiles = triggerResult . getOutputFlowFiles ( outputPortName ) ; final List < SourceRecord > sourceRecords = new ArrayList < > ( outputFlowFiles . size ( ) ) ; Map < String , ? > componentState = dataflow . getComponentStates ( Scope . CLUSTER ) ; final Map < String , ? > partitionMap ; code_block = IfStatement ; code_block = ForStatement ; logger . debug ( "Returning {} records from poll() method (took {} nanos to run dataflow)" , sourceRecords . size ( ) , nanos ) ; code_block = IfStatement ; return sourceRecords ; }
public void test() { if ( opentsdbEvent != null ) { sender . enqueue ( opentsdbEvent ) ; } else { log . debug ( "Metric=[%s] has not been configured to be emitted to opentsdb" , ( ( ServiceMetricEvent ) event ) . getMetric ( ) ) ; } }
public void test() { try { Runtime . getRuntime ( ) . removeShutdownHook ( shutdownHook ) ; } catch ( IllegalStateException e ) { logger . debug ( "Unable to remove shutdown hook for {}, shutdown already in progress" , serviceName , e ) ; } catch ( Throwable t ) { logger . warn ( "Exception while un-registering {}'s shutdown hook." , serviceName , t ) ; } }
public void test() { try { Runtime . getRuntime ( ) . removeShutdownHook ( shutdownHook ) ; } catch ( IllegalStateException e ) { logger . debug ( "Unable to remove shutdown hook for {}, shutdown already in progress" , serviceName , e ) ; } catch ( Throwable t ) { logger . warn ( "Exception while un-registering {}'s shutdown hook." , serviceName , t ) ; } }
public void test() { if ( OAuth2LoginAuthenticationToken . class . isAssignableFrom ( auth . getClass ( ) ) ) { OAuth2LoginAuthenticationToken authenticationToken = ( OAuth2LoginAuthenticationToken ) auth ; DhisOidcUser principal = ( DhisOidcUser ) authenticationToken . getPrincipal ( ) ; UserCredentials userCredentials = principal . getUserCredentials ( ) ; username = userCredentials . getUsername ( ) ; WebAuthenticationDetails tokenDetails = ( WebAuthenticationDetails ) authenticationToken . getDetails ( ) ; String remoteAddress = tokenDetails . getRemoteAddress ( ) ; log . debug ( String . format ( "OIDC login attempt succeeded for remote IP: %s" , remoteAddress ) ) ; } }
public void test() { if ( OAuth2LoginAuthenticationToken . class . isAssignableFrom ( auth . getClass ( ) ) ) { OAuth2LoginAuthenticationToken authenticationToken = ( OAuth2LoginAuthenticationToken ) auth ; DhisOidcUser principal = ( DhisOidcUser ) authenticationToken . getPrincipal ( ) ; UserCredentials userCredentials = principal . getUserCredentials ( ) ; username = userCredentials . getUsername ( ) ; WebAuthenticationDetails tokenDetails = ( WebAuthenticationDetails ) authenticationToken . getDetails ( ) ; String remoteAddress = tokenDetails . getRemoteAddress ( ) ; log . debug ( String . format ( "OIDC login attempt succeeded for remote IP: %s" , remoteAddress ) ) ; } }
public void test() { try { preWalk ( ) ; specWalker . walk ( profileInstance . getProfileSpec ( ) , walkState ) ; } catch ( InterruptedException e ) { log . debug ( e . getMessage ( ) , e ) ; } catch ( IOException e ) { inError = true ; log . error ( e . getMessage ( ) , e ) ; throw new ProfileException ( e ) ; } finally { postWalk ( ) ; counter . cancel ( ) ; countFuture . cancel ( false ) ; code_block = IfStatement ; submissionGateway . save ( ) ; profileWalkerDao . delete ( ) ; } }
public void test() { try { preWalk ( ) ; specWalker . walk ( profileInstance . getProfileSpec ( ) , walkState ) ; } catch ( InterruptedException e ) { log . debug ( e . getMessage ( ) , e ) ; } catch ( IOException e ) { inError = true ; log . error ( e . getMessage ( ) , e ) ; throw new ProfileException ( e ) ; } finally { postWalk ( ) ; counter . cancel ( ) ; countFuture . cancel ( false ) ; code_block = IfStatement ; submissionGateway . save ( ) ; profileWalkerDao . delete ( ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( noSuchFolderException , noSuchFolderException ) ; } }
@ Test public void testPrepare ( ) { TimeHelper . setProvider ( new FixedTimeProvider ( 12345l ) ) ; List < CipherSuite > suiteList = new LinkedList < > ( ) ; context . getConfig ( ) . setHighestProtocolVersion ( ProtocolVersion . TLS12 ) ; suiteList . add ( CipherSuite . TLS_DHE_DSS_WITH_AES_256_CBC_SHA256 ) ; suiteList . add ( CipherSuite . TLS_DHE_RSA_WITH_CHACHA20_POLY1305_SHA256 ) ; suiteList . add ( CipherSuite . TLS_DHE_PSK_WITH_AES_128_GCM_SHA256 ) ; context . setClientSupportedCiphersuites ( suiteList ) ; List < CipherSuite > ourSuiteList = new LinkedList < > ( ) ; ourSuiteList . add ( CipherSuite . TLS_DHE_RSA_WITH_CHACHA20_POLY1305_SHA256 ) ; List < CompressionMethod > ourCompressionList = new LinkedList < > ( ) ; ourCompressionList . add ( CompressionMethod . LZS ) ; context . getConfig ( ) . setDefaultClientSupportedCiphersuites ( ourSuiteList ) ; context . getConfig ( ) . setDefaultServerSupportedCompressionMethods ( ourCompressionList ) ; context . setHighestClientProtocolVersion ( ProtocolVersion . TLS11 ) ; List < CompressionMethod > compressionList = new LinkedList < > ( ) ; compressionList . add ( CompressionMethod . NULL ) ; compressionList . add ( CompressionMethod . LZS ) ; context . setClientSupportedCompressions ( compressionList ) ; context . getConfig ( ) . setDefaultServerSessionId ( new byte [ ] code_block = "" ; ) ; preparator . prepare ( ) ; assertArrayEquals ( ProtocolVersion . TLS11 . getValue ( ) , message . getProtocolVersion ( ) . getValue ( ) ) ; assertArrayEquals ( ArrayConverter . longToUint32Bytes ( 12345l ) , message . getUnixTime ( ) . getValue ( ) ) ; LOGGER . info ( ArrayConverter . bytesToHexString ( message . getRandom ( ) . getValue ( ) ) ) ; assertArrayEquals ( ArrayConverter . concatenate ( ArrayConverter . longToUint32Bytes ( 12345l ) , ArrayConverter . hexStringToByteArray ( "60B420BB3851D9D47ACB933DBE70399BF6C92DA33AF01D4FB770E98C" ) ) , message . getRandom ( ) . getValue ( ) ) ; assertArrayEquals ( ArrayConverter . hexStringToByteArray ( "000102030405" ) , message . getSessionId ( ) . getValue ( ) ) ; assertTrue ( 6 == message . getSessionIdLength ( ) . getValue ( ) ) ; assertTrue ( message . getExtensionBytes ( ) . getValue ( ) . length == 0 ) ; assertTrue ( 0 == message . getExtensionsLength ( ) . getValue ( ) ) ; }
public void test() { try { code_block = IfStatement ; } catch ( final InterruptedException e ) { CompletionAdaptor . logger . error ( e , e ) ; } }
private void updateRack ( ) { Rack rack = this . rackClient . getByName ( RACK_NAME ) . get ( 0 ) ; String resourceId = rack . getResourceId ( ) ; rack . setThermalLimit ( Integer . valueOf ( 1000 ) ) ; Rack updatedRack = this . rackClient . update ( resourceId , rack ) ; LOGGER . info ( "Rack object returned to client : " + updatedRack . toJsonString ( ) ) ; }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( "Found {} in cache for {}, row='{}', locateType={}, replicaId={}" , loc , tableName , Bytes . toStringBinary ( row ) , RegionLocateType . BEFORE , replicaId ) ; } }
public void test() { if ( entry == null ) { recordCacheMiss ( ) ; return null ; } }
public void test() { if ( loc == null ) { recordCacheMiss ( ) ; return null ; } }
public void test() { if ( jvmargs != null ) { LOGGER . info ( "Using JAVA_OPTS from conf file (jvm.args)" ) ; } else { jvmargs = env . get ( "JAVA_OPTS" ) ; code_block = IfStatement ; } }
public void test() { if ( jvmargs != null ) { LOGGER . info ( "Using JAVA_OPTS from environment" ) ; } else { LOGGER . info ( "Using default JAVA_OPTS" ) ; jvmargs = "" ; } }
public void test() { if ( jvmargs != null ) { LOGGER . info ( "Using JAVA_OPTS from environment" ) ; } else { LOGGER . info ( "Using default JAVA_OPTS" ) ; jvmargs = "" ; } }
public void test() { if ( ( ExecutionPathDebugLog . isDebugEnabled ) && ( log . isDebugEnabled ( ) ) ) { log . debug ( ".sendEventJson Processing event " + json ) ; } }
public void afterPropertiesSet ( ) throws Exception { JdbcTemplate jdbcTemplate = new JdbcTemplate ( dataSource ) ; jdbcTemplate . execute ( CREATE_TABLES ) ; String query = "insert into users values ('user', '" + new Sha256Hash ( "user" ) . toBase64 ( ) + "' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created user." ) ; query = "insert into users values ( 'admin', '" + new Sha256Hash ( "admin" ) . toBase64 ( ) + "' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created admin." ) ; query = "insert into roles values ( 'user' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created user" ) ; query = "insert into roles values ( 'admin' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created admin" ) ; query = "insert into roles_permissions values ( 'user', 'view')" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created permission view for role user" ) ; query = "insert into roles_permissions values ( 'admin', 'user:*')" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created permission user:* for role admin" ) ; query = "insert into user_roles values ( 'user', 'user' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Assigned user role user" ) ; query = "insert into user_roles values ( 'admin', 'admin' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Assigned admin role admin" ) ; }
public void afterPropertiesSet ( ) throws Exception { JdbcTemplate jdbcTemplate = new JdbcTemplate ( dataSource ) ; jdbcTemplate . execute ( CREATE_TABLES ) ; String query = "insert into users values ('user', '" + new Sha256Hash ( "user" ) . toBase64 ( ) + "' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created user." ) ; query = "insert into users values ( 'admin', '" + new Sha256Hash ( "admin" ) . toBase64 ( ) + "' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created admin." ) ; query = "insert into roles values ( 'user' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created user" ) ; query = "insert into roles values ( 'admin' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created admin" ) ; query = "insert into roles_permissions values ( 'user', 'view')" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created permission view for role user" ) ; query = "insert into roles_permissions values ( 'admin', 'user:*')" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created permission user:* for role admin" ) ; query = "insert into user_roles values ( 'user', 'user' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Assigned user role user" ) ; query = "insert into user_roles values ( 'admin', 'admin' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Assigned admin role admin" ) ; }
public void afterPropertiesSet ( ) throws Exception { JdbcTemplate jdbcTemplate = new JdbcTemplate ( dataSource ) ; jdbcTemplate . execute ( CREATE_TABLES ) ; String query = "insert into users values ('user', '" + new Sha256Hash ( "user" ) . toBase64 ( ) + "' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created user." ) ; query = "insert into users values ( 'admin', '" + new Sha256Hash ( "admin" ) . toBase64 ( ) + "' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created admin." ) ; query = "insert into roles values ( 'user' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created user" ) ; query = "insert into roles values ( 'admin' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created admin" ) ; query = "insert into roles_permissions values ( 'user', 'view')" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created permission view for role user" ) ; query = "insert into roles_permissions values ( 'admin', 'user:*')" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created permission user:* for role admin" ) ; query = "insert into user_roles values ( 'user', 'user' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Assigned user role user" ) ; query = "insert into user_roles values ( 'admin', 'admin' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Assigned admin role admin" ) ; }
public void afterPropertiesSet ( ) throws Exception { JdbcTemplate jdbcTemplate = new JdbcTemplate ( dataSource ) ; jdbcTemplate . execute ( CREATE_TABLES ) ; String query = "insert into users values ('user', '" + new Sha256Hash ( "user" ) . toBase64 ( ) + "' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created user." ) ; query = "insert into users values ( 'admin', '" + new Sha256Hash ( "admin" ) . toBase64 ( ) + "' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created admin." ) ; query = "insert into roles values ( 'user' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created user" ) ; query = "insert into roles values ( 'admin' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created admin" ) ; query = "insert into roles_permissions values ( 'user', 'view')" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created permission view for role user" ) ; query = "insert into roles_permissions values ( 'admin', 'user:*')" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created permission user:* for role admin" ) ; query = "insert into user_roles values ( 'user', 'user' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Assigned user role user" ) ; query = "insert into user_roles values ( 'admin', 'admin' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Assigned admin role admin" ) ; }
public void afterPropertiesSet ( ) throws Exception { JdbcTemplate jdbcTemplate = new JdbcTemplate ( dataSource ) ; jdbcTemplate . execute ( CREATE_TABLES ) ; String query = "insert into users values ('user', '" + new Sha256Hash ( "user" ) . toBase64 ( ) + "' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created user." ) ; query = "insert into users values ( 'admin', '" + new Sha256Hash ( "admin" ) . toBase64 ( ) + "' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created admin." ) ; query = "insert into roles values ( 'user' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created user" ) ; query = "insert into roles values ( 'admin' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created admin" ) ; query = "insert into roles_permissions values ( 'user', 'view')" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created permission view for role user" ) ; query = "insert into roles_permissions values ( 'admin', 'user:*')" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created permission user:* for role admin" ) ; query = "insert into user_roles values ( 'user', 'user' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Assigned user role user" ) ; query = "insert into user_roles values ( 'admin', 'admin' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Assigned admin role admin" ) ; }
public void afterPropertiesSet ( ) throws Exception { JdbcTemplate jdbcTemplate = new JdbcTemplate ( dataSource ) ; jdbcTemplate . execute ( CREATE_TABLES ) ; String query = "insert into users values ('user', '" + new Sha256Hash ( "user" ) . toBase64 ( ) + "' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created user." ) ; query = "insert into users values ( 'admin', '" + new Sha256Hash ( "admin" ) . toBase64 ( ) + "' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created admin." ) ; query = "insert into roles values ( 'user' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created user" ) ; query = "insert into roles values ( 'admin' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created admin" ) ; query = "insert into roles_permissions values ( 'user', 'view')" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created permission view for role user" ) ; query = "insert into roles_permissions values ( 'admin', 'user:*')" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created permission user:* for role admin" ) ; query = "insert into user_roles values ( 'user', 'user' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Assigned user role user" ) ; query = "insert into user_roles values ( 'admin', 'admin' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Assigned admin role admin" ) ; }
public void afterPropertiesSet ( ) throws Exception { JdbcTemplate jdbcTemplate = new JdbcTemplate ( dataSource ) ; jdbcTemplate . execute ( CREATE_TABLES ) ; String query = "insert into users values ('user', '" + new Sha256Hash ( "user" ) . toBase64 ( ) + "' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created user." ) ; query = "insert into users values ( 'admin', '" + new Sha256Hash ( "admin" ) . toBase64 ( ) + "' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created admin." ) ; query = "insert into roles values ( 'user' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created user" ) ; query = "insert into roles values ( 'admin' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created admin" ) ; query = "insert into roles_permissions values ( 'user', 'view')" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created permission view for role user" ) ; query = "insert into roles_permissions values ( 'admin', 'user:*')" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created permission user:* for role admin" ) ; query = "insert into user_roles values ( 'user', 'user' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Assigned user role user" ) ; query = "insert into user_roles values ( 'admin', 'admin' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Assigned admin role admin" ) ; }
public void afterPropertiesSet ( ) throws Exception { JdbcTemplate jdbcTemplate = new JdbcTemplate ( dataSource ) ; jdbcTemplate . execute ( CREATE_TABLES ) ; String query = "insert into users values ('user', '" + new Sha256Hash ( "user" ) . toBase64 ( ) + "' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created user." ) ; query = "insert into users values ( 'admin', '" + new Sha256Hash ( "admin" ) . toBase64 ( ) + "' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created admin." ) ; query = "insert into roles values ( 'user' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created user" ) ; query = "insert into roles values ( 'admin' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created admin" ) ; query = "insert into roles_permissions values ( 'user', 'view')" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created permission view for role user" ) ; query = "insert into roles_permissions values ( 'admin', 'user:*')" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Created permission user:* for role admin" ) ; query = "insert into user_roles values ( 'user', 'user' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Assigned user role user" ) ; query = "insert into user_roles values ( 'admin', 'admin' )" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( "Assigned admin role admin" ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { t0 = System . currentTimeMillis ( ) ; log . debug ( "storing blob " + digest + " to S3" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "blob " + digest + " is already in S3" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { long dtms = System . currentTimeMillis ( ) - t0 ; log . debug ( "stored blob " + digest + " to S3 in " + dtms + "ms" ) ; } }
@ Override public byte [ ] serializeHandshakeMessageContent ( ) { LOGGER . debug ( "Serializing RSAClientKeyExchangeMessage" ) ; code_block = IfStatement ; writeSerializedPublickey ( msg ) ; return getAlreadySerialized ( ) ; }
public void test() { try { log . error ( t , "Error while deleting key %s from MapDb" , mapDbKey ) ; } catch ( Exception e ) { t . addSuppressed ( e ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceTaxMethodServiceUtil . class , "getCommerceTaxMethods" , _getCommerceTaxMethodsParameterTypes6 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , groupId ) ; Object returnObj = null ; code_block = TryStatement ;  return ( java . util . List < com . liferay . commerce . tax . model . CommerceTaxMethod > ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
@ Test public void test_00 ( ) { Log . debug ( "Test" ) ; initRand ( ) ; for ( int len = 1 ; len < 1000 ; len ++ ) DnaSequenceBaseAt ( len ) ; }
public void terminated ( TaskAttemptID taskid ) { LOG . info ( "Task '" + taskid . getTaskID ( ) . toString ( ) + "' has failed." ) ; TaskStatus status = taskStatuses . get ( taskid ) ; status . setRunState ( TaskStatus . State . FAILED ) ; activeTasks . remove ( taskid ) ; }
public void test() { try { java . util . List < com . liferay . oauth2 . provider . model . OAuth2Authorization > returnValue = OAuth2AuthorizationServiceUtil . getApplicationOAuth2Authorizations ( oAuth2ApplicationId , start , end , orderByComparator ) ; return com . liferay . oauth2 . provider . model . OAuth2AuthorizationSoap . toSoapModels ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( ! txStillOpen && janusGraphManagerIsInBadState ) { log . error ( "JanusGraphManager should be instantiated on this server, but it is not. " + "Please restart with proper server settings. " + "As a result, we could not evict graph {} from the cache." , graphName ) ; break ; } else-if ( ! txStillOpen ) { DataOutput out = graph . getDataSerializer ( ) . getDataOutput ( 64 ) ; out . writeObjectNotNull ( MgmtLogType . CACHED_TYPE_EVICTION_ACK ) ; out . writeObjectNotNull ( originId ) ; VariableLong . writePositive ( out , evictionId ) ; code_block = IfStatement ; code_block = TryStatement ;  break ; } }
public void test() { try { sysLog . add ( out . getStaticBuffer ( ) ) ; log . debug ( "Sent {}: evictionID={} originID={}" , MgmtLogType . CACHED_TYPE_EVICTION_ACK , evictionId , originId ) ; } catch ( ResourceUnavailableException e ) { log . warn ( "System log has already shut down. Did not sent {}: evictionID={} originID={}" , MgmtLogType . CACHED_TYPE_EVICTION_ACK , evictionId , originId ) ; } }
public void test() { try { times . sleepPast ( times . getTime ( ) . plus ( SLEEP_INTERVAL ) ) ; } catch ( InterruptedException e ) { log . error ( "Interrupted eviction ack thread to " + getId ( ) , e ) ; break ; } }
public void test() { if ( validationResult . isPresent ( ) ) { LOG . debug ( validationResult . get ( ) ) ; return null ; } }
public void test() { try { isSystemUpdatingWebView = false ; return inflater . inflate ( R . layout . fragment_authenticated_webview , container , false ) ; } catch ( InflateException e ) { logger . error ( e , true ) ; isSystemUpdatingWebView = true ; return inflater . inflate ( R . layout . content_error , container , false ) ; } }
public void test() { try { zkClient . newNamespaceAwareEnsurePath ( path ) . ensure ( zkClient . getZookeeperClient ( ) ) ; zkClient . setData ( ) . forPath ( path , Bytes . toBytes ( layoutID ) ) ; LOG . debug ( "Updated layout ID for table {} to {}." , tableURI , layoutID ) ; } catch ( Exception e ) { wrapAndRethrow ( e ) ; } }
public void test() { try { logger . debug ( "Reliability loss with policy of reconnect and membership thread doing reconnect" ) ; initializationLatchAfterMemberTimeout . await ( ) ; getSystem ( ) . tryReconnect ( false , "Role Loss" , getCache ( ) ) ; synchronized ( missingRequiredRoles ) code_block = "" ; } catch ( Exception e ) { logger . fatal ( "Unexpected exception:" , e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( CancelException ignor ) { throw ignor ; } catch ( Exception e ) { logger . fatal ( "Unexpected exception:" , e ) ; } }
public void test() { if ( "true" . equals ( OscarProperties . getInstance ( ) . getProperty ( "integrator.send.documents.disabled" , "false" ) ) ) { logger . debug ( "Pushing documents is disabled: integrator.send.documents.disabled = false" ) ; return ; } }
public void test() { try { is = evidence . getBufferedStream ( ) ; extractFile ( is , evidence , null ) ; } catch ( IOException e ) { LOGGER . warn ( "{} Error exporting {} \t{}" , Thread . currentThread ( ) . getName ( ) , evidence . getPath ( ) , e . toString ( ) ) ; } finally { IOUtil . closeQuietly ( is ) ; } }
public void test() { if ( externalInitializer != null ) { InetSocketAddress externalLocalAddress = configureInitializer ( externalInitializer ) ; portRegister . register ( BoltConnector . NAME , externalLocalAddress ) ; var host = externalInitializer . address ( ) . getHostname ( ) ; var port = externalLocalAddress . getPort ( ) ; userLog . info ( "Bolt enabled on %s." , SocketAddress . format ( host , port ) ) ; } }
public void test() { if ( internalInitializer != null ) { var internalLocalAddress = configureInitializer ( internalInitializer ) ; portRegister . register ( BoltConnector . INTERNAL_NAME , internalLocalAddress ) ; var host = internalInitializer . address ( ) . getHostname ( ) ; var port = internalLocalAddress . getPort ( ) ; userLog . info ( "Bolt (Routing) enabled on %s." , SocketAddress . format ( host , port ) ) ; } }
public void test() { try { return authorizationIntraCloudRepository . findAll ( PageRequest . of ( validatedPage , validatedSize , validatedDirection , validatedSortField ) ) ; } catch ( final Exception ex ) { logger . debug ( ex . getMessage ( ) , ex ) ; throw new ArrowheadException ( CoreCommonConstants . DATABASE_OPERATION_EXCEPTION_MSG ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( String . format ( "execUpdate: %s" , query ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( String . format ( "%s" , LogUtil . getStackTrace ( e ) ) ) ; } }
public void test() { try { code_block = IfStatement ; code_block = IfStatement ; } catch ( final ProviderException e ) { LOG . error ( "ProviderException requesting version history to " + pageName , e ) ; } }
public void test() { try { onFinish ( ) ; } catch ( final Exception e ) { logger . error ( "Failed to perform tasks when enumerator was finished" , e ) ; } }
public void test() { if ( child instanceof UIPortlet < ? , ? > ) { return targetContainer . hasMoveAppsPermission ( ) ; } else-if ( child instanceof org . exoplatform . portal . webui . container . UIContainer ) { return targetContainer . hasMoveContainersPermission ( ) ; } else-if ( child instanceof org . exoplatform . portal . webui . page . UIPageBody ) { return true ; } else { log . warn ( "Unexpected uiSource type '" + child . getClass ( ) . getName ( ) + "'." ) ; return false ; } }
public void test() { -> { LOG . warn ( "Failover triggered by {}" , user . getId ( ) ) ; abort . abort ( AbortReason . MANUAL , Optional . of ( new RuntimeException ( String . format ( "Forced failover by %s" , user . getId ( ) ) ) ) ) ; } }
@ Test public void createDatasetVersionTest ( ) { CreateDatasetVersion createDatasetVersionRequest = getDatasetVersionRequest ( dataset . getId ( ) ) ; CreateDatasetVersion . Response createDatasetVersionResponse = datasetVersionServiceStub . createDatasetVersion ( createDatasetVersionRequest ) ; DatasetVersion datasetVersion2 = createDatasetVersionResponse . getDatasetVersion ( ) ; LOGGER . info ( "CreateDatasetVersion Response : \n" + datasetVersion2 ) ; assertEquals ( "DatasetVersion datsetId not match with expected DatasetVersion datsetId" , dataset . getId ( ) , datasetVersion2 . getDatasetId ( ) ) ; DeleteDatasetVersion deleteDatasetVersionRequest = DeleteDatasetVersion . newBuilder ( ) . setId ( datasetVersion2 . getId ( ) ) . build ( ) ; DeleteDatasetVersion . Response deleteDatasetVersionResponse = datasetVersionServiceStub . deleteDatasetVersion ( deleteDatasetVersionRequest ) ; LOGGER . info ( "DeleteDatasetVersion deleted successfully" ) ; LOGGER . info ( deleteDatasetVersionResponse . toString ( ) ) ; createDatasetVersionRequest = getDatasetVersionRequest ( dataset . getId ( ) ) ; createDatasetVersionRequest = createDatasetVersionRequest . toBuilder ( ) . setDatasetBlob ( CommitTest . getDatasetBlobFromPath ( "datasetVersion" ) . getDataset ( ) ) . build ( ) ; createDatasetVersionResponse = datasetVersionServiceStub . createDatasetVersion ( createDatasetVersionRequest ) ; DatasetVersion datasetVersion = createDatasetVersionResponse . getDatasetVersion ( ) ; LOGGER . info ( "CreateDatasetVersion Response : \n" + datasetVersion ) ; assertEquals ( "DatasetVersion datsetId not match with expected DatasetVersion datsetId" , createDatasetVersionRequest . getDatasetBlob ( ) , datasetVersion . getDatasetBlob ( ) ) ; deleteDatasetVersionRequest = DeleteDatasetVersion . newBuilder ( ) . setId ( datasetVersion . getId ( ) ) . build ( ) ; deleteDatasetVersionResponse = datasetVersionServiceStub . deleteDatasetVersion ( deleteDatasetVersionRequest ) ; LOGGER . info ( "DeleteDatasetVersion deleted successfully" ) ; LOGGER . info ( deleteDatasetVersionResponse . toString ( ) ) ; }
@ Test public void createDatasetVersionTest ( ) { CreateDatasetVersion createDatasetVersionRequest = getDatasetVersionRequest ( dataset . getId ( ) ) ; CreateDatasetVersion . Response createDatasetVersionResponse = datasetVersionServiceStub . createDatasetVersion ( createDatasetVersionRequest ) ; DatasetVersion datasetVersion2 = createDatasetVersionResponse . getDatasetVersion ( ) ; LOGGER . info ( "CreateDatasetVersion Response : \n" + datasetVersion2 ) ; assertEquals ( "DatasetVersion datsetId not match with expected DatasetVersion datsetId" , dataset . getId ( ) , datasetVersion2 . getDatasetId ( ) ) ; DeleteDatasetVersion deleteDatasetVersionRequest = DeleteDatasetVersion . newBuilder ( ) . setId ( datasetVersion2 . getId ( ) ) . build ( ) ; DeleteDatasetVersion . Response deleteDatasetVersionResponse = datasetVersionServiceStub . deleteDatasetVersion ( deleteDatasetVersionRequest ) ; LOGGER . info ( "DeleteDatasetVersion deleted successfully" ) ; LOGGER . info ( deleteDatasetVersionResponse . toString ( ) ) ; createDatasetVersionRequest = getDatasetVersionRequest ( dataset . getId ( ) ) ; createDatasetVersionRequest = createDatasetVersionRequest . toBuilder ( ) . setDatasetBlob ( CommitTest . getDatasetBlobFromPath ( "datasetVersion" ) . getDataset ( ) ) . build ( ) ; createDatasetVersionResponse = datasetVersionServiceStub . createDatasetVersion ( createDatasetVersionRequest ) ; DatasetVersion datasetVersion = createDatasetVersionResponse . getDatasetVersion ( ) ; LOGGER . info ( "CreateDatasetVersion Response : \n" + datasetVersion ) ; assertEquals ( "DatasetVersion datsetId not match with expected DatasetVersion datsetId" , createDatasetVersionRequest . getDatasetBlob ( ) , datasetVersion . getDatasetBlob ( ) ) ; deleteDatasetVersionRequest = DeleteDatasetVersion . newBuilder ( ) . setId ( datasetVersion . getId ( ) ) . build ( ) ; deleteDatasetVersionResponse = datasetVersionServiceStub . deleteDatasetVersion ( deleteDatasetVersionRequest ) ; LOGGER . info ( "DeleteDatasetVersion deleted successfully" ) ; LOGGER . info ( deleteDatasetVersionResponse . toString ( ) ) ; }
public void test() { try { ManagementFactory . getPlatformMBeanServer ( ) . unregisterMBean ( MoodleUserProviderFactory . getObjectName ( pid ) ) ; } catch ( Exception e ) { logger . warn ( "Unable to unregister mbean for pid='{}': {}" , pid , e . getMessage ( ) ) ; } }
public void test() { try { sleeper . sleepFor ( retryIntervalMs , TimeUnit . MILLISECONDS ) ; } catch ( InterruptedException e ) { Thread . currentThread ( ) . interrupt ( ) ; log . warn ( "Error occurred while sleeping" , e ) ; return false ; } }
public void test() { if ( sourceFileName instanceof Text ) { return sourceFileName . toString ( ) ; } else { LOGGER . warn ( MessageFormat . format ( this . getActionExecution ( ) . getAction ( ) . getType ( ) + " does not accept {0} as type for sourceFileName" , sourceFileName . getClass ( ) ) ) ; return sourceFileName . toString ( ) ; } }
public void test() { try { resolved = identityResolver . resolveSubject ( subject , IDENTITY_TYPES , credentialName ) ; } catch ( Exception e ) { log . info ( "The user for OTP authN can not be found: " + subject , e ) ; return new AuthenticationResult ( Status . deny , null ) ; } }
public void test() { if ( ! valid ) { log . info ( "Code provided by {} is invalid" , subject ) ; return new AuthenticationResult ( Status . deny , null ) ; } }
public void test() { try { String dbCredential = resolved . getCredentialValue ( ) ; OTPCredentialDBState credState = JsonUtil . parse ( dbCredential , OTPCredentialDBState . class ) ; boolean valid = TOTPCodeVerificator . verifyCode ( code , credState . secret , credState . otpParams , credentialConfig . allowedTimeDriftSteps ) ; code_block = IfStatement ; AuthenticatedEntity ae = new AuthenticatedEntity ( resolved . getEntityId ( ) , subject , credState . outdated ? resolved . getCredentialName ( ) : null ) ; return new AuthenticationResult ( Status . success , ae ) ; } catch ( Exception e ) { log . warn ( "Error during TOTP verification to " + subject , e ) ; return new AuthenticationResult ( Status . deny , null ) ; } }
public void test() { try { ldapService . stop ( ) ; service . shutdown ( ) ; code_block = IfStatement ; } catch ( Exception e ) { LOG . error ( "exception while shutting down ldap" , e ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Created a folder [" + targetPath . toString ( ) + "]" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Folder [" + targetPath . toString ( ) + "] already exists" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Will remove the folder [" + targetPath . toString ( ) + "]" ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Recreated folder [" + targetPath . toString ( ) + "]" ) ; } }
public void test() { try { String unitId = O2SDKManager . Companion . instance ( ) . prefs ( ) . getString ( O2 . INSTANCE . getPRE_BIND_UNIT_ID_KEY ( ) , "" ) ; XLog . debug ( vo . toString ( ) ) ; BBSCollectionRealmObject object = new BBSCollectionRealmObject ( ) ; object . setId ( vo . getId ( ) ) ; object . setSectionName ( vo . getSectionName ( ) ) ; object . setSectionIcon ( vo . getSectionIcon ( ) ) ; object . setCreateTime ( vo . getCreateTime ( ) ) ; object . setUnitId ( unitId ) ; realm . copyToRealmOrUpdate ( object ) ; return true ; } catch ( Exception e ) { XLog . error ( "æ¶èçå" , e ) ; return false ; } }
public void test() { try { response = getResponse ( urlString , timeoutSeconds ) ; HttpEntity entity = getEntity ( response ) ; InputStream content = entity . getContent ( ) ; code_block = TryStatement ;  } catch ( Exception e ) { _log . error ( "Error handling url: " + urlString , e ) ; throw e ; } finally { code_block = IfStatement ; } }
public static List < String > getVMCommand ( Configuration conf , ApplicationId appid , PSAttemptId psAttemptId ) { Vector < String > vargs = new Vector < String > ( 8 ) ; vargs . add ( Environment . JAVA_HOME . $ ( ) + "/bin/java" ) ; String javaOpts = getChildJavaOpts ( conf , appid , psAttemptId ) ; String [ ] javaOptsSplit = javaOpts . split ( " " ) ; code_block = ForStatement ; Path childTmpDir = new Path ( Environment . PWD . $ ( ) , YarnConfiguration . DEFAULT_CONTAINER_TEMP_DIR ) ; vargs . add ( "-Djava.io.tmpdir=" + childTmpDir ) ; long logSize = 0 ; setupLog4jProperties ( conf , vargs , logSize ) ; String psClassName = conf . get ( AngelConf . ANGEL_PS_CLASS , AngelConf . DEFAULT_ANGEL_PS_CLASS ) ; vargs . add ( psClassName ) ; vargs . add ( "1>" + getTaskLogFile ( TaskLog . LogName . STDOUT ) ) ; vargs . add ( "2>" + getTaskLogFile ( TaskLog . LogName . STDERR ) ) ; StringBuilder mergedCommand = new StringBuilder ( ) ; code_block = ForStatement ; Vector < String > vargsFinal = new Vector < String > ( 1 ) ; vargsFinal . add ( mergedCommand . toString ( ) ) ; LOG . info ( "Command to launch container for PS is : " + mergedCommand ) ; return vargsFinal ; }
public void test() { { LOG . error ( "submitAndSchedule is not supported on Server.Please run your operation on Prism " ) ; throw FalconWebException . newAPIException ( "submitAndSchedule is not supported on Server. Please run your " + "operation on Prism." ) ; } }
public void persist ( StgG20Scl transientInstance ) { log . debug ( "persisting StgG20Scl instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . persist ( transientInstance ) ; log . debug ( "persist successful" ) ; } catch ( RuntimeException re ) { log . error ( "persist failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . persist ( transientInstance ) ; log . debug ( "persist successful" ) ; } catch ( RuntimeException re ) { log . error ( "persist failed" , re ) ; throw re ; } }
public void test() { try { Mockito . when ( callback . awaitCompletion ( ) ) . thenReturn ( callback ) ; } catch ( InterruptedException e ) { LOG . warn ( "Interrupted while setting up mocks" , e ) ; } }
public void test() { try { CnATreeElement element = getElement ( uuid ) ; code_block = IfStatement ; } catch ( Exception t ) { LOG . error ( "Error while removing object" , t ) ; } }
public void test() { try { gitPath = Paths . get ( FILE_SEPARATOR ) . relativize ( gitPath ) ; } catch ( IllegalArgumentException e ) { logger . debug ( "Path: " + path + " is already relative path." ) ; } }
public void test() { try { Response response = service . generate ( size , algorithm ) ; jwksTO = response . readEntity ( OIDCJWKSTO . class ) ; } catch ( Exception ge ) { LOG . error ( "While generating new OIDC JWKS" , ge ) ; } }
public void test() { if ( e . getType ( ) == ClientExceptionType . NotFound ) { code_block = TryStatement ;  } else { LOG . error ( "While reading OIDC JWKS" , e ) ; } }
@ After public void tearDown ( ) { LOG . info ( ) . $ ( "Tearing down test " ) . $ ( getClass ( ) . getSimpleName ( ) ) . $ ( '' ) . $ ( testName . getMethodName ( ) ) . $ ( ) ; engine . freeTableId ( ) ; engine . clear ( ) ; TestUtils . removeTestPath ( root ) ; configOverrideMaxUncommittedRows = - 1 ; configOverrideCommitLag = - 1 ; currentMicros = - 1 ; }
public void test() { if ( logger . isTraceEnabled ( LogMarker . BRIDGE_SERVER_VERBOSE ) ) { logger . trace ( LogMarker . BRIDGE_SERVER_VERBOSE , "{}: Adding message to queue: {}" , this , object ) ; } }
public void test() { if ( logger . isTraceEnabled ( LogMarker . BRIDGE_SERVER_VERBOSE ) ) { logger . trace ( LogMarker . BRIDGE_SERVER_VERBOSE , "{}: Adding message to queue: {}" , this , object ) ; } }
public void test() { if ( logger . isTraceEnabled ( LogMarker . BRIDGE_SERVER_VERBOSE ) ) { logger . trace ( LogMarker . BRIDGE_SERVER_VERBOSE , "{}: Adding message to queue: {}" , this , object ) ; } }
@ Override public void initialize ( ) { LOG . info ( "Trying to start the FHIR container" ) ; container . start ( ) ; registerProperties ( ) ; LOG . info ( "FHIR instance running at {}" , getServiceBaseURL ( ) ) ; }
@ Override public void initialize ( ) { LOG . info ( "Trying to start the FHIR container" ) ; container . start ( ) ; registerProperties ( ) ; LOG . info ( "FHIR instance running at {}" , getServiceBaseURL ( ) ) ; }
public void test() { try { String recipientUser = recipientMailAddress . getLocalPart ( ) . toLowerCase ( Locale . US ) ; String recipientHost = recipientMailAddress . getDomain ( ) . asString ( ) ; code_block = IfStatement ; List < String > nets = new ArrayList < > ( ) ; code_block = TryStatement ;  NetMatcher matcher = new NetMatcher ( nets , dns ) ; boolean matched = matcher . matchInetNetwork ( mail . getRemoteAddr ( ) ) ; code_block = IfStatement ; return matched ; } catch ( SQLException sqle ) { LOGGER . error ( "Error accessing database" , sqle ) ; throw new MessagingException ( "Exception thrown" , sqle ) ; } finally { theJDBCUtil . closeJDBCConnection ( conn ) ; } }
public void test() { try { final HttpResponse response = post ( String . format ( "/jobs/%s?action=start" , jobId ) , null ) ; return checkTaskStatus ( response ) ; } catch ( final IOException e ) { LOG . error ( "Failed to list Veeam jobs due to:" , e ) ; checkResponseTimeOut ( e ) ; } }
public void test() { try { List < String > rsList = client . getChildren ( ) . forPath ( replicaSetRoot ) ; List < Integer > rsIDList = Lists . transform ( rsList , new Function < String , Integer > ( ) code_block = "" ; ) ; int currMaxID = - 1 ; code_block = IfStatement ; int newReplicaSetID = currMaxID + 1 ; logger . trace ( "Id of new replica set {} is {}." , rs , newReplicaSetID ) ; rs . setReplicaSetID ( newReplicaSetID ) ; String replicaSetPath = ZKPaths . makePath ( replicaSetRoot , String . valueOf ( newReplicaSetID ) ) ; client . create ( ) . creatingParentsIfNeeded ( ) . forPath ( replicaSetPath , serializeReplicaSet ( rs ) ) ; writeSuccess . getAndIncrement ( ) ; return newReplicaSetID ; } catch ( Exception e ) { writeFail . getAndIncrement ( ) ; logger . error ( "Error when create replicaSet " + rs , e ) ; throw new StoreException ( e ) ; } }
public void test() { try { List < String > rsList = client . getChildren ( ) . forPath ( replicaSetRoot ) ; List < Integer > rsIDList = Lists . transform ( rsList , new Function < String , Integer > ( ) code_block = "" ; ) ; int currMaxID = - 1 ; code_block = IfStatement ; int newReplicaSetID = currMaxID + 1 ; logger . trace ( "Id of new replica set {} is {}." , rs , newReplicaSetID ) ; rs . setReplicaSetID ( newReplicaSetID ) ; String replicaSetPath = ZKPaths . makePath ( replicaSetRoot , String . valueOf ( newReplicaSetID ) ) ; client . create ( ) . creatingParentsIfNeeded ( ) . forPath ( replicaSetPath , serializeReplicaSet ( rs ) ) ; writeSuccess . getAndIncrement ( ) ; return newReplicaSetID ; } catch ( Exception e ) { writeFail . getAndIncrement ( ) ; logger . error ( "Error when create replicaSet " + rs , e ) ; throw new StoreException ( e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "New application orgName {} orgAppName {} id {} " , orgName , name , applicationId . toString ( ) ) ; } }
@ Override public void handleError ( @ Nullable Throwable cause ) { LOG . warn ( "Exception raised when handling request to {}" , url , cause ) ; Closeables . closeQuietly ( responseInfo ) ; }
@ Override public void stop ( Platform platform ) throws Exception { code_block = IfStatement ; log . info ( "{}: Deactivating ConsumeBenchWorker." , id ) ; doneFuture . complete ( "" ) ; executor . shutdownNow ( ) ; executor . awaitTermination ( 1 , TimeUnit . DAYS ) ; consumer . close ( ) ; this . consumer = null ; this . executor = null ; this . statusUpdater = null ; this . statusUpdaterFuture = null ; this . workerStatus = null ; this . doneFuture = null ; }
public void test() { try { opeList . addAll ( opeManager . getOperationAdm ( ) ) ; } catch ( OHServiceException ex ) { LOGGER . error ( ex . getMessage ( ) , ex ) ; } }
@ Override public void start ( ) { int numCores = Runtime . getRuntime ( ) . availableProcessors ( ) ; int numWorkerThreads = Integer . getInteger ( ZOOKEEPER_COMMIT_PROC_NUM_WORKER_THREADS , numCores ) ; workerShutdownTimeoutMS = Long . getLong ( ZOOKEEPER_COMMIT_PROC_SHUTDOWN_TIMEOUT , 5000 ) ; initBatchSizes ( ) ; LOG . info ( "Configuring CommitProcessor with {} worker threads." , numWorkerThreads > 0 ? numWorkerThreads : "no" ) ; code_block = IfStatement ; stopped = false ; stoppedMainLoop = false ; super . start ( ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Executing SQL batch update of {} statements" , sql . length ) ; } }
public void test() { try { @ SuppressWarnings ( "rawtypes" ) final GenericDao dao = GenericDaoBase . getDao ( job . getClass ( ) ) ; code_block = IfStatement ; publishOnEventBus ( job , "submit" ) ; code_block = IfStatement ; code_block = TryStatement ;  } catch ( Exception e ) { String errMsg = "Unable to schedule async job for command " + job . getCmd ( ) + ", unexpected exception." ; s_logger . warn ( errMsg , e ) ; throw new CloudRuntimeException ( errMsg ) ; } }
public void test() { for ( MetricSlice slice : slices ) { LOG . info ( "{} - {}" , formatter . print ( slice . getStart ( ) ) , formatter . print ( slice . getEnd ( ) ) ) ; } }
public void test() { try { log . trace ( "Executing python 'getLogouExternalUrl' authenticator method" ) ; PersonAuthenticationType externalAuthenticator = ( PersonAuthenticationType ) customScriptConfiguration . getExternalType ( ) ; Map < String , SimpleCustomProperty > configurationAttributes = customScriptConfiguration . getConfigurationAttributes ( ) ; return externalAuthenticator . getLogoutExternalUrl ( configurationAttributes , requestParameters ) ; } catch ( Exception ex ) { log . error ( ex . getMessage ( ) , ex ) ; saveScriptError ( customScriptConfiguration . getCustomScript ( ) , ex ) ; } }
public void test() { try { log . trace ( "Executing python 'getLogouExternalUrl' authenticator method" ) ; PersonAuthenticationType externalAuthenticator = ( PersonAuthenticationType ) customScriptConfiguration . getExternalType ( ) ; Map < String , SimpleCustomProperty > configurationAttributes = customScriptConfiguration . getConfigurationAttributes ( ) ; return externalAuthenticator . getLogoutExternalUrl ( configurationAttributes , requestParameters ) ; } catch ( Exception ex ) { log . error ( ex . getMessage ( ) , ex ) ; saveScriptError ( customScriptConfiguration . getCustomScript ( ) , ex ) ; } }
public void test() { try { userGroups . add ( ( UserGroup ) READER . read ( uri , user , new UserGroup ( ) ) ) ; } catch ( ImejiException e ) { LOGGER . info ( "User group with uri " + uri + " not found." ) ; } }
public void test() { try { sourceInfo . getCurAsyncClient ( RaftServer . getReadOperationTimeoutMS ( ) ) . fetchSingleSeries ( sourceInfo . getHeader ( ) , sourceInfo . getReaderId ( ) , handler ) ; fetchResult . wait ( RaftServer . getReadOperationTimeoutMS ( ) ) ; } catch ( TException e ) { code_block = IfStatement ; return fetchResultAsync ( ) ; } catch ( InterruptedException e ) { Thread . currentThread ( ) . interrupt ( ) ; logger . warn ( "Query {} interrupted" , sourceInfo ) ; return null ; } }
@ Test public void testGenerateGroupIdWithEntity ( ) { app = ApplicationBuilder . newManagedApp ( EntitySpec . create ( TestApplication . class ) . displayName ( "TistApp" ) , LocalManagementContextForTests . newInstance ( ) ) ; TestEntity child = app . createAndManageChild ( EntitySpec . create ( TestEntity . class ) . displayName ( "TestEnt" ) ) ; ConfigBag cfg = new ConfigBag ( ) . configure ( CloudLocationConfig . CALLER_CONTEXT , child ) ; String result = new BasicCloudMachineNamer ( ) . generateNewGroupId ( cfg ) ; log . info ( "test entity child group id gives: " + result ) ; Assert . assertTrue ( result . length ( ) <= 60 ) ; String user = Strings . maxlen ( System . getProperty ( "user.name" ) , 4 ) . toLowerCase ( ) ; Assert . assertTrue ( result . indexOf ( user ) >= 0 ) ; Assert . assertTrue ( result . indexOf ( "-tistapp-" ) >= 0 ) ; Assert . assertTrue ( result . indexOf ( "-testent-" ) >= 0 ) ; Assert . assertTrue ( result . indexOf ( "-" + Strings . maxlen ( app . getId ( ) , 4 ) . toLowerCase ( ) ) >= 0 ) ; Assert . assertTrue ( result . indexOf ( "-" + Strings . maxlen ( child . getId ( ) , 4 ) . toLowerCase ( ) ) >= 0 ) ; }
public void test() { if ( _logger . isWarnEnabled ( ) ) { _logger . warn ( getLogPrefix ( ) + backlogDroppedMsg ) ; } }
public boolean dropRole ( String roleName ) throws SentryClientException { String dropRoleStmt = "drop role " + roleName ; this . sentryJdbcTemplate . execute ( dropRoleStmt ) ; log . info ( "Role  {} dropped successfully " , roleName ) ; return true ; }
public void test() { try { code_block = ForStatement ; code_block = ForStatement ; code_block = ForStatement ; code_block = ForStatement ; code_block = ForStatement ; } catch ( RuntimeException ex ) { LOGGER . error ( "An exception occurred publishing metrics to Parfait" , ex ) ; } }
@ Scheduled ( fixedDelayString = "${statistics.key.interval.delay:3600000}" ) private void setLatestStatisticsTimeline ( ) { LOG . info ( "Getting the latest statistics timeline map" ) ; Map < StatisticsEnum , StatisticsTimeline > latestStatisticsTimelineMap = new HashMap < StatisticsEnum , StatisticsTimeline > ( ) ; code_block = ForStatement ; code_block = IfStatement ; LOG . info ( "Latest statistics timeline map is set" ) ; }
@ Override public String getLogoutRedirectionUrl ( WebContext context ) { init ( ) ; final String state = RandomStringUtils . randomAlphanumeric ( 10 ) ; final String postLogoutRedirectUri = this . appConfiguration . getOpenIdPostLogoutRedirectUri ( ) ; String idToken = ( String ) context . getSessionAttribute ( getName ( ) + SESSION_ID_TOKEN_PARAMETER ) ; code_block = IfStatement ; final EndSessionRequest endSessionRequest = new EndSessionRequest ( idToken , postLogoutRedirectUri , state ) ; final String redirectionUrl = this . openIdConfiguration . getEndSessionEndpoint ( ) + "?" + endSessionRequest . getQueryString ( ) ; logger . debug ( "oxAuth redirection Url: '{}'" , redirectionUrl ) ; return redirectionUrl ; }
public void test() { try { PersistentBundle persistentBundle = ModuleUtils . persist ( bundle ) ; String newLocation = persistentBundle . getLocation ( ) ; logger . info ( "Transformed bundle {} with location {} to be handled by the DX protocol handler under new location {}" , getDisplayName ( bundle ) , bundle . getLocation ( ) , newLocation ) ; return newLocation ; } catch ( ModuleManagementException e ) { throw e ; } catch ( Exception e ) { String msg = "Unable to transform bundle " + bundle + ". Cause: " + e . getMessage ( ) ; logger . error ( msg , e ) ; throw new ModuleManagementException ( msg , e ) ; } }
public void test() { try { PersistentBundle persistentBundle = ModuleUtils . persist ( bundle ) ; String newLocation = persistentBundle . getLocation ( ) ; logger . info ( "Transformed bundle {} with location {} to be handled by the DX protocol handler under new location {}" , getDisplayName ( bundle ) , bundle . getLocation ( ) , newLocation ) ; return newLocation ; } catch ( ModuleManagementException e ) { throw e ; } catch ( Exception e ) { String msg = "Unable to transform bundle " + bundle + ". Cause: " + e . getMessage ( ) ; logger . error ( msg , e ) ; throw new ModuleManagementException ( msg , e ) ; } }
public void test() { if ( getLogger ( ) . isDebugEnabled ( ) ) { getLogger ( ) . debug ( "SoapServerImpl.configure() complete" ) ; } }
public void test() { try { List < ResultsGroup > queryResults = searchService . query ( user , project , query , document , annotationLayer , annotationFeature , first , count ) . entrySet ( ) . stream ( ) . map ( e -> new ResultsGroup ( e . getKey ( ) , e . getValue ( ) ) ) . collect ( Collectors . toList ( ) ) ; pagesCacheModel . getObject ( ) . putPage ( first , count , queryResults ) ; return queryResults . iterator ( ) ; } catch ( IOException | ExecutionException e ) { LOG . error ( "Unable to retrieve results" , e ) ; return emptyIterator ( ) ; } }
public void test() { try { return JAXBContext . newInstance ( TemplateListModel . class ) ; } catch ( JAXBException e ) { LOGGER . error ( "Exception creating JAXBContext for templates: {}" , e . getLocalizedMessage ( ) , e ) ; return null ; } }
public void test() { try { atomUri = new URI ( uri ) ; code_block = ForStatement ; } catch ( URISyntaxException e ) { logger . warn ( uri + " atom uri problem" , e ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( ParseException e ) { logger . error ( e . getMessage ( ) , e ) ; } }
public void test() { if ( duration < 0 || duration >= 255 ) { logger . debug ( "Permit join to {} invalid period of {} seconds." , destination , duration ) ; return ZigBeeStatus . INVALID_ARGUMENTS ; } }
@ Test public void testInsufficientArgsLogsErrorForWarn ( ) { String format = "some message: %s, %d" ; String param = "blah" ; logger . warn ( format , param ) ; assertLogLike ( Level . SEVERE , ImmutableList . of ( "Invalid format" , "WARN" , format , param ) , IllegalArgumentException . class ) ; assertLog ( Level . WARNING , String . format ( "'%s' [%s]" , format , param ) ) ; }
public void test() { try { user = userManager . saveUser ( user ) ; } catch ( AccessDeniedException ade ) { log . warn ( ade . getMessage ( ) ) ; getResponse ( ) . sendError ( HttpServletResponse . SC_FORBIDDEN ) ; return null ; } catch ( UserExistsException e ) { addMessage ( "errors.existing.user" , new Object [ ] code_block = "" ; ) ; user . setPassword ( user . getConfirmPassword ( ) ) ; return null ; } }
public void test() { if ( ! databaseValue . errorMessages ( ) . isEmpty ( ) ) { return ; } }
public void test() { if ( nativeDecoding ) { LOG . info ( "Native decoding is enabled to " + inboundName + ". Inbound deserialization done at the broker." ) ; } else { LOG . info ( "Native decoding is disabled to " + inboundName + ". Inbound message conversion done by Spring Cloud Stream." ) ; } }
public void test() { if ( nativeDecoding ) { LOG . info ( "Native decoding is enabled to " + inboundName + ". Inbound deserialization done at the broker." ) ; } else { LOG . info ( "Native decoding is disabled to " + inboundName + ". Inbound message conversion done by Spring Cloud Stream." ) ; } }
public void test() { try { LOG . info ( "invalidating connection" ) ; m_impl . close ( ) ; LOG . info ( "connection invalidated" ) ; } finally { m_impl = null ; } }
public void test() { if ( factory == null ) { log . warn ( "There is no Asterisk configured" ) ; return null ; } }
public void test() { if ( r != null ) { log . debug ( "{}" , r ) ; } }
public void test() { try { connectManager ( ) ; ManagerResponse r = con . sendAction ( action ) ; code_block = IfStatement ; return ( r instanceof ManagerError ) ? null : r ; } catch ( Exception e ) { log . error ( "Error while executing ManagerAction: {}" , action , e ) ; } }
public void test() { if ( args != null ) { code_block = ForStatement ; } }
public void test() { if ( error . isJsonPrimitive ( ) ) { logger . debug ( "A remote exception occurred: '{}'" , error . getAsString ( ) ) ; } else-if ( error . isJsonObject ( ) ) { JsonObject o = error . getAsJsonObject ( ) ; Integer code = ( o . has ( "code" ) ? o . get ( "code" ) . getAsInt ( ) : null ) ; String message = ( o . has ( "message" ) ? o . get ( "message" ) . getAsString ( ) : null ) ; String data = ( o . has ( "data" ) ? ( o . get ( "data" ) instanceof JsonObject ? o . get ( "data" ) . toString ( ) : o . get ( "data" ) . getAsString ( ) ) : null ) ; logger . debug ( "A remote exception occurred: '{}':'{}':'{}'" , code , message , data ) ; } else { logger . debug ( "An unknown remote exception occurred: '{}'" , error . toString ( ) ) ; } }
public void test() { if ( error . isJsonPrimitive ( ) ) { logger . debug ( "A remote exception occurred: '{}'" , error . getAsString ( ) ) ; } else-if ( error . isJsonObject ( ) ) { JsonObject o = error . getAsJsonObject ( ) ; Integer code = ( o . has ( "code" ) ? o . get ( "code" ) . getAsInt ( ) : null ) ; String message = ( o . has ( "message" ) ? o . get ( "message" ) . getAsString ( ) : null ) ; String data = ( o . has ( "data" ) ? ( o . get ( "data" ) instanceof JsonObject ? o . get ( "data" ) . toString ( ) : o . get ( "data" ) . getAsString ( ) ) : null ) ; logger . debug ( "A remote exception occurred: '{}':'{}':'{}'" , code , message , data ) ; } else { logger . debug ( "An unknown remote exception occurred: '{}'" , error . toString ( ) ) ; } }
public void test() { if ( error . isJsonPrimitive ( ) ) { logger . debug ( "A remote exception occurred: '{}'" , error . getAsString ( ) ) ; } else-if ( error . isJsonObject ( ) ) { JsonObject o = error . getAsJsonObject ( ) ; Integer code = ( o . has ( "code" ) ? o . get ( "code" ) . getAsInt ( ) : null ) ; String message = ( o . has ( "message" ) ? o . get ( "message" ) . getAsString ( ) : null ) ; String data = ( o . has ( "data" ) ? ( o . get ( "data" ) instanceof JsonObject ? o . get ( "data" ) . toString ( ) : o . get ( "data" ) . getAsString ( ) ) : null ) ; logger . debug ( "A remote exception occurred: '{}':'{}':'{}'" , code , message , data ) ; } else { logger . debug ( "An unknown remote exception occurred: '{}'" , error . toString ( ) ) ; } }
public void test() { if ( ! wonMessage . getForwardedMessageURIs ( ) . isEmpty ( ) ) { logger . debug ( "This message contains the forwarded message(s) {}" , wonMessage . getForwardedMessageURIs ( ) ) ; } }
public void test() { try { errorConsumer . accept ( exception ) ; } catch ( Exception e ) { LOGGER . warn ( "Operation " + operationModel . getName ( ) + " from extension " + extensionModel . getName ( ) + " threw exception while executing the onError FlowListener" , e ) ; } }
public void test() { try { errorConsumer . accept ( exception ) ; } catch ( Exception e ) { LOGGER . warn ( "Operation " + operationModel . getName ( ) + " from extension " + extensionModel . getName ( ) + " threw exception while executing the onError FlowListener" , e ) ; } }
public void test() { try { payload = message . getObject ( ) ; code_block = IfStatement ; } catch ( JMSException ex ) { LOG . error ( ex . getLocalizedMessage ( ) ) ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( "Request {} is already done" , requestContext . hashCode ( ) ) ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( "Request {} reached timeout of {}, current calculation {}" , requestContext . hashCode ( ) , requestContext . getTimeout ( ) , calculateTimeout ) ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( "Request {} reached timeout of {}, current calculation {}" , requestContext . hashCode ( ) , requestContext . getTimeout ( ) , calculateTimeout ) ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( "Request {} reached timeout of {}, current calculation {}" , requestContext . hashCode ( ) , requestContext . getTimeout ( ) , calculateTimeout ) ; } }
public void test() { if ( LOG . isTraceEnabled ( ) ) { LOG . trace ( "Send request {} to {} with topic {}" , requestContext . hashCode ( ) , requestContext . getNodeAddress ( ) , requestContext . getTopicName ( ) ) ; } }
public void test() { try { Class < ? > aClass = Class . forName ( className , false , TaskExecutors . class . getClassLoader ( ) ) ; Method method = aClass . getDeclaredMethod ( methodName , parameterTypes ) ; method . setAccessible ( true ) ; return method ; } catch ( Exception e ) { LOG . trace ( "failed to activate method: {}{}" , className , methodName , e ) ; return null ; } }
@ Activate public void activate ( ComponentContext cc ) throws Exception { logger . debug ( "Activating admin group loader" ) ; BundleContext bundleCtx = cc . getBundleContext ( ) ; adminUserName = StringUtils . trimToNull ( bundleCtx . getProperty ( SecurityConstants . GLOBAL_ADMIN_USER_PROPERTY ) ) ; adminPassword = StringUtils . trimToNull ( bundleCtx . getProperty ( OPT_ADMIN_PASSWORD ) ) ; adminEmail = StringUtils . trimToNull ( bundleCtx . getProperty ( OPT_ADMIN_EMAIL ) ) ; adminRoles = StringUtils . trimToNull ( bundleCtx . getProperty ( OPT_ADMIN_ROLES ) ) ; code_block = IfStatement ; componentCtx = cc ; code_block = ForStatement ; }
public void test() { if ( DEFAULT_ADMIN_PASSWORD_CONFIGURATION . equals ( adminPassword ) ) { logger . warn ( "\n" + "\n" + "                                                    \n" + " WARNING: Opencast still uses the default admin     \n" + "          credentials. Never do this in production. \n" + "                                                    \n" + "          To change the password, edit the key      \n" + "          org.opencastproject.security.admin.pass   \n" + "          in custom.properties.                     \n" + "                                                    \n" + "" ) ; } }
public void test() { try { result = SchemaMethodResource . wrapInResult ( typedIdResource . getEntity ( ) . invokeMethod ( securityContext , methodName , propertySet , true , new EvaluationHints ( ) ) ) ; } catch ( Throwable t ) { logger . warn ( "Unable to execute {}.{}: {}" , entityType . getSimpleName ( ) , methodName , t . getMessage ( ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Removing publish in progress marker from updated node, because update was not successful. (node=" + nodeRef . toString ( ) + ")" ) ; } }
protected void init ( StartupStrategy startupStrategy ) { logger . info ( "Selected startup strategy {}" , startupStrategy ) ; code_block = ForStatement ; logger . info ( "Configured '{}' server state repository" , this . repository . getClass ( ) . getSimpleName ( ) ) ; this . context = new KieServerRegistryImpl ( ) ; this . context . registerIdentityProvider ( new JACCIdentityProvider ( ) ) ; this . context . registerStateRepository ( repository ) ; ContainerLocatorProvider . get ( ) ; ContainerManager containerManager = getContainerManager ( ) ; KieServerState currentState = repository . load ( KieServerEnvironment . getServerId ( ) ) ; List < KieServerExtension > extensions = sortKnownExtensions ( ) ; code_block = ForStatement ; policyManager = new PolicyManager ( ) ; policyManager . start ( this , context ) ; kieServerActive . set ( true ) ; eventSupport . fireBeforeServerStarted ( this ) ; startTimestamp = System . currentTimeMillis ( ) ; startupStrategy . startup ( this , containerManager , currentState , kieServerActive ) ; eventSupport . fireAfterServerStarted ( this ) ; }
protected void init ( StartupStrategy startupStrategy ) { logger . info ( "Selected startup strategy {}" , startupStrategy ) ; code_block = ForStatement ; logger . info ( "Configured '{}' server state repository" , this . repository . getClass ( ) . getSimpleName ( ) ) ; this . context = new KieServerRegistryImpl ( ) ; this . context . registerIdentityProvider ( new JACCIdentityProvider ( ) ) ; this . context . registerStateRepository ( repository ) ; ContainerLocatorProvider . get ( ) ; ContainerManager containerManager = getContainerManager ( ) ; KieServerState currentState = repository . load ( KieServerEnvironment . getServerId ( ) ) ; List < KieServerExtension > extensions = sortKnownExtensions ( ) ; code_block = ForStatement ; policyManager = new PolicyManager ( ) ; policyManager . start ( this , context ) ; kieServerActive . set ( true ) ; eventSupport . fireBeforeServerStarted ( this ) ; startTimestamp = System . currentTimeMillis ( ) ; startupStrategy . startup ( this , containerManager , currentState , kieServerActive ) ; eventSupport . fireAfterServerStarted ( this ) ; }
public void test() { if ( extension . isInitialized ( ) ) { logger . info ( "{} has been successfully registered as server extension" , extension ) ; } else { logger . warn ( "{} has not been registered as server extension" , extension ) ; } }
public void test() { if ( extension . isInitialized ( ) ) { logger . info ( "{} has been successfully registered as server extension" , extension ) ; } else { logger . warn ( "{} has not been registered as server extension" , extension ) ; } }
public void test() { try { extension . init ( this , this . context ) ; this . context . registerServerExtension ( extension ) ; code_block = IfStatement ; } catch ( Exception e ) { serverMessages . add ( new Message ( Severity . ERROR , "Error when initializing server extension of type " + extension + " due to " + e . getMessage ( ) ) ) ; logger . error ( "Error when initializing server extension of type {}" , extension , e ) ; } }
public void test() { try { BooleanQuery . setMaxClauseCount ( propertyService . get ( LUCENE_BOOLEAN_QUERY_MAX_CLAUSE_COUNT ) ) ; } catch ( PropertyServiceIncompleteRegistrationException e ) { LOGGER . warn ( String . format ( "Property boolean query max clause count doesn't exists, value is set to %d" , BooleanQuery . getMaxClauseCount ( ) ) ) ; } catch ( NullPointerException e ) { LOGGER . warn ( String . format ( "Property service is null, boolean query max clause count value is set to %d" , BooleanQuery . getMaxClauseCount ( ) ) ) ; } }
public void test() { try { BooleanQuery . setMaxClauseCount ( propertyService . get ( LUCENE_BOOLEAN_QUERY_MAX_CLAUSE_COUNT ) ) ; } catch ( PropertyServiceIncompleteRegistrationException e ) { LOGGER . warn ( String . format ( "Property boolean query max clause count doesn't exists, value is set to %d" , BooleanQuery . getMaxClauseCount ( ) ) ) ; } catch ( NullPointerException e ) { LOGGER . warn ( String . format ( "Property service is null, boolean query max clause count value is set to %d" , BooleanQuery . getMaxClauseCount ( ) ) ) ; } }
private JSONObject getTweets ( ) throws IOException { currentRequest ++ ; String url = getApiURL ( lastMaxID - 1 ) ; LOGGER . info ( "    Retrieving " + url ) ; Document doc = Http . url ( url ) . ignoreContentType ( ) . header ( "Authorization" , "Bearer " + accessToken ) . header ( "Content-Type" , "application/x-www-form-urlencoded;charset=UTF-8" ) . header ( "User-agent" , "ripe and zipe" ) . get ( ) ; String body = doc . body ( ) . html ( ) . replaceAll ( "&quot;" , "\"" ) ; Object jsonObj = new JSONTokener ( body ) . nextValue ( ) ; JSONArray statuses ; code_block = IfStatement ; JSONObject r = new JSONObject ( ) ; r . put ( "tweets" , statuses ) ; return r ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "There has not been found info of dependencies for module called " + moduleName ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Module'" + moduleName + "' had dependencies but it id not contain information of other modules" ) ; } }
public void test() { try { java . util . List < com . liferay . portal . reports . engine . console . model . Entry > returnValue = EntryServiceUtil . getEntries ( groupId , definitionName , userName , createDateGT , createDateLT , andSearch , start , end , orderByComparator ) ; return com . liferay . portal . reports . engine . console . model . EntrySoap . toSoapModels ( returnValue ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
@ Test public void intervalTest2 ( ) throws BackendException { String [ ] [ ] values = generateValues ( ) ; log . debug ( "Loading values..." ) ; loadValues ( values ) ; newTx ( ) ; Set < KeyColumn > deleted = deleteValues ( 7 ) ; clopen ( ) ; checkRandomSlices ( values , deleted ) ; }
public void test() { try { bindingPOA . deactivate_object ( objectId ) ; } catch ( ObjectNotActive ona ) { LOG . info ( "Caught ObjectNotActive exception: " + ona + " during deactivate_object() call on POA: " + bindingPOA ) ; } catch ( Exception ex ) { throw new CorbaBindingException ( "Unable to deactivate CORBA servant" , ex ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { ActiveMQRALogger . LOGGER . trace ( "isEnable1xPrefixes()" ) ; } }
public void test() { try { addEids ( patientsBuilder , eids ) ; addIds ( patientsBuilder , ids ) ; final String json = this . objectMapper . writeValueAsString ( patientsBuilder . build ( ) ) ; return Response . ok ( json , MediaType . APPLICATION_JSON_TYPE ) . build ( ) ; } catch ( final JsonProcessingException ex ) { this . slf4Jlogger . error ( "Failed to serialize patients [{}] to JSON: {}" , eids , ex . getMessage ( ) ) ; return Response . status ( Response . Status . INTERNAL_SERVER_ERROR ) . build ( ) ; } catch ( final QueryException ex ) { this . slf4Jlogger . error ( "Failed to retrieve patients with external ids [{}]: {}" , eids , ex . getMessage ( ) ) ; return Response . status ( Response . Status . INTERNAL_SERVER_ERROR ) . build ( ) ; } }
public void test() { try { addEids ( patientsBuilder , eids ) ; addIds ( patientsBuilder , ids ) ; final String json = this . objectMapper . writeValueAsString ( patientsBuilder . build ( ) ) ; return Response . ok ( json , MediaType . APPLICATION_JSON_TYPE ) . build ( ) ; } catch ( final JsonProcessingException ex ) { this . slf4Jlogger . error ( "Failed to serialize patients [{}] to JSON: {}" , eids , ex . getMessage ( ) ) ; return Response . status ( Response . Status . INTERNAL_SERVER_ERROR ) . build ( ) ; } catch ( final QueryException ex ) { this . slf4Jlogger . error ( "Failed to retrieve patients with external ids [{}]: {}" , eids , ex . getMessage ( ) ) ; return Response . status ( Response . Status . INTERNAL_SERVER_ERROR ) . build ( ) ; } }
public void test() { if ( isDebug ) { log . error ( basicMsg + " while processing Metric '" + tmpl + "'" , exc ) ; } else { log . error ( basicMsg + ": " + msg ) ; code_block = IfStatement ; } }
public void test() { if ( printStack ) { log . error ( "Stack trace follows:" , exc ) ; } }
public void test() { if ( _log . isInfoEnabled ( ) ) { _log . info ( "...Setting queryLogLevelInfo: " + queryLogLevelInfo ) ; } }
@ Override public void initialize ( IAsyncResultHandler < Void > startupHandler ) { log . info ( "Starting an EBInMemoryRegistry on UUID {0}" , registryUuid ) ; this . proxy = new EBRegistryProxy ( vertx , address ( ) , registryUuid ) ; listenProxyHandler ( startupHandler ) ; }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Node to bucketId map: {}" , ret ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Validating attachment contentType: " + receivedAttachment . getContentType ( ) + "='" + controlAttachment . getContentType ( ) + "': OK." ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( AssetTagServiceUtil . class , "mergeTags" , _mergeTagsParameterTypes21 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , fromTagIds , toTagId ) ; code_block = TryStatement ;  } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
@ Override public ODistributedTxContext popTxContext ( final ODistributedRequestId requestId ) { final ODistributedTxContext ctx = activeTxContexts . remove ( requestId ) ; ODistributedServerLog . debug ( this , localNodeName , null , DIRECTION . NONE , "Distributed transaction: pop request %s for database %s -> %s" , requestId , databaseName , ctx ) ; return ctx ; }
public void trace ( final String message , final Throwable error ) { Logger . trace ( id , message , error ) ; }
public void test() { if ( getLogger ( ) . isDebugEnabled ( ) ) { getLogger ( ) . debug ( "[" + getEmbeddedHandler ( ) . getMainSyncHandler ( ) . getSpaceEngine ( ) . getFullSpaceName ( ) + "]" + " EmbeddedSyncSegment will not transfer " + oi . getOriginalOpInfo ( ) + " current generation [" + _currentGenerationId + "]" ) ; } }
public void test() { if ( getLogger ( ) . isDebugEnabled ( ) ) { getLogger ( ) . debug ( "[" + getEmbeddedHandler ( ) . getMainSyncHandler ( ) . getSpaceEngine ( ) . getFullSpaceName ( ) + "]" + " EmbeddedSyncSegment will not transfer " + oi . getOriginalOpInfo ( ) + " current generation [" + _currentGenerationId + "] because afterRecovery already cleared it" ) ; } }
public void test() { if ( getLogger ( ) . isDebugEnabled ( ) ) { getLogger ( ) . debug ( "[" + getEmbeddedHandler ( ) . getMainSyncHandler ( ) . getSpaceEngine ( ) . getFullSpaceName ( ) + "]" + " EmbeddedSyncSegment will not transfer " + oi . getOriginalOpInfo ( ) + " current generation [" + _currentGenerationId + "], relevant? [" + relevant + "], should add to main sync list? [" + addToMainList + "]" ) ; } }
public void test() { try { updatedVariant . merge ( webPushVariant ) ; validateModelClass ( webPushVariant ) ; } catch ( ConstraintViolationException cve ) { logger . info ( "Unable to update WebPush Variant '{}'" , webPushVariant . getVariantID ( ) ) ; logger . debug ( "Details: {}" , cve ) ; Response . ResponseBuilder builder = createBadRequestResponse ( cve . getConstraintViolations ( ) ) ; return builder . build ( ) ; } }
@ Override public void doExecute ( TestContext context ) { log . info ( context . getVariable ( "index" ) ) ; }
public void test() { try { log . debug ( "Checking for next block of results from {}" , querier . toString ( ) ) ; querier . maybeStartQuery ( cachedConnectionProvider . getConnection ( ) ) ; int batchMaxRows = config . getInt ( JdbcSourceTaskConfig . BATCH_MAX_ROWS_CONFIG ) ; boolean hadNext = true ; code_block = WhileStatement ; code_block = IfStatement ; code_block = IfStatement ; log . debug ( "Returning {} records for {}" , results . size ( ) , querier . toString ( ) ) ; return results ; } catch ( SQLException sqle ) { log . error ( "Failed to run query for table {}: {}" , querier . toString ( ) , sqle ) ; resetAndRequeueHead ( querier ) ; return null ; } catch ( Throwable t ) { resetAndRequeueHead ( querier ) ; closeResources ( ) ; throw t ; } }
public void test() { if ( ! querier . querying ( ) ) { final long nextUpdate = querier . getLastUpdate ( ) + config . getInt ( JdbcSourceTaskConfig . POLL_INTERVAL_MS_CONFIG ) ; final long now = time . milliseconds ( ) ; final long sleepMs = Math . min ( nextUpdate - now , 100 ) ; code_block = IfStatement ; } }
public void test() { try { log . debug ( "Checking for next block of results from {}" , querier . toString ( ) ) ; querier . maybeStartQuery ( cachedConnectionProvider . getConnection ( ) ) ; int batchMaxRows = config . getInt ( JdbcSourceTaskConfig . BATCH_MAX_ROWS_CONFIG ) ; boolean hadNext = true ; code_block = WhileStatement ; code_block = IfStatement ; code_block = IfStatement ; log . debug ( "Returning {} records for {}" , results . size ( ) , querier . toString ( ) ) ; return results ; } catch ( SQLException sqle ) { log . error ( "Failed to run query for table {}: {}" , querier . toString ( ) , sqle ) ; resetAndRequeueHead ( querier ) ; return null ; } catch ( Throwable t ) { resetAndRequeueHead ( querier ) ; closeResources ( ) ; throw t ; } }
public void test() { try { log . debug ( "Checking for next block of results from {}" , querier . toString ( ) ) ; querier . maybeStartQuery ( cachedConnectionProvider . getConnection ( ) ) ; int batchMaxRows = config . getInt ( JdbcSourceTaskConfig . BATCH_MAX_ROWS_CONFIG ) ; boolean hadNext = true ; code_block = WhileStatement ; code_block = IfStatement ; code_block = IfStatement ; log . debug ( "Returning {} records for {}" , results . size ( ) , querier . toString ( ) ) ; return results ; } catch ( SQLException sqle ) { log . error ( "Failed to run query for table {}: {}" , querier . toString ( ) , sqle ) ; resetAndRequeueHead ( querier ) ; return null ; } catch ( Throwable t ) { resetAndRequeueHead ( querier ) ; closeResources ( ) ; throw t ; } }
@ Override public void delete ( ScriptKey scriptKey ) { log . trace ( MessageFormat . format ( "Deleting script {0}" , scriptKey . toString ( ) ) ) ; ScriptVersionKey scriptVersionKey = new ScriptVersionKey ( new ScriptKey ( scriptKey . getScriptId ( ) , scriptKey . getScriptVersion ( ) ) ) ; ScriptVersionConfiguration . getInstance ( ) . delete ( scriptVersionKey ) ; ActionConfiguration . getInstance ( ) . deleteByScript ( scriptKey ) ; ScriptParameterConfiguration . getInstance ( ) . deleteByScript ( scriptKey ) ; ScriptLabelConfiguration . getInstance ( ) . deleteByScript ( scriptKey ) ; getDeleteStatement ( scriptKey ) . ifPresent ( getMetadataRepository ( ) :: executeUpdate ) ; }
public void test() { try { capabilities = liveJvmService . getCapabilities ( agentId ) ; } catch ( AgentNotConnectedException e ) { logger . debug ( e . getMessage ( ) , e ) ; return "{\"agentNotConnected\":true}" ; } }
public void test() { try { CloudFileDirectory dir = rootDir . getDirectoryReference ( note . getId ( ) ) ; dir . createIfNotExists ( ) ; CloudFile cloudFile = dir . getFileReference ( "note.json" ) ; cloudFile . uploadFromByteArray ( buffer , 0 , buffer . length ) ; } catch ( URISyntaxException | StorageException e ) { String msg = String . format ( "Error saving notebook %s to Azure storage" , note . getId ( ) ) ; LOG . error ( msg , e ) ; throw new IOException ( msg , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Expected " + expectAuthRefs + " authority references, found " + numAuthRefsFound ) ; } }
public void test() { for ( AuthorityRefList . AuthorityRefItem item : items ) { logger . debug ( testName + ": list-item[" + i + "] Field:" + item . getSourceField ( ) + " =" + " item display name = " + item . getAuthDisplayName ( ) + " auth display name = " + item . getItemDisplayName ( ) ) ; logger . debug ( testName + ": list-item[" + i + "] refName=" + item . getRefName ( ) ) ; logger . debug ( testName + ": list-item[" + i + "] URI=" + item . getUri ( ) ) ; i ++ ; } }
public void test() { for ( AuthorityRefList . AuthorityRefItem item : items ) { logger . debug ( testName + ": list-item[" + i + "] Field:" + item . getSourceField ( ) + " =" + " item display name = " + item . getAuthDisplayName ( ) + " auth display name = " + item . getItemDisplayName ( ) ) ; logger . debug ( testName + ": list-item[" + i + "] refName=" + item . getRefName ( ) ) ; logger . debug ( testName + ": list-item[" + i + "] URI=" + item . getUri ( ) ) ; i ++ ; } }
public void test() { for ( AuthorityRefList . AuthorityRefItem item : items ) { logger . debug ( testName + ": list-item[" + i + "] Field:" + item . getSourceField ( ) + " =" + " item display name = " + item . getAuthDisplayName ( ) + " auth display name = " + item . getItemDisplayName ( ) ) ; logger . debug ( testName + ": list-item[" + i + "] refName=" + item . getRefName ( ) ) ; logger . debug ( testName + ": list-item[" + i + "] URI=" + item . getUri ( ) ) ; i ++ ; } }
public void test() { for ( String transportName : crucialExceptions . keySet ( ) ) { JournalException e = crucialExceptions . get ( transportName ) ; logger . error ( "Exception thrown from crucial Journal Transport: '" + transportName + "'" , e ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( portalException , portalException ) ; } }
public void test() { if ( aEvt . getSeverityLevel ( ) . equals ( SeverityLevel . WARNING ) ) { log . warn ( sb . toString ( ) ) ; } else-if ( aEvt . getSeverityLevel ( ) . equals ( SeverityLevel . INFO ) ) { log . info ( sb . toString ( ) ) ; } else { log . error ( sb . toString ( ) ) ; } }
public void test() { if ( aEvt . getSeverityLevel ( ) . equals ( SeverityLevel . WARNING ) ) { log . warn ( sb . toString ( ) ) ; } else-if ( aEvt . getSeverityLevel ( ) . equals ( SeverityLevel . INFO ) ) { log . info ( sb . toString ( ) ) ; } else { log . error ( sb . toString ( ) ) ; } }
public void test() { if ( aEvt . getSeverityLevel ( ) . equals ( SeverityLevel . WARNING ) ) { log . warn ( sb . toString ( ) ) ; } else-if ( aEvt . getSeverityLevel ( ) . equals ( SeverityLevel . INFO ) ) { log . info ( sb . toString ( ) ) ; } else { log . error ( sb . toString ( ) ) ; } }
public void test() { try { value = URLDecoder . decode ( elements [ 1 ] , "UTF8" ) ; } catch ( UnsupportedEncodingException e ) { logger . info ( "Unsupported encoding" , e ) ; } }
@ Test @ Ignore public void ensureInternalEndpointIsSecured ( ) throws Throwable { final String connectorTasksEndpoint = connect . endpointForResource ( String . format ( "connectors/%s/tasks" , CONNECTOR_NAME ) ) ; final Map < String , String > emptyHeaders = new HashMap < > ( ) ; final Map < String , String > invalidSignatureHeaders = new HashMap < > ( ) ; invalidSignatureHeaders . put ( SIGNATURE_HEADER , "S2Fma2Flc3F1ZQ==" ) ; invalidSignatureHeaders . put ( SIGNATURE_ALGORITHM_HEADER , "HmacSHA256" ) ; log . info ( "Making a POST request to the {} endpoint with no connector started and no signature header; " + "expecting 400 error response" , connectorTasksEndpoint ) ; assertEquals ( BAD_REQUEST . getStatusCode ( ) , connect . requestPost ( connectorTasksEndpoint , "[]" , emptyHeaders ) . getStatus ( ) ) ; log . info ( "Making a POST request to the {} endpoint with no connector started and an invalid signature header; " + "expecting 403 error response" , connectorTasksEndpoint ) ; assertEquals ( FORBIDDEN . getStatusCode ( ) , connect . requestPost ( connectorTasksEndpoint , "[]" , invalidSignatureHeaders ) . getStatus ( ) ) ; Map < String , String > connectorProps = new HashMap < > ( ) ; connectorProps . put ( CONNECTOR_CLASS_CONFIG , MonitorableSinkConnector . class . getSimpleName ( ) ) ; connectorProps . put ( TASKS_MAX_CONFIG , String . valueOf ( 1 ) ) ; connectorProps . put ( TOPICS_CONFIG , "test-topic" ) ; connectorProps . put ( KEY_CONVERTER_CLASS_CONFIG , StringConverter . class . getName ( ) ) ; connectorProps . put ( VALUE_CONVERTER_CLASS_CONFIG , StringConverter . class . getName ( ) ) ; log . info ( "Starting the {} connector" , CONNECTOR_NAME ) ; StartAndStopLatch startLatch = connectorHandle . expectedStarts ( 1 ) ; connect . configureConnector ( CONNECTOR_NAME , connectorProps ) ; startLatch . await ( CONNECTOR_SETUP_DURATION_MS , TimeUnit . MILLISECONDS ) ; log . info ( "Making a POST request to the {} endpoint with the connector started and no signature header; " + "expecting 400 error response" , connectorTasksEndpoint ) ; assertEquals ( BAD_REQUEST . getStatusCode ( ) , connect . requestPost ( connectorTasksEndpoint , "[]" , emptyHeaders ) . getStatus ( ) ) ; log . info ( "Making a POST request to the {} endpoint with the connector started and an invalid signature header; " + "expecting 403 error response" , connectorTasksEndpoint ) ; assertEquals ( FORBIDDEN . getStatusCode ( ) , connect . requestPost ( connectorTasksEndpoint , "[]" , invalidSignatureHeaders ) . getStatus ( ) ) ; }
@ Test @ Ignore public void ensureInternalEndpointIsSecured ( ) throws Throwable { final String connectorTasksEndpoint = connect . endpointForResource ( String . format ( "connectors/%s/tasks" , CONNECTOR_NAME ) ) ; final Map < String , String > emptyHeaders = new HashMap < > ( ) ; final Map < String , String > invalidSignatureHeaders = new HashMap < > ( ) ; invalidSignatureHeaders . put ( SIGNATURE_HEADER , "S2Fma2Flc3F1ZQ==" ) ; invalidSignatureHeaders . put ( SIGNATURE_ALGORITHM_HEADER , "HmacSHA256" ) ; log . info ( "Making a POST request to the {} endpoint with no connector started and no signature header; " + "expecting 400 error response" , connectorTasksEndpoint ) ; assertEquals ( BAD_REQUEST . getStatusCode ( ) , connect . requestPost ( connectorTasksEndpoint , "[]" , emptyHeaders ) . getStatus ( ) ) ; log . info ( "Making a POST request to the {} endpoint with no connector started and an invalid signature header; " + "expecting 403 error response" , connectorTasksEndpoint ) ; assertEquals ( FORBIDDEN . getStatusCode ( ) , connect . requestPost ( connectorTasksEndpoint , "[]" , invalidSignatureHeaders ) . getStatus ( ) ) ; Map < String , String > connectorProps = new HashMap < > ( ) ; connectorProps . put ( CONNECTOR_CLASS_CONFIG , MonitorableSinkConnector . class . getSimpleName ( ) ) ; connectorProps . put ( TASKS_MAX_CONFIG , String . valueOf ( 1 ) ) ; connectorProps . put ( TOPICS_CONFIG , "test-topic" ) ; connectorProps . put ( KEY_CONVERTER_CLASS_CONFIG , StringConverter . class . getName ( ) ) ; connectorProps . put ( VALUE_CONVERTER_CLASS_CONFIG , StringConverter . class . getName ( ) ) ; log . info ( "Starting the {} connector" , CONNECTOR_NAME ) ; StartAndStopLatch startLatch = connectorHandle . expectedStarts ( 1 ) ; connect . configureConnector ( CONNECTOR_NAME , connectorProps ) ; startLatch . await ( CONNECTOR_SETUP_DURATION_MS , TimeUnit . MILLISECONDS ) ; log . info ( "Making a POST request to the {} endpoint with the connector started and no signature header; " + "expecting 400 error response" , connectorTasksEndpoint ) ; assertEquals ( BAD_REQUEST . getStatusCode ( ) , connect . requestPost ( connectorTasksEndpoint , "[]" , emptyHeaders ) . getStatus ( ) ) ; log . info ( "Making a POST request to the {} endpoint with the connector started and an invalid signature header; " + "expecting 403 error response" , connectorTasksEndpoint ) ; assertEquals ( FORBIDDEN . getStatusCode ( ) , connect . requestPost ( connectorTasksEndpoint , "[]" , invalidSignatureHeaders ) . getStatus ( ) ) ; }
@ Test @ Ignore public void ensureInternalEndpointIsSecured ( ) throws Throwable { final String connectorTasksEndpoint = connect . endpointForResource ( String . format ( "connectors/%s/tasks" , CONNECTOR_NAME ) ) ; final Map < String , String > emptyHeaders = new HashMap < > ( ) ; final Map < String , String > invalidSignatureHeaders = new HashMap < > ( ) ; invalidSignatureHeaders . put ( SIGNATURE_HEADER , "S2Fma2Flc3F1ZQ==" ) ; invalidSignatureHeaders . put ( SIGNATURE_ALGORITHM_HEADER , "HmacSHA256" ) ; log . info ( "Making a POST request to the {} endpoint with no connector started and no signature header; " + "expecting 400 error response" , connectorTasksEndpoint ) ; assertEquals ( BAD_REQUEST . getStatusCode ( ) , connect . requestPost ( connectorTasksEndpoint , "[]" , emptyHeaders ) . getStatus ( ) ) ; log . info ( "Making a POST request to the {} endpoint with no connector started and an invalid signature header; " + "expecting 403 error response" , connectorTasksEndpoint ) ; assertEquals ( FORBIDDEN . getStatusCode ( ) , connect . requestPost ( connectorTasksEndpoint , "[]" , invalidSignatureHeaders ) . getStatus ( ) ) ; Map < String , String > connectorProps = new HashMap < > ( ) ; connectorProps . put ( CONNECTOR_CLASS_CONFIG , MonitorableSinkConnector . class . getSimpleName ( ) ) ; connectorProps . put ( TASKS_MAX_CONFIG , String . valueOf ( 1 ) ) ; connectorProps . put ( TOPICS_CONFIG , "test-topic" ) ; connectorProps . put ( KEY_CONVERTER_CLASS_CONFIG , StringConverter . class . getName ( ) ) ; connectorProps . put ( VALUE_CONVERTER_CLASS_CONFIG , StringConverter . class . getName ( ) ) ; log . info ( "Starting the {} connector" , CONNECTOR_NAME ) ; StartAndStopLatch startLatch = connectorHandle . expectedStarts ( 1 ) ; connect . configureConnector ( CONNECTOR_NAME , connectorProps ) ; startLatch . await ( CONNECTOR_SETUP_DURATION_MS , TimeUnit . MILLISECONDS ) ; log . info ( "Making a POST request to the {} endpoint with the connector started and no signature header; " + "expecting 400 error response" , connectorTasksEndpoint ) ; assertEquals ( BAD_REQUEST . getStatusCode ( ) , connect . requestPost ( connectorTasksEndpoint , "[]" , emptyHeaders ) . getStatus ( ) ) ; log . info ( "Making a POST request to the {} endpoint with the connector started and an invalid signature header; " + "expecting 403 error response" , connectorTasksEndpoint ) ; assertEquals ( FORBIDDEN . getStatusCode ( ) , connect . requestPost ( connectorTasksEndpoint , "[]" , invalidSignatureHeaders ) . getStatus ( ) ) ; }
@ Test @ Ignore public void ensureInternalEndpointIsSecured ( ) throws Throwable { final String connectorTasksEndpoint = connect . endpointForResource ( String . format ( "connectors/%s/tasks" , CONNECTOR_NAME ) ) ; final Map < String , String > emptyHeaders = new HashMap < > ( ) ; final Map < String , String > invalidSignatureHeaders = new HashMap < > ( ) ; invalidSignatureHeaders . put ( SIGNATURE_HEADER , "S2Fma2Flc3F1ZQ==" ) ; invalidSignatureHeaders . put ( SIGNATURE_ALGORITHM_HEADER , "HmacSHA256" ) ; log . info ( "Making a POST request to the {} endpoint with no connector started and no signature header; " + "expecting 400 error response" , connectorTasksEndpoint ) ; assertEquals ( BAD_REQUEST . getStatusCode ( ) , connect . requestPost ( connectorTasksEndpoint , "[]" , emptyHeaders ) . getStatus ( ) ) ; log . info ( "Making a POST request to the {} endpoint with no connector started and an invalid signature header; " + "expecting 403 error response" , connectorTasksEndpoint ) ; assertEquals ( FORBIDDEN . getStatusCode ( ) , connect . requestPost ( connectorTasksEndpoint , "[]" , invalidSignatureHeaders ) . getStatus ( ) ) ; Map < String , String > connectorProps = new HashMap < > ( ) ; connectorProps . put ( CONNECTOR_CLASS_CONFIG , MonitorableSinkConnector . class . getSimpleName ( ) ) ; connectorProps . put ( TASKS_MAX_CONFIG , String . valueOf ( 1 ) ) ; connectorProps . put ( TOPICS_CONFIG , "test-topic" ) ; connectorProps . put ( KEY_CONVERTER_CLASS_CONFIG , StringConverter . class . getName ( ) ) ; connectorProps . put ( VALUE_CONVERTER_CLASS_CONFIG , StringConverter . class . getName ( ) ) ; log . info ( "Starting the {} connector" , CONNECTOR_NAME ) ; StartAndStopLatch startLatch = connectorHandle . expectedStarts ( 1 ) ; connect . configureConnector ( CONNECTOR_NAME , connectorProps ) ; startLatch . await ( CONNECTOR_SETUP_DURATION_MS , TimeUnit . MILLISECONDS ) ; log . info ( "Making a POST request to the {} endpoint with the connector started and no signature header; " + "expecting 400 error response" , connectorTasksEndpoint ) ; assertEquals ( BAD_REQUEST . getStatusCode ( ) , connect . requestPost ( connectorTasksEndpoint , "[]" , emptyHeaders ) . getStatus ( ) ) ; log . info ( "Making a POST request to the {} endpoint with the connector started and an invalid signature header; " + "expecting 403 error response" , connectorTasksEndpoint ) ; assertEquals ( FORBIDDEN . getStatusCode ( ) , connect . requestPost ( connectorTasksEndpoint , "[]" , invalidSignatureHeaders ) . getStatus ( ) ) ; }
@ Test @ Ignore public void ensureInternalEndpointIsSecured ( ) throws Throwable { final String connectorTasksEndpoint = connect . endpointForResource ( String . format ( "connectors/%s/tasks" , CONNECTOR_NAME ) ) ; final Map < String , String > emptyHeaders = new HashMap < > ( ) ; final Map < String , String > invalidSignatureHeaders = new HashMap < > ( ) ; invalidSignatureHeaders . put ( SIGNATURE_HEADER , "S2Fma2Flc3F1ZQ==" ) ; invalidSignatureHeaders . put ( SIGNATURE_ALGORITHM_HEADER , "HmacSHA256" ) ; log . info ( "Making a POST request to the {} endpoint with no connector started and no signature header; " + "expecting 400 error response" , connectorTasksEndpoint ) ; assertEquals ( BAD_REQUEST . getStatusCode ( ) , connect . requestPost ( connectorTasksEndpoint , "[]" , emptyHeaders ) . getStatus ( ) ) ; log . info ( "Making a POST request to the {} endpoint with no connector started and an invalid signature header; " + "expecting 403 error response" , connectorTasksEndpoint ) ; assertEquals ( FORBIDDEN . getStatusCode ( ) , connect . requestPost ( connectorTasksEndpoint , "[]" , invalidSignatureHeaders ) . getStatus ( ) ) ; Map < String , String > connectorProps = new HashMap < > ( ) ; connectorProps . put ( CONNECTOR_CLASS_CONFIG , MonitorableSinkConnector . class . getSimpleName ( ) ) ; connectorProps . put ( TASKS_MAX_CONFIG , String . valueOf ( 1 ) ) ; connectorProps . put ( TOPICS_CONFIG , "test-topic" ) ; connectorProps . put ( KEY_CONVERTER_CLASS_CONFIG , StringConverter . class . getName ( ) ) ; connectorProps . put ( VALUE_CONVERTER_CLASS_CONFIG , StringConverter . class . getName ( ) ) ; log . info ( "Starting the {} connector" , CONNECTOR_NAME ) ; StartAndStopLatch startLatch = connectorHandle . expectedStarts ( 1 ) ; connect . configureConnector ( CONNECTOR_NAME , connectorProps ) ; startLatch . await ( CONNECTOR_SETUP_DURATION_MS , TimeUnit . MILLISECONDS ) ; log . info ( "Making a POST request to the {} endpoint with the connector started and no signature header; " + "expecting 400 error response" , connectorTasksEndpoint ) ; assertEquals ( BAD_REQUEST . getStatusCode ( ) , connect . requestPost ( connectorTasksEndpoint , "[]" , emptyHeaders ) . getStatus ( ) ) ; log . info ( "Making a POST request to the {} endpoint with the connector started and an invalid signature header; " + "expecting 403 error response" , connectorTasksEndpoint ) ; assertEquals ( FORBIDDEN . getStatusCode ( ) , connect . requestPost ( connectorTasksEndpoint , "[]" , invalidSignatureHeaders ) . getStatus ( ) ) ; }
public void test() { if ( failureCount > MAXIMUM_FAILURE_COUNT ) { LOGGER . error ( "Failed {} processing {} consecutive times. Abort. Mail is saved in {}" , mail . getName ( ) , failureCount , configuration . getErrorRepositoryURL ( ) . asString ( ) ) ; storeInErrorRepository ( queueItem ) ; } else { LOGGER . error ( "Failed {} processing {} consecutive times. Mail is requeued with increased failure count." , mail . getName ( ) , failureCount , processingException ) ; reEnqueue ( queueItem , failureCount ) ; } }
public void test() { if ( failureCount > MAXIMUM_FAILURE_COUNT ) { LOGGER . error ( "Failed {} processing {} consecutive times. Abort. Mail is saved in {}" , mail . getName ( ) , failureCount , configuration . getErrorRepositoryURL ( ) . asString ( ) ) ; storeInErrorRepository ( queueItem ) ; } else { LOGGER . error ( "Failed {} processing {} consecutive times. Mail is requeued with increased failure count." , mail . getName ( ) , failureCount , processingException ) ; reEnqueue ( queueItem , failureCount ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( Exception nestedE ) { LOGGER . error ( "Could not apply standard error handling for {}, defaulting to nack" , mail . getName ( ) , nestedE ) ; nack ( queueItem , processingException ) ; } }
public void test() { if ( ! isPortForwarded ( vmName , port ) ) { LOGGER . info ( "Creating VirtualBox port forward to " + port ) ; commandExecutor . exec ( String . format ( "VBoxManage controlvm " + vmName + " natpf1 %d,tcp,127.0.0.1,%d,,%d" , port , port , port ) ) ; } }
public void test() { try { Set < IPAddress > servers = getConfiguredDnsServers ( netInterfaceConfig ) ; logger . trace ( "{} is WAN, adding its dns servers: {}" , netInterfaceConfig . getName ( ) , servers ) ; serverList . addAll ( servers ) ; } catch ( KuraException e ) { logger . error ( "Error adding dns servers for {}" , netInterfaceConfig . getName ( ) , e ) ; } }
public void test() { if ( testDriver != null && ! setUpClassCalled ) { log . info ( "setting up class" ) ; testDriver . setUpClass ( ) ; setUpClassCalled = true ; } }
public void test() { try { dos = new DataOutputStream ( new FileOutputStream ( f ) ) ; dos . writeBytes ( OperationResultPanel . this . getModel ( ) . getObject ( ) . getXml ( ) ) ; } catch ( IOException e ) { LOGGER . error ( "Could not download result: {}" , e . getMessage ( ) , e ) ; } finally { IOUtils . closeQuietly ( dos ) ; } }
public void test() { if ( _logger . isDebugEnabled ( ) ) { _logger . debug ( "Fail to update the " + _containerName + " container XML file to " + spaceName + " space; the DOM element for spaces is null" ) ; } }
public void test() { try { String encryptedData = encryptionService . encryptData ( decryptedData , null ) ; return encryptedData ; } catch ( Exception e ) { logger . error ( "BulkMigrationUser:encryptData:error occurred while encrypting data" , e ) ; throw new ProjectCommonException ( ResponseCode . SERVER_ERROR . getErrorCode ( ) , ResponseCode . userDataEncryptionError . getErrorMessage ( ) , ResponseCode . userDataEncryptionError . getResponseCode ( ) ) ; } }
@ Override public void afterConnectionEstablished ( Session session ) throws Exception { log . debug ( "Connection established with sessionId: " + session . getSessionId ( ) ) ; afterConnectionEstablishedLatch . countDown ( ) ; }
public void test() { try { BlogsPortletInstanceConfiguration blogsPortletInstanceConfiguration = portletDisplay . getPortletInstanceConfiguration ( BlogsPortletInstanceConfiguration . class ) ; code_block = IfStatement ; } catch ( ConfigurationException configurationException ) { _log . error ( "Unable to get blogs portlet instance configuration" , configurationException ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "SQL : " + query . toString ( ) ) ; LOG . debug ( "SQL.param.system : " + system ) ; LOG . debug ( "SQL.param.test : " + test ) ; LOG . debug ( "SQL.param.testcase : " + testcase ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "SQL : " + query . toString ( ) ) ; LOG . debug ( "SQL.param.system : " + system ) ; LOG . debug ( "SQL.param.test : " + test ) ; LOG . debug ( "SQL.param.testcase : " + testcase ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "SQL : " + query . toString ( ) ) ; LOG . debug ( "SQL.param.system : " + system ) ; LOG . debug ( "SQL.param.test : " + test ) ; LOG . debug ( "SQL.param.testcase : " + testcase ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "SQL : " + query . toString ( ) ) ; LOG . debug ( "SQL.param.system : " + system ) ; LOG . debug ( "SQL.param.test : " + test ) ; LOG . debug ( "SQL.param.testcase : " + testcase ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( SQLException e ) { LOG . warn ( "Exception Closing the connection : " + e . toString ( ) ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "intersect contract m = " + m ) ; } }
public void test() { try { Transaction transaction = tracer . currentTransaction ( ) ; code_block = IfStatement ; } catch ( Exception e ) { logger . error ( "Error in Kafka iterator wrapper" , e ) ; } }
public void test() { try { code_block = WhileStatement ; buffer . mark ( ) ; buffer . compact ( ) ; } catch ( final BufferUnderflowException ex ) { this . logger . warn ( "Unexpected buffer underflow. Resetting and compacting buffer." , ex ) ; buffer . reset ( ) ; buffer . compact ( ) ; } }
public void test() { try { doProcess ( auditMessage ) ; } catch ( CTTransactionException ctTransactionException ) { throw ctTransactionException ; } catch ( Exception exception ) { _log . fatal ( "Unable to process audit message " + auditMessage , exception ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( exception , exception ) ; } }
public void test() { try { ConfigurationParameters config = securityProvider . getConfiguration ( AuthorizationConfiguration . class ) . getParameters ( ) ; supportedPaths = CugUtil . getSupportedPaths ( config , mountInfoProvider ) ; importBehavior = CugUtil . getImportBehavior ( config ) ; code_block = IfStatement ; initialized = true ; } catch ( RepositoryException e ) { log . warn ( "Error while initializing cug importer" , e ) ; } }
protected void informAdministrator ( String errorMessage ) { code_block = IfStatement ; _informAdministrator = false ; StringBundler sb = new StringBundler ( 7 ) ; sb . append ( "Liferay does not have the Xuggler native libraries " ) ; sb . append ( "installed. In order to generate video and audio previews, " ) ; sb . append ( "please follow the instructions for Xuggler in the Server " ) ; sb . append ( "Administration section of the Control Panel at: " ) ; sb . append ( "http://<server>/group/control_panel/manage/-/server" ) ; sb . append ( "/external-services. Warning: " ) ; sb . append ( errorMessage ) ; _log . warn ( sb . toString ( ) ) ; }
public void test() { try { final ServerSocket socket = new ServerSocket ( configuration . getEmbeddedLdapPort ( ) ) ; socket . close ( ) ; return ; } catch ( Exception e ) { LOG . info ( "Directory service still alive. Waiting for shutdown..." ) ; LOG . debug ( "Exception trying to acquire the server socket" , e ) ; } }
public void test() { try { final ServerSocket socket = new ServerSocket ( configuration . getEmbeddedLdapPort ( ) ) ; socket . close ( ) ; return ; } catch ( Exception e ) { LOG . info ( "Directory service still alive. Waiting for shutdown..." ) ; LOG . debug ( "Exception trying to acquire the server socket" , e ) ; } }
public void test() { if ( destroyVirtualMachine != null ) { logger . debug ( ">> destroying virtualMachine(%s) job(%s)" , virtualMachineId , destroyVirtualMachine ) ; awaitCompletion ( destroyVirtualMachine ) ; } else { logger . trace ( "<< virtualMachine(%s) not found" , virtualMachineId ) ; } }
public void test() { if ( destroyVirtualMachine != null ) { logger . debug ( ">> destroying virtualMachine(%s) job(%s)" , virtualMachineId , destroyVirtualMachine ) ; awaitCompletion ( destroyVirtualMachine ) ; } else { logger . trace ( "<< virtualMachine(%s) not found" , virtualMachineId ) ; } }
public void test() { try { fileURL = new URL ( file ) ; } catch ( MalformedURLException e ) { logger . error ( e ) ; return false ; } }
@ Transactional ( rollbackFor = ArrowheadException . class ) public ServiceDefinitionResponseDTO createServiceDefinitionResponse ( final String serviceDefinition ) { logger . debug ( "createServiceDefinitionResponse started..." ) ; final ServiceDefinition serviceDefinitionEntry = createServiceDefinition ( serviceDefinition ) ; return DTOConverter . convertServiceDefinitionToServiceDefinitionResponseDTO ( serviceDefinitionEntry ) ; }
public void test() { if ( processed % logInterval == 0 ) { logger . info ( String . format ( "Recalculated computed location for %d of %d cataloging records." , processed , recordsToProcess ) ) ; } }
public void test() { if ( logger . isTraceEnabled ( ) ) { logger . trace ( "Skipping soft-deleted CollectionObject record with CSID " + collectionObjectCsid ) ; } }
public void test() { try { code_block = ForStatement ; } catch ( Exception e ) { String errMsg = "Error encountered in " + CLASSNAME + ": " + e . getLocalizedMessage ( ) + " " ; errMsg = errMsg + "Successfully updated " + numUpdated + " CollectionObject record(s) prior to error." ; logger . error ( errMsg ) ; setErrorResult ( errMsg ) ; getResults ( ) . setNumAffected ( numUpdated ) ; return getResults ( ) ; } }
public void test() { try { MimeMessage message = mail . getMessage ( ) ; message . writeTo ( System . err ) ; } catch ( IOException ioe ) { LOGGER . error ( "error printing message" , ioe ) ; } }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( sb . toString ( ) ) ; } }
static void banner ( String text ) { log . info ( "" ) ; log . info ( repeat ( "" , 100 ) ) ; log . info ( text ) ; log . info ( repeat ( "" , 100 ) ) ; log . info ( "" ) ; }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Forwarding request to master {" + master . toString ( ) + "}" ) ; } }
public void test() { if ( request . isEnded ( ) ) { log . warn ( "Request to be proxied is already read" ) ; proxyEndHandler ( forwardRequest , rc . getBody ( ) ) ; } else { request . exceptionHandler ( e -> log . error ( "Could not forward request to Mesh: {}" , e , e . getMessage ( ) ) ) . endHandler ( v -> proxyEndHandler ( forwardRequest , null ) ) ; Pump . pump ( request , forwardRequest ) . setWriteQueueMaxSize ( 8192 ) . start ( ) ; } }
public void test() { try ( final DefaultClient client = factory . getClient ( ) ) { MultivaluedHashMap < String , Object > headers = new MultivaluedHashMap < > ( ) ; List < Object > objectList = new ArrayList < > ( ) ; objectList . add ( pem ) ; headers . put ( GlobalDataRest . X_SSL_CLIENT_CERT , objectList ) ; client . checkStatus ( headers ) ; } catch ( final VitamException e ) { LOGGER . error ( "THIS SHOULD NOT RAIZED AN EXCEPTION" , e ) ; fail ( "THIS SHOULD NOT RAIZED AN EXCEPTION" ) ; } finally { code_block = TryStatement ;  } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Found cloud offer with info:" + offer . toString ( ) ) ; } }
public void test() { if ( offer != null ) { code_block = IfStatement ; listOffers . add ( offer ) ; } else { log . warn ( "not found information for cloud offer name '" + offerName + "'" ) ; } }
public void test() { try { log . debug ( "Transforming frame using Word2Vec algorithm: {} " , model ) ; String url = h2oEndpoint + H2ORestApiUrls . TRANSFORM_WORD2VEC ; JsonObject queryParams = new JsonObject ( ) ; queryParams . addProperty ( "model" , model ) ; queryParams . addProperty ( "words_frame" , inputFrame + ".hex" ) ; queryParams . addProperty ( "aggregate_method" , "AVERAGE" ) ; String response = RestApiHandler . httpQueryParamRequest ( url , queryParams , null , "Get" ) ; JsonObject payload = new Gson ( ) . fromJson ( response , JsonObject . class ) ; JsonObject vectors = payload . get ( "vectors_frame" ) . getAsJsonObject ( ) ; String frameUrl = vectors . get ( "URL" ) . getAsString ( ) ; url = h2oEndpoint + frameUrl ; queryParams = new JsonObject ( ) ; queryParams . addProperty ( "row_count" , rowCount ) ; response = RestApiHandler . httpQueryParamRequest ( url , queryParams , null , "Get" ) ; return getVectorColumns ( response ) ; } catch ( Exception e ) { log . error ( e . getMessage ( ) ) ; throw new InsightsCustomException ( e . getMessage ( ) ) ; } }
public void test() { try { log . debug ( "Transforming frame using Word2Vec algorithm: {} " , model ) ; String url = h2oEndpoint + H2ORestApiUrls . TRANSFORM_WORD2VEC ; JsonObject queryParams = new JsonObject ( ) ; queryParams . addProperty ( "model" , model ) ; queryParams . addProperty ( "words_frame" , inputFrame + ".hex" ) ; queryParams . addProperty ( "aggregate_method" , "AVERAGE" ) ; String response = RestApiHandler . httpQueryParamRequest ( url , queryParams , null , "Get" ) ; JsonObject payload = new Gson ( ) . fromJson ( response , JsonObject . class ) ; JsonObject vectors = payload . get ( "vectors_frame" ) . getAsJsonObject ( ) ; String frameUrl = vectors . get ( "URL" ) . getAsString ( ) ; url = h2oEndpoint + frameUrl ; queryParams = new JsonObject ( ) ; queryParams . addProperty ( "row_count" , rowCount ) ; response = RestApiHandler . httpQueryParamRequest ( url , queryParams , null , "Get" ) ; return getVectorColumns ( response ) ; } catch ( Exception e ) { log . error ( e . getMessage ( ) ) ; throw new InsightsCustomException ( e . getMessage ( ) ) ; } }
public void test() { try { userService . save ( user ) ; } catch ( ValidationException e ) { LOG . error ( "Failed to update permssions on user <{}>" , user . getName ( ) , e ) ; } }
public void test() { if ( grnTypeCapability != null ) { final Capability capability = grnTypeCapability . capability ; GRN targetGRN ; if ( permissions . stream ( ) . anyMatch ( p code_block = LoopStatement ; code_block = "" ; code_block = IfStatement ; final List < String > updatedPermissions = user . getPermissions ( ) ; updatedPermissions . removeAll ( permissions . stream ( ) . map ( p -> p + ":" + entityID ) . collect ( Collectors . toSet ( ) ) ) ; user . setPermissions ( updatedPermissions ) ; code_block = TryStatement ;  LOG . info ( "Migrating entity <{}> permissions <{}> to <{}> grant for user <{}>" , targetGRN , permissions , capability , user . getName ( ) ) ; } else { LOG . info ( "Skipping non-migratable entity <{}>. Permissions <{}> cannot be converted to a grant capability" , entityID , permissions ) ; } }
private void reportMessageSizes ( final Map < URI , URI > msgUris , String name ) { int [ ] counter = new int [ 4 ] ; Set < URI > keys = msgUris . keySet ( ) ; code_block = ForStatement ; String sizeInfo = "\nSIZES to " + name + ":\n" + "messages=" + counter [ 0 ] + ", named-graphs=" + counter [ 1 ] + ", " + "quads=" + counter [ 2 ] + ", bytes-in-Trig-UTF8=" + counter [ 3 ] ; logger . info ( sizeInfo ) ; }
public void test() { if ( optimizer != null ) { final boolean success = optimizer . optimizeTable ( ) ; log . info ( "Optimization success status [{0}]" , success ) ; } else { log . warn ( "OptimizerRunner: The optimizer is null. Could not optimize table." ) ; } }
public void test() { if ( optimizer != null ) { final boolean success = optimizer . optimizeTable ( ) ; log . info ( "Optimization success status [{0}]" , success ) ; } else { log . warn ( "OptimizerRunner: The optimizer is null. Could not optimize table." ) ; } }
public void test() { try { orderedFields = SchemaHelper . getOrderedFieldsByMetaField ( schema , "dbFieldPosition" , new Comparator < String > ( ) code_block = "" ; ) ; int cnt = 0 ; code_block = ForStatement ; return ( T ) new SourceEvent ( keyValuePairs , getPkListFromSchema ( schema ) , schema . getName ( ) , schema . getNamespace ( ) , eventType ) ; } catch ( Exception e ) { LOGGER . error ( "Error while mapping to MysqlBinLogEvent . Exception : " + e . getMessage ( ) + " Cause: " + e . getCause ( ) , e ) ; } }
public void test() { try { WikiPageServiceUtil . deletePageAttachments ( nodeId , title ) ; } catch ( Exception exception ) { _log . error ( exception , exception ) ; throw new RemoteException ( exception . getMessage ( ) ) ; } }
public void test() { if ( ignoredEntities > 0 || prunedEntities > 0 ) { LOG . info ( "preprocess: ignored entities={}; pruned entities={}. topic-offset={}, partition={}" , ignoredEntities , prunedEntities , context . getKafkaMessageOffset ( ) , context . getKafkaPartition ( ) ) ; } }
public void test() { if ( StringUtil . isNullOrEmpty ( endpointUri ) ) { logger . debug ( "Endpoint missing. Component is disabled." ) ; return ; } }
@ Override public StreamDiscoverListsDTO execute ( final Serializable inRequest ) { log . info ( "Beginning to generate Stream Discovery lists for all users." ) ; log . info ( "Regenerating weekday count temp data to " + numberOfDaysOfWeekdayCountDataToGenerate + " days" ) ; repopulateTempWeekdaysSinceDateStrategy . execute ( numberOfDaysOfWeekdayCountDataToGenerate ) ; StreamDiscoverListsDTO result = new StreamDiscoverListsDTO ( ) ; log . info ( "Generating the list of featured streams" ) ; result . setFeaturedStreams ( featuredStreamDTOMapper . execute ( null ) ) ; log . info ( "Generating the list of most active streams" ) ; result . setMostActiveStreams ( mostActiveStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most viewed streams." ) ; result . setMostViewedStreams ( mostViewedStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most followed streams." ) ; result . setMostFollowedStreams ( mostFollowedStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most recent streams." ) ; result . setMostRecentStreams ( mostRecentStreamsMapper . execute ( null ) ) ; log . info ( "Finished generating Stream Discovery lists for all users." ) ; return result ; }
@ Override public StreamDiscoverListsDTO execute ( final Serializable inRequest ) { log . info ( "Beginning to generate Stream Discovery lists for all users." ) ; log . info ( "Regenerating weekday count temp data to " + numberOfDaysOfWeekdayCountDataToGenerate + " days" ) ; repopulateTempWeekdaysSinceDateStrategy . execute ( numberOfDaysOfWeekdayCountDataToGenerate ) ; StreamDiscoverListsDTO result = new StreamDiscoverListsDTO ( ) ; log . info ( "Generating the list of featured streams" ) ; result . setFeaturedStreams ( featuredStreamDTOMapper . execute ( null ) ) ; log . info ( "Generating the list of most active streams" ) ; result . setMostActiveStreams ( mostActiveStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most viewed streams." ) ; result . setMostViewedStreams ( mostViewedStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most followed streams." ) ; result . setMostFollowedStreams ( mostFollowedStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most recent streams." ) ; result . setMostRecentStreams ( mostRecentStreamsMapper . execute ( null ) ) ; log . info ( "Finished generating Stream Discovery lists for all users." ) ; return result ; }
@ Override public StreamDiscoverListsDTO execute ( final Serializable inRequest ) { log . info ( "Beginning to generate Stream Discovery lists for all users." ) ; log . info ( "Regenerating weekday count temp data to " + numberOfDaysOfWeekdayCountDataToGenerate + " days" ) ; repopulateTempWeekdaysSinceDateStrategy . execute ( numberOfDaysOfWeekdayCountDataToGenerate ) ; StreamDiscoverListsDTO result = new StreamDiscoverListsDTO ( ) ; log . info ( "Generating the list of featured streams" ) ; result . setFeaturedStreams ( featuredStreamDTOMapper . execute ( null ) ) ; log . info ( "Generating the list of most active streams" ) ; result . setMostActiveStreams ( mostActiveStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most viewed streams." ) ; result . setMostViewedStreams ( mostViewedStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most followed streams." ) ; result . setMostFollowedStreams ( mostFollowedStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most recent streams." ) ; result . setMostRecentStreams ( mostRecentStreamsMapper . execute ( null ) ) ; log . info ( "Finished generating Stream Discovery lists for all users." ) ; return result ; }
@ Override public StreamDiscoverListsDTO execute ( final Serializable inRequest ) { log . info ( "Beginning to generate Stream Discovery lists for all users." ) ; log . info ( "Regenerating weekday count temp data to " + numberOfDaysOfWeekdayCountDataToGenerate + " days" ) ; repopulateTempWeekdaysSinceDateStrategy . execute ( numberOfDaysOfWeekdayCountDataToGenerate ) ; StreamDiscoverListsDTO result = new StreamDiscoverListsDTO ( ) ; log . info ( "Generating the list of featured streams" ) ; result . setFeaturedStreams ( featuredStreamDTOMapper . execute ( null ) ) ; log . info ( "Generating the list of most active streams" ) ; result . setMostActiveStreams ( mostActiveStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most viewed streams." ) ; result . setMostViewedStreams ( mostViewedStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most followed streams." ) ; result . setMostFollowedStreams ( mostFollowedStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most recent streams." ) ; result . setMostRecentStreams ( mostRecentStreamsMapper . execute ( null ) ) ; log . info ( "Finished generating Stream Discovery lists for all users." ) ; return result ; }
@ Override public StreamDiscoverListsDTO execute ( final Serializable inRequest ) { log . info ( "Beginning to generate Stream Discovery lists for all users." ) ; log . info ( "Regenerating weekday count temp data to " + numberOfDaysOfWeekdayCountDataToGenerate + " days" ) ; repopulateTempWeekdaysSinceDateStrategy . execute ( numberOfDaysOfWeekdayCountDataToGenerate ) ; StreamDiscoverListsDTO result = new StreamDiscoverListsDTO ( ) ; log . info ( "Generating the list of featured streams" ) ; result . setFeaturedStreams ( featuredStreamDTOMapper . execute ( null ) ) ; log . info ( "Generating the list of most active streams" ) ; result . setMostActiveStreams ( mostActiveStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most viewed streams." ) ; result . setMostViewedStreams ( mostViewedStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most followed streams." ) ; result . setMostFollowedStreams ( mostFollowedStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most recent streams." ) ; result . setMostRecentStreams ( mostRecentStreamsMapper . execute ( null ) ) ; log . info ( "Finished generating Stream Discovery lists for all users." ) ; return result ; }
@ Override public StreamDiscoverListsDTO execute ( final Serializable inRequest ) { log . info ( "Beginning to generate Stream Discovery lists for all users." ) ; log . info ( "Regenerating weekday count temp data to " + numberOfDaysOfWeekdayCountDataToGenerate + " days" ) ; repopulateTempWeekdaysSinceDateStrategy . execute ( numberOfDaysOfWeekdayCountDataToGenerate ) ; StreamDiscoverListsDTO result = new StreamDiscoverListsDTO ( ) ; log . info ( "Generating the list of featured streams" ) ; result . setFeaturedStreams ( featuredStreamDTOMapper . execute ( null ) ) ; log . info ( "Generating the list of most active streams" ) ; result . setMostActiveStreams ( mostActiveStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most viewed streams." ) ; result . setMostViewedStreams ( mostViewedStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most followed streams." ) ; result . setMostFollowedStreams ( mostFollowedStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most recent streams." ) ; result . setMostRecentStreams ( mostRecentStreamsMapper . execute ( null ) ) ; log . info ( "Finished generating Stream Discovery lists for all users." ) ; return result ; }
@ Override public StreamDiscoverListsDTO execute ( final Serializable inRequest ) { log . info ( "Beginning to generate Stream Discovery lists for all users." ) ; log . info ( "Regenerating weekday count temp data to " + numberOfDaysOfWeekdayCountDataToGenerate + " days" ) ; repopulateTempWeekdaysSinceDateStrategy . execute ( numberOfDaysOfWeekdayCountDataToGenerate ) ; StreamDiscoverListsDTO result = new StreamDiscoverListsDTO ( ) ; log . info ( "Generating the list of featured streams" ) ; result . setFeaturedStreams ( featuredStreamDTOMapper . execute ( null ) ) ; log . info ( "Generating the list of most active streams" ) ; result . setMostActiveStreams ( mostActiveStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most viewed streams." ) ; result . setMostViewedStreams ( mostViewedStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most followed streams." ) ; result . setMostFollowedStreams ( mostFollowedStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most recent streams." ) ; result . setMostRecentStreams ( mostRecentStreamsMapper . execute ( null ) ) ; log . info ( "Finished generating Stream Discovery lists for all users." ) ; return result ; }
@ Override public StreamDiscoverListsDTO execute ( final Serializable inRequest ) { log . info ( "Beginning to generate Stream Discovery lists for all users." ) ; log . info ( "Regenerating weekday count temp data to " + numberOfDaysOfWeekdayCountDataToGenerate + " days" ) ; repopulateTempWeekdaysSinceDateStrategy . execute ( numberOfDaysOfWeekdayCountDataToGenerate ) ; StreamDiscoverListsDTO result = new StreamDiscoverListsDTO ( ) ; log . info ( "Generating the list of featured streams" ) ; result . setFeaturedStreams ( featuredStreamDTOMapper . execute ( null ) ) ; log . info ( "Generating the list of most active streams" ) ; result . setMostActiveStreams ( mostActiveStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most viewed streams." ) ; result . setMostViewedStreams ( mostViewedStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most followed streams." ) ; result . setMostFollowedStreams ( mostFollowedStreamsMapper . execute ( null ) ) ; log . info ( "Generating the list of most recent streams." ) ; result . setMostRecentStreams ( mostRecentStreamsMapper . execute ( null ) ) ; log . info ( "Finished generating Stream Discovery lists for all users." ) ; return result ; }
public void test() { if ( _log . isDebugEnabled ( ) ) { _log . debug ( _NO_SUCH_ENTITY_WITH_PRIMARY_KEY + primaryKey ) ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( KBArticleServiceUtil . class , "getLatestKBArticle" , _getLatestKBArticleParameterTypes26 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , resourcePrimKey , status ) ; Object returnObj = null ; code_block = TryStatement ;  return ( com . liferay . knowledge . base . model . KBArticle ) returnObj ; } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
public void test() { try { MethodKey methodKey = new MethodKey ( CommerceBOMFolderServiceUtil . class , "deleteCommerceBOMFolder" , _deleteCommerceBOMFolderParameterTypes1 ) ; MethodHandler methodHandler = new MethodHandler ( methodKey , commerceBOMFolderId ) ; code_block = TryStatement ;  } catch ( com . liferay . portal . kernel . exception . SystemException systemException ) { _log . error ( systemException , systemException ) ; throw systemException ; } }
@ Test public void mysqlParse6 ( ) { final String jdbcUrl = CONNECTION_STRING + "?useUnicode=true&characterEncoding=UTF-8&noAccessToProcedureBodies=true&autoDeserialize=true&elideSetAutoCommits=true&sessionVariables=time_zone='%2B09:00',tx_isolation='READ-COMMITTED'" ; DatabaseInfo dbInfo = jdbcUrlParser . parse ( jdbcUrl ) ; Assert . assertTrue ( dbInfo . isParsingComplete ( ) ) ; Assert . assertEquals ( dbInfo . getType ( ) , SERVICE_TYPE ) ; Assert . assertEquals ( dbInfo . getHost ( ) . get ( 0 ) , IP_PORT ) ; Assert . assertEquals ( dbInfo . getDatabaseId ( ) , DATABASE_ID ) ; Assert . assertEquals ( dbInfo . getUrl ( ) , CONNECTION_STRING ) ; logger . debug ( dbInfo . toString ( ) ) ; }
private void logException ( IOException e ) { code_block = IfStatement ; logger . warn ( "Unable to write trace to disk" , e ) ; exceptionLogged = true ; }
public void test() { try { HeartbeatBean bean = heartbeatBeanRef . get ( ) ; code_block = IfStatement ; final HeartbeatPayload hbPayload = new HeartbeatPayload ( ) ; hbPayload . setSystemStartTime ( systemStartTime ) ; hbPayload . setActiveThreadCount ( getActiveThreadCount ( ) ) ; hbPayload . setRevisionUpdateCount ( revisionManager . getRevisionUpdateCount ( ) ) ; final QueueSize queueSize = getTotalFlowFileCount ( bean . getRootGroup ( ) ) ; hbPayload . setTotalFlowFileCount ( queueSize . getObjectCount ( ) ) ; hbPayload . setTotalFlowFileBytes ( queueSize . getByteCount ( ) ) ; hbPayload . setClusterStatus ( clusterCoordinator . getConnectionStatuses ( ) ) ; final NodeIdentifier nodeId = getNodeId ( ) ; code_block = IfStatement ; final Heartbeat heartbeat = new Heartbeat ( nodeId , connectionStatus , hbPayload . marshal ( ) ) ; final HeartbeatMessage message = new HeartbeatMessage ( ) ; message . setHeartbeat ( heartbeat ) ; LOG . debug ( "Generated heartbeat" ) ; return message ; } catch ( final Throwable ex ) { LOG . warn ( "Failed to create heartbeat due to: " + ex , ex ) ; return null ; } }
public void persist ( TmpItvSel transientInstance ) { log . debug ( "persisting TmpItvSel instance" ) ; code_block = TryStatement ;  }
public void test() { try { sessionFactory . getCurrentSession ( ) . persist ( transientInstance ) ; log . debug ( "persist successful" ) ; } catch ( RuntimeException re ) { log . error ( "persist failed" , re ) ; throw re ; } }
public void test() { try { sessionFactory . getCurrentSession ( ) . persist ( transientInstance ) ; log . debug ( "persist successful" ) ; } catch ( RuntimeException re ) { log . error ( "persist failed" , re ) ; throw re ; } }
public void test() { try { assertSshable ( machineConfig ) ; Assert . fail ( "ssh should not have succeeded " + machineConfig ) ; } catch ( Exception e ) { LOG . debug ( "Exception as expected when testing sshable " + machineConfig ) ; } }
public void test() { if ( LOGGER . isTraceEnabled ( ) ) { LOGGER . trace ( getClass ( ) . getSimpleName ( ) + "::read" ) ; } }
public void test() { try { long requestFlow = ( long ) ( ( ByteBuf ) msg ) . readableBytes ( ) ; counterContainer . addRequestFlow ( requestFlow ) ; } catch ( Throwable e ) { LOGGER . error ( e . getMessage ( ) , e ) ; } finally { ctx . fireChannelRead ( msg ) ; } }
public void test() { { final Integer amount = Optional . ofNullable ( result ) . map ( Collection :: size ) . orElse ( 0 ) ; LOG . debug ( "[epPolicyTemplateProvider] harvestAll succeeded: {}" , amount ) ; } }
public void test() { try { logger . trace ( "Liste Filme lesen von: {}" , source ) ; listeFilme . clear ( ) ; notifyStart ( source ) ; checkDays ( days ) ; code_block = IfStatement ; else processFromFile ( source , listeFilme ) ; } catch ( MalformedURLException ex ) { logger . warn ( ex ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "Failure handling failure message for Event " + event + " (" + eventType + ") and Request " + request , e ) ; } }
public void test() { try ( AccessInternalClient client = accessInternalClientFactory . getClient ( ) ) { return client . startTransferReplyWorkflow ( transferReply ) . toResponse ( ) ; } catch ( Exception e ) { LOGGER . error ( e ) ; return Response . status ( Status . INTERNAL_SERVER_ERROR ) . entity ( getErrorEntity ( Status . INTERNAL_SERVER_ERROR , e . getLocalizedMessage ( ) ) ) . build ( ) ; } }
public void test() { try { return retrieve ( URI . create ( id ) , Imeji . adminUser ) ; } catch ( ImejiException e ) { LOGGER . info ( "Cannot retrieve space: " + id ) ; } }
public void test() { try { HiveConf hiveConf = new HiveConf ( ) ; hiveConf . set ( "hive.metastore.uris" , IMConfig . getProperty ( "etl.hive-metastore-uris" ) ) ; hiveConf . set ( "hive.exec.dynamic.partition.mode" , "nonstrict" ) ; hiveConf . set ( "hive.exec.dynamic.partition" , "true" ) ; hiveConf . set ( "hive.exec.max.dynamic.partitions.pernode" , "1000" ) ; hiveClient = new HiveMetaStoreClient ( hiveConf ) ; } catch ( MetaException e ) { LOGGER . error ( e ) ; throw new ETLException ( e ) ; } }
public void test() { if ( DEBUG_ENABLED ) { LOG . debug ( "Creating new category anchor, category = {}, glossary = {}" , storeObject . getGuid ( ) , updatedCategory . getAnchor ( ) . getGlossaryGuid ( ) ) ; } }
public void test() { if ( DEBUG_ENABLED ) { LOG . debug ( "Updating category anchor, currAnchor = {}, newAnchor = {} and category = {}" , storeObject . getAnchor ( ) . getGlossaryGuid ( ) , updatedCategory . getAnchor ( ) . getGlossaryGuid ( ) , storeObject . getGuid ( ) ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "Derived qualifiedName = {}" , storeObject . getQualifiedName ( ) ) ; } }
public void test() { if ( DEBUG_ENABLED ) { LOG . debug ( "Deleting category anchor" ) ; } }
public void test() { if ( ++ windowCount == logWindows ) { long endTime = System . currentTimeMillis ( ) ; totalCount += tupleCount ; logger . info ( "total: count: {}; time: {}; average: {}; period: count: {}; time: {}; average: {}" , totalCount , endTime - totalBeginTime , totalCount * 1000 / ( endTime - totalBeginTime ) , tupleCount , endTime - beginTime , tupleCount * 1000 / ( endTime - beginTime ) ) ; windowCount = 0 ; beginTime = System . currentTimeMillis ( ) ; tupleCount = 0 ; } }
public void test() { if ( cache . isLocalLocked ( key , false ) ) { log . info ( "Key is locked [key=" + key + ", node=" + ignite . name ( ) + ']' ) ; return false ; } }
public void test() { try { result . put ( "id" , this . getId ( ) ) ; result . put ( "level" , this . getLevel ( ) ) ; result . put ( "fileDesc" , this . getFileDesc ( ) ) ; result . put ( "fileName" , this . getFileName ( ) ) ; result . put ( "fileType" , this . getFileType ( ) ) ; } catch ( JSONException ex ) { LOG . error ( ex . toString ( ) , ex ) ; } }
public void test() { try { return ( NextState ) scannerState . get ( instance ) ; } catch ( final Exception e ) { LOGGER . warn ( "Unable to check partial result of scanner context" , e ) ; exception = true ; } }
private DataObjectAttrExecutor getDayProfileTablePassiveExecutor ( final Set < DayProfileDto > dayProfileSet ) { final AttributeAddress dayProfileTablePassive = new AttributeAddress ( CLASS_ID , OBIS_CODE , ATTRIBUTE_ID_DAY_PROFILE_TABLE_PASSIVE ) ; final DataObject dayArray = DataObject . newArrayData ( this . configurationMapper . mapAsList ( dayProfileSet , DataObject . class ) ) ; LOGGER . info ( "DayProfileTablePassive to set is: {}" , this . dlmsHelper . getDebugInfo ( dayArray ) ) ; return new DataObjectAttrExecutor ( "DAYS" , dayProfileTablePassive , dayArray , CLASS_ID , OBIS_CODE , ATTRIBUTE_ID_DAY_PROFILE_TABLE_PASSIVE ) ; }
public void test() { if ( terminationPointIdInTopology != null ) { InstanceIdentifier < TerminationPoint > iiToTopologyTerminationPoint = provideIIToTopologyTerminationPoint ( terminationPointIdInTopology , iiToNodeInInventory ) ; TerminationPoint point = prepareTopologyTerminationPoint ( terminationPointIdInTopology , iiToNodeInInventory ) ; sendToTransactionChain ( point , iiToTopologyTerminationPoint ) ; removeLinks ( modification . getRootNode ( ) . getDataAfter ( ) , point ) ; } else { LOG . debug ( "Inventory node connector key is null. Data can't be written to topology termination point" ) ; } }
public void test() { if ( ! ( nci instanceof NormalizedFieldAndValue ) ) { log . warn ( "Can't handle a " + nci . getClass ( ) + "; must be a NormalizedFieldAndValue." ) ; } }
public void test() { if ( ! ( nci instanceof NormalizedFieldAndValue ) ) { log . warn ( "Can't handle a " + nci . getClass ( ) + "; must be a NormalizedFieldAndValue." ) ; } }
public void test() { try { daoManager . startTransaction ( em ) ; result = em . find ( UserTokenEntity . class , token ) ; daoManager . commitTransaction ( em ) ; } catch ( Exception e ) { daoManager . rollBackTransaction ( em ) ; logger . error ( "**** Error in UserTokenDAO:" , e ) ; } finally { daoManager . closeEntityManager ( em ) ; } }
public void test() { if ( _log . isWarnEnabled ( ) ) { _log . warn ( exception , exception ) ; } }
public void test() { if ( liveMsgContent . equals ( textMessage . getText ( ) ) ) { success . countDown ( ) ; } else { listenerFailure . set ( true ) ; LOG . error ( "Received unexpected message:" + incoming ) ; } }
public void test() { try { TextMessage textMessage = ( TextMessage ) incoming ; code_block = IfStatement ; } catch ( Exception e ) { listenerFailure . set ( true ) ; LOG . error ( "Exception in listener" , e ) ; } }
public void test() { if ( updatedController . canHandleRequest ( request ) ) { return updatedController . processRequest ( request , nextRequestMono ) . doOnError ( throwable -> this . handleException ( throwable ) ) ; } else { logger . warn ( "Can not find request controller to handle request {} with pkRangeId {}" , request . getActivityId ( ) , this . getResolvedPartitionKeyRangeId ( request ) ) ; return nextRequestMono ; } }
public void test() { try { Collection collectionToUpdate = collectionService . findByCode ( code ) ; code_block = IfStatement ; return getUIWrapper ( null , true ) ; } catch ( Exception e ) { logger . error ( "Error while updating isMicromapperEnabled flag for crisis: " + code , e ) ; return getUIWrapper ( false , "Unable to update micromapperEnabled for collection, code = " + code ) ; } }
public void test() { try { checkInitialized ( events . get ( 0 ) . getRegion ( ) ) ; } catch ( RuntimeException ex ) { changeFailedEvents ( events . size ( ) ) ; logger . error ( "Exception initializing JdbcAsyncWriter" , ex ) ; return true ; } }
public void test() { if ( isInstall ) { installServerAssembly ( ) ; } else { log . info ( MessageFormat . format ( messages . getString ( "info.install.type.preexisting" ) , "" ) ) ; checkServerHomeExists ( ) ; } }
public void test() { if ( createServer ) { log . info ( MessageFormat . format ( messages . getString ( "info.server.start.create" ) , serverName ) ) ; ServerTask serverTask = initializeJava ( ) ; serverTask . setOperation ( "create" ) ; serverTask . setTemplate ( template ) ; serverTask . setNoPassword ( noPassword ) ; serverTask . execute ( ) ; log . info ( MessageFormat . format ( messages . getString ( "info.server.create.created" ) , serverName , serverDirectory . getCanonicalPath ( ) ) ) ; } }
public void test() { try { item = handler . getPresetContainer ( ) . get ( command . intValue ( ) ) ; postContentItem ( item ) ; } catch ( NoPresetFoundException e ) { logger . warn ( "{}: No preset found at id: {}" , handler . getDeviceName ( ) , command . intValue ( ) ) ; } }
@ Test public void test_02_long ( ) { Log . debug ( "Test" ) ; long seed = 20100614 ; int lenMask = 0xffff ; int numTests = 10 ; randDnaSeqTest ( numTests , lenMask , seed ) ; }
@ Override public void setFirewallPortForwardingConfiguration ( List < FirewallPortForwardConfigIP < ? extends IPAddress > > firewallConfiguration ) throws KuraException { logger . debug ( "setFirewallPortForwardingConfiguration() :: Deleting port forward rules" ) ; deleteAllPortForwardRules ( ) ; ArrayList < PortForwardRule > portForwardRules = new ArrayList < > ( ) ; code_block = ForStatement ; addPortForwardRules ( portForwardRules ) ; }
public void test() { try { portForwardEntry . setPermittedNetwork ( getNetworkPair00 ( ) ) ; } catch ( UnknownHostException e ) { logger . info ( e . getMessage ( ) , e ) ; } }
@ Override public void memberRemoved ( MembershipEvent membershipEvent ) { LOGGER . info ( "Member removed: " + membershipEvent . getMember ( ) ) ; LOGGER . info ( "list of members now :" + membershipEvent . getMembers ( ) ) ; }
@ Override public void memberRemoved ( MembershipEvent membershipEvent ) { LOGGER . info ( "Member removed: " + membershipEvent . getMember ( ) ) ; LOGGER . info ( "list of members now :" + membershipEvent . getMembers ( ) ) ; }
public void test() { if ( metaClient . getFs ( ) . exists ( indexPath ) ) { LOG . info ( "Dropping bootstrap index. Deleting file : " + indexPath ) ; metaClient . getFs ( ) . delete ( indexPath ) ; } }
public void test() { try { String json = dbManager . selectClonedProject ( token ) ; code_block = IfStatement ; code_block = IfStatement ; } catch ( Exception e ) { log . error ( "Error cloning project." , e ) ; ctx . writeAndFlush ( serverError ( "Error getting cloned project." ) , ctx . voidPromise ( ) ) ; } }
public void test() { if ( properties . getConnectionProperties ( ) . authenticationType . getValue ( ) != AuthType . ACTIVE_DIRECTORY_CLIENT_CREDENTIAL ) { throw new ComponentException ( e ) ; } else { LOGGER . debug ( "Azure tables not supported with Active directory authentication" ) ; return Collections . emptyList ( ) ; } }
public void test() { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( "[" + id_ + "] setGlobalAlpha(" + globalAlpha + ")" ) ; } }
public void test() { try { PluginLoader loader = PluginLoader . create ( jarPath , _loader ) ; PluginLoader . setClassLoader ( loader ) ; pluginClass = loader . loadPlugin ( ) ; handler = ( AgentServerHandler ) pluginClass . newInstance ( ) ; } catch ( Exception e ) { this . logger . error ( "Error loading server handler jar" , e ) ; throw new AgentLoaderException ( "Unable to load server handler " + "jar: " + e . getMessage ( ) ) ; } finally { code_block = IfStatement ; } }
public void test() { try { checkThreads ( ) ; } catch ( Throwable t ) { LOG . error ( "While checking threads" , t ) ; } finally { LOG . trace ( "Finished checking threads after {}" , JavaUtils . duration ( start ) ) ; } }
public void test() { try { checkThreads ( ) ; } catch ( Throwable t ) { LOG . error ( "While checking threads" , t ) ; } finally { LOG . trace ( "Finished checking threads after {}" , JavaUtils . duration ( start ) ) ; } }
private void benchmarkAddExtendedDataRows ( Random random , int vertexCount , int extendedDataRowCount ) { double startTime = System . currentTimeMillis ( ) ; code_block = ForStatement ; graph . flush ( ) ; double endTime = System . currentTimeMillis ( ) ; LOGGER . info ( "add rows in %.3fs" , ( endTime - startTime ) / 1000 ) ; }
public void test() { try { GSSManager manager = GSSManager . getInstance ( ) ; GSSName peerName = manager . createName ( servicePrincipalName , GSSName . NT_HOSTBASED_SERVICE ) ; GSSContext context = manager . createContext ( peerName , null , null , GSSContext . DEFAULT_LIFETIME ) ; code_block = WhileStatement ; return context ; } catch ( Exception e ) { log . error ( "Unable to authenticate client against Kerberos" , e ) ; return null ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Attempting to read ConfigMap " + configMapName ) ; } }
public void test() { if ( log . isDebugEnabled ( ) ) { log . debug ( "Done reading ConfigMap " + configMap . getMetadata ( ) . getName ( ) ) ; } }
public void test() { for ( String body : bodies ) { LOG . error ( "Never found event body: {}" , body ) ; } }
public static void createIndices ( ) { logger . trace ( "Creating SQL indices" ) ; code_block = TryStatement ;  logger . trace ( "Finished creating SQL indices" ) ; }
public void test() { try ( var connection = PooledDatabaseConnection . INSTANCE . getDataSource ( ) . getConnection ( ) ; var ignored = new SqlAutoSetAutoCommit ( connection , false ) ; var tm = new SqlAutoRollback ( connection ) ; var statement = connection . createStatement ( ) ) { statement . executeUpdate ( "CREATE INDEX IF NOT EXISTS IDX_DESC_ID ON mediathekview.description (id)" ) ; statement . executeUpdate ( "CREATE INDEX IF NOT EXISTS IDX_WEBSITE_LINKS_ID ON mediathekview.website_links (id)" ) ; tm . commit ( ) ; } catch ( SQLException ex ) { logger . error ( ex ) ; } }
public void test() { try { byName = InetAddress . getByName ( substringBeforeSlash ( rendEp . getAddress ( ) ) ) ; } catch ( UnknownHostException e ) { LOG . error ( "Failed to parse IP address {}" , e ) ; return null ; } }
public void test() { try { return new URI ( this . getURIPart ( ) ) ; } catch ( final URISyntaxException e ) { BPELRESTLightElement . LOG . error ( "Internal Error: Stored URI was invalid" , e ) ; } }
public void test() { try { LOGGER . debug ( "Got response from operator {} after: {}" , mergedBlocksNumber , ( System . currentTimeMillis ( ) - startTime ) ) ; CombineService . mergeTwoBlocks ( _brokerRequest , mergedBlock , blockToMerge ) ; LOGGER . debug ( "Merged response from operator {} after: {}" , mergedBlocksNumber , ( System . currentTimeMillis ( ) - startTime ) ) ; } catch ( Exception e ) { mergedBlock . getExceptions ( ) . add ( QueryException . getException ( QueryException . MERGE_RESPONSE_ERROR , e ) ) ; } }
public void test() { try { LOGGER . debug ( "Got response from operator {} after: {}" , mergedBlocksNumber , ( System . currentTimeMillis ( ) - startTime ) ) ; CombineService . mergeTwoBlocks ( _brokerRequest , mergedBlock , blockToMerge ) ; LOGGER . debug ( "Merged response from operator {} after: {}" , mergedBlocksNumber , ( System . currentTimeMillis ( ) - startTime ) ) ; } catch ( Exception e ) { mergedBlock . getExceptions ( ) . add ( QueryException . getException ( QueryException . MERGE_RESPONSE_ERROR , e ) ) ; } }
public void test() { try { sendAndStateLock . lock ( ) ; handleEvent ( new Event ( Event . Type . FAILED_SEND_RECORD , createAccountRequest ( request ) ) ) ; } catch ( Exception e ) { logger . debug ( "Can not handle timeout event" , e ) ; } finally { sendAndStateLock . unlock ( ) ; } }
public void test() { try { code_block = IfStatement ; } catch ( ArrayIndexOutOfBoundsException e ) { logger . debug ( "Error detecting the EZSP frame type" , e ) ; return null ; } }
public void test() { try { ctor = ezspClass . getConstructor ( int [ ] . class ) ; EzspFrameResponse ezspFrame = ( EzspFrameResponse ) ctor . newInstance ( data ) ; return ezspFrame ; } catch ( SecurityException | NoSuchMethodException | IllegalArgumentException | InstantiationException | IllegalAccessException | InvocationTargetException e ) { logger . debug ( "Error creating instance of EzspFrame" , e ) ; } }
public void test() { try { return resourceBundleLocator . getBundle ( INTRIGUE_BASE_NAME ) . getString ( "validation.attribute.unsupported" ) ; } catch ( IOException e ) { LOGGER . debug ( "Failed getting {} resource bundle, using default \"{}\"" , INTRIGUE_BASE_NAME , DEFAULT_MESSAGE_FORMAT ) ; return DEFAULT_MESSAGE_FORMAT ; } }
public void test() { try { Configuration [ ] configurations = configAdmin . listConfigurations ( "(service.factoryPid=" + factoryPid + ")" ) ; code_block = IfStatement ; } catch ( InvalidSyntaxException e ) { logger . debug ( "invalid syntax" , e ) ; } }
public void test() { if ( logger . isDebugEnabled ( ) ) { logger . debug ( "MembershipViewReplyProcessor received {}" , view ) ; } }
public void test() { if ( localeResolver != null ) { LOG . debug ( "Setting custom LocaleResolver of type " + localeResolver . getClass ( ) . getName ( ) ) ; } }
public void checkPolicy ( ) { log . info ( "controller.policyFlags()" ) ; }
public void test() { try { HuiRelation relation = HUITypeFactory . getInstance ( ) . getRelation ( linkId ) ; String hql ; code_block = IfStatement ; IBaseDao < Configuration , Serializable > dao = getDaoFactory ( ) . getDAO ( Configuration . class ) ; List < ? > result = dao . findByQuery ( hql , new String [ ] code_block = "" ; ) ; code_block = IfStatement ; } catch ( Exception t ) { logger . error ( "Error while loading username for control uuid: " + uuid , t ) ; } }
@ Override public Void call ( ) throws Exception { SchedulableProgramType programType = node . getProgram ( ) . getProgramType ( ) ; String programName = node . getProgram ( ) . getProgramName ( ) ; String prettyProgramType = ProgramType . valueOf ( programType . name ( ) ) . getPrettyName ( ) ; ProgramWorkflowRunner programWorkflowRunner = workflowProgramRunnerFactory . getProgramWorkflowRunner ( programType , token , node . getNodeId ( ) , nodeStates ) ; code_block = IfStatement ; Runnable programRunner = programWorkflowRunner . create ( programName ) ; LOG . info ( "Starting {} Program '{}' in workflow" , prettyProgramType , programName ) ; programRunner . run ( ) ; LOG . info ( "{} Program '{}' in workflow completed" , prettyProgramType , programName ) ; return null ; }
@ Override public Void call ( ) throws Exception { SchedulableProgramType programType = node . getProgram ( ) . getProgramType ( ) ; String programName = node . getProgram ( ) . getProgramName ( ) ; String prettyProgramType = ProgramType . valueOf ( programType . name ( ) ) . getPrettyName ( ) ; ProgramWorkflowRunner programWorkflowRunner = workflowProgramRunnerFactory . getProgramWorkflowRunner ( programType , token , node . getNodeId ( ) , nodeStates ) ; code_block = IfStatement ; Runnable programRunner = programWorkflowRunner . create ( programName ) ; LOG . info ( "Starting {} Program '{}' in workflow" , prettyProgramType , programName ) ; programRunner . run ( ) ; LOG . info ( "{} Program '{}' in workflow completed" , prettyProgramType , programName ) ; return null ; }
public void test() { try { index ( upperSession , entity ( upperSession , tuple ) ) ; } catch ( Throwable e ) { errorHandler . handleException ( log . massIndexerUnexpectedErrorMessage ( ) , e ) ; } finally { log . debug ( "finished" ) ; } }
public void test() { switch ( status ) { case FAULT : evt . setAspect ( ProcessMessageExchangeEvent . PARTNER_FAULT ) ; responseChannel . onFault ( ) ; break ; case RESPONSE : evt . setAspect ( ProcessMessageExchangeEvent . PARTNER_OUTPUT ) ; responseChannel . onResponse ( ) ; break ; case FAILURE : evt . setAspect ( ProcessMessageExchangeEvent . PARTNER_FAILURE ) ; responseChannel . onFailure ( ) ; break ; default : __log . error ( "Invalid response state for mex " + mexid + ": " + status ) ; } }